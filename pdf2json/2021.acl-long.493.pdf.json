{
  "name" : "2021.acl-long.493.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "StructuralLM: Structural Pre-training for Form Understanding",
    "authors" : [ "Chenliang Li", "Bin Bi", "Ming Yan", "Wei Wang", "Songfang Huang", "Fei Huang" ],
    "emails" : [ "hebian.ww}@alibaba-inc.com", "luo.si}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6309–6318\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6309"
    }, {
      "heading" : "1 Introduction",
      "text" : "Document understanding is an essential problem in NLP, which aims to read and analyze textual documents. In addition to plain text, many realworld applications require to understand scanned documents with rich text. As shown in Figure 1, such scanned documents contain various structured information, like tables, digital forms, receipts, and invoices. The information of a document image is usually presented in natural language, but the format can be organized in many ways from multicolumn layout to various tables/forms.\nInspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) have pushed the limits of a variety of document image understanding tasks, which learn the interaction be-\ntween text and layout information across scanned document images.\nXu et al. (2019) propose LayoutLM, which is a pre-training method of text and layout for document image understanding tasks. It uses 2Dposition embeddings to model the word-level layout information. However, it is not enough to model the word-level layout information, and the model should consider the cell as a semantic unit. It is important to know which words are from the same cell and to model the cell-level layout information. For example, as shown in Figure 1 (a), which is from form understanding task (Jaume et al., 2019), determining that the ”LORILLARD” and the ”ENTITIES” are from the same cell is critical for semantic entity labeling. The ”LORILLARD ENTITIES” should be predicted as Answer entity, but LayoutLM predicts ”LORILLARD” and ”ENTITIES” as two separate entities.\nThe input to traditional natural language tasks is usually presented as plain text, and text-only models need to obtain the semantic representation of the input sentences and the semantic relationship between sentences. In contrast, document images like forms and tables are composed of cells that are recognized as bounding boxes by OCR. As shown in Figure 1, the words from the same cell generally express a meaning together and should be modeled as a semantic unit. This requires a text-layout model to capture not only the semantic representation of individual cells but also the spatial relationship between cells.\nIn this paper, we propose StructuralLM to jointly exploit cell and layout information from scanned documents. Different from previous text-based pretrained models (Devlin et al., 2019; Wang et al., 2019) and LayoutLM (Xu et al., 2019), StructuralLM uses cell-level 2D-position embeddings with tokens in a cell sharing the same 2D-position. This makes StructuralLM aware of which words are\nfrom the same cell, and thus enables the model to derive representation for the cells. In addition, we keep classic 1D-position embeddings to preserve the positional relationship of the tokens within every cell. We propose a new pre-training objective called cell position classification, in addition to the masked visual-language model. Specifically, we first divide an image into N areas of the same size, and then mask the 2D-positions of some cells. StructuralLM is asked to predict which area the masked cells are located in. In this way, StructuralLM is capable of learning the interactions between cells and layout. We conduct experiments on three benchmark datasets publicly available, all of which contain table or form images. Empirical results show that our StructuralLM outperforms strong baselines and achieves new state-of-the-art results in the downstream tasks. In addition, StructuralLM does not rely on image features, and thus is readily applicable to real-world document understanding tasks.\nWe summarize the major contributions in this paper as follows:\n• We propose a structural pre-trained model for table and form understanding. It jointly leverages cells and layout information in two ways: cell-level positional embeddings and a new pre-training objective called cell position classification.\n• StructuralLM significantly outperforms all state-of-the-art models in several downstream tasks including form understanding (from 78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and document image classification (from 94.43 to 96.08)."
    }, {
      "heading" : "2 StructuralLM",
      "text" : "We present StructuralLM, a self-supervised pretraining method designed to better model the interactions of cells and layout information in scanned document images. The overall framework of StructuralLM is shown in Figure 2. Our approach is inspired by LayoutLM (Xu et al., 2019), but different from it in three ways. First, we use cell-level 2D-position embeddings to model the layout information of cells rather than word-level 2D-position embeddings. We also introduce a novel training objective, the cell position classification, which tries to predict the position of the cells only depending on the position of surrounding cells and the semantic relationship between them. Finally, StructuralLM retains the 1D-position embeddings to model the positional relationship between tokens from the same cell, and removes the image embeddings in LayoutLM that is only used in the downstream tasks."
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "The architecture overview of StructuralLM is shown in Figure 2. To take advantage of existing pre-trained models and adapt to document image understanding tasks, we use the BERT (Devlin et al., 2019) architecture as the backbone. The BERT model is an attention-based bidirectional language modeling approach. It has been verified that the BERT model shows effective knowledge transfer from the self-supervised nlp tasks with a large-scale pre-training corpus.\nBased on the architecture, we propose to utilize the cell-level layout information from document images and incorporate them into the transformer encoder. First, given a set of tokens from different\ncells and the layout information of cells, the celllevel input embeddings are computed by summing the corresponding word embeddings, cell-level 2Dposition embeddings, and original 1D-position embeddings. Then, these input embeddings are passed through a bidirectional Transformer encoder that can generate contextualized representations with an attention mechanism."
    }, {
      "heading" : "2.2 Cell-level Input Embedding",
      "text" : "Given document images, we use an OCR tool to recognize text and serialize the cells (bounding boxes) from top-left to bottom-right. Each document image is represented as a sequence of cells {c1, ..., cn}, and each cell is composed of a sequence of words ci = {w1i , ..., wmi }. Given the sequences of cells and words, we first introduce the method of cell-level input embedding.\nCell-level Layout Embedding. Unlike the position embedding that models the word position in a sequence, the 2D-position embedding aims to model the relative spatial position in a document image. To represent the spatial position of cells in scanned document images, we consider a document page as a coordinate system with the top-left origin. In this setting, the cell (bounding box) can be precisely defined by (x0, y0, x1, y1), where (x0, y0) corresponds to the top-left position, and (x1, y1) represents the bottom-right position. Therefore, we add two cell-level position embedding layers to embed x-axis features and y-axis features separately. The words {w1i , ..., wmi } in i-th cell ci share the same 2D-position embeddings, which is different from the word-level 2D-position embedding in LayoutLM. As shown in Figure 2, the input to-\nkens with the same color background are from the same cell, and the corresponding 2D-positions are also the same. In this way, StructuralLM can not only learn the layout information of cells but also know which words are from the same cell, which is better to obtain the contextual representation of cells. In addition, we keep the classic 1D-position embeddings to preserve the positional relationship of the tokens within the same cell. Finally, the celllevel layout embeddings are computed by summing the four 2D-position embeddings and the classic 1D-position embeddings.\nInput Embedding. Given a sequence of cells {c1, ..., cn}, we use WordPiece (Wu et al., 2016) to tokenize the words in the cells. The length of the text sequence is limited to ensure that the length of the final sequence is not greater than the maximum sequence length L. The final cell-level input embedding is the sum of the three embeddings. Word embedding represents the word itself, 1Dposition embedding represents the token index, and cell-level 2D-position embedding is used to model the relative spatial position of cells in a document image."
    }, {
      "heading" : "2.3 Pre-training StructuralLM",
      "text" : "We adopt two self-supervised tasks during the pretraining stage, which are described as follows.\nMasked Visual-Language Modeling. We use the Masked Visual-Language Modeling (MVLM) (Xu et al., 2019) to make the model learn the cell representation with the clues of cell-level 2Dposition embeddings and text embeddings. We randomly mask some of the input tokens but keep the corresponding cell-level position embeddings, and\nthen the model is pre-trained to predict the masked tokens. With the cell-level layout information, StructuralLM can know which words surrounding the mask token are in the same cell and which are in adjacent cells. In this way, StructuralLM not only utilizes the corresponding cell-level position information but also understands the cell-level contextual representation. Therefore, compared with the MVLM in LayoutLM, StructuralLM makes use of the cell-level layout information and predicts the mask tokens more accurately. We will compare the performance of the MVLM with the cell-level layout embeddings and word-level layout embeddings respectively in Section 3.5.\nCell Position Classification. In addition to the MVLM, we propose a new Cell Position Classification (CPC) task to model the relative spatial position of cells in a document. The previous models represent the layout information at the bottom of the transformer, but the layout information at the top of the transformer may be weakened. Therefore, we consider introducing the cell position classification task so that StructuralLM can model the cell-level layout information from the bottom up. Given a set of scanned documents, this task aims to predict where the cells are in the documents. First, we split them into N areas of the same size. Then we calculate the area to which the cell belongs to through the center 2D-position of the cell. Meanwhile, some cells are randomly selected, and the 2D-positions of tokens in the selected cells are replaced with (0; 0; 0; 0). In this way, StructuralLM is capable of learning the interactions between cells and layout. During the pre-training, a classification layer is built above the encoder outputs. This layer predicts a label [1, N ] of the area where the selected cell is located, and computes the cross-entropy loss. Considering the MVLM and CPC are performed simultaneously, the cells with masked tokens will not be selected for the CPC task. This prevents the model from not utilizing cell-level layout information when doing the MVLM task. We will compare the performance of different N in Section 3.1.\nPre-training. StructuralLM is pre-trained with the two pre-training tasks and we add the two task losses with equal weights. We will compare the performance of MVLM and MVLM+CPC in Section 3.5."
    }, {
      "heading" : "2.4 Fine-tuning",
      "text" : "The pre-trained StructuralLM model is fine-tuned on three document image understanding tasks, each of which contains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Pre-training Configuration",
      "text" : "Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Therefore, we need to reprocess the scanned document images to obtain the layout information of cells. Like the pre-processing method of LayoutLM, we similarly process the dataset by using Tesseract 1, which is an opensource OCR engine. We normalize the actual coordinates to integers in the range from 0 to 1,000, and an empty bounding box (0; 0; 0; 0) is attached to special tokens [CLS], [SEP] and [PAD], which is similar to (Devlin et al., 2019).\nImplementation Details. StructuralLM is based on the Transformer which consists of a 24- layer encoder with 1024 embedding/hidden size, 4096 feed-forward filter size, and 16 attention heads. To take advantage of existing pre-trained models and adapt to document image understanding tasks, we initialize the weight of StructuralLM model with the pre-trained RoBERTa (Liu et al., 2019) large model except for the 2D-position embedding layers.\n1https://github.com/tesseract-ocr/tesseract\nFollowing Devlin et al. (2019), for the masked visual-language model task, we select 15% of the input tokens for prediction. We replace these masked tokens with the mask token 80% of the time, a random token 10% of the time, and an unchanged token 10% of the time. Then, the model predicts the corresponding token with the crossentropy loss. For the Bounding-box position classification task, we split the document image into N areas of the same size, and then select 15% of the cells for prediction. We replace the 2D-positions of words in the masked cells with the (0; 0; 0; 0) 90% of the time, and an unchanged position 10% of the time.\nStructuralLM is pre-trained on 16 NVIDIA Tesla V100 32GB GPUs for 480K steps, with each mini-batch containing 128 sequences of maximum length 512 tokens. The Adam optimizer is used with an initial learning rate of 1e-5 and a linear decay learning rate schedule. For the downstream tasks, we use a single Tesla V100 16GB GPU.\nHyperparameter N. For the cell position classification task, we test the performances of StructuralLM using different hyperparameter N during pre-training. Considering that the complete pretraining takes too long, we pre-train StructuralLM for 100k steps with a single GPU card to compare the performance of different N . As shown in Figure 3, when the N is set as 16, StructuralLM obtains the highest F1-score on the FUNSD dataset. Therefore, we set N as 16 during the pre-training."
    }, {
      "heading" : "3.2 Fine-tuning on Form Understanding",
      "text" : "We experiment with fine-tuning StructuralLM on several downstream document image understanding tasks, especially form understanding tasks. The FUNSD (Jaume et al., 2019) is a dataset for form understanding. It includes 199 real, fully annotated, scanned forms with 9,707 semantic entities and 31,485 words. The 199 scanned forms are\nsplit into 149 for training and 50 for testing. The FUNSD dataset is suitable for a variety of tasks, where we just fine-tuning StructuralLM on semantic entity labeling. Specifically, each word in the dataset is assigned to a semantic entity label from a set of four predefined categories: question, answer, header, or other. Following the previous works, we also use the word-level F1 score as the evaluation metric.\nWe fine-tune the pre-trained StructuralLM on the FUNSD training set for 25 epochs. We set the batch size to 4, the learning rate to 1e-5. The other hyperparameters are kept the same as pre-training.\nTable 1 presents the experimental results on the FUNSD test set. StructuralLM achieves better performance than all pre-training models. First, we compare the StructuralLM model with two SOTA text-only pre-trained models: BERT and RoBERTa (Liu et al., 2019). RoBERTa outperforms the BERT model by a large margin in terms of the BASE and LARGE settings. Compared with the text-only models, the text+layout model LayoutLM brings significant performance improvement. The best performance is achieved by StructuralLM, where an improvement of 6% F1 point compared with\nLayoutLM under the same model size. All the LayoutLM models compared in this paper are initialized by RoBERTa. By consistently outperforming the pre-training methods, StructuralLM confirms its effectiveness in leveraging cell-level layout information for form understanding."
    }, {
      "heading" : "3.3 Fine-tuning on Document Visual QA",
      "text" : "DocVQA (Mathew et al., 2020) is a VQA dataset on the scanned document understanding field. The objective of this task is to answer questions asked on a document image. The images provided are sourced from the documents hosted at the Industry Documents Library, maintained by the UCSF. It consists of 12,000 pages from a variety of documents including forms, tables, etc. These pages are manually labeled with 50,000 question-answer pairs, which are split into the training set, validation set and test set with a ratio of about 8:1:1. The dataset is organized as a set of triples (page image, questions, answers). The official provides the OCR results of the page images, and there is no objection to using other OCR recognition tools. Our experiment is based on the official OCR results. The task is evaluated using an edit distance based metric ANLS (aka average normalized Levenshtein similarity). Results on the test set are provided by the official evaluation site.\nWe fine-tune the pre-trained StructuralLM on the DocVQA train set and validation set for 5 epochs. We set the batch size to 8, the learning rate to 1e-5.\nTable 2 shows the Average Normalized Levenshtein Similarity (ANLS) scores on the DocVQA test set. We still compare the StructuralLM model with the text-only models and the text-layout model. Compared with LayoutLM, StructuralLM achieved an improvement of over 11% ANLS point under the same model size. In addition, we also compare\nthe Form&Table subset from the test set. StructuralLM achieved an improvement of over 14% ANLS point, which shows that StructuralLM can learn better on form and table understanding."
    }, {
      "heading" : "3.4 Fine-tuning on Document Classification",
      "text" : "Finally, we evaluate the document image classification task using the RVL-CDIP dataset (Harley et al., 2015). It consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. There are 320,000 images for the training set, 40,000 images for the validation set, and 40,000 images for the test set. A multi-class single-label classification task is defined on RVL-CDIP, including letter, form, invoice, etc. The evaluation metric is the overall classification accuracy. Text and layout information is extracted by Tesseract OCR.\nWe fine-tune the pre-trained StructuralLM on the RVL-CDIP train set for 20 epochs. We set the batch size to 8, the learning rate to 1e-5.\nDifferent from other natural images, the document images are texts in a variety of layouts. As shown in Table 3, image-based classification models (Afzal et al., 2017; Das et al., 2018; Szegedy et al., 2017) with pre-training perform much better than the text-based models, which illustrates that text information is not sufficient for this task and it still needs layout information. The experiment results show that the text-layout model LayoutLM outperforms the image-based approaches and textbased models. Incorporating the cell-level layout\ninformation, StructuralLM achieves a new state-ofthe-art result with an improvement of over 1.5% accuracy point."
    }, {
      "heading" : "3.5 Ablation Study",
      "text" : "We conduct ablation studies to assess the individual contribution of every component in StructuralLM. Table 4 reports the results of full StructuralLM and its ablations on the test set of FUNSD form understanding task. First, we evaluate how much the cell-level layout embedding contributes to form understanding by removing it from StructuralLM pre-training.\nThis ablation results in a drop from 0.8514 to 0.8024 on F1 score, demonstrating the important role of the cell-level layout embedding. To study the effect of the cell position classification task in StructuralLM, we ablate it and the F1 score significantly drops from 0.8514 to 0.8125. Finally, we study the significance of full StructuralLM pretraining. Over 15% of performance degradation resulted from ablating pre-training clearly demonstrates the power of StructuralLM in leveraging an unlabeled corpus for downstream form understanding tasks.\nActually, after ablating the cell position clas-\nsification, the biggest difference between StructuralLM and LayoutLM is cell-level 2D-position embeddings or word-level 2D-position embeddings. The results show that StructuralLM with cell-level 2D-position embeddings performs better than LayoutLM with word-level position embeddings with an improvement of over 2% F1-score point (from 0.7895 to 0.8125). Furthermore, we compare the performance of the MVLM with cell-level layout embeddings and word-level layout embeddings respectively. As shown in Figure 4, the results show that under the same pre-training settings, the MVLM training loss with cell-level 2D-position embeddings can converge lower."
    }, {
      "heading" : "3.6 Case Study",
      "text" : "The motivation behind StructuralLM is to jointly exploit cell and layout information across scanned document images. As stated above, compared with LayoutLM, StructuralLM improves interactions between cells and layout information. To verify this, we show some examples of the output of LayoutLM and StructuralLM on the FUNSD test set, as shown in Figure 5. Take the image on the top-left of Figure 5 as an example. In this example, the model needs to label ”Call Connie Drath or Carol Musgrave at 800/424-9876” with the Answer entity. The result of LayoutLM missed ”at 800/424-9876”. Actually, all the tokens of this Answer entity are from the same cell. Therefore, StructuralLM predicts the correct result with the understanding of celllevel layout information. These examples show that StructuralLM predicts the entities more accurately with the cell-level layout information. The same results can be observed in the Figure 5."
    }, {
      "heading" : "4 Related Work",
      "text" : ""
    }, {
      "heading" : "4.1 Machine Learning Approaches",
      "text" : "Statistical machine learning approaches (Marinai et al., 2005; Shilman et al., 2005) became the mainstream for document segmentation tasks during the past decade. (Shilman et al., 2005) consider the layout information of a document as a parsing problem. They use a grammar-based loss function to globally search the optimal parsing tree, and utilize a machine learning approach to select features and train all parameters during the parsing process. In addition, most efforts have been devoted to the recognition of isolated handwritten and printed characters with widely recognized successful results. For machine learning approaches (Shilman\net al., 2005; Wei et al., 2013), they are usually time-consuming to design manually features and difficult to obtain a high-level abstract semantic context. In addition, these methods usually relied on visual cues but ignored textual information."
    }, {
      "heading" : "4.2 Deep Learning Approaches",
      "text" : "Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019). (Yang et al., 2017) propose a pixel-by-pixel classification to solve the document semantic structure extraction problem. Specifically, they propose a multimodal neural network that considers visual and textual information, while this work is an end-toend approach. (Katti et al., 2018) first propose a fully convolutional encoder-decoder network to predict a segmentation mask and bounding boxes. In this way, the model significantly outperforms approaches based on sequential text or document images. In addition, (Soto and Yoo, 2019) incorporate contextual information into the Faster R-CNN model. They involve the inherently localized nature of article contents to improve region detection performance."
    }, {
      "heading" : "4.3 Pre-training Approaches",
      "text" : "In recent years, self-supervised pre-training has achieved great success in natural language understanding (NLU) and a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019). (Devlin et al., 2019) introduced BERT, a new language representation model, which is designed to pre-train deep bidirectional representations based on the large-scale unsupervised corpus. It can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. Inspired by the development of the pre-trained language models in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) do have pushed the limits of a variety of document image understanding tasks, which learn the interaction between text and layout information across scanned document images. (Xu et al., 2019) propose LayoutLM, which is a simple but effective pre-training method of text and layout for the document image understanding tasks. By incorporating the visual information into the fine-tuning stage, LayoutLM achieves new state-of-the-art results in several downstream tasks. (Hong et al., 2021) propose a pre-trained language model that represents the semantics of spatially distributed texts. Different from previous pre-training methods on 1D text, BROS is pre-trained on large-scale semi-\nstructured documents with a novel area-masking strategy while efficiently including the spatial layout information of input documents."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose StructuralLM, a novel structural pre-training approach on large unlabeled documents. It is built upon an extension of the Transformer encoder, and jointly exploit cell and layout information from scanned documents.\nDifferent from previous pre-trained models, StructuralLM uses cell-level 2D-position embeddings with tokens in the cell sharing the same 2Dposition. This makes StructuralLM aware of which words are from the same cell, and thus enables the model to derive representation for the cells. We propose a new pre-training objective called cell position classification. In this way, StructuralLM is capable of learning the interactions between cells and layout. We conduct experiments on three benchmark datasets publicly available, and StructuralLM outperforms strong baselines and achieves new state-of-the-art results in the downstream tasks."
    } ],
    "references" : [ {
      "title" : "Cutting the error by half: Investigation of very deep cnn and advanced training strategies for document image classification",
      "author" : [ "Muhammad Zeshan Afzal", "Andreas Kölsch", "Sheraz Ahmed", "Marcus Liwicki." ],
      "venue" : "2017 14th IAPR International Con-",
      "citeRegEx" : "Afzal et al\\.,? 2017",
      "shortCiteRegEx" : "Afzal et al\\.",
      "year" : 2017
    }, {
      "title" : "Fast cnnbased document layout analysis",
      "author" : [ "D.A. Borges Oliveira", "M.P. Viana." ],
      "venue" : "2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pages 1173–1180.",
      "citeRegEx" : "Oliveira and Viana.,? 2017",
      "shortCiteRegEx" : "Oliveira and Viana.",
      "year" : 2017
    }, {
      "title" : "Document image classification with intra-domain transfer learning and stacked generalization of deep convolutional neural networks",
      "author" : [ "Arindam Das", "Saikat Roy", "Ujjwal Bhattacharya", "Swapan K Parui." ],
      "venue" : "2018 24th International Conference",
      "citeRegEx" : "Das et al\\.,? 2018",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2018
    }, {
      "title" : "Modular multimodal architecture for document classification",
      "author" : [ "Tyler Dauphinee", "Nikunj Patel", "Mohammad Rashidi." ],
      "venue" : "arXiv preprint arXiv:1912.04376.",
      "citeRegEx" : "Dauphinee et al\\.,? 2019",
      "shortCiteRegEx" : "Dauphinee et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluation of deep convolutional nets for document image classification and retrieval",
      "author" : [ "Adam W Harley", "Alex Ufkes", "Konstantinos G Derpanis." ],
      "venue" : "2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 991–995.",
      "citeRegEx" : "Harley et al\\.,? 2015",
      "shortCiteRegEx" : "Harley et al\\.",
      "year" : 2015
    }, {
      "title" : "BROS}: A pre-trained language model for understanding texts",
      "author" : [ "Teakgyu Hong", "DongHyun Kim", "Mingi Ji", "Wonseok Hwang", "Daehyun Nam", "Sungrae Park" ],
      "venue" : null,
      "citeRegEx" : "Hong et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2021
    }, {
      "title" : "FUNSD: A dataset for form understanding in noisy scanned documents",
      "author" : [ "Guillaume Jaume", "Hazim Kemal Ekenel", "JeanPhilippe Thiran." ],
      "venue" : "CoRR, abs/1905.13538.",
      "citeRegEx" : "Jaume et al\\.,? 2019",
      "shortCiteRegEx" : "Jaume et al\\.",
      "year" : 2019
    }, {
      "title" : "Chargrid: Towards understanding 2d documents",
      "author" : [ "Anoop Raveendra Katti", "Christian Reisswig", "Cordula Guder", "Sebastian Brarda", "Steffen Bickel", "Johannes Höhne", "Jean Baptiste Faddoul." ],
      "venue" : "arXiv preprint arXiv:1809.08799.",
      "citeRegEx" : "Katti et al\\.,? 2018",
      "shortCiteRegEx" : "Katti et al\\.",
      "year" : 2018
    }, {
      "title" : "Building a test collection for complex document information processing",
      "author" : [ "D. Lewis", "G. Agam", "S. Argamon", "O. Frieder", "D. Grossman", "J. Heard." ],
      "venue" : "SIGIR ’06, page 665–666, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Lewis et al\\.,? 2006",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2006
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Artificial neural networks for document analysis and recognition",
      "author" : [ "S. Marinai", "M. Gori", "G. Soda." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(1):23–35.",
      "citeRegEx" : "Marinai et al\\.,? 2005",
      "shortCiteRegEx" : "Marinai et al\\.",
      "year" : 2005
    }, {
      "title" : "Docvqa: A dataset for vqa on document images",
      "author" : [ "M. Mathew", "Dimosthenis Karatzas", "R. Manmatha", "C. Jawahar." ],
      "venue" : "ArXiv, abs/2007.00398.",
      "citeRegEx" : "Mathew et al\\.,? 2020",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Deterministic routing between layout abstractions for multi-scale classification of visually rich documents",
      "author" : [ "Ritesh Sarkhel", "Arnab Nandi." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-",
      "citeRegEx" : "Sarkhel and Nandi.,? 2019",
      "shortCiteRegEx" : "Sarkhel and Nandi.",
      "year" : 2019
    }, {
      "title" : "Learning nongenerative grammatical models for document analysis",
      "author" : [ "M. Shilman", "P. Liang", "P. Viola." ],
      "venue" : "Tenth IEEE International Conference",
      "citeRegEx" : "Shilman et al\\.,? 2005",
      "shortCiteRegEx" : "Shilman et al\\.",
      "year" : 2005
    }, {
      "title" : "Visual detection with context for document layout analysis",
      "author" : [ "Carlos Soto", "Shinjae Yoo." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Soto and Yoo.,? 2019",
      "shortCiteRegEx" : "Soto and Yoo.",
      "year" : 2019
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alexander Alemi." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Szegedy et al\\.,? 2017",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2017
    }, {
      "title" : "Structbert: Incorporating language structures into pretraining for deep language understanding",
      "author" : [ "Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Zuyi Bao", "Jiangnan Xia", "Liwei Peng", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:1908.04577.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multigranularity hierarchical attention fusion networks for reading comprehension and question answering",
      "author" : [ "Wei Wang", "Ming Yan", "Chen Wu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluation of svm, mlp and gmm classifiers for layout analysis of historical documents",
      "author" : [ "H. Wei", "M. Baechler", "F. Slimane", "R. Ingold." ],
      "venue" : "2013 12th International Conference on Document Analysis and Recognition, pages 1220–1224.",
      "citeRegEx" : "Wei et al\\.,? 2013",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2013
    }, {
      "title" : "Layoutlm: Pretraining of text and layout for document image understanding",
      "author" : [ "Yiheng Xu", "Minghao Li", "Lei Cui", "Shaohan Huang", "Furu Wei", "Ming Zhou." ],
      "venue" : "CoRR, abs/1912.13318.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to extract semantic structure from documents using multimodal fully convolutional neural networks",
      "author" : [ "Xiao Yang", "Ersin Yumer", "Paul Asente", "Mike Kraley", "Daniel Kifer", "C Lee Giles." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vi-",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al.",
      "startOffset" : 65,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al.",
      "startOffset" : 65,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al.",
      "startOffset" : 65,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : ", 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) have pushed the limits of a variety of document image understanding tasks, which learn the interaction between text and layout information across scanned document images.",
      "startOffset" : 75,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "For example, as shown in Figure 1 (a), which is from form understanding task (Jaume et al., 2019), determining that the ”LORILLARD”",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "Different from previous text-based pretrained models (Devlin et al., 2019; Wang et al., 2019) and LayoutLM (Xu et al.",
      "startOffset" : 53,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "Different from previous text-based pretrained models (Devlin et al., 2019; Wang et al., 2019) and LayoutLM (Xu et al.",
      "startOffset" : 53,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : ", 2019) and LayoutLM (Xu et al., 2019), StructuralLM uses cell-level 2D-position embeddings with tokens in a cell sharing the same 2D-position.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "inspired by LayoutLM (Xu et al., 2019), but different from it in three ways.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "To take advantage of existing pre-trained models and adapt to document image understanding tasks, we use the BERT (Devlin et al., 2019) architecture as the backbone.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "We use the Masked Visual-Language Modeling (MVLM) (Xu et al., 2019) to make the model learn the cell representation with the clues of cell-level 2Dposition embeddings and text embeddings.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "We normalize the actual coordinates to integers in the range from 0 to 1,000, and an empty bounding box (0; 0; 0; 0) is attached to special tokens [CLS], [SEP] and [PAD], which is similar to (Devlin et al., 2019).",
      "startOffset" : 191,
      "endOffset" : 212
    }, {
      "referenceID" : 10,
      "context" : "To take advantage of existing pre-trained models and adapt to document image understanding tasks, we initialize the weight of StructuralLM model with the pre-trained RoBERTa (Liu et al., 2019) large model except for the 2D-position embedding layers.",
      "startOffset" : 174,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "6313 Model Precision Recall F1 Parameters BERTBASE (Devlin et al., 2019) 0.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "The FUNSD (Jaume et al., 2019) is a dataset for form understanding.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "First, we compare the StructuralLM model with two SOTA text-only pre-trained models: BERT and RoBERTa (Liu et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "DocVQA (Mathew et al., 2020) is a VQA dataset on the scanned document understanding field.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : ", 2018); (Szegedy et al., 2017);d (Sarkhel and Nandi, 2019); (Dauphinee et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : ", 2017);d (Sarkhel and Nandi, 2019); (Dauphinee et al., 2019)",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Finally, we evaluate the document image classification task using the RVL-CDIP dataset (Harley et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "As shown in Table 3, image-based classification models (Afzal et al., 2017; Das et al., 2018; Szegedy et al., 2017) with pre-training perform much better than the text-based models, which illustrates that text information is not sufficient for this task and it still needs layout information.",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "As shown in Table 3, image-based classification models (Afzal et al., 2017; Das et al., 2018; Szegedy et al., 2017) with pre-training perform much better than the text-based models, which illustrates that text information is not sufficient for this task and it still needs layout information.",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "As shown in Table 3, image-based classification models (Afzal et al., 2017; Das et al., 2018; Szegedy et al., 2017) with pre-training perform much better than the text-based models, which illustrates that text information is not sufficient for this task and it still needs layout information.",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "(Shilman et al., 2005) consider the layout information of a document as a parsing problem.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 22,
      "context" : "Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019).",
      "startOffset" : 94,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019).",
      "startOffset" : 94,
      "endOffset" : 186
    }, {
      "referenceID" : 16,
      "context" : "Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019).",
      "startOffset" : 94,
      "endOffset" : 186
    }, {
      "referenceID" : 22,
      "context" : "(Yang et al., 2017) propose a pixel-by-pixel classification to solve the document semantic structure extraction problem.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "(Katti et al., 2018) first propose a fully convolutional encoder-decoder network to predict a segmentation mask and bounding boxes.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "In addition, (Soto and Yoo, 2019) incorporate contextual information into the Faster R-CNN model.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "In recent years, self-supervised pre-training has achieved great success in natural language understanding (NLU) and a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "In recent years, self-supervised pre-training has achieved great success in natural language understanding (NLU) and a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 201
    }, {
      "referenceID" : 18,
      "context" : "In recent years, self-supervised pre-training has achieved great success in natural language understanding (NLU) and a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "(Devlin et al., 2019) introduced BERT, a new language representation model, which is de-",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "Inspired by the development of the pre-trained language models in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) do have pushed the limits of a variety of document image understanding tasks, which learn the inter-",
      "startOffset" : 130,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "(Xu et al., 2019) propose LayoutLM, which is a simple but effective pre-training method of text and layout for the document image understanding tasks.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "(Hong et al., 2021) propose a pre-trained language model that represents the semantics of spatially distributed texts.",
      "startOffset" : 0,
      "endOffset" : 19
    } ],
    "year" : 2021,
    "abstractText" : "Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, they almost exclusively focus on text-only representation, while neglecting cell-level layout information that is important for form image understanding. In this paper, we propose a new pre-training approach, StructuralLM, to jointly leverage cell and layout information from scanned documents. Specifically, we pre-train StructuralLM with two new designs to make the most of the interactions of cell and layout information: 1) each cell as a semantic unit; 2) classification of cell positions. The pre-trained StructuralLM achieves new state-of-the-art results in different types of downstream tasks, including form understanding (from 78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and document image classification (from 94.43 to 96.08).",
    "creator" : "LaTeX with hyperref"
  }
}