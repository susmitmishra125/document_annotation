{
  "name" : "2021.acl-long.571.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Vocabulary Learning via Optimal Transport for Neural Machine Translation",
    "authors" : [ "Jingjing Xu", "Hao Zhou", "Chun Gan", "Zaixiang Zheng", "Lei Li" ],
    "emails" : [ "xujingjing.melody@bytedance.com", "zhouhao.nlp@bytedance.com", "lileilab@bytedance.com", "cgan5@wisc.edu", "zhengzx@smail.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7361–7373\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7361"
    }, {
      "heading" : "1 Introduction",
      "text" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-jussà and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, †This work is done during the internship at ByteDance AI Lab.\n2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016).\nHowever, the effects of vocabulary size are not sufficiently taken into account since current approaches only consider frequency (or entropy) as the main criteria. Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks. Due to the lack of appropriate inductive bias about size, trial training (namely traversing all possible sizes) is usually required to search for the optimal size, which takes high computation costs. For convenience, most existing studies only adopt the widely-used settings in implementation. For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding et al., 2019).\nIn this paper, we propose to explore automatic vocabularization by simultaneously considering entropy and vocabulary size without expensive trial training. Designing such a vocabularization approach is non-trivial for two main reasons. First, it is challenging to find an appropriate objective function to optimize them at the same time. Roughly speaking, the corpus entropy decreases with the increase of vocabulary size, which benefits model learning (Martin and England, 2011). On the other side, too many tokens cause token sparsity, which hurts model learning (Allison et al., 2006). Second, supposing that an appropriate measurement is given, it is still challenging to\nsolve such a discrete optimization problem due to the exponential search space.\nTo address the above problems, we propose a VOcabulary Learning approach via optimal Transport, VOLT for short. It can give an appropriate vocabulary in polynomial time by considering corpus entropy and vocabulary size. Specifically, given the above insight of contradiction between entropy and size, we first borrow the concept of Marginal Utility in economics (Samuelson, 1937) and propose to use Marginal Utility of Vocabularization (MUV) as the measurement. The insight is quite simple: in economics, marginal utility is used to balance the benefit and the cost and we use MUV to balance the entropy (benefit) and vocabulary size (cost). Higher MUV is expected for Pareto optimality. Formally, MUV is defined as the negative derivative of entropy to vocabulary size. Figure 1 gives an example about marginal utility. Preliminary results verify that MUV correlates with the downstream performances on two-thirds of tasks (See Figure 2).\nThen our goal turns to maximize MUV in tractable time complexity. We reformulate our discrete optimization objective into an optimal transport problem (Cuturi, 2013) that can be solved in polynomial time by linear programming. Intuitively, the vocabularization process can be regarded as finding the optimal transport matrix from the character distribution to the vocabulary token distribution. Finally, our proposed VOLT will yield a vocabulary from the optimal transport matrix.\nWe evaluate our approach on multiple machine\ntranslation tasks, including WMT-14 EnglishGerman translation, TED bilingual translation, and TED multilingual translation. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios. Furthermore, VOLT is a lightweight solution and does not require expensive computation resources. On English-German translation, VOLT only takes 30 GPU hours to find vocabularies, while the traditional BPE-Search solution takes 384 GPU hours."
    }, {
      "heading" : "2 Related Work",
      "text" : "Initially, most neural models were built upon word-level vocabularies (Costa-jussà and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019). While achieving promising results, it is a common constraint that word-level vocabularies fail on handling rare words under limited vocabulary sizes.\nResearchers recently have proposed several advanced vocabularization approaches, like bytelevel approaches (Wang et al., 2020), characterlevel approaches (Costa-jussà and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al., 2016; Kudo and Richardson, 2018). Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies. The general idea is to merge pairs of frequent character sequences to create sub-word units. Sub-word vocabularies can be regarded as a trade-off between character-level vocabularies and word-level vocabularies. Compared to word-level vocabularies, it can decrease the sparsity of tokens and increase the shared features between similar words, which probably have similar semantic meanings, like “happy” and “happier”. Compared to character-level vocabularies, it has shorter sentence lengths without rare words. Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on.\nDespite promising results, most existing subword approaches only consider frequency while the effects of vocabulary size is neglected. Thus, trial training is required to find the optimal size, which brings high computation costs. More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020)."
    }, {
      "heading" : "3 Marginal Utility of Vocabularization",
      "text" : "In this section, we propose to find a good vocabulary measurement by considering entropy and size. As introduced in Section 1, it is non-trivial to find an appropriate objective function to optimize them simultaneously. On one side, with the increase of vocabulary size, the corpus entropy is decreased, which benefits model learning (Bentz and Alikaniotis, 2016). On the other side, a large vocabulary causes parameter explosion and token sparsity problems, which hurts model learning (Allison et al., 2006).\nTo address this problem, we borrow the concept of Marginal Utility in economics (Samuelson, 1937) and propose to use Marginal Utility of Vocabularization (MUV) as the optimization objective. MUV evaluates the benefits (entropy) a corpus can get from an increase of cost (size). Higher MUV is expected for higher benefit-cost ratio. Preliminary results verify that MUV correlates with downstream performances on two-thirds of translation tasks (See Figure 2). According to this feature, our goal turns to maximize MUV in tractable time complexity.\nDefinition of MUV Formally, MUV represents the negative derivation of entropy to size. For simplification, we leverage a smaller vocabulary to estimate MUV in implementation. Specially, MUV is calculated as:\nMv(k+m) = −(Hv(k+m) −Hv(k))\nm , (1)\nwhere v(k), v(k + m) are two vocabularies with k and k + m tokens, respectively. Hv represents the corpus entropy with the vocabulary v, which is\ndefined by the sum of token entropy. To avoid the effects of token length, here we normalize entropy with the average length of tokens and the final entropy is defined as:\nHv = − 1\nlv ∑ i∈v P (i) logP (i), (2)\nwhere P (i) is the relative frequency of token i from the training corpus and lv is the average length of tokens in vocabulary v.\nPreliminary Results To verify the effectiveness of MUV as the vocabulary measurement, we conduct experiments on 45 language pairs from TED and calculate the Spearman correlation score∗ between MUV and BLEU scores. We adopt the same and widely-used settings to avoid the effects of other attributes on BLEU scores, such as model hyper-parameters and training hyper-parameters. We generate a sequence of vocabularies with incremental sizes via BPE. All experiments use the same hyper-parameters. Two-thirds of pairs show positive correlations as shown in Figure 2. The middle Spearman score is 0.4. We believe that it is a good signal to show MUV matters. Please refer to Section 5 for more dataset details and Appendix A for more implementation details.\nGiven MUV, we have two natural choices to get the final vocabulary: search and learning. In the search-based direction, we can combine MUV with widely-used vocabularization solutions. For example, the optimal vocabularies can be obtained by enumerating all candidate vocabularies generated by BPE. While being simple and effective, it is not a self-sufficient approach. Furthermore, it still requires a lot of time to generate vocabularies and calculate MUV. To address these problems, we further explore a learning-based solution VOLT for more vocabulary possibilities. We empirically compare MUV-Search and VOLT in Section 5."
    }, {
      "heading" : "4 Maximizing MUV via Optimal Transport",
      "text" : "This section describes the details of the proposed approach. We first show the general idea of VOLT in Section 4.1, then describe the optimal transport solution in Section 4.2, followed by the implementation details in Section 4.3. ∗https://www.statstutor.ac.uk/ resources/uploaded/spearmans.pdf"
    }, {
      "heading" : "4.1 Overview",
      "text" : "We formulate vocabulary construction as a discrete optimization problem whose target is to find the vocabulary with the highest MUV according to Eq. 1. However, the vocabulary is discrete and such discrete search space is too large to traverse, which makes the discrete optimization intractable.\nIn this paper, we simplify the original discrete optimization problem by searching for the optimal vocabulary from vocabularies with fixed sizes. Intuitively, MUV is the first derivative of entropy according to the vocabulary size (Eq. 1), and we introduce an auxiliary variable S (S is an incremental integer sequence) to approximate the computation by only computing MUV between vocabulary sizes as adjacent integers in S.\nFormally, S = {i, 2 · i, ..., (t−1) · i, · · · } where each timestep t represents a set of vocabularies with the number up to S[t]. For any vocabulary, its MUV score can be calculated based on a vocabulary from its previous timestep. With sequence S, the target to find the optimal vocabulary v(t) with the highest MUV can be formulated as:\nargmax v(t−1)∈VS[t−1],v(t)∈VS[t]\nMv(t) =\nargmax v(t−1)∈VS[t−1],v(t)∈VS[t]\n−1 i\n[ Hv(t) −Hv(t−1) ] ,\nwhere VS[t−1] and VS[t] are two sets containing all vocabularies with upper bound of size S[t − 1] and S[t]. Due to exponential search space, we propose to optimize its lower bound:\nargmax t\n1 i\n[ max\nv(t)∈VS[t] Hv(t) − max v(t−1)∈VS[t−1] Hv(t−1)\n] .\n(3)\nwhere i means the size difference between t − 1 vocabulary and t vocabulary. MUV requires the size difference as a denominator. Based on this equation, the whole solution is split into two steps: 1) searching for the optimal vocabulary with the highest entropy at each timestep t; 2) enumerating all timesteps and outputing the vocabulary corresponding to the time step satisfying Eq. 3.\nThe first step of our approach is to search for the vocabulary with the highest entropy from VS[t]. Formally, the goal is to find a vocabulary v(t) such that entropy is maximized,\nargmax v(t)∈VS[t] − 1 lv(t) ∑ i∈v(t) P (i) logP (i), (4)\nwhere lv is the average length for tokens in v(t), P (i) is the probability of token i. However, notice that this problem is in general intractable due to the extensive vocabulary size. Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm (Cuturi, 2013).\nIntuitively, we can imagine vocabulary construction as a transport process that transports chars into token candidates with the number up to S[t]. As shown in Figure 3, the number of chars is fixed, and not all token candidates can get enough chars. Each transport matrix can build a vocabulary by collecting tokens with chars. Different transport matrices bring different transport costs. The target of optimal transport is to find a transport matrix to minimize the transfer cost, i.e., negative entropy in our setting."
    }, {
      "heading" : "4.2 Vocabularization via Optimal Transport",
      "text" : "Given a set of vocabularies VS[t], we want to find the vocabulary with the highest entropy. Consequently, the objective function in Eq. 4 becomes\nmin v∈VS[t]\n1 lv ∑ i∈v P (i) logP (i),\ns.t. P (i) = Token(i)∑ i∈v Token(i) , lv =\n∑ i∈v len(i)\n|v| .\nToken(i) is the frequency of token i in the vocabulary v. len(i) represents the length of token i. Notice that both the distribution P (i) and the average length lv depend on the choice of v.\nObjective Approximation To obtain a tractable lower bound of entropy, it suffices to give a tractable upper bound of the above objective function. We adopt the merging rules to segment raw text similar with BPE where two consecutive tokens will be merged into one if the merged one is in the vocabulary. To this end, let T ∈ VS[t] be\nthe vocabulary containing top S[t] most frequent tokens, C be the set of chars and |T|, |C| be their sizes respectively. Since T is an element of VS[t], clearly, we have\nmin v∈VS[t]\n1 lv ∑ i∈v P (i) logP (i) ≤ 1 lT ∑ i∈T P (i) logP (i). (5)\nHere we start from the upper bound of the above objective function, that is 1lT ∑ i∈T P (i) logP (i) and then search for a refined token set from T. In this way, we reduce the search space into the subsets of T. Let P (i, j) be the joint probability distribution of the tokens and chars that we want to learn. Then we have∑\ni∈T P (i) logP (i) = ∑ i∈T ∑ j∈C P (i, j) logP (i)\n= ∑ i∈T ∑ j∈C P (i, j) logP (i, j)\n︸ ︷︷ ︸ L1\n+ ∑ i∈T ∑ j∈C P (i, j)(− logP (j|i))\n︸ ︷︷ ︸ L2\n.\n(6)\nThe details of proof can be found at Appendix C. Since L1 is nothing but the negative entropy of the joint probability distribution P (i, j), we shall denote it as −H(P ).\nLet D be the |C| × |T| matrix whose (i, j)-th entry is given by − logP (j|i), and let P be the joint probability matrix, then we can write\nL2 = 〈P ,D〉 = ∑ i ∑ j P (i, j)D(i, j). (7)\nIn this way, Eq. 6 can be reformulated as the following objective function which has the same form as the objective function in optimal transport:\nmin P∈Rm×n\n〈P ,D〉 − γH(P ). (8)\nSetup of OT From the perspective of optimal transport, P can be regarded as the transport matrix, and D can be regarded as the distance matrix. Intuitively, optimal transport is about finding the best transporting mass from the char distribution to the target token distribution with the minimum work defined by 〈P ,D〉.\nTo verify the validness of transport solutions, we add the following constraints. First, to avoid invalid transport between char j and token i, we set the distance to +∞ if the target token i does not contain the char j. Otherwise, we use 1len(i) to\nestimate P (j|i) where len(i) is the length of token i. Formally, the distance matrix is defined as\nD(i, j) = { − logP (j|i) = +∞, if j /∈ i − logP (j|i) = − log 1\nlen(i) , otherwise\nFurthermore, the number of chars is fixed and we set the sum of each row in the transport matrix to the probability of char j. The upper bound of the char requirements for each token is fixed and we set the sum of each column in the transport matrix to the probablity of token j. Formally, the constraints are defined as:\n| ∑ j P (i, j)− P (i)| ≤ , (9)\nand ∑ i P (i, j) = P (j). (10)\nGiven transport matrix P and distance matrix D, the final objective can be formulated as:\nargmin P∈R|C|×|T|\n−H(P ) + 〈P ,D〉 ,\ns.t. ∑ i P (i, j) = P (j), | ∑ j P (i, j)− P (i)| ≤ ,\nwith small > 0. Figure 4 shows the details of optimal transport solution. Strictly speaking, this is an unbalanced entropy regularized optimal transport problem. Nonetheless, we can still use the generalized Sinkhorn algorithm to efficiently find the target vocabulary as detailed in Section 4.6 of Peyré and Cuturi (2019). The algorithm details are shown in Algorithm 1. At each timestep t, we can generate a new vocabulary associated with entropy scores based on the transport matrix P . Finally, we collect these vocabularies associated with entropy scores, and output the vocabulary satisfying Eq. 3."
    }, {
      "heading" : "4.3 Implementation",
      "text" : "Algorithm 1 lists the process of VOLT. First, we rank all token candidates according to their frequencies. For simplification, we adopt BPEgenerated tokens (e.g. BPE-100K) as the token candidates. It is important to note that any segmentation algorithms can be used to initialize token candidates. Experiments show that different initialization approaches result in similar results. We simply adopt BPE-100K for bilingual translation and BPE-300K for multilingual translation in this work. All token candidates with their probabilities are then used to initialize L in Algorithm 1.\nThe size of the incremental integer sequence S is a hyper-parameter and set to (1K, ..., 10K) for bilingual translation, (40K, ..., 160K) for multilingual settings. At each timestep, we can get the vocabulary with the maximum entropy based on the transport matrix. It is inevitable to handle illegal transport case due to relaxed constraints. We remove tokens with distributed chars less than 0.001 token frequencies. Finally, we enumerate all timesteps and select the vocabulary satisfying Eq. 3 as the final vocabulary.\nAfter generating the vocabulary, VOLT uses a greedy strategy to encode text similar to BPE. To encode text, it first splits sentences into characterlevel tokens. Then, we merge two consecutive tokens into one token if the merged one is in the vocabulary. This process keeps running until no tokens can be merged. Out-of-vocabulary tokens will be split into smaller tokens."
    }, {
      "heading" : "5 Experiments",
      "text" : "To evaluate the performance of VOLT, we conduct experiments on three datasets, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation."
    }, {
      "heading" : "5.1 Settings",
      "text" : "We run experiments on the following machine translation datasets. See Appendix B for more model and training details.\n1. WMT-14 English-German (En-De) dataset: This dataset has 4.5M sentence pairs. The\nAlgorithm 1: VOLT Input: A sequence of token candidates L ranked by\nfrequencies, an incremental integer sequence S where the last item of S is less than |L|, a character sequence C, a training corpus Dc\nParameters: u ∈ R|C|+ , v ∈ R |T| + vocabularies = [] for item in S do\n// Begin of Sinkhorn algorithm Initialize u = ones() and v = ones() T = L[: item] Calculate token frequencies P (T) based on Dc Calculate char frequencies P (C) based on Dc Calculate D while not converge do\nu = P (T)/Dv v = P (C)/DT u\noptimal matrix = u.reshape(-1, 1) * D * v.reshape(1, -1) // End of Sinkhorn algorithm entropy, vocab = get vocab(optimal matrix) vocabularies.append(entropy,vocab)\nOutput v∗ from vocabularies satisfying Eq. 3\ndataset is processed following Ott et al. (2018). We choose newstest14 as the test set.\n2. TED bilingual dataset: We include two settings: X-to-English translation and Englishto-X translation. We choose 12 languagepairs with the most training data. We use the language code according to ISO-639-1 standard†. TED data is provided by Qi et al. (2018).\n3. TED multilingual dataset: We conduct experiments with 52 language pairs on a many-toEnglish setting. The network is trained on all language pairs. We adopt the same preprocessing pipeline in the WMT-14 En-De dataset."
    }, {
      "heading" : "5.2 Main Results",
      "text" : "Vocabularies Searched by VOLT are Better than Widely-used Vocabularies on Bilingual MT Settings. Ding et al. (2019) gather 42 papers that have been accepted by the research track of Conference of Machine Translation (WMT) through 2017 and 2018. Among these papers, the authors find that 30K-40K is the most popular range for the number of BPE merge actions. Following this work, we first compare our methods with dominant BPE-30K. The results are listed in Table 1. As we can see, the vocabularies searched by VOLT achieve higher BLEU scores with large †http://www.lingoes.net/en/translator/langcode.htm\nsize reduction. The promising results demonstrate that VOLT is a practical approach that can find a well-performing vocabulary with higher BLEU and smaller size.\nVocabularies Searched by VOLT are on Par with Heuristically-searched Vocabularies on Low-resource Datasets. Ding et al. (2019) study how the size of BPE affects the model performance in low-resource settings. They conduct experiments on four language pairs and find that smaller vocabularies are more suitable for lowresource datasets. For Transformer architectures, the optimal vocabulary size is less than 4K, around up to 2K merge actions. We compare VOLT and BPE-1K on an X-to-English bilingual setting. The results are shown in Table 2. We can see that VOLT can find a good vocabulary on par with heuristically searched vocabularies in terms of BLEU scores. Note that BPE-1K is selected based on plenty of experiments. In contrast, VOLT only requires one trials for evaluation and only takes 0.5 CPU hours plus 30 GPU hours to find the optimal vocabulary.\nVOLT Works Well on Multilingual MT Settings. We conduct a multilingual experiment. These languages come from multiple language\nfamilies and have diverse characters. We compare VOLT with BPE-60K, the most popular setting in multilingual translation tasks. Table 3 lists the full results. The size of the searched vocabulary is around 110K. As we can see, VOLT achieves better BLEU scores on most pairs.\nVOLT is a Green Vocabularization Solution. One advantage of VOLT lies in its low resource consumption. We compare VOLT with BPESearch, a method to select the best one from a BPE-generated vocabulary set based on their BLEU scores. The results are shown in Table 4. In BPE-Search, we first define a vocabulary set including BPE-1K, BPE-2K, BPE-3K, BPE-4K, BPE-5K, BPE-6K, BPE-7K, BPE-8K, BPE-9K, BPE-10K, BPE-20K, BPE-30K. Then, we run full experiments to select the best vocabulary. Table 4 demonstrates that VOLT is a lightweight solution that can find a competitive vocabulary within 0.5 hours on a single CPU, compared to BPE-Search that takes hundreds of GPU hours. The cost of BPE-Search is the sum of the training time on all vocabularies. Furthermore, we also compare VOLT with MUV-Search as introduced in Section 3. MUV-Search is a method that combines MUV and popular approaches by selecting the vocabulary with the highest MUV as the final vocabulary.\nWe generate a sequence of BPE vocabularies with incremental size 1K, 2K, 3K, 4K, 5K, 6K, 7K, 8K, 9K, 10K, 20K. For t-th vocabulary v(t), its MUV score is calculated according to v(t) and v(t− 1). We enumerate all vocabularies and select the vocabulary with the highest MUV as the final vocabulary. The comparison between VOLT and MUVSearch is shown in Table 4. Although MUVSearch does not require downstream full-training, it still takes a lot of time to generate vocabularies and calculate MUV. Among them, VOLT is the most efficient approach."
    }, {
      "heading" : "5.3 Discussion",
      "text" : "We conduct more experiments to answer the following questions: 1) can a baseline beat strong approaches with a better vocabulary; 2) can VOLT beat recent vocabulary solutions, like SentencePiece; 3) can VOLT work on diverse architectures?\nA Simple Baseline with a VOLT-generated Vocabulary Reaches SOTA Results. We compare VOLT and several strong approaches on the En-De\ndataset. Table 5 shows surprisingly good results. Compared to the approaches in the top block, VOLT achieves almost the best performance with a much smaller vocabulary. These results demonstrate that a simple baseline can achieve good results with a well-defined vocabulary.\nVOLT Beats SentencePiece and WordPiece. SentencePiece and WordPiece are two variants of sub-word vocabularies. We also compare our approach with them on WMT-14 En-De translation\nto evaluate the effectiveness of VOLT. The middle block of Table 5 lists the results of SentenPiece and WordPiece. We implement these two approaches with the default settings. We can observe that VOLT outperforms SentencePiece and WordPiece by a large margin, with over 1 BLEU improvements.\nVOLT Works on Various Architectures. This work mainly uses Transformer-big in experiments. We are curious about whether VOLT works on other architectures. We take WMT-14 En-De translation as an example and implement a Convolutional Seq2Seq model. The network uses the default settings from Fairseq‡. We set the maximum epochs to 100 and average the last five models as the final network for evaluation. Table 6 demonstrates that vocabularies searched by VOLT also works on Convolutional Seq2Seq with competitive BLEU but much smaller size. In this work, we verify the effectiveness of VOLT on architectures with standard sizes. Since model capacity is also an important factor on BLEU scores, we recommend larger vocabularies associated with more embedding parameters for small architectures.\nVOLT can Bring Slight Speedup During Training. We evaluate the running time for VOLT vocabulary and BPE-30K on WMT En-De translation. The model with VOLT-searched vocabulary (11.6k tokens) can process 133 sentences per second, while the model with BPE-30K (33.6k tokens) only executes 101 sentences per second. All experiments run on the same environment (2 Tesla-V100-GPUs + 1 Gold-6130-CPU), with the same beam size for decoding. The speedup mainly comes from larger batch size with reduced embedding parameters. We also find that although VOLT reduces the Softmax computations, it does not significantly boost the Softmax running time due to optimized parallel computation in GPUs.\nVOLT Vocabularies and BPE Vocabularies are Highly Overlapped. For simplification, VOLT starts from BPE-segmented tokens. We take WMT En-De as an example to see the difference between VOLT vocabulary and BPE vocabulary. The size of VOLT vocabulary is around 9K and we adopt BPE-9K vocabulary for comparison. We find that these two vocabularies are highly overlapped, especially for those high-frequency words. ‡https://github.com/pytorch/fairseq/ tree/master/examples/translation\nThey also have similar downstream performance. Therefore, from an empirical perspective, BPE with VOLT size is also a good choice."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose a new vocabulary search approach without trail training. The whole framework starts from an informtaion-therotic understanding. According to this understanding, we formulate vocabularization as a two-step discrete optimization objective and propose a principled optimal transport solution VOLT. Experiments show that VOLT can effectively find a well-performing vocabulary in diverse settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers, Demi Guo, for their helpful feedback. Lei Li and Hao Zhou are corresponding authors."
    }, {
      "heading" : "Appendix A: MUV",
      "text" : "To evaluate the relationship between MUV and BLEU scores, we conduct experiments on 45 language pairs (X-En) with most resources (including ar-en, eg-en, cs-en, da-en, de-en, el-en, es-en, eten, fa-en, fi-en, fr-ca-en, fr-en, gl-en, he-en, hi-en, hr-en, hu-en, hy-en, id-en, it-en, ja-en, ka-en, koen, ku-en, lt-en, mk-en, my-en, nb-en, nl-en, pl-en, pt-br-en, pt-en, ro-en, ru-en, sk-en, sl-en, sq-en, sren, sv-en, th-en, tr-en, uk-en, vi-en, zh-cn-en, zhtw-en) from TED and calculate the Spearman correlation score beween MUV and BLEU. We merge all bilingual training data together and pre-train a multilingual network. To avoid the effects of unsteady BLEU scores, we use the multilingual network to initialize bilingual networks. All bilingual datasets are segment by four multilingual vocabularies, including BPE-20K, BPE-60K, BPE-100K, BPE-140K. In this way, we can get four bilingual corpora for each translation task. The MUV is calculated based on these corpora. For each corpus, we leverage a corpus with a smaller vocabulary to calculate MUV. For example, the MUV score of Ar-En (BPE-20K) is calculated based on ArEn (BPE-20K) and Ar-En (BPE-10K). It is important to note that all corpora adopt the same interval, 10K, to calculate MUV. All bilingual datasets share the same model hyper-parameters and training hyper-parameters (Please refer to Appendix B for more implementation details). We set the maximum training epoch to 50 and average the last five models as the final network for evaluation."
    }, {
      "heading" : "Appendix B: Experiments",
      "text" : "Models. We use Fairseq to train a Transformerbig model with the same setting in the original paper (Ott et al., 2018). The input embedding and output embeddings are shared. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate 5e-4 and an inverse sqrt decay schedule. The warm-up step is 4, 000, the dropout rate is 0.3, the update frequency is 4, the number of tokens is 9, 600, or 4, 800 in a single batch.\nTraining and Evaluation. We run WMT-14 EnDe experiments with 8 GPUs, TED bilingual translation with 4 GPUs, TED multilingual translation with 16 GPUs. We set a beamwidth to 4 for En-De and 5 for the other. For bilingual translation, we run approaches 40 epochs, average the last five models on all datasets, and use\nthe averaged model to generate translation results. For multilingual translation, all approaches run 10 epochs and we adopt the last model for evaluation. We calculate case-sensitive tokenized BLEU for evaluation.\nAppendix C: Proofs for Eq. 6∑ i∈T P (i) logP (i) = ∑ i∈T ∑ j∈C P (i, j) logP (i)\n= ∑ i∈T ∑ j∈C P (i, j) logP (i, j) · P (i) P (i, j)\n= ∑ i∈T ∑ j∈C P (i, j) logP (i, j) + ∑ i∈T ∑ j∈C P (i, j) log P (i) P (i, j)\n= ∑ i∈T ∑ j∈C\nP (i, j) logP (i, j)︸ ︷︷ ︸ L1\n+ ∑ i∈T ∑ j∈C\nP (i, j)(− logP (j|i))︸ ︷︷ ︸ L2 ."
    } ],
    "references" : [ {
      "title" : "Character-level language modeling with deeper self-attention",
      "author" : [ "Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Ap-",
      "citeRegEx" : "Al.Rfou et al\\.,? 2019",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2019
    }, {
      "title" : "Another look at the data sparsity problem",
      "author" : [ "Ben Allison", "David Guthrie", "Louise Guthrie." ],
      "venue" : "Text, Speech and Dialogue, 9th International Conference, TSD 2006, Brno, Czech Republic, September 11-15, 2006, Proceedings, volume 4188 of Lecture Notes in",
      "citeRegEx" : "Allison et al\\.,? 2006",
      "shortCiteRegEx" : "Allison et al\\.",
      "year" : 2006
    }, {
      "title" : "The word entropy of natural languages",
      "author" : [ "Christian Bentz", "Dimitrios Alikaniotis." ],
      "venue" : "arXiv preprint arXiv:1606.06996.",
      "citeRegEx" : "Bentz and Alikaniotis.,? 2016",
      "shortCiteRegEx" : "Bentz and Alikaniotis.",
      "year" : 2016
    }, {
      "title" : "How large a vocabulary does text classification need? A variational approach to vocabulary selection",
      "author" : [ "Wenhu Chen", "Yu Su", "Yilin Shen", "Zhiyu Chen", "Xifeng Yan", "William Yang Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting character-based neural machine translation with capacity and compression",
      "author" : [ "Colin Cherry", "George F. Foster", "Ankur Bapna", "Orhan Firat", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 2018",
      "citeRegEx" : "Cherry et al\\.,? 2018",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "Marta R. Costa-jussà", "José A.R. Fonollosa." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short",
      "citeRegEx" : "Costa.jussà and Fonollosa.,? 2016",
      "shortCiteRegEx" : "Costa.jussà and Fonollosa.",
      "year" : 2016
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A call for prudent choice of subword merge operations in neural machine translation",
      "author" : [ "Shuoyang Ding", "Adithya Renduchintala", "Kevin Duh." ],
      "venue" : "Proceedings of Machine Translation Summit XVII Volume 1: Research Track, MTSummit 2019,",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "C Users Journal, 12(2):23–38.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning to segment inputs for NMT favors character-level processing",
      "author" : [ "Julia Kreutzer", "Artem Sokolov." ],
      "venue" : "CoRR, abs/1810.01480.",
      "citeRegEx" : "Kreutzer and Sokolov.,? 2018",
      "shortCiteRegEx" : "Kreutzer and Sokolov.",
      "year" : 2018
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:365–378.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Very deep transformers for neural machine translation",
      "author" : [ "Xiaodong Liu", "Kevin Duh", "Liyuan Liu", "Jianfeng Gao." ],
      "venue" : "CoRR, abs/2008.07772.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Mathematical theory of entropy",
      "author" : [ "Nathaniel FG Martin", "James W England." ],
      "venue" : "12. Cambridge university press.",
      "citeRegEx" : "Martin and England.,? 2011",
      "shortCiteRegEx" : "Martin and England.",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Scaling neural machine translation",
      "author" : [ "Myle Ott", "Sergey Edunov", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018,",
      "citeRegEx" : "Ott et al\\.,? 2018",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2018
    }, {
      "title" : "Computational optimal transport",
      "author" : [ "Gabriel Peyré", "Marco Cuturi." ],
      "venue" : "Found. Trends Mach. Learn., 11(5-6):355–607.",
      "citeRegEx" : "Peyré and Cuturi.,? 2019",
      "shortCiteRegEx" : "Peyré and Cuturi.",
      "year" : 2019
    }, {
      "title" : "Bpe-dropout: Simple and effective subword regularization",
      "author" : [ "Ivan Provilkov", "Dmitrii Emelianenko", "Elena Voita." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
      "citeRegEx" : "Provilkov et al\\.,? 2020",
      "shortCiteRegEx" : "Provilkov et al\\.",
      "year" : 2020
    }, {
      "title" : "When and why are pre-trained word embeddings useful for neural machine translation",
      "author" : [ "Ye Qi", "Devendra Singh Sachan", "Matthieu Felix", "Sarguna Padmanabhan", "Graham Neubig" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American",
      "citeRegEx" : "Qi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2018
    }, {
      "title" : "Optimizing segmentation granularity for neural machine translation",
      "author" : [ "Elizabeth Salesky", "Andrew Runge", "Alex Coda", "Jan Niehues", "Graham Neubig." ],
      "venue" : "Machine Translation, pages 1–19.",
      "citeRegEx" : "Salesky et al\\.,? 2020",
      "shortCiteRegEx" : "Salesky et al\\.",
      "year" : 2020
    }, {
      "title" : "A note on measurement of utility",
      "author" : [ "Paul A Samuelson." ],
      "venue" : "The review of economic studies, 4(2):155– 161.",
      "citeRegEx" : "Samuelson.,? 1937",
      "shortCiteRegEx" : "Samuelson.",
      "year" : 1937
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting lowresource neural machine translation: A case study",
      "author" : [ "Rico Sennrich", "Biao Zhang." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:",
      "citeRegEx" : "Sennrich and Zhang.,? 2019",
      "shortCiteRegEx" : "Sennrich and Zhang.",
      "year" : 2019
    }, {
      "title" : "Self-attention with relative position representations",
      "author" : [ "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Shaw et al\\.,? 2018",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2018
    }, {
      "title" : "The evolved transformer",
      "author" : [ "David R. So", "Quoc V. Le", "Chen Liang." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine",
      "citeRegEx" : "So et al\\.,? 2019",
      "shortCiteRegEx" : "So et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation with byte-level subwords",
      "author" : [ "Changhan Wang", "Kyunghyun Cho", "Jiatao Gu." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelli-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning for sentiment analysis: A survey",
      "author" : [ "Lei Zhang", "Shuai Wang", "Bing Liu." ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4).",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Recurrent neural network for text classification with hierarchical multiscale dense connections",
      "author" : [ "Yi Zhao", "Yanyan Shen", "Junjie Yao." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019).",
      "startOffset" : 219,
      "endOffset" : 327
    }, {
      "referenceID" : 28,
      "context" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019).",
      "startOffset" : 219,
      "endOffset" : 327
    }, {
      "referenceID" : 10,
      "context" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019).",
      "startOffset" : 219,
      "endOffset" : 327
    }, {
      "referenceID" : 30,
      "context" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019).",
      "startOffset" : 219,
      "endOffset" : 327
    }, {
      "referenceID" : 7,
      "context" : "Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019).",
      "startOffset" : 219,
      "endOffset" : 327
    }, {
      "referenceID" : 18,
      "context" : "Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al.",
      "startOffset" : 94,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al.",
      "startOffset" : 94,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al.",
      "startOffset" : 94,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016).",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016).",
      "startOffset" : 184,
      "endOffset" : 239
    }, {
      "referenceID" : 2,
      "context" : "In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016).",
      "startOffset" : 184,
      "endOffset" : 239
    }, {
      "referenceID" : 25,
      "context" : "Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks.",
      "startOffset" : 22,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks.",
      "startOffset" : 22,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks.",
      "startOffset" : 22,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks.",
      "startOffset" : 22,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Roughly speaking, the corpus entropy decreases with the increase of vocabulary size, which benefits model learning (Martin and England, 2011).",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "On the other side, too many tokens cause token sparsity, which hurts model learning (Allison et al., 2006).",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "Specifically, given the above insight of contradiction between entropy and size, we first borrow the concept of Marginal Utility in economics (Samuelson, 1937) and propose to use Marginal Utility of Vocabularization (MUV) as the measurement.",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "We reformulate our discrete optimization objective into an optimal transport problem (Cuturi, 2013) that can be solved in polynomial time by linear programming.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "Initially, most neural models were built upon word-level vocabularies (Costa-jussà and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "Initially, most neural models were built upon word-level vocabularies (Costa-jussà and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 31,
      "context" : "Initially, most neural models were built upon word-level vocabularies (Costa-jussà and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 144
    }, {
      "referenceID" : 29,
      "context" : "Researchers recently have proposed several advanced vocabularization approaches, like bytelevel approaches (Wang et al., 2020), characterlevel approaches (Costa-jussà and Fonollosa, 2016; Lee et al.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : ", 2020), characterlevel approaches (Costa-jussà and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : ", 2020), characterlevel approaches (Costa-jussà and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : ", 2020), characterlevel approaches (Costa-jussà and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 24,
      "context" : "Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : ", 2020), SentencePiece (Kudo and Richardson, 2018), and so on.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : "More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "On one side, with the increase of vocabulary size, the corpus entropy is decreased, which benefits model learning (Bentz and Alikaniotis, 2016).",
      "startOffset" : 114,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "On the other side, a large vocabulary causes parameter explosion and token sparsity problems, which hurts model learning (Allison et al., 2006).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "To address this problem, we borrow the concept of Marginal Utility in economics (Samuelson, 1937) and propose to use Marginal Utility of Vocabularization (MUV) as the optimization objective.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm (Cuturi, 2013).",
      "startOffset" : 157,
      "endOffset" : 171
    } ],
    "year" : 2021,
    "abstractText" : "The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether one can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of the role of vocabulary from the perspective of information theory. Motivated by this, we formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT outperforms widely-used vocabularies in diverse scenarios, including WMT-14 English-German and TED’s 52 translation directions. For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https: //github.com/Jingjing-NLP/VOLT.",
    "creator" : "LaTeX with hyperref"
  }
}