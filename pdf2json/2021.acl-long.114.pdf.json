{
  "name" : "2021.acl-long.114.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models",
    "authors" : [ "Peter West", "Ximing Lu", "Ari Holtzman", "Chandra Bhagavatula", "Jena Hwang", "Yejin Choi" ],
    "emails" : [ "yejin}@cs.washington.edu", "jenah}@allenai.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1435–1450\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1435\nIn this paper, we present REFLECTIVE DECODING, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks. Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward. First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing). Second, in the reflection step, we condition on these “context ensembles”, generating outputs that are compatible with them. Comprehensive empirical results1 demonstrate that REFLECTIVE DECODING outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods. REFLECTIVE DECODING surpasses multiple supervised baselines on various metrics including human evaluation."
    }, {
      "heading" : "1 Introduction",
      "text" : "Language Models (LMs) like GPT-2 (Radford et al., 2019), trained over vast unstructured data, can leverage enhanced generation methods (Holtzman et al., 2020; Martins et al., 2020; Welleck et al., 2019) to give fluent and coherent continuations to given input text—e.g. news articles or stories.\n1Further results and resource are available at https://homes.cs.washington.edu/˜pawest/ ReflectiveDecoding.html\nRD.\nGPT-3 (Brown et al., 2020) takes this a step further: given a small number of examples and a well-constructed prompt, it shows remarkable performance on tasks where vast quantities of supervised data and finetuning were thought to be necessary. While this demonstrates the potential for LM-decoding in few-shot or even zero-shot outof-the-box settings, limited access to GPT-3 and immense computational cost keep this from being a widely usable or efficient solution.\nYet recent work shows that GPT-2 may hold similar capabilities when it is primed correctly. Li and\nLiang (2021) achieve supervised-level performance in a few-shot setting using smaller, accessible models like GPT-2. They learn a small number of taskspecific vectors as a prefix to the input, without tuning the model itself. Off-the-shelf GPT-2 is capable of few-shot learning given the right setup; our work aims to push this concept further, by showing that out-of-the-box LMs can solve complex generation problems simply by using the right decoding algorithm.\nWe introduce REFLECTIVE DECODING—a novel decoding method that allows LMs to be applied to generation tasks that break the “text continuation” paradigm, such as paraphrasing and textinfilling. REFLECTIVE DECODING requires no supervision, only two complementary off-the-shelf LMs—one forward ( −→ LM) and one backward ( ←− LM). That means no per-task finetuning, even on unstructured text in the target domain.\nInspired by the distributional hypothesis (Firth, 1957), REFLECTIVE DECODING works by generating text that might occupy the same contexts as an input. We use two LMs ( −→ LM and ←− LM) to generate contexts for a given input, which implicitly capture aspects of its meaning (the contextualization step). Then, in the reflection step, we condition on this ensemble of contexts, decoding over the input with generations that are distributionally related to—or replace—the input.\nParaphrasing is a natural application: a good paraphrase should intuitively be compatible with the same contexts as the original text. REFLECTIVE DECODING shows strong unsupervised paraphrasing performance: On the Quora question pair dataset, we find one variant of our model (RD30) outperforms unsupervised baselines on all but one metric, and supervised baselines on both the SARI metric and human evaluation. We see the same trends on the Twitter URL corpus (Lan et al., 2017).\nREFLECTIVE DECODING can also be applied to tasks that only replace part of the input, or generate within it, like infilling; on αNLG (Bhagavatula et al., 2020), we outperform the best unsupervised baseline on overall quality, effectively halving the gap with supervised methods. In both applications, REFLECTIVE DECODING directly applies off-theshelf pretrained models, without finetuning on the task or target domain. This provides evidence that off-the-shelf Language Models can excel at surprising applications, when paired with decoding algorithms designed to elicit specific kinds of infor-\nmation."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "Arrows indicate the order in which sampling functions condition on and generate tokens: −→ indicates generating from the left-most token to the right (left-to-right), while←− indicates going rightto-left. For Language Models (LMs), this means−→ LM is what is often called a “forward LM”, while←− LM is a “backward LM”. For our sampling function (RD), this also indicates which generated context is being conditioned on, e.g. −→ RD conditions on left context, extending it to the right to generate output."
    }, {
      "heading" : "2.2 Overview",
      "text" : "Currently, LM-decoding is limited to a text continuation paradigm. Given an input text sinput, LM(c|sinput) generates contexts c that might come after (forward, i.e.\n−→ LM) or before (backward, i.e.←− LM) the input. LM-decoding generates outside of the input by continuing it, but many tasks require us to generate over or within the input: paraphrasing requires reformulating the input, while infilling requires inserting text in the middle of it.\nReflective Decoding approaches this shortcoming by turning conventional LM-decoding around. While LM(c|sinput) generates the kinds of contexts c the input might appear in, RD generates s that might replace sinput in these same contexts. The distributional hypothesis (Firth, 1957) suggests semantically similar texts appear in similar contexts, meaning RD is also likely to sample in the semantic neighborhood of sinput.\nConcretely, REFLECTIVE DECODING samples s that fits the same contexts as sinput in 2 simple steps. We first sample many representative contexts ci that could neighbor the input, e.g. using −→ LM in Figure 1. This is the contextualization step. Second, in the reflection step, we generate text in the opposite direction (using ←− LM in Figure 1), which fits these contexts as well as sinput fits them. To consider all ci’s while decoding, we ensemble the different distributions imposed by conditioning on each ci:\n←− RD (s) =\n∏ i\n←− LM(s|ci)wi Z(s, c, w)\n(1)\nwhere Z normalizes the fraction to a proper probability distribution (see Equation 2). In essence, this\nAlgorithm 1: Learn REFLECTIVE DECODER←−RD Input: Forward language model −→ LM\nBackward language model ←− LM\nSource text sinput 1: Sample contexts, c1...cnc ∼ −→ LM(c|sinput)\n2: Initialize parameters w = w1...wnc s.t.∑ wi = 1, wi ≥ 0 3: learn w = argmaxw ←− RD(sinput)\ns.t. ∑ wi = 1, wi ≥ 0\nOutput: ←− RD\nensemble ←− RD restricts generations to fit all contexts ci. Reversed −→ RD is the same, except it uses −→ LM with left contexts ci generated by ←− LM.\nBy ensembling the contexts in a Product of Experts (Hinton, 2002) framework, we can generate a hypothesis s that fits the full contextual fingerprint. Yet, some contexts are more informative than others: probable but generic contexts like “See the appendix for details.” are not descriptive of neighboring text. We learn weights wi to prioritize contexts ci in the ensemble that are most informative for sinput, by maximizing the probability of sinput under Equation 1 (described in Algorithm 1). In effect, we are learning an on-the-fly autoencoder at inference time, using weighted ensembles of contexts as a representation (see §2.7, §A.1).\nTo motivate how this method functions, consider the paraphrasing example from Figure 1 with input sinput = How are circulatory system tissues formed? Generated contexts reflect different aspects of sinput: c1 situates sinput as a question (This is a medical question...), while c2 and c3 explore central concepts (as with all tissue...; about the circulatory system). Even though each context could follow many sentences, together they form a fingerprint for sinput. A sentence that could be followed by all of c1, c2, c3 will likely be a question (c1), about tissue formation (c2), and the circulatory system (c3), and generally occupy the same semantic neighborhood as sinput, e.g. How do circulatory systems form?\nIn the case of paraphrasing, our task is to replace all of sinput with something that might appear in the same contexts. Other tasks, however, might require us to replace only part of a sentence (e.g. incontext paraphrasing) or even insert text at a given position (e.g. infilling). REFLECTIVE DECODING makes this easy: simply hold part of sinput static when we generate from RD."
    }, {
      "heading" : "2.3 REFLECTIVE DECODING",
      "text" : "Here we dive into the details of REFLECTIVE DECODING, by considering the right-hand context ensemble ( ←− RD), keeping in mind that the process is repeated on the left-hand as well ( −→ RD).\nFirst, in the contextualization step (line 1 of Algorithm 1), we sample many right-hand contexts ci for sinput, using −→ LM. These will be used as a representative sample of the contexts sinput appears in. Second, in the reflection step (lines 2 & 3) our goal is to construct a sampling function ←− RD that will yield texts similar to sinput. We define ←− RD as:\n←− RD(s) =\n∏ i\n←− LM(s|ci)wi∏|s|\nj=0 ∑ t∈V ∏ i ←− LM(t|sj+1:|s| + ci)wi\n(2) This is equivalent to Equation 1, but giving the exact normalization factor in the denominator.\nEquation 2 is a token-wise Product of Experts model, that captures the semantic neighborhood of sinput via the combination of contexts ci and their weights wi (§2.7). We learn wi that maximize←− RD(sinput) (probability of generating sinput under ←− RD), thereby up-weighting contexts specific to sinput. We initialize these weights (line 2), then train them (line 3) using the Adam optimizer (Kingma and Ba, 2014). We normalize weights into a proper probability distribution at every step.\nReverse-direction −→ RD is learned symmetrically,\nflipping the roles of −→ LM and ←− LM and sampling lefthand context instead (see §B.1 for details). Finally, we generate from ←− RD (and −→ RD), sampling outputs that would appear in the same contexts as sinput. Depending on the application, we rank and select a final output in different ways, always using −→ LM and ←− LM together to capture bidirectional fit."
    }, {
      "heading" : "2.4 Implementation",
      "text" : "Weight Learning and Pruning Context weights wi are learned using the Adam optimizer (Kingma and Ba, 2014). In practice this takes under 100 steps (negligible time compared to LM decoding). While we sample tens of contexts (line 1 of Algorithm 1), many end up with negligible weight under the learned distribution (Equation 2). To efficiently sample from ←− RD and −→ RD, we drop all but the top kc contexts and renormalize weights: kc < nc contexts are used during the reflection step.\nParameters We sample nc contexts to describe the source sinput. We use nucleus sampling (Holtz-\nman et al., 2020) with parameter pc, and a maximum length of lenc. Once −→ RD and ←− RD are learned, we sample ns generations from each, of length lens. We again use nucleus sampling, but choose ps per-example to account for vastly different entropy in RD (§B.3). Values for all hyperparameters are available in §B.4. Language Models We train large forward ( −→ LM) and backward ( ←− LM) Language Models based on GPT-2 (Radford et al., 2019) using the OpenWebText training corpus (Gokaslan and Cohen, 2019)2. Our implementation details follow those of past work retraining GPT-2 (Zellers et al., 2019)."
    }, {
      "heading" : "2.5 Application: Paraphrasing",
      "text" : "To paraphrase, we begin by generating candidate outputs. Following §2.3 the REFLECTIVE DECODING sampling function is learned in each direction ( −→ RD, ←− RD) using the source sentence sinput. Then, ns generations are sampled from both −→ RD and ←− RD:\ns1, ..., sns ∼ −→ RD, sns+1, ..., s2∗ns ∼ ←− RD\nThis gives a robust set of candidates that are compatible with the same left and right contexts as sinput. Many of these will be semantically related to sinput, but must be scored and ranked in order to select true paraphrases. REFLECTIVE DECODING is based on the notion that good “fit” with the same contexts is a robust measurement of similarity, yielding a natural “contextual scoring function” (Equation 7 and §2.7). We measure how likely candidate s is to generate the same contexts that sinput did when constructing −→ RD and ←− RD:\nscore(s) = 1\nnc ∑ crh −→ LM(crh|s)+ 1 nc ∑ clh ←− LM(clh|s)\n(3)\nwhere crh are the generated contexts used in ←− RD,\n2https://github.com/yet-another-account/openwebtext\nand clh for −→ RD. This explicitly estimates how similar the contexts of s and sinput are on both sides, the underlying objective of REFLECTIVE DECODING."
    }, {
      "heading" : "2.6 Application: Abductive Reasoning",
      "text" : "Abductive natural language generation (αNLG from Bhagavatula et al. 2020) is the task of filling in the blank between 2 observations o1 and o2, with a hypothesis h that abductively explains them. The challenge for LM-decoding is making use of context from both sides (o1 on the left and o2 on the right). This is particularly challenging for unsupervised decoding methods because unidirectional LMs cannot naturally condition on both sides when generating h.\nREFLECTIVE DECODING simplifies this problem by capturing information about both o1 and o2 in a single decoding function ( ←− RD or −→ RD), then holding o1 and o2 static at generation time (i.e. teacher forcing). Concretely, we use concatenated o1+o2 as sinput in Algorithm 1, and construct sampling functions −→ RD, ←− RD informed by both observations. We are interested in sampling in between o1 and o2, so when sampling hypotheses h from ←− RD we condition on the right-side observation o2 (and vice-versa for −→ RD and o1). This is equivalent to appending the given observation to sampled contexts:\nh1, ..., hnŝ ∼ ←− RD(h|o2)\nhnŝ+1, ..., h2∗nŝ ∼ −→ RD(h|o1)\n(4)\nNote that both −→ RD and ←− RD contain information about both o1 and o2, effectively turning a 2-sided contextual constraint into a 1-sided one.\nWe also use a task-specific scoring function to rank sampled hypotheses. We would like a hypothesis h that best explains both observations, and so use Language Models to measure this:\nscore(h) = ←− LM(o1|h+o2)+ −→ LM(o2|o1+h) (5)\nAdding h should help to “explain” each observation given the other, i.e. that o2 follows from o1+h and o1 from h+ o2. To filter hypotheses that only explain one of the two observations, we remove any that make either observation less probable than the empty hypothesis, imposing:\n←− LM(o1|h+ o2) > ←− LM(o1|o2) −→ LM(o2|o1 + h) > −→ LM(o2|o1)"
    }, {
      "heading" : "2.7 Intuitions and Theory",
      "text" : "Here we discuss the theoretical intuition for REFLECTIVE DECODING, as a way to sample generations that share contextual “fit” with a source text, deriving the sampling function of Equation 2.\nWe start by considering how to relate the meaning of two texts, generation s and input sinput. We follow a distributional intuition (Firth, 1957), that meaning can be understood through the contexts in which text appears. Many distributional approaches learn contentful neural representations by predicting context given input text (Mikolov et al., 2013; Kiros et al., 2015), then compare these representations to establish semantic similarity. We can, instead, compare contexts directly—judging the difference in meaning between texts sinput and s by their divergence:\nDKL( −→ LM(c|sinput), −→ LM(c|ŝ)) (6)\nWe use −→ LM to interchangeably denote the theoretical left-to-right distribution of text, and the LM estimating it. Thus, −→ LM(c|s) is the distribution over right contexts c given sentence s, and Equation 6 can be understood as the “contextual information difference” we expect s to have from sinput. Note, we could similarly use left-hand context and ←− LM —and do so in practice. We use finite-sample cross entropy as an effec-\ntive empirical proxy for DKL:\nĤ( −→ LM(c|sinput), −→ LM(c|s)) = 1\nN ∑ ci∼ −→ LM(c|sinput) −log−→LM(ci|s)\n(7)\nWhere ci ∼ −→ LM(c|sinput) indicates sampling contexts for sinput from −→ LM. Intuitively, we want to minimize this score when generating s: an optimal output has a similar meaning to sinput and so fills\napproximately the same contextual hole, minimizing the value of this “contextual distance”.\nIn this form, Ĥ compares 2 complete texts–s and sinput–but we are trying to generate s for which the divergence from sinput is low. We flip the role of “text” and “context”3 to define a function from which we can sample s:\n←− RD(sj |, sj+1:n) =\n∏ i\n←− LM(sj |sj+1:n + ci)wi∑ t∈V ∏ i ←− LM(t|sj+1:n + ci)wi\n(8)\n(equivalent to Equation 2, derived in §A.1) sj is the jth token in s (sampled right-to-left from n to 0), and V is the vocabulary. Weights wi are learned by maximizing the probability of sinput.\nEquation 8, estimates the probability of predicting sinput and s from a finite set of contexts ci generated from sinput. This approximately minimizes Equation 6, as being generated by the same weighted ensemble of contexts strongly correlates with generating the same contexts in the same proportions, i.e. low divergence, due to the sparsity of language. We can sample s with low contextual distance from sinput using ←− RD. Further, we can use left context to construct −→ RD by simply reversing the directions of the LMs used."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Task: Paraphrase Generation",
      "text" : "Task: Following past work, we test our paraphrasing method (§2.5) on the Quora question pair dataset. We hold out 1000 examples for testing, with the rest for training and validation (used by supervised baselines), disallowing overlap with the test set. We test a subset of models (compatible unsupervised models, MT) on the Twitter URL corpus (Lan et al., 2017), using 1000 examples from the canonical test split.\nMetrics: Following past work, we include automatic metrics BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and TERp (Snover et al., 2009). These measure agreement with references, but high reference/input overlap means copying is rewarded (Mao and Lee, 2019); indeed, copying source sentences as-is wins on these metrics (Table 1), meaning both BLEU and METEOR can be easily gamed.\n3Context is a symmetric relation: a given text serves as the one-sided context of its own context.\nPast work has emphasized the important challenge of generating novel paraphrases (Liu et al., 2010; Chen and Dolan, 2011) We address this in 3 ways. First, we explicitly quantify a simple notion of novelty:\nNovelty(ŝ) = 100−BLEU(ŝ, sinput) (9)\nto quantify the novelty-quality trade-off. Second, we include the SARI metric (Xu et al., 2016) which explicitly balances novelty from input with reference overlap. Third, we quantify an overall human quality metric accounting for this.\nWe have humans evaluate fluency, consistency, and novelty on Amazon Mechanical Turk. The overall score (“Human” in Table 1) is the rate examples meet thresholds for all 3: fluent enough to understand, with at most minor differences in meaning and at least minor differences in wording. On quora, we test 200 examples, with agreement (Fleiss’ κ Fleiss, 1971) of 0.40 (fluency), 0.54 (consistency), 0.77 (novelty) and 0.48 (overall) i.e. moderate to substantial agreement (Landis and Koch, 1977). On the Twitter corpus, we use 100 examples with agreement of 0.39, 0.42, 0.54, and 0.36, indicating fair to moderate agreement. On both we have 3 raters per example. See §C.2 for more.\nBaselines: Parameters for REFLECTIVE DECODING are given in §B.4. We mainly compare against 3 unsupervised baselines: Controlled Sentence Generation by Metropolis Hastings (CGMH from Miao et al. 2019), Simulated Annealing (UPSA\nfrom Liu et al. 2019) and the residual VQ-VAE of Roy and Grangier (2019a) (R-VQVAE). This is a cross-section of recent approaches (VAE, editing).\nWe also compare against a machine-translation approach (see Sec 6), pivoting through German using Transformer (Vaswani et al., 2017) models trained on WMT19 data (Barrault et al., 2019). MT is included in a separate section in our results as it uses supervised bilingual data (Table 1).\nWe include supervised baselines: the pointer generator trained by imitation learning (PG-IL) as in Du and Ji (2019), the diversity-promoting DiPS model (Kumar et al., 2019), and a finetuned BART model (Lewis et al., 2019), which uses a more complex pretraining method than our LMs. Note that DiPS generates multiple diverse paraphrases so we pick one at random.\nCGMH and REFLECTIVE DECODING both return multiple sampled, ranked paraphrases. We can easily control for Novelty by taking the highestranked output that meets a Novelty threshold. For both, we have a version with no threshold (Top), and with thresholds such that average Novelty is 30 and 45. Novelty cutoffs do not depend on the reference, only the source, and are equivalent to selecting with BLEU-ori (Novelty is 100 − BLEU-ori) by Miao et al. (2019) or Bao et al. (2019)."
    }, {
      "heading" : "3.2 Task: Abductive NLG",
      "text" : "Task: The Abductive natural language generation task (αNLG) presented in Bhagavatula et al. (2020) requires generating a hypothesis that fits\nbetween observations o1 and o2, and explains them. We apply REFLECTIVE DECODING to this problem as outlined in §2.6, using the given data splits.\nMetrics: For human evaluation, over 200 examples we ask 3 raters on Amazon Mechanical Turk about coherence between h and o1, o2, o1+o2, and overall quality on 4-value likert scales. We found Fleiss’ kappa (Fleiss, 1971) of 0.32, 0.40, 0.41, and 0.41 respectively, indicating fair to moderate agreement (Landis and Koch, 1977).\nBaselines: Parameters for REFLECTIVE DECODING are given in §B.4. We include baselines from the original work: different supervised variants of GPT-2 large with access to the observations, and optionally commonsense embeddings or generations from COMET (Bosselut et al., 2019). We include unsupervised baselines of GPT-2 conditioned on o1 + o2 directly, the gradient-based DeLorean model of Qin et al. (2020), and ILM infilling model of Donahue et al. (2020), representing recent unsupervised methods."
    }, {
      "heading" : "4 Results and Analysis",
      "text" : "Paraphrasing First, the Quora dataset: On automatic metrics from past works (BLEU, METEOR, TERP ) our lowest-Novelty model setting (RDTop) achieves the highest unsupervised scores, and highest overall on BLEU. Other high scoring rows (Source, PG-IL) are similarly low-Novelty. The SARI metric explicitly balancesNovelty with similarity to reference. On SARI we see such lowNovelty models perform worse. The best overall model on SARI is our medium-Novelty setting (RD30) which outperforms MT and supervised models.\nOur human evaluation measures what fraction of\noutputs are found to be fluent, consistent, and novel. As with SARI, both our mid and high-Novelty models perform quite well, again with the mediumNovelty setting outperforming all baselines. As further validation for SARI as a proxy for human, they share the same top-5 models.\nResults on the Twitter URL corpus largely support those on Quora. REFLECTIVE DECODING achieves the best unsupervised scores on noveltyaware metrics (Table 2), with the best overall SARI, even outperforming reference on the human metric, although MT achieves the highest overall.\nIn sum, REFLECTIVE DECODING is able to compete on previously used quality metrics favoring low-Novelty, but can produce more varied outputs preferred by humans. RD45 is among the best models by SARI and Human on Quora despite exceeding the novelty of even the reference.\nαNLG Results on αNLG (Table 3) present a strong case that REFLECTIVE DECODING can effectively use bidirectional context. Strong hypotheses use information from both initial the observation o1 and the future observation o2. Humans ranked the ability of REFLECTIVE DECODING to capture this 42.4, about 17 points above the nextbest unsupervised baseline and only 15 points below the best supervised method tested. We see similar results for overall evaluation. A likely factor in this is the (comparatively) high degree of coherence between h and o2 by REFLECTIVE DECODING. Where other methods seem to pay more attention to observation o1 (the o2 column generally has much lower values), REFLECTIVE DECODING has comparably high coherence with left-hand (o1) and right-hand (o2) contexts.\nWe also include example generations in Figure 2 to demonstrate the ability of REFLECTIVE DECODING to combine o1 and o2. For example, h = He put her on the swing, and while she was on the swing, she fell off and was lying on the ground. incorporates information from both observations. Specifically, it takes into account the swing that Ray is building for his daughter which is only mentioned in o1, and hypothesizes about a potential injury due to Ray checking on his daughter in o2. See appendix for more generations.\nOverall, the strong performance of REFLECTIVE DECODING on αNLG shows that unsupervised generation with context ensemble applies to infilling in addition to paraphrasing."
    }, {
      "heading" : "5 Discussion",
      "text" : "REFLECTIVE DECODING Out-of-the-Box A major advantage to applying REFLECTIVE DECODING is ease-of-use: armed with our pretrained language models, practitioners can immediately begin generating. With general pretrained models and underlying principles that are domain-agnostic, REFLECTIVE DECODING works across a broad range of text style–no finetuning required–making exploration and adaptation simple. Multiple rounds of generation mean REFLECTIVE DECODING may run slower than other methods at inference time4, but it avoids training time. There are clearly settings that favor supervised learning (narrow, known domain with abundant training data), but REFLECTIVE DECODING is a good option to begin generating and exploring immediately with high quality generation.\nA useful abstraction for understanding REFLECTIVE DECODING for current applications is “prompting”, i.e., writing a prefix to implicitly or explicitly describe a task for a pretrained model. REFLECTIVE DECODING generates natural contexts that the desired generation would appear in. This breaks from other methods of automatic prompting, which often forego “natural” prompts (Shin et al., 2020; Reynolds and McDonell, 2021), even making them continuous (Li and Liang, 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Qin and Eisner, 2021). REFLECTIVE DECODING also notably creates a set of prompts (contexts) for each example, where other methods attempt to learn an overall task prompt. Still, all of these are connected by the popular intuition that useful behavior in pretrained models can be induced through contextual input.\nFuture Applications REFLECTIVE DECODING can extend beyond our experiments here, however. A simple example is in-context paraphrasing, i.e. writing a paraphrase that fits the true context that the original sentence appears in. Most existing paraphrasing methods consider only out-of-context sentences, and would require significant changes to consider context as a constraint; for REFLECTIVE DECODING we can simply combine true and generated contexts without with the same algorithm.\nDriving REFLECTIVE DECODING is a notion of context as a representation, with clear poten-\n4Depending on parameters we found most baselines took multiple seconds per example vs. 10s of seconds for REFLECTIVE DECODING on a multi-gpu machine.\ntial for future work. Pretrained LMs capture rich information about text spans, but accessing it without fine-tuning is nontrivial; within the model it is an uninterpretable mass of parameters and activation weights. Our work observes that unidirectional LMs are only capturing this information to predict adjacent context–this is the sole learning signal–so all of this information is expressed in the model’s context prediction. Thus, we capture some of this rich information to represent spans, by capturing a finite-sample version of this full predictive distribution in generated contexts. In REFLECTIVE DECODING specifically, we use this form of representation to generate back into the source span–paraphrasing or infilling–but the notion can be applied much more generally. In translation for instance, we might first generate contexts for the source sentence that represent its meaning, noisily translate these contexts, then impose that any translations for the source fit the same contexts under a translation-language LM. Constraining translations in this way can add robustness to existing systems by anchoring translations to informative contexts. Beyond explicit generation even, we might use a very large LM like GPT-3 to define a strong scoring function or metric as in Equation 7, first generating contexts for some target sentence, then scoring candidates by how well they generate these same contexts. As in our work, such a score could indicate how well the option fills the same contextual role as the target, harnessing the strong reasoning of whatever model is used."
    }, {
      "heading" : "6 Related Work",
      "text" : "Distributional Intuitions A key aspect of REFLECTIVE DECODING is using a distributional intuition to represent the meaning of a text through many contexts. Kiros et al. (2015); Miao et al. (2019) quantify semantic relationships and Lin and Pantel (2001) identify paraphrastic relationships under similar intuitions. A major point of difference between past work and ours is that we sample explicit contexts, allowing unsupervised generation back from these contexts, while past work typically learns a neural representation based on contexts and conditions on this vector-encoded representation.\nUnsupervised Paraphrasing Some approaches train neural variational auto-encoders unsupervised to represent source sentences, then decodes from these representations to paraphrase (Roy\nand Grangier, 2019b; Bao et al., 2019). This requires training specialized representations, whereas REFLECTIVE DECODING applies general-purpose LMs. We compare to Roy and Grangier (2019b).\nParaphrasing by editing the input (Miao et al., 2019; Liu et al., 2019) has shown promise. Like REFLECTIVE DECODING, these approaches can be applied without training specialized models, but are necessarily limited by edit-paths and local minima, as edits are often restricted to single-word replacement, insertion, and deletion. Generated paraphrases must follow a continuous local edit path, while REFLECTIVE DECODING can generate new sentences from scratch.\nREFLECTIVE DECODING and MT-based paraphrasing both pivot through an alternative textual form to paraphrase (context and translation, respectively). But MT paraphrasing systems cycletranslate through a pivot language (Federmann et al., 2019; Wieting and Gimpel, 2018), which requires supervised bilingual translation data, with an implicit notion of interlingual paraphrasing.\nNovelty in Paraphrasing Mao and Lee (2019) observe that paraphrases close to the source often win on automatic quality metrics. However, dissimilarity from the source correlates with human notions of paraphrasing (Liu et al., 2010). Kumar et al. (2019) increase novelty through their diversity-promoting sampling method. Alternative metrics that consider novelty alongside quality have been proposed (Sun and Zhou, 2012; Federmann et al., 2019). The SARI metric (Xu et al., 2016), included here, combines these notions.\nAbductive Text Infilling αNLG (Bhagavatula et al., 2020) is a text infilling task that specifically measures the ability of models to explain bidirectional context (observations o1, o2) with a hypothesis that fits between them. This naturally fits REFLECTIVE DECODING, which fills in contextual gaps. Recent work has directly addressed this task (Qin et al., 2020) while the infilling literature is also quite applicable (Donahue et al., 2020). We compare to both of these methods on abductive infilling, showing superior results."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We present REFLECTIVE DECODING, a novel unsupervised text generation method for tasks that do not fit the text continuation paradigm. It uses just two pretrained Language Models to generate contexts that capture aspects of input text, generating back into the input from there. It significantly outperforms unsupervised baselines in quality and novelty for paraphrasing. Further, in abductive natural language generation it outperforms unsupervised baselines by a significant margin and halves the gap with supervised models. REFLECTIVE DECODING uses the concept of representing meaning with generated contexts, offering new possibilities for unsupervised conditional text generation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank anonymous reviewers for many helpful comments. This research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number 401233309), DARPA CwC through ARO (W911NF15-1-0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the Allen Institute for AI, and a gift from Intel Labs Cognitive Computing Research.\nEthical Considerations\nIn order to complete our human evaluation we used Amazon Mechanical Turk. We estimated the range of times we expected our task to take, and made sure that at minimum workers would be paid a wage of $15.00 per hour if they were solely completing our task.\nAs part of this effort, we plan to release our code and model. Our forward and backward language models are the same size as the publicly available GPT-2 (Radford et al., 2019). Training time/energy was likely significantly smaller than\nthe original release; existing code and hyperparameters were available, and we use a smaller dataset. Further, there is no publicly available backward GPT-2 model that we are aware of, so releasing a pair of forward and backward models that were trained on the same data allows for proper comparisons about left-to-right vs. right-to-left processing of English text.\nWe estimate that the potential dangers of releasing this from a malicious generation perspective are low. Our forward model is similar to already released GPT-2 models. While the backward model adds new generation potential and scientific novelty, it is unlikely to compare to GPT-3 (Brown et al., 2020) which many hobbyists and private companies now have access to. We believe that releasing a pair of forward and backward models will be more useful to researchers who wish to study the symmetries and asymmetries of the linguistic distribution."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Derivation of Sampling Function Here we derive the sampling function used for REFLECTIVE DECODING, which allows generation using contextual similarity. This supplements §2.7. Pc|s denotes the distribution of contexts c for sentence s. This will be 1-sided context, for instance right-hand context crh (i.e. Pc|s would be estimated by left-to-right −→ LM conditioned on s −→ LM(c|s)). Reversed Ps|c goes back from context towards text. With right-hand context, this is estimated by ←− LM(s|c).\nIn §2.7, we consider the task of comparing a source sentence ssrc with another sentence s. For instance, we may want to know if s is a paraphrase of ssrc. Following a distributional intuition (Firth, 1957) we define a simple way to compare meaning:\nDKL(Pc|ssrc , Pc|s) (10)\nWhere DKL is the Kullback–Leibler divergence measuring the difference between distributions Pc|ssrc and Pc|s. This captures a notion above: we take the amount the contexts of ssrc and s differ as a proxy for their difference in meaning.\nIn paraphrase generation, we want to select for contextual closeness, and thus only need to rank options. We will then use cross-entropy:\nH( −→ LM(c|ssrc), −→ LM(c|s))\n= ∑ c −−→LM(c|ssrc)log(Pc|s(c))\n(11)\nwhich is equivalent to DKL up to a constant offset, and is easier to estimate. Here, the sum over c indicates every possible context c, but in practice we us finite samples.\nFrom Sec 2.7, this quantifies contextual difference in meaning. For paraphrasing, we want a sentence s that minimizes this, which is equivalent to maximizing the exponent of its negation:\nScore(s) = e ∑ c−Pc|slog(Pc|s(c))\n= ∏ c ( Ps|c(s)P (c) P (s) )Pc|s(c) =\na0 P (s) ∏ c Ps|c(s) Pc|s(c)\n(12)\nConstant a0 results from factors of P (c). The result is a Product of Experts (Hinton, 2002). P (s)−1 will\nprioritize more context-specific paraphrases (low probability but likely in context). However, our LMs are not well equipped to handle unlikely text, (expressivity is likely spent on likely text). Second, while less likely text can have higher similarity, this may not be the goal of our system. Rather we want related sentences that are also fluent and reasonable, so we drop P (s)−1, the equivalent of multiplying in P (s), biasing the model towards likely sequences:\nScore(s) = c0 ∏ c Ps|c(s) Pc|s(c)\n(13)\nA product of experts of the form:\nScore(s) = ∏ c Ps|c(s) wc|s\n(14)\nWe must set the weights wc|s in the finite sample setting. To keep in line with this the format, we will enforce that weights constitute a proper distribution. In the limiting case (unlimited samples) wc|s should be set to Pc|s(c). However, these are likely not efficient estimation weights. Further, exponentiating by this estimate will magnify errors. Instead, we learn these weights using a heuristic, discussed later.\nNext, we move to the finite-sample setting, replacing distributions with LM estimates. Here we will consider right-context (meaning Ps|c is estimated by ←− LM) but the left-context case proceeds symmetrically. Substituting in the LM distribution:\nScore(s) = ∏ c ←− LM(s|c)wc|s (15)\nWhere now the product over c indicates product over the finite sampled contexts. We convert this to a sampling function, decomposing into tokens of generation s = s0...sn:\nScore(s0:n) = ∏ j ∏ c ←− LM(sj |sj+1:n)wc|s (16)\nThis restates equation 15 factorizing LM probability by tokens. Renormalizing and decomposing by token position gives a natural distribution to sample from:\nPsample(sj |sj+1:n) =∏ c\n←− LM(sj |sj+1:n)wc|s∑ t∈V ∏ c ←− LM(t|sj+1:n)wc|s\n(17)\nAlgorithm 2: Learn REFLECTIVE DECODING sampling function (left-to-right)\nInput: Left to right language model −→ LM\nRight to left language model ←− LM\nSource text: ssrc 1: Sample contexts, c1...cnc ∼ ←− LM(c|ssrc) 2: Initialize parameters w = w1...wnc s.t.∑ wi = 1, wi ≥ 0 3: learn w = argmaxw −→ RD(ssrc)\nunder ∑ wi = 1, wi ≥ 0\nOutput: −→ RD\nnormalizing token-wise over the vocabulary V to a proper distribution (sampling right-to-left, index n down, to match convention). This is referred to as ←− RD in the body of the paper, and stated in equation 8. This samples candidate generations that encourage adherence to the contextual scoring function.\nFinally, we learn the weights (a proper distribution): ssrc should receive the highest score (or similarly, should have the lowest contextual difference with itself, as it is likely in its own contexts).\nB Implementation Details\nB.1 Left-to-Right REFLECTIVE DECODING sampling function From §2.3, −→ RD is learned similar to ←− RD, switching the roles of −→ LM and ←− LM in algorithm 1. First, the roles of the language models are flipped in the sampling function:\n−→ RD (s) =\n∏ i\n−→ LM(s|ci)wi∏|s|\nj=0 ∑ t∈V ∏ i −→ LM(t|s0:j−1 + ci)wi\n(18) ci are now generated by right-to-left ←− LM (i.e. leftcontexts). see Algorithm 2.\nB.2 Post-processing Generations\nWithout learning stop-tokens, REFLECTIVE DECODING samples fixed number (lens) of tokens. Candidates are extracted from raw generations using sentence tokenization.\nB.3 Entropy Calibration\nEntropy calibration is used when sampling candidate generations (§2.4). When sampling output\ngenerations, generation parameters (truncation parameter ps from nucleus sampling, in paraphrasing) control how “greedy” or stochastic sampling is. However, the effect of ps depends on many dynamic (example-wise) factors. Setting ps too low may sample only the most likely option, too high gives off-topic candidates. The “correct” value of ps is highly example-dependent.\nWe define entropy calibration to control how much “randomness” is used in sampling in a robust way. Rather than directly setting a ps for all examples, this specifies the approximate entropy ĥ to sample with for each example. In the greedy case for instance, the desired entropy ĥ is set to 0 (i.e. picking from a set of 1 possible option).\nWe search for ps in each case that is expected to give the correct entropy for the full generation, although ps is a token-level parameter. To estimate this, we take sampling entropy over the source text s0...sn under the nucleus-sampling truncated distribution Pp:\nĥ = (19)∑ i ∑ w∈Vp −Pp(w|s0...si−1)logPp(w|s0...si−1)\n(20)\nVp is the truncated vocabulary with parameter ps. We select ps that gives a desired entropy, setthing this to 4 or 6 which we found effective (App. B.4).\nB.4 Parameters\nHere, we give model settings for our 2 experimental settings, paraphrasing and αNLG. See Table 4. αNLG requires higher variety (higher hsample, pc), and fewer generated contexts (nc). We experimented with different reasonable values on the dev set of each model, evaluating manually. We use transformer language models (Mega size) trained on TPU pods (TensorFlow) of size 512. These will be made publicly available. For generation we used 2 NVIDIA Titan Xp GPUs."
    }, {
      "heading" : "C Evaluation",
      "text" : "C.1 Automatic Metrics\nLinks to the automatic metrics: ROUGE, BLEU, METEOR, TERP , SARI, BERTScore,BLEURT. We include extra further metrics tested for Quora in table 5: ROUGE (Lin, 2004), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2020). For BLEURT, and BERTScore we use default settings. We also include iBLEU Sun and Zhou (2012) with α = 0.9.\nC.2 Human Evaluation\nHuman evaluation for Quora and Twitter are largely described in §3. We reiterate that thresholds are used for each measure, and “overall” is the rate that all thresholds are met. Agreement is calculated on these binary combined threshold categories (fol-\nlowing Schouten 1986). Full human results for paraphrasing are in Table 6. Human eval for αNLG is described in §3.\nC.3 Twitter Dataset We include here the full results for paraphrasing on the Twitter URL corpus (Lan et al., 2017), a set of paraphrase pairs created by linking tweets with matching shared URLs. We test unsupervised models CGMH, R-VQVAE (UPSA Twitter model is not available), and the backtranslation MT model. We include supplementary results to the main paper in Table 7."
    }, {
      "heading" : "D Further Generations",
      "text" : "See Figures 3,4 for outputs of REFLECTIVE DECODING and baselines."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2021,
    "abstractText" : "Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right. As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision. In this paper, we present REFLECTIVE DECODING, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks. Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward. First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing). Second, in the reflection step, we condition on these “context ensembles”, generating outputs that are compatible with them. Comprehensive empirical results1 demonstrate that REFLECTIVE DECODING outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods. REFLECTIVE DECODING surpasses multiple supervised baselines on various metrics including human evaluation.",
    "creator" : "LaTeX with hyperref"
  }
}