{
  "name" : "2021.acl-long.59.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CitationIE: Leveraging the Citation Graph for Scientific Information Extraction",
    "authors" : [ "Vijay Viswanathan", "Graham Neubig", "Pengfei Liu" ],
    "emails" : [ "pfliu3}@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 719–731\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n719"
    }, {
      "heading" : "1 Introduction",
      "text" : "The rapid expansion in published scientific knowledge has enormous potential for good, if it can only be harnessed correctly. For example, during the first five months of the global COVID-19 pandemic, at least 11000 papers were published online about the novel disease (Hallenbeck, 2020), with each representing a potential faster end to a global pandemic and saved lives. Despite the value of this quantity of focused research, it is infeasible\n1https://github.com/viswavi/ScigraphIE\nfor the scientific community to read this many papers in a time-critical situation, and make accurate judgements to help separate signal from the noise.\nTo this end, how can machines help researchers quickly identify relevant papers? One step in this direction is to automatically extract and organize scientific information (e.g. important concepts and their relations) from a collection of research articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; Gábor et al., 2018; Jain et al., 2020).\nExisting works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different conceptual relations. In this paper, we claim a better under-\nstanding of a research article relies not only on its content but also on its relations with associated works, using both the content of related papers and the paper’s position in the larger citation network.\nWe use a concrete example to motivate how information from the citation graph helps with SciIE, considering the task of identifying key entities in a long document (known as “salient entity classification”) in Figure 1.\nIn this example, we see a paper describing a speech recognition system (Saon et al., 2016). Focusing on two specific entities in the paper (“ImageNet classification challenge” and “Switchboard task”), we are tasked with classifying whether each is critical to the paper. This task requires reasoning about each entity in relation to the central topic of the paper, which is a daunting task for NLP considering that this paper contains over 3000 words across 11 sections. An existing state-of-the-art model (Jain et al., 2020) mistakenly predicts the non-salient entity “ImageNet classification challenge” as salient due to the limited contextual information. However, this problem is more approachable when informed of the structure of the citation graph that conveys how this paper correlates with other research works. Examining this example paper’s position in the surrounding citation network suggests it is concerned with speech processing, which makes it unlikely that “ImageNet” is salient.2\nThe clear goal of incorporating inter-article information, however, is hindered by a resource challenge: existing SciIE datasets that annotate papers with rich entity and relation information fail to include their references in a fine-grained, machinereadable way. To overcome this difficulty, we build on top of an existing SciIE dataset and align it with a source of citation graph information, which finally allows us to explore citation-aware SciIE.\nArchitecturally, we adopt the neural multi-task model introduced by Jain et al. (2020), and establish a proof of concept by comparing simple ways of incorporating the network structure and textual content of the citation graph into this model. Experimentally, we rigorously evaluate our methods, which we call CitationIE, on three tasks: mention identification, salient entity classification, and document-level relation extraction. We find that leveraging citation graph information provides significant improvements in the latter two tasks, in-\n2Our proposed method actually makes correct predictions on both these samples, where the baseline model fails on both.\ncluding a 10 point improvement on F1 score for relation extraction. This leads to a sizable increase in the performance of the end-to-end CitationIE system relative to the current state-of-the-art, Jain et al. (2020). We offer qualitative analysis of why our methods may work in §5.3."
    }, {
      "heading" : "2 Document-level Scientific IE",
      "text" : ""
    }, {
      "heading" : "2.1 Task Definition",
      "text" : "We consider the task of extracting document-level relations from scientific texts.\nMost work on scientific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; Gábor et al., 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016).\nWe focus on the task of open-domain documentlevel relation extraction from long, full-text documents. This is in contrast to the above methods that only use paper abstracts. Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al., 2011).\nWe base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research.\nEach document consists of sections D = {S1, . . . , SN}, where each section contains a sequence of words Si = {wi,1, . . . , wi,Ni}. Each document comes with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level relations. We break down the end-to-end information extraction process as a sequence of these four related tasks, with each task taking the output of the preceding tasks as input.\nMention Identification For each span of text within a section, this task aims to recognize if the span describes a Task, Dataset, Method, or Metric entity, if any.\nCoreference This task requires clustering all entity mentions in a document such that, in each cluster, every mention refers to the same entity (Varkel and Globerson, 2020). The SciREX dataset\n3The dataset contains 306 documents for training, 66 for validation, and 66 for testing.\nincludes coreference annotations for each Task, Dataset, Method, and Metric mention.\nSalient Entity Classification Given a cluster of mentions corresponding to the same entity, the model must predict whether the entity is key to the work described in a paper. We follow the definition from the SciREX dataset (Jain et al., 2020), where an entity in a paper is deemed salient if it plays a role in the paper’s evaluation.\nRelation Extraction The ultimate task in our IE pipeline is relation extraction. We consider relations as 4-ary tuples of typed entities (ETask, EDataset, EMethod, EMetric), which are required to be salient entities. Given a set of candidate relations, we must determine which relations are contained in the main result of the paper."
    }, {
      "heading" : "2.2 Baseline Model",
      "text" : "We base our work on top of the model of Jain et al. (2020), which was introduced as a strong baseline accompanying the SciREX dataset. We refer the reader to their paper for full architectural details, and briefly summarize their model here.\nThis multi-task model performs three of our tasks (mention identification, saliency classification, and relation extraction) in a sequence, treating coreference resolution as an external black box. While word and span representations are shared across all tasks and updated to minimize multi-task loss, the model trains each task on gold input. Figure 2 summarizes the baseline model’s end-to-end architecture, and highlights the places where we propose improvements for our CitationIE model.\nFeature Extraction The model extracts features from raw text in two stages. First, contextualized word embeddings are obtained for each section by running SciBERT (Beltagy et al., 2019) on that section of text (up to 512 tokens). Then, the embeddings from all words over all sections are passed through a bidirectional LSTM (Graves et al., 2005) to contextualize each word’s representation with those from other sections.\nMention Identification The baseline model treats this named entity recognition task as an IOBES sequence tagging problem (Reimers and Gurevych, 2017). The tagger takes the SciBERTBiLSTM (Beltagy et al., 2019; Graves et al., 2005) word embeddings (as shown in the Figure 2), feeds them through two feedforward networks (not\nshown in Figure 2), and produces tag potentials at each word. These are then passed to a CRF (Lafferty et al., 2001) which predicts discrete tags.\nSpan Embeddings For a given mention span, its span embedding is produced via additive attention (Bahdanau et al., 2014) over the tokens in the span.\nCoreference Using an external model, pairwise coreference predictions are made for all entity mentions, forming coreference clusters.\nSalient Entity Classification Saliency is a property of entity clusters, but it is first predicted at the entity mention level. Each entity mention’s span embedding is simply passed through two feedforward networks, giving a binary saliency prediction.\nTo turn these mention-level predictions into cluster-level predictions, the predicted saliency scores are max-pooled over all mentions in a coreference cluster to give cluster-level saliency scores.\nRelation Extraction The model treats relation extraction as binary classification, taking as input a set of 4 typed salient entity clusters. For each entity cluster in the relation, per-section entity cluster representations are computed by taking the set of that entity’s mentions in a given section, and max-pooling over the span embeddings of these mentions. The four entity-section embeddings (one for each entity in the relation) are then concatenated and passed through a feedforward network to produce a relation-section embedding. Then, the relation-section embeddings are averaged over all sections and passed through another feedforward network which returns a binary prediction."
    }, {
      "heading" : "3 Citation-aware SciIE Dataset",
      "text" : "Although citation network information has been shown to be effective in other tasks, few works have recently tried using it in SciIE systems. One potential reason is the lack of a suitable dataset.\nThus, as a first contribution of this paper, we address this bottleneck by constructing a SciIE dataset that is annotated with citation graph information.4 Specifically, we combine the rich annotations of SciREX with a source of citation graph information, S2ORC (Lo et al., 2020). For each paper, S2ORC includes parsed metadata about which other papers cite this paper, which other papers are\n4We have released code to construct this dataset: https: //github.com/viswavi/ScigraphIE\ncited by this paper, and locations in the body text where reference markers are embedded.\nTo merge SciREX with S2ORC, we link records using metadata obtained via the Semantic Scholar API:5 paper title, DOI string, arXiv ID, and Semantic Scholar Paper ID. For each document in SciREX, we check against all 81M documents in S2ORC for exact matches on any of these identifiers, yielding S2ORC entries for 433 out of 438 documents in SciREX. The final mapping is included in our repository for the community to use. Though our work only used the SciREX dataset, our methods can be readily extended to other SciIE datasets (including those mentioned in §2.1) using our released software.\nStatistics Examining the distribution of citations for all documents in the SciREX dataset (in Figure 3), we observe a long-tailed distribution of citations per paper, and a bell-shaped distribution of references per paper.\n5https://www.semanticscholar.org/\nIn addition to the 5 documents we could not match to the S2ORC citation graph, 7 were incorrectly recorded as containing no references and 5 others were incorrectly recorded as having no citations. These errors are due to data issues in the S2ORC dataset, which relies on PDF parsers to extract information (Lo et al., 2020)."
    }, {
      "heading" : "4 CitationIE",
      "text" : "We now describe our citation-aware scientific IE architecture, which incorporates citation information into mention identification, salient entity classification, and relation extraction. For each task, we consider two types of citation graph information, either separately or together: (1) structural information from the graph network topology and (2) textual information from the content of citing and cited documents."
    }, {
      "heading" : "4.1 Structural Information",
      "text" : "The structure of the citation graph can contextualize a document within the greater body of work.\nPrior works in scientific information extraction have predominantly used the citation graph only to analyze the content of citing papers, such as CiteTextRank (Das Gollapalli and Caragea, 2014) and Citation TF-IDF (Caragea et al., 2014), which is described in detail in §4.2.2. However, the citation graph can be used to discover relationships between non-adjacent documents in the citation graph; prior works struggle to capture these relationships.\nArnold and Cohen (2009) are the only prior work, to our knowledge, to explicitly use the citation graph’s structure for scientific IE. They predict key entities related to a paper via random walks on a combined knowledge-and-citation-graph consisting of papers and entities, without considering a document’s content. This approach is simple but cannot generalize to new or unseen entities.\nA rich direction of recent work has studied learned representations of networks, such as social networks (Perozzi et al., 2014) and citation graphs (Sen et al., 2008; Yang et al., 2015; Bui et al., 2018; Khosla et al., 2021). In this paper, we show citation graph embeddings can improve scientific information extraction.\nConstruction of Citation Graph To construct our citation graph, we found all nodes in the S2ORC citation graph within 2 undirected edges of any document in the SciREX dataset, including all edges between those documents. This process took 10 hours on one machine due to the massive size of the full S2ORC graph, resulting in a graph with ∼1.1M nodes and ∼5M edges.\nNetwork Representation Learning We learn representations for each node (paper) using DeepWalk6 (Perozzi et al., 2014) via the GraphVite library (Zhu et al., 2019), resulting in a 128- dimensional “graph embedding” for each document in our dataset. For each task, we incorporate the document-level graph embedding into that task’s model component, by simply concatenating the document’s graph embedding with the hidden state in that component. We do not update the graph embedding values during training.\nIncorporating Graph Embedding Each task in our CitationIE system culminates in a pair of feedforward networks. Figure 4 describes this general\n6An empirical comparison by Khosla et al. (2021) found DeepWalk to be quite competitive on two citation graph node classification datasets, despite its speed and simplicity.\narchitecture, though the input to these networks varies from task to task (SciBERT-BiLSTM embeddings for mention identification, span embeddings for salient entity classification, and per-section relation embeddings for relation extraction).\nThis architecture gives two options for where to concatenate the graph embedding into the hidden state - Stage 1 or Stage 2 - marked with a light blue block in Figure 4. Intuitively, concatenating the graph embedding in a later stage feeds it more directly into the final prediction. We find Stage 1 is superior for relation extraction, and both perform comparably for salient entity classification and mention identification. We give details on this experiment in Appendix A.3."
    }, {
      "heading" : "4.2 Textual Information",
      "text" : "Most prior work using the citation graph for SciIE has focused on using the text of citing papers. We examine how to use two varieties of textual information related to citations."
    }, {
      "heading" : "4.2.1 Citances",
      "text" : "Citation sentences, also known as “citances” (Nakov et al., 2004), provide an additional source of textual context about a paper. They have seen use in automatic summarization (Yasunaga et al., 2019), but not in neural information extraction.\nIn our work, we augment each document in our training set with its citances, treating each citance as a new section in the document. In this way, we incorporate citances into our CitationIE model through the shared text representations used by each task in our system, as shown in Figure 5. If our document has many citations, we randomly sample 25 to use. For each citing document, we select citances centered on the sentence containing the first reference marker pointing to our document of interest, and include the subsequent and consequent sentences if they are both in the same section.\nWe ensure the mention identification step does not predict entities in citance sections, which would lead to false positive entities in downstream tasks."
    }, {
      "heading" : "4.2.2 Citation TF-IDF",
      "text" : "Citation TF-IDF (Caragea et al., 2014), is a feature representing the TF-IDF value (Jones, 1972) of a given token in its document’s citances. We consider a variant of this feature: for each token in a document, we compute the TF-IDF of that token in each citance of the document, and average the percitance TF-IDF values over all citances. We imple-\nmented this feature only for saliency classification, as it explicitly reasons about the significance of a token in citing texts. As a local token-level feature, it also does not apply naturally to relation extraction, which operates on entire clusters of spans."
    }, {
      "heading" : "4.3 Graph Structure and Text Content",
      "text" : "We lastly consider using graph embeddings and citances together in a single model for each task. We do this naively by including citances with the document’s input text when first computing shared text features, and then concatenating graph embeddings into downstream task-specific components."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Metrics, Baselines and Training",
      "text" : ""
    }, {
      "heading" : "5.1.1 Metrics",
      "text" : "The ultimate product of our work is an end-to-end document-level relation extraction system, but we also measure each component of our system in isolation, giving end-to-end and per-task metrics. All metrics, except where stated otherwise, are the same as described by Jain et al. (2020).\nMention Identification We evaluate mention identification with the average F1 score of classifying entities of each span type.\nSalient Entity Classification Similar to Jain et al. (2020) we evaluate this task at the mention level and cluster level. We evaluate both metrics on gold standard entity recognition inputs.\nRelation Extraction This is the ultimate task in our pipeline. We use its output and metrics to evaluate the end-to-end system, but also evaluate relation extraction separately from upstream components to isolate its performance. We specifically consider two types of metrics: (1) Document-level: For each document, given a set of ground truth 4-ary relations, we evaluate a set of predicted 4-ary relations as a sequence of binary predictions (where a matching relation is a true positive). We then compute precision, recall, and F1 scores for each document, and average each over all documents. We refer to this metric as the “document-level” relation metric. To compare with Jain et al. (2020), this is the primary metric to measure the full system. (2) Corpus-level: When evaluating the relation extraction component in isolation, we are also able to use a more standard “corpus-level” binary classification evaluation, where each candidate relation from each document is treated as a separate sample.\nWe also run both these metrics on a binary relation extraction setup, by flattening each set of 4-ary relations into a set of binary relations and evaluating these predictions as an intermediate metric."
    }, {
      "heading" : "5.1.2 Baselines",
      "text" : "For each task, we compare against Jain et al. (2020), whose architecture our system is built on. No other model to our knowledge performs all the tasks we consider on full documents. For the 4-ary relation extraction task, we also compare against the DocTAET model (Hou et al., 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al., 2020; Hou et al., 2019).\nSignificance To improve the rigor of our evaluation, we run significance tests for each of our proposed methods against its associated baseline, via paired bootstrap sampling (Koehn, 2004). In experiments where we trained multiple models with different seeds, we perform a hierarchical bootstrap procedure where we first sample a seed for each model and then sample a randomized test set."
    }, {
      "heading" : "5.1.3 Training Details",
      "text" : "We build our proposed CitationIE methods on top of the SciREX repository7 (Jain et al., 2020) in the AllenNLP framework (Gardner et al., 2018).\nFor each task, we first train that component in isolation from the rest of the system to minimize\n7https://github.com/allenai/SciREX\nthe task-specific loss. We then take the best performing modifications and use them to train endto-end IE models to minimize the sum of losses from all tasks. We train each model on a single GPU with batch size 4 for up to 20 epochs. We include detailed training configuration information in Appendix A.1.\nFor saliency classification and relation extraction, we trained the baseline and the strongest proposed models three times,8 to improve reliability of our results. For mention identification, we did not retrain models, as the first set of results strongly suggested our proposed methods were not helpful."
    }, {
      "heading" : "5.2 Quantitative Results",
      "text" : "Mention Identification For mention identification, we observe no major performance difference from using citation graphs, and include full results in Appendix A.2.\nSalient Entity Classification Table 1 shows the results of our CitationIE methods. We observe: (1) Using citation graph embeddings significantly improves the system with respect to the salient mention metric. (2) Graph embeddings do not improve cluster evaluation significantly (at 95%) due to the small test\n8See Appendix A.1 for exact seeds used 9Reported as “Component-wise Binary and 4-ary Rela-\ntions” in Jain et al. (2020)\nsize10 (66 samples) and inter-model variation. (3) Incorporating graph embeddings and citances simultaneously is no better than using either. (4) Our reimplemented baseline differs from the results reported by Jain et al. (2020) despite using their published code to train their model. This may be because we use a batch size of 4 (due to compute limits) while they reported a batch size of 50.\nRelation Extraction Table 2 shows that using graph embeddings here gives an 11.5 point improvement in document-level F1 over the reported baseline,11 and statistically significant gains on both corpus-level F1 metrics.\nDespite seemingly large gains on the documentlevel F1 metric, these are not statistically significant due to significant inter-model variability and small test set size, despite the graph embedding model performing best at every seed we tried.\nEnd-to-End Model From Table 3, we observe: (1) Using graph embeddings appears to have a positive effect on the main task of 4-ary relation extraction. However, these gains are not statistically significant (p = 0.235) despite our proposed method outperforming the baseline at every seed, for the same reasons as mentioned above. (2) On binary relation evaluation, we observe smaller improvements which had a lower p-value (p = 0.099) due to lower inter-model variation. (3) Using citances instead of graph embeddings still appears to outperform the baseline (though by a smaller margin than the graph embeddings)."
    }, {
      "heading" : "5.3 Analysis",
      "text" : "We analyzed our experimental results, guided by the following four questions:\nDo papers with few citations benefit from citation graph information? Our test set only contains two documents with zero citations, so we cannot characterize performance on such documents. However, Figure 6 shows that the gains provided by the proposed CitationIE model with graph embeddings counterintuitively shrink as the number of citations of a paper increases. We also observe\n10The limited size of this test set is an area of concern when using the SciREX dataset, and improving statistical power in SciIE evaluation is a crucial area for future work.\n11The large gap between reimplemented and reported baselines is likely due to our reproduced results averaging over 3 random seeds. When using the same seed used by Jain et al. (2020), the baseline’s document-level test F1 score is almost 20 points better than with two other random seeds.\nthis with citances, to a lesser extent. This suggests more work needs to be done to represent citation graph nodes with many edges.\nHow does citation graph information help relation extraction? With relation extraction, we found citation graph information provides strongest gains when classifying relations between distant entities in a document, seen in Figure 7. For each relation in the test set, we computed the average distance between pairs of entity mentions in that relation, normalized by total document length. We find models with graph embeddings or citances perform markedly better when these relations span\nlarge swaths of text. This is particularly useful since neural models still struggle to model longrange dependencies effectively (Brown et al., 2020).\nDoes citation graph information help contextualize important terms? Going back to our motivating example of a speech paper referring to ImageNet in passing §1, we hypothesized that adding context from citations helps deal with terms that are important in general, but not for a given document.\nTo measure this, we grouped all entities in our test dataset by their “global saliency rate” measured on the test set: given a span, what is the probability that this span is salient in any given occurrence?\nIn Figure 8, we observe that most of the improvement from graph embeddings and citances comes at terms which are labeled as salient in at least 20%\nof their training-set mentions. This suggests that citation graph information yields improvements with reasoning about important terms, without negatively interfering with less-important terms."
    }, {
      "heading" : "6 Implications and Future Directions",
      "text" : "We explore the use of citation graph information in neural scientific information extraction with CitationIE, a model that can leverage either the structure of the citation graph or the content of citing or cited documents. We find that this information, combined with document text, leads to particularly strong improvements for salient entity classification and relation extraction, and provides an increase in end-to-end IE system performance over a strong baseline.\nOur proposed methods reflect some of the simplest ways of incorporating citation graph information into a neural SciIE system. As such, these results can be considered a proof of concept. In the future we will explore ways to extract richer information from the graph using more sophisticated techniques, hopefully better capturing the interplay between citation graph structure and content. Finally, we evaluated our proof of concept here on a single dataset in the machine learning domain. While our methods are not domain-specific, verifying that these methods generalize to other scientific domains is important future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Sarthak Jain for assisting with reproducing baseline results, Bharadwaj Ramachandran for giving advice on figures, and Siddhant Arora and Rishabh Joshi for providing suggestions on the paper. The authors also thank the anonymous reviewers for their helpful comments. This work was supported by the Air Force Research Laboratory under agreement number FA8750-192-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Training Configurations\nWe train each model on a single 11GB NVIDIA GeForce RTX 2080 Ti GPU with a batch size of 4. We train for up to 20 epochs, and set the patience parameter in AllenNLP to 10; if the validation metric does not improve for 10 consecutive epochs, we stop training early. For each taskspecific model, we use a product of validation loss and corpus-level binary F1 score on the validation set as the validation metric. For salient entity classification and relation extraction, we choose the best threshold on the validation set using F1 score.\nIn total, training with these configurations takes roughly 2 hours for salient entity classification, 8 hours for mention identification, 18-24 hours for relation extraction, and 24-30 hours for the end-toend system. Our CitationIE models took roughly as long to train as the baseline SciREX models did.\nFor models that we trained three different times, we use different seeds for each software library:\n• For PyTorch, we use seeds 133,12 11, and 22\n• For Numpy, we use seeds 1337, 111, and 222\n• For Python’s random library, we use seeds 11370, 1111, and 2222\nA.2 Mention Identification Results\nWe include results from using citation graph information for the mention identification task in Table 4. We observe no major improvements in this task. Intuitively, recognizing a named entity in a document may not require global context about the document (e.g. “LSTM” almost always refers to a Method, regardless of the paper where it is used), so the lack of gains in this task is unsurprising.\n12133/1337/13370 is the default seed setting in AllenNLP.\nA.3 Combining Graph Embeddings with Word Embeddings\nEach of our task-specific components in the CitationIE model contains two feedforward networks where we may concatenate graph embedding information. We refer to these two options for where to fuse graph embedding information as ”early fusion” and ”late fusion”, illustrated in Figure 4.\nHere we show a detailed comparison of early fusion vs late fusion models on Mention Identification (Table 5), Salient Entity Classification (Table 6), and Relation Extraction (Table 7). Based on these results, we used early fusion in our final CitationIE models for mention identification and relation extraction. For saliency classification, the relative performance of early fusion and late fusion differed across our two metrics, making this inconclusive. We used early fusion for saliency classification in the end-to-end model due to strong empirical performance there."
    } ],
    "references" : [ {
      "title" : "Information extraction as link prediction: Using curated citation networks to improve gene detection",
      "author" : [ "Andrew O. Arnold", "William W. Cohen." ],
      "venue" : "International Conference on Wireless Algorithms, Systems, and Applications (WASA), 5682:541–550.",
      "citeRegEx" : "Arnold and Cohen.,? 2009",
      "shortCiteRegEx" : "Arnold and Cohen.",
      "year" : 2009
    }, {
      "title" : "SemEval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientific publications",
      "author" : [ "Isabelle Augenstein", "Mrinal Das", "Sebastian Riedel", "Lakshmi Vikraman", "Andrew McCallum." ],
      "venue" : "Proceedings of the 11th International",
      "citeRegEx" : "Augenstein et al\\.,? 2017",
      "shortCiteRegEx" : "Augenstein et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural graph learning: Training neural networks using graphs",
      "author" : [ "Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala." ],
      "venue" : "Proceedings of the Eleventh",
      "citeRegEx" : "Bui et al\\.,? 2018",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 2018
    }, {
      "title" : "Citationenhanced keyphrase extraction from research papers: A supervised approach",
      "author" : [ "Cornelia Caragea", "Florin Adrian Bulgarov", "Andreea Godea", "Sujatha Das Gollapalli." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Caragea et al\\.,? 2014",
      "shortCiteRegEx" : "Caragea et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting keyphrases from research papers using citation networks",
      "author" : [ "Sujatha Das Gollapalli", "Cornelia Caragea." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 28(1).",
      "citeRegEx" : "Gollapalli and Caragea.,? 2014",
      "shortCiteRegEx" : "Gollapalli and Caragea.",
      "year" : 2014
    }, {
      "title" : "SemEval-2018 task 7: Semantic relation extraction and classification in scientific papers",
      "author" : [ "Kata Gábor", "Davide Buscaldi", "Anne-Kathrin Schumann", "Behrang QasemiZadeh", "Haı̈fa Zargayouna", "Thierry Charnois" ],
      "venue" : "In Proceedings of The 12th Inter-",
      "citeRegEx" : "Gábor et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gábor et al\\.",
      "year" : 2018
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of Workshop for",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the 15th International Conference on Artificial Neural Networks: Formal Models and",
      "citeRegEx" : "Graves et al\\.,? 2005",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Analyzing the dynamics of research by extracting key aspects of scientific papers",
      "author" : [ "Sonal Gupta", "Christopher Manning." ],
      "venue" : "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1–9, Chiang Mai, Thailand. Asian",
      "citeRegEx" : "Gupta and Manning.,? 2011",
      "shortCiteRegEx" : "Gupta and Manning.",
      "year" : 2011
    }, {
      "title" : "The covid-19 deluge: Is it time for a new model of data disclosure? ASBMB Today: The Member Magazine of the American Society for Biochemistry and Molecular Biology",
      "author" : [ "Ken Hallenbeck" ],
      "venue" : null,
      "citeRegEx" : "Hallenbeck.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hallenbeck.",
      "year" : 2020
    }, {
      "title" : "Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction",
      "author" : [ "Yufang Hou", "Charles Jochim", "Martin Gleize", "Francesca Bonin", "Debasis Ganguly." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the",
      "citeRegEx" : "Hou et al\\.,? 2019",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2019
    }, {
      "title" : "SciREX: A challenge dataset for document-level information extraction",
      "author" : [ "Sarthak Jain", "Madeleine van Zuylen", "Hannaneh Hajishirzi", "Iz Beltagy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "Karen Sparck Jones." ],
      "venue" : "Journal of Documentation.",
      "citeRegEx" : "Jones.,? 1972",
      "shortCiteRegEx" : "Jones.",
      "year" : 1972
    }, {
      "title" : "AxCell: Automatic extraction of results from machine learning papers",
      "author" : [ "Marcin Kardas", "Piotr Czapla", "Pontus Stenetorp", "Sebastian Ruder", "Sebastian Riedel", "Ross Taylor", "Robert Stojnic." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Kardas et al\\.,? 2020",
      "shortCiteRegEx" : "Kardas et al\\.",
      "year" : 2020
    }, {
      "title" : "A comparative study for unsupervised network representation learning",
      "author" : [ "Megha Khosla", "Vinay Setty", "Avishek Anand." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 33(5):1807–1818.",
      "citeRegEx" : "Khosla et al\\.,? 2021",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2021
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "S2ORC: The semantic scholar open research corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. As-",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
      "author" : [ "Yi Luan", "Luheng He", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Luan et al\\.,? 2018",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2018
    }, {
      "title" : "A general framework for information extraction using dynamic span graphs",
      "author" : [ "Yi Luan", "Dave Wadden", "Luheng He", "Amy Shah", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Luan et al\\.,? 2019",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2019
    }, {
      "title" : "Citances: Citation sentences",
      "author" : [ "Preslav I. Nakov", "Ariel S. Schwartz", "Marti A. Hearst" ],
      "venue" : null,
      "citeRegEx" : "Nakov et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2004
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena." ],
      "venue" : "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710.",
      "citeRegEx" : "Perozzi et al\\.,? 2014",
      "shortCiteRegEx" : "Perozzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal hyperparameters for deep lstm-networks for sequence labeling tasks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "ArXiv, abs/1707.06799.",
      "citeRegEx" : "Reimers and Gurevych.,? 2017",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2017
    }, {
      "title" : "The IBM 2016 english conversational telephone speech recognition system",
      "author" : [ "George Saon", "Tom Sercu", "Steven Rennie", "HongKwang J. Kuo." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Saon et al\\.,? 2016",
      "shortCiteRegEx" : "Saon et al\\.",
      "year" : 2016
    }, {
      "title" : "Collective classification in network data",
      "author" : [ "Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Gallagher", "Tina Eliassi-Rad." ],
      "venue" : "AI Magazine, 29:93–106.",
      "citeRegEx" : "Sen et al\\.,? 2008",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2008
    }, {
      "title" : "Pre-training mention representations in coreference models",
      "author" : [ "Yuval Varkel", "Amir Globerson." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Varkel and Globerson.,? 2020",
      "shortCiteRegEx" : "Varkel and Globerson.",
      "year" : 2020
    }, {
      "title" : "Assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease",
      "author" : [ "Chih-Hsuan Wei", "Yifan Peng", "Robert Leaman", "Allan Peter Davis", "Carolyn J Marringly", "Jiao Li", "Thomas C Wiegers", "Zhiyong Lu" ],
      "venue" : null,
      "citeRegEx" : "Wei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2016
    }, {
      "title" : "Network representation learning with rich text information",
      "author" : [ "Cheng Yang", "Zhiyuan Liu", "Deli Zhao", "Maosong Sun", "Edward Y. Chang." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Rui Zhang", "Alexander R. Fabbri", "Irene Li", "Dan Friedman", "Dragomir R. Radev" ],
      "venue" : null,
      "citeRegEx" : "Yasunaga et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting a scientific community’s response to an article",
      "author" : [ "Dani Yogatama", "Michael Heilman", "Brendan O’Connor", "Chris Dyer", "Bryan R. Routledge", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Yogatama et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2011
    }, {
      "title" : "Graphvite: A high-performance cpu-gpu hybrid system for node embedding",
      "author" : [ "Zhaocheng Zhu", "Shizhen Xu", "Meng Qu", "Jian Tang." ],
      "venue" : "The World Wide Web Conference, pages 2494–2504. ACM.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    }, {
      "title" : "Training Configurations We train each model on a single 11GB NVIDIA GeForce RTX 2080 Ti GPU with a batch size of 4. We train for up to 20 epochs, and set",
      "author" : [ "A Appendices A" ],
      "venue" : null,
      "citeRegEx" : "A.1,? \\Q2080\\E",
      "shortCiteRegEx" : "A.1",
      "year" : 2080
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "For example, during the first five months of the global COVID-19 pandemic, at least 11000 papers were published online about the novel disease (Hallenbeck, 2020), with each representing a potential faster end to a global pandemic and saved lives.",
      "startOffset" : 143,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "Figure 1: Example of using the citation graph to improve the task of salient entity classification (Jain et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al.",
      "startOffset" : 42,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : "Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al.",
      "startOffset" : 42,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : ", 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; Gábor et al., 2018; Jain et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 234
    }, {
      "referenceID" : 7,
      "context" : ", 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; Gábor et al., 2018; Jain et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 234
    }, {
      "referenceID" : 13,
      "context" : ", 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; Gábor et al., 2018; Jain et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 234
    }, {
      "referenceID" : 1,
      "context" : "Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 200
    }, {
      "referenceID" : 21,
      "context" : "Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 200
    }, {
      "referenceID" : 25,
      "context" : "In this example, we see a paper describing a speech recognition system (Saon et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "An existing state-of-the-art model (Jain et al., 2020) mistakenly predicts the non-salient entity “ImageNet classification challenge” as salient due to the limited contextual information.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Most work on scientific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; Gábor et al., 2018), the SciERC dataset (Luan et al.",
      "startOffset" : 170,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "Most work on scientific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; Gábor et al., 2018), the SciERC dataset (Luan et al.",
      "startOffset" : 170,
      "endOffset" : 215
    }, {
      "referenceID" : 20,
      "context" : ", 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 28,
      "context" : ", 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "We base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 27,
      "context" : "Coreference This task requires clustering all entity mentions in a document such that, in each cluster, every mention refers to the same entity (Varkel and Globerson, 2020).",
      "startOffset" : 144,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "We follow the definition from the SciREX dataset (Jain et al., 2020), where an entity in a paper is deemed salient if it plays a role in the paper’s evaluation.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "First, contextualized word embeddings are obtained for each section by running SciBERT (Beltagy et al., 2019) on that section of text (up to 512 tokens).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Then, the embeddings from all words over all sections are passed through a bidirectional LSTM (Graves et al., 2005) to contextualize each word’s representation with those from other sections.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "Mention Identification The baseline model treats this named entity recognition task as an IOBES sequence tagging problem (Reimers and Gurevych, 2017).",
      "startOffset" : 121,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "The tagger takes the SciBERTBiLSTM (Beltagy et al., 2019; Graves et al., 2005) word embeddings (as shown in the Figure 2), feeds them through two feedforward networks (not shown in Figure 2), and produces tag potentials at each word.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "The tagger takes the SciBERTBiLSTM (Beltagy et al., 2019; Graves et al., 2005) word embeddings (as shown in the Figure 2), feeds them through two feedforward networks (not shown in Figure 2), and produces tag potentials at each word.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "These are then passed to a CRF (Lafferty et al., 2001) which predicts discrete tags.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Span Embeddings For a given mention span, its span embedding is produced via additive attention (Bahdanau et al., 2014) over the tokens in the span.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "4 Specifically, we combine the rich annotations of SciREX with a source of citation graph information, S2ORC (Lo et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "These errors are due to data issues in the S2ORC dataset, which relies on PDF parsers to extract information (Lo et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Prior works in scientific information extraction have predominantly used the citation graph only to analyze the content of citing papers, such as CiteTextRank (Das Gollapalli and Caragea, 2014) and Citation TF-IDF (Caragea et al., 2014), which is described in detail in §4.",
      "startOffset" : 214,
      "endOffset" : 236
    }, {
      "referenceID" : 23,
      "context" : "cial networks (Perozzi et al., 2014) and citation graphs (Sen et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "Network Representation Learning We learn representations for each node (paper) using DeepWalk6 (Perozzi et al., 2014) via the GraphVite library (Zhu et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 32,
      "context" : ", 2014) via the GraphVite library (Zhu et al., 2019), resulting in a 128dimensional “graph embedding” for each document in our dataset.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "Citation sentences, also known as “citances” (Nakov et al., 2004), provide an additional source of textual context about a paper.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 30,
      "context" : "They have seen use in automatic summarization (Yasunaga et al., 2019), but not in neural information extraction.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Citation TF-IDF (Caragea et al., 2014), is a feature representing the TF-IDF value (Jones, 1972) of a given token in its document’s citances.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : ", 2014), is a feature representing the TF-IDF value (Jones, 1972) of a given token in its document’s citances.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "For the 4-ary relation extraction task, we also compare against the DocTAET model (Hou et al., 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : ", 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al., 2020; Hou et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : ", 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al., 2020; Hou et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "Significance To improve the rigor of our evaluation, we run significance tests for each of our proposed methods against its associated baseline, via paired bootstrap sampling (Koehn, 2004).",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "We build our proposed CitationIE methods on top of the SciREX repository7 (Jain et al., 2020) in the AllenNLP framework (Gardner et al.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : ", 2020) in the AllenNLP framework (Gardner et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "Baseline (Jain et al., 2020) and Graph Embedding model evaluations are each trained with 3 different model seeds, then metrics averaged; rest are from single model due to computational limitations.",
      "startOffset" : 9,
      "endOffset" : 28
    } ],
    "year" : 2021,
    "abstractText" : "Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. Prior work has considered extracting documentlevel entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider extraction solely based on the content of an individual paper, without considering the paper’s place in the broader literature. In contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers. On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks. When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction. We release software tools to facilitate citation-aware SciIE development.1",
    "creator" : "LaTeX with hyperref"
  }
}