{
  "name" : "2021.acl-long.451.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Unified Generative Framework for Various NER Subtasks",
    "authors" : [ "Hang Yan", "Tao Gui", "Junqi Dai", "Qipeng Guo", "Zheng Zhang", "Xipeng Qiu" ],
    "emails" : [ "hyan19@fudan.edu.cn", "tgui16@fudan.edu.cn", "jqdai19@fudan.edu.cn", "qpguo16@fudan.edu.cn", "xpqiu@fudan.edu.cn", "zz@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5808–5822\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5808"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) has been a fundamental task of Natural Language Processing (NLP), and three kinds of NER subtasks have been recognized in previous work (Sang and Meulder, 2003; Pradhan et al., 2013a; Doddington et al., 2004; Kim et al., 2003; Karimi et al., 2015), including flat NER, nested NER, and discontinuous NER. As shown in Figure 1, the nested NER contains overlapping entities, and the entity in the discontinuous NER may contain several nonadjacent spans.\n∗Corresponding author. 1Code is available at https://github.com/yhcc/\nBARTNER.\nThe sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a). Inspired by sequence labelling’s success in the flat NER subtask, Metke-Jimenez and Karimi (2016); Muis and Lu (2017) tried to formulate the nested and discontinuous NER into the sequence labelling problem. For the nested and discontinuous NER subtasks, instead of assigning labels to each token directly, Xu et al. (2017); Wang and Lu (2019); Yu et al. (2020); Li et al. (2020b) tried to enumerate all possible spans and conduct the span-level classification. Another way to efficiently represent spans is to use the hypergraph (Lu\nand Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016).\nAlthough the sequence labelling formulation has dramatically advanced the NER task, it has to design different tagging schemas to fit various NER subtasks. One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Straková et al., 2019; Dai et al., 2020). While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020). Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018). Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017). Because the problems lie in different formulations, no publication has tested their model or framework in three NER subtasks simultaneously to the best of our knowledge.\nIn this paper, we propose using a novel and simple sequence-to-sequence (Seq2Seq) framework with the pointer mechanism (Vinyals et al., 2015) to generate the entity sequence directly. On the source side, the model inputs the sentence, and on the target side, the model generates the entity pointer index sequence. Since flat, continuous and discontinuous entities can all be represented as entity pointer index sequences, this formulation can tackle all the three kinds of NER subtasks in a unified way. Besides, this formulation can even solve the crossing structure entity3 and multi-type entity4. By converting the NER task into a Seq2Seq generation task, we can smoothly use the Seq2Seq pre-training model BART (Lewis et al., 2020) to enhance our model. To better utilize the pre-trained BART, we propose three kinds of entity representations to linearize entities into entity pointer index sequences.\nOur contribution can be summarized as follows: 2Attempts made for discontinuous constituent parsing may tackle three NER subtasks in one tagging schema (Vilares and Gómez-Rodrı́guez, 2020).\n3Namely, for span ABCD, both ABC and BCD are entities. Although this is rare, it exists (Dai et al., 2020).\n4An entity can have multiple entity types, as proteins can be annotated as drug/compound in the EPPI corpus (Alex et al., 2007).\n• We propose a novel and simple generative solution to solve the flat NER, nested NER, and discontinuous NER subtasks in a unified framework, in which NER subtasks are formulated as an entity span sequence generation problem.\n• We incorporate the pre-trained Seq2Seq model BART into our framework and exploit three kinds of entity representations to linearize entities into sequences. The results can shed some light on further exploration of BART into the entity sequence generation.\n• The proposed framework not only avoids the sophisticated design of tagging schema or span enumeration but also achieves SoTA or near SoTA performance on eight popular datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 NER Subtasks",
      "text" : "The term “Named Entity” was coined in the Sixth Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996). After that, the release of CoNLL-2003 NER dataset has greatly advanced the flat NER subtask (Sang and Meulder, 2003). Kim et al. (2003) found that in the field of molecular biology domain, some entities could be nested. Karimi et al. (2015) provided a corpus that contained medical forum posts on patient-reported Adverse Drug Events (ADEs), some entities recognized in this corpus may be discontinuous. Despite the difference between the three kinds of NER subtasks, the methods adopted by previous publications can be roughly divided into three types.\nToken-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al., 2001) or tag sequence generation methods can be used for decoding. Though the work of (Straková et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all\ntried to predict a tagging sequence. Therefore, they still need to design tagging schemas for different NER subtasks.\nSpan-level classification When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Straková et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020). Therefore, the second line of work directly conducted the span-level classification. The main difference between publications in this line of work is how to get the spans. Finkel and Manning (2009) regarded the parsing nodes as a span. Xu et al. (2017); Luan et al. (2019); Yamada et al. (2020); Li et al. (2020b); Yu et al. (2020); Wang et al. (2020a) tried to enumerate all spans. Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016).\nCombined token-level and span-level classification To avoid enumerating all possible spans and incorporate the entity boundary information into the model, Wang and Lu (2019); Zheng et al. (2019); Lin et al. (2019); Wang et al. (2020b); Luo and Zhao (2020) proposed combining the tokenlevel classification and span-level classification."
    }, {
      "heading" : "2.2 Sequence-to-Sequence Models",
      "text" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015). Gillick et al. (2016) proposed a Seq2Seq model to predict the entity’s start, span length and label for the NER task. Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020). We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al., 2019). And the sentencepiece tokenization used in T5 (Raffel et al., 2020) will cause different tokenizations for the same token, making it hard to generate pointer indexes to conduct the entity extraction.\nBART is formed by several transformer encoder\nand decoder layers, like the transformer model used in the machine translation (Vaswani et al., 2017). BART’s pre-training task is to recover corrupted text into the original text. BART uses the encoder to input the corrupted sentence and the decoder to recover the original sentence. BART has base and large versions. The base version has 6 encoder layers and 6 decoder layers, while the large version has 12. Therefore, the number of parameters is similar to its equivalently sized BERT 5."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "In this part, we first introduce the task formulation, then we describe how we use the Seq2Seq model with the pointer mechanism to generate the entity index sequences. After that, we present the detailed formulation of our model with BART."
    }, {
      "heading" : "3.1 NER Task",
      "text" : "The three kinds of NER tasks can all be formulated as follows, given an input sentence of n tokens X = [x1, x2, ..., xn], the target sequence is Y = [s11, e11, ..., s1j , e1j , t1, ..., si1, ei1, ..., sik, eik, ti], where s, e are the start and end index of a span, since an entity may contain one (for flat and nested NER) or more than one (for discontinuous NER) spans, each entity is represented as [si1, ei1, ..., sij , eij , ti], where ti is the entity tag index. We use G = [g1, ..., gl] to denote the entity tag tokens (such as “Person”, “Location”, etc.), where l is the number of entity tags. We make ti ∈ (n, n+ l], the n shift is to make sure ti is not confusing with pointer indexes (pointer indexes will be in range [1, n])."
    }, {
      "heading" : "3.2 Seq2Seq for Unified Decoding",
      "text" : "Since we formulate the NER task in a generative way, we can view the NER task as the following equation:\nP (Y |X) = m∏ t=1 P (yt|X,Y<t) (1)\nwhere y0 is the special “start of sentence” control token.\nWe use the Seq2Seq framework with the pointer mechanism to tackle this task. Therefore, our model consists of two components:\n5Because of the cross-attention between encoder and decoder, the number of parameters of BART is about 10% larger than its equivalently sized of BERT (Lewis et al., 2020).\n(1) Encoder encodes the input sentence X into vectors He, which formulates as follows:\nHe = Encoder(X) (2)\nwhere He ∈ Rn×d, and d is the hidden dimension. (2) Decoder is to get the index probability distribution for each step Pt = P (yt|X,Y<t). However, since Y<t contains the pointer and tag index, it cannot be directly inputted to the Decoder. We use the Index2Token conversion to convert indexes into tokens\nŷt = ® Xyt , if yt ≤ n, Gyt−n, if yt > n\n(3)\nAfter converting each yt this way, we can get the last hidden state hdt ∈ Rd with Ŷ<t = [ŷ1, ..., ŷt−1] as follows\nhdt = Decoder(H e; Ŷ<t) (4)\nThen, we can use the following equations to\nachieve the index probability distribution Pt\nEe = TokenEmbed(X) (5)\nĤe = MLP(He) (6)\nH̄e = α ∗ Ĥe + (1− α) ∗Ee (7) Gd = TokenEmbed(G) (8)\nPt = Softmax([H̄ e ⊗ hdt ;Gd ⊗ hdt ]) (9)\nwhere TokenEmbed is the embeddings shared between the Encoder and Decoder; Ee, Ĥe, H̄e ∈ Rn×d; α ∈ R is a hyper-parameter; Gd ∈ Rl×d; [ · ; · ] means concatenation in the first dimension; ⊗ means the dot product.\nDuring the training phase, we use the negative log-likelihood loss and the teacher forcing method. During the inference, we use an autoregressive manner to generate the target sequence. We use the decoding algorithm presented in Algorithm 1 to convert the index sequence into entity spans."
    }, {
      "heading" : "3.3 Detailed Entity Representation with BART",
      "text" : "Since our model is a Seq2Seq model, it is natural to utilize the pre-training Seq2Seq model BART to enhance our model. We present a visualization of\nAlgorithm 1 Decoding Algorithm to Convert the Entity Representation Sequence into Entity Spans\nInput: Target sequence Y = [y1, ..., ym] and yi ∈ [1, n+ |G|] Output: Entity spans E = {(e1, t1), ..., (ei, ti)} 1: E = {}, e = [], i = 1 2: while i <= m do 3: yi = Y [i] 4: if yi > n then 5: if len(e) > 0 then 6: E.add((e,Gyi−n)) 7: end if 8: e = [] 9: else\nour model based on BART in Figure 2. However, BART’s adoption is non-trivial because the BytePair-Encoding (BPE) tokenization used in BART might tokenize one token into several BPEs. To exploit how to use BART efficiently, we propose three kinds of pointer-based entity representations to locate entities in the original sentence unambiguously. The three entity representations are as follows:\nSpan The position index of the first BPE of the starting entity word and the last BPE of the ending\nentity word. If this entity includes multiple discontinuous spans of words, each span is represented in the same way.\nBPE The position indexes of all BPEs of the entity words.\nWord Only the position index of the first BPE of each entity word is used.\nFor all cases, we will append the entity tag to the entity representation. An example of the entity representations is presented in Figure 3. If a word does not belong to any entity, it will not appear in the target sequence. If a whole sentence has no entity, the prediction should be an empty sequence (only contains the “start of sentence” (<s>) token and the “end of sentence” (</s>) token )."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "To show that our proposed method can be used in various NER subtasks, we conducted experiments on eight datasets.\nFlat NER Datasets We adopt the CoNLL-2003 (Sang and Meulder, 2003) and the OntoNotes dataset 6 (Pradhan et al., 2013b). For CoNLL-2003, we follow Lample et al. (2016); Yu et al. (2020) to train our model on the concatenation of the train and development sets. For the OntoNotes dataset, we use the same train, development, test splits as Pradhan et al. (2012); Yu et al. (2020), and the New Testaments portion were excluded since there is no entity in this portion (Chiu and Nichols, 2016).\nNested NER Datasets We conduct experiments on ACE 20047 (Doddington et al., 2004), ACE 20058 (Walker and Consortium, 2005), Genia corpus (Kim et al., 2003). For ACE2004 and ACE2005, we use the same data split as Lu and Roth (2015); Muis and Lu (2017); Yu et al. (2020), the ratio between train, development and test set is 8:1:1. For Genia, we follow Wang et al. (2020b); Shibuya and Hovy (2020) to use five types of entities and split the train/dev/test as 8.1:0.9:1.0.\n6https://catalog.ldc.upenn.edu/ LDC2013T19\n7https://catalog.ldc.upenn.edu/ LDC2005T09\n8https://catalog.ldc.upenn.edu/ LDC2006T06\n9In the reported experiments, they included the document context. We rerun their code with only the sentence context. The lack of document context might cause performance degradation is also confirmed by the author himself in https://github.com/juntaoy/biaffine-ner/ issues/8#issuecomment-650813813.\nDiscontinuous NER Datasets We follow Dai et al. (2020) to use CADEC (Karimi et al., 2015), ShARe13 (Pradhan et al., 2013a) and ShARe14 (Mowery et al., 2014) corpus. Since only the Adverse Drug Events (ADEs) entities include discontinuous annotation, only these entities were considered (Dai et al., 2020; Metke-Jimenez and Karimi, 2016; Tang et al., 2018)."
    }, {
      "heading" : "4.2 Experiment Setup",
      "text" : "We use the BART-Large model, whose encoder and decoder each has 12 layers for all experiments, making it the same number of transformer layers as the BERT-Large and RoBERTa-Large model. We did not use any other embeddings, and the BART model is fine-tuned during the optimization. We put more detailed experimental settings in the Supplementary Material. We report the span-level F1."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Results on Flat NER",
      "text" : "Results are shown in Table 1. We do not compare with Yamada et al. (2020) since they added entity information during the pre-training process. Clark et al. (2018); Peters et al. (2018); Akbik et al. (2019); Straková et al. (2019) assigned a label to each token, and Li et al. (2020b); Yu et al. (2020) are based on span-level classifications, while our method is based on the entity sequence generation. And for both datasets, our method achieves better performance. We will discuss the performance difference between our three entity representations in Section 5.4."
    }, {
      "heading" : "5.2 Results on Nested NER",
      "text" : "Table 2 presents the results for the three nested NER datasets, and our proposed BART-based gen-\nerative models are comparable to the token-level classication (Straková et al., 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al., 2019; Li et al., 2020b; Wang et al., 2020a) models."
    }, {
      "heading" : "5.3 Results on Discontinuous NER",
      "text" : "Results in Table 3 show the comparison between our model and other models in three discontinuous NER datasets. Although Dai et al. (2020) tried to utilize BERT to enhance the model performance, they found that ELMo worked better. In all three datasets, our model achieves better performance."
    }, {
      "heading" : "5.4 Comparison Between Different Entity Representations",
      "text" : "In this part, we discuss the performance difference between the three entity representations. The “Word” entity representation achieves better performance almost in all datasets. And the comparison between the “Span” and “BPE” representations is more involved. To investigate the reason behind these results, we calculate the average and median length of entities when using different entity representations, and the results are presented in Table 4. It is clear that for a generative framework, the shorter the entity representation the better performance it should achieve. Therefore, as shown in Table 4, the “Word” representation with smaller\naverage entity length in CoNLL2003, OntoNotes, CADEC, ShARe13 achieves better performance in these datasets. However, although the average entity length of the “BPE” representation is longer than the “Span” representation, it achieves better performance in CoNLL2003, OntoNotes, ACE2004, ACE2005, this is because the “BPE” representation is more similar to the pre-training task, namely, predicting continuous BPEs. And we believe this task similarity is also the reason why the “Word” representation (Most of the words will be tokenized into a single BPE, making the “Word” representation still continuous.) achieves better performance than the “Span” representation in ACE2004, ACE2005, and ShARe14, although the former has longer entity length.\nA clear outlier is the Genia dataset, where the “Span” representation achieves better performance than the other two. We presume this is because in this dataset, a word will be tokenized into a longer BPE sequence (this can be inferred from the large entity length gap between the “Word” and “BPE” representation.) so that the “Word” representation will also be dissimilar to the pre-training tasks. For example, the protein “lipoxygenase isoforms” will be tokenized into the sequence “[‘Ġlip’, ‘oxy’, ‘gen’, ‘ase’, ‘Ġiso’, ‘forms’]”, which makes the target sequence of the “Word” representation be “[‘Ġlip’, ‘Ġiso’]”, resulting a discontiguous BPE\nsequence. Therefore, the shorter “Span” representation achieves better performance in this dataset."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Recall of Discontinuous Entities",
      "text" : "Since only about 10% of entities in the discontinuous NER datasets are discontinuous, only evaluating the whole dataset may not show our model can recognize the discontinuous entities. Therefore, like in Dai et al. (2020); Muis and Lu (2016) we report our model’s performance on the discontinuous entities in Table 6. As shown in Table 6, our model can predict the discontinuous named entities and achieve better performance."
    }, {
      "heading" : "6.2 Invalid Prediction",
      "text" : "In this part, we mainly focus on the analysis of the “Word” representation since it generally achieves better performance. We do not restrict the output distribution; therefore, the entity prediction may contain invalid predictions as show in Table 5, this\ntable shows that the BART model can learn the prediction representations quite well since, in most cases, the invalid prediction is less than 1%. We exclude all these invalid predictions during evaluation."
    }, {
      "heading" : "6.3 Entity Order Vs. Entity Recall",
      "text" : "Its appearance order in the sentence determines the entity order, and we want to study whether the entity that appears later in the target sequence will have worse recall than entities that appear early. The results are provided in Figure 4. The latter the entity appears, the larger probability that it can be recalled for the flat NER and discontinuous NER. While for the nested NER, the recall curve is quite involved. We assume this phenomenon is because, for the flat NER and discontinuous NER (more than 91.1% of entities are continuous) datasets, different entities have less dependence on each other. While in the nested NER dataset, entities in the latter position may be the outermost entity that contains the former entities. The wrong prediction of former entities may negatively influence the later entities."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we formulate NER subtasks as an entity span sequence generation problem, so that we can use a unified Seq2Seq model with the pointer mechanism to tackle flat, nested, and discontinuous NER subtasks. The Seq2Seq formulation en-\nables us to smoothly incorporate the pre-training Seq2Seq model BART to enhance the performance. To better utilize BART, we test three types of entity representation methods to linearize the entity span into sequences. Results show that the entity representation with a shorter length and more similar to continuous BPE sequences achieves better performance. Our proposed method achieves SoTA or near SoTA performance for eight different NER datasets, proving its generality to various NER subtasks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their insightful comments. The discussion with colleagues in AWS Shanghai AI Lab was quite fruitful. We also thank the developers of fastNLP10 and fitlog11. We thank Juntao Yu for helpful discussion about dataset processing. This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106700) and National Natural Science Foundation of China (No. 62022027).\nEthical Considerations\nFor the consideration of ethical concerns, we would make detailed description as following:\n(1) All of the experiments are conducted on existing datasets, which are derived from public scientific papers.\n(2) We describe the characteristics of the datasets in a specific section. Our analysis is consistent with the results.\n(3) Our work does not contain identity characteristics. It does not harm anyone.\n(4) Our experiments do not need a lots of computer resources compared to pre-trained models."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Hyper-parameters The detailed hyper-parameter used in different datasets are listed in Table 7. We use the slanted triangular learning rate warmup. All experiments are conducted in the Nvidia Ge-Force RTX-3090 Graphical Card with 24G graphical memory.\nA.2 Beam Search Since our framework is based on generation, we want to study whether using beam search will increase the performance, results are depicted in Figure 5, it shows the beam search almost has no effect on the model performance. The litte effect on the F1 value might be caused the the small searching space when generating.\nA.3 Efficiency Metrics In this section, we compare the memory footprint, training and inference time of our proposed model and BERT-based models. The experiments are conducted on the flat NER datasets, CoNLL-2003 (Sang and Meulder, 2003) and OntoNotes (Pradhan et al., 2012). We use the BERT-MLP and BERTCRF models as our baseline models. BERT-MLP and BERT-CRF are sequence labelling based models. For an input sentence X = [x1, ..., xn], both models use BERT (Devlin et al., 2019) to encode X as follows\nH = BERT(X) (10)\nwhere H ∈ Rn×d, d is the hidden state dimension. Then for the BERT-MLP model, it decodes the tags as follows\nF = Softmax(max(HWb + bb, 0)Wa + ba) (11)\nwhere Wa ∈ Rd×|T | and |T | is the number of tags, ba ∈ R|T |, Wb ∈ Rd×d, bb ∈ Rd, F ∈ Rn×|T | is the tag probability distribution. Then we use the negative log likelihood loss. And during the inference, for each token, the tag index with the largest probability is deemed as the prediction.\nFor the BERT-CRF model, we use the conditional random fields (CRF) (Lafferty et al., 2001) to decode tags. We assue the golden label sequence is Y = [y1, ..., yn], then we use the following equations to get the probability of Y\nM = max(HWb + bb, 0)Wa + ba (12)\nM = log softmax(M) (13) P (Y |X) = ∑n\ni=1 e M[i,yi]+T[yi−1,yi]∑Y(s) y′ ∑n i=1 e M[i,y′i]+T[y ′ i−1,y ′ i] ,\n(14)\nwhere M ∈ Rn×|T |, Y(s) is all valid label sequences, T ∈ R|T |×|T | is the transitation matrix, an entry (i, j) in T means the transition score from tag i to tag j. After getting the P (Y |X), we use negative log likelihood loss to optimize the model. Dur-\ning the inference, the Viterbi Algorithm is used to find the label sequence achieves the highest score.\nWe use the BERT-base version and BART-base version to calculate the memory footprint during training, seconds needed to iterate one epoch (one epoch means iterating over all training samples), and seconds needed to evaluate the development set. The batch size is 16 and 48 for training and evaluation, respectively. The comparison is presented in Table 8.\nDuring the training phase, we can use the casual mask to make the training of our model in parallel. Therefore, our proposed model can train faster than the BERT-CRF model, which needs sequential computation. While during the evaluating phase, we have to autoregressively generate tokens, which will make the inference slow. Therefore, further work like the usage of a non-autoregressive method can be studied to speed up the decoding (Gu et al., 2018)."
    } ],
    "references" : [ {
      "title" : "Pooled contextualized embeddings for named entity recognition",
      "author" : [ "Alan Akbik", "Tanja Bergmann", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Akbik et al\\.,? 2019",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2019
    }, {
      "title" : "Recognising nested named entities in biomedical text",
      "author" : [ "Beatrice Alex", "Barry Haddow", "Claire Grover." ],
      "venue" : "Biological, translational, and clinical language processing, BioNLP@ACL 2007, Prague, Czech Republic, June 29, 2007, pages 65–72. Asso-",
      "citeRegEx" : "Alex et al\\.,? 2007",
      "shortCiteRegEx" : "Alex et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning to progressively recognize new named entities with sequence to sequence models",
      "author" : [ "Lingzhen Chen", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa",
      "citeRegEx" : "Chen and Moschitti.,? 2018",
      "shortCiteRegEx" : "Chen and Moschitti.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason P.C. Chiu", "Eric Nichols." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Bel-",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "J. Mach. Learn. Res., 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Does syntax matter? A strong baseline for aspect-based sentiment analysis with roberta",
      "author" : [ "Junqi Dai", "Hang Yan", "Tianxiang Sun", "Pengfei Liu", "Xipeng Qiu." ],
      "venue" : "CoRR, abs/2104.04986.",
      "citeRegEx" : "Dai et al\\.,? 2021",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2021
    }, {
      "title" : "An effective transition-based model for discontinuous NER",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Ben Hachey", "Cécile Paris." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
      "citeRegEx" : "Dai et al\\.,? 2020",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The automatic content extraction (ACE) program - tasks, data, and evaluation",
      "author" : [ "George R. Doddington", "Alexis Mitchell", "Mark A. Przybocki", "Lance A. Ramshaw", "Stephanie M. Strassel", "Ralph M. Weischedel." ],
      "venue" : "Proceedings of the Fourth International",
      "citeRegEx" : "Doddington et al\\.,? 2004",
      "shortCiteRegEx" : "Doddington et al\\.",
      "year" : 2004
    }, {
      "title" : "Nested named entity recognition",
      "author" : [ "Jenny Rose Finkel", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, 6-7 August 2009, Singapore, A meeting of SIGDAT, a Special",
      "citeRegEx" : "Finkel and Manning.,? 2009",
      "shortCiteRegEx" : "Finkel and Manning.",
      "year" : 2009
    }, {
      "title" : "Merge and label: A novel neural network architecture for nested NER",
      "author" : [ "Joseph Fisher", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Vol-",
      "citeRegEx" : "Fisher and Vlachos.,? 2019",
      "shortCiteRegEx" : "Fisher and Vlachos.",
      "year" : 2019
    }, {
      "title" : "Multilingual language processing from bytes",
      "author" : [ "Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Gillick et al\\.,? 2016",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2016
    }, {
      "title" : "Message understanding conference- 6: A brief history",
      "author" : [ "Ralph Grishman", "Beth Sundheim." ],
      "venue" : "16th International Conference on Computational Linguistics, Proceedings of the Conference, COLING 1996, Center for Sprogteknologi, Copenhagen,",
      "citeRegEx" : "Grishman and Sundheim.,? 1996",
      "shortCiteRegEx" : "Grishman and Sundheim.",
      "year" : 1996
    }, {
      "title" : "Nonautoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "CoRR, abs/1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural layered model for nested named entity recognition",
      "author" : [ "Meizhi Ju", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Ju et al\\.,? 2018",
      "shortCiteRegEx" : "Ju et al\\.",
      "year" : 2018
    }, {
      "title" : "Cadec: A corpus of adverse drug event annotations",
      "author" : [ "Sarvnaz Karimi", "Alejandro Metke-Jimenez", "Madonna Kemp", "Chen Wang." ],
      "venue" : "J. Biomed. Informatics, 55:73–81.",
      "citeRegEx" : "Karimi et al\\.,? 2015",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2015
    }, {
      "title" : "Nested named entity recognition revisited",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018,",
      "citeRegEx" : "Katiyar and Cardie.,? 2018",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2018
    }, {
      "title" : "GENIA corpus - a semantically annotated corpus for bio-textmining",
      "author" : [ "Jin-Dong Kim", "Tomoko Ohta", "Yuka Tateisi", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the Eleventh International Conference on Intelligent Systems",
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "BART: denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "FLAT: chinese NER using flat-lattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "A unified MRC framework for named entity recognition",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, On-",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-nuggets: Nested entity mention detection via anchor-region networks",
      "author" : [ "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint mention extraction and classification with mention hypergraphs",
      "author" : [ "Wei Lu", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages",
      "citeRegEx" : "Lu and Roth.,? 2015",
      "shortCiteRegEx" : "Lu and Roth.",
      "year" : 2015
    }, {
      "title" : "A general framework for information extraction using dynamic span graphs",
      "author" : [ "Yi Luan", "Dave Wadden", "Luheng He", "Amy Shah", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Luan et al\\.,? 2019",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2019
    }, {
      "title" : "Bipartite flat-graph network for nested named entity recognition",
      "author" : [ "Ying Luo", "Hai Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6408–6418. Association",
      "citeRegEx" : "Luo and Zhao.,? 2020",
      "shortCiteRegEx" : "Luo and Zhao.",
      "year" : 2020
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portu-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons",
      "author" : [ "Andrew McCallum", "Wei Li." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in coop-",
      "citeRegEx" : "McCallum and Li.,? 2003",
      "shortCiteRegEx" : "McCallum and Li.",
      "year" : 2003
    }, {
      "title" : "Concept identification and normalisation for adverse drug event discovery in medical forums",
      "author" : [ "Alejandro Metke-Jimenez", "Sarvnaz Karimi." ],
      "venue" : "Proceedings of the First International Workshop on Biomedical Data Integration and Discovery (BMDID 2016)",
      "citeRegEx" : "Metke.Jimenez and Karimi.,? 2016",
      "shortCiteRegEx" : "Metke.Jimenez and Karimi.",
      "year" : 2016
    }, {
      "title" : "Task 2: Share/clef ehealth evaluation lab 2014",
      "author" : [ "Pradhan", "Guergana K. Savova", "Wendy W. Chapman." ],
      "venue" : "Working Notes for CLEF 2014 Conference, Sheffield, UK, September 15-18, 2014, volume 1180 of CEUR Workshop Proceedings, pages 31–42.",
      "citeRegEx" : "Pradhan et al\\.,? 2014",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to recognize discontiguous entities",
      "author" : [ "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 75–84. The Asso-",
      "citeRegEx" : "Muis and Lu.,? 2016",
      "shortCiteRegEx" : "Muis and Lu.",
      "year" : 2016
    }, {
      "title" : "Labeling gaps between words: Recognizing overlapping mentions with mention separators",
      "author" : [ "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Den-",
      "citeRegEx" : "Muis and Lu.,? 2017",
      "shortCiteRegEx" : "Muis and Lu.",
      "year" : 2017
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Task 1: Share/clef ehealth evaluation lab",
      "author" : [ "Sameer Pradhan", "Noémie Elhadad", "Brett R. South", "David Martı́nez", "Lee M. Christensen", "Amy Vogel", "Hanna Suominen", "Wendy W. Chapman", "Guergana K. Savova" ],
      "venue" : null,
      "citeRegEx" : "Pradhan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "Towards robust linguistic analysis using ontonotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Björkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong." ],
      "venue" : "Proceedings of the Seventeenth Conference on Compu-",
      "citeRegEx" : "Pradhan et al\\.,? 2013b",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang." ],
      "venue" : "Joint Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Pradhan et al\\.,? 2012",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2012
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "CoRR, abs/2003.08271.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev-Arie Ratinov", "Dan Roth." ],
      "venue" : "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL 2009, Boulder, Colorado, USA, June 4-5,",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooper-",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Nested named entity recognition via second-best sequence learning and decoding",
      "author" : [ "Takashi Shibuya", "Eduard H. Hovy." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:605–620.",
      "citeRegEx" : "Shibuya and Hovy.,? 2020",
      "shortCiteRegEx" : "Shibuya and Hovy.",
      "year" : 2020
    }, {
      "title" : "MASS: masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural architectures for nested NER through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Vol-",
      "citeRegEx" : "Straková et al\\.,? 2019",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Recognizing continuous and discontinuous adverse drug reaction mentions from social media using LSTM-CRF",
      "author" : [ "Buzhou Tang", "Jianglu Hu", "Xiaolong Wang", "Qingcai Chen." ],
      "venue" : "Wirel. Commun. Mob. Comput., 2018.",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Discontinuous constituent parsing as sequence labeling",
      "author" : [ "David Vilares", "Carlos Gómez-Rodrı́guez" ],
      "venue" : "In Proceedings of the 2020 Conference",
      "citeRegEx" : "Vilares and Gómez.Rodrı́guez.,? \\Q2020\\E",
      "shortCiteRegEx" : "Vilares and Gómez.Rodrı́guez.",
      "year" : 2020
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "ACE 2005 Multilingual Training Corpus",
      "author" : [ "C. Walker", "Linguistic Data Consortium." ],
      "venue" : "LDC corpora. Linguistic Data Consortium.",
      "citeRegEx" : "Walker and Consortium.,? 2005",
      "shortCiteRegEx" : "Walker and Consortium.",
      "year" : 2005
    }, {
      "title" : "Neural segmental hypergraphs for overlapping mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages",
      "citeRegEx" : "Wang and Lu.,? 2018",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2018
    }, {
      "title" : "Combining spans into entities: A neural two-stage approach for recognizing discontiguous entities",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wang and Lu.,? 2019",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2019
    }, {
      "title" : "Pyramid: A layered model for nested named entity recognition",
      "author" : [ "Jue Wang", "Lidan Shou", "Ke Chen", "Gang Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "HIT: nested named entity recognition via head-tail pair and token interaction",
      "author" : [ "Yu Wang", "Yun Li", "Hanghang Tong", "Ziye Zhu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, On-",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "SC-NER: A sequence-to-sequence model with sentence classification for named entity recognition",
      "author" : [ "Yu Wang", "Yun Li", "Ziye Zhu", "Bin Xia", "Zheng Liu." ],
      "venue" : "Advances in Knowledge Discovery and Data Mining - 23rd Pacific-Asia Conference,",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A local detection approach for named entity recognition and mention detection",
      "author" : [ "Mingbin Xu", "Hui Jiang", "Sedtawut Watcharawittayakul." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Van-",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "LUKE: deep contextualized entity representations with entityaware self-attention",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hiroyuki Shindo", "Hideaki Takeda", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "TENER: adapting transformer encoder for named entity recognition",
      "author" : [ "Hang Yan", "Bocao Deng", "Xiaonan Li", "Xipeng Qiu." ],
      "venue" : "CoRR, abs/1911.04474.",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "A graph-based model for joint chinese word segmentation and dependency parsing",
      "author" : [ "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:78–92.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Named entity recognition as dependency parsing",
      "author" : [ "Juntao Yu", "Bernd Bohnet", "Massimo Poesio." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6470–6476. Associa-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning tag dependencies for sequence tagging",
      "author" : [ "Yuan Zhang", "Hongshen Chen", "Yihong Zhao", "Qun Liu", "Dawei Yin." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stock-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "A boundary-aware neural model for nested named entity recognition",
      "author" : [ "Changmeng Zheng", "Yi Cai", "Jingyun Xu", "Ho-fung Leung", "Guandong Xu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "Named entity recognition (NER) has been a fundamental task of Natural Language Processing (NLP), and three kinds of NER subtasks have been recognized in previous work (Sang and Meulder, 2003; Pradhan et al., 2013a; Doddington et al., 2004; Kim et al., 2003; Karimi et al., 2015), including flat NER, nested NER, and discontinuous NER.",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 10,
      "context" : "Named entity recognition (NER) has been a fundamental task of Natural Language Processing (NLP), and three kinds of NER subtasks have been recognized in previous work (Sang and Meulder, 2003; Pradhan et al., 2013a; Doddington et al., 2004; Kim et al., 2003; Karimi et al., 2015), including flat NER, nested NER, and discontinuous NER.",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 20,
      "context" : "Named entity recognition (NER) has been a fundamental task of Natural Language Processing (NLP), and three kinds of NER subtasks have been recognized in previous work (Sang and Meulder, 2003; Pradhan et al., 2013a; Doddington et al., 2004; Kim et al., 2003; Karimi et al., 2015), including flat NER, nested NER, and discontinuous NER.",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 18,
      "context" : "Named entity recognition (NER) has been a fundamental task of Natural Language Processing (NLP), and three kinds of NER subtasks have been recognized in previous work (Sang and Meulder, 2003; Pradhan et al., 2013a; Doddington et al., 2004; Kim et al., 2003; Karimi et al., 2015), including flat NER, nested NER, and discontinuous NER.",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 31,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 6,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 16,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 3,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 22,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 46,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 60,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 24,
      "context" : "The sequence labelling formulation, which will assign a tag to each token in the sentence, has been widely used in the flat NER field (McCallum and Li, 2003; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Straková et al., 2019; Yan et al., 2019; Li et al., 2020a).",
      "startOffset" : 134,
      "endOffset" : 305
    }, {
      "referenceID" : 42,
      "context" : "One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Straková et al., 2019; Dai et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 159
    }, {
      "referenceID" : 32,
      "context" : "One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Straková et al., 2019; Dai et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 159
    }, {
      "referenceID" : 46,
      "context" : "One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Straková et al., 2019; Dai et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "One tagging schema can hardly fit for all three NER subtasks2 (Ratinov and Roth, 2009; Metke-Jimenez and Karimi, 2016; Straková et al., 2019; Dai et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 159
    }, {
      "referenceID" : 62,
      "context" : "While the span-based models need to enumerate all possible spans, which is quadratic to the length of the sentence and is almost impossible to enumerate in the discontinuous NER scenario (Yu et al., 2020).",
      "startOffset" : 187,
      "endOffset" : 204
    }, {
      "referenceID" : 58,
      "context" : "Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018).",
      "startOffset" : 69,
      "endOffset" : 124
    }, {
      "referenceID" : 28,
      "context" : "Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018).",
      "startOffset" : 69,
      "endOffset" : 124
    }, {
      "referenceID" : 53,
      "context" : "Therefore, span-based methods usually will set a maximum span length (Xu et al., 2017; Luan et al., 2019; Wang and Lu, 2018).",
      "startOffset" : 69,
      "endOffset" : 124
    }, {
      "referenceID" : 27,
      "context" : "Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017).",
      "startOffset" : 57,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017).",
      "startOffset" : 57,
      "endOffset" : 121
    }, {
      "referenceID" : 34,
      "context" : "Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017).",
      "startOffset" : 57,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : "Although hypergraphs can efficiently represent all spans (Lu and Roth, 2015; Katiyar and Cardie, 2018; Muis and Lu, 2016), it suffers from the spurious structure problem, and structural ambiguity issue during inference and the decoding is quite complicated (Muis and Lu, 2017).",
      "startOffset" : 257,
      "endOffset" : 276
    }, {
      "referenceID" : 51,
      "context" : "In this paper, we propose using a novel and simple sequence-to-sequence (Seq2Seq) framework with the pointer mechanism (Vinyals et al., 2015) to generate the entity sequence directly.",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "By converting the NER task into a Seq2Seq generation task, we can smoothly use the Seq2Seq pre-training model BART (Lewis et al., 2020) to enhance our model.",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 50,
      "context" : "Attempts made for discontinuous constituent parsing may tackle three NER subtasks in one tagging schema (Vilares and Gómez-Rodrı́guez, 2020).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "(4)An entity can have multiple entity types, as proteins can be annotated as drug/compound in the EPPI corpus (Alex et al., 2007).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "The term “Named Entity” was coined in the Sixth Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996).",
      "startOffset" : 89,
      "endOffset" : 118
    }, {
      "referenceID" : 43,
      "context" : "After that, the release of CoNLL-2003 NER dataset has greatly advanced the flat NER subtask (Sang and Meulder, 2003).",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 6,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 16,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 3,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 22,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 1,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 46,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 32,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 35,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 8,
      "context" : "Token-level classification The first line of work views the NER task as a token-level classification task, which assigns to each token a tag that usually comes from the Cartesian product between entity labels and the tag scheme, such as BIO and BILOU (Ratinov and Roth, 2009; Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Alex et al., 2007; Straková et al., 2019; Metke-Jimenez and Karimi, 2016; Muis and Lu, 2017; Dai et al., 2020), then Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 251,
      "endOffset" : 475
    }, {
      "referenceID" : 21,
      "context" : ", 2020), then Conditional Random Fields (CRF) (Lafferty et al., 2001) or tag sequence generation methods can be used for decoding.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 46,
      "context" : "Though the work of (Straková et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 57,
      "context" : "Though the work of (Straková et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 63,
      "context" : "Though the work of (Straková et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Though the work of (Straková et al., 2019; Wang et al., 2019; Zhang et al., 2018; Chen and Moschitti, 2018) are much like our method, they all",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 46,
      "context" : "Span-level classification When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Straková et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al.",
      "startOffset" : 147,
      "endOffset" : 202
    }, {
      "referenceID" : 32,
      "context" : "Span-level classification When applying the sequence labelling method to the nested NER and discontinous NER subtasks, the tagging will be complex (Straková et al., 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al.",
      "startOffset" : 147,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : ", 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020).",
      "startOffset" : 55,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : ", 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020).",
      "startOffset" : 55,
      "endOffset" : 122
    }, {
      "referenceID" : 44,
      "context" : ", 2019; Metke-Jimenez and Karimi, 2016) or multi-level (Ju et al., 2018; Fisher and Vlachos, 2019; Shibuya and Hovy, 2020).",
      "startOffset" : 55,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016).",
      "startOffset" : 186,
      "endOffset" : 250
    }, {
      "referenceID" : 53,
      "context" : "Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016).",
      "startOffset" : 186,
      "endOffset" : 250
    }, {
      "referenceID" : 34,
      "context" : "Following Lu and Roth (2015), hypergraph methods which can effectively represent exponentially many possible nested mentions in a sentence have been extensively studied in the NER tasks (Katiyar and Cardie, 2018; Wang and Lu, 2018; Muis and Lu, 2016).",
      "startOffset" : 186,
      "endOffset" : 250
    }, {
      "referenceID" : 47,
      "context" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 169
    }, {
      "referenceID" : 49,
      "context" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 169
    }, {
      "referenceID" : 51,
      "context" : "The Seq2Seq framework has been long studied and adopted in NLP (Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Vinyals et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 169
    }, {
      "referenceID" : 40,
      "context" : "Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al.",
      "startOffset" : 77,
      "endOffset" : 173
    }, {
      "referenceID" : 36,
      "context" : "Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al.",
      "startOffset" : 77,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al.",
      "startOffset" : 77,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al.",
      "startOffset" : 77,
      "endOffset" : 173
    }, {
      "referenceID" : 61,
      "context" : "Recently, the amazing performance gain achieved by PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Dai et al., 2021; Yan et al., 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al.",
      "startOffset" : 77,
      "endOffset" : 173
    }, {
      "referenceID" : 45,
      "context" : ", 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : ", 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 127
    }, {
      "referenceID" : 41,
      "context" : ", 2020) has attracted several attempts to pretrain a Seq2Seq model (Song et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "We mainly focus on the newly proposed BART (Lewis et al., 2020) model because it can achieve better performance than MASS (Song et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 45,
      "context" : ", 2020) model because it can achieve better performance than MASS (Song et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 41,
      "context" : "And the sentencepiece tokenization used in T5 (Raffel et al., 2020) will cause different tokenizations for the same token, making it hard to generate pointer indexes to conduct the entity extraction.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 49,
      "context" : "BART is formed by several transformer encoder and decoder layers, like the transformer model used in the machine translation (Vaswani et al., 2017).",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "Because of the cross-attention between encoder and decoder, the number of parameters of BART is about 10% larger than its equivalently sized of BERT (Lewis et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 43,
      "context" : "Flat NER Datasets We adopt the CoNLL-2003 (Sang and Meulder, 2003) and the OntoNotes dataset 6 (Pradhan et al.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 38,
      "context" : "Flat NER Datasets We adopt the CoNLL-2003 (Sang and Meulder, 2003) and the OntoNotes dataset 6 (Pradhan et al., 2013b).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "(2020), and the New Testaments portion were excluded since there is no entity in this portion (Chiu and Nichols, 2016).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Nested NER Datasets We conduct experiments on ACE 20047 (Doddington et al., 2004), ACE 20058 (Walker and Consortium, 2005), Genia corpus (Kim et al.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 52,
      "context" : ", 2004), ACE 20058 (Walker and Consortium, 2005), Genia corpus (Kim et al.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", 2004), ACE 20058 (Walker and Consortium, 2005), Genia corpus (Kim et al., 2003).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "(2020) to use CADEC (Karimi et al., 2015), ShARe13 (Pradhan et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "Since only the Adverse Drug Events (ADEs) entities include discontinuous annotation, only these entities were considered (Dai et al., 2020; Metke-Jimenez and Karimi, 2016; Tang et al., 2018).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 32,
      "context" : "Since only the Adverse Drug Events (ADEs) entities include discontinuous annotation, only these entities were considered (Dai et al., 2020; Metke-Jimenez and Karimi, 2016; Tang et al., 2018).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 48,
      "context" : "Since only the Adverse Drug Events (ADEs) entities include discontinuous annotation, only these entities were considered (Dai et al., 2020; Metke-Jimenez and Karimi, 2016; Tang et al., 2018).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 46,
      "context" : "erative models are comparable to the token-level classication (Straková et al., 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 44,
      "context" : "erative models are comparable to the token-level classication (Straková et al., 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : ", 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al., 2019; Li et al., 2020b; Wang et al., 2020a) models.",
      "startOffset" : 62,
      "endOffset" : 119
    }, {
      "referenceID" : 25,
      "context" : ", 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al., 2019; Li et al., 2020b; Wang et al., 2020a) models.",
      "startOffset" : 62,
      "endOffset" : 119
    }, {
      "referenceID" : 55,
      "context" : ", 2019; Shibuya and Hovy, 2020) and span-level classification (Luan et al., 2019; Li et al., 2020b; Wang et al., 2020a) models.",
      "startOffset" : 62,
      "endOffset" : 119
    } ],
    "year" : 2021,
    "abstractText" : "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-theart (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1.",
    "creator" : "LaTeX with hyperref"
  }
}