{
  "name" : "2021.acl-long.243.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
    "authors" : [ "Phillip Rust", "Jonas Pfeiffer", "Ivan Vulić", "Sebastian Ruder", "Iryna Gurevych" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3118–3135\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3118"
    }, {
      "heading" : "1 Introduction",
      "text" : "Following large transformer-based language models (LMs, Vaswani et al., 2017) pretrained on large English corpora (e.g., BERT, RoBERTa, T5; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), similar monolingual language models have been introduced for other languages (Virtanen et al., 2019;\n∗Both authors contributed equally to this work. †PR is now affiliated with the University of Copenhagen.\nOur code is available at https://github.com/Adapter-Hub/hgiyt.\nAntoun et al., 2020; Martin et al., 2020, inter alia), offering previously unmatched performance in all NLP tasks. Concurrently, massively multilingual models with the same architectures and training procedures, covering more than 100 languages, have been proposed (e.g., mBERT, XLM-R, mT5; Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021).\nThe “industry” of pretraining and releasing new monolingual BERT models continues its operations despite the fact that the corresponding languages are already covered by multilingual models. The common argument justifying the need for monolingual variants is the assumption that multilingual models—due to suffering from the so-called curse of multilinguality (Conneau et al., 2020, i.e., the lack of capacity to represent all languages in an equitable way)—underperform monolingual models when applied to monolingual tasks (Virtanen et al., 2019; Antoun et al., 2020; Rönnqvist et al., 2019, inter alia). However, little to no compelling empirical evidence with rigorous experiments and fair comparisons have been presented so far to support or invalidate this strong claim. In this regard, much of the work proposing and releasing new monolingual models is grounded in anecdotal evidence, pointing to the positive results reported for other monolingual BERT models (de Vries et al., 2019; Virtanen et al., 2019; Antoun et al., 2020).\nMonolingual BERT models are typically evaluated on downstream NLP tasks to demonstrate their effectiveness in comparison to previous monolingual models or mBERT (Virtanen et al., 2019; Antoun et al., 2020; Martin et al., 2020, inter alia). While these results do show that certain monolingual models can outperform mBERT in certain tasks, we hypothesize that this may substantially vary across different languages and language properties, tasks, pretrained models and their pretraining data, domain, and size. We further argue that\nconclusive evidence, either supporting or refuting the key hypothesis that monolingual models currently outperform multilingual models, necessitates an independent and controlled empirical comparison on a diverse set of languages and tasks.\nWhile recent work has argued and validated that mBERT is under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), providing evidence of improved performance when training monolingual models on more data, it is unclear if this is the only factor relevant for the performance of monolingual models. Another so far under-studied factor is the limited vocabulary size of multilingual models compared to the sum of tokens of all corresponding monolingual models. Our analyses investigating dedicated (i.e., language-specific) tokenizers reveal the importance of high-quality tokenizers for the performance of both model variants. We also shed light on the interplay of tokenization with other factors such as pretraining data size.\nContributions. 1) We systematically compare monolingual with multilingual pretrained language models for 9 typologically diverse languages on 5 structurally different tasks. 2) We train new monolingual models on equally sized datasets with different tokenizers (i.e., shared multilingual versus dedicated language-specific tokenizers) to disentangle the impact of pretraining data size from the vocabulary of the tokenizer. 3) We isolate factors that contribute to a performance difference (e.g., tokenizers’ “fertility”, the number of unseen (sub)words, data size) and provide an in-depth analysis of the impact of these factors on task performance. 4) Our results suggest that monolingually adapted tokenizers can robustly improve monolingual performance of multilingual models."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "Multilingual LMs. The widespread usage of pretrained multilingual Transformer-based LMs has been instigated by the release of multilingual BERT (Devlin et al., 2019), which followed on the success of the monolingual English BERT model. mBERT adopted the same pretraining regime as monolingual BERT by concatenating the 104 largest Wikipedias. Exponential smoothing was used when creating the subword vocabulary based on WordPieces (Wu et al., 2016) and a pretraining corpus. By oversampling underrepresented languages and undersampling overrepresented ones, it aims to counteract the imbalance of pretraining data sizes.\nThe final shared mBERT vocabulary comprises a total of 119,547 subword tokens.\nOther multilingual models followed mBERT, such as XLM-R (Conneau et al., 2020). Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).\nHowever, recent work has also pointed to some fundamental limitations of multilingual LMs. Conneau et al. (2020) observe that, for a fixed model capacity, adding new languages increases crosslingual performance up to a certain point, after which adding more languages results in performance drops. This phenomenon, termed the curse of multilinguality, can be attenuated by increasing the model capacity (Artetxe et al., 2020; Pfeiffer et al., 2020b; Chau et al., 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020). Another observation concerns substantially reduced crosslingual and monolingual abilities of the models for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020). Those languages remain underrepresented in the subword vocabulary and the model’s shared representation space despite oversampling. Despite recent efforts to mitigate this issue (e.g., Chung et al. (2020) propose to cluster and merge the vocabularies of similar languages, before defining a joint vocabulary across all languages), the multilingual LMs still struggle with balancing their parameters across many languages.\nMonolingual versus Multilingual LMs. New monolingual language-specific models also emerged for many languages, following BERT’s architecture and pretraining procedure. There are monolingual BERT variants for Arabic (Antoun et al., 2020), French (Martin et al., 2020), Finnish (Virtanen et al., 2019), Dutch (de Vries et al., 2019), to name only a few. Pyysalo et al. (2020) released 44 monolingual WikiBERT models trained on Wikipedia. However, only a few studies have thus far, either explicitly or implicitly, attempted to understand how monolingual and multilingual LMs compare across languages.\nNozza et al. (2020) extracted task results from the respective papers on monolingual BERTs to facilitate an overview of monolingual models and their comparison to mBERT.1 However, they have not verified the scores, nor have they performed a controlled impartial comparison.\nVulić et al. (2020) probed mBERT and monolingual BERT models across six typologically diverse languages for lexical semantics. They show that pretrained monolingual BERT models encode significantly more lexical information than mBERT.\nZhang et al. (2020) investigated the role of pretraining data size with RoBERTa, finding that the model learns most syntactic and semantic features on corpora spanning 10M–100M word tokens, but still requires massive datasets to learn higher-level semantic and commonsense knowledge.\nMulcaire et al. (2019) compared monolingual and bilingual ELMo (Peters et al., 2018) LMs across three downstream tasks, finding that contextualized representations from the bilingual models can improve monolingual task performance relative to their monolingual counterparts.2 However, it is unclear how their findings extend to massively multilingual LMs potentially suffering from the curse of multilinguality.\nRönnqvist et al. (2019) compared mBERT to monolingual BERT models for six languages (German, English, Swedish, Danish, Norwegian, Finnish) on three different tasks. They find that mBERT lags behind its monolingual counterparts in terms of performance on cloze and generation tasks. They also identified clear differences among the six languages in terms of this performance gap. They speculate that mBERT is under-trained with respect to individual languages. However, their set of tasks is limited, and their language sample is typologically narrow; it remains unclear whether these findings extend to different language families and to structurally different tasks.\nDespite recent efforts, a careful, systematic study within a controlled experimental setup, a diverse language sample and set of tasks is still lacking. We aim to address this gap in this work."
    }, {
      "heading" : "3 Controlled Experimental Setup",
      "text" : "We compare multilingual BERT with its monolingual counterparts in a spectrum of typologically\n1https://bertlang.unibocconi.it/ 2Mulcaire et al. (2019) clearly differentiate between multilingual and polyglot models. Their definition of polyglot models is in line with what we term multilingual models.\ndiverse languages and across a variety of downstream tasks. By isolating and analyzing crucial factors contributing to downstream performance, such as tokenizers and pretraining data, we can conduct unbiased and fair comparisons."
    }, {
      "heading" : "3.1 Language and Task Selection",
      "text" : "Our selection of languages has been guided by several (sometimes competing) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks.\nRegarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties.\nRegarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (Rönnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vulić et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3. We achieve task diversity and generalizability by selecting a combination of tasks driven by lower-level syntactic and higher-level semantic features (Lauscher et al., 2020).\nFinally, we select a set of 9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4\n3Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1.\n4Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4.\nNamed Entity Recognition (NER). We rely on: CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), FiNER (Ruokolainen et al., 2020), Chinese Literature (Xu et al., 2017), KMOU NER,6\nWikiAnn (Pan et al., 2017; Rahimi et al., 2019).\nSentiment Analysis (SA). We employ: HARD (Elnagar et al., 2018), IMDb Movie Reviews (Maas et al., 2011), Indonesian Prosa (Purwarianti and Crisdayanti, 2019), Yahoo Movie Reviews,7 NSMC,8 RuReviews (Smetanin and Komarov, 2019), Turkish Movie and Product Reviews (Demirtas and Pechenizkiy, 2013), ChnSentiCorp.9\nQuestion Answering (QA). We use: SQuADv1.1 (Rajpurkar et al., 2016), KorQuAD 1.0 (Lim et al., 2019), SberQuAD (Efimov et al., 2020), TQuAD,10\nDRCD (Shao et al., 2019), TyDiQA-GoldP (Clark et al., 2020).\nDependency Parsing (UDP). We rely on Universal Dependencies (Nivre et al., 2016, 2020) v2.6 (Zeman et al., 2020) for all languages.\nPart-of-Speech Tagging (POS). We again utilize Universal Dependencies v2.6."
    }, {
      "heading" : "3.2 Task-Based Fine-Tuning",
      "text" : "Fine-Tuning Setup. For all tasks besides UDP, we use the standard fine-tuning setup of Devlin et al. (2019). For UDP, we use a transformer-based variant (Glavaš and Vulić, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017). We distinguish between fully fine-tuning a monolingual BERT model and fully fine-tuning mBERT on the task. For both settings, we average scores over three random initializations on the development set. On the test set, we report\n5https://github.com/cl-tohoku/bert-japanese 6https://github.com/kmounlp/NER 7 https://github.com/dennybritz/sentiment-analysis 8 https://www.lucypark.kr/docs/2015-pyconkr/#39 9https://github.com/pengming617/bert classification\n10https://tquad.github.io/turkish-nlp-qa-dataset/"
    }, {
      "heading" : "AVG",
      "text" : "the results of the initialization that achieved the highest score on the development set.\nEvaluation Measures. We report F1 scores for NER, accuracy scores for SA and POS, unlabeled and labeled attachment scores (UAS & LAS) for UDP, and exact match and F1 scores for QA.\nHyper-Parameters and Technical Details. We use AdamW (Kingma and Ba, 2015) in all experiments, with a learning rate of 3e−5.11 We train for 10 epochs with early stopping (Prechelt, 1998).12\n11Preliminary experiments indicated this to be a well performing learning rate. Due to the large volume of our experiments, we were unable to tune all the hyper-parameters for each setting. We found that a higher learning rate of 5e− 4 works best for adapter-based fine-tuning (see later) since the task adapter parameters are learned from scratch (i.e., they are randomly initialized).\n12We evaluate a model every 500 gradient steps on the development set, saving the best-performing model based on the respective evaluation measures. We terminate training if no performance gains are observed within five consecutive evaluation runs (= 2,500 steps). For QA and UDP, we use the F1 scores and LAS, respectively. For FI and ID QA, we train for 20 epochs due to slower convergence. We train with batch size 32 and max sequence length 256 for all tasks except QA. In QA, the batch size is 24, max sequence length 384, query length 64, and document stride is set to 128."
    }, {
      "heading" : "3.3 Initial Results",
      "text" : "We report our first set of results in Table 2.13 We find that the performance gap between monolingual models and mBERT does exist to a large extent, confirming anecdotal evidence from prior work. However, we also notice that the score differences are largely dependent on the language and task at hand. The largest performance gains of monolingual models over mBERT are found for FI, TR, KO, and AR. In contrast, mBERT outperforms the IndoBERT (ID) model in all tasks except SA, and performs competitively with the JA and ZH monolingual models on most datasets. In general, the gap is particularly narrow for POS tagging, where all models tend to score high (in most cases north of 95% accuracy). ID aside, we also see a clear trend for UDP, with monolingual models outperforming fully fine-tuned mBERT models, most notably for FI and TR. In what follows, we seek to understand the causes of this behavior in relation to different factors such as tokenizers, corpora sizes, as well as languages and tasks in consideration."
    }, {
      "heading" : "4 Tokenizer versus Corpus Size",
      "text" : ""
    }, {
      "heading" : "4.1 Pretraining Corpus Size",
      "text" : "The size of the pretraining corpora plays an important role in the performance of transformers (Liu et al., 2019; Conneau et al., 2020; Zhang et al., 2020, inter alia). Therefore, we compare how much data each monolingual model was trained on with the amount of data in the respective language that mBERT has seen during training. Given that mBERT was trained on entire Wikipedia dumps, we estimate the latter by the total number of words across all articles listed for each Wiki.14 For the monolingual LMs, we extract information on pretraining data from the model documentation. If no exact numbers are explicitly stated, and the pretraining corpora are unavailable, we make estimations based on the information provided by the authors.15\nThe statistics are provided in Figure 1a. For EN, JA, RU, and ZH, both the respective monolingual BERT and mBERT were trained on similar amounts of monolingual data. On the other hand, monolingual BERTs of AR, ID, FI, KO, and TR were trained on about twice (KO) up to more than 40 times (TR) as much data in their language than mBERT.\n13See Appendix Table 8 for the results on development sets. 14Based on the numbers from\nhttps://meta.m.wikimedia.org/wiki/List of Wikipedias 15We provide further details in Appendix A.2."
    }, {
      "heading" : "4.2 Tokenizer",
      "text" : "Compared to monolingual models, mBERT is substantially more limited in terms of the parameter budget that it can allocate for each of its 104 languages in its vocabulary. In addition, monolingual tokenizers are typically trained by native-speaking experts who are aware of relevant linguistic phenomena exhibited by their target language. We thus inspect how this affects the tokenizations of monolingual data produced by our sample of monolingual models and mBERT. We tokenize examples from Universal Dependencies v2.6 treebanks and compute two metrics (Ács, 2019).16 First, the subword fertility measures the average number of subwords produced per tokenized word. A minimum fertility of 1 means that the tokenizer’s vocabulary contains every single word in the text. We plot the fertility scores in Figure 1b. We find that mBERT has similar fertility values as its monolingual counterparts for EN, ID, JA, and ZH. In contrast, mBERT has a much higher fertility for AR, FI, KO, RU, and TR, indicating that such languages are over-segmented. mBERT’s fertility is the lowest for EN; this is due to mBERT having seen the most data in this language during training, as well as English being morphologically poor in contrast to languages such as AR, FI, RU, or TR.17\nThe second metric we employ is the proportion of words where the tokenized word is continued across at least two sub-tokens (denoted by continuation symbols ##). Whereas the fertility is concerned with how aggressively a tokenizer splits, this metric measures how often it splits words. Intuitively, low scores are preferable for both metrics as they indicate that the tokenizer is well suited to the language. The plots in Figure 1c show similar trends as with the fertility statistic. In addition to AR, FI, KO, RU, and TR, which already displayed differences in fertility, mBERT also produces a proportion of continued words more than twice as high as the monolingual model for ID.18\n16We provide further details in Appendix A.3. 17The JA model is the only monolingual BERT with a fertility score higher than mBERT; its tokenizer is character-based and thus by design produces the maximum number of subwords.\n18We discuss additional tokenization statistics, further highlighting the differences (or lack thereof) between the individual monolingual tokenizers and the mBERT tokenizer, in Appendix B.1."
    }, {
      "heading" : "4.3 New Pretrained Models",
      "text" : "The differences in pretraining corpora and tokenizer statistics seem to align with the variations in downstream performance across languages. In particular, it appears that the performance gains of monolingual models over mBERT are larger for languages where the differences between the respective tokenizers and pretraining corpora sizes are also larger (AR, FI, KO, RU, TR vs. EN, JA, ZH).19 This implies that both the data size and the tokenizer are among the main driving forces of downstream task performance. To disentangle the effects of these two factors, we pretrain new models for AR, FI, ID, KO, and TR (the languages that exhibited the largest discrepancies in tokenization and pretraining data size) on Wikipedia data.\nWe train four model variants for each language. First, we train two new monolingual BERT models on the same data, one with the original monolingual tokenizer (MONOMODEL-MONOTOK) and one with the mBERT tokenizer (MONOMODEL-MBERTTOK).20\nSecond, similar to Artetxe et al. (2020), we retrain the embedding layer of mBERT, once with the respective monolingual tokenizer (MBERTMODELMONOTOK) and once with the mBERT tokenizer (MBERTMODEL-MBERTTOK). We freeze the transformer and only retrain the embedding weights, thus largely preserving mBERT’s multilinguality. The reason we retrain mBERT’s embedding layer with its own tokenizer is to further eliminate confounding factors when comparing to the version of mBERT with monolingually retrained embeddings. By comparing models\n19The only exception is ID, where the monolingual model has seen significantly more data and also scores lower on the tokenizer metrics, yet underperforms mBERT in most tasks. We suspect this exception is because IndoBERT is uncased, whereas the remaining models are cased.\n20The only exception is ID; instead of relying on the uncased IndoBERT tokenizer by Wilie et al. (2020), we introduce a new cased tokenizer with identical vocabulary size (30,521).\ntrained on the same amount of data, but with different tokenizers (MONOMODEL-MONOTOK vs."
    }, {
      "heading" : "MONOMODEL-MBERTTOK, MBERTMODEL-MBERTTOK",
      "text" : "vs. MBERTMODEL-MONOTOK), we disentangle the effect of the dataset size from the tokenizer, both with monolingual and multilingual LM variants.\nPretraining Setup. We pretrain new BERT models for each language on its respective Wikipedia dump.21 We apply two preprocessing steps to obtain clean data for pretraining. First, we use WikiExtractor (Attardi, 2015) to extract text passages from the raw dumps. Next, we follow Pyysalo et al. (2020) and utilize UDPipe (Straka et al., 2016) parsers pretrained on UD data to segment the extracted text passages into texts with document, sentence, and word boundaries.\nFollowing Liu et al. (2019); Wu and Dredze (2020), we only use the masked language modeling (MLM) objective and omit the next sentence prediction task. Besides that, we largely follow the default pretraining procedure by Devlin et al. (2019). We pretrain the new monolingual LMs (MONOMODEL-*) from scratch for 1M steps.22 We enable whole word masking (Devlin et al., 2019) for the FI monolingual models, following the pretraining procedure for FinBERT (Virtanen et al., 2019). For the retrained mBERT models (MBERTMODEL-*), we train for 250,000 steps following Artetxe et al. (2020).23 We freeze all parameters outside the embedding layer.24\nResults. We perform the same evaluations on downstream tasks for our new models as described\n21We use Wiki dumps from June 20, 2020 (e.g., fiwiki20200720-pages-articles.xml.bz2 for FI).\n22The batch size is 64; the sequence length is 128 for the first 900,000 steps, and 512 for the remaining 100,000 steps.\n23We train with batch size 64 and sequence length 512, otherwise using the same hyper-parameters as for the monolingual models.\n24For more details see Appendix A.5.\nLg Model NER SA QA UDP POS Test Test Dev Test Test F1 Acc EM / F1 UAS / LAS Acc\nAR\nMonolingual 91.1 95.9 68.3 / 82.4 90.1 / 85.6 96.8\nMONOMODEL-MONOTOK 91.7 95.6 67.7 / 81.6 89.2 / 84.4 96.6 MONOMODEL-MBERTTOK 90.0 95.5 64.1 / 79.4 88.8 / 84.0 97.0\nMBERTMODEL-MONOTOK 91.2 95.4 66.9 / 81.8 89.3 / 84.5 96.4 MBERTMODEL-MBERTTOK 89.7 95.6 66.3 / 80.7 89.1 / 84.2 96.8\nmBERT 90.0 95.4 66.1 / 80.6 88.8 / 83.8 96.8\nFI\nMonolingual 92.0 —– 69.9 / 81.6 95.9 / 94.4 98.4\nMONOMODEL-MONOTOK 89.1 —– 66.9 / 79.5 93.7 / 91.5 97.3 MONOMODEL-MBERTTOK 90.0 —– 65.1 / 77.0 93.6 / 91.5 97.0\nMBERTMODEL-MONOTOK 88.1 —– 66.4 / 78.3 92.4 / 89.6 96.6 MBERTMODEL-MBERTTOK 88.1 —– 65.9 / 77.3 92.2 / 89.4 96.7\nmBERT 88.2 —– 66.6 / 77.6 91.9 / 88.7 96.2\nID\nMonolingual 91.0 96.0 66.8 / 78.1 85.3 / 78.1 92.1\nMONOMODEL-MONOTOK 92.5 96.0 73.1 / 83.6 85.0 / 78.5 93.9 MONOMODEL-MBERTTOK 93.2 94.8 67.0 / 79.2 84.9 / 78.6 93.6\nMBERTMODEL-MONOTOK 93.9 94.6 74.1 / 83.8 86.4 / 80.2 93.8 MBERTMODEL-MBERTTOK 93.9 94.6 71.9 / 82.7 86.2 / 79.6 93.7\nmBERT 93.5 91.4 71.2 / 82.1 85.9 / 79.3 93.5\nKO\nMonolingual 88.8 89.7 74.2 / 91.1 90.3 / 87.2 97.0\nMONOMODEL-MONOTOK 87.1 88.8 72.8 / 90.3 89.8 / 86.6 96.7 MONOMODEL-MBERTTOK 85.8 87.2 68.9 / 88.7 88.9 / 85.6 96.4\nMBERTMODEL-MONOTOK 86.6 88.1 72.9 / 90.2 90.1 / 87.0 96.5 MBERTMODEL-MBERTTOK 86.2 86.6 69.3 / 89.3 89.2 / 85.9 96.2\nmBERT 86.6 86.7 69.7 / 89.5 89.2 / 85.7 96.0\nTR\nMonolingual 92.8 88.8 60.6 / 78.1 79.8 / 73.2 96.9\nMONOMODEL-MONOTOK 93.4 87.0 56.2 / 73.7 76.1 / 68.9 96.3 MONOMODEL-MBERTTOK 93.3 84.8 55.3 / 72.5 75.3 / 68.3 96.5\nMBERTMODEL-MONOTOK 93.7 85.3 59.4 / 76.7 77.1 / 70.2 96.3 MBERTMODEL-MBERTTOK 93.8 86.1 58.7 / 76.6 76.2 / 69.2 96.3\nmBERT 93.8 86.4 57.9 / 76.4 74.5 / 67.4 95.7\nAVG\nMonolingual 91.1 92.6 68.0 / 82.3 88.3 / 83.7 96.2\nMONOMODEL-MONOTOK 90.8 91.9 67.3 / 81.7 86.8 / 82.0 96.2 MONOMODEL-MBERTTOK 90.5 90.6 64.1 / 79.4 86.3 / 81.6 96.1\nin §3, and report the results in Table 3.25\nThe results indicate that the models trained with dedicated monolingual tokenizers outperform their counterparts with multilingual tokenizers in most tasks, with particular consistency for QA, UDP, and SA. In NER, the models trained with multilingual tokenizers score competitively or higher than the monolingual ones in half of the cases. Overall, the performance gap is the smallest for POS tagging (at most 0.4% accuracy). We observe the\n25Full results including development set scores are available in Table 9 of the Appendix.\nlargest gaps for QA (6.1 EM / 4.4 F1 in ID), SA (2.2% accuracy in TR), and NER (1.7 F1 in AR). Although the only language in which the monolingual counterpart always comes out on top is KO, the multilingual counterpart comes out on top at most 3/10 times (for AR and TR) in the other languages. The largest decrease in performance of a monolingual tokenizer relative to its multilingual counterpart is found for SA in TR (0.8% accuracy).\nOverall, we find that for 38 out of 48 task, model, and language combinations, the monolingual tokenizer outperforms the mBERT counterpart. We were able to improve the monolingual performance of the original mBERT for 20 out of 24 languages and tasks by only replacing the tokenizer and, thus, leveraging a specialized monolingual version. Similar to how the chosen method of tokenization affects neural machine translation quality (Domingo et al., 2019), these results establish that, in fact, the designated pretrained tokenizer plays a fundamental role in the monolingual downstream task performance of contemporary LMs.\nIn 18/24 language and task settings, the monolingual model from prior work (trained on more data) outperforms its corresponding MONOMODELMONOTOK model. 4/6 settings in which our MONOMODEL-MONOTOK model performs better are found for ID, where IndoBERT uses an uncased tokenizer and our model uses a cased one, which may affect the comparison. Expectedly, these results strongly indicate that data size plays a major role in downstream performance and corroborate prior research findings (Liu et al., 2019; Conneau et al., 2020; Zhang et al., 2020, inter alia)."
    }, {
      "heading" : "4.4 Adapter-Based Training",
      "text" : "Another way to provide more language-specific capacity to a multilingual LM beyond a dedicated tokenizer, thereby potentially making gains in monolingual downstream performance, is to introduce adapters (Pfeiffer et al., 2020b,c; Üstün et al., 2020), a small number of additional parameters at every layer of a pretrained model. To train adapters, usually all pretrained weights are frozen, while only the adapter weights are fine-tuned.26 The adapterbased approaches thus offer increased efficiency and modularity; it is crucial to verify to which extent our findings extend to the more efficient and\n26Pfeiffer et al. (2020b) propose to stack task-specific adapters on top of language adapters and extend this approach in Pfeiffer et al. (2020c) by additionally training new embeddings for the target language.\nLg Model NER SA QA UDP POS Test Test Dev Test Test F1 Acc EM / F1 UAS / LAS Acc\nAR mBERT 90.0 95.4 66.1 / 80.6 88.8 / 83.8 96.8 + ATask 89.6 95.6 66.7 / 81.1 87.8 / 82.6 96.8 + ATask + ALang 89.7 95.7 66.9 / 81.0 88.0 / 82.8 96.8 + ATask + ALang + MONOTOK 91.1 95.7 67.7 / 82.1 88.5 / 83.4 96.5\nFI mBERT 88.2 —– 66.6 / 77.6 91.9 / 88.7 96.2 + ATask 88.5 —– 65.2 / 77.3 90.8 / 87.0 95.7 + ATask + ALang 88.4 —– 65.7 / 77.1 91.8 / 88.5 96.6 + ATask + ALang + MONOTOK 88.1 —– 66.7 / 79.0 92.8 / 90.1 97.3\nID mBERT 93.5 91.4 71.2 / 82.1 85.9 / 79.3 93.5 + ATask 93.5 90.6 70.6 / 82.5 84.8 / 77.4 93.4 + ATask + ALang 93.5 93.6 70.8 / 82.2 85.4 / 78.1 93.4 + ATask + ALang + MONOTOK 93.4 93.8 74.4 / 84.4 85.1 / 78.3 93.5\nKO mBERT 86.6 86.7 69.7 / 89.5 89.2 / 85.7 96.0 + ATask 86.2 86.5 69.8 / 89.7 87.8 / 83.9 96.2 + ATask + ALang 86.2 86.3 70.0 / 89.8 88.3 / 84.3 96.2 + ATask + ALang + MONOTOK 86.5 87.9 73.1 / 90.4 88.9 / 85.2 96.5\nTR mBERT 93.8 86.4 57.9 / 76.4 74.5 / 67.4 95.7 + ATask 93.0 83.9 55.3 / 75.1 72.4 / 64.1 95.7 + ATask + ALang 93.5 84.8 56.9 / 75.8 73.0 / 64.7 95.9 + ATask + ALang + MONOTOK 92.7 85.3 60.0 / 77.0 75.7 / 68.1 96.3\nAVG mBERT 90.4 90.0 66.3 / 81.2 86.0 / 81.0 95.6 + ATask 90.2 89.2 65.5 / 81.1 84.7 / 79.0 95.6 + ATask + ALang 90.3 90.1 66.1 / 81.2 85.3 / 79.7 95.8 + ATask + ALang + MONOTOK 90.4 90.7 68.4 / 82.6 86.2 / 81.0 96.0\nTable 4: Performance on the different tasks leveraging mBERT with different adapter components (see §4.4).\nmore versatile adapter-based fine-tuning setup. We evaluate the impact of different adapter com-\nponents on the downstream task performance and their complementarity with monolingual tokenizers in Table 4.27 Here, +ATask and +ALang implies adding task- and language-adapters respectively, whereas +MONOTOK additionally includes a new embedding layer. As mentioned, we only fine-tune adapter weights on the downstream task, leveraging the adapter architecture proposed by Pfeiffer et al. (2021). For the +ATask +ALang setting we leverage pretrained language adapter weights available at AdapterHub.ml (Pfeiffer et al., 2020a). Language adapters are added to the model and frozen while only task adapters are trained on the target task. For the +ATask+ALang+ MONOTOK we train language adapters and new embeddings with the corresponding monolingual tokenizer equally as described in the previous section (e.g. MBERTMODELMONOTOK), task adapters are trained with a learning rate of 5e− 4 and 30 epochs with early stopping. Results. Similar to previous findings, adapters improve upon mBERT in 18/24 language, and task settings, 13 of which can be attributed to the improved MBERTMODEL-MONOTOK tokenizer. Figure 2 illustrates the average performance of the different adapter components in comparison to the monolingual models. We find that adapters with dedicated tokenizers reduce the performance gap con-\n27See Appendix Table 10 for the results on dev sets.\nsiderably without leveraging more training data, and even outperform the monolingual models in QA. This finding shows that adding additional language-specific capacity to existing multilingual LMs, which can be achieved with adapters in a portable and efficient way, is a viable alternative to monolingual pretraining."
    }, {
      "heading" : "5 Further Analysis",
      "text" : "At first glance, our results displayed in Table 2 seem to confirm the prevailing view that monolingual models are more effective than multilingual models (Rönnqvist et al., 2019; Antoun et al., 2020; de Vries et al., 2019, inter alia). However, the broad scope of our experiments reveals certain nuances that were previously undiscovered. Unlike prior work, which primarily attributes gaps in performance to mBERT being under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), our disentangled results (Table 3) suggest that a large portion of existing performance gaps can be attributed to the capability of the tokenizer.\nWith monolingual tokenizers with lower fertility and proportion-of-continued-words values than the mBERT tokenizer (such as for AR, FI, ID, KO, TR), consistent gains can be achieved, irrespective of whether the LMs are monolingual (the MONOMODEL-* comparison) or multilingual (a comparison of MBERTMODEL-* variants).\nWhenever the differences between monolingual models and mBERT with respect to the tokenizer properties and the pretraining corpus size are small (e.g., for EN, JA, and ZH), the performance gap is typically negligible. In QA, we even find mBERT to be favorable for these languages. Therefore, we conclude that monolingual models are not superior to multilingual ones per se, but gain advantage in direct comparisons by incorporating more pretraining data and using language-adapted tokenizers.\nCorrelation Analysis. To uncover additional patterns in our results (Tables 2, 3, 4), we perform a statistical analysis assessing the correlation between the individual factors (pretraining data size, subword fertility, proportion of continued words) and the downstream performance. Although our framework may not provide enough data points to be statistically representative, we argue that the correlation coefficient can still provide reasonable indications and reveal relations not immediately evident by looking at the tables.\nFigure 3 shows that both decreases in the proportion of continued words and the fertility correlate with an increase in downstream performance relative to fully fine-tuned mBERT across all tasks. The correlation is stronger for UDP and QA, where we find models with monolingual tokenizers to outperform their counterparts with the mBERT tokenizer consistently. The correlation is weaker for NER and POS tagging, which is also expected, considering the inconsistency of the results.28\nOverall, we find that the fertility and the proportion of continued words have a similar effect on the monolingual downstream performance as the corpus size for pretraining; This indicates that the tokenizer’s ability of representing a language plays a crucial role; Consequently, choosing a suboptimal tokenizer typically results in deteriorated downstream performance.\n28For further information, see Appendix B.2."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have conducted the first comprehensive empirical investigation concerning the monolingual performance of monolingual and multilingual language models (LMs). While our results support the existence of a performance gap in most but not all languages and tasks, further analyses revealed that the gaps are often substantially smaller than what was previously assumed. The gaps exist in certain languages due to the discrepancies in 1) pretraining data size, and 2) chosen tokenizers, and the level of their adaptation to the target language.\nFurther, we have disentangled the impact of pretrained corpora size from the influence of the tokenizers on the downstream task performance. We have trained new monolingual LMs on the same data, but with two different tokenizers; one being the dedicated tokenizer of the monolingual LM provided by native speakers; the other being the automatically generated multilingual mBERT tokenizer. We have found that for (almost) every task and language, the use of monolingual tokenizers outperforms the mBERT tokenizer.\nConsequently, in line with recent work by Chung et al. (2020), our results suggest that investing more effort into 1) improving the balance of individual languages’ representations in the vocabulary of multilingual LMs, and 2) providing languagespecific adaptations and extensions of multilingual tokenizers (Pfeiffer et al., 2020c) can reduce the gap between monolingual and multilingual LMs. Another promising future research direction is completely disposing of any (language-specific or multilingual) tokenizers during pretraining (Clark et al., 2021).\nOur code, pretrained models, and adapters are available at https://github.com/Adapter-Hub/hgiyt."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Jonas Pfeiffer is supported by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. The work of Ivan Vulić is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909).\nWe thank Nils Reimers, Prasetya Ajie Utama, and Adhiguna Kuncoro for insightful feedback and suggestions on a draft of this paper."
    }, {
      "heading" : "A Reproducibility",
      "text" : ""
    }, {
      "heading" : "A.1 Pretrained Models",
      "text" : "All of the pretrained language models we use are available on the HuggingFace model hub29 and compatible with the HuggingFace transformers Python library (Wolf et al., 2020). Table 5 displays the model hub identifiers of our selected models."
    }, {
      "heading" : "A.2 Estimating the Pretraining Corpora Sizes",
      "text" : "Since mBERT was pretrained on the entire Wikipedia dumps of all languages it covers (Devlin et al., 2019), we estimate the language-specific shares of the mBERT pretraining corpus by word counts of the respective raw Wikipedia dumps, according to numbers obtained from Wikimedia30: 327M words for AR, 3.7B for EN, 134M for FI, 142M for ID, 1.1B for JA, 125M for KO, 781M for RU, 104M for TR, 482M for ZH.31 Devlin et al. (2019) only included text passages from the articles, and used older Wikipedia dumps, so these numbers should serve as upper limits, yet be reasonably accurate. For the monolingual models, we rely on information provided by the authors.32"
    }, {
      "heading" : "A.3 Data for Tokenizer Analyses",
      "text" : "We tokenize the training and development splits of the UD (Nivre et al., 2016, 2020) v2.6 (Zeman et al., 2020) treebanks listed in Table 6."
    }, {
      "heading" : "A.4 Fine-Tuning Datasets",
      "text" : "We list the datasets we used, including the number of examples per dataset split, in the Table 7."
    }, {
      "heading" : "A.5 Training Procedure of New Models",
      "text" : "We pretrain our models on single Nvidia Tesla V100, A100, and Titan RTX GPUs with 32GB, 40GB, and 24GB of video memory, respectively. To support larger batch sizes, we train in mixedprecision (fp16) mode. Following Wu and Dredze (2020), we only use masked language modeling (MLM) as pretraining objective and omit the next sentence prediction task as Liu et al. (2019) find it does not yield performance gains. We otherwise\n29https://huggingface.co/models 30https://meta.m.wikimedia.org/wiki/List of Wikipedias 31We obtained the numbers for ID and TR on Dec 10, 2020 and for the remaining languages on Sep 10, 2020. 32For JA, RU, and ZH, the authors do not provide exact word counts. Therefore, we estimate them using other provided information (RU, ZH) or scripts for training corpus reconstruction (JA).\nmostly follow the default pretraining procedure by Devlin et al. (2019). We pretrain the new monolingual models (MONOMODEL-*) from scratch for 1M steps with batch size 64. We choose a sequence length of 128 for the first 900,000 steps and 512 for the remaining 100,000 steps. In both phases, we warm up the learning rate to 1e − 4 over the first 10,000 steps, then decay linearly. We use the Adam optimizer with weight decay (AdamW) (Loshchilov and Hutter, 2019) with default hyper-parameters and a weight decay of 0.01. We enable whole word masking (Devlin et al., 2019) for the FI monolingual models, following the pretraining procedure for FinBERT (Virtanen et al., 2019). To lower computational requirements for the monolingual models with mBERT tokenizers, we remove all tokens from mBERT’s vocabulary that do not appear in the pretraining data. We, thereby, obtain vocabularies of size 78,193 (AR), 60,827 (FI), 72,787 (ID), 66,268 (KO), and 71,007 (TR), which for all languages reduces the number of parameters in the embedding layer significantly, compared to the 119,547 word piece vocabulary of mBERT. For the retrained mBERT models (i.e., MBERTMODEL-*), we run MLM for 250,000 steps (similar to Artetxe et al. (2020)) with batch size 64 and sequence length 512, otherwise using the same hyper-parameters as for the monolingual models. In order to retrain the embedding layer, we first resize it to match the vocabulary of the respective tokenizer. For the MBERTMODELMBERTTOK models, we use the mBERT tokenizers with reduced vocabulary as outlined above. We initialize the positional embeddings, segment embeddings, and embeddings of special tokens ([CLS], [SEP], [PAD], [UNK], [MASK]) from mBERT, and reinitialize the remaining embeddings randomly. We freeze all parameters outside the embedding layer. For all pretraining runs, we set the random seed to 42."
    }, {
      "heading" : "A.6 Code",
      "text" : "Our code with usage instructions for finetuning, pretraining, data preprocessing, and calculating the tokenizer statistics is available at https://github.com/Adapter-Hub/hgiyt. The repository also contains further links to a collection of our new pretrained models and language adapters."
    }, {
      "heading" : "B Further Analyses and Discussions",
      "text" : ""
    }, {
      "heading" : "B.1 Tokenization Analysis",
      "text" : "In our tokenization analysis in §4.2 of the main text, we only include the fertility and the proportion of continued words as they are sufficient to illustrate and quantify the differences between tokenizers. In support of the findings in §4.2 and for completeness, we provide additional tokenization statistics here.\nFor each tokenizer, Table 5 lists the respective vocabulary size and the proportion of its vocabulary also contained in mBERT. It shows that the tokenizers scoring lower in fertility (and accordingly performing better) than mBERT are often not adequately covered by mBERT’s vocabulary. For instance, only 5.6% of the AraBERT (AR) vocabulary is covered by mBERT.\nFigure 4 compares the proportion of unknown tokens ([UNK]) in the tokenized data. It shows that the proportion is generally extremely low, i.e., the tokenizers can typically split unknown words into known subwords.\nSimilar to the work by Ács (2019), Figure 5 compares the tokenizations produced by the monolingual models and mBERT with the reference tokenizations provided by the human dataset annotators with respect to their sentence lengths. We find that the tokenizers scoring low in fertility and the proportion of continued words typically exhibit sentence length distributions much closer to the reference tokenizations by human UD annotators, indicating they are more capable than the mBERT tokenizer. Likewise, the monolingual models’ and mBERT’s sentence length distributions are closer for languages with similar fertility and proportion of continued words, such as EN, JA, and ZH."
    }, {
      "heading" : "B.2 Correlation Analysis",
      "text" : "To uncover some of the hidden patterns in our results (Tables 2, 3, 4), we perform a statistical analysis assessing the correlation between the individual factors (pretraining data size, subword fertility, proportion of continued words) and the downstream performance.\nFigure 6b shows that both decreases in the proportion of continued words and the fertility correlate with an increase in downstream performance relative to fully fine-tuned mBERT across all tasks. The correlation is stronger for UDP and QA, where we found models with monolingual tokenizers to outperform their counterparts with the mBERT to-\nkenizer consistently. The correlation is weaker for NER and POS tagging, which is also expected, considering the inconsistency of the results.\nSomewhat surprisingly, the tokenizer metrics seem to be more indicative of high downstream performance than the size of the pretraining corpus. We believe that this in parts due to the overall poor performance of the uncased IndoBERT model, which we (in this case unfairly) compare to our cased ID-MONOMODEL-MONOTOK model. Therefore, we plot the same correlation matrix excluding ID in Figure 3.\nCompared to Figure 6b, the overall correlations for the proportion of continued words and the fertility remain mostly unaffected. In contrast, the correlation for the pretraining corpus size becomes much stronger, confirming that the subpar performance of IndoBERT is indeed an outlier in this scenario. Leaving out Indonesian also strengthens the indication that the performance in POS tagging correlates more with the data size than with the tokenizer, although we argue that this indication may be misleading. The performance gap is generally very minor in POS tagging. Therefore, the Spearman correlation coefficient, which only takes the rank into account, but not the absolute score differences, is particularly sensitive to changes in POS tagging performance.\nFinally, we plot the correlation between the three metrics and the downstream performance under consideration of all languages and models, including the adapter-based fine-tuning settings, to gain an understanding of how pronounced their effects are in a more “noisy” setting.\nAs Figure 6a shows, the three factors still correlate with the downstream performance in a similar manner even when not isolated. This correlation tells us that even when there may be other factors that could have an influence, these three factors are still highly indicative of the downstream performance.\nWe also see that the correlation coefficients for the proportion of continued words and the fertility are nearly identical, which is expected based on the visual similarity of the respective plots (seen in Figures 1b and 1c)."
    }, {
      "heading" : "C Full Results",
      "text" : "For compactness, we have only reported the performance of our models on the respective test datasets in the main text.33 For completeness, we also include the full tables, including development (dev) dataset performance averaged over three random initializations, as described in §3. Table 8 shows the full results corresponding to Table 2 (initial results), Table 9 shows the full results corresponding to Table 3 (results for our new models), and Table 10 shows the full results corresponding to Table 4 (adapter-based training)."
    }, {
      "heading" : "Lang Treebank # Words",
      "text" : "33Except for QA, where we do not use any test data\nAll NER POS QA SA UDP\nTask\nCont. Proportion\nFertility\nPre-Train Size\nM et\nri c\n0.39 0.28 0.30 0.61 0.41 0.48\n0.42 0.31 0.34 0.64 0.40 0.52\n0.20 0.20 0.13 0.32 0.21 0.23 −1\n0\n1\n(a) We consider all languages and models.\nAll NER POS QA SA UDP\nTask\nCont. Proportion\nFertility\nPre-Train Size\nM et\nri c\n0.34 0.20 0.18 0.67 0.32 0.45\n0.35 0.23 0.20 0.65 0.33 0.48\n0.23 0.06 0.27 0.34 0.31 0.43 −1\n0\n1\n(b) For the proportion of continued words and the fertility, we consider fully fine-tuned mBERT, the MONOMODEL-* models, and the MBERTMODEL-* models. For the pretraining corpus size, we consider the original monolingual models and the MONOMODEL-MONOTOK models.\nAR Monolingual 91.5 91.1 96.1 95.9 68.3 / 82.4 89.4 / 85.0 90.1 / 85.6 97.5 96.8 mBERT 90.3 90.0 95.8 95.4 66.1 / 80.6 87.8 / 83.0 88.8 / 83.8 97.2 96.8 EN Monolingual 95.4 91.5 91.6 91.6 80.5 / 88.0 92.6 / 90.3 92.1 / 89.7 97.1 97.0 mBERT 95.7 91.2 90.1 89.8 80.9 / 88.4 92.1 / 89.6 91.6 / 89.1 97.0 96.9 FI Monolingual 93.3 92.0 —– —– 69.9 / 81.6 95.7 / 93.9 95.9 / 94.4 98.1 98.4 mBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2 ID Monolingual 90.9 91.0 94.6 96.0 66.8 / 78.1 84.5 / 77.4 85.3 / 78.1 92.0 92.1 mBERT 93.7 93.5 93.1 91.4 71.2 / 82.1 85.0 / 78.4 85.9 / 79.3 93.3 93.5 JA Monolingual 72.1 72.4 88.7 88.0 —– / —– 96.0 / 94.7 94.7 / 93.0 98.3 98.1 mBERT 73.4 73.4 88.8 87.8 —– / —– 95.5 / 94.2 94.0 / 92.3 98.1 97.8 KO Monolingual 88.6 88.8 89.8 89.7 74.2 / 91.1 88.5 / 85.0 90.3 / 87.2 96.4 97.0 mBERT 87.3 86.6 86.7 86.7 69.7 / 89.5 86.9 / 83.2 89.2 / 85.7 95.8 96.0 RU Monolingual 91.9 91.0 95.2 95.2 64.3 / 83.7 92.4 / 90.1 93.1 / 89.9 98.6 98.4 mBERT 90.2 90.0 95.2 95.0 63.3 / 82.6 91.5 / 88.8 91.9 / 88.5 98.4 98.2 TR Monolingual 93.1 92.8 89.3 88.8 60.6 / 78.1 78.0 / 70.9 79.8 / 73.2 97.0 96.9 mBERT 93.7 93.8 86.4 86.4 57.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7 ZH Monolingual 77.0 76.5 94.8 95.3 82.3 / 89.3 88.1 / 84.9 88.6 / 85.6 96.6 97.2 mBERT 76.0 76.1 93.1 93.8 82.0 / 89.3 87.1 / 83.7 88.1 / 85.0 96.1 96.7"
    }, {
      "heading" : "AVG",
      "text" : "Lg Model NER SA QA UDP POS\nDev Test Dev Test Dev Dev Test Dev Test F1 F1 Acc Acc EM / F1 UAS / LAS UAS / LAS Acc Acc\nAR\nMonolingual 91.5 91.1 96.1 95.9 68.3 / 82.4 89.4 / 85.0 90.1 / 85.6 97.5 96.8 MONOMODEL-MONOTOK 88.6 91.7 96.0 95.6 67.7 / 81.6 88.4 / 83.7 89.2 / 84.4 97.3 96.6 MONOMODEL-MBERTTOK 90.1 90.0 95.9 95.5 64.1 / 79.4 87.8 / 83.2 88.8 / 84.0 97.4 97.0 MBERTMODEL-MONOTOK 91.9 91.2 95.9 95.4 66.9 / 81.8 88.2 / 83.5 89.3 / 84.5 97.2 96.4 MBERTMODEL-MBERTTOK 90.0 89.7 95.8 95.6 66.3 / 80.7 87.8 / 83.0 89.1 / 84.2 97.3 96.8 mBERT 90.3 90.0 95.8 95.4 66.1 / 80.6 87.8 / 83.0 88.8 / 83.8 97.2 96.8\nFI\nMonolingual 93.3 92.0 —– —– 69.9 / 81.6 95.7 / 93.9 95.9 / 94.4 98.1 98.4 MONOMODEL-MONOTOK 91.9 89.1 —– —– 66.9 / 79.5 93.6 / 91.0 93.7 / 91.5 97.0 97.3 MONOMODEL-MBERTTOK 91.8 90.0 —– —– 65.1 / 77.0 93.1 / 90.6 93.6 / 91.5 96.2 97.0 MBERTMODEL-MONOTOK 91.0 88.1 —– —– 66.4 / 78.3 92.2 / 89.3 92.4 / 89.6 96.3 96.6 MBERTMODEL-MBERTTOK 92.0 88.1 —– —– 65.9 / 77.3 92.1 / 89.2 92.2 / 89.4 96.6 96.7 mBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2\nID\nMonolingual 90.9 91.0 94.6 96.0 66.8 / 78.1 84.5 / 77.4 85.3 / 78.1 92.0 92.1 MONOMODEL-MONOTOK 93.0 92.5 93.9 96.0 73.1 / 83.6 83.4 / 76.8 85.0 / 78.5 93.6 93.9 MONOMODEL-MBERTTOK 93.3 93.2 93.9 94.8 67.0 / 79.2 84.0 / 77.4 84.9 / 78.6 93.4 93.6 MBERTMODEL-MONOTOK 93.8 93.9 94.4 94.6 74.1 / 83.8 85.5 / 78.8 86.4 / 80.2 93.5 93.8 MBERTMODEL-MBERTTOK 93.9 93.9 93.7 94.6 71.9 / 82.7 85.3 / 78.6 86.2 / 79.6 93.4 93.7 mBERT 93.7 93.5 93.1 91.4 71.2 / 82.1 85.0 / 78.4 85.9 / 79.3 93.3 93.5\nKO\nMonolingual 88.6 88.8 89.8 89.7 74.2 / 91.1 88.5 / 85.0 90.3 / 87.2 96.4 97.0 MONOMODEL-MONOTOK 87.9 87.1 89.0 88.8 72.8 / 90.3 87.9 / 84.2 89.8 / 86.6 96.4 96.7 MONOMODEL-MBERTTOK 86.9 85.8 87.3 87.2 68.9 / 88.7 86.9 / 83.2 88.9 / 85.6 96.1 96.4 MBERTMODEL-MONOTOK 87.9 86.6 88.2 88.1 72.9 / 90.2 87.9 / 83.9 90.1 / 87.0 96.2 96.5 MBERTMODEL-MBERTTOK 86.7 86.2 86.6 86.6 69.3 / 89.3 87.2 / 83.3 89.2 / 85.9 95.9 96.2 mBERT 87.3 86.6 86.7 86.7 69.7 / 89.5 86.9 / 83.2 89.2 / 85.7 95.8 96.0\nTR\nMonolingual 93.1 92.8 89.3 88.8 60.6 / 78.1 78.0 / 70.9 79.8 / 73.2 97.0 96.9 MONOMODEL-MONOTOK 93.5 93.4 87.5 87.0 56.2 / 73.7 74.4 / 67.3 76.1 / 68.9 95.9 96.3 MONOMODEL-MBERTTOK 93.2 93.3 85.8 84.8 55.3 / 72.5 73.2 / 66.0 75.3 / 68.3 96.4 96.5 MBERTMODEL-MONOTOK 93.5 93.7 86.1 85.3 59.4 / 76.7 74.7 / 67.6 77.1 / 70.2 96.1 96.3 MBERTMODEL-MBERTTOK 93.9 93.8 86.0 86.1 58.7 / 76.6 73.2 / 66.1 76.2 / 69.2 95.9 96.3 mBERT 93.7 93.8 86.4 86.4 57.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7\nAVG\nMonolingual 91.5 91.1 92.5 92.6 68.0 / 82.3 87.2 / 82.4 88.3 / 83.7 96.2 96.2 MONOMODEL-MONOTOK 91.0 90.8 91.6 91.9 67.3 / 81.7 85.5 / 80.6 86.8 / 82.0 96.0 96.2 MONOMODEL-MBERTTOK 91.1 90.5 90.7 90.6 64.1 / 79.4 85.0 / 80.1 86.3 / 81.6 95.9 96.1 MBERTMODEL-MONOTOK 91.6 90.7 91.2 90.9 68.0 / 82.2 85.7 / 80.6 87.1 / 82.3 95.9 95.9 MBERTMODEL-MBERTTOK 91.3 90.3 90.5 90.7 66.4 / 81.3 85.1 / 80.0 86.6 / 81.7 95.8 95.9 mBERT 91.2 90.4 90.5 90.0 66.3 / 81.2 84.7 / 79.6 86.1 / 81.0 95.6 95.6\nAR mBERT 90.3 90.0 95.8 95.4 66.1 / 80.6 87.8 / 83.0 88.8 / 83.8 97.2 96.8 + ATask 90.0 89.6 96.1 95.6 66.7 / 81.1 86.7 / 81.6 87.8 / 82.6 97.3 96.8 + ATask + ALang 90.2 89.7 96.1 95.7 66.9 / 81.0 87.0 / 81.9 88.0 / 82.8 97.3 96.8 + ATask + ALang + MONOTOK 91.5 91.1 96.0 95.7 67.7 / 82.1 87.7 / 82.8 88.5 / 83.4 97.3 96.5\nFI mBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2 + ATask 91.2 88.5 —– —– 65.2 / 77.3 90.2 / 86.3 90.8 / 87.0 95.8 95.7 + ATask + ALang 91.6 88.4 —– —– 65.7 / 77.1 91.1 / 87.7 91.8 / 88.5 96.3 96.6 + ATask + ALang + MONOTOK 90.8 88.1 —– —– 66.7 / 79.0 92.8 / 89.9 92.8 / 90.1 96.9 97.3\nID mBERT 93.7 93.5 93.1 91.4 71.2 / 82.1 85.0 / 78.4 85.9 / 79.3 93.3 93.5 + ATask 93.3 93.5 92.9 90.6 70.6 / 82.5 83.7 / 76.5 84.8 / 77.4 93.5 93.4 + ATask + ALang 93.6 93.5 93.1 93.6 70.8 / 82.2 84.3 / 77.4 85.4 / 78.1 93.6 93.4 + ATask + ALang + MONOTOK 93.0 93.4 94.5 93.8 74.4 / 84.4 84.6 / 77.6 85.1 / 78.3 93.7 93.5\nKO mBERT 87.3 86.6 86.7 86.7 69.7 / 89.5 86.9 / 83.2 89.2 / 85.7 95.8 96.0 + ATask 87.1 86.2 86.7 86.5 69.8 / 89.7 85.5 / 81.1 87.8 / 83.9 95.9 96.2 + ATask + ALang 87.3 86.2 86.6 86.3 70.0 / 89.8 85.9 / 81.6 88.3 / 84.3 96.0 96.2 + ATask + ALang + MONOTOK 87.7 86.5 87.9 87.9 73.1 / 90.4 87.0 / 82.7 88.9 / 85.2 96.3 96.5\nTR mBERT 93.7 93.8 86.4 86.4 57.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7 + ATask 93.0 93.0 86.1 83.9 55.3 / 75.1 70.4 / 62.0 72.4 / 64.1 95.5 95.7 + ATask + ALang 93.3 93.5 86.2 84.8 56.9 / 75.8 71.1 / 63.0 73.0 / 64.7 96.0 95.9 + ATask + ALang + MONOTOK 92.7 92.7 86.1 85.3 60.0 / 77.0 73.5 / 65.6 75.7 / 68.1 96.4 96.3\nAVG mBERT 91.2 90.4 90.5 90.0 66.3 / 81.2 84.7 / 79.6 86.0 / 81.0 95.6 95.6 + ATask 90.9 90.2 90.5 89.2 65.5 / 81.1 83.3 / 77.5 84.7 / 79.0 95.6 95.6 + ATask + ALang 91.2 90.3 90.5 90.1 66.1 / 81.2 83.9 / 78.3 85.3 / 79.7 95.8 95.8 + ATask + ALang + MONOTOK 91.1 90.4 91.1 90.7 68.4 / 82.6 85.1 / 79.7 86.2 / 81.0 96.1 96.0\nTable 10: Full Results - Performance on the different tasks leveraging mBERT with different adapter components (see §4.4)."
    } ],
    "references" : [ {
      "title" : "Exploring BERT’s Vocabulary",
      "author" : [ "Judit Ács." ],
      "venue" : "Blog Post.",
      "citeRegEx" : "Ács.,? 2019",
      "shortCiteRegEx" : "Ács.",
      "year" : 2019
    }, {
      "title" : "AraBERT: Transformer-based model for Arabic language understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Lan-",
      "citeRegEx" : "Antoun et al\\.,? 2020",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Wikiextractor",
      "author" : [ "Giusepppe Attardi." ],
      "venue" : "GitHub Repository.",
      "citeRegEx" : "Attardi.,? 2015",
      "shortCiteRegEx" : "Attardi.",
      "year" : 2015
    }, {
      "title" : "Parsing with multilingual BERT, a small corpus, and a small treebank",
      "author" : [ "Ethan C. Chau", "Lucy H. Lin", "Noah A. Smith." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1324–1334, Online. Association for Computational",
      "citeRegEx" : "Chau et al\\.,? 2020",
      "shortCiteRegEx" : "Chau et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving multilingual models with language-clustered vocabularies",
      "author" : [ "Hyung Won Chung", "Dan Garrette", "Kiat Chuan Tan", "Jason Riesa." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Chung et al\\.,? 2020",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2020
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "CANINE: Pre-training an efficient tokenization-free encoder for language representation",
      "author" : [ "Jonathan H. Clark", "Dan Garrette", "Iulia Turc", "John Wieting." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Clark et al\\.,? 2021",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Crosslingual polarity detection with machine translation",
      "author" : [ "Erkin Demirtas", "Mykola Pechenizkiy." ],
      "venue" : "Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM ’13), pages 9:1–8, Chicago, USA.",
      "citeRegEx" : "Demirtas and Pechenizkiy.,? 2013",
      "shortCiteRegEx" : "Demirtas and Pechenizkiy.",
      "year" : 2013
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "How much does tokenization affect neural machine translation",
      "author" : [ "Miguel Domingo", "Mercedes Garcıa-Martınez", "Alexandre Helle", "Francisco Casacuberta", "Manuel Herranz" ],
      "venue" : null,
      "citeRegEx" : "Domingo et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Domingo et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations (ICLR), Toulon, France. OpenReview.net.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "SberQuAD – Russian Reading Comprehension Dataset: Description and analysis",
      "author" : [ "Pavel Efimov", "Andrey Chertok", "Leonid Boytsov", "Pavel Braslavski." ],
      "venue" : "CLEF 2020: Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 3–15.",
      "citeRegEx" : "Efimov et al\\.,? 2020",
      "shortCiteRegEx" : "Efimov et al\\.",
      "year" : 2020
    }, {
      "title" : "Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications",
      "author" : [ "Ashraf Elnagar", "Yasmin S. Khalifa", "Anas Einea." ],
      "venue" : "Intelligent Natural Language Processing: Trends and Applications, pages 35–52. Springer, Cham, Switzerland.",
      "citeRegEx" : "Elnagar et al\\.,? 2018",
      "shortCiteRegEx" : "Elnagar et al\\.",
      "year" : 2018
    }, {
      "title" : "On the relation between linguistic typology and (limitations of) multilingual language modeling",
      "author" : [ "Daniela Gerz", "Ivan Vulić", "Edoardo Maria Ponti", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Gerz et al\\.,? 2018",
      "shortCiteRegEx" : "Gerz et al\\.",
      "year" : 2018
    }, {
      "title" : "Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation",
      "author" : [ "Goran Glavaš", "Ivan Vulić." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
      "citeRegEx" : "Glavaš and Vulić.,? 2021",
      "shortCiteRegEx" : "Glavaš and Vulić.",
      "year" : 2021
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th International",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual ability of multilingual BERT: an empirical study",
      "author" : [ "Karthikeyan K", "Zihan Wang", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "Proceedings of the 8th International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia. Open-",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, USA.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Adaptation of deep bidirectional multilingual transformers for russian language",
      "author" : [ "Yuri Kuratov", "Mikhail Arkhipov." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Kuratov and Arkhipov.,? 2019",
      "shortCiteRegEx" : "Kuratov and Arkhipov.",
      "year" : 2019
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "KR-BERT: A small-scale Korean-specific language model",
      "author" : [ "Sangah Lee", "Hansol Jang", "Yunmee Baik", "Suzi Park", "Hyopil Shin." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315–",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "KorQuAD1.0: Korean QA dataset for machine reading comprehension",
      "author" : [ "Seungyoung Lim", "Myungji Kim", "Jooyoul Lee" ],
      "venue" : null,
      "citeRegEx" : "Lim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lim et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations (ICLR), New Orleans, LA, USA. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "CamemBERT: A tasty French language model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro Javier Ortiz Suárez", "Yoann Dupont", "Laurent Romary", "Éric de la Clergerie", "Djamé Seddah", "Benoı̂t Sagot" ],
      "venue" : "In Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Polyglot contextual representations improve crosslingual transfer",
      "author" : [ "Phoebe Mulcaire", "Jungo Kasai", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Mulcaire et al\\.,? 2019",
      "shortCiteRegEx" : "Mulcaire et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal Dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajič", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Universal Dependencies v2: An evergrowing multilingual treebank collection",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Jan Hajič", "Christopher D. Manning", "Sampo Pyysalo", "Sebastian Schuster", "Francis Tyers", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2020
    }, {
      "title" : "What the [MASK]? Making sense of languagespecific BERT models",
      "author" : [ "Debora Nozza", "Federico Bianchi", "Dirk Hovy." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Nozza et al\\.,? 2020",
      "shortCiteRegEx" : "Nozza et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "AdapterFusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Associ-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2021",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2021
    }, {
      "title" : "AdapterHub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Em-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020c",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "XCOPA: A multilingual dataset for causal commonsense reasoning",
      "author" : [ "Edoardo Maria Ponti", "Goran Glavaš", "Olga Majewska", "Qianchu Liu", "Ivan Vulić", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ponti et al\\.,? 2020",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling language variation and universals: A survey on typological linguistics for natural language processing",
      "author" : [ "Edoardo Maria Ponti", "Helen O’Horan", "Yevgeni Berzak", "Ivan Vulić", "Roi Reichart", "Thierry Poibeau", "Ekaterina Shutova", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Ponti et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2019
    }, {
      "title" : "Early stopping-but when? In Neural Networks: Tricks of the Trade, pages 55–69",
      "author" : [ "Lutz Prechelt." ],
      "venue" : "Springer, Berlin, Germany.",
      "citeRegEx" : "Prechelt.,? 1998",
      "shortCiteRegEx" : "Prechelt.",
      "year" : 1998
    }, {
      "title" : "Improving Bi-LSTM performance for Indonesian sentiment analysis using paragraph vector",
      "author" : [ "Ayu Purwarianti", "Ida Ayu Putu Ari Crisdayanti." ],
      "venue" : "Proceedings of the 2019 International Conference of",
      "citeRegEx" : "Purwarianti and Crisdayanti.,? 2019",
      "shortCiteRegEx" : "Purwarianti and Crisdayanti.",
      "year" : 2019
    }, {
      "title" : "WikiBERT models: Deep transfer learning for many languages",
      "author" : [ "Sampo Pyysalo", "Jenna Kanerva", "Antti Virtanen", "Filip Ginter." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Pyysalo et al\\.,? 2020",
      "shortCiteRegEx" : "Pyysalo et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A Finnish news corpus for named entity recognition",
      "author" : [ "Teemu Ruokolainen", "Pekka Kauppinen", "Miikka Silfverberg", "Krister Lindén." ],
      "venue" : "Language Resources and Evaluation, 54(1):247–272.",
      "citeRegEx" : "Ruokolainen et al\\.,? 2020",
      "shortCiteRegEx" : "Ruokolainen et al\\.",
      "year" : 2020
    }, {
      "title" : "Is multilingual BERT fluent in language generation",
      "author" : [ "Samuel Rönnqvist", "Jenna Kanerva", "Tapio Salakoski", "Filip Ginter" ],
      "venue" : "In Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing,",
      "citeRegEx" : "Rönnqvist et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Rönnqvist et al\\.",
      "year" : 2019
    }, {
      "title" : "BERTurk - BERT models for Turkish",
      "author" : [ "Stefan Schweter." ],
      "venue" : "Zenodo.",
      "citeRegEx" : "Schweter.,? 2020",
      "shortCiteRegEx" : "Schweter.",
      "year" : 2020
    }, {
      "title" : "DRCD: a Chinese machine reading comprehension dataset",
      "author" : [ "Chih Chieh Shao", "Trois Liu", "Yuting Lai", "Yiying Tseng", "Sam Tsai." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Shao et al\\.,? 2019",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis of product reviews in Russian using convolutional neural networks",
      "author" : [ "Sergey Smetanin", "Michail Komarov." ],
      "venue" : "Proceedings of the 2019 IEEE 21st Conference on Business Informatics (CBI), pages 482–486, Moscow, Russia.",
      "citeRegEx" : "Smetanin and Komarov.,? 2019",
      "shortCiteRegEx" : "Smetanin and Komarov.",
      "year" : 2019
    }, {
      "title" : "UDPipe: Trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing",
      "author" : [ "Milan Straka", "Jan Hajič", "Jana Straková." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Re-",
      "citeRegEx" : "Straka et al\\.,? 2016",
      "shortCiteRegEx" : "Straka et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "UDapter: Language adaptation for truly Universal Dependency parsing",
      "author" : [ "Ahmet Üstün", "Arianna Bisazza", "Gosse Bouma", "Gertjan van Noord." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Üstün et al\\.,? 2020",
      "shortCiteRegEx" : "Üstün et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008, Long Beach,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilingual is not enough: BERT for Finnish",
      "author" : [ "Antti Virtanen", "Jenna Kanerva", "Rami Ilo", "Jouni Luoma", "Juhani Luotolahti", "Tapio Salakoski", "Filip Ginter", "Sampo Pyysalo." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Virtanen et al\\.,? 2019",
      "shortCiteRegEx" : "Virtanen et al\\.",
      "year" : 2019
    }, {
      "title" : "BERTje: A Dutch BERT Model",
      "author" : [ "Wietse de Vries", "Andreas van Cranenburgh", "Arianna Bisazza", "Tommaso Caselli", "Gertjan van Noord", "Malvina Nissim." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Vries et al\\.,? 2019",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2019
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo Maria Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "IndoNLU: Benchmark and resources for evaluating Indonesian",
      "author" : [ "Bryan Wilie", "Karissa Vincentio", "Genta Indra Winata", "Samuel Cahyawijaya", "Xiaohong Li", "Zhi Yuan Lim", "Sidik Soleman", "Rahmad Mahendra", "Pascale Fung", "Syafri Bahar", "Ayu Purwarianti" ],
      "venue" : null,
      "citeRegEx" : "Wilie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wilie et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120–130, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Wu and Dredze.,? 2020",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "A discourse-level named entity recognition and relation extraction dataset for Chinese literature text",
      "author" : [ "Jingjing Xu", "Ji Wen", "Xu Sun", "Qi Su." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "2020), we only use masked language modeling (MLM) as pretraining objective and omit the next sentence prediction task",
      "author" : [ "Wu", "Dredze" ],
      "venue" : null,
      "citeRegEx" : "Wu and Dredze,? \\Q2019\\E",
      "shortCiteRegEx" : "Wu and Dredze",
      "year" : 2019
    }, {
      "title" : "We pretrain the new monolingual models (MONOMODEL-*) from scratch for 1M steps with batch size 64",
      "author" : [ "Devlin" ],
      "venue" : null,
      "citeRegEx" : "Devlin,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin",
      "year" : 2019
    }, {
      "title" : "2019) with default hyper-parameters and a weight decay of 0.01. We enable whole word masking (Devlin et al., 2019) for the FI monolingual models, following the pretraining procedure for FinBERT (Virtanen",
      "author" : [ "Loshchilov", "Hutter" ],
      "venue" : null,
      "citeRegEx" : "Loshchilov and Hutter,? \\Q2019\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter",
      "year" : 2019
    }, {
      "title" : "Selection of pretrained models used in our experiments. We display the respective vocabulary sizes and the proportion of tokens",
      "author" : [ "Devlin" ],
      "venue" : null,
      "citeRegEx" : "Devlin,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : ", 2017) pretrained on large English corpora (e.g., BERT, RoBERTa, T5; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), similar monolingual language models have been introduced for other languages (Virtanen et al.",
      "startOffset" : 44,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : ", 2017) pretrained on large English corpora (e.g., BERT, RoBERTa, T5; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), similar monolingual language models have been introduced for other languages (Virtanen et al.",
      "startOffset" : 44,
      "endOffset" : 129
    }, {
      "referenceID" : 46,
      "context" : ", 2017) pretrained on large English corpora (e.g., BERT, RoBERTa, T5; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), similar monolingual language models have been introduced for other languages (Virtanen et al.",
      "startOffset" : 44,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "Concurrently, massively multilingual models with the same architectures and training procedures, covering more than 100 languages, have been proposed (e.g., mBERT, XLM-R, mT5; Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 236
    }, {
      "referenceID" : 8,
      "context" : "Concurrently, massively multilingual models with the same architectures and training procedures, covering more than 100 languages, have been proposed (e.g., mBERT, XLM-R, mT5; Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 236
    }, {
      "referenceID" : 66,
      "context" : "Concurrently, massively multilingual models with the same architectures and training procedures, covering more than 100 languages, have been proposed (e.g., mBERT, XLM-R, mT5; Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021).",
      "startOffset" : 150,
      "endOffset" : 236
    }, {
      "referenceID" : 58,
      "context" : "In this regard, much of the work proposing and releasing new monolingual models is grounded in anecdotal evidence, pointing to the positive results reported for other monolingual BERT models (de Vries et al., 2019; Virtanen et al., 2019; Antoun et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : "In this regard, much of the work proposing and releasing new monolingual models is grounded in anecdotal evidence, pointing to the positive results reported for other monolingual BERT models (de Vries et al., 2019; Virtanen et al., 2019; Antoun et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 258
    }, {
      "referenceID" : 50,
      "context" : "While recent work has argued and validated that mBERT is under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), providing evidence of improved performance when training monolingual models on more data, it is unclear if this is the only factor relevant for the performance of monolingual models.",
      "startOffset" : 71,
      "endOffset" : 116
    }, {
      "referenceID" : 64,
      "context" : "While recent work has argued and validated that mBERT is under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), providing evidence of improved performance when training monolingual models on more data, it is unclear if this is the only factor relevant for the performance of monolingual models.",
      "startOffset" : 71,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "The widespread usage of pretrained multilingual Transformer-based LMs has been instigated by the release of multilingual BERT (Devlin et al., 2019), which followed on the success of the monolingual English BERT model.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Other multilingual models followed mBERT, such as XLM-R (Conneau et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 40,
      "context" : "Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).",
      "startOffset" : 246,
      "endOffset" : 403
    }, {
      "referenceID" : 63,
      "context" : "Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).",
      "startOffset" : 246,
      "endOffset" : 403
    }, {
      "referenceID" : 2,
      "context" : "Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).",
      "startOffset" : 246,
      "endOffset" : 403
    }, {
      "referenceID" : 18,
      "context" : "Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).",
      "startOffset" : 246,
      "endOffset" : 403
    }, {
      "referenceID" : 20,
      "context" : "Concurrently, many studies analyzed mBERT’s and XLM-R’s capabilities and limitations, finding that the multilingual models work surprisingly well for cross-lingual tasks, despite the fact that they do not rely on direct cross-lingual supervision (e.g., parallel or comparable data, translation dictionaries; Pires et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Hu et al., 2020; K et al., 2020).",
      "startOffset" : 246,
      "endOffset" : 403
    }, {
      "referenceID" : 38,
      "context" : ", 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 113
    }, {
      "referenceID" : 41,
      "context" : ", 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 113
    }, {
      "referenceID" : 64,
      "context" : "for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "There are monolingual BERT variants for Arabic (Antoun et al., 2020), French (Martin et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : ", 2020), French (Martin et al., 2020), Finnish (Virtanen et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 58,
      "context" : ", 2020), Finnish (Virtanen et al., 2019), Dutch (de Vries et al.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 35,
      "context" : "(2019) compared monolingual and bilingual ELMo (Peters et al., 2018) LMs across three downstream tasks, finding that contextualized representations from the bilingual models can improve monolingual task performance relative to their monolingual counterparts.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 163
    }, {
      "referenceID" : 42,
      "context" : "Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 163
    }, {
      "referenceID" : 50,
      "context" : "Unlike prior work, which typically lacks either language (Rönnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vulić et al.",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 64,
      "context" : ", 2020) or task diversity (Wu and Dredze, 2020; Vulić et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3.",
      "startOffset" : 26,
      "endOffset" : 67
    }, {
      "referenceID" : 60,
      "context" : ", 2020) or task diversity (Wu and Dredze, 2020; Vulić et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3.",
      "startOffset" : 26,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "We achieve task diversity and generalizability by selecting a combination of tasks driven by lower-level syntactic and higher-level semantic features (Lauscher et al., 2020).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : ", 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "Arabic AR Afroasiatic AraBERT (Antoun et al., 2020) English EN Indo-European BERT (Devlin et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : ", 2020) English EN Indo-European BERT (Devlin et al., 2019) Finnish FI Uralic FinBERT (Virtanen et al.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 58,
      "context" : ", 2019) Finnish FI Uralic FinBERT (Virtanen et al., 2019) Indonesian ID Austronesian IndoBERT (Wilie et al.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 61,
      "context" : ", 2019) Indonesian ID Austronesian IndoBERT (Wilie et al., 2020) Japanese JA Japonic Japanese-char BERT5 Korean KO Koreanic KR-BERT (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : ", 2020) Japanese JA Japonic Japanese-char BERT5 Korean KO Koreanic KR-BERT (Lee et al., 2020) Russian RU Indo-European RuBERT (Kuratov and Arkhipov, 2019) Turkish TR Turkic BERTurk (Schweter, 2020)",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 22,
      "context" : ", 2020) Russian RU Indo-European RuBERT (Kuratov and Arkhipov, 2019) Turkish TR Turkic BERTurk (Schweter, 2020)",
      "startOffset" : 40,
      "endOffset" : 68
    }, {
      "referenceID" : 51,
      "context" : ", 2020) Russian RU Indo-European RuBERT (Kuratov and Arkhipov, 2019) Turkish TR Turkic BERTurk (Schweter, 2020)",
      "startOffset" : 95,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "Chinese ZH Sino-Tibetan Chinese BERT (Devlin et al., 2019)",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 49,
      "context" : "We rely on: CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), FiNER (Ruokolainen et al., 2020), Chinese Literature (Xu et al.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 65,
      "context" : ", 2020), Chinese Literature (Xu et al., 2017), KMOU NER,6 WikiAnn (Pan et al.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "We employ: HARD (Elnagar et al., 2018), IMDb Movie Reviews (Maas et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 44,
      "context" : ", 2011), Indonesian Prosa (Purwarianti and Crisdayanti, 2019), Yahoo Movie Reviews,7 NSMC,8 RuReviews (Smetanin and Komarov, 2019), Turkish Movie and Product Reviews (Demirtas and Pechenizkiy, 2013), ChnSentiCorp.",
      "startOffset" : 26,
      "endOffset" : 61
    }, {
      "referenceID" : 53,
      "context" : ", 2011), Indonesian Prosa (Purwarianti and Crisdayanti, 2019), Yahoo Movie Reviews,7 NSMC,8 RuReviews (Smetanin and Komarov, 2019), Turkish Movie and Product Reviews (Demirtas and Pechenizkiy, 2013), ChnSentiCorp.",
      "startOffset" : 102,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : ", 2011), Indonesian Prosa (Purwarianti and Crisdayanti, 2019), Yahoo Movie Reviews,7 NSMC,8 RuReviews (Smetanin and Komarov, 2019), Turkish Movie and Product Reviews (Demirtas and Pechenizkiy, 2013), ChnSentiCorp.",
      "startOffset" : 166,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : ", 2019), SberQuAD (Efimov et al., 2020), TQuAD,10 DRCD (Shao et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 52,
      "context" : ", 2020), TQuAD,10 DRCD (Shao et al., 2019), TyDiQA-GoldP (Clark et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "For UDP, we use a transformer-based variant (Glavaš and Vulić, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017).",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "For UDP, we use a transformer-based variant (Glavaš and Vulić, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017).",
      "startOffset" : 127,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "We use AdamW (Kingma and Ba, 2015) in all experiments, with a learning rate of 3e−5.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 43,
      "context" : "11 We train for 10 epochs with early stopping (Prechelt, 1998).",
      "startOffset" : 46,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "6 treebanks and compute two metrics (Ács, 2019).",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : ", the average number of subword tokens produced per tokenized word (Ács, 2019)), and proportion of continued words (i.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : ", words split into multiple subword tokens (Ács, 2019)).",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "First, we use WikiExtractor (Attardi, 2015) to extract text passages from the raw dumps.",
      "startOffset" : 28,
      "endOffset" : 43
    }, {
      "referenceID" : 54,
      "context" : "(2020) and utilize UDPipe (Straka et al., 2016) parsers pretrained on UD data to segment the extracted text passages into texts with document, sentence, and word boundaries.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "22 We enable whole word masking (Devlin et al., 2019) for the FI monolingual models, following the pretraining procedure for FinBERT (Virtanen et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 58,
      "context" : ", 2019) for the FI monolingual models, following the pretraining procedure for FinBERT (Virtanen et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "Similar to how the chosen method of tokenization affects neural machine translation quality (Domingo et al., 2019), these results establish that, in fact,",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 56,
      "context" : "Another way to provide more language-specific capacity to a multilingual LM beyond a dedicated tokenizer, thereby potentially making gains in monolingual downstream performance, is to introduce adapters (Pfeiffer et al., 2020b,c; Üstün et al., 2020), a small number of additional parameters at every layer of a pretrained model.",
      "startOffset" : 203,
      "endOffset" : 249
    }, {
      "referenceID" : 50,
      "context" : "Unlike prior work, which primarily attributes gaps in performance to mBERT being under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), our disentangled results (Table 3) suggest that a large portion of existing performance gaps can be attributed to the capability of the tokenizer.",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 64,
      "context" : "Unlike prior work, which primarily attributes gaps in performance to mBERT being under-trained (Rönnqvist et al., 2019; Wu and Dredze, 2020), our disentangled results (Table 3) suggest that a large portion of existing performance gaps can be attributed to the capability of the tokenizer.",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 39,
      "context" : "(2020), our results suggest that investing more effort into 1) improving the balance of individual languages’ representations in the vocabulary of multilingual LMs, and 2) providing languagespecific adaptations and extensions of multilingual tokenizers (Pfeiffer et al., 2020c) can reduce the gap between monolingual and multilingual LMs.",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 7,
      "context" : "Another promising future research direction is completely disposing of any (language-specific or multilingual) tokenizers during pretraining (Clark et al., 2021).",
      "startOffset" : 141,
      "endOffset" : 161
    } ],
    "year" : 2021,
    "abstractText" : "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation model of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",
    "creator" : "LaTeX with hyperref"
  }
}