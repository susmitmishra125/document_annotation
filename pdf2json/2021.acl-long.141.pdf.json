{
  "name" : "2021.acl-long.141.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
    "authors" : [ "Hongliang Dai", "Yangqiu Song", "Haixun Wang" ],
    "emails" : [ "hdai@cse.ust.hk", "yqsong@cse.ust.hk", "haixun.wang@instacart.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1790–1799\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1790"
    }, {
      "heading" : "1 Introduction",
      "text" : "Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc. Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.g., person, actor, company, victim) to label noun phrases including not only named entity mentions, but also pronouns and nominal nouns. This task directly uses type words or phrases as tags. Its tag set can contain more than 10,000 types. A challenge is that with the large type set, it is extremely difficult and\ntime-consuming for humans to annotate samples. As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020).\nThere are two main approaches to obtaining weakly labeled training examples. One approach is to find the Wikipedia pages that correspond to entity mentions, which can be done by using hyperlinks to Wikipedia or applying entity linking. Then the entity types can be obtained from knowledge bases. The other approach is to directly use the head words of nominal mentions as ultra-fine type labels. For example, if a nominal mention is “a famous actor,” then the head word “actor” can be used as its type label.\nSeveral problems exist when using these weak labels for the ultra-fine typing task. First, in the dataset created by Choi et al. (2018), on average there are fewer than two labels (types) for each sample annotated through either entity linking or head word supervision. On the other hand, a human annotated sample has on average 5.4 labels. As a result, models trained from the automatically obtained labels have a low recall. Second, neither of the above approaches can create a large number of training samples for pronoun mentions. Third, it is difficult to obtain types that are highly dependent on the context. For example, in “I met the movie star Leonardo DiCaprio on the plane to L.A.,” the type passenger is correct for “Leonardo DiCaprio.” However, this type cannot be obtained by linking to knowledge bases.\nIn this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al., 2019), to generate weak labels for ultra-fine entity typing. Given a sentence that contains a mention, our approach adds a short piece of text that contains a “[MASK]” token into it\nto construct an input to BERT. Then, the pretrained MLM will predict the hypernyms of the mention as the most probable words for “[MASK].” These words can then be used as type labels. For example, consider the first example in Table 1. The original sentence is “In late 2015, Leonardo DiCaprio starred in The Revenant.” We construct an input for the BERT MLM by inserting “[MASK] such as” before the mention “Leonardo DiCaprio.” With this input, the pretrained BERT MLM predicts “actors,” “stars,” “actor,” “directors,” and “filmmakers” as the five most probable words for “[MASK].” Most of them are correct types for the mention after singularization. This approach can generate labels for different kinds of mentions, including named entity mentions, pronoun mentions, and nominal mentions. Another advantage is that it can produce labels that needs to be inferred from the context. This allows us to generate more context-dependent labels for each mention, such as passenger, patient, etc.\nThen, we propose a method to select from the results obtained through different hypernym extraction patterns to improve the quality of the weak labels. We also use a weighted loss function to make better use of the generated labels for model training. Finally, we adopt a self-training step to further improve the performance of the model. We evaluate our approach with the dataset created by Choi et al. (2018), which to the best of our knowledge, is the only English ultra-fine entity typing dataset currently available. On this dataset, we achieve more than 4% absolute F1 improvement over the previously reported best result. Additionally, we also apply our approach to a traditional fine-grained entity typing dataset: Ontonotes (Gillick et al., 2014), where it also yields better performance than the state of the art.\nOur contributions are summarized as follows.\n• We propose a new way to generate weak labels for ultra-fine entity typing.\n• We propose an approach to make use of the newly obtained weak labels to improve entity typing results.\n• We conduct experiments on both an ultra-fine entity typing dataset and a traditional finegrained entity typing dataset to verify the effectiveness of our method.\nOur code is available at https://github.com/ HKUST-KnowComp/MLMET."
    }, {
      "heading" : "2 Related Work",
      "text" : "The ultra-fine entity typing task proposed by Choi et al. (2018) uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task (Ling and Weld, 2012) that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by (Onoe and Durrett, 2019) filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by (Xiong et al., 2019) to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. Onoe et al. (2021) use box embeddings to capture latent type hierarchies. There is also some work on the applications of ultra-fine entity typing: Onoe and Durrett (2020) apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation.\nThe traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely\nrelated to ultra-fine entity typing. Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc.\nOur work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks. Different from them, we use the predictions produced by BERT as intermediate results that are regarded as weak supervision to train better models. (Zhang et al., 2020) also uses Hearst patterns to probe masked language models. However, they target at the entity set expansion task."
    }, {
      "heading" : "3 Methodology",
      "text" : "Our methodology consists of two main steps. First, we obtain weak ultra-fine entity typing labels from a BERT masked language model. Second, we use the generated labels in model training to learn better ultra-fine entity typing models."
    }, {
      "heading" : "3.1 Labels from BERT MLM",
      "text" : "Given a sentence and a mention of interest in the sentence, our goal is to derive the hypernym or the type of the mention using a BERT MLM. To do this, we insert into the sentence a few tokens to create an artificial Hearst pattern (Hearst, 1992). One of the inserted tokens is a special “[MASK]” token, which serves as the placeholder of the hypernym of the mention. As the BERT MLM predicts the “[MASK]” token, we derive the hypernyms of the mention.\nConsider the first sentence in Table 1 as an example: “In late 2015, Leonardo DiCaprio starred in The Revenant.” To find the hypernym or the type of “Leonardo DiCaprio”, we insert three tokens to create a “such as” pattern: “In late 2015, [MASK] such as Leonardo DiCaprio starred in The Revenant.” Applying the BERT MLM on the sentence, we derive hypernyms such as “actors,”\n“stars,” “directors,” “filmmakers.” Table 1 shows a few more examples.\nWe consider the 63 Hearst-like patterns (Hearst, 1992) presented in (Seitner et al., 2016) that express a hypernym-hypnonym relationship between two terms. Table 2 lists some of the patterns, wherein H and M denote a hypernym and a hyponym, respectively. For example, “M and some other H” can be used to match “Microsoft and some other companies.”\nThe general procedure to use these patterns to create input samples for BERT MLM and obtain labels from its predictions is as follows. We first regard the mention as M . Then, we insert the rest of the pattern either before or after the mention, and we replace H with the special “[MASK]” token. After applying the BERT MLM on sentences with artificial Hearst patterns, we derive top k type labels from the prediction for “[MASK].” To drive these k labels, we first sigularize the most probable words that are plural. Then, remove those that are not in the type vocabulary of the dataset. Finally, use the most probable k different words as k labels. For example, if we want to obtain 3 labels, and the most probable words are “people,” “actors,” “celebrities,” “famous,” “actor,” etc. Then the 3 labels should be person, actor, celebrity. Because “actor” is the singluar form of “actors,” and “famous” is not in the type vocabulary.\nWe show the performance of our method for obtaining 10 type labels for each mention with different patterns in Table 2. A pre-trained BERTBase-Cased MLM is used to obtain the results1.\nFor nominal mentions, directly applying the patterns that starts with “M” with the above procedure\n1We use the pretrained model provided in the Transformers library. We also tried using BERT-Large and RoBERTa models. However, they do not yield better performance.\nmay sometimes be problematic. For example, consider the noun phrase “the factory in Thailand” as a mention. If we use the “M and some other H” pattern and insert “and other [MASK]” after the mention, the BERT MLM will predict the type country for Thailand instead of for the entire mention. To avoid such errors, while applying patterns that starts with “M” for nominal mentions, we regard the head word of the mention as M instead.\nA more subtle and challenging problem is that the quality of the type labels derived from different patterns for different mentions can be very different. For example, for the mention “He” in sentence “He has won some very tough elections and he’s governor of the largest state,” the pattern “H such as M” leads to person, election, time, thing, leader as the top five types. But using the pattern “M and any other H ,” we get candidate, politician, man, person, governor. On the other hand, for mention “the Al Merreikh Stadium” in “It was only Chaouchi’s third cap during that unforgettable night in the Al Merreikh Stadium,” the results of using “H such as M” (the top five types are venue, place, facility, location, area) is better than using “M and any other H” (the top five types are venue, stadium, game, match, time).\nTo address the above problem, we do not use a same pattern for all the mentions. Instead, for each mention, we try to select the best pattern to apply from a list of patterns. This is achieved by using a baseline ultra-fine entity typing model, BERTUltra-Pre, which is trained beforehand without using labels generated with our BERT MLM based approach. Details of BERT-Ultra-Pre can be found in Section 5.2. Denote the pattern list as L. With each pattern in L, we can apply it on the given mention to derive a set of labels from the BERT MLM. Then, we find the set of labels that have the most overlap with the labels predicted by BERTUltra-Pre. Finally, the given mention is annotated with this set of labels.\nIt is not necessary to use all the patterns in (Seitner et al., 2016). To construct L, the list of patterns used for annotation, we perform the following procedure.\nStep 1: Initialize L to contain the best performing pattern (i.e., “M and any other H”) only.\nStep 2: From all the patterns not in L, find the one that may bring the greatest improvement in F1 score if it is added to L.\nStep 3: Add the pattern found in Step 2 to the L\nif the improvement brought by it is larger than a threshold.\nStep 4: Repeat steps 2-3 until no patterns can be added.\nDiscussion on Type Coverage Since we only use one [MASK] token to generate labels, the model cannot produce multi-word types (e.g., football player) or single word types that are not present in the BERT MLM vocabulary. The BERT MLM vocabulary covers about 92% of the labels in the human annotated dataset constructed by Choi et al. (2018). Type coverage is a known issue with weak supervision, and is tolerable if the generated labels can be used to achieve our final goal: improving the performance of the ultra-fine entity typing model."
    }, {
      "heading" : "3.2 Training Data",
      "text" : "Our approach generates type labels for all three types of mentions: named entity mentions, pronoun mentions, and nominal mentions. For named entity mentions and nominal mentions, existing automatic annotation approaches can already provide some labels for them by using the entity types in knowledge bases or using the head words as types (Ling and Weld, 2012; Choi et al., 2018). Thus, we combine these labels with the labels generated by us. For pronoun mentions, no other labels are used.\nBesides the automatically annotated samples, we can also use a small amount of human annotated samples provided by the dataset for model training."
    }, {
      "heading" : "3.3 Model Training",
      "text" : "Our ultra-fine entity typing model follows the BERT-based model in (Onoe and Durrett, 2019). Given a sentence that contains an entity mention, we form the sequence “[CLS] sentence [SEP] mention string [SEP]” as the input to BERT. Then, denoting the final hidden vector of the “[CLS]” token as u, we add a linear classification layer on top of u to model the probability of each type:\np = σ(Wu), (1)\nwhere σ is the sigmoid function, W is a trainable weight matrix. p ∈ Rd, where d is the number of types used by the dataset. We assign a type t to the mention if pt, its corresponding element in p, is larger than 0.5. If no such types are found, we assign the one with the largest predicted probability to the mention.\nTo make use of the automatically labeled samples, some existing approaches mix them with high quality human annotated samples while training models (Choi et al., 2018; Onoe and Durrett, 2019). However, we find that better performance can be obtained by pretraining the model on automatically labeled samples, then fine-tuning it on human annotated samples.\nFollowing (Choi et al., 2018), we partition the whole type vocabulary used by the dataset into three non-overlapping sets: general, fine, and ultrafine types, denoted with Tg, Tf and Tu, respectively. Then, we use the following objective for training:\nJ (x) = L(x, Tg)1(L, Tg) + L(x, Tf )1(L, Tf ) + L(x, Tu)1(L, Tu),\n(2)\nwhere x is a training sample; L denotes the set of type labels assigned to x through either human or automatic annotation. The function 1(L, T ) equals 1 when a type in L is in set T and 0 otherwise. This loss can avoid penalizing some false negative labels.\nUnlike existing studies, we define the function L differently for human annotated samples and automatically labeled samples. While pretraining with automatically labeled samples, the labels obtained through entity linking and head word supervision are usually of higher precision than those obtained through BERT MLM. Thus, we propose to assign different weights in the training objective to the labels generated with different methods:\nL(x, T ) = − ∑ t∈T α(t)[yt · log(pt)\n+ (1− yt) · log(1− pt)], (3)\nwhere yt equals to 1 if t is annotated as a type for x and 0 otherwise; pt is the probability of whether t should be assigned to x predicted by the model. The value of α(t) indicates how confident we are about the label t for x. Specifically, it equals to a predefined constant value larger than 1 when t is a positive type for x obtained through entity linking or head word supervision, otherwise, it equals to 1.\nWhile fine-tuning with human annotated samples, we directly use the binary cross entropy loss: L(x, T ) = − ∑ t∈T [yt·log(pt)+(1−yt)·log(1−pt)].\n(4)"
    }, {
      "heading" : "3.4 Self-Training",
      "text" : "Denote the ultra-fine entity typing model obtained after pretraining on the automatically labeled data as h, and the model obtained after fine-tuning h with human annotated data as m. A weakness of m is that at the fine-tuning stage, it is trained with only a small number of samples. Thus, we employ self-training to remedy this problem.\nBy using m as a teacher model, our self-training step fine-tunes the model h again with a mixture of the samples from the automatically labeled data and the human annotated data. This time, for the automatically annotated samples, we use pseudo labels generated based on the predictions of m instead of their original weak labels. The newly fine-tuned model should perform better than m, and is used for evaluation.\nDenote the set of human annotated samples as H , the set of automatically labeled samples as A. The training objective at this step is\nJST = 1 |H| ∑ x∈H J (x) + λ 1 |A| ∑ x∈A LST (x), (5)\nwhere λ is a hyperparameter that controls the strength of the supervision from the automatically labeled data.\nWhile computing loss for the samples in A, we only use the types that are very likely to be positive or negative. For a sample x, let pt be the probability of it belonging to type t predicted by the model m. We consider a type t very likely to be positive if pt is larger than a threshold P , or if t is a weak label of x and pt is larger than a smaller threshold Pw. Denote the set of such types as Ŷ +(x). We consider a type t very likely to be negative if pt is smaller than 1 − P . Denote the set of such types as Ŷ −(x). Then we have:\nLST (x) = − ∑\nt∈Ŷ +(x)\nlog(pt)\n− ∑\nt∈Ŷ −(x)\nlog(1− pt). (6)\nThus, we compute the binary cross entropy loss with only the types in Ŷ +(x) and Ŷ −(x)."
    }, {
      "heading" : "4 Application to Traditional Fine-grained Entity Typing",
      "text" : "Our approach to generating weak entity type labels with BERT MLM can also be applied to the\ntraditional fine-grained entity typing task. Different from ultra-fine entity typing, traditional finegrained entity typing uses a manually designed entity type ontology to annotate mentions. The types in the ontology are organized in an hierarchical structure. For example, the ontology used by the Ontonotes dataset contains 89 types including /organization, /organization/company, /person, /person/politician, etc. On this dataset, our automatic annotation approach can mainly be helpful to generate better labels for nominal mentions.\nWe still use the same method described in Section 3.1 to create input for BERT MLM based on the given mention. But with traditional finegrained entity typing, most mentions are assigned only one type path (e.g., a company mention will only be assigned labels {/organization, /organization/company}, which includes all the types along the path of /organization/company). Thus, while generating labels, we only use the most probable word predicted by the BERT MLM, which is mapped to the types used by the dataset if possible. For example, the word “company” and its plural form are both mapped to /organization/company. Such a mapping from free-form entity type words to the types used by the dataset can be created manually, which does not require much effort. We mainly construct the mapping with two ways: 1) Check each type used by the dataset, and think of a few words that should belong to it, if possible. For example, for the type /person/artist/author, corresponding words can be “author,” “writer,” etc. 2) Run the BERT MLM on a large number of inputs constructed with unannotated mentions, then try to map the words that are most frequently predicted as the most probable word to the entity type ontology.\nSince only the most probable word predicted by the BERT MLM is used to produce labels, we also only use one hypernym relation pattern: “M and any other H .”\nFor traditional fine-grained entity typing, we use our approach to generate labels for mentions that are not previously annotated with other automatic annotation approaches. While training, all the automatically labeled mentions are used together. The typing model is the same as the model described in 3.3. The binary cross entropy loss is directly employed as the training objective."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conduct experiments on our primary task: ultrafine entity typing. In addition, we evaluate the performance of our approach when applied to traditional fine-grained entity typing."
    }, {
      "heading" : "5.1 Evaluation on Ultrafine",
      "text" : "For ultra-fine entity typing, we use the dataset created by Choi et al. (2018). It uses a type set that contains 10,331 types. These types are partitioned into three categories: 9 general types, 121 finegrained types, and 10,201 ultra-fine types. There are 5,994 human annotated samples. They are split into train/dev/test with ratio 1:1:1. It also provides 5.2M samples weakly labeled through entity linking and 20M samples weakly labeled through head word supervision.\nWe compare with the following approaches:\n• UFET (Choi et al., 2018). This approach obtains the feature vector for classification by using a bi-LSTM, a character level CNN, and pretrained word embeddings.\n• LabelGCN (Xiong et al., 2019). LabelGCN uses a graph propagation layer to capture label correlations.\n• LDET (Onoe and Durrett, 2019). LDET learns a model that performs relabeling and sample filtering to the automatically labeled samples. Their typing model, which employs ELMo embeddings and a bi-LSTM, is train with the denoised labels.\n• Box (Onoe et al., 2021). Box represents entity types with box embeddings to capture latent type hierarchies. Their model is BERT-based.\nWe use the BERT-Base-Cased version of BERT for both weak label generation and the typing model in Section 3.3. The hyperparameters are tuned through grid search using F1 on the dev set as criterion. The value of α(t) in Equation (3) is set to 5.0 for positive types obtained through entity linking or head word supervision. λ in Equation (5) is set to 0.01. P and Pw in Section 3.4 are set to 0.9 and 0.7, respectively. Our approach to generate labels through BERT MLM is applied to each weak sample provided in the original dataset. In addition, we also use our approach to annotate about 3.7M pronoun mentions, which are extracted through string matching from the English Gigaword corpus\n(Parker et al., 2011). We generate 10 types for each sample2. With the procedure described in Sectiton 3.1, three hypernym extraction patterns are used while generating labels with BERT MLM: “M and any other H ,” “H such as M ,” “M and some other H .” Specifically, adding “H such as M” and “M and some other H” improves the F1 score from 0.253 to 0.274, and from 0.274 to 0.279, respectively. Adding any more patterns cannot improve the F1 score for more than 0.007.\nFollowing existing work (Onoe et al., 2021; Onoe and Durrett, 2019), we evaluate the macroaveraged precision, recall, and F1 of different approaches on the manually annotated test set. The results are in Table 3. Our approach achieves the best F1 score. It obtains more than 4% F1 score improvement over the existing best reported performance by Box in (Onoe et al., 2021). This demonstrates the effectiveness of our approach."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "For ablation study, we verify the effectiveness of the different techniques used in our full entity typing approach by evaluating the performance of the following variants: Ours (Single Pattern) only\n2The performance of the trained model is relatively insensitive with respect to the number of labels generated with MLM. The difference between the F1 scores of the models trained using 10 and 15 generated types is less than 0.005.\nuses one pattern: M and any other H; Ours (Unweighted Loss) removes the α(t) term in Equation (3); Ours (No Self-train) does not perform the self-training step. We also evaluate two baseline approaches: BERT-Ultra-Direct uses the same BERT based model described in Section 3.3, but is trained with only the human annotated training samples; BERT-Ultra-Pre also uses the same BERT based model, but is first pretrained with the existing automatically generated training samples in the dataset provided by Choi et al. (2018), then fine-tuned on the human annotated training data.\nFirst, the benefit of using the labels generated through BERT MLM can be verified by comparing Ours (No Self-train) and BERT-Ultra-Pre. Because the techniques employed in Ours (No Self-train), including the use of multiple hypernym extraction patterns and the weighted loss, are both for better utilization of our automatic entity type label generation method.\nThe effectiveness of the use of multiple hypernym extraction patterns, the weighted loss, and the self-training step can be verified by comparing Ours with Ours (Single Pattern), Ours (Unweighted Loss) and Ours (No Self-train), respectively. Among them, self-training is most beneficial."
    }, {
      "heading" : "5.3 Evaluation on Different Kinds of Mentions",
      "text" : "It is also interesting to see how our approach performs on different kinds of mentions. Table 5 lists the performance of our full approach and two baseline systems on the three kinds of mentions in the dataset: named entity mention, pronoun mentions, and nominal mentions.\nOur approach performs much better than BERTUltra-Pre on all three kinds of mentions. The improvements in F1 on pronoun and nominal mentions are relatively more substantial."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "Table 6 presents several ultra-fine entity typing examples, along with the human annotated labels, and the labels predicted by BERT-Ultra-Pre, BERT MLM, and our full approach.\nIn the first example, the label prisoner is a type that depends on the context, and is usually not assigned to humans in knowledge bases. We think that since we can assign such labels to the training samples with our BERT MLM based approach, our\nmodel is better at predicting them than the baseline model.\nThe second and third examples demonstrate that our model may not only improve the recall by predicting more correct types, but also reduce incorrect predictions that do not fit the mention or the context well."
    }, {
      "heading" : "5.5 Evaluation on Ontonotes",
      "text" : "The Ontonotes dataset uses an ontology that contains 89 types to label entity mentions. We use the version provided by Choi et al. (2018). It includes\n11,165 manually annotated mentions, which are split into a test set that contains 8,963 mentions, and a dev set that contain 2,202 mentions. It also provides about 3.4M automatically labeled mentions.\nSince existing annotations for named entity mentions may be more accurate than the annotations obtained through our approach, we only apply our method to label nominal mentions. Applying the approach in Section 4, we create 1M new automatically labeled mentions with the head word supervision samples (such samples contain mostly nominal mentions) in the ultra-fine dataset. They are used together with the originally provided 3.4M mentions to train the typing model.\nOn this dataset, we compare with the following approaches: UFET (Choi et al., 2018), LDET (Onoe and Durrett, 2019), DSAM (Hu et al., 2020), LTRFET (Lin and Ji, 2019), BERT-Direct. Where BERT-Direct uses the same BERT based model as our approach, but trains with only the weak samples provided in the dataset. LTRFET adopts a hybrid classification method to exploit type inter-dependency. DSAM is a diversified semantic attention model with both mention-level attention and context-level attention.\nFor our approach and BERT-Direct, we still use the pretrained BERT-Base-Cased model for initialization. Although a very large number of weakly labeled mentions are provided, not all of them are needed for training the models. In our experiments, for both our approach and BERT-Direct, the performance does not increase after training on about 0.3M mentions.\nWe report strict accuracy, macro-averaged F1, and micro-averaged F1 (Ling and Weld, 2012). The results are in Table 7. As we can see, our approach also achieves the best performance on this dataset. Comparing it with BERT-Direct demonstrates the benefit of the samples automatically labeled with BERT MLM.\nHowever, less improvement is achieved on OntoNotes than on the ultra-fine entity typing\ndataset. We think there are two main reasons. First, OntoNotes uses a much smaller entity type set (89 types) than the ultra-fine entity typing dataset (10,331 types). As a result, some finer grained types that can be produced by our approach become less beneficial. Second, generating type labels that are highly dependent on the context (e.g., types like criminal, speaker) is an advantage of our approach, and the ultra-fine entity typing dataset contains more such type labels."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose a new approach to automatically generate ultra-fine entity typing labels. Given a sentence that contains a mention, we insert a hypernym extraction pattern with a “[MASK]” token in it, so that a pretrained BERT MLM may predict hypernyms of the mention for “[MASK].” Multiple patterns are used to produce better labels for each mention. We also propose to use a weighted loss and perform a self-training step to learn better entity typing models. Experimental results show that our approach greatly outperforms state-of-theart systems. Additionally, we also apply our approach to traditional fine-grained entity typing, and verify its effectiveness with experiments."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This paper was supported by the NSFC Grant (No. U20B2053) from China, the Early Career Scheme (ECS, No. 26206717), the General Research Fund (GRF, No. 16211520), and the Research Impact Fund (RIF, No. R6020-19 and No. R6021-20) from the Research Grants Council (RGC) of Hong Kong, with special thanks to the WeChat-HKUST WHAT Lab on Artificial Intelligence Technology."
    } ],
    "references" : [ {
      "title" : "Hierarchical entity typing via multi-level learning to rank",
      "author" : [ "Tongfei Chen", "Yunmo Chen", "Benjamin Van Durme." ],
      "venue" : "Proceedings of ACL, pages 8465–8475.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Ultra-fine entity typing",
      "author" : [ "Eunsol Choi", "Omer Levy", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of ACL, pages 87–96.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving fine-grained entity typing with entity linking",
      "author" : [ "Hongliang Dai", "Donghong Du", "Xin Li", "Yangqiu Song." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 6211–6216.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting semantic relations for fine-grained entity typing",
      "author" : [ "Hongliang Dai", "Yangqiu Song", "Xin Li." ],
      "venue" : "Automated Knowledge Base Construction.",
      "citeRegEx" : "Dai et al\\.,? 2020",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171– 4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextdependent fine-grained entity type tagging",
      "author" : [ "Dan Gillick", "Nevena Lazic", "Kuzman Ganchev", "Jesse Kirchner", "David Huynh." ],
      "venue" : "arXiv preprint arXiv:1412.1820.",
      "citeRegEx" : "Gillick et al\\.,? 2014",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A Hearst." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Hearst.,? 1992",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Diversified semantic attention model for finegrained entity typing",
      "author" : [ "Yanfeng Hu", "Xue Qiao", "Xing Luo", "Chen Peng." ],
      "venue" : "IEEE Access.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-grained entity typing via hierarchical multi graph convolutional networks",
      "author" : [ "Hailong Jin", "Lei Hou", "Juanzi Li", "Tiansi Dong." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 4970–4979.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Type-aware distantly supervised relation extraction with linked arguments",
      "author" : [ "Mitchell Koch", "John Gilmer", "Stephen Soderland", "Daniel S. Weld." ],
      "venue" : "Proceedings of EMNLP, pages 1891–1901.",
      "citeRegEx" : "Koch et al\\.,? 2014",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2014
    }, {
      "title" : "A chinese corpus for fine-grained entity typing",
      "author" : [ "Chin Lee", "Hongliang Dai", "Yangqiu Song", "Xin Li." ],
      "venue" : "Proceedings of LREC, pages 4451–4457.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "An attentive fine-grained entity typing model with latent type representation",
      "author" : [ "Ying Lin", "Heng Ji." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 6198– 6203.",
      "citeRegEx" : "Lin and Ji.,? 2019",
      "shortCiteRegEx" : "Lin and Ji.",
      "year" : 2019
    }, {
      "title" : "Design challenges for entity linking",
      "author" : [ "Xiao Ling", "Sameer Singh", "Daniel S Weld." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:315–328.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Fine-grained entity recognition",
      "author" : [ "Xiao Ling", "Daniel S Weld." ],
      "venue" : "Proceedings of AAAI, volume 12, pages 94–100.",
      "citeRegEx" : "Ling and Weld.,? 2012",
      "shortCiteRegEx" : "Ling and Weld.",
      "year" : 2012
    }, {
      "title" : "Hierarchical losses and new resources for fine-grained entity typing and linking",
      "author" : [ "Shikhar Murty", "Patrick Verga", "Luke Vilnis", "Irena Radovanovic", "Andrew McCallum." ],
      "venue" : "Proceedings of ACL, pages 97–109.",
      "citeRegEx" : "Murty et al\\.,? 2018",
      "shortCiteRegEx" : "Murty et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling fine-grained entity types with box embeddings",
      "author" : [ "Yasumasa Onoe", "Michael Boratko", "Greg Durrett." ],
      "venue" : "arXiv preprint arXiv:2101.00345.",
      "citeRegEx" : "Onoe et al\\.,? 2021",
      "shortCiteRegEx" : "Onoe et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to denoise distantly-labeled data for entity typing",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2407–2417.",
      "citeRegEx" : "Onoe and Durrett.,? 2019",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2019
    }, {
      "title" : "Interpretable entity representations through large-scale typing",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "Proceedings of EMNLP, pages 612–624.",
      "citeRegEx" : "Onoe and Durrett.,? 2020",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2020
    }, {
      "title" : "English gigaword fifth edition",
      "author" : [ "Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Parker et al\\.,? 2011",
      "shortCiteRegEx" : "Parker et al\\.",
      "year" : 2011
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of EMNLP-IJCNLP,",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Label noise reduction in entity typing by heterogeneous partial-label embedding",
      "author" : [ "Xiang Ren", "Wenqi He", "Meng Qu", "Clare R Voss", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of ACM SIGKDD, pages 1825– 1834.",
      "citeRegEx" : "Ren et al\\.,? 2016",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "A large database of hypernymy relations extracted from the web",
      "author" : [ "Julian Seitner", "Christian Bizer", "Kai Eckert", "Stefano Faralli", "Robert Meusel", "Heiko Paulheim", "Simone Paolo Ponzetto." ],
      "venue" : "Proceedings of LREC, pages 360–367.",
      "citeRegEx" : "Seitner et al\\.,? 2016",
      "shortCiteRegEx" : "Seitner et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural fine-grained entity typing with knowledge attention",
      "author" : [ "Ji Xin", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of AAAI, volume 32.",
      "citeRegEx" : "Xin et al\\.,? 2018",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2018
    }, {
      "title" : "Imposing label-relational inductive bias for extremely fine-grained entity typing",
      "author" : [ "Wenhan Xiong", "Jiawei Wu", "Deren Lei", "Mo Yu", "Shiyu Chang", "Xiaoxiao Guo", "William Yang Wang." ],
      "venue" : "Proceedings of NAACL-HLT, pages 773–784.",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "Hyena: Hierarchical type classification for entity names",
      "author" : [ "Mohamed Amir Yosef", "Sandro Bauer", "Johannes Hoffart", "Marc Spaniol", "Gerhard Weikum." ],
      "venue" : "Proceedings of COLING, pages 1361– 1370.",
      "citeRegEx" : "Yosef et al\\.,? 2012",
      "shortCiteRegEx" : "Yosef et al\\.",
      "year" : 2012
    }, {
      "title" : "Empower entity set expansion via language model probing",
      "author" : [ "Yunyi Zhang", "Jiaming Shen", "Jingbo Shang", "Jiawei Han." ],
      "venue" : "Proceedings of ACL, pages 8151–8160.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al.",
      "startOffset" : 204,
      "endOffset" : 247
    }, {
      "referenceID" : 18,
      "context" : "Fine-grained entity typing (Ling and Weld, 2012) has been long studied in the natural language processing community as the extracted type information is useful for downstream tasks such as entity linking (Ling et al., 2015; Onoe and Durrett, 2020), relation extraction (Koch et al.",
      "startOffset" : 204,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : ", 2015; Onoe and Durrett, 2020), relation extraction (Koch et al., 2014), coreference resolution (Onoe and Durrett, 2020), etc.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : ", 2014), coreference resolution (Onoe and Durrett, 2020), etc.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Recently, ultra-fine entity typing (Choi et al., 2018) extends the effort to using a richer set of types (e.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "As a result, most existing works use weak labels that are automatically generated (Ling and Weld, 2012; Choi et al., 2018; Lee et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al.",
      "startOffset" : 114,
      "endOffset" : 150
    }, {
      "referenceID" : 22,
      "context" : "In this paper, to alleviate the problems above, we propose an approach that combines hypernym extraction patterns (Hearst, 1992; Seitner et al., 2016) with a masked language model (MLM), such as BERT (Devlin et al.",
      "startOffset" : 114,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : ", 2016) with a masked language model (MLM), such as BERT (Devlin et al., 2019), to generate weak labels for ultra-fine entity typing.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Additionally, we also apply our approach to a traditional fine-grained entity typing dataset: Ontonotes (Gillick et al., 2014), where it also yields better performance than the state of the art.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "(2018) uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task (Ling and Weld, 2012) that uses manually designed entity type ontologies.",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 17,
      "context" : "There are only limited studies on this newly proposed task: A neural model introduced by (Onoe and Durrett, 2019) filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "A graph propagation layer is introduced by (Xiong et al., 2019) to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "The traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "The traditional fine-grained entity typing task (Ling and Weld, 2012; Yosef et al., 2012) is closely",
      "startOffset" : 48,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data.",
      "startOffset" : 21,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data.",
      "startOffset" : 21,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "Automatic annotation (Ling and Weld, 2012; Gillick et al., 2014; Dai et al., 2020) is also commonly used in the studies of this task to produce large size training data.",
      "startOffset" : 21,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "For example, denoising the automatically generated labels (Ren et al., 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : ", 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al.",
      "startOffset" : 84,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : ", 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al.",
      "startOffset" : 84,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : ", 2016), taking advantage of the entity type hierarchies or type inter-dependencies (Chen et al., 2020; Murty et al., 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al.",
      "startOffset" : 84,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : ", 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc.",
      "startOffset" : 121,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : ", 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc.",
      "startOffset" : 121,
      "endOffset" : 175
    }, {
      "referenceID" : 23,
      "context" : ", 2018; Lin and Ji, 2019), exploiting external resources such as the information of entities provided in knowledge bases (Jin et al., 2019; Dai et al., 2019; Xin et al., 2018), etc.",
      "startOffset" : 121,
      "endOffset" : 175
    }, {
      "referenceID" : 20,
      "context" : "Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks.",
      "startOffset" : 43,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks.",
      "startOffset" : 43,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "Our work is also related to recent studies (Petroni et al., 2019; Jiang et al., 2020; Zhang et al., 2020) that probe pretrained language models to obtain knowledge or results for target tasks.",
      "startOffset" : 43,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "To do this, we insert into the sentence a few tokens to create an artificial Hearst pattern (Hearst, 1992).",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "The F1 score is evaluated with the development set of the ultra-fine dataset (Choi et al., 2018) for the labels generated with the corresponding pattern.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "We consider the 63 Hearst-like patterns (Hearst, 1992) presented in (Seitner et al.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "We consider the 63 Hearst-like patterns (Hearst, 1992) presented in (Seitner et al., 2016) that express a hypernym-hypnonym relationship between two terms.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "It is not necessary to use all the patterns in (Seitner et al., 2016).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "Our ultra-fine entity typing model follows the BERT-based model in (Onoe and Durrett, 2019).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "1794 To make use of the automatically labeled samples, some existing approaches mix them with high quality human annotated samples while training models (Choi et al., 2018; Onoe and Durrett, 2019).",
      "startOffset" : 153,
      "endOffset" : 196
    }, {
      "referenceID" : 17,
      "context" : "1794 To make use of the automatically labeled samples, some existing approaches mix them with high quality human annotated samples while training models (Choi et al., 2018; Onoe and Durrett, 2019).",
      "startOffset" : 153,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "Following (Choi et al., 2018), we partition the whole type vocabulary used by the dataset into three non-overlapping sets: general, fine, and ultrafine types, denoted with Tg, Tf and Tu, respectively.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "Following existing work (Onoe et al., 2021; Onoe and Durrett, 2019), we evaluate the macroaveraged precision, recall, and F1 of different approaches on the manually annotated test set.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Following existing work (Onoe et al., 2021; Onoe and Durrett, 2019), we evaluate the macroaveraged precision, recall, and F1 of different approaches on the manually annotated test set.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "It obtains more than 4% F1 score improvement over the existing best reported performance by Box in (Onoe et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "On this dataset, we compare with the following approaches: UFET (Choi et al., 2018), LDET (Onoe and Durrett, 2019), DSAM (Hu et al.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : ", 2018), LDET (Onoe and Durrett, 2019), DSAM (Hu et al., 2020), LTRFET (Lin and Ji, 2019), BERT-Direct.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "We report strict accuracy, macro-averaged F1, and micro-averaged F1 (Ling and Weld, 2012).",
      "startOffset" : 68,
      "endOffset" : 89
    } ],
    "year" : 2021,
    "abstractText" : "Recently, there is an effort to extend finegrained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.",
    "creator" : "LaTeX with hyperref"
  }
}