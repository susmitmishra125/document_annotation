{
  "name" : "2021.acl-long.300.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-language Sentence Selection via Data Augmentation and Rationale Training",
    "authors" : [ "Yanda Chen", "Chris Kedzie", "Suraj Nair", "Petra Galuščáková", "Rui Zhang", "Douglas W. Oard", "Kathleen McKeown" ],
    "emails" : [ "yc3384@columbia.edu,", "kathy}@cs.columbia.edu", "oard}@umd.edu,", "rmz5227@psu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3881"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentence-level query relevance prediction is important for downstream tasks such as query-focused summarization and open-domain question answering; accurately pinpointing sentences containing information that is relevant to the query is critical to generating a responsive summary/answer (e.g., Baumel et al. (2016, 2018)). In this work, we focus on sentence-level query relevance prediction in a cross-lingual setting, where the query and sentence collection are in different languages and the sentence collection is drawn from a low-resource language. Our approach enables English speakers (e.g., journalists) to find relevant information expressed in local sources (e.g., local reaction to the pandemic and vaccines in Somalia).\nWhile we can use machine translation (MT) to translate either the query or each sentence into a common language, and then use a monolingual Information Retrieval (IR) system to find relevant sentences, work on Probabilistic Structured Queries\n(PSQ) (Darwish and Oard, 2003) has shown that the performance of such MT+IR pipelines is hindered by errors in MT. As is well known, complete translation of the sentence collection is not necessary. Inspired by previous work (Vulić and Moens, 2015), we go a step further and propose a simple cross-lingual embedding-based model that avoids translation entirely and directly predicts the relevance of a query-sentence pair (where the query and sentence are in different languages).\nFor training, we treat a sentence as relevant to a query if there exists a translation equivalent of the query in the sentence. Our definition of relevance is most similar to the lexical-based relevance used in Gupta et al. (2007) and Baumel et al. (2018) but our query and sentence are from different languages. We frame the task as a problem of finding sentences that are relevant to an input query, and thus, we need relevance judgments for query-sentence pairs. Our focus, however, is on low-resource languages where we have no sentence-level relevance judgments with which to train our query-focused relevance model. We thus leverage noisy parallel sentence collections previously collected from the web. We use a simple data augmentation and negative sampling scheme to generate a labeled dataset of relevant and irrelevant pairs of queries and sentences from these noisy parallel corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space.\nWhile our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011),\nthree strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019).\nTo summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech. Additionally, (iii) we propose a rationale training secondary objective to further improve SECLR performance, which we call SECLR-RT. Finally, (iv) we conduct training data ablation and hubness studies that show our method’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanović et al., 2010). These findings are validated by empirical results of experiments in a low-resource sentence selection task, with English queries over sentence collections of text and speech in Somali, Swahili, and Tagalog."
    }, {
      "heading" : "2 Related Work",
      "text" : "Query-focused Sentence Selection Sentencelevel query relevance prediction is important for various downstream NLP tasks such as queryfocused summarization (Baumel et al., 2016, 2018; Feigenblat et al., 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018). Such applications often depend on a sentence selection system to provide attention signals on which sentences to focus upon to generate a query-focused summary or answer a question.\nCross-language Sentence Selection A common approach to cross-language sentence selection is to use MT to first translate either the query or the sentence to the same language and then perform standard monolingual IR (Nie, 2010). The risk of this approach is that errors in translation cascade to the IR system.\nAs an alternative to generating full translations, PSQ (Darwish and Oard, 2003) uses word-\nalignments from SMT to obtain weighted query term counts in the passage collection. In other work, Xu and Weischedel (2000) use a 2-state hidden Markov model (HMM) to estimate the probability that a passage is relevant given the query.\nCross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vulić and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018). Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings.\nOur approach differs from previous cross-lingual word embedding methods in two aspects. First, the focus of previous work has mostly been on learning a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work.\nSecond, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approach trains embeddings on an artificial labeled dataset augmented from a parallel corpus and directly represents relevance across languages. Our data augmentation scheme to build a relevance model is inspired by Boschee et al. (2019), but we achieve significant performance improvement by incorporating rationale information into the embedding training process and provide detailed comparisons of performance with other sentence selection approaches.\nTrained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat-\ning alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016)."
    }, {
      "heading" : "3 Methods",
      "text" : "We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT."
    }, {
      "heading" : "3.1 Training Set Generation Algorithm",
      "text" : "Relevant query/sentence generation. Assume we have a parallel corpus of bilingual sentence pairs equivalent in meaning. Let (E,S) be one such sentence pair, where E is in the query language (in our case, English) and S is in the retrieval collection language (in our case, low-resource languages). For every unigram q in E that is not a stopword, we construct a positive relevant sample by viewing q as a query and S as a relevant sentence. Because sentences E and S are (approximately) equivalent in meaning, we know that there likely exists a translation equivalent of q in the sentence S and so we label the (q, S) pair as relevant (i.e. r = 1).\nFor example, one English-Somali sentence pair is E=“true president gaas attend meeting copenhagen”, S=“ma runbaa madaxweyne gaas baaqday shirka copenhegan” (stopwords removed). By extracting unigrams from E as queries, we generate the following positive examples: (q=“true”, S, r = 1), (q=“president”, S, r = 1), (q=“gaas”, S, r = 1), ..., (q=“copenhagen”, S, r = 1).\nWe generate the positive half of the training set by repeating the above process for every sentence pair in the parallel corpus. We limit model training to unigram queries since higher order ngrams appear fewer times and treating them independently reduces the risk of over-fitting. However, our model processes multi-word queries during evaluation, as described in Section 3.2.\nIrrelevant query/sentence generation. Since learning with only positive examples is a challenging task, we opt to create negative examples, i.e. tuples (q, S, r = 0), via negative sampling. For each positive sample (q, S, r = 1), we randomly select another sentence pair (E′, S′) from the parallel corpus. We then check whether S′ is relevant to q or not. Note that both the query q and sentence\nE′ are in the same language, so checking whether q or a synonym can be found in E′ is a monolingual task. If we can verify that there is no direct match or synonym equivalent of q in E′ then by transitivity it is unlikely there exists a translation equivalent in S′, making the pair (q, S′) a negative example. To account for synonymy when we check for matches, we represent q and the words in E′ with pretrained word embeddings. Let wq, wq′ ∈ Rd be the embeddings associated with q and the words q′ ∈ E′. We judge the pair (q, S′) to be irrelevant (i.e. r = 0) if:\nmax q′∈E′\ncos-sim(wq, wq′) ≤ λ1\nwhere λ1 is a parameter. We manually tuned the relevance threshold λ1 on a small development set of query-sentence pairs randomly generated by the algorithm, and set λ1 = 0.4 to achieve highest label accuracy on the development set. If (q, S′) is not relevant we add (q, S′, r = 0) to our synthetic training set, otherwise we re-sample (E′, S′) until a negative sample is found. We generate one negative sample for each positive sample to create a balanced dataset.\nFor example, if we want to generate a negative example for the positive example (q=“meeting”, S=“ma runbaa madaxweyne gaas baaqday shirka copenhegan”, r = 1), we randomly select another sentence pair (E′=“many candidates competing elections one hopes winner”, S′=“musharraxiin tiro badan sidoo u tartamaysa doorashada wuxuuna mid kasta rajo qabaa guusha inay dhinaciisa ahaato”) from the parallel corpus. To check whether q=“meeting” is relevant to S′, by transitivity it suffices to check whether q=“meeting” or a synonym is present in E′, a simpler monolingual task. If q is irrelevant to S′, we add (q, S′, r = 0) as a negative example."
    }, {
      "heading" : "3.2 Cross-Lingual Relevance Model",
      "text" : "We propose SECLR, a model that directly makes relevance classification judgments for queries and sentences of different languages without MT as an intermediate step by learning a cross-lingual embedding space between the two languages. Not only should translation of equivalent words in either language map to similar regions in the embedding space, but dot products between query and sentence words should be correlated with the probability of relevance. We assume the training set generation process (Section 3.1) provides us with a corpus of n query-sentence pairs along\nwith their corresponding relevance judgements, i.e. D = {(qi, Si, ri)}|ni=1. We construct a bilingual vocabulary V = VQ ∪ VS and associate with it a matrix W ∈ Rd×|V| where wx =W·,x is the word embedding associated with word x ∈ V .\nWhen the query is a unigram q (which is true by design in our training data D), we model the probability of relevance to a sentence S as:\np(r = 1|q, S;W ) = σ ( max s∈S wᵀqws ) where σ denotes the logistic sigmoid (σ(x) = 1/ (1 + exp(−x))).\nIn our evaluation setting, the query is very often a phrase Q = [q1, . . . , q|Q|]. In this case, we require all query words to appear in a sentence in order for a sentence to be considered as relevant. Thus, we modify our relevance model to be:\np(r = 1|Q,S;W ) = σ ( min q∈Q max s∈S wᵀqws ) Our only model parameter is the embedding matrix W which is initialized with pretrained monolingual word embeddings and learned via minimization of the cross entropy of the relevance classification task:\nLrel = − log p(r|q, S;W )"
    }, {
      "heading" : "3.3 Guided Alignment with Rationale Training",
      "text" : "We can improve SECLR by incorporating additional alignment information as a secondary training objective, yielding SECLR-RT. Our intuition is that after training, the word ŝ = argmaxs∈S w ᵀ swq should correspond to a translation of q. However, it is possible that ŝ simply co-occurs frequently with the true translation in our parallel data but its association is coincidental or irrelevant outside the training contexts. We use alignment information to correct for this. We run two SMT word alignment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al., 2009), on the orginal parallel sentence corpus. The two resulting alignments are concatenated as in Zbib et al. (2019) to estimate a unidirectional probabilistic word translation matrix A ∈ [0, 1]|VQ|×|VS |, such that A maps each word in the query language vocabulary to a list of document language words with different probabilities, i.e. Aq,s is the probability of translating q to s and ∑ s∈VS Aq,s = 1.\nFor each relevant training sample, i.e. (q, S, r = 1), we create a rationale distribution ρ ∈ [0, 1]|S|\nwhich is essentially a re-normalization of possible query translations found in S and represents our intuitions about which words s ∈ S that q should be most similar to in embedding space, i.e.\nρs = Aq,s∑\ns′∈S Aq,s′ .\nfor s ∈ S. We similarly create a distribution under our model, α ∈ [0, 1]|S|, where\nαs = exp (wᵀqws)∑\ns′∈S exp (w ᵀ qws′)\nfor s ∈ S. To encourage α to match ρ, we impose a Kullback–Leibler (KL) divergence penalty, denoted as:\nLrat = KL(ρ‖α)\nto our overall loss function. The total loss for a single positive sample then will be a weighted sum of the relevance classification objective and the KL divergence penalty, i.e.\nL = Lrel + λ2Lrat\nwhere λ2 is a relative weight between the classification loss and rationale similarity loss.\nNote that we do not consider rationale loss for the following three types of samples: negative samples, positive samples where the query word is not found in the translation matrix, and positive samples where none of the translations of the query in the matrix are present in the source sentence."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset Generation from Parallel Corpus",
      "text" : "The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A.\n1https://www.iarpa.gov/index.php/ research-programs/material\n2https://dumps.wikimedia.org/"
    }, {
      "heading" : "4.2 Query Sets and Evaluation Sets",
      "text" : "We evaluate our sentence-selection model on English (EN) queries over three collections in SO, SW, and TL recently made available as part of the IARPA MATERIAL program. In contrast to our training data which is synthetic, our evaluation datasets are human-annotated for relevance between real-world multi-domain queries and documents. For each language there are three partitions (Analysis, Dev, and Eval), with the former two being smaller collections intended for system development, and the latter being a larger evaluation corpus. In our main experiments we do not use Analysis or Dev for development and so we report results for all three (the ground truth relevance judgements for the TL Eval collection have not been released yet so we do not report Eval for TL). See Table 3 for evaluation statistics. All queries are text. The speech documents are first transcribed with an ASR system (Ragni and Gales, 2018), and the 1-best ASR output is used in the sentence selection task. Examples of the evaluation datasets are shown in Appendix B. We refer readers to Rubino (2020) for further details about MATERIAL test collections used in this work.\nWhile our model and baselines work at the sentence-level, the MATERIAL relevance judgements are only at the document level. Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and Zo-\nbel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019). Given a document D = [S1, . . . , S|D|], which is a sequence of sentences, and a query Q, following Liu and Croft (2002) we assign a relevance score by:\nr̂ = max S∈D\np(r = 1|Q,S;W )"
    }, {
      "heading" : "4.3 Experiment Settings",
      "text" : "We initialize English word embeddings with word2vec (Mikolov et al., 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al., 2018). For training we use a SparseAdam (Kingma and Ba, 2015) optimizer with learning rate 0.001. The hyperparameter λ2 in Section 3.3 is set to be 3 so that Lrel and λ2Lrat are approximately on the same scale during training. More details on experiments are included in Appendix C."
    }, {
      "heading" : "4.4 Baselines",
      "text" : "Cross-Lingual Word Embeddings. We compare our model with three other cross-lingual embedding methods, Bivec (Luong et al., 2015), MUSE (Lample et al., 2018), and SID-SGNS (Levy et al., 2017). Bivec and SID-SGNS are trained using the same parallel sentence corpus as the dataset generation algorithm used to train SECLR; thus, Bivec and SID-SGNS are trained on parallel sentences while SECLR is trained on query-sentence pairs derived from that corpus. We train MUSE with the bilingual dictionary from Wiktionary that is used in previous work (Zhang et al., 2019). The SO-EN, SW-EN and TL-EN dictionaries have 7633, 5301, and 7088 words respectively. Given embeddings W ′ from any of these methods, we compute sentence level relevance scores similarly to our model but use the cosine similarity:\np(r = 1|Q,S;W ′) = min q∈Q max s∈S cos-sim(w′s, w ′ q)\nsince these models are optimized for this comparison function (Luong et al., 2015; Lample et al., 2018; Levy et al., 2017). Document aggregation scoring is handled identically to our SECLR models (see Section 4.2).\nMT+IR. We also compare to a pipeline of NMT (Niu et al., 2018) with monolingual IR and a pipeline of SMT 3 with monolingual IR. Both MT systems are trained on the same parallel sentence\n3We used Moses (Koehn et al., 2007) and KenLM for the language model (Heafield, 2011).\ndata as our SECLR models. The 1-best output from each MT system is then scored with Indri (Strohman et al., 2005) to obtain relevance scores. Details of NMT and SMT systems are included in Appendix C.2.\nPSQ. To implement the PSQ model of Darwish and Oard (2003), we use the same alignment matrix as in rationale training (see Section 3.3) ex-\ncept that here we normalize the matrix such that ∀s ∈ VD, ∑ q∈VQ Aq,s = 1. Additionally, we embed the PSQ scores into a two-state hidden Markov model which smooths the raw PSQ scores with a background unigram language model (Xu and Weischedel, 2000). The PSQ model scores each sentence and then aggregates the scores to document level as in Section 4.2.\nMultilingual XLM-RoBERTa. We compare our model to the cross-lingual model XLM-RoBERTa (Conneau et al., 2020), which in previous research has been shown to have better performance on lowresource languages than multilingual BERT (Devlin et al., 2019). We use the Hugging Face implementation (Wolf et al., 2019) of XLM-RoBERTa (Base). We fine-tuned the model on the same augmented dataset of labeled query-sentence pairs as the SECLR models, but we apply the XLMRoBERTa tokenizer before feeding examples to the model. We fine-tuned the model for four epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 2 × 10−5. Since XLMRoBERTa is pretrained on Somali and Swahili but not Tagalog, we only compare our models to XLMRoBERTa on Somali and Swahili."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "We report Mean Average Precision (MAP) of our main experiment in Table 4 (SO & SW) and Table 5 (TL). Overall, we see that SECLR-RT consistently outperforms the other baselines in 15 out of 16 settings, and in the one case where it is not the best (SW Dev text), SECLR is the best. SECLR-RT is statistically significantly better than the best baseline on all Eval partitions.4 Since Analysis/Dev are relatively small, only three out of 12 Analysis/Dev settings are significant. The differences between SECLR and SECLR-RT can be quite large (e.g., as large as 70.4% relative improvement on SO Eval text), suggesting that the rationale training provides a crucial learning signal to the model.\nBivec and MUSE under-perform both of our model variants across all test conditions, suggesting that for the sentence selection task the relevance classification objective is more important than learning monolingual distributional signals. Curiously, SID-SGNS is quite competitive with SECLR, beating it on SO and SW Eval (both modalities) and TL Dev speech (five out of 16 test conditions) and is competitive with the other baselines. Again, the rationale training proves more effective as SID-SGNS never surpasses SECLR-RT.\nWhile MT+IR is a competitive baseline, it is consistently outperformed by PSQ across all test conditions, suggesting that in low-resource settings it is not necessary to perform full translation to achieve good sentence selection performance. SMT, PSQ, and SECLR-RT all make use of the same word-alignment information but only SMT generates translations, adding additional evidence to this claim. PSQ and SECLR are close in performance on Analysis and Dev sets with SECLR eking out a slight advantage on seven of 12 Anaylsis/Dev set conditions.\nOn the larger Eval partitions, it becomes clearer that PSQ is superior to SECLR, suggesting that the relevance classification objective is not as informative as word alignment information. The relevance classification and trained rationale objectives capture slightly different information it seems; SECLR-RT, which uses both, out-performs PSQ across all 16 test conditions."
    }, {
      "heading" : "6 Training Data Ablation Study",
      "text" : "In Section 5, we have shown that SECLR-RT consistently out-performs all baselines across all languages. Since this work targets cross-language sentence selection in a low-resource setting, we perform a training data ablation study to understand how training data size affects effectiveness.\nWe performed the ablation study for our two models SECLR and SECLR-RT, and the two strongest baseline methods PSQ and SID-SGNS. To simulate further the scenario of data scarcity, we sub-sampled our parallel corpus uniformly at random for 5%, 10%, 25%, 50% of the sentence pairs of the original corpus. Each sentence pair in the parallel corpus is sampled with equal probability regardless of sentence length. For consistency, for each sample size, the same sampled parallel corpus is used across all models. The word alignment probability matrix used by PSQ and SECLR-RT is generated from the same sampled corpus. Since we tune the vocabulary size on the Dev set, for fair comparison we only report MAP scores on the Analysis and Eval sets.\nWe plot MAP scores of the four models as a function of percentage of data sampled in Figure 1. Overall, we see that SECLR-RT consistently outperforms other baselines across all sample sizes in 9 out of 10 settings, and in the one case where it does not yield consistent improvement (Tagalog Analysis speech), SECLR-RT achieves comparable performance to PSQ.\nIn the low-resource setting when the sample size is 5% or 10%, SECLR consistently underperforms other models, confirming our observation that SECLR is sensitive to noise and vulnerable to learning co-occurrences of word pairs that are in fact irrelevant. When the sample size is 5% or 10%, PSQ consistently achieves better performance than SID-SGNS and SECLR (although still under-performing SECLR-RT), indicating that alignment-based methods are more robust to noise and especially useful when data is extremely scarce. The fact that SECLR-RT consistently out-performs SECLR by a wide margin for small sample sizes indicates the necessity and effectiveness of incorporating alignment-based information into SECLR to improve the robustness of the model and learn more precise alignments.\n4We use a two-tailed paired t-test with Bonferroni correction for multiple comparisons at p < 0.01 for all significance tests."
    }, {
      "heading" : "7 Alleviating the Hubness Problem",
      "text" : "In this section, we show that by incorporating alignment information through rationale training, SECLR-RT significantly alleviates the hubness problem present in the trained cross-lingual embedding space produced by SECLR. Previous research on cross-lingual word embeddings has observed that a high-dimensional representation space with a similarity-based metric often induces a hub structure (Dinu and Baroni, 2015). Specifically, in a high-dimensional space (e.g., a cross-lingual word embedding space) defined with a pairwise similarity metric (e.g., cosine similarity), there exist a few vectors that are the nearest neighbors of many other vectors. Such vectors are referred to as “hubs.” The hub structure is problematic in IR since the hub vectors are often wrongly predicted as relevant and similar in meaning to queries that are in fact irrelevant (Radovanović et al., 2010).\nLet VQ and VS be the embedding spaces for the query and sentence collection languages respectively. We define the size of the neighborhood of embeddings around y ∈ VS as\nNk(y) = |{x ∈ VQ|rx(y) ≤ k}|\nwhere rx(y) is the rank of y if we order VS by similarity to x from highest to lowest, and k is a\npositive integer. A large value of Nk(y) indicates that y is similar to many x ∈ VQ, and suggests that y is a likely hub in embedding space.\nFollowing Radovanović et al. (2010), we use SN10 = Ey∈VS [(N10(y)− µ)3/σ3] to measure the skewness of the distribution of N10, where µ and σ refer to the mean and standard deviation of N10(y) respectively. Since cosine similarity is more frequently used as the similarity metric in hubness analysis, we re-train SECLR and SECLR-RT by replacing the dot product similarity metric with cosine similarity and still get performance comparable to Table 4 and Table 5.\nWe report SN10 scores for SECLR and SECLRRT respectively in Table 6. We see that SECLRRT consistently has lower SN10 value compared to SECLR on all three languages, indicating that the extra alignment information incorporated with rationale training is helpful in reducing hubness."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work, we presented a supervised crosslingual embedding-based query relevance model, SECLR, for cross-language sentence selection and also applied a rationale training objective to further increase model performance. The resulting SECLR-RT model outperforms a range of baseline methods on a cross-language sentence selection task. Study of data ablation and hubness further indicate our model’s efficacy in handling lowresource settings and reducing hub structures. In future work, we hope to apply our sentence-level query relevance approach to downstream NLP tasks such as query-focused summarization and opendomain question answering."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA865017-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes not withstanding any copyright annotation therein."
    }, {
      "heading" : "A Extra Training Dataset Details",
      "text" : "When we train SECLR and SECLR-RT via data augmentation, we randomly split the parallel corpus into train set (96%), validation set (3%) and test set (1%). We then use the dataset augmentation technique introduced in Section 3.1 to generate positive and negative samples for each set. Augmenting the dataset upon the split corpus allows us to achieve more independence between train/validation/test set compared to splitting the dataset augmented on the entire parallel corpus. Note that we only use the validation set for early stopping but we do not tune hyperparameters with the validation set.\nWe preprocess the parallel corpus, the query collection and the sentence collection with the Moses toolkit (Koehn et al., 2007). The same preprocessing steps are used for all four languages (English, Somali, Swahili, Tagalog). First, we use Moses puncutation normalizer to normalize the raw text. Second, we use the Moses tokenizer to tokenize the normalized text. Finally, we remove the diacritics in the tokenized text as a cleaning step."
    }, {
      "heading" : "B Examples of Evaluation Data",
      "text" : "In this section we demonstrate some examples from the MATERIAL dataset used for evaluation. Example queries include: “evidence”, “human rights”, “chlorine”, “academy”, “ratify”, “constitution”, “carnage” and “Kenya”. On average only 0.13% of the documents in the Eval collection are relevant to each query, which makes the task hard.\nHere are two examples from Somali Analysis text. Because the documents are long, here we only include the relevant segment of a long relevant document. In the first example, the English query is “contravention” and the relevant segment of a long relevant document (translated from Somali to English by human) is “the security forces captured military equipment coming into the country illegally.” This segment is relevant to the query because of the word “illegally”.\nHere is another example where the the English query is “integrity”. The relevant segment of a long relevant document (translated from Somali to English by human) is “Hargeisa (Dawan) - Ahmed Mohamed Diriye (Nana) the member of parliament who is part of the Somaliland house of representatives has accused the opposition parties (Waddani and UCID) of engaging in acts of national destruction, that undermines the existence and\nsovereignty of the country of Somaliland.” This segment is relevant to the query because of the word “sovereignty”.\nSince there are multiple ways to translate a word and since MT performance is relatively poor in lowresource settings, the task is far more challenging than a simple lexical match between queries and translated documents."
    }, {
      "heading" : "C Extra Experimental Details",
      "text" : "In this section we include extra implementation and experiment details that are not included in the main paper. Information already included in the main paper are not repeated here for conciseness.\nC.1 Model and Training Details We train our SECLR and SECLR-RT models on Tesla V100 GPUs. Each model is trained on a single GPU. We report training time of SECLR and SECLR-RT on Somali, Swahili and Tagalog in Table 7.\nAs is discussed in Section 3.2, the only trainable model parameters of SECLR and SECLR-RT are the word embedding matrices. Thus, SECLR and SECLR-RT have the same number of model parameters. We report the number of trainable parameters of both models on Somali, Swahili and Tagalog in Table 8.\nWe used Mean Average Precision (MAP) as the evaluation metric in this work. We use the following implementation to compute MAP: https: //trec.nist.gov/trec_eval/.\nC.2 MT Baseline Details For NMT we train bidirectional MT systems with a 6-layer Transformer architecture with model size of\n512, feed-forward network size of 2048, 8 attention heads, and residual connections. We adopt layer normalization and label smoothing. We tie the output weight matrix with the source and target embeddings. We use Adam optimizer with a batch size of 2048 words. We checkpoint models every 1000 updates. Training stops after 20 checkpoints without improvement. During inference, the beam size is set to 5.\nOur SMT system uses the following feature functions: phrase translation model, distance-based reordering model, lexicalized reordering model, 5-gram language model on the target side, word penalty, distortion, unknown word penalty and phrase penalty.\nWe use backtranslation in earlier versions of MT systems. Following previous work (Niu et al., 2018), we train a bidirectional NMT model that backtranslates source or target monolingual data without an auxiliary model. This backtranslationbased model was the state-of-the-art MT model on Somali and Swahili when the above paper is published.\nLater, we discover that decoder pretraining with monolingual data achieves better performance compared to backtranslation. The decoder pretraining scheme we use now is most similar to the paper by Ramachandran et al. (2017), where the authors show state-of-the-art results on the WMT English to German translation task with decoder pretraining.\nThere is no WMT benchmark for Somali, Swahili or Tagalog, but we use state-of-the-art techniques in our MT systems. We have also experimented with the bilingual data selection method (Junczys-Dowmunt, 2018). However, this technique does not work well, mostly because lowresource MT systems are not good enough to do scoring."
    }, {
      "heading" : "D Extra Experimental Results",
      "text" : "In this section we include extra experimental results that are not included in the main text due to limited space.\nD.1 SECLR Architecture Exploration\nWhen we are designing the SECLR model, we experiment with adding LSTMs and using the dot product between LSTM hidden states to compute pairwise similarity between the query and the sentence. We report MAP scores of SECLR with LSTM in Table 9. Experimental results show that adding LSTMs reduces model performance consistently across all three languages. We conjecture that in low-resource settings, contextualized models create spurious correlations (Section 3.3). In fact, the XLM-RoBERTa baseline, which captures context effectively via self-attention, also underperforms our SECLR model consistently.\nD.2 Word Embeddings Initialization In our SECLR and SECLR-RT models, we initialize word embeddings with monolingual word embeddings in English, Somali, Swahili and Tagalog (Mikolov et al., 2013; Grave et al., 2018). One natural question is whether we can achieve performance improvement if we directly initialize with crosslingual word embeddings. Because SID-SGNS out-performs both Bivec and MUSE consistently by a wide margin (Table 4 and Table 5), in this experiment we initialize SECLR-RT with the crosslingual embeddings produced by SID-SGNS. The results of monolingual and cross-lingual embedding initialization (SID-SGNS) are shown in Table 10. We see that overall monolingual initialization slightly out-performs cross-lingual initialization. Monolingual initialization yields better performance in eight out of 12 Analysis/Dev set conditions and a MAP improvement of 1.7 points when we take the average across Analysis/Dev and all three languages."
    } ],
    "references" : [ {
      "title" : "Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval",
      "author" : [ "Zeynep Akkalyoncu Yilmaz", "Wei Yang", "Haotian Zhang", "Jimmy Lin." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Yilmaz et al\\.,? 2019",
      "shortCiteRegEx" : "Yilmaz et al\\.",
      "year" : 2019
    }, {
      "title" : "A Robust Self-learning Method for Fully Unsupervised Cross-lingual Mappings of Word Embeddings",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Artetxe et al\\.,? 2018",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "Deriving Machine Attention from Human Rationales",
      "author" : [ "Yujia Bao", "Shiyu Chang", "Mo Yu", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1903–1913, Brussels, Belgium. Association",
      "citeRegEx" : "Bao et al\\.,? 2018",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2018
    }, {
      "title" : "Topic Concentration in Query Focused Summarization Datasets",
      "author" : [ "Tal Baumel", "Raphael Cohen", "Michael Elhadad." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages",
      "citeRegEx" : "Baumel et al\\.,? 2016",
      "shortCiteRegEx" : "Baumel et al\\.",
      "year" : 2016
    }, {
      "title" : "Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models",
      "author" : [ "Tal Baumel", "Matan Eyal", "Michael Elhadad." ],
      "venue" : "CoRR, abs/1801.07704.",
      "citeRegEx" : "Baumel et al\\.,? 2018",
      "shortCiteRegEx" : "Baumel et al\\.",
      "year" : 2018
    }, {
      "title" : "Reading Wikipedia to Answer OpenDomain Questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Guided Alignment Training for Topic-Aware Neural Machine Translation",
      "author" : [ "Wenhu Chen", "Evgeny Matusov", "Shahram Khadivi", "Jan-Thorsten Peter." ],
      "venue" : "CoRR, abs/1607.01628.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Overview of the DARPA LORELEI Program",
      "author" : [ "Caitlin Christianson", "Jason Duncan", "Boyan A. Onyshkevych." ],
      "venue" : "Machine Translation, 32(12):3–9.",
      "citeRegEx" : "Christianson et al\\.,? 2018",
      "shortCiteRegEx" : "Christianson et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised Cross-lingual Representation Learning at Scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Probabilistic Structured Query Methods",
      "author" : [ "Kareem Darwish", "Douglas W. Oard." ],
      "venue" : "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03, page 338–344, New York, NY,",
      "citeRegEx" : "Darwish and Oard.,? 2003",
      "shortCiteRegEx" : "Darwish and Oard.",
      "year" : 2003
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Quasar: Datasets for Question Answering by Search and Reading",
      "author" : [ "Bhuwan Dhingra", "Kathryn Mazaitis", "William W. Cohen." ],
      "venue" : "CoRR, abs/1707.03904.",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving Zero-shot Learning by Mitigating the Hubness Problem",
      "author" : [ "Georgiana Dinu", "Marco Baroni." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings.",
      "citeRegEx" : "Dinu and Baroni.,? 2015",
      "shortCiteRegEx" : "Dinu and Baroni.",
      "year" : 2015
    }, {
      "title" : "Modeling Diverse Relevance Patterns in Ad-hoc Retrieval",
      "author" : [ "Yixing Fan", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Chengxiang Zhai", "Xueqi Cheng." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval,",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised QueryFocused Multi-Document Summarization Using the Cross Entropy Method",
      "author" : [ "Guy Feigenblat", "Haggai Roitman", "Odellia Boni", "David Konopnicki." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Re-",
      "citeRegEx" : "Feigenblat et al\\.,? 2017",
      "shortCiteRegEx" : "Feigenblat et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning Word Vectors for 157 Languages",
      "author" : [ "Edouard Grave", "Piotr Bojanowski", "Prakhar Gupta", "Armand Joulin", "Tomás Mikolov." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki,",
      "citeRegEx" : "Grave et al\\.,? 2018",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2018
    }, {
      "title" : "Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization",
      "author" : [ "Surabhi Gupta", "Ani Nenkova", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion",
      "citeRegEx" : "Gupta et al\\.,? 2007",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2007
    }, {
      "title" : "Better Word Alignments with Supervised ITG Models",
      "author" : [ "Aria Haghighi", "John Blitzer", "John DeNero", "Dan Klein." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural",
      "citeRegEx" : "Haghighi et al\\.,? 2009",
      "shortCiteRegEx" : "Haghighi et al\\.",
      "year" : 2009
    }, {
      "title" : "KenLM: Faster and Smaller Language Model Queries",
      "author" : [ "Kenneth Heafield." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland. Association for Computational Linguistics.",
      "citeRegEx" : "Heafield.,? 2011",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Studying Topical Relevance with EvidenceBased Crowdsourcing",
      "author" : [ "Oana Inel", "Giannis Haralabopoulos", "Dan Li", "Christophe Van Gysel", "Zoltán Szlávik", "Elena Simperl", "Evangelos Kanoulas", "Lora Aroyo." ],
      "venue" : "Proceedings of the 27th",
      "citeRegEx" : "Inel et al\\.,? 2018",
      "shortCiteRegEx" : "Inel et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is not Explanation",
      "author" : [ "Sarthak Jain", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion",
      "author" : [ "Armand Joulin", "Piotr Bojanowski", "Tomas Mikolov", "Hervé Jégou", "Edouard Grave." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Joulin et al\\.,? 2018",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2018
    }, {
      "title" : "Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 888–895, Belgium, Brussels. Association for Computational",
      "citeRegEx" : "Junczys.Dowmunt.,? 2018",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2018
    }, {
      "title" : "Open-Domain Question Answering using Feature Encoded Dynamic Coattention Networks",
      "author" : [ "S. Kale", "A. Kulkarni", "R. Patil", "Y. Haribhakta", "K. Bhattacharjee", "S. Mehta", "S. Mithran", "A. Kumar." ],
      "venue" : "2018 International Conference on Advances in Comput-",
      "citeRegEx" : "Kale et al\\.,? 2018",
      "shortCiteRegEx" : "Kale et al\\.",
      "year" : 2018
    }, {
      "title" : "Panlex: Building a Resource for Panlingual Lexical Translation",
      "author" : [ "David Kamholz", "Jonathan Pool", "Susan M. Colowick." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Ice-",
      "citeRegEx" : "Kamholz et al\\.,? 2014",
      "shortCiteRegEx" : "Kamholz et al\\.",
      "year" : 2014
    }, {
      "title" : "Passage Retrieval Revisited",
      "author" : [ "Marcin Kaszkiel", "Justin Zobel." ],
      "venue" : "Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’97, page 178–185, New York, NY, USA. Associa-",
      "citeRegEx" : "Kaszkiel and Zobel.,? 1997",
      "shortCiteRegEx" : "Kaszkiel and Zobel.",
      "year" : 1997
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Moses: Open Source Toolkit for Statistical Machine Translation",
      "author" : [ "Richard Zens", "Chris Dyer", "Ondřej Bojar", "Alexandra Constantin", "Evan Herbst." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Compan-",
      "citeRegEx" : "Zens et al\\.,? 2007",
      "shortCiteRegEx" : "Zens et al\\.",
      "year" : 2007
    }, {
      "title" : "Word Translation without Parallel Data",
      "author" : [ "Guillaume Lample", "Alexis Conneau", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : "In 6th International Conference on Learning Representations,",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments",
      "author" : [ "Omer Levy", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating Resource-Lean CrossLingual Embedding Models in Unsupervised Retrieval",
      "author" : [ "Robert Litschko", "Goran Glavaš", "Ivan Vulic", "Laura Dietz." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Devel-",
      "citeRegEx" : "Litschko et al\\.,? 2019",
      "shortCiteRegEx" : "Litschko et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised Cross-Lingual Information Retrieval Using Monolingual Data Only",
      "author" : [ "Robert Litschko", "Goran Glavaš", "Simone Paolo Ponzetto", "Ivan Vulić." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in In-",
      "citeRegEx" : "Litschko et al\\.,? 2018",
      "shortCiteRegEx" : "Litschko et al\\.",
      "year" : 2018
    }, {
      "title" : "Passage Retrieval Based on Language Models",
      "author" : [ "Xiaoyong Liu", "W. Bruce Croft." ],
      "venue" : "Proceedings of the Eleventh International Conference on Information and Knowledge Management, CIKM ’02, page 375–382, New York, NY, USA. Association for",
      "citeRegEx" : "Liu and Croft.,? 2002",
      "shortCiteRegEx" : "Liu and Croft.",
      "year" : 2002
    }, {
      "title" : "Decoupled Weight Decay Regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Bilingual Word Representations with Monolingual Quality in Mind",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151–159, Denver, Col-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Cross-Language Information Retrieval",
      "author" : [ "Jian-yun Nie." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 3:1–125.",
      "citeRegEx" : "Nie.,? 2010",
      "shortCiteRegEx" : "Nie.",
      "year" : 2010
    }, {
      "title" : "Bi-Directional Neural Machine Translation with Synthetic Parallel Data",
      "author" : [ "Xing Niu", "Michael Denkowski", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation.",
      "citeRegEx" : "Niu et al\\.,? 2018",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2018
    }, {
      "title" : "A Systematic Comparison of Various Statistical Alignment Models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational Linguistics, 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "On the Existence of Obstinate Results in Vector Space Models",
      "author" : [ "Milos Radovanović", "Alexandros Nanopoulos", "Mirjana Ivanović." ],
      "venue" : "Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Radovanović et al\\.,? 2010",
      "shortCiteRegEx" : "Radovanović et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic Speech Recognition System Development in the “Wild",
      "author" : [ "Anton Ragni", "Mark Gales." ],
      "venue" : "Proc. Interspeech 2018, pages 2217–2221.",
      "citeRegEx" : "Ragni and Gales.,? 2018",
      "shortCiteRegEx" : "Ragni and Gales.",
      "year" : 2018
    }, {
      "title" : "Unsupervised Pretraining for Sequence to Sequence Learning",
      "author" : [ "Prajit Ramachandran", "Peter Liu", "Quoc Le." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 383–391, Copenhagen, Denmark. Association",
      "citeRegEx" : "Ramachandran et al\\.,? 2017",
      "shortCiteRegEx" : "Ramachandran et al\\.",
      "year" : 2017
    }, {
      "title" : "The Effect of Linguistic Parameters in CLIR Performance",
      "author" : [ "Carl Rubino." ],
      "venue" : "Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020), pages 1– 6, Marseille, France. European Language Resources",
      "citeRegEx" : "Rubino.,? 2020",
      "shortCiteRegEx" : "Rubino.",
      "year" : 2020
    }, {
      "title" : "Indri: A Language Modelbased Search Engine for Complex Queries",
      "author" : [ "Trevor Strohman", "Donald Metzler", "Howard Turtle", "W Bruce Croft." ],
      "venue" : "Proceedings of the international conference on intelligent analysis, volume 2, pages 2–6. Citeseer.",
      "citeRegEx" : "Strohman et al\\.,? 2005",
      "shortCiteRegEx" : "Strohman et al\\.",
      "year" : 2005
    }, {
      "title" : "Parallel Data, Tools and Interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 2325, 2012, pages 2214–2218. European Language Re-",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings",
      "author" : [ "Ivan Vulić", "Marie-Francine Moens." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Informa-",
      "citeRegEx" : "Vulić and Moens.,? 2015",
      "shortCiteRegEx" : "Vulić and Moens.",
      "year" : 2015
    }, {
      "title" : "Passage Retrieval and Evaluation",
      "author" : [ "Courtney Wade", "James Allan." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Wade and Allan.,? 2005",
      "shortCiteRegEx" : "Wade and Allan.",
      "year" : 2005
    }, {
      "title" : "HuggingFace’s Transformers: State-of-the-art Natural Language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-Lingual Information Retrieval Using Hidden Markov Models",
      "author" : [ "Jinxi Xu", "Ralph Weischedel." ],
      "venue" : "Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Con-",
      "citeRegEx" : "Xu and Weischedel.,? 2000",
      "shortCiteRegEx" : "Xu and Weischedel.",
      "year" : 2000
    }, {
      "title" : "NeuralNetwork Lexical Translation for Cross-Lingual IR",
      "author" : [ "Rabih Zbib", "Lingjun Zhao", "Damianos Karakos", "William Hartmann", "Jay DeYoung", "Zhongqiang Huang", "Zhuolin Jiang", "Noah Rivkin", "Le Zhang", "Richard Schwartz", "John Makhoul" ],
      "venue" : null,
      "citeRegEx" : "Zbib et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zbib et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations",
      "author" : [ "Rui Zhang", "Caitlin Westerfield", "Sungrok Shim", "Garrett Bingham", "Alexander Fabbri", "William Hu", "Neha Verma", "Dragomir Radev" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "While we can use machine translation (MT) to translate either the query or each sentence into a common language, and then use a monolingual Information Retrieval (IR) system to find relevant sentences, work on Probabilistic Structured Queries (PSQ) (Darwish and Oard, 2003) has shown that the performance of such MT+IR pipelines is hindered by errors in MT.",
      "startOffset" : 249,
      "endOffset" : 273
    }, {
      "referenceID" : 37,
      "context" : "With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011),",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011),",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 34,
      "context" : "3882 three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : ", 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : ", 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 48,
      "context" : ", 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al.",
      "startOffset" : 43,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : ", 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019).",
      "startOffset" : 189,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "Finally, (iv) we conduct training data ablation and hubness studies that show our method’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanović et al., 2010).",
      "startOffset" : 170,
      "endOffset" : 219
    }, {
      "referenceID" : 39,
      "context" : "Finally, (iv) we conduct training data ablation and hubness studies that show our method’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanović et al., 2010).",
      "startOffset" : 170,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "Query-focused Sentence Selection Sentencelevel query relevance prediction is important for various downstream NLP tasks such as queryfocused summarization (Baumel et al., 2016, 2018; Feigenblat et al., 2017) and open-domain question answering (Chen et al.",
      "startOffset" : 155,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : ", 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : ", 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : ", 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 36,
      "context" : "Cross-language Sentence Selection A common approach to cross-language sentence selection is to use MT to first translate either the query or the sentence to the same language and then perform standard monolingual IR (Nie, 2010).",
      "startOffset" : 216,
      "endOffset" : 227
    }, {
      "referenceID" : 9,
      "context" : "As an alternative to generating full translations, PSQ (Darwish and Oard, 2003) uses wordalignments from SMT to obtain weighted query term counts in the passage collection.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 45,
      "context" : "Cross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vulić and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018).",
      "startOffset" : 194,
      "endOffset" : 267
    }, {
      "referenceID" : 21,
      "context" : "Cross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vulić and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018).",
      "startOffset" : 194,
      "endOffset" : 267
    }, {
      "referenceID" : 29,
      "context" : "Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al.",
      "startOffset" : 64,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al.",
      "startOffset" : 64,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : ", 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings.",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings.",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019).",
      "startOffset" : 319,
      "endOffset" : 343
    }, {
      "referenceID" : 2,
      "context" : "Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018).",
      "startOffset" : 168,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "3883 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 38,
      "context" : "ment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "ment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al., 2009), on the orginal parallel sentence corpus.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "allel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English).",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 44,
      "context" : "Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 40,
      "context" : "The speech documents are first transcribed with an ASR system (Ragni and Gales, 2018), and the 1-best ASR output is used in the sentence selection task.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and Zobel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019).",
      "startOffset" : 141,
      "endOffset" : 258
    }, {
      "referenceID" : 46,
      "context" : "Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and Zobel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019).",
      "startOffset" : 141,
      "endOffset" : 258
    }, {
      "referenceID" : 13,
      "context" : "Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and Zobel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019).",
      "startOffset" : 141,
      "endOffset" : 258
    }, {
      "referenceID" : 19,
      "context" : "Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and Zobel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019).",
      "startOffset" : 141,
      "endOffset" : 258
    }, {
      "referenceID" : 35,
      "context" : "We initialize English word embeddings with word2vec (Mikolov et al., 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : ", 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 26,
      "context" : "For training we use a SparseAdam (Kingma and Ba, 2015) optimizer with learning rate 0.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : "We compare our model with three other cross-lingual embedding methods, Bivec (Luong et al., 2015), MUSE (Lample et al.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : ", 2015), MUSE (Lample et al., 2018), and SID-SGNS (Levy et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 50,
      "context" : "We train MUSE with the bilingual dictionary from Wiktionary that is used in previous work (Zhang et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : "since these models are optimized for this comparison function (Luong et al., 2015; Lample et al., 2018; Levy et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "since these models are optimized for this comparison function (Luong et al., 2015; Lample et al., 2018; Levy et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 122
    }, {
      "referenceID" : 29,
      "context" : "since these models are optimized for this comparison function (Luong et al., 2015; Lample et al., 2018; Levy et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 122
    }, {
      "referenceID" : 37,
      "context" : "We also compare to a pipeline of NMT (Niu et al., 2018) with monolingual IR and a pipeline of SMT 3 with monolingual IR.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : ", 2007) and KenLM for the language model (Heafield, 2011).",
      "startOffset" : 41,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : "The 1-best output from each MT system is then scored with Indri (Strohman et al., 2005) to obtain relevance scores.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 48,
      "context" : "Additionally, we embed the PSQ scores into a two-state hidden Markov model which smooths the raw PSQ scores with a background unigram language model (Xu and Weischedel, 2000).",
      "startOffset" : 149,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "We compare our model to the cross-lingual model XLM-RoBERTa (Conneau et al., 2020), which in previous research has been shown to have better performance on lowresource languages than multilingual BERT (De-",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 47,
      "context" : "We use the Hugging Face implementation (Wolf et al., 2019) of XLM-RoBERTa (Base).",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 33,
      "context" : "We fine-tuned the model for four epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 2 × 10−5.",
      "startOffset" : 65,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "a similarity-based metric often induces a hub structure (Dinu and Baroni, 2015).",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 39,
      "context" : "” The hub structure is problematic in IR since the hub vectors are often wrongly predicted as relevant and similar in meaning to queries that are in fact irrelevant (Radovanović et al., 2010).",
      "startOffset" : 165,
      "endOffset" : 191
    } ],
    "year" : 2021,
    "abstractText" : "This paper proposes an approach to crosslanguage sentence selection in a low-resource setting. It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model. Results show that this approach performs as well as or better than multiple state-of-theart machine translation + monolingual retrieval systems trained on the same parallel data. Moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (EnglishSomali, English-Swahili and English-Tagalog) over a variety of state-of-the-art baselines.",
    "creator" : "LaTeX with hyperref"
  }
}