{
  "name" : "2021.acl-long.11.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances",
    "authors" : [ "Zekang Li", "Jinchao Zhang", "Zhengcong Fei", "Yang Feng", "Jie Zhou" ],
    "emails" : [ "lizekang19g@ict.ac.cn", "feizhengcong@ict.ac.cn", "fengyang@ict.ac.cn", "dayerzhang@tencent.com", "withtomzhou@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 128–138\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n128"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However,\n∗Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI.\n1https://github.com/ictnlp/DialoFlow\neffectively modeling the dialogue history in largescale dialogue pre-training is still challenging.\nMost of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utterance, while the history information is essential for understanding dialogue utterances. Thus, all the aforementioned methods are deficient in modeling the dynamic information in the dialogue history.\nIn this work, inspired by the human cognitive\nprocess that humans always consider the goal or influence of the next response before they continue the conversation (Brown-Schmidt and Konopka, 2015), we propose the DialoFlow to model the dynamic information flow in the dialogue history by addressing the semantic influence brought about by each utterance. As shown in Figure 1, we define the dense representation of the dialogue history at different utterances as the contexts (gray dot line) and the context transformation as the semantic influence brought by each utterance. In particular, our DialoFlow constructs the process of the utterancelevel history context flow. Correspondingly, the semantic influence of each utterance can be measured by the difference between two adjacent contexts, which will be further used to guide the current response generation.\nPractically, we first employ a transformer to encode the whole conversation to get the dense context representation. Then we design a unidirectional Flow module to capture the context flow on the utterance level, and design three training objectives to model the context flow and measure the semantic influence brought about by each utterance: 1) Context Flow Modeling, which aims to capture the context flow schema. 2) Semantic Influence Modeling, which targets to measure the predicted semantic influence. 3) Response Generation Modeling, which is to generate the response under the guidance of the predicted semantic influence. Furthermore, to demonstrate the effect of modeling dynamic information flow in the dialogue understanding, we propose the Flow score based on the DialoFlow, an automatic reference-free evaluation metric for interactive dialogue evaluation by measuring the semantic influence perplexity.\nWe pre-train the proposed DialoFlow on the large-scale Reddit comments and conduct experiments on dialogue generation and interactive dialogue quality evaluation. For dialogue generation, DialoFlow achieves significant improvements on the Reddit multi-reference dataset and the DailyDialog dataset compared to the baseline DialoGPT (Zhang et al., 2020). For interactive dialogue quality evaluation, our proposed Flow score obtains an impressively high chatbot-level correlation (r = 0.9) with human ratings on 2200 human-bot dialogues from 11 chatbots.\nOur contributions are summarized as follows:\n• We propose the DialoFlow, a new paradigm to construct the dynamic information flow in\nthe dialogue history by addressing the semantic influence brought about by each utterance. Besides, we design an automatic referencefree evaluation metric Flow score based on the pre-trained DialoFlow for interactive dialogue quality evaluation.\n• The experimental results illustrate that DialoFlow achieves significant improvements on dialogue generation compared to the DialoGPT, and Flow score shows impressively high chatbot-level correlation (r = 0.9) with human ratings."
    }, {
      "heading" : "2 Method",
      "text" : "The proposed DialoFlow models the dynamic information flow in the whole dialogue history by addressing the semantic influence brought about by each utterance in sequence."
    }, {
      "heading" : "2.1 Model Overview",
      "text" : "Before introducing the DialoFlow in detail, we first define some terms. Formally, let D = {u1, u2, ..., uN} denotes a whole dialogue. And for each utterance uk = {u1k, u2k, ..., uTk } where utk denotes the t-th word in the k-th utterance. We further denote u<k = {u1, u2, ..., uk−1} as the dialogue history at the k-th utterance. Besides, the dense representation of the dialogue history u<k at the k-th utterance is represented as the context Ck. And the difference between the new context Ck+1 at the (k+1)-th utterance and the previous contexts Ck at the k-th utterance can be defined as the semantic influence Ik of the k-th utterance, which can be formulated as:\nIk = Ck+1 −Ck. (1)\nIn our method, DialoFlow first encodes the dialogue history and predicts the future context C′k+1 according to all the previous history context C1,C2, ...,Ck. Then at the response generation stage, the model acquires the predicted target semantic influence I′k, and generate the target response uk auto-regressively considering both the predicted semantic influence and the historical subsentences. Specifically, as shown in Figure 2, DialoFlow models the context flow by designing a unidirectional Flow module upon the transformer, and we introduce three multi-task training objectives to supervise the context flow, semantic influence, and\nLayer Norm\nMulti-Head Attention\nLayer Norm\nFeed Forward Transformer Block L-1\nTransformer Block L\n[C]\nFlow Module PE+ PE+\nC2 <latexit sha1_base64=\"2OjzjvwvlGQCh7kVC/I3JEQD7P8=\">AAACz3icjVHLSsNAFD2Nr/quunQTLIKrkhRBl8VuXLZgH9CWMplO29C8SCZKKRW3/oBb/SvxD/QvvDOm4APRCUnOnHvPmbn3OpHnJtKyXnLG0vLK6lp+fWNza3tnt7C330zCNOaiwUMvjNsOS4TnBqIhXemJdhQL5jueaDmTqoq3rkWcuGFwJaeR6PlsFLhDlzNJVLfrMzl2hrPqvF/uF4pWydLL/AnsDBSRrVpYeEYXA4TgSOFDIIAk7IEhoacDGxYi4nqYERcTcnVcYI4N0qaUJSiDETuh74h2nYwNaK88E63mdIpHb0xKE8ekCSkvJqxOM3U81c6K/c17pj3V3ab0dzIvn1iJMbF/6RaZ/9WpWiSGONc1uFRTpBlVHc9cUt0VdXPzU1WSHCLiFB5QPCbMtXLRZ1NrEl276i3T8VedqVi151luijd1Sxqw/X2cP0GzXLKtkl0/LVYuslHncYgjnNA8z1DBJWpokHeEBzziyagbN8atcfeRauQyzQG+LOP+HfaelA0=</latexit><latexit sha1_base64=\"2OjzjvwvlGQCh7kVC/I3JEQD7P8=\">AAACz3icjVHLSsNAFD2Nr/quunQTLIKrkhRBl8VuXLZgH9CWMplO29C8SCZKKRW3/oBb/SvxD/QvvDOm4APRCUnOnHvPmbn3OpHnJtKyXnLG0vLK6lp+fWNza3tnt7C330zCNOaiwUMvjNsOS4TnBqIhXemJdhQL5jueaDmTqoq3rkWcuGFwJaeR6PlsFLhDlzNJVLfrMzl2hrPqvF/uF4pWydLL/AnsDBSRrVpYeEYXA4TgSOFDIIAk7IEhoacDGxYi4nqYERcTcnVcYI4N0qaUJSiDETuh74h2nYwNaK88E63mdIpHb0xKE8ekCSkvJqxOM3U81c6K/c17pj3V3ab0dzIvn1iJMbF/6RaZ/9WpWiSGONc1uFRTpBlVHc9cUt0VdXPzU1WSHCLiFB5QPCbMtXLRZ1NrEl276i3T8VedqVi151luijd1Sxqw/X2cP0GzXLKtkl0/LVYuslHncYgjnNA8z1DBJWpokHeEBzziyagbN8atcfeRauQyzQG+LOP+HfaelA0=</latexit><latexit sha1_base64=\"2OjzjvwvlGQCh7kVC/I3JEQD7P8=\">AAACz3icjVHLSsNAFD2Nr/quunQTLIKrkhRBl8VuXLZgH9CWMplO29C8SCZKKRW3/oBb/SvxD/QvvDOm4APRCUnOnHvPmbn3OpHnJtKyXnLG0vLK6lp+fWNza3tnt7C330zCNOaiwUMvjNsOS4TnBqIhXemJdhQL5jueaDmTqoq3rkWcuGFwJaeR6PlsFLhDlzNJVLfrMzl2hrPqvF/uF4pWydLL/AnsDBSRrVpYeEYXA4TgSOFDIIAk7IEhoacDGxYi4nqYERcTcnVcYI4N0qaUJSiDETuh74h2nYwNaK88E63mdIpHb0xKE8ekCSkvJqxOM3U81c6K/c17pj3V3ab0dzIvn1iJMbF/6RaZ/9WpWiSGONc1uFRTpBlVHc9cUt0VdXPzU1WSHCLiFB5QPCbMtXLRZ1NrEl276i3T8VedqVi151luijd1Sxqw/X2cP0GzXLKtkl0/LVYuslHncYgjnNA8z1DBJWpokHeEBzziyagbN8atcfeRauQyzQG+LOP+HfaelA0=</latexit><latexit sha1_base64=\"2OjzjvwvlGQCh7kVC/I3JEQD7P8=\">AAACz3icjVHLSsNAFD2Nr/quunQTLIKrkhRBl8VuXLZgH9CWMplO29C8SCZKKRW3/oBb/SvxD/QvvDOm4APRCUnOnHvPmbn3OpHnJtKyXnLG0vLK6lp+fWNza3tnt7C330zCNOaiwUMvjNsOS4TnBqIhXemJdhQL5jueaDmTqoq3rkWcuGFwJaeR6PlsFLhDlzNJVLfrMzl2hrPqvF/uF4pWydLL/AnsDBSRrVpYeEYXA4TgSOFDIIAk7IEhoacDGxYi4nqYERcTcnVcYI4N0qaUJSiDETuh74h2nYwNaK88E63mdIpHb0xKE8ekCSkvJqxOM3U81c6K/c17pj3V3ab0dzIvn1iJMbF/6RaZ/9WpWiSGONc1uFRTpBlVHc9cUt0VdXPzU1WSHCLiFB5QPCbMtXLRZ1NrEl276i3T8VedqVi151luijd1Sxqw/X2cP0GzXLKtkl0/LVYuslHncYgjnNA8z1DBJWpokHeEBzziyagbN8atcfeRauQyzQG+LOP+HfaelA0=</latexit>\nC03 <latexit sha1_base64=\"/4AFNNMB+VGplUU8zEc+J1/3lgk=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkRhdkVZNdElk4xKNPBIgpB0GaOjLdmokhBi3/oBb/SrjH+hfeGcsiUqMTtP2zLn3nJl7rx26TiwM4zWjzc0vLC5ll3Mrq2vrG/nNrVocJBHjVRa4QdSwrZi7js+rwhEub4QRtzzb5XV7WJbx+g2PYifwr8Qo5G3P6vtOz2GWIKrd8iwxsHvj8qRztN/JF4yioZY+C8wUFJCuSpB/QQtdBGBI4IHDhyDswkJMTxMmDITEtTEmLiLkqDjHBDnSJpTFKcMidkjfPu2aKevTXnrGSs3oFJfeiJQ69kgTUF5EWJ6mq3iinCX7m/dYecq7jehvp14esQIDYv/STTP/q5O1CPRwqmpwqKZQMbI6lrokqivy5vqXqgQ5hMRJ3KV4RJgp5bTPutLEqnbZW0vF31SmZOWepbkJ3uUtacDmz3HOgtph0TSK5sVxoXSWjjqLHezigOZ5ghLOUUGVvK/xiCc8a5farXan3X+maplUs41vS3v4AIAelD8=</latexit><latexit sha1_base64=\"/4AFNNMB+VGplUU8zEc+J1/3lgk=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkRhdkVZNdElk4xKNPBIgpB0GaOjLdmokhBi3/oBb/SrjH+hfeGcsiUqMTtP2zLn3nJl7rx26TiwM4zWjzc0vLC5ll3Mrq2vrG/nNrVocJBHjVRa4QdSwrZi7js+rwhEub4QRtzzb5XV7WJbx+g2PYifwr8Qo5G3P6vtOz2GWIKrd8iwxsHvj8qRztN/JF4yioZY+C8wUFJCuSpB/QQtdBGBI4IHDhyDswkJMTxMmDITEtTEmLiLkqDjHBDnSJpTFKcMidkjfPu2aKevTXnrGSs3oFJfeiJQ69kgTUF5EWJ6mq3iinCX7m/dYecq7jehvp14esQIDYv/STTP/q5O1CPRwqmpwqKZQMbI6lrokqivy5vqXqgQ5hMRJ3KV4RJgp5bTPutLEqnbZW0vF31SmZOWepbkJ3uUtacDmz3HOgtph0TSK5sVxoXSWjjqLHezigOZ5ghLOUUGVvK/xiCc8a5farXan3X+maplUs41vS3v4AIAelD8=</latexit><latexit sha1_base64=\"/4AFNNMB+VGplUU8zEc+J1/3lgk=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkRhdkVZNdElk4xKNPBIgpB0GaOjLdmokhBi3/oBb/SrjH+hfeGcsiUqMTtP2zLn3nJl7rx26TiwM4zWjzc0vLC5ll3Mrq2vrG/nNrVocJBHjVRa4QdSwrZi7js+rwhEub4QRtzzb5XV7WJbx+g2PYifwr8Qo5G3P6vtOz2GWIKrd8iwxsHvj8qRztN/JF4yioZY+C8wUFJCuSpB/QQtdBGBI4IHDhyDswkJMTxMmDITEtTEmLiLkqDjHBDnSJpTFKcMidkjfPu2aKevTXnrGSs3oFJfeiJQ69kgTUF5EWJ6mq3iinCX7m/dYecq7jehvp14esQIDYv/STTP/q5O1CPRwqmpwqKZQMbI6lrokqivy5vqXqgQ5hMRJ3KV4RJgp5bTPutLEqnbZW0vF31SmZOWepbkJ3uUtacDmz3HOgtph0TSK5sVxoXSWjjqLHezigOZ5ghLOUUGVvK/xiCc8a5farXan3X+maplUs41vS3v4AIAelD8=</latexit><latexit sha1_base64=\"/4AFNNMB+VGplUU8zEc+J1/3lgk=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkRhdkVZNdElk4xKNPBIgpB0GaOjLdmokhBi3/oBb/SrjH+hfeGcsiUqMTtP2zLn3nJl7rx26TiwM4zWjzc0vLC5ll3Mrq2vrG/nNrVocJBHjVRa4QdSwrZi7js+rwhEub4QRtzzb5XV7WJbx+g2PYifwr8Qo5G3P6vtOz2GWIKrd8iwxsHvj8qRztN/JF4yioZY+C8wUFJCuSpB/QQtdBGBI4IHDhyDswkJMTxMmDITEtTEmLiLkqDjHBDnSJpTFKcMidkjfPu2aKevTXnrGSs3oFJfeiJQ69kgTUF5EWJ6mq3iinCX7m/dYecq7jehvp14esQIDYv/STTP/q5O1CPRwqmpwqKZQMbI6lrokqivy5vqXqgQ5hMRJ3KV4RJgp5bTPutLEqnbZW0vF31SmZOWepbkJ3uUtacDmz3HOgtph0TSK5sVxoXSWjjqLHezigOZ5ghLOUUGVvK/xiCc8a5farXan3X+maplUs41vS3v4AIAelD8=</latexit>\nResponse Generator\n- =\nToken Embedding\nSegment Embedding\nPosition Embedding\nUtterance 1 Pre-normalization\nTransformer Block 1\n[C]\nUtterance k-1\n[C]\nUtterance k\n…\nh1k<latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit> h 2 k<latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit> h 2 k<latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit><latexit sha1_base64=\"BSM8o213O4Met+w0FNA5d+MVlAk=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVZIi6LLoxmVF+4A+JEmnbWheTCZCKQVx6w+41Z8S/0D/wjvjFHwgOiHJmXPvOTP3XjcJ/FRY1kvOWFhcWl7JrxbW1jc2t4rbO400zrjH6l4cxLzlOikL/IjVhS8C1ko4c0I3YE13fCbjzRvGUz+OrsQkYd3QGUb+wPccQVSvEzpi5A6mo9n1uFe5LpassqWW+RPYGpSgVy0uPqODPmJ4yBCCIYIgHMBBSk8bNiwkxHUxJY4T8lWcYYYCaTPKYpThEDum75B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKk95twn9Xe0VEiswIvYv3TzzvzpZi8AAJ6oGn2pKFCOr87RLproib25+qkqQQ0KcxH2Kc8KeUs77bCpNqmqXvXVU/FVlSlbuPZ2b4U3ekgZsfx/nT9ColG2rbF8claqnetR57GEfhzTPY1Rxjhrq5M3xgEc8GZfGxLg17j5SjZzW7OLLMu7fAYJ2lQ8=</latexit>\nu2k 1<latexit sha1_base64=\"8BT758PbIxiJiVRyaS+2U72IlwA=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaR2aF8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3urHPU2FZrwVjbn5hcam4XFpZXVvfKG9uNdMoSzzW8CI/StqukzKfh6whuPBZO06YE7g+a7mjMxlv3bMk5VF4JcYx6wbOMOQD7jmCqOusl48O7MntYa9csaqWWuYssDWoQK96VH7BDfqI4CFDAIYQgrAPByk9HdiwEBPXRU5cQoirOMMEJdJmlMUowyF2RN8h7TqaDWkvPVOl9ugUn96ElCb2SBNRXkJYnmaqeKacJfubd6485d3G9He1V0CswB2xf+mmmf/VyVoEBjhRNXCqKVaMrM7TLpnqiry5+aUqQQ4xcRL3KZ4Q9pRy2mdTaVJVu+yto+JvKlOycu/p3Azv8pY0YPvnOGdB87BqW1X78qhSO9WjLmIHu9ineR6jhnPU0SDvAI94wrNxYQgjNyafqUZBa7bxbRkPHwKFkno=</latexit><latexit sha1_base64=\"8BT758PbIxiJiVRyaS+2U72IlwA=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaR2aF8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3urHPU2FZrwVjbn5hcam4XFpZXVvfKG9uNdMoSzzW8CI/StqukzKfh6whuPBZO06YE7g+a7mjMxlv3bMk5VF4JcYx6wbOMOQD7jmCqOusl48O7MntYa9csaqWWuYssDWoQK96VH7BDfqI4CFDAIYQgrAPByk9HdiwEBPXRU5cQoirOMMEJdJmlMUowyF2RN8h7TqaDWkvPVOl9ugUn96ElCb2SBNRXkJYnmaqeKacJfubd6485d3G9He1V0CswB2xf+mmmf/VyVoEBjhRNXCqKVaMrM7TLpnqiry5+aUqQQ4xcRL3KZ4Q9pRy2mdTaVJVu+yto+JvKlOycu/p3Azv8pY0YPvnOGdB87BqW1X78qhSO9WjLmIHu9ineR6jhnPU0SDvAI94wrNxYQgjNyafqUZBa7bxbRkPHwKFkno=</latexit><latexit sha1_base64=\"8BT758PbIxiJiVRyaS+2U72IlwA=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaR2aF8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3urHPU2FZrwVjbn5hcam4XFpZXVvfKG9uNdMoSzzW8CI/StqukzKfh6whuPBZO06YE7g+a7mjMxlv3bMk5VF4JcYx6wbOMOQD7jmCqOusl48O7MntYa9csaqWWuYssDWoQK96VH7BDfqI4CFDAIYQgrAPByk9HdiwEBPXRU5cQoirOMMEJdJmlMUowyF2RN8h7TqaDWkvPVOl9ugUn96ElCb2SBNRXkJYnmaqeKacJfubd6485d3G9He1V0CswB2xf+mmmf/VyVoEBjhRNXCqKVaMrM7TLpnqiry5+aUqQQ4xcRL3KZ4Q9pRy2mdTaVJVu+yto+JvKlOycu/p3Azv8pY0YPvnOGdB87BqW1X78qhSO9WjLmIHu9ineR6jhnPU0SDvAI94wrNxYQgjNyafqUZBa7bxbRkPHwKFkno=</latexit><latexit sha1_base64=\"8BT758PbIxiJiVRyaS+2U72IlwA=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaR2aF8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3urHPU2FZrwVjbn5hcam4XFpZXVvfKG9uNdMoSzzW8CI/StqukzKfh6whuPBZO06YE7g+a7mjMxlv3bMk5VF4JcYx6wbOMOQD7jmCqOusl48O7MntYa9csaqWWuYssDWoQK96VH7BDfqI4CFDAIYQgrAPByk9HdiwEBPXRU5cQoirOMMEJdJmlMUowyF2RN8h7TqaDWkvPVOl9ugUn96ElCb2SBNRXkJYnmaqeKacJfubd6485d3G9He1V0CswB2xf+mmmf/VyVoEBjhRNXCqKVaMrM7TLpnqiry5+aUqQQ4xcRL3KZ4Q9pRy2mdTaVJVu+yto+JvKlOycu/p3Azv8pY0YPvnOGdB87BqW1X78qhSO9WjLmIHu9ineR6jhnPU0SDvAI94wrNxYQgjNyafqUZBa7bxbRkPHwKFkno=</latexit> …\n…\nu1k 1<latexit sha1_base64=\"0FNuh+ZhchJ9NFloTrNE6icYLqM=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaQ3Ni8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3ukngp8KyXgvG3PzC4lJxubSyura+Ud7caqZxxj3W8OIg5m3XSVngR6whfBGwdsKZE7oBa7mjMxlv3TOe+nF0JcYJ64bOMPIHvucIoq6zXj46sCe3dq9csaqWWuYssDWoQK96XH7BDfqI4SFDCIYIgnAAByk9HdiwkBDXRU4cJ+SrOMMEJdJmlMUowyF2RN8h7TqajWgvPVOl9uiUgF5OShN7pIkpjxOWp5kqnilnyf7mnStPebcx/V3tFRIrcEfsX7pp5n91shaBAU5UDT7VlChGVudpl0x1Rd7c/FKVIIeEOIn7FOeEPaWc9tlUmlTVLnvrqPibypSs3Hs6N8O7vCUN2P45zlnQPKzaVtW+PKrUTvWoi9jBLvZpnseo4Rx1NMg7xCOe8GxcGMLIjclnqlHQmm18W8bDBwAlknk=</latexit><latexit sha1_base64=\"0FNuh+ZhchJ9NFloTrNE6icYLqM=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaQ3Ni8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3ukngp8KyXgvG3PzC4lJxubSyura+Ud7caqZxxj3W8OIg5m3XSVngR6whfBGwdsKZE7oBa7mjMxlv3TOe+nF0JcYJ64bOMPIHvucIoq6zXj46sCe3dq9csaqWWuYssDWoQK96XH7BDfqI4SFDCIYIgnAAByk9HdiwkBDXRU4cJ+SrOMMEJdJmlMUowyF2RN8h7TqajWgvPVOl9uiUgF5OShN7pIkpjxOWp5kqnilnyf7mnStPebcx/V3tFRIrcEfsX7pp5n91shaBAU5UDT7VlChGVudpl0x1Rd7c/FKVIIeEOIn7FOeEPaWc9tlUmlTVLnvrqPibypSs3Hs6N8O7vCUN2P45zlnQPKzaVtW+PKrUTvWoi9jBLvZpnseo4Rx1NMg7xCOe8GxcGMLIjclnqlHQmm18W8bDBwAlknk=</latexit><latexit sha1_base64=\"0FNuh+ZhchJ9NFloTrNE6icYLqM=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaQ3Ni8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3ukngp8KyXgvG3PzC4lJxubSyura+Ud7caqZxxj3W8OIg5m3XSVngR6whfBGwdsKZE7oBa7mjMxlv3TOe+nF0JcYJ64bOMPIHvucIoq6zXj46sCe3dq9csaqWWuYssDWoQK96XH7BDfqI4SFDCIYIgnAAByk9HdiwkBDXRU4cJ+SrOMMEJdJmlMUowyF2RN8h7TqajWgvPVOl9uiUgF5OShN7pIkpjxOWp5kqnilnyf7mnStPebcx/V3tFRIrcEfsX7pp5n91shaBAU5UDT7VlChGVudpl0x1Rd7c/FKVIIeEOIn7FOeEPaWc9tlUmlTVLnvrqPibypSs3Hs6N8O7vCUN2P45zlnQPKzaVtW+PKrUTvWoi9jBLvZpnseo4Rx1NMg7xCOe8GxcGMLIjclnqlHQmm18W8bDBwAlknk=</latexit><latexit sha1_base64=\"0FNuh+ZhchJ9NFloTrNE6icYLqM=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwY0lE0GXRjSupYB9Sa0nSaQ3Ni8lEKKFbf8Ctfpf4B/oX3hmnoBbRCUnOnHvPmbn3ukngp8KyXgvG3PzC4lJxubSyura+Ud7caqZxxj3W8OIg5m3XSVngR6whfBGwdsKZE7oBa7mjMxlv3TOe+nF0JcYJ64bOMPIHvucIoq6zXj46sCe3dq9csaqWWuYssDWoQK96XH7BDfqI4SFDCIYIgnAAByk9HdiwkBDXRU4cJ+SrOMMEJdJmlMUowyF2RN8h7TqajWgvPVOl9uiUgF5OShN7pIkpjxOWp5kqnilnyf7mnStPebcx/V3tFRIrcEfsX7pp5n91shaBAU5UDT7VlChGVudpl0x1Rd7c/FKVIIeEOIn7FOeEPaWc9tlUmlTVLnvrqPibypSs3Hs6N8O7vCUN2P45zlnQPKzaVtW+PKrUTvWoi9jBLvZpnseo4Rx1NMg7xCOe8GxcGMLIjclnqlHQmm18W8bDBwAlknk=</latexit> u 1 k<latexit sha1_base64=\"ezTQapOyOpLhgdL0OcB4k+rCMxo=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4L1sLi0vJKcbW0tr6xuVXe3mkmUSoZb7AoiGTb9xIeiJA3lFABb8eSe2M/4C1/dK7jrXsuExGF12oS8+7YG4ZiIJiniGqlvWw0vXV75YpTdcyy54GbgwryVY/KL7hBHxEYUozBEUIRDuAhoacDFw5i4rrIiJOEhIlzTFEib0oqTgqP2BF9h7Tr5GxIe50zMW5GpwT0SnLaOCBPRDpJWJ9mm3hqMmv2t9yZyanvNqG/n+caE6twR+xfvpnyvz5di8IAp6YGQTXFhtHVsTxLarqib25/qUpRhpg4jfsUl4SZcc76bBtPYmrXvfVM/M0oNav3LNemeNe3pAG7P8c5D5pHVdepulfHldpZPuoi9rCPQ5rnCWq4QB0NU+UjnvBsXVrSmljZp9Qq5J5dfFvWwwfQcpIH</latexit><latexit sha1_base64=\"ezTQapOyOpLhgdL0OcB4k+rCMxo=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4L1sLi0vJKcbW0tr6xuVXe3mkmUSoZb7AoiGTb9xIeiJA3lFABb8eSe2M/4C1/dK7jrXsuExGF12oS8+7YG4ZiIJiniGqlvWw0vXV75YpTdcyy54GbgwryVY/KL7hBHxEYUozBEUIRDuAhoacDFw5i4rrIiJOEhIlzTFEib0oqTgqP2BF9h7Tr5GxIe50zMW5GpwT0SnLaOCBPRDpJWJ9mm3hqMmv2t9yZyanvNqG/n+caE6twR+xfvpnyvz5di8IAp6YGQTXFhtHVsTxLarqib25/qUpRhpg4jfsUl4SZcc76bBtPYmrXvfVM/M0oNav3LNemeNe3pAG7P8c5D5pHVdepulfHldpZPuoi9rCPQ5rnCWq4QB0NU+UjnvBsXVrSmljZp9Qq5J5dfFvWwwfQcpIH</latexit><latexit sha1_base64=\"ezTQapOyOpLhgdL0OcB4k+rCMxo=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4L1sLi0vJKcbW0tr6xuVXe3mkmUSoZb7AoiGTb9xIeiJA3lFABb8eSe2M/4C1/dK7jrXsuExGF12oS8+7YG4ZiIJiniGqlvWw0vXV75YpTdcyy54GbgwryVY/KL7hBHxEYUozBEUIRDuAhoacDFw5i4rrIiJOEhIlzTFEib0oqTgqP2BF9h7Tr5GxIe50zMW5GpwT0SnLaOCBPRDpJWJ9mm3hqMmv2t9yZyanvNqG/n+caE6twR+xfvpnyvz5di8IAp6YGQTXFhtHVsTxLarqib25/qUpRhpg4jfsUl4SZcc76bBtPYmrXvfVM/M0oNav3LNemeNe3pAG7P8c5D5pHVdepulfHldpZPuoi9rCPQ5rnCWq4QB0NU+UjnvBsXVrSmljZp9Qq5J5dfFvWwwfQcpIH</latexit><latexit sha1_base64=\"ezTQapOyOpLhgdL0OcB4k+rCMxo=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4L1sLi0vJKcbW0tr6xuVXe3mkmUSoZb7AoiGTb9xIeiJA3lFABb8eSe2M/4C1/dK7jrXsuExGF12oS8+7YG4ZiIJiniGqlvWw0vXV75YpTdcyy54GbgwryVY/KL7hBHxEYUozBEUIRDuAhoacDFw5i4rrIiJOEhIlzTFEib0oqTgqP2BF9h7Tr5GxIe50zMW5GpwT0SnLaOCBPRDpJWJ9mm3hqMmv2t9yZyanvNqG/n+caE6twR+xfvpnyvz5di8IAp6YGQTXFhtHVsTxLarqib25/qUpRhpg4jfsUl4SZcc76bBtPYmrXvfVM/M0oNav3LNemeNe3pAG7P8c5D5pHVdepulfHldpZPuoi9rCPQ5rnCWq4QB0NU+UjnvBsXVrSmljZp9Qq5J5dfFvWwwfQcpIH</latexit> u 2 k<latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit>\nCk+1 <latexit sha1_base64=\"aOJPCuP2SKS5N5+jXxP8AHPUq50=\">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkUQhJKIoMtiNy4r2Ae0pSTptIbmRTIplJKduPUH3OoviX+gf+GdcQpqEZ2Q5My595yZe68T+17KTfO1oC0tr6yuFddLG5tb2zv67l4zjbLEZQ038qOk7dgp872QNbjHfdaOE2YHjs9azrgm4q0JS1IvCm/4NGa9wB6F3tBzbU5UX9e7gc1vneGslvdn4xMr7+tls2LKZSwCS4Ey1KpH+gu6GCCCiwwBGEJwwj5spPR0YMFETFwPM+ISQp6MM+QokTajLEYZNrFj+o5o11FsSHvhmUq1S6f49CakNHBEmojyEsLiNEPGM+ks2N+8Z9JT3G1Kf0d5BcRy3BL7l26e+V+dqIVjiAtZg0c1xZIR1bnKJZNdETc3vlTFySEmTuABxRPCrlTO+2xITSprF721ZfxNZgpW7F2Vm+Fd3JIGbP0c5yJonlYss2Jdn5Wrl2rURRzgEMc0z3NUcYU6GuQ9wSOe8Ky1tFy70+4/U7WC0uzj29IePgDk+ZXz</latexit><latexit sha1_base64=\"aOJPCuP2SKS5N5+jXxP8AHPUq50=\">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkUQhJKIoMtiNy4r2Ae0pSTptIbmRTIplJKduPUH3OoviX+gf+GdcQpqEZ2Q5My595yZe68T+17KTfO1oC0tr6yuFddLG5tb2zv67l4zjbLEZQ038qOk7dgp872QNbjHfdaOE2YHjs9azrgm4q0JS1IvCm/4NGa9wB6F3tBzbU5UX9e7gc1vneGslvdn4xMr7+tls2LKZSwCS4Ey1KpH+gu6GCCCiwwBGEJwwj5spPR0YMFETFwPM+ISQp6MM+QokTajLEYZNrFj+o5o11FsSHvhmUq1S6f49CakNHBEmojyEsLiNEPGM+ks2N+8Z9JT3G1Kf0d5BcRy3BL7l26e+V+dqIVjiAtZg0c1xZIR1bnKJZNdETc3vlTFySEmTuABxRPCrlTO+2xITSprF721ZfxNZgpW7F2Vm+Fd3JIGbP0c5yJonlYss2Jdn5Wrl2rURRzgEMc0z3NUcYU6GuQ9wSOe8Ky1tFy70+4/U7WC0uzj29IePgDk+ZXz</latexit><latexit sha1_base64=\"aOJPCuP2SKS5N5+jXxP8AHPUq50=\">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkUQhJKIoMtiNy4r2Ae0pSTptIbmRTIplJKduPUH3OoviX+gf+GdcQpqEZ2Q5My595yZe68T+17KTfO1oC0tr6yuFddLG5tb2zv67l4zjbLEZQ038qOk7dgp872QNbjHfdaOE2YHjs9azrgm4q0JS1IvCm/4NGa9wB6F3tBzbU5UX9e7gc1vneGslvdn4xMr7+tls2LKZSwCS4Ey1KpH+gu6GCCCiwwBGEJwwj5spPR0YMFETFwPM+ISQp6MM+QokTajLEYZNrFj+o5o11FsSHvhmUq1S6f49CakNHBEmojyEsLiNEPGM+ks2N+8Z9JT3G1Kf0d5BcRy3BL7l26e+V+dqIVjiAtZg0c1xZIR1bnKJZNdETc3vlTFySEmTuABxRPCrlTO+2xITSprF721ZfxNZgpW7F2Vm+Fd3JIGbP0c5yJonlYss2Jdn5Wrl2rURRzgEMc0z3NUcYU6GuQ9wSOe8Ky1tFy70+4/U7WC0uzj29IePgDk+ZXz</latexit><latexit sha1_base64=\"aOJPCuP2SKS5N5+jXxP8AHPUq50=\">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkUQhJKIoMtiNy4r2Ae0pSTptIbmRTIplJKduPUH3OoviX+gf+GdcQpqEZ2Q5My595yZe68T+17KTfO1oC0tr6yuFddLG5tb2zv67l4zjbLEZQ038qOk7dgp872QNbjHfdaOE2YHjs9azrgm4q0JS1IvCm/4NGa9wB6F3tBzbU5UX9e7gc1vneGslvdn4xMr7+tls2LKZSwCS4Ey1KpH+gu6GCCCiwwBGEJwwj5spPR0YMFETFwPM+ISQp6MM+QokTajLEYZNrFj+o5o11FsSHvhmUq1S6f49CakNHBEmojyEsLiNEPGM+ks2N+8Z9JT3G1Kf0d5BcRy3BL7l26e+V+dqIVjiAtZg0c1xZIR1bnKJZNdETc3vlTFySEmTuABxRPCrlTO+2xITSprF721ZfxNZgpW7F2Vm+Fd3JIGbP0c5yJonlYss2Jdn5Wrl2rURRzgEMc0z3NUcYU6GuQ9wSOe8Ky1tFy70+4/U7WC0uzj29IePgDk+ZXz</latexit> Ck <latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit>\n…\nh2k 1<latexit sha1_base64=\"BOWbRVkosmkAT6wz4iD6WvBcc3c=\">AAAC13icjVHLSsNAFD2Nr/qOdekmWAQ3lqQIuiy6cVnBPqStJUmnbWheJBOxhOJO3PoDbvWPxD/Qv/DOmIJaRCckOXPuPWfm3muFrhNzXX/NKXPzC4tL+eWV1bX1jU11q1CPgySyWc0O3CBqWmbMXMdnNe5wlzXDiJme5bKGNToV8cY1i2In8C/4OGQdzxz4Tt+xTU5UVy20PZMPrX46nHTT0YExuSp31aJe0uXSZoGRgSKyVQ3UF7TRQwAbCTww+OCEXZiI6WnBgI6QuA5S4iJCjowzTLBC2oSyGGWYxI7oO6BdK2N92gvPWKptOsWlNyKlhj3SBJQXERanaTKeSGfB/uadSk9xtzH9rczLI5ZjSOxfumnmf3WiFo4+jmUNDtUUSkZUZ2cuieyKuLn2pSpODiFxAvcoHhG2pXLaZ01qYlm76K0p428yU7Bib2e5Cd7FLWnAxs9xzoJ6uWToJeP8sFg5yUadxw52sU/zPEIFZ6iiRt43eMQTnpVL5Va5U+4/U5VcptnGt6U8fADxtJa+</latexit><latexit sha1_base64=\"BOWbRVkosmkAT6wz4iD6WvBcc3c=\">AAAC13icjVHLSsNAFD2Nr/qOdekmWAQ3lqQIuiy6cVnBPqStJUmnbWheJBOxhOJO3PoDbvWPxD/Qv/DOmIJaRCckOXPuPWfm3muFrhNzXX/NKXPzC4tL+eWV1bX1jU11q1CPgySyWc0O3CBqWmbMXMdnNe5wlzXDiJme5bKGNToV8cY1i2In8C/4OGQdzxz4Tt+xTU5UVy20PZMPrX46nHTT0YExuSp31aJe0uXSZoGRgSKyVQ3UF7TRQwAbCTww+OCEXZiI6WnBgI6QuA5S4iJCjowzTLBC2oSyGGWYxI7oO6BdK2N92gvPWKptOsWlNyKlhj3SBJQXERanaTKeSGfB/uadSk9xtzH9rczLI5ZjSOxfumnmf3WiFo4+jmUNDtUUSkZUZ2cuieyKuLn2pSpODiFxAvcoHhG2pXLaZ01qYlm76K0p428yU7Bib2e5Cd7FLWnAxs9xzoJ6uWToJeP8sFg5yUadxw52sU/zPEIFZ6iiRt43eMQTnpVL5Va5U+4/U5VcptnGt6U8fADxtJa+</latexit><latexit sha1_base64=\"BOWbRVkosmkAT6wz4iD6WvBcc3c=\">AAAC13icjVHLSsNAFD2Nr/qOdekmWAQ3lqQIuiy6cVnBPqStJUmnbWheJBOxhOJO3PoDbvWPxD/Qv/DOmIJaRCckOXPuPWfm3muFrhNzXX/NKXPzC4tL+eWV1bX1jU11q1CPgySyWc0O3CBqWmbMXMdnNe5wlzXDiJme5bKGNToV8cY1i2In8C/4OGQdzxz4Tt+xTU5UVy20PZMPrX46nHTT0YExuSp31aJe0uXSZoGRgSKyVQ3UF7TRQwAbCTww+OCEXZiI6WnBgI6QuA5S4iJCjowzTLBC2oSyGGWYxI7oO6BdK2N92gvPWKptOsWlNyKlhj3SBJQXERanaTKeSGfB/uadSk9xtzH9rczLI5ZjSOxfumnmf3WiFo4+jmUNDtUUSkZUZ2cuieyKuLn2pSpODiFxAvcoHhG2pXLaZ01qYlm76K0p428yU7Bib2e5Cd7FLWnAxs9xzoJ6uWToJeP8sFg5yUadxw52sU/zPEIFZ6iiRt43eMQTnpVL5Va5U+4/U5VcptnGt6U8fADxtJa+</latexit><latexit sha1_base64=\"BOWbRVkosmkAT6wz4iD6WvBcc3c=\">AAAC13icjVHLSsNAFD2Nr/qOdekmWAQ3lqQIuiy6cVnBPqStJUmnbWheJBOxhOJO3PoDbvWPxD/Qv/DOmIJaRCckOXPuPWfm3muFrhNzXX/NKXPzC4tL+eWV1bX1jU11q1CPgySyWc0O3CBqWmbMXMdnNe5wlzXDiJme5bKGNToV8cY1i2In8C/4OGQdzxz4Tt+xTU5UVy20PZMPrX46nHTT0YExuSp31aJe0uXSZoGRgSKyVQ3UF7TRQwAbCTww+OCEXZiI6WnBgI6QuA5S4iJCjowzTLBC2oSyGGWYxI7oO6BdK2N92gvPWKptOsWlNyKlhj3SBJQXERanaTKeSGfB/uadSk9xtzH9rczLI5ZjSOxfumnmf3WiFo4+jmUNDtUUSkZUZ2cuieyKuLn2pSpODiFxAvcoHhG2pXLaZ01qYlm76K0p428yU7Bib2e5Cd7FLWnAxs9xzoJ6uWToJeP8sFg5yUadxw52sU/zPEIFZ6iiRt43eMQTnpVL5Va5U+4/U5VcptnGt6U8fADxtJa+</latexit>h 1 k 1<latexit sha1_base64=\"ek0HzBtIiatHlvpcNURaxJ/vJw0=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIvgxpKIoMuiG5cV7EPaWpJ02obmRTIRSyjuxK0/4Fb/SPwD/QvvjCmoRXRCkjPn3nNm7r1W6Dox1/XXnDI3v7C4lF8urKyurW+om8V6HCSRzWp24AZR0zJj5jo+q3GHu6wZRsz0LJc1rNGpiDeuWRQ7gX/BxyHreObAd/qObXKiumqx7Zl8aPXT4aSbjvaNyZXRVUt6WZdLmwVGBkrIVjVQX9BGDwFsJPDA4IMTdmEipqcFAzpC4jpIiYsIOTLOMEGBtAllMcowiR3Rd0C7Vsb6tBeesVTbdIpLb0RKDbukCSgvIixO02Q8kc6C/c07lZ7ibmP6W5mXRyzHkNi/dNPM/+pELRx9HMsaHKoplIyozs5cEtkVcXPtS1WcHELiBO5RPCJsS+W0z5rUxLJ20VtTxt9kpmDF3s5yE7yLW9KAjZ/jnAX1g7Khl43zw1LlJBt1HtvYwR7N8wgVnKGKGnnf4BFPeFYulVvlTrn/TFVymWYL35by8AHvVJa9</latexit><latexit sha1_base64=\"ek0HzBtIiatHlvpcNURaxJ/vJw0=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIvgxpKIoMuiG5cV7EPaWpJ02obmRTIRSyjuxK0/4Fb/SPwD/QvvjCmoRXRCkjPn3nNm7r1W6Dox1/XXnDI3v7C4lF8urKyurW+om8V6HCSRzWp24AZR0zJj5jo+q3GHu6wZRsz0LJc1rNGpiDeuWRQ7gX/BxyHreObAd/qObXKiumqx7Zl8aPXT4aSbjvaNyZXRVUt6WZdLmwVGBkrIVjVQX9BGDwFsJPDA4IMTdmEipqcFAzpC4jpIiYsIOTLOMEGBtAllMcowiR3Rd0C7Vsb6tBeesVTbdIpLb0RKDbukCSgvIixO02Q8kc6C/c07lZ7ibmP6W5mXRyzHkNi/dNPM/+pELRx9HMsaHKoplIyozs5cEtkVcXPtS1WcHELiBO5RPCJsS+W0z5rUxLJ20VtTxt9kpmDF3s5yE7yLW9KAjZ/jnAX1g7Khl43zw1LlJBt1HtvYwR7N8wgVnKGKGnnf4BFPeFYulVvlTrn/TFVymWYL35by8AHvVJa9</latexit><latexit sha1_base64=\"ek0HzBtIiatHlvpcNURaxJ/vJw0=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIvgxpKIoMuiG5cV7EPaWpJ02obmRTIRSyjuxK0/4Fb/SPwD/QvvjCmoRXRCkjPn3nNm7r1W6Dox1/XXnDI3v7C4lF8urKyurW+om8V6HCSRzWp24AZR0zJj5jo+q3GHu6wZRsz0LJc1rNGpiDeuWRQ7gX/BxyHreObAd/qObXKiumqx7Zl8aPXT4aSbjvaNyZXRVUt6WZdLmwVGBkrIVjVQX9BGDwFsJPDA4IMTdmEipqcFAzpC4jpIiYsIOTLOMEGBtAllMcowiR3Rd0C7Vsb6tBeesVTbdIpLb0RKDbukCSgvIixO02Q8kc6C/c07lZ7ibmP6W5mXRyzHkNi/dNPM/+pELRx9HMsaHKoplIyozs5cEtkVcXPtS1WcHELiBO5RPCJsS+W0z5rUxLJ20VtTxt9kpmDF3s5yE7yLW9KAjZ/jnAX1g7Khl43zw1LlJBt1HtvYwR7N8wgVnKGKGnnf4BFPeFYulVvlTrn/TFVymWYL35by8AHvVJa9</latexit><latexit sha1_base64=\"ek0HzBtIiatHlvpcNURaxJ/vJw0=\">AAAC13icjVHLSsNAFD2Nr1pfsS7dBIvgxpKIoMuiG5cV7EPaWpJ02obmRTIRSyjuxK0/4Fb/SPwD/QvvjCmoRXRCkjPn3nNm7r1W6Dox1/XXnDI3v7C4lF8urKyurW+om8V6HCSRzWp24AZR0zJj5jo+q3GHu6wZRsz0LJc1rNGpiDeuWRQ7gX/BxyHreObAd/qObXKiumqx7Zl8aPXT4aSbjvaNyZXRVUt6WZdLmwVGBkrIVjVQX9BGDwFsJPDA4IMTdmEipqcFAzpC4jpIiYsIOTLOMEGBtAllMcowiR3Rd0C7Vsb6tBeesVTbdIpLb0RKDbukCSgvIixO02Q8kc6C/c07lZ7ibmP6W5mXRyzHkNi/dNPM/+pELRx9HMsaHKoplIyozs5cEtkVcXPtS1WcHELiBO5RPCJsS+W0z5rUxLJ20VtTxt9kpmDF3s5yE7yLW9KAjZ/jnAX1g7Khl43zw1LlJBt1HtvYwR7N8wgVnKGKGnnf4BFPeFYulVvlTrn/TFVymWYL35by8AHvVJa9</latexit>\nI0k <latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit>\nI0k <latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit> I0k <latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit><latexit sha1_base64=\"FXWNdodQi2dOqEdCfEM1yEEiYo0=\">AAAC1HicjVHLSsNAFD2Nr1ofjbp0Eyyiq5KIoMuiG91VsA9oS0nSaRuaF5OJUGpX4tYfcKvfJP6B/oV3xhTUIjohyZlzz7kz914n9r1EmOZrTltYXFpeya8W1tY3Nov61nY9iVLuspob+RFvOnbCfC9kNeEJnzVjzuzA8VnDGZ3LeOOG8cSLwmsxjlknsAeh1/dcWxDV1YvtwBZDpz+5nHYno+lBVy+ZZVMtYx5YGSghW9VIf0EbPURwkSIAQwhB2IeNhJ4WLJiIietgQhwn5Kk4wxQF8qakYqSwiR3Rd0C7VsaGtJc5E+V26RSfXk5OA/vkiUjHCcvTDBVPVWbJ/pZ7onLKu43p72S5AmIFhsT+5Zsp/+uTtQj0capq8KimWDGyOjfLkqquyJsbX6oSlCEmTuIexTlhVzlnfTaUJ1G1y97aKv6mlJKVezfTpniXt6QBWz/HOQ/qR2XLLFtXx6XKWTbqPHaxh0Oa5wkquEAVNTXzRzzhWatrt9qddv8p1XKZZwfflvbwAUoqlbo=</latexit>\nPE+\nh1k<latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit><latexit sha1_base64=\"2hDWUksPO3HucBzuPW+qo2DrXZs=\">AAAC0XicjVHLSsNAFD3GV62vqks3wSK4KokIuiy6cVnRPqAvJum0Dc2LyUQopSBu/QG3+lPiH+hfeGdMQS2iE5KcOfeeM3PvdWLfS6RlvS4Yi0vLK6u5tfz6xubWdmFnt5ZEqXB51Y38SDQclnDfC3lVetLnjVhwFjg+rzujCxWv33KReFF4I8cxbwdsEHp9z2WSqE4rYHLo9CfDaXfUsbuFolWy9DLngZ2BIrJViQovaKGHCC5SBOAIIQn7YEjoacKGhZi4NibECUKejnNMkSdtSlmcMhixI/oOaNfM2JD2yjPRapdO8ekVpDRxSJqI8gRhdZqp46l2Vuxv3hPtqe42pr+TeQXESgyJ/Us3y/yvTtUi0ceZrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrZ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMe1I5LtlWyr06K5fNs1Dns4wBHNM9TlHGJCqrkLfCIJzwb18bYuDPuP1ONhUyzh2/LePgAgBaVDg==</latexit>\nCk <latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit><latexit sha1_base64=\"R7tqcvCgMhrKmzYDK0B19C9ArQg=\">AAAC0XicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnsxmVF+4BWS5JO29C8mEyEUgri1h9wqz8l/oH+hXfGKahFdEKSM+fec2buvW4S+KmwrNecsbC4tLySXy2srW9sbhW3dxppnHGP1b04iHnLdVIW+BGrC18ErJVw5oRuwJruqCrjzVvGUz+OrsQ4YdehM4j8vu85gqibTuiIodufVKfdyWjaLZassqWWOQ9sDUrQqxYXX9BBDzE8ZAjBEEEQDuAgpacNGxYS4q4xIY4T8lWcYYoCaTPKYpThEDui74B2bc1GtJeeqVJ7dEpALyeliQPSxJTHCcvTTBXPlLNkf/OeKE95tzH9Xe0VEiswJPYv3SzzvzpZi0Afp6oGn2pKFCOr87RLproib25+qUqQQ0KcxD2Kc8KeUs76bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznPGgclW2rbF8clypnetR57GEfhzTPE1Rwjhrq5M3xiCc8G5fG2Lgz7j9TjZzW7OLbMh4+ACEZlVI=</latexit>\nu2k<latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit><latexit sha1_base64=\"r6h6YTsP2Cjh4gzlxZYLbW3TnHw=\">AAACynicjVHLSsNAFD3GV62vqks3wSK4KkkRdFl048JFBfuAWksyndahaRImE6GE7vwBt/ph4h/oX3hnTEEtohOSnDn3nDtz7/XjQCTKcV4XrMWl5ZXVwlpxfWNza7u0s9tMolQy3mBREMm27yU8ECFvKKEC3o4l98Z+wFv+6FzHW/dcJiIKr9Uk5t2xNwzFQDBPEdVKe9loelvtlcpOxTHLngduDsrIVz0qveAGfURgSDEGRwhFOICHhJ4OXDiIiesiI04SEibOMUWRvCmpOCk8Ykf0HdKuk7Mh7XXOxLgZnRLQK8lp45A8EekkYX2abeKpyazZ33JnJqe+24T+fp5rTKzCHbF/+WbK//p0LQoDnJoaBNUUG0ZXx/IsqemKvrn9pSpFGWLiNO5TXBJmxjnrs208iald99Yz8Tej1Kzes1yb4l3fkgbs/hznPGhWK65Tca+Oy7WzfNQF7OMARzTPE9RwgToapspHPOHZurSkNbGyT6m1kHv28G1ZDx/S0pII</latexit>\nFigure 2: Overview of our DialoFlow. We present the detail model architecture, and the self-attention visualization in DialoFlow. “[C]” is a special token placed at the end of each utterance to model the dense representation of dialogue history Ck named as the context. The future context C′k+1 can be predicted by context history [C1, . . . ,Ck]. For the simplicity, we only plot two tokens for each utterance.\nresponse generation, which are referred to as context flow modeling, semantic influence modeling, and response generation modeling, respectively."
    }, {
      "heading" : "2.2 Model Architecture",
      "text" : "Figure 2 demonstrates the infrastructure of DialoFlow, which consists of the input embeddings, transformer blocks, a uni-directional Flow module, and a response generator. Input Embedding. DialoFlow takes the sum of token embedding, segment embedding, and position embedding as the model input. In particular, we insert a special token “[C]” at the end of each utterance, which is used to capture the overall dense representation of the dialogue history. To enhance the modeling of different speakers, we utilize segment embedding containing two types: “[Speaker1]” and “[Speaker2]”. Transformer Block. A transformer block consists of the following key components: layer normalization, multi-head attention, and feed-forward layers. We employ the pre-normalization used in GPT-2 (Radford et al., 2019) instead of the post-normalization used in BERT (Devlin et al., 2019), as (Shoeybi et al., 2019) show that the post-normalization leads to performance degradation when the model size increases while prenormalization enables stable large-scale training.\nDialoFlow keeps the uni-directional dialogue encoding and enables training on the dialogue level rather than on the context-response setting. We can obtain the history context at the k-th utterance encoded by the transformer blocks:\nCk = Transformer(u<k), (2)\nwhere Ck is the hidden states at the position of special token “[C]”. And the hidden states at the position of each token utk in the input sequence are denoted as htk. Flow Module. To capture the dynamic information flow across the dialogue utterances, we design a Flow module to model the context changing scheme. The architecture of the Flow module is the same with one layer of transformer block. The Flow module takes all the previous context {C1,C2, ...,Ck} as input and predicts the context at the (k+1)-th utterance C′k+1:\nC′k+1 = Flow(C1,C2, ...,Ck). (3)\nThe predicted semantic influence brought about by the k-th utterance can be computed as:\nI′k = C ′ k+1 −Ck. (4)\nResponse Generator. DialoFlow generates the utterance uk with the guidance of the predicted semantic influence I′k. The response generator contains a feed-forward layer and a softmax layer to\nconvert the hidden states to tokens. When generating the t-th word, the response generator takes the predicted semantic influence I′k and the hidden states ht−1k as input, and outputs the probability distribution of the t-th word:\np(utk|I′k, u<k, u<tk ) = softmax(W1[I ′ k;h t−1 k ] + b1) ∈ R |V |, (5)\nwhere |V | refers to the vocabulary size, W1 and b1 are learnable parameters."
    }, {
      "heading" : "2.3 Training Objectives",
      "text" : "Different from traditional training approaches with context-response pair, DialoFlow is trained with the whole dialogue containing N utterances. Correspondingly, we design three training tasks to optimize the model: 1) Context Flow Modeling, 2) Semantic Influence Modeling, and 3) Response Generation Modeling. Context Flow Modeling. To capture the dynamic context flow, DialoFlow predicts the context at the k-th utterance C′k based on the previous context sequence {C1, ...,Ck−1}. We minimize the L2 distance between the predicted context C′k and the real context Ck:\nLCFM = N∑ k=1 ||Ck −C′k||22. (6)\nSemantic Influence Modeling. To force the effectively modeling of semantic influence brought about by the n-th utterance at the context Cn−1, we design a bag-of-words loss using the predicted semantic influence I′n:\nLSIM = − N∑ k=1 T∑ t=1 log p(utk|I′k)\n= − N∑ k=1 T∑ t=1 log futk , (7)\nwhere futk denotes the estimated probability of the t-th word utk in the utterance uk. The function f is used to predict the words in the utterance uk in a non-autoregressive way:\nf = softmax(W2I ′ k + b2) ∈ R|V |, (8)\nwhere |V | refers to the vocabulary size, W2 and b2 are learnable parameters.\nResponse Generation Modeling. The predicted semantic influence I′k can also be regarded as a semantic expectation of the k-th utterance. We incorporate the predicted semantic influence I′k into the response generation stage to guide the generation. The response generation objective is as follows:\nLRGM = − N∑ k=1 log p(uk|I′k, u<k)\n= − N∑ k=1 T∑ t=1 log p(utk|I′k, u<k, u<tk ). (9)\nThe overall training objective of DialoFlow can be computed as follows:\nL = LCFM + LSIM + LRGM . (10)"
    }, {
      "heading" : "2.4 Flow Score",
      "text" : "By optimizing with the aforementioned three training objectives, DialoFlow can capture the dynamic information flow across the dialogue history. As the DialoFlow is trained on human-human dialogues, the context flow scheme can be regarded as the general expectation of the dialogue development. Therefore, the closer gap between the semantic influence brought by the chatbot’s utterance and the expectation means the more human-likeness.\nBased on the consideration, we propose an automatic reference-free metric Flow score for interactive dialogue evaluation based on DialoFlow. In the human-bot conversation, when the bot generates a new utterance uk, we measure the similarity between the predicted semantic influence I′k and the real semantic influence Ik brought about by the utterance uk, which can be considered as the probability of the human-likeness of the utterance. To compute the similarity between the semantic influences, we measure both the cosine similarity and the length similarity:\nsk = cos(〈I′k, Ik〉) · length(I′k, Ik)\n= I′k · Ik ||I′k|| ||Ik|| · min(||I′k||, ||Ik||) max(||I′k||, ||Ik||) . (11)\nNote that we introduce the length similarity to consider the influence of length difference on semantic similarity. For the overall quality of the chatbot in the dialogue, we design a metric, which can be regarded as the dialogue-level perplexity:\nFlow score = 2− 1 M ∑M k log( sk+1 2 ), (12)\nwhere M denotes the turn numbers of the chatbot utterances and sk+12 is to scale the similarity value to [0, 1]. A lower Flow score corresponds to better dialogue quality."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "For model pre-training, we use the Reddit comments, which are collected by a third party and made publicly available on pushshift.io (Baumgartner et al., 2020). We clean the data following the pipeline used in the DialoGPT.2 For response generation, we employ the multireference Reddit Test Dataset (Zhang et al., 2020) which contains 6k examples with multiple references. We evaluate our pre-trained DialoFlow model on this dataset. The average length of the dialogue history in this dataset is 1.47. To further explore the dynamic information flow in the long dialogue history situation, we choose another popular open-domain dialogue dataset – DailyDialog Dataset (Li et al., 2017), in which the average dialogue history length is about 4.66. DialoFlow is fine-tuned on the DailyDialog training set and evaluated on the DailyDialog multi-reference test set (Gupta et al., 2019). For interactive dialogue quality evaluation, we employ the collected data from the Interactive Evaluation of Dialog Track @ The Ninth Dialog System Technology Challenge (DSTC9) (Gunasekara et al., 2021), which contains 2200 human-bot conversations from 11 chatbots. For each conversation, there are 3 human ratings on the overall quality (0-5). We calculate the correlation between the results of our proposed metric and the human ratings on the chatbot level. Human-human conversations are always regarded to be better than human-bot conversations. Therefore, we randomly sample 200 human-human dialogues from the BST (Smith et al., 2020) dataset to see the metric’s performance on the real human-human conversations."
    }, {
      "heading" : "3.2 Experimental Setting",
      "text" : "Pre-training Details. DialoFlow is pre-trained based on the pre-trained GPT-2 (Radford et al., 2019), since Zhang et al. (2020) show that DialoGPT trained from the pre-trained GPT-2 is much better than from scratch. There are three different model sizes: DialoFlow-base, DialoFlowmedium, and DialoFlow-large, which are trained\n2https://github.com/microsoft/DialoGPT\nfrom the pre-trained GPT2-base, GPT2-medium, GPT2-large, respectively. We used AdamW optimizer (Loshchilov and Hutter, 2019) with 0.01 weight decay and linear learning rate scheduler with 12000 warm-up steps. The learning rate is 2e-4 for the base and medium version and 1e-4 for the large version. We use the batch size of 1024 for all model sizes. We trained the base and medium models for up to 4 epochs and trained the large model for 2 epochs. It costs about two months on 8 Nvidia V100 GPUs to train the large model. Decoding Details. On the 6K Reddit multireference dataset, we use beam search (with beam width 10) on the DialoFlow-medium model and the DialoFlow-large model. We employ greedy search on the DialoFlow-base model, which keeps the same with (Zhang et al., 2020). On the DailyDialog dataset, we fine-tune the pre-trained DialoFlow and DialoGPT, select the checkpoint based on the validation loss, and then use beam search (with beam width 5) for decoding."
    }, {
      "heading" : "3.3 Baseline",
      "text" : "For response generation, we compare our proposed DialoFlow with DialoGPT, a popular dialogue generation model pre-trained on the Reddit Comments. We choose the version trained from pre-trained OpenAI GPT-2 for comparison. For interactive dialogue evaluation, we compare our metric with the following metrics: 1) FED score (Mehri and Eskénazi, 2020) is an automatic evaluation metric which uses DialoGPT-large, without any fine-tuning or supervision. FED takes the DialoGPT-large as the user and calculates the likelihood of follow-up utterances based on several pre-set usual human utterances. FED works under the pre-set common human utterances, which can reveal the dialogue quality. 2) Perplexity is used to measure the coherence of an utterance under the dialogue context. We employ DialoGPT-large to measure the perplexity for each utterance of the chatbot. We average the perplexity of all utterances in the whole dialogue as the baseline metric."
    }, {
      "heading" : "3.4 Evaluation Metrics",
      "text" : "For dialogue response generation, we perform automatic evaluation using common referencebased metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Lin and Och, 2004). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams\nsuch as “I don’t know”, which is a more suitable metric than BLEU when dealing with multireference test sets. We also use Entropy (Zhang et al., 2018) to evaluate the lexical diversity. We employ the evaluation scripts used by DialoGPT. For interactive dialogue evaluation, we compute the Pearson and Spearman correlation between the automatic metrics and human ratings. We use the pre-trained DialoFlow-large to compute our proposed Flow score."
    }, {
      "heading" : "4 Results and Analysis",
      "text" : "In this section, we show the performance of our pre-trained DialoFlow model on response generation as well as the performance of Flow score on interactive dialogue quality evaluation."
    }, {
      "heading" : "4.1 Response Generation",
      "text" : "Table 1 lists the comparison of our pre-trained DialoFlow with the pre-trained DialoGPT on the Reddit multi-reference dataset. Generally, DialoFlowlarge achieves the highest score on the NIST and METEOR, while DialoGPT-medium performs better on the BLEU. The performance of our DialoFlow increases with the model size, while the DialoGPT gets the best performance with the medium size rather than the large size. As NIST can effec-\ntively penalize common n-grams such as “I don’t know”, the results reveal that DialoGPT tends to generate general responses while our DialoFlow model can create more informative responses. The results also reflect that modeling the dynamic flow is helpful to boost the conversion quality and avoid converging to the general responses. For the lexical diversity, DialoFlow performs similarly with the DialoGPT on Entropy.\nThe average history length of the multi-reference Reddit dataset is only 1.45, which is a bit short. Thus, we conduct extensive experiments on the DailyDialog dataset (average history length = 4.66) to verify the performance gain on the long dialogue history. As shown in Table 1, DialoFlow shows significant improvements on all model sizes and on all metrics compared to the DialoGPT. The improvements on the DailyDialog dataset demonstrate that our DialoFlow model shows a great capacity to capture the dynamic information flow with a long history. Note that the performance improvement of the DailyDialog dataset is more remarkable than Reddit. In our opinion, conversations in Reddit are mainly the comments in forums, while in DailyDialog the dialogues are derived from daily life. Thus, in the DailyDialog dataset, the context flows are in the more similar schema, and the semantic influences are more predictable compared to the Reddit dataset. Human Evaluation. We conduct human evaluation on 200 randomly sampled cases from the DailyDialog test dataset using crowd-sourcing. We compare DialoFlow and DialoGPT on the medium version. Each response pair is randomly presented to 3 judges, who rank them for relevance, informa-\ntiveness, and human-likeness. The overall judge preferences are presented as a percentage of the total, as shown in Table 2. There is a strong preference for the responses generated by DialoFlow. The human evaluation demonstrates that modeling the dynamic information flow is effective for improving the quality of dialogue generation. Analysis of dialogue history length. Figure 3 shows the performance of our DialoFlow and the DialoGPT on different history lengths. Overall, our DialoFlow achieves better performance on all history lengths. In particular, when history length equals 1, that is, the response is generated based on one history utterance, our DialoFlow also gains a prominent boosting. We attribute it to the guidance of predicted semantic inference. Ablation Study. To explore the effect of the proposed training objectives, we conduct ablation studies on the medium version of DialoFlow, as shown in Table 1. With all three training objectives, DialoFlow model achives the best performance on NIST and METEOR. When we drop the Semantic Influence Modeling task, the performance slightly decreases. When we further drop the Context Flow Modeling task, which means the end-to-end training, the performance decreases again. The results reveal that the Context Influence Modeling task is effective for dialogue modeling and the Semantic Influence Modeling task can prompt the CIM task."
    }, {
      "heading" : "4.2 Dialogue Evaluation",
      "text" : "Results. Table 4 shows the chatbot-level correlations of different automatic metrics with human ratings on the DSTC9 Interactive Conversation dataset. Our proposed Flow score achieves strong Spearman correlation of 0.90 (p<0.001) and strong Pearson correlation of 0.91 (p<0.001). FED only shows moderate correlations with a chatbot-level Spearman correlation of 0.56 (p<0.1). Perplexity score shows a very weak correlation. On the one hand, the results reveal that our proposed Flow score can effectively estimate the overall chatbot\nquality. On the other hand, high correlation also demonstrates that the DialoFlow model captures the general dynamic information flow in the natural human-human conversation.\nResults Analysis. Table 3 shows the detailed human ratings, FED scores, perplexity, and our proposed Flow score for the 11 chatbots in the DSTC9 Interactive Dialogue Evaluation Track and the sampled human-human conversations. Good automatic metrics should perform well not only on humanbot conversations but also human-human conversations because the ultimate goal of the chatbot is to generate human-like responses. FED performs poorly on the human-human conversations compared to its performance on the other 11 chatbots. Our proposed Flow score takes the human-human conversations as the best one, and the Flow score gap between human-human conversations and the best chatbot is similar to the human rating gap.\nAnalysis about Flow score. The Flow score can be regarded as the perplexity on the utterance level. There are many different expressions for a specific semantic in natural conversations. Traditional word-level perplexity can estimate the coherence and fluency of the utterance but always performs unstably on variable expressions. The Flow score directly measures the semantic similarity and alleviates the problem with the traditional perplexity."
    }, {
      "heading" : "4.3 Case Study",
      "text" : "Figure 4 shows the 2-D T-SNE visualization of the semantic context of a human-bot conversation encoded by our pre-trained DialoFlow model. The conversation can be split into four topics: greetings (1∼4), talking about why bad day (5∼13), explaining the terrible experience seeing the doctor (14∼18), and discuss swimming (19∼26). Correspondingly, in the visualization, the semantic context flow visualization changes a lot when the topic switches, revealing that DialoFlow can capture the dynamic information flow in the dialogue and effectively measure the semantic influence brought about by each utterance. Besides, it seems like that different speakers keep their own context flows."
    }, {
      "heading" : "5 Related Works",
      "text" : "Multi-turn dialogue modeling. The modeling of multi-turn dialogue history mainly falls into two categories: 1) Flat concatenation. These works directly concatenate the dialogue history as the input sequence (Zhang et al., 2020), which can not capture the information dynamics. 2) Hierarchical architectures. The hierarchical architecture is commonly used in the dialogue history understanding. Serban et al. (2016a) propose the hierarchical LSTM to generate responses. Li et al. (2019) introduce an incremental transformer to capture multi-turn dependencies. Shan et al. (2020); Gu et al. (2020) employ pre-trained BERT to encode individual utterances and design the utterance-\nlevel encoder to capture the turn-level structure. These methods suffer from the lack of context wordlevel information when encoding utterances. Different from these methods, our DialoFlow takes full advantage of both word-level information and utterance-level dynamic information. Besides, the proposed DialoFlow is pre-trained on the largescale open-domain dialogue dataset. Pre-trained models for dialogue generation. Recent advances in pre-trained language models have great success in dialogue response generation. DialoGPT(Zhang et al., 2020), Plato-2 (Bao et al., 2020), Meena(Adiwardana et al., 2020), and Blender(Smith et al., 2020) achieve strong generation performances by training transformer-based language models on open-domain conversation corpus. In contrast, our proposed DialoFlow focuses on modeling the dynamic information flow in the pre-training process, and we design three training objectives to optimize the model. Interactive Dialogue Evaluation. Evaluating the quality of interactive dialogue automatically is a challenging problem, as there is no gold reference for the utterances. Mehri and Eskénazi (2020) propose the FED score, an automatic dialogue evaluation metric using pre-trained DialoGPT-large, which works with pre-set common human comments, like “It is interesting to talk with you.”, revealing the dialogue quality. However, the FED score has limited performance on those dialogues without apparent comments. Our Flow score entirely depends on the pre-trained DialoFlow model with no need for human integration."
    }, {
      "heading" : "6 Conclusion and Future work",
      "text" : "In this work, we proposed the DialoFlow to model the dynamic information flow across dialogue utterances by addressing the semantic influence brought about by each utterance. Specifically, we employed a uni-directional Flow module to model the context flow and designed three training objectives to optimize the DialoFlow model. Besides, upon the DialoFlow, we proposed the Flow score, an automatic reference-free evaluation metric for interactive dialogue evaluation, with the pre-trained DialoFlow. Experiments on response generation and dialogue evaluation all demonstrate that our method could effectively capture the dynamic information flow across utterances. For future work, we would like to apply the DialoFlow to the task-oriented dialogue and explore the application on the long text\ngeneration, such as the story generation."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions. This work is supported by National Key R&D Program of China (NO. 2018AAA0102502)."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le." ],
      "venue" : "CoRR, abs/2001.09977.",
      "citeRegEx" : "Adiwardana et al\\.,? 2020",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "PLATO-2: towards building an open-domain chatbot via curriculum learning",
      "author" : [ "Siqi Bao", "Huang He", "Fan Wang", "Hua Wu", "Haifeng Wang", "Wenquan Wu", "Zhen Guo", "Zhibin Liu", "Xinchao Xu." ],
      "venue" : "CoRR, abs/2006.16779.",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "The pushshift reddit dataset",
      "author" : [ "Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn." ],
      "venue" : "Proceedings of the Fourteenth International AAAI Conference on Web and Social Media, ICWSM 2020, Held Virtually, Origi-",
      "citeRegEx" : "Baumgartner et al\\.,? 2020",
      "shortCiteRegEx" : "Baumgartner et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Processes of incremental message planning during conversation",
      "author" : [ "Sarah Brown-Schmidt", "Agnieszka E Konopka." ],
      "venue" : "Psychonomic bulletin & review, 22(3):833–843.",
      "citeRegEx" : "Brown.Schmidt and Konopka.,? 2015",
      "shortCiteRegEx" : "Brown.Schmidt and Konopka.",
      "year" : 2015
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
      "author" : [ "Xiaodong Gu", "Kang Min Yoo", "Jung-Woo Ha." ],
      "venue" : "CoRR, abs/2012.01775.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of the ninth dialog system technology",
      "author" : [ "Chulaka Gunasekara", "Seokhwan Kim", "Luis Fernando D’Haro", "Abhinav Rastogi", "Yun-Nung Chen", "Mihail Eric", "Behnam Hedayatnia", "Karthik Gopalakrishnan", "Yang Liu", "Chao-Wei Huang" ],
      "venue" : null,
      "citeRegEx" : "Gunasekara et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Gunasekara et al\\.",
      "year" : 2021
    }, {
      "title" : "Investigating evaluation of open-domain dialogue systems with human generated multiple references",
      "author" : [ "Prakhar Gupta", "Shikib Mehri", "Tiancheng Zhao", "Amy Pavel", "Maxine Eskénazi", "Jeffrey P. Bigham." ],
      "venue" : "Proceedings of the 20th Annual SIG-",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, WMT@ACL 2007, Prague, Czech Re-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei,",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Incremental transformer with deliberation decoder for document grounded conversations",
      "author" : [ "Zekang Li", "Cheng Niu", "Fandong Meng", "Yang Feng", "Qian Li", "Jie Zhou." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "Chin-Yew Lin", "Franz Josef Och." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Lin and Och.,? 2004",
      "shortCiteRegEx" : "Lin and Och.",
      "year" : 2004
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations,",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Unsupervised evaluation of interactive dialog with dialogpt",
      "author" : [ "Shikib Mehri", "Maxine Eskénazi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3, 2020,",
      "citeRegEx" : "Mehri and Eskénazi.,? 2020",
      "shortCiteRegEx" : "Mehri and Eskénazi.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Do neural dialog systems use the conversation history effectively? an empirical study",
      "author" : [ "Chinnadhurai Sankar", "Sandeep Subramanian", "Chris Pal", "Sarath Chandar", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Compu-",
      "citeRegEx" : "Sankar et al\\.,? 2019",
      "shortCiteRegEx" : "Sankar et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus",
      "author" : [ "Iulian Vlad Serban", "Alberto Garcı́a-Durán", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Arti-",
      "citeRegEx" : "Serban et al\\.,? 2016b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A contextual hierarchical attention network with adaptive objective for dialogue state tracking",
      "author" : [ "Yong Shan", "Zekang Li", "Jinchao Zhang", "Fandong Meng", "Yang Feng", "Cheng Niu", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Shan et al\\.,? 2020",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2020
    }, {
      "title" : "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "author" : [ "Mohammad Shoeybi", "Mostofa Patwary", "Raul Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro." ],
      "venue" : "arXiv preprint arXiv:1909.08053.",
      "citeRegEx" : "Shoeybi et al\\.,? 2019",
      "shortCiteRegEx" : "Shoeybi et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you put it all together: Evaluating conversational agents’ ability to blend skills",
      "author" : [ "Eric Michael Smith", "Mary Williamson", "Kurt Shuster", "Jason Weston", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Smith et al\\.,? 2020",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating informative and diverse conversational responses via adversarial information maximization",
      "author" : [ "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan." ],
      "venue" : "Advances in Neural Information Processing Sys-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al.",
      "startOffset" : 40,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al.",
      "startOffset" : 40,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al.",
      "startOffset" : 40,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : ", 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al.",
      "startOffset" : 114,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : ", 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al.",
      "startOffset" : 114,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : ", 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : ", 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training.",
      "startOffset" : 106,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training.",
      "startOffset" : 106,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training.",
      "startOffset" : 106,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder.",
      "startOffset" : 83,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder.",
      "startOffset" : 83,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder.",
      "startOffset" : 83,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "129 process that humans always consider the goal or influence of the next response before they continue the conversation (Brown-Schmidt and Konopka, 2015), we propose the DialoFlow to model the dynamic information flow in the dialogue history by addressing the semantic influence brought about by each utterance.",
      "startOffset" : 121,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "For dialogue generation, DialoFlow achieves significant improvements on the Reddit multi-reference dataset and the DailyDialog dataset compared to the baseline DialoGPT (Zhang et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "We employ the pre-normalization used in GPT-2 (Radford et al., 2019) instead of the post-normalization used in BERT (Devlin et al.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : ", 2019) instead of the post-normalization used in BERT (Devlin et al., 2019), as (Shoeybi et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : ", 2019), as (Shoeybi et al., 2019) show that the post-normalization leads to performance degradation when the model size increases while prenormalization enables stable large-scale training.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "2 For response generation, we employ the multireference Reddit Test Dataset (Zhang et al., 2020) which contains 6k examples with multiple references.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "dialogue history situation, we choose another popular open-domain dialogue dataset – DailyDialog Dataset (Li et al., 2017), in which the average dialogue history length is about 4.",
      "startOffset" : 105,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "DialoFlow is fine-tuned on the DailyDialog training set and evaluated on the DailyDialog multi-reference test set (Gupta et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "For interactive dialogue quality evaluation, we employ the collected data from the Interactive Evaluation of Dialog Track @ The Ninth Dialog System Technology Challenge (DSTC9) (Gunasekara et al., 2021), which contains 2200 human-bot conversations from 11 chatbots.",
      "startOffset" : 177,
      "endOffset" : 202
    }, {
      "referenceID" : 23,
      "context" : "Therefore, we randomly sample 200 human-human dialogues from the BST (Smith et al., 2020) dataset to see the metric’s performance on the real human-human conversations.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "DialoFlow is pre-trained based on the pre-trained GPT-2 (Radford et al., 2019), since Zhang et al.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "We employ greedy search on the DialoFlow-base model, which keeps the same with (Zhang et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "For interactive dialogue evaluation, we compare our metric with the following metrics: 1) FED score (Mehri and Eskénazi, 2020) is an automatic evaluation metric which uses DialoGPT-large, without any fine-tuning or supervision.",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "For dialogue response generation, we perform automatic evaluation using common referencebased metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Lin and Och, 2004).",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Lin and Och, 2004).",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Lin and Och, 2004).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "For 6K Reddit multireference dataset, as the DialoGPT do not release the decoding code, we directly quote the results from (Zhang et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "We also use Entropy (Zhang et al., 2018) to evaluate the lexical diversity.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "These works directly concatenate the dialogue history as the input sequence (Zhang et al., 2020), which can not capture the information dynamics.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : ", 2020), Plato-2 (Bao et al., 2020), Meena(Adiwardana et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", 2020), Meena(Adiwardana et al., 2020), and Blender(Smith et al.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : ", 2020), and Blender(Smith et al., 2020) achieve strong generation performances by training transformer-based language models on open-domain conversation corpus.",
      "startOffset" : 20,
      "endOffset" : 40
    } ],
    "year" : 2021,
    "abstractText" : "Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pretrained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task. Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation (r = 0.9) with human ratings among 11 chatbots. Code and pre-trained models will be public. 1",
    "creator" : "LaTeX with hyperref"
  }
}