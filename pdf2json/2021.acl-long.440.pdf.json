{
  "name" : "2021.acl-long.440.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
    "authors" : [ "Jingwen Hu", "Yuchen Liu", "Jinming Zhao", "Qin Jin" ],
    "emails" : [ "qjin}@ruc.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5666–5675\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5666"
    }, {
      "heading" : "1 Introduction",
      "text" : "Emotion is an important part of human daily communication. Emotion Recognition in Conversation (ERC) aims to automatically identify and track the emotional status of speakers during a dialogue. It has attracted increasing attention from researchers in the field of natural language processing and multimodal processing. ERC has a wide range of potential applications such as assisting conversation analysis for legal trials and e-health services etc. It is also a key component for building natural humancomputer interactions that can produce emotional responses in a dialogue.\nThe fast growing availability of conversational data on social media is one of the factors that boost ∗Corresponding Author\nthe research focus on emotion recognition in conversation. Different from traditional emotion recognition on isolated utterances, emotion recognition in conversation requires context modeling of individual utterances. The context can be attributed to the preceding utterances, temporality in conversation turns, or speaker related information etc. Different models have been proposed to capture the contextual information in previous works, including the LSTM-based model (Poria et al., 2017), the conversational memory network (CMN) model (Hazarika et al., 2018b), interactive conversational memory network (ICON) model (Hazarika et al., 2018a), and DialogueRNN model (Majumder et al., 2019) etc. In the example conversation as shown in Figure 1, the two speakers are chatting in the context of the male speaker being admitted to USC. In this chatting scene, they change topics a few\ntimes, such as the female speaker inviting the male speaker out to play and so on. But they keep coming back to the topic of USC, and then both of them express an excitement emotional status. It shows that long-distance contextual information is of great help to the prediction of speakers’ emotions. However, previous models can not effectively capture both speaker and long-distance dialogue contextual information simultaneously in multi-speaker conversation scenarios. Ghosal et al.(Ghosal et al., 2019), therefore, first propose the DialogueGCN model which applies graph convolutional network (GCN) to capture long-distance contextual information in a conversation. DialogueGCN takes each utterance as a node and connects any nodes that are in the same window within a conversation. It can well model both the dialogue context and speaker information which leads to the state-of-the-art ERC performance. However, like most previous models, DialogGCN only focuses on the textual modality of the conversation, ignoring effective combination of other modalities such as visual and acoustic modalities. Works that consider multimodal contextual information often conduct the simple feature concatenation type of multimodal fusion.\nIn order to effectively explore the multimodal information and at the same time capture longdistance contextual information, we propose a new multimodal fused graph convolutional network (MMGCN) model in this work. MMGCN constructs the fully connected graph in each modality, and builds edge connections between nodes corresponding to the same utterance across different modalities, so that contextual information across different modalities can interact. In addition, the speaker information is injected into MMGCN via speaker embedding. Furthermore, different from DialogueGCN, which is a non-spectral domain GCN and its many optimized matrices occupy too much computing resource, we encode the multimodal graph using spectral domain GCN and extend the GCN from a single layer to deep layers. To verify the effectiveness of the proposed model, we carry out experiments on two benchmark multimodal conversation datasets, IEMOCAP and MELD. MMGCN significantly outperforms other models on both datasets.\nThe rest of the paper is organized as follows: Section 2 discusses some related works; Section 3 introduces the proposed MMGCN model in details;\nSection 4 and 5 present the experiment setups on two public benchmark datasets and the analysis of experiment results and ablation study; Finally, Section 6 draws some conclusions."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Emotion Recognition in Conversation",
      "text" : "With the fast development of social media, much more interaction data become available, including several open-sourced conversation datasets such as IEMOCAP(Busso et al., 2008), AVEC(Schuller et al., 2012), MELD(Poria et al., 2018), etc. ERC has attracted much research attention recently.\nMany previous works focus on modeling contextual information due to its importance in ERC. Poria et al. (Poria et al., 2017) leverage a LSTMbased model to capture interaction history context. Hazarika et al. (Hazarika et al., 2018b,a) first pay attention to the importance of speaker information and exploit different memory networks to model different speakers. DialogueRNN (Majumder et al., 2019) leverage distinct GRUs to capture speakers’ contextual information. DialogueGCN (Ghosal et al., 2019) construct the graph considering both speaker and conversation sequential information and achieve the state-of-the-art performance."
    }, {
      "heading" : "2.2 Multimodal Fusion",
      "text" : "Most recent studies on ERC focus primarily on the textual modality. (Poria et al., 2017; Hazarika et al., 2018b,a) leverage multimodal information through concatenating features from three modalities without modeling the interaction between modalities. (Chen et al., 2017) conduct multimodal fusion at the word-level for emotion recognition of isolated utterances. (Sahay et al., 2018) consider contextual information and use relations in the emotion labels across utterances to predict the emotion (Zadeh et al., 2018) propose MFN to fuse information of multi-views, which aligns features from different modalities well. However, MFN neglects to model speaker information, which is significant to ERC as well. The state-of-the-art dialogueGCN model only considers the textual modality. In order to explore a more effective way of fusing multiple modalities and at the same time capturing contextual conversation information, we propose MMGCN which constructs a graph based on all three muoldalities."
    }, {
      "heading" : "2.3 Graph Convolutional Network",
      "text" : "Graph convolutional networks have been widely used in the past few years for their ability to cope with non-Euclidean data. Mainstream GCN methods can be divided into spectral domain methods and non-spectral domain methods (Veličković et al., 2017). Spectral domain GCN methods (Zhang et al., 2019) are based on Laplace Spectral decomposition theory. They can only deal with undirected graphs. Non-spectral domain GCN methods (Veličković et al., 2017; Schlichtkrull et al., 2018; Li et al., 2015) can be applied to both directed and undirected graphs, but consuming larger computing resource. Recently, researchers have proposed methods to make spectral domain GCN deeper without over-smoothing (Li et al., 2019; Chen et al., 2020). In order to further improve MMGCN on ERC, we encode the multimodal graph using spectral domain GCN with deep layers."
    }, {
      "heading" : "3 Method",
      "text" : "A dialogue can be defined as a sequence of utterances {u1, u2, ..., uN}, where N is the number of utterances. Each utterance involves three sources of utterance-aligned data corresponding to three modalities, including acoustic (a), visual (v) and textual (t) modalities, which can be represented as follows:\nui = {uai , uvi , uti} (1)\nwhere uai , u v i , u t i denote the raw feature represen-\ntation of ui from the acoustic, visual and textual\nmodality, respectively. The emotion recognition in conversation task aims to predict the emotional status label for each utterance ui in the conversation based on the available information from all three modalities. Figure 2 illustrates the overall framework of our proposed emotion recognition in conversation system, which consists of three key modules: Modality Encoder, Multimodal Fused Graph Convolutional Network (MMGCN), and Emotion Classifier."
    }, {
      "heading" : "3.1 Modality Encoder",
      "text" : "As we mentioned above, the dialog context information is important for predicting the emotion label of each utterance. Therefore, it is beneficial to encode the contextual information into the utterance feature representation. We generate the contextaware utterance feature encoding for each modality through the corresponding modality encoder. To be specific, we apply a bidirectional Long Short Term Memory (LSTM) network to encode the sequential textual context information for the textual modality. For the acoustic and visual modalities, we apply a fully connected network. The context-aware feature encoding for each utterance can be formulated as follows:\nhti = [ −−−−→ LSTM(uti, h t i−1), ←−−−− LSTM(uti, h t i+1)] hai =W a e u a i + b a i hvi =W v e u v i + b v i\n(2)\nwhere uai , u v i , u t i are the context-independent raw\nfeature representation of utterance i from the acoustic, visual and textual modalities, respectively. The\nmodality encoder outputs the context-aware raw feature encoding hai , h v i , and h t i accordingly."
    }, {
      "heading" : "3.2 Multimodal fused GCN (MMGCN)",
      "text" : "In order to capture the utterance-level contextual dependencies across multiple modalities, we propose a Multimodal fused Graph Convolutional Network (MMGCN). We construct a spectral domain graph convolutional network to encode the multimodal contextual information inspired by (Li et al., 2019; Chen et al., 2020). We also stack more layers to construct a deep GCN. Furthermore, we add learned speaker-embeddings to encode the speakerlevel contextual information."
    }, {
      "heading" : "3.2.1 Speaker Embedding",
      "text" : "As mentioned above, speaker information is important for ERC. In order to encode the speaker identity information, we add speaker embeddings to the features before constructing the graph. Assuming there are M parties in a dialogue, then the size of the speaker embedding is M . We show a two-speaker conversation case in Figure 2. The original speaker identity can be denoted with a one-hot vector si and the speaker embedding Si is calculated as follows:\nSi =Wssi + b s i (3)\nThe speaker embedding can then be leveraged to attach speaker information in the graph construction."
    }, {
      "heading" : "3.2.2 Graph Construction",
      "text" : "A dialogue withN utterances can be represented as an undirected graph G = (V, E), where V (|V| = 3N ) denotes utterance nodes in three modalities and E ⊂ V × V is a set of relationships containing context, speaker and modality dependency. We construct the graph as follows: Nodes: Each utterance is represented by three nodes vai , v v i , v t i in a graph, initialized with h ′a i ,h ′v i ,h ′l i , which represent [h a i , Si], [h v i , Si], [hti, Si] respectively, corresponding to the three modalities. Thus, given a dialogue with N utterances, we construct a graph with 3N nodes. Edges: We assume that each utterance has certain connection to other utterances in the same dialogue. Therefore, any two nodes in the same modality in the same dialogue are connected in the graph. Furthermore, each node is connected with the nodes which correspond to the same utterance but from\ndifferent modalities. For example, vai will be connected with vvi and v t i in the graph. Edge Weighting: We assume that if two nodes have higher similarity, the information interaction between them is also more important, and the edge weight between them should be higher. In order to capture the similarities between node representations, following (Skianis et al., 2018), we use the angular similarity to represent the edge weight between two nodes.\nThere are two types of edges in the graph: 1) edges connecting nodes from the same modality, and 2) edges connecting nodes from different modalities. To differentiate them, we use different edge weighting strategies. For the first type of edges, the edge weight is computed as:\nAij = 1− arccos(sim(ni, nj))\nπ (4)\nwhere ni and nj denote the feature representations of the i-th and j-th node in the graph. For the second type of edges, the edge weight is computed as:\nAij = γ(1− arccos(sim(ni, nj))\nπ ) (5)\nwhere γ is a hyper parameter. Graph Learning: Inspired by (Chen et al., 2020), we build a deep graph convolutional network based on the undirected graph formed following the above construction steps to further encode the contextual dependencies. To be specific, given the undirected graph G = (V, E), let P̃ be the renormalized graph Laplacian matrix (Kipf and Welling, 2016) of G:\nP̃ = D̃−1/2ÃD̃−1/2\n= (D + I)−1/2(A+ I)(D + I)−1/2 (6)\nwhere A denotes the adjacency matrix, D denotes the diagonal degree matrix of graph G, and I denotes identity matrix. The iteration of GCN from different layers can be formulated as:\nH(l+1) = σ(((1−α)P̃H(l)+αH(0))((1−β(l))I+β(l)W(l))) (7)\nwhere α and β(l) are two hyper parameters, σ denotes the activation function andW(l) is a learnable weight matrix. To ensure the decay of the weight matrix adaptively increases when stacking more layers, we set β(l) = log(ηl + 1), where η is also a hyper parameter. A residual connection to the first layerH(0) is added to the representation P̃H(l) and an identity mapping I is added to the weight matrix W(l). With such residual connection, we can make MMGCN deeper to further improve performance."
    }, {
      "heading" : "3.3 Emotion Classifier",
      "text" : "As described in sec. 3.2.2, we initialize nodes with the combination of utterance feature and speaker embedding, h\n′ i.\nh ′ i = [h ′a i , h ′v i , h ′t i ]. (8)\nLet gai , g v i and g t i be the features of different modal-\nities encoded by the GCN. The features corresponding to the same utterance are concatenated:\ngi = [g a i , g v i , g t i ]. (9)\nWe then can concatenate gi and hi to generate the final feature representation for each utterance:\nei = [h ′ i, gi], (10)\nei is then fed into a MLP with fully connected layers to predict the emotion label ŷi for the utterance:\nli = RELU(Wlei + bl) Pi = Softmax(Wsmaxli + bsmax) ŷi = argmin k (Pi[k]) (11)"
    }, {
      "heading" : "3.4 Training Objectives",
      "text" : "We use categorical cross-entropy along with L2regularization as the loss function during training:\nL = − 1∑N s=1 c(s) N∑ i=1 c(i)∑ j=1 logPi,j [yi,j ] + λ ‖θ‖2 (12)\nwhere N is the number of dialogues, c(i) is the number of utterances in dialogue i, Pi,j is the probability distribution of predicted emotion labels of utterance j in dialogue i, yi,j is the expected class label of utterance j in dialogue i, λ is the L2-regularization weight, and θ is the set of all trainable parameters. We use stochastic gradient descent based Adam (Kingma and Ba, 2014) optimizer to train our network. Hyper parameters are optimized using grid search."
    }, {
      "heading" : "4 Experiment Setups",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We evaluate our proposed MMGCN model on two benchmark datasets, IEMOCAP(Busso et al., 2008) and MELD(Poria et al., 2018). Both are multimodal datasets with aligned acoustical, visual and textual information of each utterance in a conversation. Followed (Ghosal et al., 2019), we partition both datasets into train and test sets with roughly\n8:2 ratio. Table 1 shows the distribution of train and test samples for both datasets.\nIEMOCAP: The dataset contains 12 hours of videos of two-way conversations from ten unique speakers, where only the first eight speakers from session one to four are used in the training set. Each video contains a single dyadic dialogue, segmented into utterances. There are in total 7433 utterances and 151 dialogues. Each utterance in the dialogue is annotated with an emotion label from six classes, including happy, sad, neutral, angry, excited and frustrated.\nMELD: Multi-modal Emotion Lines Dataset (MELD) is a multi-modal and multi-speaker conversation dataset. Compared to the Emotion Lines dataset (Chen et al., 2018), MELD has three modality-aligned conversation data with higher quality. There are in total 13708 utterances, 1433 conversations and 304 different speakers. Specifically, different from dyadic conversation datasets such as IEMOCAP, MELD has three or more speakers in a conversation. Each utterance in the dialogue is annotated with an emotion label from seven classes, including anger, disgust, fear, joy, neutral, sadness and surprise."
    }, {
      "heading" : "4.2 Utterance-level Raw Feature Extraction",
      "text" : "The textual raw features are extracted using TextCNN following (Hazarika et al., 2018a). The acoustic raw features are extracted using the OpenSmile toolkit with IS10 configuration (Schuller et al., 2011). The visual facial expression features are extracted using a DenseNet (Huang et al., 2015) pre-traind on the Facial Expression Recognition Plus (FER+) corpus (Barsoum et al., 2016)."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "The hyperparameters are set as follows: the number of GCN layers are both 4 for IEMOCAP and MELD. The dropout is 0.4. The learning rate is 0.0003. The L2 regularization parameter is 0.00003. α, η and γ are set as 0.1, 0.5 and 0.7 respectively. Considering the class-imbalance in\nMELD, we use focal loss when training MMGCN on MELD. In addition, we add layer normalization after the speaker embedding."
    }, {
      "heading" : "4.4 Evaluation Metrics and Significance Test",
      "text" : "Following previous works (Hazarika et al., 2018a; Majumder et al., 2019; Ghosal et al., 2019), we use weighted average f1-score as the evaluation metric. Paired t-test is performed to test the significance of performance improvement with a default significance level of 0.05."
    }, {
      "heading" : "4.5 Compared Baselines",
      "text" : "In order to verify the effectiveness of our model, We implement and compare the following models on emotion recognition in conversation. BC-LSTM (Poria et al., 2017): it encodes contextual information through Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) network. The context-aware features are then used for emotion classification. BC-LSTM ignores speaker information as it doesn’t attach any speaker-related information to their model. CMN (Hazarika et al., 2018b): it leverages speaker-dependent GRUs to model utterance context combining dialogue history information. The utterance features with contextual information are subject to two distinct memory networks for both speakers. Due to the fixed number of Memory network blocks, CMN can only serve in dyadic conversation scenarios. ICON (Hazarika et al., 2018a): it extends CMN to model distinct speakers respectively. Same with CMN, two speaker-dependent GRUs are leveraged. Besides, A global GRU is used to track the change of emotion status in the entire conversation and multi-layer memory networks are leveraged to model the global emotion status. Though ICON\nimproves the result of ERC, it still cannot adapt to a multi-speaker scenario. DialogueRNN (Majumder et al., 2019): it models speakers and sequential information in dialogues through three different GRUs, which include Global GRU, Speaker GRU and Emotion GRU. Specifically, Global GRU models context information, while Speaker dependent GRU models the status of the certain speaker. The two modules update interactively. Emotion GRU detects emotion of utterances in conversation. Furthermore, in the multimodal setting, the concatenation of acoustical, visual, and textual features is used when the speaker talks, but only use visual features otherwise. However, DialogueRNN doesn’t improve much in multimodal settings. DialogueGCN (Ghosal et al., 2019): it applies GCN to ERC, in which the generated features can integrate rich information. Specifically, utterancelevel features encoded by bi-lstm are used to initialize the nodes of the graph, edges are constructed within a certain window. Utterances in the same dialogue but with long distance can be connected directly. Relation GCN(Schlichtkrull et al., 2018) and GNN(Morris et al., 2019), which are both nonspectral domain GCN models, are leveraged to encode the graph. However, DialogueGCN only focuses on the textual modality. In order to compare with our MMGCN under the multimodal setting, we extend DialogueGCN by simply concatenating features of three modalities."
    }, {
      "heading" : "5 Results and Discussions",
      "text" : "We compare our proposed MMGCN with all the baseline models presented in section 4.5 on IEMOCAP and MELD datasets under the multimodal setting. In order to compare the results under the same experiment settings, we reimplement the models in\nthe following experiments."
    }, {
      "heading" : "5.1 Comparison with other models",
      "text" : "Table 2 shows the performance comparison of MMGCN with other models on the two benchmark datasets under the multimodal setting. DialougeGCN was the best performing model when using only the textual modality. Under the multimodal setting, DialogueGCN which is fed with the concatenation of acoustic, visual and textual features achieves some slight improvement over the single textual modality. Our proposed MMGCN improves the F1-score performance over DialogueGCN under the multimodal setting by absolute 1.18% on IEMOCAP and 0.42% on MELD on average, and the improvement is significant with p-value < 0.05."
    }, {
      "heading" : "5.2 MMGCN under various modality setting",
      "text" : "Table 3 shows the performance comparison of MMGCN under different multimodal settings on both benchmark datasets. From Table 3 we can see that the best single modality performance is achieved on the textual modality and the worst is on the visual modality, which is consistent with previously reported findings. Adding acoustic and visual modalities can bring additional performance improvement over the textual modality."
    }, {
      "heading" : "5.3 Comparison with other fusion methods",
      "text" : "To verify the effectiveness of MMGCN in multimodal fusion, we compare it with other multimodal fusion methods, including early fusion, late fusion, fusion through gated attention and other representative fusion methods such as MFN(Zadeh et al., 2018) and MulT(Tsai et al., 2019). The first three fusion methods are illustrated in Figure 3. As for\nearly fusion, multimodal features are concatenated and fed into GCN directly. As for late fusion, features of different modalities are fed into different GCNs respectively and concatenated afterwards. As for fusion through gated attention, features are fed into different GCNs the same way as in late fusion, and then to a gated attention module. Specifically, the gated attention module can be formulated as follows:\nr mj i = tanh(Wmj · h mj i ) (13) r mk i = tanh(Wmk · h mk i ) (14) z = σ(Wz · h mj i ) (15) r (mj ,mk)\ni = z ∗ r mj i + (1− z) ∗ r mk i (16)\nei = [r (a,v) i , r (a,t) i , r (v,t) i ] (17)\nwhere mj and mk could be any modality among {a, v, t}, hmji and h mk i represent the feature encoded by the corresponding modality encoder, ei represents the final feature representation for the ith utterance. Considering MFN and MulT are leveraged to fuse multimodal information sequentially, they are used to replace the Modality Encoder. The fused multimodal features are fed to the GCN module subsequently.\nTable 4 shows that MMGCN with the graphbased multimodal fusion outperforms all other com-\npared multimodal fusion methods."
    }, {
      "heading" : "5.4 MMGCN with different layers",
      "text" : "We investigate the impact of the number of layers in MMGCN on the ERC performance in Table 5. The experiment results show that a different number of layers does affect the ERC recognition performance. Specifically, MMGCN achieves the best performance with 4 layers on both IEMOCAP and MELD."
    }, {
      "heading" : "5.5 Impact of Speaker Embedding",
      "text" : "Speaker Embedding can differentiate input features from different speakers. Previous works have reported that speaker information can help improve emotion recognition performance. We conduct the ablation study to verify the contribution of speaker embedding in MMGCN as shown in Table 6. As expected, dropping speaker embedding in MMGCN leads to performance degradation, which is significant by t-test with p<0.05."
    }, {
      "heading" : "5.6 Case Study",
      "text" : "Fig 4 depicts a scene in which a man and a woman quarrel with each other over a female friend of the man who came to meet with him across 700 miles. They are frustrated or angry in most cases. At the beginning of the conversation, their emotion states are both neutral. Over time, they become emotional. They are both angry at the end of the conversation. The heatmaps of the adjacent matrix for the 20th utterance in the conversation from the three modalities demonstrate that different from simple sequential models, MMGCN pays attention not only to the close context, but also relate to the context in long-distance. For example, as shown\nin the textual heatmap, MMGCN can successfully aggregate information from the most relevant utterances, even from long-distance utterances, for example the 3rd utterance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose an multimodal fused graph convolutional network (MMGCN) for multimodal emotion recognition in conversation (ERC). MMGCN provides a more effective way of utilizing both multimodal and long-distance contextual information. It constructs a graph that captures not only intra-speaker context dependency but also inter-modality dependency. With the residual connection, MMGCN can have deep layers to further improve recognition performance. We carry out experiments on two public benchmark datasets, IEMOCAP and MELD, and the experiment results prove the effectiveness of MMGCN, which outperforms other state-of-the-art methods by a significant margin under the multimodal conversation setting."
    }, {
      "heading" : "7 Acknowledgement",
      "text" : "This work was supported by the National Key R&D Program of China under Grant No. 2020AAA0108600, National Natural Science Foundation of China (No. 62072462), National Natural Science Foundation of China (No. 61772535), and Beijing Natural Science Foundation (No. 4192028)."
    } ],
    "references" : [ {
      "title" : "Training deep networks for facial expression recognition with crowdsourced label distribution",
      "author" : [ "Emad Barsoum", "Cha Zhang", "Cristian Canton Ferrer", "Zhengyou Zhang." ],
      "venue" : "New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Barsoum et al\\.,? 2016",
      "shortCiteRegEx" : "Barsoum et al\\.",
      "year" : 2016
    }, {
      "title" : "Iemocap: Interactive emotional dyadic motion capture database",
      "author" : [ "Carlos Busso", "Murtaza Bulut", "Chi-Chun Lee", "Abe Kazemzadeh", "Emily Mower", "Samuel Kim", "Jeannette N Chang", "Sungbok Lee", "Shrikanth S Narayanan." ],
      "venue" : "Language re-",
      "citeRegEx" : "Busso et al\\.,? 2008",
      "shortCiteRegEx" : "Busso et al\\.",
      "year" : 2008
    }, {
      "title" : "Simple and deep graph convolutional networks",
      "author" : [ "Ming Chen", "Zhewei Wei", "Zengfeng Huang", "Bolin Ding", "Yaliang Li." ],
      "venue" : "International Conference on Machine Learning, pages 1725–1735. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Emotionlines: An emotion corpus of multi-party conversations",
      "author" : [ "Sheng-Yeh Chen", "Chao-Chun Hsu", "Chuan-Chun Kuo", "Lun-Wei Ku" ],
      "venue" : "arXiv preprint arXiv:1802.08379",
      "citeRegEx" : "Chen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "author" : [ "Deepanway Ghosal", "Navonil Majumder", "Soujanya Poria", "Niyati Chhaya", "Alexander Gelbukh." ],
      "venue" : "arXiv preprint arXiv:1908.11540.",
      "citeRegEx" : "Ghosal et al\\.,? 2019",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2019
    }, {
      "title" : "Icon: Interactive conversational memory network for multimodal emotion detection",
      "author" : [ "Devamanyu Hazarika", "Soujanya Poria", "Rada Mihalcea", "Erik Cambria", "Roger Zimmermann." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in",
      "citeRegEx" : "Hazarika et al\\.,? 2018a",
      "shortCiteRegEx" : "Hazarika et al\\.",
      "year" : 2018
    }, {
      "title" : "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "author" : [ "Devamanyu Hazarika", "Soujanya Poria", "Amir Zadeh", "Erik Cambria", "Louis-Philippe Morency", "Roger Zimmermann." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2122–",
      "citeRegEx" : "Hazarika et al\\.,? 2018b",
      "shortCiteRegEx" : "Hazarika et al\\.",
      "year" : 2018
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian Q. Weinberger" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Deepgcns: Can gcns go as deep as cnns",
      "author" : [ "Guohao Li", "Matthias Muller", "Ali Thabet", "Bernard Ghanem" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Gated graph sequence neural networks",
      "author" : [ "Yujia Li", "Daniel Tarlow", "Marc Brockschmidt", "Richard Zemel." ],
      "venue" : "arXiv preprint arXiv:1511.05493.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "author" : [ "Navonil Majumder", "Soujanya Poria", "Devamanyu Hazarika", "Rada Mihalcea", "Alexander Gelbukh", "Erik Cambria." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Majumder et al\\.,? 2019",
      "shortCiteRegEx" : "Majumder et al\\.",
      "year" : 2019
    }, {
      "title" : "Weisfeiler and leman go neural: Higher-order graph neural networks",
      "author" : [ "Christopher Morris", "Martin Ritzert", "Matthias Fey", "William L Hamilton", "Jan Eric Lenssen", "Gaurav Rattan", "Martin Grohe." ],
      "venue" : "In",
      "citeRegEx" : "Morris et al\\.,? 2019",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-dependent sentiment analysis in user-generated videos",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Navonil Majumder", "Amir Zadeh", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 55th annual meeting of the association for compu-",
      "citeRegEx" : "Poria et al\\.,? 2017",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2017
    }, {
      "title" : "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "author" : [ "Soujanya Poria", "Devamanyu Hazarika", "Navonil Majumder", "Gautam Naik", "Erik Cambria", "Rada Mihalcea." ],
      "venue" : "arXiv preprint arXiv:1810.02508.",
      "citeRegEx" : "Poria et al\\.,? 2018",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal relational tensor network for sentiment and emotion classification",
      "author" : [ "Saurav Sahay", "Shachi H Kumar", "Rui Xia", "Jonathan Huang", "Lama Nachman." ],
      "venue" : "arXiv preprint arXiv:1806.02923.",
      "citeRegEx" : "Sahay et al\\.,? 2018",
      "shortCiteRegEx" : "Sahay et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European Semantic Web Conference, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "author" : [ "Björn Schuller", "Anton Batliner", "Stefan Steidl", "Dino Seppi." ],
      "venue" : "Speech Communication, 53(9-10):1062–1087.",
      "citeRegEx" : "Schuller et al\\.,? 2011",
      "shortCiteRegEx" : "Schuller et al\\.",
      "year" : 2011
    }, {
      "title" : "Avec 2012: the continuous audio/visual emotion challenge",
      "author" : [ "Björn Schuller", "Michel Valster", "Florian Eyben", "Roddy Cowie", "Maja Pantic." ],
      "venue" : "Proceedings of the 14th ACM international conference on Multimodal interaction, pages 449–456.",
      "citeRegEx" : "Schuller et al\\.,? 2012",
      "shortCiteRegEx" : "Schuller et al\\.",
      "year" : 2012
    }, {
      "title" : "Fusing document, collection and label graph-based representations with word embeddings for text classification",
      "author" : [ "Konstantinos Skianis", "Fragkiskos Malliaros", "Michalis Vazirgiannis." ],
      "venue" : "Proceedings of the Twelfth Workshop on Graph-Based Meth-",
      "citeRegEx" : "Skianis et al\\.,? 2018",
      "shortCiteRegEx" : "Skianis et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the conference. Association for Com-",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Lio", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1710.10903.",
      "citeRegEx" : "Veličković et al\\.,? 2017",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2017
    }, {
      "title" : "Memory fusion network for multiview sequential learning",
      "author" : [ "Amir Zadeh", "Paul Pu Liang", "Navonil Mazumder", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Zadeh et al\\.,? 2018",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "author" : [ "Dong Zhang", "Liangqing Wu", "Changlong Sun", "Shoushan Li", "Qiaoming Zhu", "Guodong Zhou." ],
      "venue" : "IJCAI, pages 5415–5421.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Different models have been proposed to capture the contextual information in previous works, including the LSTM-based model (Poria et al., 2017), the conversational memory network (CMN) model (Hazarika et al.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : ", 2017), the conversational memory network (CMN) model (Hazarika et al., 2018b), interactive conversational memory network (ICON) model (Hazarika et al.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : ", 2018b), interactive conversational memory network (ICON) model (Hazarika et al., 2018a), and DialogueRNN model (Majumder et al.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ", 2018a), and DialogueRNN model (Majumder et al., 2019) etc.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Ghosal et al.(Ghosal et al., 2019), therefore, first propose the DialogueGCN model which applies graph convolutional network (GCN) to capture long-distance contextual information in a conversation.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "With the fast development of social media, much more interaction data become available, including several open-sourced conversation datasets such as IEMOCAP(Busso et al., 2008), AVEC(Schuller et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "(Poria et al., 2017) leverage a LSTMbased model to capture interaction history context.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "DialogueRNN (Majumder et al., 2019) leverage distinct GRUs to capture speakers’ contextual information.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "DialogueGCN (Ghosal et al., 2019) construct the graph considering both speaker and conversation sequential information and achieve the state-of-the-art performance.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "(Sahay et al., 2018) consider contextual information and use relations in the emotion labels across utterances to predict the emotion (Zadeh et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : ", 2018) consider contextual information and use relations in the emotion labels across utterances to predict the emotion (Zadeh et al., 2018) propose MFN to fuse information of multi-views, which aligns features from different modalities well.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "Mainstream GCN methods can be divided into spectral domain methods and non-spectral domain methods (Veličković et al., 2017).",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : "Spectral domain GCN methods (Zhang et al., 2019) are based on Laplace Spectral decomposition theory.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "Non-spectral domain GCN methods (Veličković et al., 2017; Schlichtkrull et al., 2018; Li et al., 2015) can be applied to both directed and undirected graphs, but consuming larger computing resource.",
      "startOffset" : 32,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "Non-spectral domain GCN methods (Veličković et al., 2017; Schlichtkrull et al., 2018; Li et al., 2015) can be applied to both directed and undirected graphs, but consuming larger computing resource.",
      "startOffset" : 32,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Non-spectral domain GCN methods (Veličković et al., 2017; Schlichtkrull et al., 2018; Li et al., 2015) can be applied to both directed and undirected graphs, but consuming larger computing resource.",
      "startOffset" : 32,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Recently, researchers have proposed methods to make spectral domain GCN deeper without over-smoothing (Li et al., 2019; Chen et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "Recently, researchers have proposed methods to make spectral domain GCN deeper without over-smoothing (Li et al., 2019; Chen et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "We construct a spectral domain graph convolutional network to encode the multimodal contextual information inspired by (Li et al., 2019; Chen et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "We construct a spectral domain graph convolutional network to encode the multimodal contextual information inspired by (Li et al., 2019; Chen et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 155
    }, {
      "referenceID" : 21,
      "context" : "In order to capture the similarities between node representations, following (Skianis et al., 2018), we use the angular similarity to represent the edge weight between two nodes.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Graph Learning: Inspired by (Chen et al., 2020), we build a deep graph convolutional network based on the undirected graph formed following the above construction steps to further encode the contextual dependencies.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "To be specific, given the undirected graph G = (V, E), let P̃ be the renormalized graph Laplacian matrix (Kipf and Welling, 2016) of G:",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "We use stochastic gradient descent based Adam (Kingma and Ba, 2014) optimizer to train our network.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "We evaluate our proposed MMGCN model on two benchmark datasets, IEMOCAP(Busso et al., 2008) and MELD(Poria et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Followed (Ghosal et al., 2019), we partition both datasets into train and test sets with roughly Dataset dialogues utterances train+val test train+val test IEMOCAP 120 31 5810 1623 MELD 1153 280 11098 2610",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "Compared to the Emotion Lines dataset (Chen et al., 2018), MELD has three modality-aligned conversation data with higher quality.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "The textual raw features are extracted using TextCNN following (Hazarika et al., 2018a).",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "The acoustic raw features are extracted using the OpenSmile toolkit with IS10 configuration (Schuller et al., 2011).",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "The visual facial expression features are extracted using a DenseNet (Huang et al., 2015) pre-traind on the Facial Expression Recognition Plus (FER+) corpus (Barsoum et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : ", 2015) pre-traind on the Facial Expression Recognition Plus (FER+) corpus (Barsoum et al., 2016).",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Following previous works (Hazarika et al., 2018a; Majumder et al., 2019; Ghosal et al., 2019), we use weighted average f1-score as the evaluation metric.",
      "startOffset" : 25,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Following previous works (Hazarika et al., 2018a; Majumder et al., 2019; Ghosal et al., 2019), we use weighted average f1-score as the evaluation metric.",
      "startOffset" : 25,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Following previous works (Hazarika et al., 2018a; Majumder et al., 2019; Ghosal et al., 2019), we use weighted average f1-score as the evaluation metric.",
      "startOffset" : 25,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "BC-LSTM (Poria et al., 2017): it encodes contextual information through Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) network.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : ", 2017): it encodes contextual information through Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) network.",
      "startOffset" : 71,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "CMN (Hazarika et al., 2018b): it leverages speaker-dependent GRUs to model utterance context combining dialogue history information.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "ICON (Hazarika et al., 2018a): it extends CMN to model distinct speakers respectively.",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "DialogueRNN (Majumder et al., 2019): it models speakers and sequential information in dialogues through three different GRUs, which include Global GRU, Speaker GRU and Emotion GRU.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "DialogueGCN (Ghosal et al., 2019): it applies GCN to ERC, in which the generated features can integrate rich information.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : ", 2018) and GNN(Morris et al., 2019), which are both nonspectral domain GCN models, are leveraged to encode the graph.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "To verify the effectiveness of MMGCN in multimodal fusion, we compare it with other multimodal fusion methods, including early fusion, late fusion, fusion through gated attention and other representative fusion methods such as MFN(Zadeh et al., 2018) and MulT(Tsai et al.",
      "startOffset" : 230,
      "endOffset" : 250
    } ],
    "year" : 2021,
    "abstractText" : "Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.",
    "creator" : "LaTeX with hyperref package"
  }
}