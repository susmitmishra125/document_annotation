{
  "name" : "2021.acl-long.2.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements",
    "authors" : [ "Chen Shani", "Nadav Borenstein", "Dafna Shahaf" ],
    "emails" : [ "dshahaf}@cs.huji.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 14–28\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n14\nIn this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: “Are cows more likely to lie down the longer they stand?”). This challenging task has unique characteristics that make it particularly suitable for automatic learning.\nWe construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP. We use our models to identify potentially funny papers in a large dataset of over 630,000 articles. The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines."
    }, {
      "heading" : "1 Introduction",
      "text" : "Humor is an important aspect of the way we interact with each other, serving complex social functions (Martineau, 1972). Humor can function either as a lubricant or as an abrasive: it can be used as a key for improving interpersonal relations and building trust (Wanzer et al., 1996; Wen et al., 2015), or help us work through difficult topics. It can also aid in breaking taboos and holding power to account. Enhancing the humor capabilities of computers has tremendous potential to better understand interactions between people, as well as build more natural human-computer interfaces.\n∗Equal contribution\nNevertheless, computational humor remains a long-standing challenge in AI; It requires complex language understanding, manipulation capabilities, creativity, common sense, and empathy. Some even claim that computational humor is an AI-complete problem (Stock and Strapparava, 2002).\nAs humor is a broad phenomenon, most works on computational humor focus on specific humor types, such as knock-knock jokes or one-liners (Mihalcea and Strapparava, 2006; Taylor and Mazlack, 2004). In this work, we present a novel humor recognition task: identifying quirky, funny scientific contributions. We are inspired by the Ig Nobel prize1, a satiric prize awarded annually to ten scientific achievements that “first make people laugh, and then think”. Past Ig Nobel winners include “Chickens prefer beautiful humans” and “Beauty is in the eye of the beer holder: People who think they are drunk also think they are attractive”.\nAutomatically identifying candidates for the Ig Nobel prize provides a unique perspective on humor. Unlike most humor recognition tasks, the humor involved is sophisticated, and requires common sense, as well as specialized knowledge and understanding of the scientific culture. On the other hand, this task has several characteristics rendering it attractive: the funniness of the paper can often be recognized from its title alone, which is short, with simple syntax and no complex narrative structure (as opposed to longer jokes). Thus, this is a relatively clean setting to explore our methods.\nWe believe humor in science is also particularly interesting to explore, as humor is strongly tied to creativity. Quirky contributions could sometimes indicate fresh perspectives and pioneering attempts to expand the frontiers of science. For example, Andre Geim won an Ig Nobel in 2000 for levitating a frog using magnets and a Nobel Prize in Physics\n1improbable.com/ig-about\nin 2010. The Nobel committee explicitly attributed the win to his playfulness (The Royal Swedish Academy of Science, 2010).\nOur contributions are: • We formulate a novel humor recognition task\nin the scientific domain. • We construct a dataset containing thousands\nof funny scientific papers. • We develop multiple classifiers, combining\nfindings from psychology and linguistics with recent NLP advances. We evaluate them both on our dataset and in a real-world setting, identifying potential Ig Nobel candidates in a large corpus of over 0.6M papers. • We devise a rigorous, data-driven way to aggregate crowd workers’ annotations for subjective questions.\n• We release data and code2. Beyond the tongue-in-cheek nature of our application, we more broadly wish to promote combining data-driven research with more-traditional works in areas such as psychology. We believe insights from such fields could complement machine learning models, improving performance as well as enriching our understanding of the problem."
    }, {
      "heading" : "2 Related Work",
      "text" : "Humor in the Humanities. A large body of theoretical work on humor stems from linguistics and psychology. Ruch (1992) divided humor into three categories: incongruity, sexual, and nonsense (and created a three-dimensional humor test to account for them). Since our task is to detect humor in scientific contributions, we believe that the third category can be neglected under the assumption that no-nonsense article would (or at least, should) be published (notable exception: the Sokal hoax (Sokal, 1996)).\nThe first category, incongruity, was first fully conceptualized by Kant in the eighteenth century (Shaw, 2010). The well-agreed extensions to incongruity theory are the linguistics incongruity resolution model and semantic script theory of humor (Suls, 1972; Raskin, 1985). Both state that if a situation ended in a manner that contradicted our prediction (in our case, the title contains an unexpected term) and there exists a different, less likely rule to explain it – the result is a humorous experience. Simply put, the source of humor lies in\n2github.com/nadavborenstein/Iggy\nviolation of expectations. Example Ig Nobel winners include: “Will humans swim faster or slower in syrup?” and ”Coordination modes in the multisegmental dynamics of hula hooping”.\nThe second category, sex-related humor is also common among Ig Nobel winning papers. Examples include: “Effect of different types of textiles on sexual activity. Experimental study” and “Magnetic resonance imaging of male and female genitals during coitus and female sexual arousal”.\nHumor Detection in AI. Most computational humor detection work done in the context of AI relies on supervised or semi-supervised methods and focuses on specific, narrow, types of jokes or humor.\nHumor detection is usually formulated as a binary text classification problem. Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al., 2020; Donahue et al., 2017; Ortega-Bueno et al., 2018; Zhang and Liu, 2014), humorous product reviews (Ziser et al., 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al., 2015), and even “That’s what she said” jokes (Hossain et al., 2017; Kiddon and Brun, 2011). Related tasks such as irony, sarcasm and satire have also been explored in similarly narrow domains (Davidov et al., 2010; Reyes et al., 2012; Ptáček et al., 2014)."
    }, {
      "heading" : "3 Problem Formulation and Dataset",
      "text" : "Our goal in this paper is to automatically identify candidates for the Ig Nobel prize. More precisely, to automatically detect humor in scientific papers.\nFirst, we consider the question of input to our algorithm. Sagi and Yechiam (2008) found a strong correlation between funny title and humorous subject in scientific papers. Motivated by this correlation, we manually inspected a subset of Ig Nobel winners. For the vast majority of them, reading the title was enough to determine whether it is funny; very rarely did we need to read the abstract, let alone the full paper. Typical past winners’ titles include “Why do old men have big ears?” and “If you drop it, should you eat it? Scientists weigh in on the 5-second rule”. An example of a non-informative title is “Pouring flows”, a paper calculating the optimal way to dunk a biscuit in a cup of tea.\nBased on this observation, we decided to focus on the papers’ titles. More formally: Given a title t of an article, our goal is to learn a binary function ϕ(t)→ {0, 1}, reflecting whether the paper is humorous, or ‘Ig Nobel-worthy’. The main challenge, of course, lies in the construction of ϕ.\nTo take a data-driven approach to tackle this problem, we crafted a first-of-its-kind dataset containing titles of funny scientific papers2. We started from the 211 Ig Nobel winners. Next, we manually collected humorous papers from online forums and blogs3, resulting in 1,707 papers. We manually verified all of these papers can be used as positive examples. In Section 6 we give more indication these papers are indeed useful for our task.\nFor negative examples, we randomly sampled 1,707 titles from Semantic Scholar4 (to obtain a balanced dataset). We then classify each paper into one of the following scientific fields: neuroscience, medicine, biology, or exact sciences5. We balanced the dataset in a per-field manner. While some of these randomly sampled papers could, in principle, be funny, the vast majority of scientific papers are not (we validated this assumption through sampling)."
    }, {
      "heading" : "4 Humor-Theory Inspired Features",
      "text" : "In deep learning, architecture engineering largely took the place of feature engineering. One of the goals of our work is to evaluate the value of features inspired by domain experts. In this section, we describe and formalize 127 features implementing insights from humor literature. To validate the predictive power of the features that require training, we divide our data to train and test sets (80%/20%). We now describe the four major feature families."
    }, {
      "heading" : "4.1 Unexpected Language",
      "text" : "Research suggests that surprise is an important source of humor (Raskin, 1985; Suls, 1972). Indeed, we notice that titles of Ig Nobel winners often include an unexpected term or unusual language, e.g.: “On the rheology of cats”, “Effect of coke on sperm motility” and “Pigeons’ discrimination of paintings by Monet and Picasso”. To quantify unexpectedness, we create several different languagemodels (LMs):\n3E.g., reddit.com/r/ScienceHumour, popsci.com/read/funny-science-blog, goodsciencewriting.wordpress.com\n4api.semanticscholar.org/corpus/ 5Using scimagojr.com to map venues to fields.\nN-gram Based LMs. We train simple N-gram LMs with n ∈ {1, 2, 3} on two corpora – 630,000 titles from Semantic Scholar, and 231,600 one-line jokes (Moudgil, 2016). Syntax-Based LMs. Here we test the hypothesis that humorous text has more surprising grammatical structure (Oaks, 1994). We replace each word in our Semantic Scholar corpus with its corresponding part-of-speech (POS) tag6. We then trained N-gram based LMs (n ∈ {1, 2, 3}) on this corpus. Transformer-Based LMs. We use three different Transformers based (Vaswani et al., 2017) models: 1) BERT (Devlin et al., 2018) (pre-trained on Wikipedia and the BookCorpus), 2) SciBERT (Beltagy et al., 2019), a variant of BERT optimized on scientific text from Semantic Scholar, and 3) GPT-2 (Radford et al., 2019), a large Transformer-based LM, trained on a dataset of 8M web pages. We fine-tuned GPT-2 on our Semantic Scholar corpora (details in Appendix C.1). Using the LMs. For each word in a title, we compute the word’s perplexity. For the N-gram LMs and GPT-2, we compute the probability to see the word given the previous words in the sentence (n − 1 previous words in the case of the N-gram models and all the previous words in the case of GPT-2). For the BERT-based models, we compute the masked loss of the word given the sentence. For each title, we computed the mean, maximum, and variance of the perplexity across all words in the title."
    }, {
      "heading" : "4.2 Simple Language",
      "text" : "Inspired by previous findings (Ruch, 1992; Gultchin et al., 2019), we hypothesize that titles of funny papers tend to be simpler (e.g., the past Ig Nobel winners: “Chickens prefer beautiful humans” and “Walking with coffee: Why does it spill?”). We utilize several simplicity measures: Length. Short titles and titles containing many short words tend to be simpler. We compute title length and word lengths (mean, maximum, and variance of word lengths in the title). Readability. We use the automated readability index (Smith and Senter, 1967). Age of Acquisition (AoA). A well-established measure for word’s difficulty in psychology (Brysbaert and Biemiller, 2017), denoting word’s difficulty by the age a child acquires it. We compute mean, maximum and variance AoA.\n6Obtained using NLTK (nltk.org)\nAoA and Perplexity. Many basic words can be found in serious titles (e.g., ‘water’ in a hydraulics paper). Funny titles, however, contain simple words which are also unexpected. Thus, we combine AoA with perplexity. We compute word perplexity using the Semantic Scholar N-gram LMs and divide it by AoA. Higher values correspond to simpler and unexpected words. We compute the mean, maximum, minimum, and variance."
    }, {
      "heading" : "4.3 Crude Language",
      "text" : "According to relief theory, crude and scatological connotations are often considered humorous (Shurcliff, 1968) (e.g., the Ig Nobel winners “Duration of urination does not change with body size”, “Acute management of the zipper-entrapped penis”).\nWe trained a Naive Bayes SVM (Wang and Manning, 2012) classifier over a dataset of toxic and rude Wikipedia comments (Zafar, 2018), and compute title probability to be crude. Similar to the AoA feature, we believe that crude words should also be unexpected to be considered funny. As before, we divide perplexity by the word’s probability of being benign. Higher values correspond to crude and unexpected words. We compute the mean, maximum, minimum, and variance."
    }, {
      "heading" : "4.4 Funny Language",
      "text" : "Some words (e.g., nincompoop, razzmatazz) are inherently funnier than others (due to various reasons surveyed by Gultchin et al. (2019)). It is reasonable that the funniness of a title is correlated with the funniness of its words. We measure funniness using the model of Westbury and Hollis (2019), quantifying noun funniness based on humor theories and human ratings. We measure the funniness of each noun in a title. We also multiplied perplexity and funniness (for funny and unexpected) and use the mean, maximum, minimum, and variance."
    }, {
      "heading" : "4.5 Feature Importance",
      "text" : "As a first reality check, we plotted the distribution of our features between funny and not-funny papers (see Appendix A.1 for representative examples). For example, we hypothesized that titles of funny papers might be linguistically similar to one-liners, and indeed we saw that the one-liner LM assigns lower perplexity to funny papers. Similarly, we saw a difference between the readability scores.\nTo measure the predictive power of our literatureinspired features, we use the Wilcoxon signed-rank\ntest7 (see Table 1). Interestingly, all feature families include useful features. Combining perplexity with other features (e.g., surprising and simple words) was especially prominent. In the next sections, we describe how we use those features to train models for detecting Ig Nobel worthy papers."
    }, {
      "heading" : "5 Models",
      "text" : "We can now create models to automatically detect scientific humor. As mentioned in Section 4, one of our goals in this paper is to compare between the NLP SOTA huge-models approach and the literature-inspired approach. Thus, we trained a binary multi-layer perceptron (MLP) classifier using our dataset (described in Section 3, see reproducibility details in Appendix C.2), receiving as input the 127 features from Section 4. We named this classifier ‘Iggy’, after the Ig Nobel prize.\n7A non-parametric paired difference test used to assess whether the mean ranks of two related samples differ.\nAs baselines representing the contemporary NLP approach (requiring huge compute and training data), we used BERT (Devlin et al., 2018) and SciBERT (Beltagy et al., 2019), which is a BERT variant optimized on scientific corpora, rendering it potentially more relevant for our task. We finetuned SciBERT and BERT for Ig Nobel classification using our dataset (see Appendix C.3 for implementation details).\nWe also experimented with two models combining BERT/SciBERT with our features (see Figure 6 in Appendix C.4), denoted as BERTf / SciBERTf . In the spirit of the original BERT paper, we added two linear layers on top of the models and used a standard cross-entropy loss. The input to this final MLP is the concatenation of two vectors: our features’ embedding and the last hidden vector from BERT/SciBERT ([CLS]). See Appendix C.4 for implementation details.\nFor the sake of completeness, we note that we also conducted exploratory experiments with simple syntactic baselines (title length, maximal word length, title containing a question, title containing a colon) as well as BERT trained on sarcasm detection8. None of these baselines was strong enough on its own. We note that the colon-baseline tended to catch smart-aleck titles, but the topic was not necessarily funny. The sarcasm baseline achieved near guess-level accuracy (0.482), emphasizing the distinction between the two humor tasks."
    }, {
      "heading" : "6 Evaluation on the Dataset",
      "text" : "We first evaluate the five models (Iggy, SciBERT, BERT, SciBERTf and BERTf ) on our labeled dataset in terms of general accuracy and Ig Nobel retrieval ability. As naive baselines, we added two bag of words (BoW) based classifies: random forest (RF) and logistic regression (LR).\nAccuracy. We randomly split the dataset to train, development, and test sets (80%−10%−10%), and used the development set to tune hyper-parameters (e.g., learning rate, number of training epochs). Table 2 summarizes the results. We note that all five models achieve very high accuracy scores and that the simple BoW models fall behind. This gives some indication about the inherent difficulty of the task. Both features-based Iggy and BERT-based models outperform simple baseline. SciBERTf outperforms the other models across all measures. 8kaggle.com/raghavkhemka/ sarcasm-detection-using-bert-92-accuracy\nIg Nobel Winners Retrieval. Our positive examples consist of 211 Ig Nobel winners and additional 1,496 humorous papers found on the web. Thus, the portion of real Ig Nobel winning papers in our data is relatively small. We now measure whether our web-originated papers serve as a good proxy for Ig Nobel winners. Thus, we split the dataset differently: the test set consists of the 211 Ig Nobel winners, plus a random sample of 211 negative titles (slightly increasing the test set size to 12%). Train set consists of the remaining 2,992 papers. This experiment follows our initial inspiration of finding Ig Nobel-worthy papers, as we test our models’ ability to retrieve only the real winners.\nTable 3 demonstrate that our web-based funny papers are indeed a good proxy for Ig Nobel winners. Similar to the previous experiment, the combination of SOTA pretrained models with literature based features is superior.\nBased on both experiments, we conclude that our features are indeed informative for our Ig Nobelworthy papers detection task."
    }, {
      "heading" : "7 Evaluation “in the Wild”",
      "text" : "Our main motivation in this work is to recommend papers worthy of an Ig Nobel prize. In this section,\nwe test our models in a more realistic setting; we run them on a large sample of scientific papers, ranking each paper according to their certainty in the label (‘humorous’), and identifying promising candidates. We use the same dataset of 630k papers from Semantic Scholar used for training the LMs (Section 4). We compute funniness according to our models (excluding random forest and logistic regression, which performed poorly). Table 4 shows examples of top-rated titles. We use the Amazon Mechanical Turk (MTurk) crowdsourcing platform to assess models’ performance.\nIn an exploratory study, we asked people to rate the funniness of titles on a Likert scale of 1-5. We noted that people tended to confuse funny research topic and funny title. For example, titles like “Are you certain about SIRT?” or “NASH may be trash” received high funniness scores, even though the research topic is not even clear from the title. To mitigate this problem, we redesigned the study to include two 5-point Likert scale questions: 1) whether the title is funny, and 2) whether the research topic is funny. This addition seems to indeed help workers understand the task better. Example papers rated as serious title, funny topic include “Hat-wearing patterns in spectators attending baseball games: a 10-year retrospective comparison”. Funny title, serious topic include “Slicing the psychoanalytic pie: or, shall we bake a new one? Commentary on Greenberg”. Unless stated otherwise, the evaluation in the reminder of the paper was done on the “funny topic” Likert scale.\nWe paid crowd workers $0.04 per title. As this task is challenging, we created a qualification test with 4 titles (8 questions), allowing for one mis-\ntake. The code for task and test can be found in the repository2. We also required workers to have completed at least 1,000 approved HITs with at least 97% success rate.\nAll algorithms classified and ranked (according to certainty) all 630k papers. However, in any reasonable use-case, only the top of the ranked list will ever be examined. There is a large body of work, both in academia and industry, studying how people interact with ranked lists (in particular, search result pages) (Kelly and Azzopardi, 2015; Beus, 2020). Many information retrieval algorithms assume the likelihood of the user examining a result to exponentially decrease with rank. The conventional wisdom is that users rarely venture into the second page of search results.\nThus, we posit that in our scenario of Ig Nobel recommendations, users will be willing to read only the several tens of results. We choose to evaluate the top-300 titles for each of our five models, to study (in addition to the performance at the top of the list) how performance decays. We also included a baseline of 300 randomly sampled titles from Semantic Scholar. Altogether we evaluated 1375 titles (due to overlap). Each title was rated by five crowd workers. Overall, 13 different workers passed our test. Seven workers annotated less than 300 titles, while four annotated above 1,300 each.\nDecision rule. Each title was rated by five different crowd workers on a 1-5 scale. There are several reasonable ways to aggregate these five continuous scores to a binary decision. A commonly-used aggregation method is the majority vote. The majority vote should return the clear-cut humorous titles. However, we stress that humor is very subjective\n(and in the case of scientific humor, quite subtle). Indeed, annotators had low agreement on the topic question (average pairwise Spearman ρ = 0.27).\nThus, we explored more aggregation methods9. Our hypothesis class is of the general form “at least k annotators gave a score at least m” 10. To pick the best rule, we conducted two exploratory experiments: In the first one, we recruited an expert scientist and thoroughly trained him on the problem. He then rated 90 titles and we measured the correlation of different aggregations with his ratings. Results are summarized in table 5: The highest-correlation aggregation is when at least one annotator crossed the 3 threshold (Spearman ρ = 0.7).\nIn the second experiment, we used the exact same experimental setup as the original task, but with labeled data. We used 100 Ig Nobel winners as positives and a random sample of 100 papers as negatives. The idea was to see how crowd workers rate papers that we know are funny (or not). Table 5 shows the accuracy of each aggregation method. Interestingly, the highest accuracy is achieved with the same rule as in the first experiment (at least one crossing 3). Thus, we chose this aggregation rule.\nWe believe the method outlined in this section could be more broadly applicable to aggregation of crowd sourced annotations for subjective questions.\nResults. Figure 1 shows precision at k for the top-rated 300 titles according to each model. The random baseline is∼ 0.03. Upon closer inspection, these seem to be false positives of the annotation.\nWe have argued that in our setting it is reasonable for users to read the first several tens of results.\n9For completeness, see Figure 3 in Appendix A.2. 10There is a long-running debate about whether it is valid to average Likert scores. We believe we cannot treat the ratings in this study as interval data.\nIn this range, Iggy slightly outperforms the other four models (BERT is particularly bad, as it picks up on short, non-informative titles). For larger k values SciBERT and BERTf take the lead. We note that even at k = 300, all models still achieve considerable (absolute) precision.\nWe obtain similar results using normalized discounted cumulative gain (nDCG), a common measure for ranking quality (see Table 6 for nDCG scores for the top 50 and the 300 papers). Overall, these relatively high scores suggest that our models are able to identify funny papers.\nWe stress that Iggy is a small and simple network (∼ 33k parameters), compared to pretrained 110 million parameters BERT-based models. Yet despite its simplicity, Iggy’s performance is roughly comparable to BERT-based methods. We believe this demonstrates the power of implementing insights from domain experts. We hypothesize that if the fine-tuning dataset were larger, BERTf and SciBERTf would outperform the other models."
    }, {
      "heading" : "8 Analysis",
      "text" : ""
    }, {
      "heading" : "8.1 Importance of Literature-based Features",
      "text" : "Taking a closer look at the actual papers in the experiment of Section 7, the overlap between the three feature-based models is 26− 56% (for 1 < k < 50) and 39− 62% (for 1 < k < 300). BERT had very low overlaps with all other models (0% in top 50, 10% in all 300). SciBERT had almost no overlap in top 50 (maximum 2%), 10− 40% in all 300 (see full details in Appendix A.3). We believe this implies that the features were indeed important and informative for both BERTf and SciBERTf ."
    }, {
      "heading" : "8.2 Interpreting Iggy",
      "text" : "We have seen Iggy performs surprisingly well, given its relative simplicity. In this section, we\nwish to better understand the reasons. We chose to analyze Iggy with Shapely additive explanations (SHAP) (Lundberg and Lee, 2017). SHAP is a feature attribution method to explain the output of any black-box model, shown to be superior to more traditional feature importance methods. Importantly, SHAP provides insights both globally and locally (i.e., for specific data points).\nGlobal interpretability. We compute feature importance globally. Among top contributing features we see multiple features corresponding to incongruity (both alone and combined with funniness) and to word/sentence simplicity. Interestingly, features based on the one-liner jokes seem to play an important role (See Figure 4 in Appendix A.4).\nLocal interpretability. To understand how Iggy errs, we examined the SHAP decision plots for false positives and false negatives (See Figure 5 in Appendix A.4). These show the contribution of each feature to the final prediction for a given title, and thus can help “debugging” the model.\nLooking at false negatives, it appears that various perplexity features misled Iggy, while funniness and joke LM steered it in the right direction. We see a contrary trend in false positives: perplexity helped, and joke LM confused the classifier.\nWe also observe that the model learned that a long title is an indication of a serious paper. We expected our rudeness classifier to play a bigger role in some of the titles (e.g., “Adaptive interpopulation differences in blue tit life-history traits on Corsica”), but the signal was inconclusive, perhaps indicating our rudeness classifier is lacking."
    }, {
      "heading" : "8.3 Observations",
      "text" : "We now take a more qualitative approach to understand the models. First, we set out to explore whether the models confuse funny titles and funny topics. Using the crowd sourced annotations from Section 7, we measure the portion of this mistake in the top-rated 300 titles of all five models. That is, we check in how many cases our models classify a title as “Ig Nobel-worthy” while the workers have classified it as “funny title and non-funny topic”. Iggy had the highest degree of such confusion (0.28). Similarly, BERTf and SciBERTf exhibit more confusion than the versions without features (0.24, 0.19 compared to 0.13, 0.08). Random baseline is 0.02. Examples of this kind of error include “A victim of the Occam’s razor.”, “While waiting to buy a Ferrari, do not leave your current car in the garage!”, and “Reinforcement learning: The good, the bad and the ugly?”. All were classified as Ig Nobel-worthy, although their topic is serious (or even unclear from the title).\nLooking closer at the data, we observe that a high portion of these are editorials with catchy titles. As our dataset does not differentiate between editorials and real research contributions, filtering editorials is not straightforward. Interestingly, the portion of editorials is also greater in the lowest annotators’ agreement area, hinting that this confusion also occurs in humans.\nIn addition to editorials, we notice another category of papers causing the same type of confusion. There are papers dealing with disturbing or unfortunate topics (violence, death, sexual abuse), whose titles include literary devices used to lighten\nthe mood. Censored (for the readers’ own wellbeing) examples include “Licorice for hepatitis C: yum-yum or just ho-hum?”, “The song of the siren: Dealing with masochistic thoughts and behaviors”.\nA note on scientific disciplines. Another observation we make concerns with the portion of Ig Nobelworthiness across the different scientific disciplines. We notice that most papers classified by our models as funny belong to social sciences (“Dogs can discriminate human smiling faces from blank expressions”) or medicine (“What, if anything, can monkeys tell us about human amnesia when they can’t say anything at all?”), compared to exact sciences (“The kinematics of eating with a spoon: bringing the food to the mouth, or the mouth to the food?”). We believe this might be the case since, quite often, social sciences and medicine papers study topics that are more familiar to the layperson. We also note that although our models performed about the same across the different disciplines, they were slightly better in psychology."
    }, {
      "heading" : "9 Conclusions & Future Work",
      "text" : "In this work, we presented a novel task in humor recognition – detecting funny and unusual scientific papers, which represents a subtle and sophisticated humor type. It has important characteristics (short, simple syntax, stand-alone) making it a (relatively) clean setting to explore computational humor.\nWe created a dataset of funny papers and constructed models, distilling humor literature into features as well as harnessing SOTA advances in NLP. We conducted experiments both on our dataset and in a real-world setting, identifying funny papers in a corpus of over 0.6M papers. All models were able to identify funny papers, achieving high nDCG scores. Interestingly, despite the simplicity of the literature-based Iggy, its performance was overall comparable to complex, BERT-based models.\nOur dataset can be further used for various humor related tasks. For example, it is possible to use it to create an aligned corpus, pairing every funny paper title with a nearly identical but serious title, using methods similar to West and Horvitz (2019). This would allow us to understand why a paper is funny at a finer granularity, by identifying the exact words that make the difference. This technique will also allow exploring different types of “funny”.\nAnother possible use of our dataset is to collect additional meta-data about the papers (e.g., citations, author information) to explore questions\nabout whether funny science achieves disproportionate attention and engagement, who tends to produce it (and at which career stage), with implications to science of science and science communication.\nAnother interesting direction is to expand beyond paper titles and consider the paper abstract, or even full text. This could be useful in examples such as the Ig Nobel winner “Cure for a Headache”, which takes inspiration from woodpeckers to help cure headaches in humans.\nFinally, we believe multi-task learning is a direction worth pursuing towards creating a more holistic and robust humor classifier. In multi-task learning, the learner is challenged to solve multiple problems at the same time, often resulting in better generalization and better performance on each individual task (Ruder, 2017). As multi-task learning enables unraveling cross-task similarities, we believe it might be particularly fruitful to apply to tasks highlighting different aspects of humor. We believe our dataset, combined with other task specific humor datasets, could assist in pursuing such a direction.\nDespite the tongue-in-cheek nature of our task, we believe that computational humor has tremendous potential to create personable interactions, and can greatly contribute to a range of NLP applications, from chatbots to educational tutors. We also wish to promote complementing data-driven research with insights from more-traditional fields. We believe combining such insights could, in addition to improving performance, enrich our understanding of core aspects of being human."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their insightful comments. We thank Omri Abend, Michael Doron and Meirav Segal, Ronen Tamari and Moran Mizrahi for their help, and Shuki Cohen for preliminary discussions. This work was supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant no. 852686, SIAM), US National Science Foundation, US-Israel Binational Science Foundation (NSF-BSF) grant no. 2017741, and Amazon Research Awards."
    }, {
      "heading" : "A Supplementary figures",
      "text" : "A.1 Dataset Analysis\nIn Section 4 we presented 127 humor literaturebased features. Here we present the distribution of two example features in funny vs. serious papers in our dataset (described in Section 3). These examples represent the general trend, as many features show predictive power (see Figure 2).\nA.2 “In the Wild” Study Results\nFor the “in the wild” evaluation executed using Semantic Scholar data, we used crowdsourcing annotations (see Section 7). Each title was rated by five different crowd workers on a 1-5 scale, while our models provide binary decision. There are several reasonable ways to aggregate these five continuous scores to a binary decision. We choose a rule in a data-driven manner (see “Decision rule” in Section 7). For completeness, here we show the commonlyused aggregation method of majority. We show here the precision at k of our five models using the majority vote aggregation rule with a cutoff at 3 (see Figure 3). Iggy outperforms until k = 30, where SciBERTf takes the lead afterwards.\nA.3 Models’ Overlap\nIn Section 8.1 we discuss the importance of our literature-based features by showing that models who received them as input indeed found them useful. The overlap was measured on the top 50 and top 300 papers retrieved using our five models on the Semantic Scholar data (see Section 7 for the full experimental setup). The overlap between the 3 features-based models was found to be high (see Table 7). Both BERT and SciBERT had very low overlaps with all other models. We believe this implies that the features were indeed important for our SOTA based models, BERTf and SciBERTf .\nA.4 SHAP Analysis\nIn Section 8.2 we analysed Iggy using SHAP (Lundberg and Lee, 2017). We compute feature importance globally (Figure 4). To understand how Iggy errs, we examined the SHAP decision plots for false positives and false negatives (Figure 5). Decision plots show the contribution of each feature to the final prediction for a given title. Thus, it can help “debugging” the model’s mistakes."
    }, {
      "heading" : "B Reproducibility",
      "text" : "B.1 Code and Data Availability Dataset, code, and data files can be found in our Github repository2.\nC Implementation details\nC.1 Fine-Tuning GPT-2 LM To fine-tune GPT-2 we used Huggingface’s Transformers package11. We fine-tuned the model using learning rate = 5e−5, one epoch, batch size of 4, weight decay = 0, max gradient norm = 1 and random seed = 42. Optimization was done using Adam with epsilon = 1e−8. Model configurations were set to default.\nC.2 Iggy Classifier We used a simple MLP with a single hidden layer of 256 neurons. We trained the MLP until convergence, using Adam optimizer, a learning rate of 0.001 and an L2 penalty of 2.\nC.3 Fine-tuning SciBERT & BERT To fine-tune SciBERT & BERT we used Huggingface’s Transformers package. We fine-tuned both models with learning rate = 5e−5 for 3 epochs with batch size of 32, maximal sequence length of 128 and random seed = 42. Optimization was done using Adam with warm-up = 0.1 and weight decay of 0.01 Model configurations were set to default.\nC.4 SciBERTf & BERTf Models As specified in Section 5, these models were constructed as follows (see Figure 6). Each model had two inputs – the raw text of the title, and a vector of our 127 features. The feature vector is fed to an MLP with a single hidden layer of 512 neurons and an output size of 512 neurons as well. The raw text is fed to a frozen SciBERT /BERT model. We collect the last hidden vector ([CLS]) from BERT /SciBERT. Next, we concatenate this vector to the output of the features-MLP network and pass the result to a second MLP with a single hidden layer of 1,024 neurons. The output of this MLP, then, is fed to a Softmax layer, which represents the final prediction of the model.\nWe train the model using a cross-entropy loss and the same parameters that were used to train the vanilla SciBERT /BERT model. Those parameters are described in Appendix C.3.\n11huggingface.co/transformers/\nIggy BERTf SciBERTf BERT BERTf 0.56 ‖ 0.62 SciBERTf 0.26 ‖ 0.39 0.36 ‖ 0.48 BERT 0 ‖ 0 0 ‖ 0.1 0 ‖ 0.1 SciBERT 0 ‖ 0.3 0.02 ‖ 0.4 0.02 ‖ 0.2 0 ‖ 0.1\n(a) SHAP decision plot for the 12 false negative of Iggy from our test set. Perplexity features misled Iggy, while funniness and joke LM ones provided informative input.\n(b) SHAP decision plot for the 11 false positive of Iggy from our test set. Perplexity helped shifting the output towards the correct label, joke LM features confused the classifier\nFigure 5: SHAP decision plot for Iggy’s false negatives and positives from our test set. Decision plots show the contribution of each feature to the final prediction for a given data point. Starting at the bottom of the plot, the prediction line shows how the SHAP values (i.e., the feature effects) accumulate to arrive at the model’s final score at the top of the plot. To get a better intuition, one can think of it in terms of a linear model where the sum of effects, plus an intercept, equals the prediction."
    } ],
    "references" : [ {
      "title" : "Scibert: Pretrained contextualized embeddings for scientific",
      "author" : [ "Iz Beltagy", "Arman Cohan", "Kyle Lo" ],
      "venue" : null,
      "citeRegEx" : "Beltagy et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "A long shortterm memory framework for predicting humor in dialogues",
      "author" : [ "Dario Bertero", "Pascale Fung." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Bertero and Fung.,? 2016",
      "shortCiteRegEx" : "Bertero and Fung.",
      "year" : 2016
    }, {
      "title" : "Why (almost) everything you knew about google ctr is no longer valid",
      "author" : [ "Johannes Beus" ],
      "venue" : null,
      "citeRegEx" : "Beus.,? \\Q2020\\E",
      "shortCiteRegEx" : "Beus.",
      "year" : 2020
    }, {
      "title" : "Large dataset and language model fun-tuning for humor recognition",
      "author" : [ "Vladislav Blinov", "Valeria Bolotova-Baranova", "Pavel Braslavski." ],
      "venue" : "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 4027–",
      "citeRegEx" : "Blinov et al\\.,? 2019",
      "shortCiteRegEx" : "Blinov et al\\.",
      "year" : 2019
    }, {
      "title" : "Testbased age-of-acquisition norms for 44 thousand english word meanings",
      "author" : [ "Marc Brysbaert", "Andrew Biemiller." ],
      "venue" : "Behavior research methods, 49(4):1520–1523.",
      "citeRegEx" : "Brysbaert and Biemiller.,? 2017",
      "shortCiteRegEx" : "Brysbaert and Biemiller.",
      "year" : 2017
    }, {
      "title" : "Semi-supervised recognition of sarcastic sentences in twitter and amazon",
      "author" : [ "Dmitry Davidov", "Oren Tsur", "Ari Rappoport." ],
      "venue" : "Proceedings of the fourteenth conference on computational natural language learning, pages 107–116. Association for",
      "citeRegEx" : "Davidov et al\\.,? 2010",
      "shortCiteRegEx" : "Davidov et al\\.",
      "year" : 2010
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Humorhawk at semeval-2017 task 6: Mixing meaning and sound for humor recognition",
      "author" : [ "David Donahue", "Alexey Romanov", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),",
      "citeRegEx" : "Donahue et al\\.,? 2017",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2017
    }, {
      "title" : "Humor in word embeddings: Cockamamie gobbledegook for nincompoops",
      "author" : [ "Limor Gultchin", "Genevieve Patterson", "Nancy Baym", "Nathaniel Swinger", "Adam Tauman Kalai." ],
      "venue" : "arXiv preprint arXiv:1902.02783.",
      "citeRegEx" : "Gultchin et al\\.,? 2019",
      "shortCiteRegEx" : "Gultchin et al\\.",
      "year" : 2019
    }, {
      "title" : "Filling the blanks (hint: plural noun) for mad libs humor",
      "author" : [ "Nabil Hossain", "John Krumm", "Lucy Vanderwende", "Eric Horvitz", "Henry Kautz." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 638–647.",
      "citeRegEx" : "Hossain et al\\.,? 2017",
      "shortCiteRegEx" : "Hossain et al\\.",
      "year" : 2017
    }, {
      "title" : "How many results per page? a study of serp size, search behavior and user experience",
      "author" : [ "Diane Kelly", "Leif Azzopardi." ],
      "venue" : "Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 183–",
      "citeRegEx" : "Kelly and Azzopardi.,? 2015",
      "shortCiteRegEx" : "Kelly and Azzopardi.",
      "year" : 2015
    }, {
      "title" : "That’s what she said: double entendre identification",
      "author" : [ "Chloe Kiddon", "Yuriy Brun." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for",
      "citeRegEx" : "Kiddon and Brun.,? 2011",
      "shortCiteRegEx" : "Kiddon and Brun.",
      "year" : 2011
    }, {
      "title" : "Modeling sentiment association in discourse for humor recognition",
      "author" : [ "Lizhen Liu", "Donghai Zhang", "Wei Song." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 586–591.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "A unified approach to interpreting model predictions",
      "author" : [ "Scott M Lundberg", "Su-In Lee." ],
      "venue" : "Advances in neural information processing systems, pages 4765–4774.",
      "citeRegEx" : "Lundberg and Lee.,? 2017",
      "shortCiteRegEx" : "Lundberg and Lee.",
      "year" : 2017
    }, {
      "title" : "Analyzing political parody in social media",
      "author" : [ "Antonios Maronikolakis", "Danae Sánchez Villegas", "Daniel Preoţiuc-Pietro", "Nikolaos Aletras." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4373–",
      "citeRegEx" : "Maronikolakis et al\\.,? 2020",
      "shortCiteRegEx" : "Maronikolakis et al\\.",
      "year" : 2020
    }, {
      "title" : "A model of the social functions of humor",
      "author" : [ "William H Martineau." ],
      "venue" : "The psychology of humor, pages 101–125.",
      "citeRegEx" : "Martineau.,? 1972",
      "shortCiteRegEx" : "Martineau.",
      "year" : 1972
    }, {
      "title" : "Making computers laugh: Investigations in automatic humor recognition",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2005",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2005
    }, {
      "title" : "Learning to laugh (automatically): Computational models for humor recognition",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Computational Intelligence, 22(2):126–142.",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2006",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2006
    }, {
      "title" : "Semeval-2017 task 7: Detection and interpretation of english puns",
      "author" : [ "Tristan Miller", "Christian F Hempelmann", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 58–68.",
      "citeRegEx" : "Miller et al\\.,? 2017",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2017
    }, {
      "title" : "Creating structural ambiguities in humor: getting english grammar to cooperate",
      "author" : [ "Dallin D Oaks." ],
      "venue" : "Humor, 7(4):377–402.",
      "citeRegEx" : "Oaks.,? 1994",
      "shortCiteRegEx" : "Oaks.",
      "year" : 1994
    }, {
      "title" : "Uo upv: Deep linguistic humor detection in spanish social media",
      "author" : [ "Reynier Ortega-Bueno", "Carlos E Muniz-Cuza", "José E Medina Pagola", "Paolo Rosso." ],
      "venue" : "Proceedings of the Third Workshop on Evaluation of Human Language Technologies for",
      "citeRegEx" : "Ortega.Bueno et al\\.,? 2018",
      "shortCiteRegEx" : "Ortega.Bueno et al\\.",
      "year" : 2018
    }, {
      "title" : "Sarcasm detection on czech and english twitter",
      "author" : [ "Tomáš Ptáček", "Ivan Habernal", "Jun Hong." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 213–223.",
      "citeRegEx" : "Ptáček et al\\.,? 2014",
      "shortCiteRegEx" : "Ptáček et al\\.",
      "year" : 2014
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic theory",
      "author" : [ "Victor Raskin." ],
      "venue" : "Semantic Mechanisms of Humor, pages 59–98. Springer.",
      "citeRegEx" : "Raskin.,? 1985",
      "shortCiteRegEx" : "Raskin.",
      "year" : 1985
    }, {
      "title" : "Making objective decisions from subjective data: Detecting irony in customer reviews",
      "author" : [ "Antonio Reyes", "Paolo Rosso." ],
      "venue" : "Decision support systems, 53(4):754–760.",
      "citeRegEx" : "Reyes and Rosso.,? 2012",
      "shortCiteRegEx" : "Reyes and Rosso.",
      "year" : 2012
    }, {
      "title" : "From humor recognition to irony detection: The figurative language of social media",
      "author" : [ "Antonio Reyes", "Paolo Rosso", "Davide Buscaldi." ],
      "venue" : "Data & Knowledge Engineering, 74:1–12.",
      "citeRegEx" : "Reyes et al\\.,? 2012",
      "shortCiteRegEx" : "Reyes et al\\.",
      "year" : 2012
    }, {
      "title" : "Assessment of appreciation of humor: Studies with the 3 wd humor test",
      "author" : [ "Willibald Ruch." ],
      "venue" : "Advances in personality assessment, 9:27–75.",
      "citeRegEx" : "Ruch.,? 1992",
      "shortCiteRegEx" : "Ruch.",
      "year" : 1992
    }, {
      "title" : "An overview of multitask learning in deep neural networks",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "ArXiv, abs/1706.05098.",
      "citeRegEx" : "Ruder.,? 2017",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2017
    }, {
      "title" : "Amusing titles in scientific journals and article citation",
      "author" : [ "Itay Sagi", "Eldad Yechiam." ],
      "venue" : "Journal of Information Science, 34(5):680–687.",
      "citeRegEx" : "Sagi and Yechiam.,? 2008",
      "shortCiteRegEx" : "Sagi and Yechiam.",
      "year" : 2008
    }, {
      "title" : "Press release",
      "author" : [ "Nobel Prize Committee The Royal Swedish Academy of Science." ],
      "venue" : "https://www.nobelprize.org/prizes/ physics/2010/press-release/. [Online; 5 October 2010].",
      "citeRegEx" : "Science.,? 2010",
      "shortCiteRegEx" : "Science.",
      "year" : 2010
    }, {
      "title" : "Inside jokes: Identifying humorous cartoon captions",
      "author" : [ "Dafna Shahaf", "Eric Horvitz", "Robert Mankoff." ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1065–1074.",
      "citeRegEx" : "Shahaf et al\\.,? 2015",
      "shortCiteRegEx" : "Shahaf et al\\.",
      "year" : 2015
    }, {
      "title" : "Philosophy of humor",
      "author" : [ "Joshua Shaw." ],
      "venue" : "Philosophy Compass, 5(2):112–126.",
      "citeRegEx" : "Shaw.,? 2010",
      "shortCiteRegEx" : "Shaw.",
      "year" : 2010
    }, {
      "title" : "Judged humor, arousal, and the relief theory",
      "author" : [ "Arthur Shurcliff." ],
      "venue" : "Journal of personality and social psychology, 8(4p1):360.",
      "citeRegEx" : "Shurcliff.,? 1968",
      "shortCiteRegEx" : "Shurcliff.",
      "year" : 1968
    }, {
      "title" : "Predicting humorousness and metaphor novelty with gaussian process preference learning",
      "author" : [ "Edwin Simpson", "Erik-Lân Do Dinh", "Tristan Miller", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Simpson et al\\.,? 2019",
      "shortCiteRegEx" : "Simpson et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated readability index",
      "author" : [ "Edgar A Smith", "RJ Senter." ],
      "venue" : "AMRL-TR. Aerospace Medical Research Laboratories (US), page 1.",
      "citeRegEx" : "Smith and Senter.,? 1967",
      "shortCiteRegEx" : "Smith and Senter.",
      "year" : 1967
    }, {
      "title" : "Transgressing the boundaries: Toward a transformative hermeneutics of quantum gravity",
      "author" : [ "Alan D Sokal." ],
      "venue" : "Social text, (46/47):217–252.",
      "citeRegEx" : "Sokal.,? 1996",
      "shortCiteRegEx" : "Sokal.",
      "year" : 1996
    }, {
      "title" : "Hahacronym: Humorous agents for humorous acronyms",
      "author" : [ "Oliviero Stock", "Carlo Strapparava." ],
      "venue" : "Stock, Oliviero, Carlo Strapparava, and Anton Nijholt. Eds, pages 125–135.",
      "citeRegEx" : "Stock and Strapparava.,? 2002",
      "shortCiteRegEx" : "Stock and Strapparava.",
      "year" : 2002
    }, {
      "title" : "A two-stage model for the appreciation of jokes and cartoons: An informationprocessing analysis",
      "author" : [ "Jerry M Suls." ],
      "venue" : "The psychology of humor: Theoretical perspectives and empirical issues, 1:81– 100.",
      "citeRegEx" : "Suls.,? 1972",
      "shortCiteRegEx" : "Suls.",
      "year" : 1972
    }, {
      "title" : "Computationally recognizing wordplay in jokes",
      "author" : [ "Julia M Taylor", "Lawrence J Mazlack." ],
      "venue" : "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 26.",
      "citeRegEx" : "Taylor and Mazlack.,? 2004",
      "shortCiteRegEx" : "Taylor and Mazlack.",
      "year" : 2004
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classification",
      "author" : [ "Sida I Wang", "Christopher D Manning." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 90–94.",
      "citeRegEx" : "Wang and Manning.,? 2012",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2012
    }, {
      "title" : "Are funny people popular? an examination of humor orientation, loneliness, and social attraction",
      "author" : [ "Melissa Bekelja Wanzer", "Melanie Booth-Butterfield", "Steve Booth-Butterfield." ],
      "venue" : "Communication Quarterly, 44(1):42–52.",
      "citeRegEx" : "Wanzer et al\\.,? 1996",
      "shortCiteRegEx" : "Wanzer et al\\.",
      "year" : 1996
    }, {
      "title" : "Omg ur funny! computer-aided humor with an application to chat",
      "author" : [ "Miaomiao Wen", "Nancy Baym", "Omer Tamuz", "Jaime Teevan", "Susan T Dumais", "Adam Kalai." ],
      "venue" : "ICCC, pages 86–93.",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Reverseengineering satire, or” paper on computational humor accepted despite making serious advances",
      "author" : [ "Robert West", "Eric Horvitz." ],
      "venue" : "arXiv preprint arXiv:1901.03253.",
      "citeRegEx" : "West and Horvitz.,? 2019",
      "shortCiteRegEx" : "West and Horvitz.",
      "year" : 2019
    }, {
      "title" : "Wriggly, squiffy, lummox, and boobs: What makes some words funny",
      "author" : [ "Chris Westbury", "Geoff Hollis" ],
      "venue" : "Journal of Experimental Psychology: General,",
      "citeRegEx" : "Westbury and Hollis.,? \\Q2019\\E",
      "shortCiteRegEx" : "Westbury and Hollis.",
      "year" : 2019
    }, {
      "title" : "Modelling suspense in short stories as uncertainty reduction over neural representation",
      "author" : [ "David Wilmot", "Frank Keller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1763–1788, Online. Association",
      "citeRegEx" : "Wilmot and Keller.,? 2020",
      "shortCiteRegEx" : "Wilmot and Keller.",
      "year" : 2020
    }, {
      "title" : "Recognizing humor on twitter",
      "author" : [ "Renxian Zhang", "Naishi Liu." ],
      "venue" : "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 889–898. ACM.",
      "citeRegEx" : "Zhang and Liu.,? 2014",
      "shortCiteRegEx" : "Zhang and Liu.",
      "year" : 2014
    }, {
      "title" : "Humor detection in product question answering systems",
      "author" : [ "Yftah Ziser", "Elad Kravi", "David Carmel." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 519–528.",
      "citeRegEx" : "Ziser et al\\.,? 2020",
      "shortCiteRegEx" : "Ziser et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Humor is an important aspect of the way we interact with each other, serving complex social functions (Martineau, 1972).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 41,
      "context" : "Humor can function either as a lubricant or as an abrasive: it can be used as a key for improving interpersonal relations and building trust (Wanzer et al., 1996; Wen et al., 2015), or help us work through difficult topics.",
      "startOffset" : 141,
      "endOffset" : 180
    }, {
      "referenceID" : 42,
      "context" : "Humor can function either as a lubricant or as an abrasive: it can be used as a key for improving interpersonal relations and building trust (Wanzer et al., 1996; Wen et al., 2015), or help us work through difficult topics.",
      "startOffset" : 141,
      "endOffset" : 180
    }, {
      "referenceID" : 36,
      "context" : "Some even claim that computational humor is an AI-complete problem (Stock and Strapparava, 2002).",
      "startOffset" : 67,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "As humor is a broad phenomenon, most works on computational humor focus on specific humor types, such as knock-knock jokes or one-liners (Mihalcea and Strapparava, 2006; Taylor and Mazlack, 2004).",
      "startOffset" : 137,
      "endOffset" : 195
    }, {
      "referenceID" : 38,
      "context" : "As humor is a broad phenomenon, most works on computational humor focus on specific humor types, such as knock-knock jokes or one-liners (Mihalcea and Strapparava, 2006; Taylor and Mazlack, 2004).",
      "startOffset" : 137,
      "endOffset" : 195
    }, {
      "referenceID" : 35,
      "context" : "Since our task is to detect humor in scientific contributions, we believe that the third category can be neglected under the assumption that no-nonsense article would (or at least, should) be published (notable exception: the Sokal hoax (Sokal, 1996)).",
      "startOffset" : 237,
      "endOffset" : 250
    }, {
      "referenceID" : 31,
      "context" : "The first category, incongruity, was first fully conceptualized by Kant in the eighteenth century (Shaw, 2010).",
      "startOffset" : 98,
      "endOffset" : 110
    }, {
      "referenceID" : 37,
      "context" : "The well-agreed extensions to incongruity theory are the linguistics incongruity resolution model and semantic script theory of humor (Suls, 1972; Raskin, 1985).",
      "startOffset" : 134,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "The well-agreed extensions to incongruity theory are the linguistics incongruity resolution model and semantic script theory of humor (Suls, 1972; Raskin, 1985).",
      "startOffset" : 134,
      "endOffset" : 160
    }, {
      "referenceID" : 38,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 33,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 12,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 16,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 17,
      "context" : "Example domains include knock-knock jokes (Taylor and Mazlack, 2004), one-liners (Miller et al., 2017; Simpson et al., 2019; Liu et al., 2018; Mihalcea and Strapparava, 2005; Blinov et al., 2019; Mihalcea and Strapparava, 2006), humorous tweets (Maronikolakis et al.",
      "startOffset" : 81,
      "endOffset" : 227
    }, {
      "referenceID" : 47,
      "context" : ", 2018; Zhang and Liu, 2014), humorous product reviews (Ziser et al., 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : ", 2018; Zhang and Liu, 2014), humorous product reviews (Ziser et al., 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : ", 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 45,
      "context" : ", 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al.",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : ", 2020; Reyes and Rosso, 2012), TV sitcoms (Bertero and Fung, 2016), short stories (Wilmot and Keller, 2020), cartoons captions (Shahaf et al., 2015), and even “That’s what she said” jokes (Hossain et al.",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : ", 2015), and even “That’s what she said” jokes (Hossain et al., 2017; Kiddon and Brun, 2011).",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : ", 2015), and even “That’s what she said” jokes (Hossain et al., 2017; Kiddon and Brun, 2011).",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Related tasks such as irony, sarcasm and satire have also been explored in similarly narrow domains (Davidov et al., 2010; Reyes et al., 2012; Ptáček et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 163
    }, {
      "referenceID" : 25,
      "context" : "Related tasks such as irony, sarcasm and satire have also been explored in similarly narrow domains (Davidov et al., 2010; Reyes et al., 2012; Ptáček et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "Related tasks such as irony, sarcasm and satire have also been explored in similarly narrow domains (Davidov et al., 2010; Reyes et al., 2012; Ptáček et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "Research suggests that surprise is an important source of humor (Raskin, 1985; Suls, 1972).",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : "Research suggests that surprise is an important source of humor (Raskin, 1985; Suls, 1972).",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "Here we test the hypothesis that humorous text has more surprising grammatical structure (Oaks, 1994).",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 39,
      "context" : "We use three different Transformers based (Vaswani et al., 2017) models: 1) BERT (Devlin et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : ", 2017) models: 1) BERT (Devlin et al., 2018) (pre-trained on",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Wikipedia and the BookCorpus), 2) SciBERT (Beltagy et al., 2019), a variant of BERT optimized on scientific text from Semantic Scholar, and 3) GPT-2 (Radford et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : ", 2019), a variant of BERT optimized on scientific text from Semantic Scholar, and 3) GPT-2 (Radford et al., 2019), a large Transformer-based LM, trained on a dataset of 8M web pages.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "Inspired by previous findings (Ruch, 1992; Gultchin et al., 2019), we hypothesize that titles of funny papers tend to be simpler (e.",
      "startOffset" : 30,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "Inspired by previous findings (Ruch, 1992; Gultchin et al., 2019), we hypothesize that titles of funny papers tend to be simpler (e.",
      "startOffset" : 30,
      "endOffset" : 65
    }, {
      "referenceID" : 34,
      "context" : "We use the automated readability index (Smith and Senter, 1967).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "A well-established measure for word’s difficulty in psychology (Brysbaert and Biemiller, 2017), denoting word’s difficulty by the age a child acquires it.",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 40,
      "context" : "We trained a Naive Bayes SVM (Wang and Manning, 2012) classifier over a dataset of toxic and rude Wikipedia comments (Zafar, 2018), and compute title probability to be crude.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "18 As baselines representing the contemporary NLP approach (requiring huge compute and training data), we used BERT (Devlin et al., 2018) and SciBERT (Beltagy et al.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : ", 2018) and SciBERT (Beltagy et al., 2019), which is a BERT variant optimized on scientific corpora, rendering it potentially more relevant for our task.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "There is a large body of work, both in academia and industry, studying how people interact with ranked lists (in particular, search result pages) (Kelly and Azzopardi, 2015; Beus, 2020).",
      "startOffset" : 146,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "There is a large body of work, both in academia and industry, studying how people interact with ranked lists (in particular, search result pages) (Kelly and Azzopardi, 2015; Beus, 2020).",
      "startOffset" : 146,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "We chose to analyze Iggy with Shapely additive explanations (SHAP) (Lundberg and Lee, 2017).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "In multi-task learning, the learner is challenged to solve multiple problems at the same time, often resulting in better generalization and better performance on each individual task (Ruder, 2017).",
      "startOffset" : 183,
      "endOffset" : 196
    } ],
    "year" : 2021,
    "abstractText" : "Humor is an important social phenomenon, serving complex social and psychological functions. However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: “Are cows more likely to lie down the longer they stand?”). This challenging task has unique characteristics that make it particularly suitable for automatic learning. We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP. We use our models to identify potentially funny papers in a large dataset of over 630,000 articles. The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines.",
    "creator" : "LaTeX with hyperref"
  }
}