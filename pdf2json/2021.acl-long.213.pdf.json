{
  "name" : "2021.acl-long.213.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Factoring Statutory Reasoning as Language Understanding Challenges",
    "authors" : [ "Nils Holzenberger", "Benjamin Van Durme" ],
    "emails" : [ "nilsh@jhu.edu", "vandurme@cs.jhu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2742–2758\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2742"
    }, {
      "heading" : "1 Introduction",
      "text" : "As more data becomes available, Natural Language Processing (NLP) techniques are increasingly being applied to the legal domain, including for the prediction of case outcomes (Xiao et al., 2018; Vacek et al., 2019; Chalkidis et al., 2019a). In the US, cases are decided based on previous case outcomes, but also on the legal statutes compiled in the US code. For our purposes, a case is a set of facts described in natural language, as in Figure 1, in blue. The US code is a set of documents called statutes, themselves decomposed into subsections. Taken together, subsections can be viewed as a body of interdependent rules specified in natural language, prescribing how case outcomes are to be determined. Statutory reasoning is the task of determining whether a given subsection of a statute applies to a given case, where both are expressed in natural language. Subsections are implicitly framed as predicates, which may be true or\nfalse of a given case. Holzenberger et al. (2020) introduced SARA, a benchmark for the task of statutory reasoning, as well as two different approaches to solving this problem. First, a manually-crafted symbolic reasoner based on Prolog is shown to perfectly solve the task, at the expense of experts writing the Prolog code and translating the natural language case descriptions into Prolog-understandable facts. The second approach is based on statistical machine learning models. While these models can be induced computationally, they perform poorly because the complexity of the task far surpasses the amount of training data available.\nWe posit that statutory reasoning as presented to statistical models is underspecified, in that it was cast as Recognizing Textual Entailment (Dagan et al., 2005) and linear regression. Taking inspiration from the structure of Prolog programs, we re-frame statutory reasoning as a sequence of four tasks, prompting us to introduce a novel extension of the SARA dataset (Section 2), referred to as SARA v2. Beyond improving the model’s performance, as shown in Section 3, the additional structure makes it more interpretable, and so more suitable for practical applications. We put our results in perspective in Section 4 and review related work in Section 5."
    }, {
      "heading" : "2 SARA v2",
      "text" : "The symbolic solver requires experts translating the statutes and each new case’s description into Prolog. In contrast, a machine learning-based model has the potential to generalize to unseen cases and to changing legislation, a significant advantage for a practical application. In the following, we argue that legal statutes share features with the symbolic solver’s first-order logic. We formalize this connection in a series of four challenge tasks, described in this section, and depicted in Figure 1. We hope\nthey provide structure to the problem, and a more efficient inductive bias for machine learning algorithms. The annotations mentioned throughout the remainder of this section were developed by the authors, entirely by hand, with regular guidance from a legal scholar1. Examples for each task are given in Appendix A. Statistics are shown in Figure 2 and further detailed in Appendix B.\nArgument identification This first task, in conjunction with the second, aims to identify the arguments of the predicate that a given subsection represents. Some terms in a subsection refer to something concrete, such as “the United States” or “April 24th, 2017”. Other terms can take a range of values depending on the case at hand, and act as placeholders. For example, in the top left box of Figure 1, the terms “a taxpayer” and “the taxable year” can take different values based on the context, while the terms “section 152” and “this paragraph” have concrete, immutable values. Formally, given a sequence of tokens t1, ..., tn, the task is to return a set of start and end indices (s, e) ∈ {1, 2, ..., n}2 where each pair represents a span. We borrow from the terminology of predicate argument alignment (Roth and Frank, 2012; Wolfe et al., 2013) and call these placeholders arguments. The first task, which we call argument identification, is tagging which parts of a subsection denote such placeholders. We provide annotations for argument identification as character-level spans representing arguments. Since each span is a pointer to the corresponding argument, we made each span the shortest meaningful phrase. Figure 2(b) shows corpus statistics about placeholders.\nArgument coreference Some arguments detected in the previous task may appear multiple times within the same subsection. For instance, in the top left of Figure 1, the variable representing the taxpayer in §2(a)(1)(B) is referred to twice. We refer to the task of resolving this coreference problem at the level of the subsection as argument coreference. While this coreference can span across subsections, as is the case in Figure 1, we intentionally leave it to the next task. Keeping the notation of the above paragraph, given a set of spans {(si, ei)}Si=1, the task is to return a matrix C ∈ {0, 1}S×S where Ci,j = 1 if spans (si, ei) and (sj , ej) denote the same variable, 0 otherwise.\n1The dataset can be found under https://nlp.jhu. edu/law/\nCorpus statistics about argument coreference can be found in Figure 2(a). After these first two tasks, we can extract a set of arguments for every subsection. In Figure 1, for §2(a)(1)(A), that would be {Taxp, Taxy, Spouse, Years}, as shown in the bottom left of Figure 1.\nStructure extraction A prominent feature of legal statutes is the presence of references, implicit and explicit, to other parts of the statutes. Resolving references and their logical connections, and passing arguments appropriately from one subsection to the other, are major steps in statutory reasoning. We refer to this as structure extraction. This mapping can be trivial, with the taxpayer and taxable year generally staying the same across subsections. Some mappings are more involved, such as the taxpayer from §152(b)(1) becoming the dependent in §152(a). Providing annotations for this task in general requires expert knowledge, as many references are implicit, and some must be resolved using guidance from Treasury Regulations. Our approach contrasts with recent efforts in breaking down complex questions into atomic questions, with the possibility of referring to previous answers (Wolfson et al., 2020). Statutes contain their own breakdown into atomic questions. In addition, our structure is interpretable by a Prolog engine.\nWe provide structure extraction annotations for SARA in the style of Horn clauses (Horn, 1951), using common logical operators, as shown in the bottom left of Figure 1. We also provide character offsets for the start and end of each subsection. Argument identification and coreference, and structure extraction can be done with the statutes only. They correspond to extracting a shallow version of the symbolic solver of Holzenberger et al. (2020).\nArgument instantiation We frame legal statutes as a set of predicates specified in natural language. Each subsection has a number of arguments, provided by the preceding tasks. Given the description of a case, each argument may or may not be associated with a value. Each subsection has an @truth argument, with possible values True or False, reflecting whether the subsection applies or not. Concretely, the input is (1) the string representation of the subsection, (2) the annotations from the first three tasks, and (3) values for some or all of its arguments. Arguments and values are represented as an array of key-value pairs, where the names of arguments specified in the structure an-\nnotations are used as keys. In Figure 1, compare the names of arguments in the green box with the key names in the blue boxes. The output is values for its arguments, in particular for the @truth argument. In the example of the top right in Figure 1, the input values are taxpayer = Alice and taxable year = 2017, and one expected output is @truth = True. We refer to this task as argument instantiation. Values for arguments can be found as spans in the case description, or must be predicted based on the case description. The latter happens often for dollar amounts, where incomes must be added, or tax must be computed. Figure 1 shows two examples of this task, in blue. Before determining whether a subsection applies, it may be necessary to infer the values of unspecified arguments. For example, in the top of Figure 1, it is necessary to determine who Alice’s deceased spouse and who the dependent mentioned in §2(a)(1)(B) are. If applicable, we provide values for these arguments, not as inputs, but as additional supervision for the model. We provide manual annotations for all (subsection, case) pairs in SARA. In addition, we run the Prolog solver of Holzenberger et al. (2020) to generate annotations for all possible (subsection, case) pairs, to be used as a silver standard, in contrast to the gold manual annotations. We exclude from the silver data any (subsection, case) pair where the case is part of\nthe test set. This increases the amount of available training data by a factor of 210."
    }, {
      "heading" : "3 Baseline models",
      "text" : "We provide baselines for three tasks, omitting structure extraction because it is the one task with the highest return on human annotation effort2. In other words, if humans could annotate for any of these four tasks, structure extraction is where we posit their involvement would be the most worthwhile. Further, Pertierra et al. (2017) have shown that the related task of semantic parsing of legal statutes is a difficult task, calling for a complex model."
    }, {
      "heading" : "3.1 Argument identification",
      "text" : "We run the Stanford parser (Socher et al., 2013) on the statutes, and extract all noun phrases as spans – specifically, all NNP, NNPS, PRP$, NP and NML constituents. While de-formatting legal text can boost parser performance (Morgenstern, 2014), we found it made little difference in our case.\nAs an orthogonal approach, we train a BERTbased CRF model for the task of BIO tagging. With the 9 sections in the SARA v2 statutes, we create 7 equally-sized splits by grouping §68, 3301 and 7703 into a single split. We run a 7-fold crossvalidation, using 1 split as a dev set, 1 split as a test set, and the remaining as training data. We embed each paragraph using BERT, classify each contextual subword embedding into a 3-dimensional logit with a linear layer, and run a CRF (Lafferty et al., 2001). The model is trained with gradient descent to maximize the log-likelihood of the sequence of gold tags. We experiment with using Legal BERT (Holzenberger et al., 2020) and BERT-base-cased (Devlin et al., 2019) as our BERT model. We freeze its parameters and optionally unfreeze the last layer. We use a batch size of 32 paragraphs, a learning rate of 10−3 and the Adam optimizer (Kingma and Ba, 2015). Based on F1 score measured on the dev set, the best model uses Legal BERT and unfreezes its last layer. Test results are shown in Table 1."
    }, {
      "heading" : "3.2 Argument coreference",
      "text" : "Argument coreference differs from the usual coreference task (Pradhan et al., 2014), even though we are using similar terminology, and frame it in a similar way. In argument coreference, it is equally\n2Code for the experiments can be found under https: //github.com/SgfdDttt/sara_v2\nas important to link two coreferent argument mentions as it is not to link two different arguments. In contrast, regular coreference emphasizes the prediction of links between mentions. We thus report a different metric in Tables 2 and 4, exact match coreference, which gives credit for returning a cluster of mentions that corresponds exactly to an argument. In Figure 1, a system would be rewarded for linking together both mentions of the taxpayer in §2(a)(1)(B), but not if any of the two mentions were linked to any other mention within §2(a)(1)(B). This custom metric gives as much credit for correctly linking a single-mention argument (no links), as for a 5-mention argument (10 links).\nSingle mention baseline Here, we predict no coreference links. Under usual coreference metrics, this system can have low performance.\nString matching baseline This baseline predicts a coreference link if the placeholder strings of two arguments are identical, up to the presence of the words such, a, an, the, any, his and every.\nWe also provide usual coreference metrics in Table 3, using the code associated with Pradhan et al. (2014). This baseline perfectly resolves coreference for 80.8% of subsections, versus 68.9% for the single mention baseline.\nIn addition, we provide a cascade of the best methods for argument identification and coreference, and report results in Table 4. The cascade perfectly resolves a subsection’s arguments in only 16.4% of cases. This setting, which groups the first two tasks together, offers a significant challenge."
    }, {
      "heading" : "3.3 Argument instantiation",
      "text" : "Argument instantiation takes into account the information provided by previous tasks. We start by instantiating the arguments of a single subsection, without regard to the structure of the statutes. We then describe how the structure information is incorporated into the model.\nAlgorithm 1 Argument instantiation for a single subsection Require: argument spans with coreference information A,\ninput argument-value pairs D, subsection text s, case description c\nEnsure: output argument-value pairs P 1: function ARGINSTANTIATION(A,D, s, c) 2: P ← ∅ 3: for a in A \\ {@truth} do 4: r ← INSERTVALUES(s,A,D, P ) 5: y ← BERT(c, r) 6: x← COMPUTEATTENTIVEREPS(y, a) 7: v ← PREDICTVALUE(x) 8: P ← P ∪ (a, v) 9: end for 10: r ← INSERTVALUES(s,A,D, P ) 11: y ← BERT CLS(c, r) 12: t← TRUTHPREDICTOR(y) 13: P ← P ∪ (@truth, t) 14: return P 15: end function\nSingle subsection We follow the paradigm of Chen et al. (2020), where we iteratively modify the text of the subsection by inserting argument values, and predict values for uninstantiated arguments. Throughout the following, we refer to Algorithm 1 and to its notation.\nFor each argument whose value is provided, we replace the argument’s placeholders in subsection s by the argument’s value, using INSERTVALUES (line 4). This yields mostly grammatical sentences, with occasional hiccups. With §2(a)(1)(A) and the top right case from Figure 1, we obtain “(A) Alice spouse died during either of the two years immediately preceding 2017”.\nWe concatenate the text of the case c with the modified text of the subsection r, and embed it using BERT (line 5), yielding a sequence of contextual subword embeddings y = {yi ∈ R768 | i = 1...n}. Keeping with the notation of Chen et al. (2020), assume that the embedded case is represented by the sequence of vectors t1, ..., tm and the embedded subsection by s1, ..., sn. For a given argument a, compute its attentive representation s̃1, ..., s̃m and its augmented feature vectors x1, ...,xm. This operation, described by Chen et al. (2020), is performed by COMPUTEATTENTIVEREPS (line 6). The augmented feature vectors x1, ...,xm represent the argument’s placeholder, conditioned on the text of the statute and case.\nBased on the name of the argument span, we predict its value v either as an integer or a span from the case description, using PREDICTVALUE (line 7). For integers, as part of the model training, we run k-means clustering on the set of all integer values in the training set, with enough centroids such that returning the closest centroid instead of the true value yields a numerical accuracy of 1 (see below). For any argument requiring an integer (e.g. tax), the model returns a weighted average of the centroids. The weights are predicted by a linear layer followed by a softmax, taking as input an average-pooling and a maxpooling of x1, ...,xm. For a span from the case description, we follow the standard procedure for fine-tuning BERT on SQuAD (Devlin et al., 2019). The unnormalized probability of the span from tokens i to j is given by el·xi+r·xj where l, r are learnable parameters.\nThe predicted value v is added to the set of predictions P (line 8), and will be used in subsequent iterations to replace the argument’s placeholder\nin the subsection. We repeat this process until a value has been predicted for every argument, except @truth (lines 3-9). Arguments are processed in order of appearance in the subsection. Finally, we concatenate the case and fully grounded subsection and embed them with BERT (lines 10-11), then use a linear predictor on top of the representation for the [CLS] token to predict the value for the @truth argument (line 12).\nAlgorithm 2 Argument instantiation with dependencies Require: argument spans with coreference information A,\nstructure information T , input argument-value pairs D, subsection s, case description c\nEnsure: output argument-value pairs P 1: function ARGINSTANTIATIONFULL(A, T,D, s, c) 2: t← BUILDDEPENDENCYTREE(s, T ) 3: t← POPULATEARGVALUES(t,D) 4: Q← depth-first traversal of t 5: for q in Q do 6: if q is a subsection and a leaf node then 7: Dq ← GETARGVALUEPAIRS(q) 8: s̃← GETSUBSECTIONTEXT(q) 9: q ← ARGINSTANTIATION(A,Dq, s̃, c) 10: else if q is a subsection and not a leaf node then 11: Dq ← GETARGVALUEPAIRS(q) 12: x← GETCHILD(q) 13: Dx ← GETARGVALUEPAIRS(x) 14: Dq ← Dq ∪Dx 15: s̃← GETSUBSECTIONTEXT(q) 16: q ← ARGINSTANTIATION(A,Dq, s̃, c) 17: else if q ∈ {AND, OR, NOT} then 18: C ← GETCHILDREN(q) 19: q ← DOOPERATION(C, q) 20: end if 21: end for 22: x← ROOT(t) 23: P ← GETARGVALUEPAIRS(x) 24: return P 25: end function\nSubsection with dependencies To describe our procedure at a high-level, we use the structure of the statutes to build out a computational graph, where nodes are either subsections with argumentvalue pairs, or logical operations. We resolve nodes one by one, depth first. We treat the singlesubsection model described above as a function, taking as input a set of argument-value pairs, a string representation of a subsection, and a string representation of a case, and returning a set of argument-value pairs. Algorithm 2 and Figure 3 summarize the following.\nWe start by building out the subsection’s dependency tree, as specified by the structure annotations (lines 2-4). First, we build the tree structure using BUILDDEPENDENCYTREE. Then, values for arguments are propagated from parent to child, from the\nroot down, with POPULATEARGVALUES. The tree is optionally capped to a predefined depth. Each node is either an input for the single-subsection function or its output, or a logical operation. We then traverse the tree depth first, performing the following operations, and replacing the node with the result of the operation:\n• If the node q is a leaf, resolve it using the single-subsection function ARGINSTANTIATION (lines 6-9 in Algorithm 2; step 1 in Figure 3).\n• If the node q is a subsection that is not a leaf, find its child node x (GETCHILD, line 12), and corresponding argument-value pairs other than @truth, Dx (GETARGVALUEPAIRS, line 13). Merge Dx with Dq, the argument-value pairs of the main node q (line 14). Finally, resolve the parent node q using the single-subsection function (lines 15-16; step 3 in Figure 3.\n• If node q is a logical operation (line 17), get its children C (GETCHILDREN, line 18), to which the operation will be applied with DOOPERATION (line 19) as follows:\n– If q == NOT, assign the negation of the child’s @truth value to q.\n– If q == OR, pick its child with the highest @truth value, and assign its arguments’ values to q.\n– If q == AND, transfer the argument-value pairs from all its children to q. In case of conflicting values, use the value associated with the lower @truth value. This operation can be seen in step 4 of Figure 3.\nThis procedure follows the formalism of neural module networks (Andreas et al., 2016) and is illustrated in Figure 3. Reentrancy into the dependency tree is not possible, so that a decision made earlier cannot be backtracked on at a later stage. One could imagine doing joint inference, or using heuristics for revisiting decisions, for example with a limited number of reentrancies. Humans are generally able to resolve this task in the order of the text, and we assume it should be possible for a computational model too. Our solution is meant to be computationally efficient, with the hope of not sacrificing too much performance. Revisiting this assumption is left for future work.\nMetrics and evaluation Arguments whose value needs to be predicted fall into three categories. The @truth argument calls for a binary truth value, and we score a model’s output using binary accuracy. The values of some arguments, such as gross income, are dollar amounts. We score such values using numerical accuracy, as 1 if ∆(y, ŷ) = |y−ŷ|max(0.1∗y,5000) < 1 else 0, where ŷ is the prediction and y the target. All other argument values are treated as strings. In those cases, we compute accuracy as exact match between predicted and gold value. Each of these three metrics defines a form of accuracy. We average the three metrics, weighted by the number of samples, to obtain a unified accuracy metric, used to compare the performance of models.\nTraining Based on the type of value expected, we use different loss functions. For @truth, we use binary cross-entropy. For numerical values, we use the hinge loss max(∆(y, ŷ) − 1, 0). For strings, let S be all the spans in the case description equal to the expected value. The loss function is log( ∑ i≤j e l·xi+r·xj ) − log( ∑ i,j∈S e l·xi+r·xj ) (Clark and Gardner, 2018). The model is trained end-to-end with gradient descent.\nWe start by training models on the silver data, as a pre-training step. We sweep the values of the learning rate in {10−2, 10−3, 10−4, 10−5} and the batch size in {64, 128, 256}. We try both BERTbase-cased and Legal BERT, allowing updates to the parameters of its top layer. We set aside 10% of the silver data as a dev set, and select the best model based on the unified accuracy on the dev set. Training is split up into three stages. The single-subsection model iteratively inserts values for arguments into the text of the subsection. In\nthe first stage, regardless of the predicted value, we insert the gold value for the argument, as in teacher forcing (Kolen and Kremer, 2001). In the second and third stages, we insert the value predicted by the model. When initializing the model from one stage to the next, we pick the model with the highest unified accuracy on the dev set. In the first two stages, we ignore the structure of the statutes, which effectively caps the depth of each dependency tree at 1.\nPicking the best model from this pre-training step, we perform fine-tuning on the gold data. We take a k-fold cross-validation approach (Stone, 1974). We randomly split the SARA v2 training set into 10 splits, taking care to put pairs of cases testing the same subsection into the same split. Each split contains nearly exactly the same proportion of binary and numerical cases. We sweep the values of the learning rate and batch size in the same ranges as above, and optionally allow updates to the parameters of BERT’s top layer. For a given set of hyperparameters, we run training on each split, using the dev set and the unified metric for early stopping. We use the performance on the dev set averaged across the 10 splits to evaluate the performance of a given set of hyperparameters. Using that criterion, we pick the best set of hyperparameters. We then pick the final model as that which achieves median performance on the dev set, across the 10 splits. We report the performance of that model on the test set.\nIn Table 5, we report the relevant argument instantiation metrics, under @truth, dollar amount and string. For comparison, we also report binary and numerical accuracy metrics defined in Holzenberger et al. (2020). The reported\nbaseline has three parameters. For @truth, it returns the most common value for that argument on the train set. For arguments that call for a dollar amount, it returns the one number that minimizes the dollar amount hinge loss on the training set. For all other arguments, it returns the most common string answer in the training set. Those parameters vary depending on whether the training set is augmented with the silver data."
    }, {
      "heading" : "4 Discussion",
      "text" : "Our goal in providing the baselines of Section 3 is to identify performance bottlenecks in the proposed sequence of tasks. Argument identification poses a moderate challenge, with a language model-based approach achieving non-trivial F1 score. The simple parser-based method is not a sufficient solution, but with its high recall could serve as the backbone to a statistical method. Argument coreference is a simpler task, with string matching perfectly resolving nearly 80% of the subsections. This is in line with the intuition that legal language is very explicit about disambiguating coreference. As reported in Table 3, usual coreference metrics seem lower, but only reflect a subset of the full task: coreference metrics are only concerned with links, so that arguments appearing exactly once bear no weight under that metric, unless they are wrongly linked to another argument.\nArgument instantiation is by far the most challenging task, as the model needs strong natural language understanding capabilities. Simple baselines can achieve accuracies above 50% for @truth, since for all numerical cases, @truth = True. We receive a slight boost in binary accuracy from using the proposed paradigm, departing from previous results on this benchmark. As compared to the baseline, the models mostly lag behind for the dollar\namount and numerical accuracies, which can be explained by the lack of a dedicated numerical solver, and sparse data. Further, we have made a number of simplifying assumptions, which may be keeping the model from taking advantage of the structure information: arguments are instantiated in order of appearance, forbidding joint prediction; revisiting past predictions is disallowed, forcing the model to commit to wrong decisions made earlier; the depth of the dependency tree is capped at 3; and finally, information is being passed along the dependency tree in the form of argument values, as opposed to dense, high-dimensional vector representations. The latter limits both the flow of information and the learning signal. This could also explain why the use of dependencies is detrimental in some cases. Future work would involve joint prediction (Chan et al., 2019), and more careful use of structure information.\nLooking at the errors made by the best model in Table 5 for binary accuracy, we note that for 39 positive and negative case pairs, it answers each pair identically, thus yielding 39 correct answers. In the remaining 11 pairs, there are 10 pairs where it gets both cases right. This suggests it may be guessing randomly on 39 pairs, and understanding 10. The best BERT-based model for dollar amounts predicts the same number for each case, as does the baseline. The best models for string arguments generally make predictions that match the category of the expected answer (date, person, etc) while failing to predict the correct string.\nPerformance gains from silver data are noticeable and generally consistent, as can be seen by comparing brown and blue cells in Table 5. The silver data came from running a human-written Prolog program, which is costly to produce. A possible substitute is to find mentions of applicable statutes in large corpora of legal cases (Caselaw, 2019), for\nexample using high-precision rules (Ratner et al., 2017), which has been successful for extracting information from cases (Boniol et al., 2020).\nIn this work, each task uses the gold annotations from upstream tasks. Ultimately, the goal is to pass the outputs of models from one task to the next."
    }, {
      "heading" : "5 Related Work",
      "text" : "Law-related NLP tasks have flourished in the past years, with applications including answering bar exam questions (Yoshioka et al., 2018; Zhong et al., 2020), information extraction (Chalkidis et al., 2019b; Boniol et al., 2020; Lam et al., 2020), managing contracts (Elwany et al., 2019; Liepiņa et al., 2020; Nyarko, 2021) and analyzing court decisions (Sim et al., 2015; Lee and Mouritsen, 2017). Case-based reasoning has been approached with expert systems (Popp and Schlink, 1974; Hellawell, 1980; v. d. L. Gardner, 1983), high-level handannotated features (Ashley and Brüninghaus, 2009) and transformer-based models (Rabelo et al., 2019). Closest to our work is Saeidi et al. (2018), where a dialog agent’s task is to answer a user’s question about a set of regulations. The task relies on a set of questions provided within the dataset.\nClark et al. (2019) as well as preceding work (Friedland et al., 2004; Gunning et al., 2010) tackle a similar problem in the science domain, with the goal of using the prescriptive knowledge from science textbooks to answer exam questions. The core of their model relies on several NLP and specialized reasoning techniques, with contextualized language models playing a major role. Clark et al. (2019) take the route of sorting questions into different types, and working on specialized solvers. In contrast, our approach is to treat each question identically, but to decompose the process of answering into a sequence of subtasks.\nThe language of statutes is related to procedural language, which describes steps in a process. Zhang et al. (2012) collect how-to instructions in a variety of domains, while Wambsganss and Fromm (2019) focus on automotive repair instructions. Branavan et al. (2012) exploit instructions in a game manual to improve an agent’s performance. Dalvi et al. (2019) and Amini et al. (2020) turn to modeling textual descriptions of physical and biological mechanisms. Weller et al. (2020) propose models that generalize to new task descriptions.\nThe tasks proposed in this work are germane to standard NLP tasks, such as named entity recog-\nnition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al., 2014). Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al. (2017) attempt for a subsection of tax law.\nArgument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013). We frame argument instantiation as iteratively completing a statement in natural language. Chen et al. (2020) refine generic statements by copying strings from input text, with the goal of detecting events. Chan et al. (2019) extend transformer-based language models to permit inserting tokens anywhere in a sequence, thus allowing to modify an existing sequence. For argument instantiation, we make use of neural module networks (Andreas et al., 2016), which are used in the visual (Yi et al., 2018) and textual domains (Gupta et al., 2020). In that context, arguments and their values can be thought of as the hints from Khot et al. (2020). The Prolog-based data augmentation is related to data augmentation for semantic parsing (Campagna et al., 2019; Weir et al., 2019)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Solutions to tackle statutory reasoning may range from high-structure, high-human involvement expert systems, to less structured, largely selfsupervised language models. Here, taking inspiration from Prolog programs, we introduce a novel paradigm, by breaking statutory reasoning down into a sequence of tasks. Each task can be annotated for with far less expertise than would be required to translate legal language into code, and comes with its own performance metrics. Our contribution enables finer-grained scoring and debugging of models for statutory reasoning, which facilitates incremental progress and identification of performance bottlenecks. In addition, argument instantiation and explicit resolution of dependencies introduce further interpretability. This novel approach could possibly inform the design of models that reason with rules specified in natural language, for the domain of legal NLP and beyond."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Andrew Blair-Stanek for helpful comments, and Ryan Culkin for help with the parser-based argument identification baseline."
    }, {
      "heading" : "A Task examples",
      "text" : "In the following, we provide several examples for each of the tasks defined in Section 2."
    }, {
      "heading" : "A.1 Argument identification",
      "text" : "For ease of reading, the spans mentioned in the output are underlined in the input.\nInput 1 (§3306(a)(1)(B))\n(B) on each of some 10 days during the calendar year or during the preceding calendar year, each day being in a different calendar week, employed at least one individual in employment for some portion of the day.\nOutput 1\n{(15, 26), (35, 51), (62, 88), (92, 99), (122, 134), (155, 168), (173, 182), (188, 210)}\nInput 2 (§63(c)(5))\nIn the case of an individual with respect to whom a deduction under section 151 is allowable to another taxpayer for a taxable year beginning in the calendar year in which the individual’s taxable year begins, the basic standard deduction applicable to such individual for such individual’s taxable year shall not exceed the greater of-\nOutput 2\n{(15, 27), (50, 60), (96, 111), (117, 130), (145, 161), (172, 185), (189, 200), (210, 237), (253, 267), (273, 287), (291, 302), (321, 331)}\nInput 3 (§1(d)(iv))\n(iv) $31,172, plus 36% of the excess over $115,000 if the taxable income is over $115,000 but not over $250,000;\nOutput 3\n{(5, 45), (50, 67)}"
    }, {
      "heading" : "A.2 Argument coreference",
      "text" : "We report the full matrix C. In addition, for ease of reading, coreference clusters are marked with superscripts in the input.\nInput 1 (§3306(a)(1)(B))\n(B) on each of some 10 days1 during the calendar year2 or during the preceding calendar year3, each day1 being in a different calendar week4, employed at least one individual5 in employment6 for some portion of the day7.\n{(15, 26), (35, 51), (62, 88), (92, 99), (122, 134), (155, 168), (173, 182), (188, 210)} Output 1\n 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1  Input 2 (§63(c)(5))\nIn the case of an individual1 with respect to whom a deduction2 under section 151 is allowable to another taxpayer3 for a taxable year4 beginning in the calendar year5 in which the individual1’s taxable year6 begins, the basic standard deduction7 applicable to such individual1 for such individual1’s taxable year6 shall not exceed the greater8 of{(15, 27), (50, 60), (96, 111), (117, 130), (145, 161), (172, 185), (189, 200), (210, 237), (253, 267), (273, 287), (291, 302), (321, 331)} Output 2\n 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1  Input 3 (§1(d)(iv))\n(iv) $31,172, plus 36% of the excess over $115,0001 if the taxable income2 is over $115,000 but not over $250,000; {(5, 45), (50, 67)}\nOutput 3\n( 1 0 0 1 )"
    }, {
      "heading" : "A.3 Structure extraction",
      "text" : "To clarify the link between the input and the output, we are adding superscripts to argument names in the output. While the output is represented as plain text, a graph-based representation would likely be used in a practical system, to facilitate learning and inference. Arguments are keyword based. For example, in Output 2, the value of the Taxp argument of §63(c)(5) is passed to the Spouse argument of §151(b). If no equal sign is specified, it means the argument names match. For example, part of Output 2 could have been rewritten more explicitly as §151(b)(Spouse=Taxp, Taxp=S45, Taxy=Taxy).\nInput 1 (§3306(a)(1)(B))\n(B) on each of some 10 days1 during the calendar year2 or during the preceding calendar year3, each day1 being in a different calendar week4, employed at least one individual5 in employment6 for some portion of the day7. {(15, 26), (35, 51), (62, 88), (92, 99), (122, 134), (155, 168), (173, 182), (188, 210)} 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1  Output 1\n§3306(a)(1)(B)(Caly2, S167, Workday1, Employment6, Preccaly3, Employee5, S13A4, Employer, Service) :-\n§3306(c)(Employee, Employer, Service).\nInput 2 (§63(c)(5))\nIn the case of an individual1 with respect to whom a deduction2 under section 151 is allowable to another taxpayer3 for a taxable year4 beginning in the calendar year5 in which the individual1’s taxable year6 begins, the basic standard deduction7 applicable to such individual1 for such individual1’s taxable year6 shall not exceed the greater8 of-\n{(15, 27), (50, 60), (96, 111), (117, 130), (145, 161), (172, 185), (189, 200), (210, 237), (253, 267), (273, 287), (291, 302), (321, 331)}\n 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1  Output 2\n§63(c)(5)(Bassd7, Grossinc, S453, Taxp1, Taxy6, S44B2, S46B4, S475, S488) :-\n[ §151(b)(Spouse=Taxp, Taxp=S45, Taxy) OR §151(c)(S24A=Taxp, Taxp=S45, Taxy) ] AND §63(c)(5)(A)() AND §63(c)(5)(B)(Grossinc, Taxp).\nInput 3 (§1(d)(iv))\n(iv) $31,172, plus 36% of the excess over $115,0001 if the taxable income2 is over $115,000 but not over $250,000; {(5, 45), (50, 67)} (\n1 0 0 1 ) Output 3\n§1(d)(iv)(Tax1, Taxinc2)."
    }, {
      "heading" : "A.4 Argument instantiation",
      "text" : "The following are example cases. In addition to the case description, subsection to apply and input argument-value pairs, the agent has access to the output of Argument identification, Argument coreference and Structure extraction, for the entirety of the statutes.\nInput 1: case 3306(a)(1)(B)-positive\nCase description: Alice has employed Bob on various occasions during the year 2017: Jan 24, Feb 4, Mar 3, Mar 19, Apr 2, May 9, Oct 15, Oct 25, Nov 8, Nov 22, Dec 1, Dec 3. Subsection to apply: §3306(a)(1)(B)\nArgument-value pairs: {Employer=“Alice”, Caly=“2017”}\nOutput 1\n{Workday=[“Jan 24”, “Feb 4”, “Mar 3”, “Mar 19”, “Apr 2”, “May 9”, “Oct 15”, “Oct 25”, “Nov 8”, “Nov 22”, “Dec 1”, “Dec 3”], Employee=“Bob”, Employment=“has employed”, “S13A”: [4, 5, 9, 11, 13, 19, 41, 43, 45, 47], @truth=True}\nInput 2: case §63(c)(5)-negative\nCase description: In 2017, Alice was paid $33200. Alice and Bob have been married since Feb 3rd, 2017. Bob earned $10 in 2017. Alice and Bob file separate returns. Alice is not entitled to a deduction for Bob under section 151.\nSubsection to apply: §63(c)(5)\nArgument-value pairs: {Taxp=“Bob”, Taxy=“2017”, Bassd=500}\nOutput 2\n{@truth=False}\nInput 3: tax case 5\nCase description: In 2017, Alice’s gross income was $326332. Alice and Bob have been married since Feb 3rd, 2017, and have had the same principal place of abode since 2015. Alice was born March 2nd, 1950 and Bob was born March 3rd, 1955. Alice and Bob file separately in 2017. Bob has no gross income that year. Alice takes the standard deduction.\nSubsection to apply: Tax\nArgument-value pairs: {Taxy=“2017”, Taxp=“Alice”}\nOutput 3\n{Tax=116066, @truth=True}"
    }, {
      "heading" : "B Dataset statistics",
      "text" : ""
    }, {
      "heading" : "B.1 Argument identification",
      "text" : "Table 6 reports statistics on the annotations for the argument identification task. The numbers in that table were used to plot the top histogram in Figure 2(a)."
    }, {
      "heading" : "B.2 Argument coreference",
      "text" : "In Tables 7 and 8, we report statistics on the annotations for the argument coreference task. The numbers in Table 7 (resp. 8) were used to plot the middle (resp. bottom) histogram in Figure 2(a)."
    }, {
      "heading" : "B.3 Structure identification",
      "text" : "Table 9 reports statistics on the annotations for the structure extraction task. These numbers for arguments differ from those in Table 6, because any subsection is allowed to contain the arguments of any subsections it refers to."
    }, {
      "heading" : "B.4 Argument instantiation",
      "text" : "Tables 10 and 11 show statistics for the annotations for the argument instantiation task. In the gold data, we separate training and test data, to show that both distributions are close."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26,",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Procedural reading comprehension with attribute-aware context flow",
      "author" : [ "Aida Amini", "Antoine Bosselut", "Bhavana Dalvi Mishra", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June",
      "citeRegEx" : "Amini et al\\.,? 2020",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural module networks",
      "author" : [ "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 39–48. IEEE Computer Society.",
      "citeRegEx" : "Andreas et al\\.,? 2016",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatically classifying case texts and predicting outcomes",
      "author" : [ "Kevin D. Ashley", "Stefanie Brüninghaus." ],
      "venue" : "Artif. Intell. Law, 17(2):125–165.",
      "citeRegEx" : "Ashley and Brüninghaus.,? 2009",
      "shortCiteRegEx" : "Ashley and Brüninghaus.",
      "year" : 2009
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 Octo-",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Performance in the courtroom: Automated processing and visualization of appeal court decisions in france",
      "author" : [ "Paul Boniol", "George Panagopoulos", "Christos Xypolopoulos", "Rajaa El Hamdani", "David Restrepo Amariles", "Michalis Vazirgiannis" ],
      "venue" : null,
      "citeRegEx" : "Boniol et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Boniol et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning high-level planning from text",
      "author" : [ "S.R.K. Branavan", "Nate Kushman", "Tao Lei", "Regina Barzilay." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 126–135, Jeju Island,",
      "citeRegEx" : "Branavan et al\\.,? 2012",
      "shortCiteRegEx" : "Branavan et al\\.",
      "year" : 2012
    }, {
      "title" : "Genie: a generator of natural language semantic parsers for virtual assistant commands",
      "author" : [ "Giovanni Campagna", "Silei Xu", "Mehrad Moradshahi", "Richard Socher", "Monica S. Lam." ],
      "venue" : "Proceedings of the 40th ACM SIGPLAN Conference on Programming",
      "citeRegEx" : "Campagna et al\\.,? 2019",
      "shortCiteRegEx" : "Campagna et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural legal judgment prediction in English",
      "author" : [ "Ilias Chalkidis", "Ion Androutsopoulos", "Nikolaos Aletras." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317–4323, Florence, Italy. Association",
      "citeRegEx" : "Chalkidis et al\\.,? 2019a",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "Largescale multi-label text classification on EU legislation",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,",
      "citeRegEx" : "Chalkidis et al\\.,? 2019b",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "KERMIT: generative insertion-based modeling for sequences",
      "author" : [ "William Chan", "Nikita Kitaev", "Kelvin Guu", "Mitchell Stern", "Jakob Uszkoreit." ],
      "venue" : "CoRR, abs/1906.01604.",
      "citeRegEx" : "Chan et al\\.,? 2019",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2019
    }, {
      "title" : "Reading the manual: Event extraction as definition comprehension",
      "author" : [ "Yunmo Chen", "Tongfei Chen", "Seth Ebner", "Aaron Steven White", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Fourth Workshop on Structured Prediction for NLP@EMNLP 2020, On-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Vol-",
      "citeRegEx" : "Clark and Gardner.,? 2018",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2018
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment,",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "Everything happens for a reason: Discovering the purpose of actions in procedural text",
      "author" : [ "Bhavana Dalvi", "Niket Tandon", "Antoine Bosselut", "Wentau Yih", "Peter Clark." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Dalvi et al\\.,? 2019",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2019
    }, {
      "title" : "2019. BERT: pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Neural module networks",
      "author" : [ "Matt Gardner" ],
      "venue" : null,
      "citeRegEx" : "Gardner.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gardner.",
      "year" : 2020
    }, {
      "title" : "A dataset for statutory",
      "author" : [ "jamin Van Durme" ],
      "venue" : null,
      "citeRegEx" : "Durme.,? \\Q2020\\E",
      "shortCiteRegEx" : "Durme.",
      "year" : 2020
    }, {
      "title" : "On sentences which are true",
      "author" : [ "WS.org. Alfred Horn" ],
      "venue" : null,
      "citeRegEx" : "Horn.,? \\Q1951\\E",
      "shortCiteRegEx" : "Horn.",
      "year" : 1951
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A field guide to dynamical recurrent networks",
      "author" : [ "John F Kolen", "Stefan C Kremer." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Kolen and Kremer.,? 2001",
      "shortCiteRegEx" : "Kolen and Kremer.",
      "year" : 2001
    }, {
      "title" : "The design of a legal analysis program",
      "author" : [ "Anne v. d. L. Gardner." ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence, Washington, D.C., USA, August 22-26, 1983, pages 114–118. AAAI Press.",
      "citeRegEx" : "Gardner.,? 1983",
      "shortCiteRegEx" : "Gardner.",
      "year" : 1983
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "The gap between deep learning and law: Predicting employment notice",
      "author" : [ "Jason T. Lam", "David Liang", "Samuel Dahan", "Farhana H. Zulkernine." ],
      "venue" : "Proceedings of the Natural Legal Language Processing Workshop 2020 co-located with the 26th",
      "citeRegEx" : "Lam et al\\.,? 2020",
      "shortCiteRegEx" : "Lam et al\\.",
      "year" : 2020
    }, {
      "title" : "Judging ordinary meaning",
      "author" : [ "Thomas R Lee", "Stephen C Mouritsen." ],
      "venue" : "Yale LJ, 127:788.",
      "citeRegEx" : "Lee and Mouritsen.,? 2017",
      "shortCiteRegEx" : "Lee and Mouritsen.",
      "year" : 2017
    }, {
      "title" : "Explaining potentially unfair clauses to the consumer with the CLAUDETTE tool",
      "author" : [ "Rūta Liepiņa", "Federico Ruggeri", "Francesca Lagioia", "Marco Lippi", "Kasper Drazewski", "Paolo Torroni." ],
      "venue" : "Proceedings of the Natural Legal Language Pro-",
      "citeRegEx" : "Liepiņa et al\\.,? 2020",
      "shortCiteRegEx" : "Liepiņa et al\\.",
      "year" : 2020
    }, {
      "title" : "Toward automated international law compliance monitoring (tailcm)",
      "author" : [ "Leora Morgenstern." ],
      "venue" : "Technical report, LEIDOS HOLDINGS INC RESTON VA.",
      "citeRegEx" : "Morgenstern.,? 2014",
      "shortCiteRegEx" : "Morgenstern.",
      "year" : 2014
    }, {
      "title" : "Stickiness and incomplete contracts",
      "author" : [ "Julian Nyarko." ],
      "venue" : "The University of Chicago Law Review, 88.",
      "citeRegEx" : "Nyarko.,? 2021",
      "shortCiteRegEx" : "Nyarko.",
      "year" : 2021
    }, {
      "title" : "Towards formalizing statute law as default logic through automatic semantic parsing",
      "author" : [ "Marcos A. Pertierra", "Sarah Lawsky", "Erik Hemberg", "Una-May O’Reilly" ],
      "venue" : "In Proceedings of the Second Workshop on Automated Semantic Analysis of Informa-",
      "citeRegEx" : "Pertierra et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Pertierra et al\\.",
      "year" : 2017
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Slav Petrov", "Dipanjan Das", "Ryan T. McDonald." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012, pages 2089–2096. Eu-",
      "citeRegEx" : "Petrov et al\\.,? 2012",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "Judith, a computer program to advise lawyers in reasoning a case",
      "author" : [ "Walter G Popp", "Bernhard Schlink." ],
      "venue" : "Jurimetrics J., 15:303.",
      "citeRegEx" : "Popp and Schlink.,? 1974",
      "shortCiteRegEx" : "Popp and Schlink.",
      "year" : 1974
    }, {
      "title" : "Scoring coreference partitions of predicted mentions: A reference implementation",
      "author" : [ "Sameer Pradhan", "Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association",
      "citeRegEx" : "Pradhan et al\\.,? 2014",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2014
    }, {
      "title" : "Combining similarity and transformer methods for case law entailment",
      "author" : [ "Juliano Rabelo", "Mi-Young Kim", "Randy Goebel." ],
      "venue" : "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law, ICAIL 2019, Montreal, QC,",
      "citeRegEx" : "Rabelo et al\\.,? 2019",
      "shortCiteRegEx" : "Rabelo et al\\.",
      "year" : 2019
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev-Arie Ratinov", "Dan Roth." ],
      "venue" : "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL 2009, Boulder, Colorado, USA, June 4-5,",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Snorkel: Rapid training data creation with weak supervision",
      "author" : [ "Alexander Ratner", "Stephen H. Bach", "Henry R. Ehrenberg", "Jason Alan Fries", "Sen Wu", "Christopher Ré." ],
      "venue" : "Proc. VLDB Endow., 11(3):269– 282.",
      "citeRegEx" : "Ratner et al\\.,? 2017",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2017
    }, {
      "title" : "Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task",
      "author" : [ "Michael Roth", "Anette Frank." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics, *SEM 2012, June 7-8, 2012,",
      "citeRegEx" : "Roth and Frank.,? 2012",
      "shortCiteRegEx" : "Roth and Frank.",
      "year" : 2012
    }, {
      "title" : "Interpretation of natural language rules in conversational machine reading",
      "author" : [ "Marzieh Saeidi", "Max Bartolo", "Patrick S.H. Lewis", "Sameer Singh", "Tim Rocktäschel", "Mike Sheldon", "Guillaume Bouchard", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2018",
      "citeRegEx" : "Saeidi et al\\.,? 2018",
      "shortCiteRegEx" : "Saeidi et al\\.",
      "year" : 2018
    }, {
      "title" : "The utility of text: The case of amicus briefs and the supreme court",
      "author" : [ "Yanchuan Sim", "Bryan R. Routledge", "Noah A. Smith." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Sim et al\\.,? 2015",
      "shortCiteRegEx" : "Sim et al\\.",
      "year" : 2015
    }, {
      "title" : "Parsing with compositional vector grammars",
      "author" : [ "Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia,",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Cross-validatory choice and assessment of statistical predictions",
      "author" : [ "Mervyn Stone." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodological), 36(2):111–133.",
      "citeRegEx" : "Stone.,? 1974",
      "shortCiteRegEx" : "Stone.",
      "year" : 1974
    }, {
      "title" : "Litigation analytics: Case outcomes extracted from US federal court dockets",
      "author" : [ "Thomas Vacek", "Ronald Teo", "Dezhao Song", "Timothy Nugent", "Conner Cowling", "Frank Schilder." ],
      "venue" : "Proceedings of the Natural Legal Language Processing Workshop 2019,",
      "citeRegEx" : "Vacek et al\\.,? 2019",
      "shortCiteRegEx" : "Vacek et al\\.",
      "year" : 2019
    }, {
      "title" : "Mining user-generated repair instructions from automotive web communities",
      "author" : [ "Thiemo Wambsganss", "Hansjörg Fromm." ],
      "venue" : "52nd Hawaii International Conference on System Sciences, HICSS 2019, Grand Wailea, Maui, Hawaii, USA, January 8-11,",
      "citeRegEx" : "Wambsganss and Fromm.,? 2019",
      "shortCiteRegEx" : "Wambsganss and Fromm.",
      "year" : 2019
    }, {
      "title" : "Dbpal: Weak supervision for learning",
      "author" : [ "Nathaniel Weir", "Andrew Crotty", "Alex Galakatos", "Amir Ilkhechi", "Shekar Ramaswamy", "Rohin Bhushan", "Ugur Çetintemel", "Prasetya Utama", "Nadja Geisler", "Benjamin Hättasch", "Steffen Eger", "Carsten Binnig" ],
      "venue" : null,
      "citeRegEx" : "Weir et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Weir et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from task descriptions",
      "author" : [ "Orion Weller", "Nicholas Lourie", "Matt Gardner", "Matthew E. Peters." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020,",
      "citeRegEx" : "Weller et al\\.,? 2020",
      "shortCiteRegEx" : "Weller et al\\.",
      "year" : 2020
    }, {
      "title" : "PARMA: A predicate argument aligner",
      "author" : [ "Travis Wolfe", "Benjamin Van Durme", "Mark Dredze", "Nicholas Andrews", "Charley Beller", "Chris CallisonBurch", "Jay DeYoung", "Justin Snyder", "Jonathan Weese", "Tan Xu", "Xuchen Yao." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Wolfe et al\\.,? 2013",
      "shortCiteRegEx" : "Wolfe et al\\.",
      "year" : 2013
    }, {
      "title" : "Break it down: A question understanding benchmark",
      "author" : [ "Tomer Wolfson", "Mor Geva", "Ankit Gupta", "Matt Gardner", "Yoav Goldberg", "Daniel Deutch", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:183–198.",
      "citeRegEx" : "Wolfson et al\\.,? 2020",
      "shortCiteRegEx" : "Wolfson et al\\.",
      "year" : 2020
    }, {
      "title" : "CAIL2018: A large-scale legal dataset for judgment prediction",
      "author" : [ "Chaojun Xiao", "Haoxi Zhong", "Zhipeng Guo", "Cunchao Tu", "Zhiyuan Liu", "Maosong Sun", "Yansong Feng", "Xianpei Han", "Zhen Hu", "Heng Wang", "Jianfeng Xu." ],
      "venue" : "CoRR, abs/1807.02478.",
      "citeRegEx" : "Xiao et al\\.,? 2018",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural-symbolic VQA: disentangling reasoning from vision and language understanding",
      "author" : [ "Kexin Yi", "Jiajun Wu", "Chuang Gan", "Antonio Torralba", "Pushmeet Kohli", "Josh Tenenbaum." ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Yi et al\\.,? 2018",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of japanese statute law retrieval and entailment task at coliee-2018",
      "author" : [ "Masaharu Yoshioka", "Yoshinobu Kano", "Naoki Kiyota", "Ken Satoh." ],
      "venue" : "Twelfth international workshop on Juris-informatics (JURISIN 2018).",
      "citeRegEx" : "Yoshioka et al\\.,? 2018",
      "shortCiteRegEx" : "Yoshioka et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatically extracting procedural knowledge from instructional texts using natural language processing",
      "author" : [ "Ziqi Zhang", "Philip Webster", "Victoria S. Uren", "Andrea Varga", "Fabio Ciravegna." ],
      "venue" : "Proceedings of the Eighth International Conference on Lan-",
      "citeRegEx" : "Zhang et al\\.,? 2012",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "JECQA: A legal-domain question answering dataset",
      "author" : [ "Haoxi Zhong", "Chaojun Xiao", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "Bob has no gross income that year. Alice takes the standard deduction. Subsection to apply: Tax Argument-value pairs: {Taxy=“2017",
      "author" : [ "Alice", "Bob" ],
      "venue" : null,
      "citeRegEx" : "1955",
      "shortCiteRegEx" : "1955",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 46,
      "context" : "As more data becomes available, Natural Language Processing (NLP) techniques are increasingly being applied to the legal domain, including for the prediction of case outcomes (Xiao et al., 2018; Vacek et al., 2019; Chalkidis et al., 2019a).",
      "startOffset" : 175,
      "endOffset" : 239
    }, {
      "referenceID" : 40,
      "context" : "As more data becomes available, Natural Language Processing (NLP) techniques are increasingly being applied to the legal domain, including for the prediction of case outcomes (Xiao et al., 2018; Vacek et al., 2019; Chalkidis et al., 2019a).",
      "startOffset" : 175,
      "endOffset" : 239
    }, {
      "referenceID" : 8,
      "context" : "As more data becomes available, Natural Language Processing (NLP) techniques are increasingly being applied to the legal domain, including for the prediction of case outcomes (Xiao et al., 2018; Vacek et al., 2019; Chalkidis et al., 2019a).",
      "startOffset" : 175,
      "endOffset" : 239
    }, {
      "referenceID" : 13,
      "context" : "was cast as Recognizing Textual Entailment (Dagan et al., 2005) and linear regression.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "We borrow from the terminology of predicate argument alignment (Roth and Frank, 2012; Wolfe et al., 2013) and call these placeholders arguments.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 44,
      "context" : "We borrow from the terminology of predicate argument alignment (Roth and Frank, 2012; Wolfe et al., 2013) and call these placeholders arguments.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 45,
      "context" : "ing down complex questions into atomic questions, with the possibility of referring to previous answers (Wolfson et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "We provide structure extraction annotations for SARA in the style of Horn clauses (Horn, 1951), using common logical operators, as shown in the bottom left of Figure 1.",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 38,
      "context" : "We run the Stanford parser (Socher et al., 2013) on the statutes, and extract all noun phrases as spans –",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "While de-formatting legal text can boost parser performance (Morgenstern, 2014), we found it made little difference in our case.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "We embed each paragraph using BERT, classify each contextual subword embedding into a 3-dimensional logit with a linear layer, and run a CRF (Lafferty et al., 2001).",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 19,
      "context" : "We use a batch size of 32 paragraphs, a learning rate of 10−3 and the Adam optimizer (Kingma and Ba, 2015).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "Argument coreference differs from the usual coreference task (Pradhan et al., 2014), even though we are using similar terminology, and frame it in a similar way.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "This procedure follows the formalism of neural module networks (Andreas et al., 2016) and is illustrated in Figure 3.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "The loss function is log( ∑ i≤j e l·xi+r·xj ) − log( ∑ i,j∈S e l·xi+r·xj ) (Clark and Gardner, 2018).",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "In the first stage, regardless of the predicted value, we insert the gold value for the argument, as in teacher forcing (Kolen and Kremer, 2001).",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 39,
      "context" : "We take a k-fold cross-validation approach (Stone, 1974).",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "Future work would involve joint prediction (Chan et al., 2019), and more careful use of structure information.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 34,
      "context" : "2750 example using high-precision rules (Ratner et al., 2017), which has been successful for extracting information from cases (Boniol et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : ", 2017), which has been successful for extracting information from cases (Boniol et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 48,
      "context" : "Law-related NLP tasks have flourished in the past years, with applications including answering bar exam questions (Yoshioka et al., 2018; Zhong et al., 2020), information extraction (Chalkidis et al.",
      "startOffset" : 114,
      "endOffset" : 157
    }, {
      "referenceID" : 50,
      "context" : "Law-related NLP tasks have flourished in the past years, with applications including answering bar exam questions (Yoshioka et al., 2018; Zhong et al., 2020), information extraction (Chalkidis et al.",
      "startOffset" : 114,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : ", 2020), information extraction (Chalkidis et al., 2019b; Boniol et al., 2020; Lam et al., 2020), managing contracts (Elwany et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : ", 2020), information extraction (Chalkidis et al., 2019b; Boniol et al., 2020; Lam et al., 2020), managing contracts (Elwany et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : ", 2020), information extraction (Chalkidis et al., 2019b; Boniol et al., 2020; Lam et al., 2020), managing contracts (Elwany et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", 2020), managing contracts (Elwany et al., 2019; Liepiņa et al., 2020; Nyarko, 2021) and analyzing court decisions (Sim et al.",
      "startOffset" : 28,
      "endOffset" : 85
    }, {
      "referenceID" : 27,
      "context" : ", 2020), managing contracts (Elwany et al., 2019; Liepiņa et al., 2020; Nyarko, 2021) and analyzing court decisions (Sim et al.",
      "startOffset" : 28,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : ", 2020; Nyarko, 2021) and analyzing court decisions (Sim et al., 2015; Lee and Mouritsen, 2017).",
      "startOffset" : 52,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : ", 2020; Nyarko, 2021) and analyzing court decisions (Sim et al., 2015; Lee and Mouritsen, 2017).",
      "startOffset" : 52,
      "endOffset" : 95
    }, {
      "referenceID" : 30,
      "context" : "expert systems (Popp and Schlink, 1974; Hellawell, 1980; v. d. L. Gardner, 1983), high-level handannotated features (Ashley and Brüninghaus, 2009) and transformer-based models (Rabelo et al.",
      "startOffset" : 15,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "Gardner, 1983), high-level handannotated features (Ashley and Brüninghaus, 2009) and transformer-based models (Rabelo et al.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : "Gardner, 1983), high-level handannotated features (Ashley and Brüninghaus, 2009) and transformer-based models (Rabelo et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 33,
      "context" : "The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al.",
      "startOffset" : 149,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "The tasks proposed in this work are germane to standard NLP tasks, such as named entity recognition (Ratinov and Roth, 2009), part-of-speech tagging (Petrov et al., 2012; Akbik et al., 2018), and coreference resolution (Pradhan et al.",
      "startOffset" : 149,
      "endOffset" : 190
    }, {
      "referenceID" : 31,
      "context" : ", 2018), and coreference resolution (Pradhan et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 38,
      "context" : "Structure extraction is conceptually similar to syntactic (Socher et al., 2013) and semantic parsing (Berant et al.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : ", 2013) and semantic parsing (Berant et al., 2013), which Pertierra et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 44,
      "context" : "Argument instantiation is closest to the task of aligning predicate argument structures (Roth and Frank, 2012; Wolfe et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "argument instantiation, we make use of neural module networks (Andreas et al., 2016), which are used in the visual (Yi et al.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 47,
      "context" : ", 2016), which are used in the visual (Yi et al., 2018) and textual domains (Gupta et al.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "The Prolog-based data augmentation is related to data augmentation for semantic parsing (Campagna et al., 2019; Weir et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 42,
      "context" : "The Prolog-based data augmentation is related to data augmentation for semantic parsing (Campagna et al., 2019; Weir et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 130
    } ],
    "year" : 2021,
    "abstractText" : "Statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case. Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance. To address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in Prolog programs. Augmenting an existing benchmark, we provide annotations for the four tasks, and baselines for three of them. Models for statutory reasoning are shown to benefit from the additional structure, improving on prior baselines. Further, the decomposition into subtasks facilitates finer-grained model diagnostics and clearer incremental progress.",
    "creator" : "LaTeX with hyperref"
  }
}