{
  "name" : "2021.acl-long.135.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Sequence-to-Sequence Approach to Dialogue State Tracking",
    "authors" : [ "Yue Feng", "Yang Wang", "Hang Li" ],
    "emails" : [ "yue.feng.20@ucl.ac.uk", "lihang.lh}@bytedance.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1714–1725\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1714"
    }, {
      "heading" : "1 Introduction",
      "text" : "A task-oriented dialogue system usually consists of several modules: natural language understanding (NLU), dialogue state tracking (DST), dialogue policy (Policy), and natural language generation (NLG). We consider DST and also NLU in this paper. In NLU, a semantic frame representing the content of user utterance is created in each turn\n∗The work was done when the first author was an intern at ByteDance AI Lab.\nof dialogue. In DST, several semantic frames representing the ‘states’ of dialogue are created and updated in multiple turns of dialogue. Domain knowledge in dialogues is represented by a representation referred to as schema, which consists of possible intents, slots, and slot values. Slot values can be in a pre-defined set, with the corresponding slot being referred to as categorical slot, and they can also be from an open set, with the corresponding slot being referred to as non-categorical slot. Figure 1 shows an example of DST.\nWe think that a DST module (and an NLU module) should have the following abilities. (1) Global, the model can jointly represent intents, slots, and slot values. (2) Represenable, it has strong capa-\nbility to represent knowledge for the task, on top of a pre-trained language model like BERT. (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas.\nMany methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkšić et al., 2017; Goo et al., 2018). There are two lines of relevant research. (1) To enhance the scalability of DST, a problem formulation, referred to as schemaguided dialogue, is proposed. In the setting, it is assumed that descriptions on schemas in natural language across multiple domains are given and utilized. Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently infer the intent and slot value pairs for the current turn. Therefore, the proposed models are generally representable and scalable, but not global. (2) There are also a few methods which view DST as a sequence to sequence problem. Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b). Recently, a new approach is proposed which formalizes the tasks in dialogue as sequence prediction problems using a unified language model (based on GPT-2) (Hosseini-Asl et al., 2020). The method cannot deal with unseen schemas and intents, however, and thus is not scalable.\nWe propose a novel approach to DST, referred to as Seq2Seq-DU (sequence-to-sequence for dialogue understanding), which combines the advantages of the existing approaches. To the best of our knowledge, there was no previous work which studied the approach. We think that DST should be formalized as a sequence to sequence or ‘translation’ problem in which the utterances in the dialogue are transformed into semantic frames. In this way, the intents, slots, and slot values can be jointly modeled. Moreover, NLU can also be viewed as a special case of DST and thus Seq2Seq-DU can also be applied to NLU. We note that very recently the effectiveness of the sequence to sequence approach has also been verified in other language understanding tasks (Paolini et al., 2021).\nSeq2Seq-DU comprises a BERT-based encoder\nto encode the utterances in the dialogue, a BERT based encoder to encode the schema descriptions, an attender to calculate attentions between the utterance embeddings and schema embeddings, and a decoder to generate pointers of items representing the intents and slots-value pairs of state.\nSeq2Seq-DU has the following advantages. (1) Global: it relies on the sequence to sequence framework to simultaneously model the intents, slots, and slot-values. (2) Representable: It employs BERT (Devlin et al., 2019) to learn and utilize better representations of not only the current utterance but also the previous utterances in the dialogue. If schema descriptions are available, it also employs BERT for the learning and utilization of their representations. (3) Scalable: It uses the pointer generation mechanism, as in the Pointer Network (Vinyals et al., 2015), to create representations of intents, slots, and slot-values, no matter whether the slots are categorical or non-categorical, and whether the schemas are unseen or not.\nExperimental results on benchmark datasets show that Seq2Seq-DU1 performs much better than the baselines on SGD, MultiWOZ2.2, and MultiWOZ2.1 in multi-turn dialogue with schema descriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue without schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT)."
    }, {
      "heading" : "2 Related Work",
      "text" : "There has been a large amount of work on task-oriented dialogue, especially dialogue state tracking and natural language understanding (eg., (Zhang et al., 2020; Huang et al., 2020; Chen et al., 2017)). Table 1 makes a summary of existing methods on DST. We also indicate the methods on which we make comparison in our experiments."
    }, {
      "heading" : "2.1 Dialogue State Tracking",
      "text" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020). The approaches\n1The code is available at https://github.com/ sweetalyssum/Seq2Seq-DU.\ncannot deal with unseen schemas in new domains, however. To cope with the problem, a new direction called schema-guided dialogue is proposed recently, which assumes that natural language descriptions of schemas are provided and can be used to help transfer knowledge across domains. As such, a number of methods are developed in the recent dialogue competition SGD (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020; Chen et al., 2020a). Our work is partially motivated by the SGD initiative. Our model Seq2Seq-DU is unique in that it formalizes schema-guided DST as a sequence-to-sequence problem using BERT and pointer generation.\nIn fact, sequence-to-sequence models are also utilized in DST. Sequicity (Lei et al., 2018) is a two-step sequence to sequence model which first encodes the dialogue history and generates a belief span, and then generates a language response from the belief span. COMER (Ren et al., 2019) and CREDIT (Chen et al., 2020b) are hierarchical sequence-to-sequence models which represent the intents and slot-value pairs in a hierarchical way, and employ a multi-stage decoder. SimpleTOD (Hosseini-Asl et al., 2020) is a unified approach to task-oriented dialogue which employs\na single and causal language model to perform sequence prediction in DST, Policy, and NLG. Our proposed approach also uses a sequence-tosequence model. There are significant differences between our model Seq2Seq-DU and the existing models. First, there is no hierarchy in decoding of Seq2Seq-DU. A flat structure on top of BERT appears to be sufficient for jointly capturing the intents, slots, and values. Second, the decoder in Seq2Seq-DU generates pointers instead of tokens, and thus can easily and effectively handle categorical slots, non-categorical slots, as well as unseen schemas."
    }, {
      "heading" : "2.2 Natural Language Understanding",
      "text" : "Traditionally the problem of NLU is decomposed into two independent issues, namely classification of intents and sequence labeling of slot-value pairs (Liu and Lane, 2016; Hakkani-Tür et al., 2016). For example, deep neural network combined with conditional random field is employed for the task (Yao et al., 2014). Recently the pretrained language model BERT (Chen et al., 2019) is exploited to further enhance the accuracy. Methods are also proposed which can jointly train and utilize classification and sequence labeling models (Chen\net al., 2019; Goo et al., 2018). In this paper, we view NLU as special case of DST and employ our model Seq2Seq-DU to perform NLU. Seq2Seq-DU can degenerate to a BERT based NLU model."
    }, {
      "heading" : "3 Our Approach",
      "text" : "Our approach Seq2Seq-DU formalizes dialogue state tracking as a sequence to sequence problem using BERT and pointer generation. As shown in Figure 2, Seq2Seq-DU consists of an utterance encoder, a schema encoder, an utterance schema attender, and a state decoder. In each turn of dialogue, the utterance encoder transforms the current user utterance and the previous utterances in the dialogue into a sequence of utterance embeddings using BERT; the schema encoder transforms the schema descriptions into a set of schema embeddings also using BERT; the utterance schema attender calculates attentions between the utterance embeddings and the schema embeddings to create attended utterance and schema representations; finally, the state decoder sequentially generates a state representation on the basis of the attended representations using LSTM and pointer generation."
    }, {
      "heading" : "3.1 Utterance Encoder",
      "text" : "The utterance encoder takes the current user utterance as well as the previous utterances (user and system utterances) in the dialogue (a sequence of tokens) as input and employs BERT to construct a sequence of utterance embeddings. The relations between the current utterance and the previous utterances are captured by the encoder.\nThe input of the encoder is a sequence of tokens with length N , denoted as X = (x1, ..., xN ). The\nfirst token x1 is [CLS], followed by the tokens of the current user utterance and the tokens of the previous utterances, separated by [SEP]. The output is a sequence of embeddings also with length N , denoted as D = (d1, ..., dN ) and referred to as utterance embeddings, with one embedding for each token."
    }, {
      "heading" : "3.2 Schema Encoder",
      "text" : "The schema encoder takes the descriptions of intents, slots, and categorical slot values (a set of combined sequences of tokens) as input and employs BERT to construct a set of schema embeddings.\nSuppose that there are I intents, S slots, and V categorical slot values in the schemas. Each schema element is described by two descriptions as outlined in Table 2. The input is a set of combined sequences of tokens, denoted as Y = {y1, ..., yM}. Note that M = I + S + V . Each combined sequence starts with [CLS], followed by the tokens of the two descriptions with [SEP] as a separator. The final representation of [CLS] is used as the embedding of the input intent, slot, or slot value. The output is a set of embeddings, and all the embeddings are called schema embeddings E = {e1, ..., eM}.\nThe schema encoder in fact adopts the same approach of schema encoding as in (Rastogi et al., 2019). There are two advantages with the approach. First, the encoder can be trained across different domains. Schema descriptions in different domains can be utilized together. Second, once the encoder is fine-tuned, it can be used to process unseen schemas with new intents, slots, and slot values."
    }, {
      "heading" : "3.3 Utterance-Schema Attender",
      "text" : "The utterance-schema attender takes the sequence of utterance embeddings and the set of schema embeddings as input and calculates schema-attended utterance representations and utterance-attended schema representations. In this way, information from the utterances and information from the schemas are fused.\nFirst, the attender constructs an attention matrix, indicating the similarities between utterance embeddings and schema embeddings. Given the i-th utterance token embedding di and j-th schema embedding ej , it calculates the similarity as follows,\nA(i, j) = rᵀtanh(W1di +W2ej), (1)\nwhere r, W1, W2 are trainable parameters. The attender then normalizes each row of matrix A as a probability distribution, to obtain matrix A. Each row represents the attention weights of schema elements with respect to an utterance token. Then the schema-attended utterance representations are calculated as Da = EA\nᵀ. The attender also normalizes each column of matrix A as a probability distribution, to obtain matrix Ã. Each column represents the attention weights of utterance tokens with respect to a schema element. Then the utterance-attended schema representations are calculated as Ea = DÃ."
    }, {
      "heading" : "3.4 State Decoder",
      "text" : "The state decoder sequentially generates a state representation (semantic frame) for the current turn, which is represented as a sequence of pointers to elements of the schemas and tokens of the utterances (cf., Figure 1). The sequence can then be either re-formalized as a semantic frame in dialogue state tracking2,\n[intent; (slot1, value1); (slot2, value2); ...],\n2For simplicity, we assume here that there is only one semantic frame in each turn. In principle, there can be multiple frames.\nor a sequence of labels in NLU (intent-labeling and slot-filling). The pointers point to the elements of intents, slots, and slot values in the schema descriptions (categorical slot values), as well as the tokens in the utterances (non-categorical slot values). The elements in the schemas can be either words or phrases, and the tokens in the utterances form spans for extraction of slot values.\nThe state decoder is an LSTM using pointer (Vinyals et al., 2015) and attention (Bahdanau et al., 2015). It takes the two representations Da and Ea as input. At each decode step t, the decoder receives the embedding of the previous item wt−1, the utterance context vector ut, the schema context vector st, and the previous hidden state ht−1, and produces the current hidden state ht:\nht = LSTM(wt−1, ht−1, ut, st). (2)\nWe adopt the attention function in (Bahdanau et al., 2015) to calculate the context vectors as follows,\nut = attend(ht−1, Da, Da), (3)\nst = attend(ht−1, Ea, Ea). (4)\nThe decoder then generates a pointer from the set of pointers in the schema elements and the tokens of the utterances on the basis of the hidden state ht. Specifically, it generates a pointer of item w according to the following distribution,\nzw = q ᵀtanh(U1ht + U2kw), (5)\nP (#w) = softmax(zw), (6)\nwhere #w is the pointer of item w, kw is the representation of item w either in the utterance representations Da or in the schema representations Ea, q, U1, and U2 are trainable parameters, and softmax is calculated over all possible pointers.\nDuring decoding, the decoder employs beam search to find the best sequences of pointers in terms of probability of sequence."
    }, {
      "heading" : "3.5 Training",
      "text" : "The training of Seq2Seq-DU follows the standard procedure of sequence-to-sequence. The only difference is that it is always conditioned on the schema descriptions. Each instance in training consists of the current utterance and the previous utterances, and the state representation (sequence of pointers) for the current turn. Two pre-trained\nBERT models are used for representations of utterances and schema descriptions respectively. The BERT models are then fine-tuned in the training process. Cross-entropy loss is utilized to measure the loss of generating a sequence."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct experiments using the benchmark datasets on task-oriented dialogue. SGD (Rastogi et al., 2019) and MultiWOZ2.2 (Zang et al., 2020) are datasets for DST; they include schemas with categorical slots and non-categorical slots in multiple domains and natural language descriptions on the schemas, as shown in Table 2. In particular, SGD includes unseen schemas in the test set. MultiWOZ2.1 (Eric et al., 2020) is the previous version of MultiWOZ2.2, which only has categorical slots in multiple domains. WOZ2.0 (Wen et al., 2017) and DSTC2 (Henderson et al., 2014) are datasets for DST; they contain schemas with only categorical slots in a single domain. M2M (Shah et al., 2018) is a dataset for DST and it has span annotations for slot values in multiple domains. ATIS (Tur et al., 2010) and SNIPS (Coucke et al., 2018) are datasets for NLU in single-turn dialogues in a single domain. Table 3 gives the statics of datasets in the experiments."
    }, {
      "heading" : "4.2 Baselines and Variants",
      "text" : "We make comparison between our approach and the state-of-the-art methods on the datasets. SGD, MultiWOZ2.2 and MultiWOZ2.1: We compare Seq2SeqDU with six state-of-the-art methods on SGD, MultiWOZ2.2 and MultiWOZ2.1, which utilize schema descriptions, span-based and candidate-based methods, unified seq2seq model and BERT: FastSGT (Noroozi et al., 2020), SGDbaseline (Rastogi et al., 2019), TripPy (Heck et al., 2020), SimpleTOD (Hosseini-Asl et al.,\n2020), TRADE (Wu et al., 2019), and DSDST (Zhang et al., 2019). WOZ2.0 and DSTC2: Our approach is compared against the state-of-the-art methods on WOZ2.0 and DSTC2, including those using a hierarchical seq2seq model and BERT: COMER (Ren et al., 2019), BERT-DST (Chao and Lane, 2019), StateNet (Ren et al., 2018), GLAD (Zhong et al., 2018), Belief Tracking (Ramadan et al., 2018), and Neural Belief Tracker (Mrkšić et al., 2017). M2M: We evaluate our approach and the stateof-the-art methods on M2M, which respectively employ a BERT-based architecture and a jointlytrained language understanding model, BERTDST (Chao and Lane, 2019) and DST+LU (Rastogi et al., 2018). ATIS and SNIPS: We make comparison between our approach and the state-of-the-art methods on ATIS and SNIPS for NLU within the sequence labeling framework, including Joint BERT (Chen et al., 2019), Slot-Gated (Goo et al., 2018), Atten.-BiRNN (Liu and Lane, 2016), and RNNLSTM (Hakkani-Tür et al., 2016).\nWe also include two variants of Seq2Seq-DU. The differences are whether to use the schema descriptions, and the formation of dialogue state. Seq2Seq-DU-w/oSchema: It is used for datasets that do not have schema descriptions. It only contains utterance encoder and state decoder. Seq2Seq-DU-SeqLabel: It is used for NLU in a single-turn dialogue. It views the problem as sequence labeling, and only contains the utterance encoder and state decoder."
    }, {
      "heading" : "4.3 Evaluation Measures",
      "text" : "We make use of the following metrics in evaluation. Intent Accuracy: percentage of turns in dialogue for which the intent is correctly identified. Joint Goal Accuracy: percentage of turns for which all the slots are correctly identified. For non-\ncategorical slots, a fuzzy matching score is used on SGD and exact match are used on the other datasets to keep the numbers comparable with other works. Slot F1: F1 score to evaluate accuracy of slot sequence labeling."
    }, {
      "heading" : "4.4 Training",
      "text" : "We use the pre-trained BERT model ([BERT-Base, Uncased]), which has 12 hidden layers of 768 units and 12 self-attention heads to encode utterances and schema descriptions. The hidden size of LSTM decoder is also 768. The dropout probability is 0.1. We also use beam search for decoding, with a beam size of 5. The batch size is set to 8. Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate of 1e-4. Hyper parameters are chosen using the validation dataset in all cases.\nThe training curves of all models are shown in Appendix A."
    }, {
      "heading" : "4.5 Experimental Results",
      "text" : "Tables 4, 5, 6, and 7 show the results. One can see that Seq2Seq-DU performs significantly better than the baselines in DST and performs equally well as the baselines in NLU.\nDST is carried out in different settings in SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, and M2M. In all cases, Seq2Seq-DU works significantly better than the baselines. The results indicate that Seq2Seq-DU is really a general and effective model for DST, which can be applied to multiple settings. Specifically, Seq2Seq-DU can leverage the schema descriptions for DST when they are available (SGD and MultiWOZ2.2, MultiWOZ2.1)3. It can work well in zero-shot learning to deal with unseen schemas (SGD). It can also effectively handle categorical slots (MultiWOZ2.1, WOZ2.0 and DSTC2) and non-categorical slots (M2M). It appears that the success of Seq2SeqDU is due to its suitable architecture design with a sequence-to-sequence framework, BERT-based encoders, utterance-schema attender, and pointer generation decoder.\nNLU is formalized as sequence labeling in ATIS and SNIPS. Seq2Seq-DU is degenerated to Seq2Seq-DU-SeqLabel, which is equivalent to the baseline of Joint Bert. The results suggest that it is\n3There are better performing systems in the SGD competition. The systems are not based on single methods and thus are not directly comparable with our method.\nthe case. Specially, the performances of Seq2SeqDU are comparable with Joint BERT, indicating that Seq2Seq-DU can also be employed in NLU."
    }, {
      "heading" : "4.6 Ablation Study",
      "text" : "We also conduct ablation study on Seq2Seq-DU. We validate the effects of three factors: BERTbased encoder, utterance-schema attention, and pointer generation decoder. The results indicate that all the components of Seq2Seq-DU are indispensable.\nEffect of BERT To investigate the effectiveness of using BERT in the utterance encoder and schema encoder, we replace BERT with Bi-directional LSTM and run the model on SGD and MultiWOZ2.2. As shown in Figure 3, the performance of the BiLSTM-based model Seq2Seq-DU-w/oBert in terms of Joint GA and Int. Acc decreases significantly compared with Seq2Seq-DU. It indicates that the BERT-based encoders can create and utilize more accurate representations for dialogue understanding.\nEffect of Attention To investigate the effectiveness of using attention, we compare Seq2Seq-DU with Seq2Seq-DUw/oAttention which eliminates the attention mechanism, Seq2Seq-DU-w/SchemaAtt which only contains the utterance-attended schema representations, and Seq2Seq-DU-w/UtteranceAtt which only contains the schema-attended utterance representations. Figure 3 shows the results on SGD and MultiWOZ2.2 in terms of Joint GA and Int. Acc. One can observe that without attention the performances deteriorate considerably. In addition, the performances of unidirectional attentions are inferior to the performance of bidirectional attention. Thus, utilization of bidirectional attention between utterances and schema descriptions is desriable.\nEffect of Pointer Generation To investigate the effectiveness of the pointer generation mechanism, we directly generate words from the vocabulary instead of generating pointers in the decoding process. Figure 3 also shows the results of Seq2Seq-DU-w/oPointer on SGD and MultiWOZ2.2 in terms of Joint GA and Int. Acc. From the results we can see that pointer generation is crucial for coping with unseen schemas. In SGD which contains a large number of unseen schemas in the test set, there is significant performance degradation without pointer generation. The results on MultiWOZ2.2, which does not have unseen schemas in the test set, show pointer generation can also make significant improvement on\nalready seen schemas by making full use of schema descriptions."
    }, {
      "heading" : "4.7 Discussions",
      "text" : "Case Study We make qualitative analysis on the results of Seq2Seq-DU and SGD-baseline on SGD and MultiWOZ2.2. We find that Seq2Seq-DU can make more accurate inference of dialogue states by leveraging the relations existing in the utterances and schema descriptions. For example, in the first case in Table 8, the user wants to find a cheap guesthouse. Seq2Seq-DU can correctly infer that the hotel type is “guesthouse” by referring to the relation between “hotel-pricerange” and “hotel-type”. In the second case, the user wants to rent a room with in-unit laundry. In the dataset, a user who intends to rent a room will care more about the laundry property. Seq2Seq-DU can effectively extract the relation between “intent” and “in-unit-laundry”, yielding a correct result. In contrast, SGD-baseline does not model the relations in the schemas, and thus it cannot properly infer the values of “hoteltype” and “in-unit-laundry”.\nDealing with Unseen Schemas We analyze the zero-shot learning ability of Seq2Seq-DU. Table 9 presents the accuracies of Seq2Seq-DU in different domains on SGD. (Note that only SGD has unseen schemas in test set.) We observe that the best performances can be obtained\nin the domains with all seen schemas. The domains that have more partially seen schemas achieve higher accuracies, such as ”Hotels”, ”Movies”, ”Services”. The accuracies decline in the domains with more unseen schemas, such as ”Messaging” and ”RentalCars”. We conclude that Seq2Seq-DU can perform zero-shot learning across domains. However, the ability still needs enhancement.\nCategorical Slots and Non-categorical Slots Table 10 shows the accuracies of Seq2Seq-DU and the baselines with respect to categorical and noncategorical slots on SGD and MultiWOZ2.2. (We did not compare with FastSGT on SGD dataset due to unavailability of the codes.) One can see that Seq2Seq-DU can effectively deal with both categorical and non-categorical slots. Furthermore, Seq2Seq-DU demonstrates higher accuracies on categorical slots than non-categorical slots. We conjecture that it is due to the co-occurrences of categorical slot values in both the dialogue history and the schema descriptions. The utterance-schema\nattention can more easily capture the relations between the values."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have proposed a new approach to dialogue state tracking. The approach, referred to as Seq2SeqDU, takes dialogue state tracking (DST) as a problem of transforming all the utterances in a dialogue into semantic frames (state representations) on the basis of schema descriptions. Seq2Seq-DU is unique in that within the sequence to sequence framework it employs BERT in encoding of utterances and schema descriptions respectively and generates pointers in decoding of dialogue state. Seq2Seq-DU is a global, reprentable, and scalable model for DST as well as NLU (natural language understanding). Experimental results show that Seq2Seq-DU significantly outperforms the state-ofthe-arts methods in DST on the benchmark datasets of SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, M2M, and performs as well as the stateof-the-arts in NLU on the benchmark datasets of ATIS and SNIPS."
    }, {
      "heading" : "A Training Curves",
      "text" : "Figure 4 shows the training losses of Seq2Seq-DU on the training datasets, while Figure 5 shows the accuracies of Seq2Seq-DU on the test sets during training. We regard training convergence when the fluctuation of loss is less than 0.01 for consecutive 20 thousand steps. Seq2Seq-DU converges at the 180k-th step on SGD, MultiWOZ2.2, and MultiWOZ2.1. Seq2Seq-DU-w/oSchema converges at the 150k-th step on WOZ2.0 and at the 140k-th step on DSTC2, and M2M. Furthermore, Seq2Seq-DUSeqLabel converges at the 130k-th step on ATIS and SNIPS. These are consistent with the general trends in machine learning that more complex models are more difficult to train."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert-dst: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer",
      "author" : [ "Guan-Lin Chao", "Ian Lane." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Chao and Lane.,? 2019",
      "shortCiteRegEx" : "Chao and Lane.",
      "year" : 2019
    }, {
      "title" : "A survey on dialogue systems: Recent advances and new frontiers",
      "author" : [ "Hongshen Chen", "Xiaorui Liu", "Dawei Yin", "Jiliang Tang." ],
      "venue" : "Acm Sigkdd Explorations Newsletter.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Schema-guided multi-domain dialogue state tracking with graph attention neural networks",
      "author" : [ "Lu Chen", "Boer Lv", "Chi Wang", "Su Zhu", "Bowen Tan", "Kai Yu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert for joint intent classification and slot filling",
      "author" : [ "Qian Chen", "Zhu Zhuo", "Wen Wang." ],
      "venue" : "arXiv preprint arXiv:1902.10909.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Credit: Coarse-to-fine sequence generation for dialogue state tracking",
      "author" : [ "Zhi Chen", "Lu Chen", "Zihan Xu", "Yanbin Zhao", "Su Zhu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:2009.10435.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Snips voice platform: an embedded spoken language",
      "author" : [ "Alice Coucke", "Alaa Saade", "Adrien Ball", "Théodore Bluche", "Alexandre Caulier", "David Leroy", "Clément Doumouro", "Thibault Gisselbrecht", "Francesco Caltagirone", "Thibaut Lavril" ],
      "venue" : null,
      "citeRegEx" : "Coucke et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Coucke et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Abhishek Sethi", "Sanchit Agarwal", "Shuyang Gao", "Adarsh Kumar", "Anuj Goyal", "Peter Ku", "Dilek Hakkani-Tur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2020
    }, {
      "title" : "Slot-gated modeling for joint slot filling and intent prediction",
      "author" : [ "Chih-Wen Goo", "Guang Gao", "Yun-Kai Hsu", "Chih-Li Huo", "Tsung-Chieh Chen", "Keng-Wei Hsu", "YunNung Chen." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Goo et al\\.,? 2018",
      "shortCiteRegEx" : "Goo et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
      "author" : [ "Dilek Hakkani-Tür", "Gökhan Tür", "Asli Celikyilmaz", "Yun-Nung Chen", "Jianfeng Gao", "Li Deng", "YeYi Wang." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Hakkani.Tür et al\\.,? 2016",
      "shortCiteRegEx" : "Hakkani.Tür et al\\.",
      "year" : 2016
    }, {
      "title" : "TripPy: A triple copy strategy for value independent neural dialog state tracking",
      "author" : [ "Michael Heck", "Carel van Niekerk", "Nurul Lubis", "Christian Geishauser", "Hsien-Chin Lin", "Marco Moresi", "Milica Gašić." ],
      "venue" : "ACL.",
      "citeRegEx" : "Heck et al\\.,? 2020",
      "shortCiteRegEx" : "Heck et al\\.",
      "year" : 2020
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "SIGDIAL.",
      "citeRegEx" : "Henderson et al\\.,? 2014",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "A simple language model for task-oriented dialogue",
      "author" : [ "Ehsan Hosseini-Asl", "Bryan McCann", "Chien-Sheng Wu", "Semih Yavuz", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:2005.00796.",
      "citeRegEx" : "Hosseini.Asl et al\\.,? 2020",
      "shortCiteRegEx" : "Hosseini.Asl et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges in building intelligent open-domain dialog systems",
      "author" : [ "Minlie Huang", "Xiaoyan Zhu", "Jianfeng Gao." ],
      "venue" : "ACM Transactions on Information Systems (TOIS).",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention-based recurrent neural network models for joint intent detection and slot filling",
      "author" : [ "Bing Liu", "Ian Lane." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Liu and Lane.,? 2016",
      "shortCiteRegEx" : "Liu and Lane.",
      "year" : 2016
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid O Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "ACL.",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "A fast and robust bert-based dialogue state tracker for schema-guided dialogue dataset",
      "author" : [ "Vahid Noroozi", "Yang Zhang", "Evelina Bakhturina", "Tomasz Kornuta." ],
      "venue" : "arXiv preprint arXiv:2008.12335.",
      "citeRegEx" : "Noroozi et al\\.,? 2020",
      "shortCiteRegEx" : "Noroozi et al\\.",
      "year" : 2020
    }, {
      "title" : "Structured prediction as translation between augmented natural languages",
      "author" : [ "Giovanni Paolini", "Ben Athiwaratkun", "Jason Krone", "Jie Ma", "Alessandro Achille", "Rishita Anubhai", "Cicero Nogueira dos Santos", "Bing Xiang", "Stefano Soatto." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Paolini et al\\.,? 2021",
      "shortCiteRegEx" : "Paolini et al\\.",
      "year" : 2021
    }, {
      "title" : "Large-scale multi-domain belief tracking with knowledge sharing",
      "author" : [ "Osman Ramadan", "Paweł Budzianowski", "Milica Gašić." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ramadan et al\\.,? 2018",
      "shortCiteRegEx" : "Ramadan et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task learning for joint language understanding and dialogue state tracking",
      "author" : [ "Abhinav Rastogi", "Raghav Gupta", "Dilek HakkaniTur." ],
      "venue" : "SIGDIAL.",
      "citeRegEx" : "Rastogi et al\\.,? 2018",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2018
    }, {
      "title" : "Scalable multi-domain dialogue state tracking",
      "author" : [ "Abhinav Rastogi", "Dilek Hakkani-Tür", "Larry Heck." ],
      "venue" : "ASRU.",
      "citeRegEx" : "Rastogi et al\\.,? 2017",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "author" : [ "Abhinav Rastogi", "Xiaoxue Zang", "Srinivas Sunkara", "Raghav Gupta", "Pranav Khaitan." ],
      "venue" : "arXiv preprint arXiv:1909.05855.",
      "citeRegEx" : "Rastogi et al\\.,? 2019",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2019
    }, {
      "title" : "Scalable and accurate dialogue state tracking via hierarchical sequence generation",
      "author" : [ "Liliang Ren", "Jianmo Ni", "Julian McAuley." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards universal dialogue state tracking",
      "author" : [ "Liliang Ren", "Kaige Xie", "Lu Chen", "Kai Yu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Ren et al\\.,? 2018",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2018
    }, {
      "title" : "Building a conversational agent overnight with dialogue self-play",
      "author" : [ "Pararth Shah", "Dilek Hakkani-Tür", "Gokhan Tür", "Abhinav Rastogi", "Ankur Bapna", "Neha Nayak", "Larry Heck." ],
      "venue" : "arXiv preprint arXiv:1801.04871.",
      "citeRegEx" : "Shah et al\\.,? 2018",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2018
    }, {
      "title" : "What is left to be understood in atis? In SLT",
      "author" : [ "Gokhan Tur", "Dilek Hakkani-Tür", "Larry Heck" ],
      "venue" : null,
      "citeRegEx" : "Tur et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tur et al\\.",
      "year" : 2010
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "A networkbased end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young." ],
      "venue" : "EACL.",
      "citeRegEx" : "Wen et al\\.,? 2017",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "Transferable multi-domain state generator for task-oriented dialogue systems",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Ehsan HosseiniAsl", "Caiming Xiong", "Richard Socher", "Pascale Fung." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "An end-to-end approach for handling unknown slot values in dialogue state tracking",
      "author" : [ "Puyang Xu", "Qi Hu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Xu and Hu.,? 2018",
      "shortCiteRegEx" : "Xu and Hu.",
      "year" : 2018
    }, {
      "title" : "Recurrent conditional random field for language understanding",
      "author" : [ "Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Yao et al\\.,? 2014",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2014
    }, {
      "title" : "Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines",
      "author" : [ "Xiaoxue Zang", "Abhinav Rastogi", "Srinivas Sunkara", "Raghav Gupta", "Jianguo Zhang", "Jindong Chen" ],
      "venue" : null,
      "citeRegEx" : "Zang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking",
      "author" : [ "Jian-Guo Zhang", "Kazuma Hashimoto", "Chien-Sheng Wu", "Yao Wan", "Philip S Yu", "Richard Socher", "Caiming Xiong." ],
      "venue" : "arXiv preprint arXiv:1910.03544.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Recent advances and challenges in task-oriented dialog systems",
      "author" : [ "Zheng Zhang", "Ryuichi Takanobu", "Qi Zhu", "Minlie Huang", "Xiaoyan Zhu." ],
      "venue" : "Science China Technological Sciences.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Global-locally self-attentive encoder for dialogue state tracking",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zhong et al\\.,? 2018",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkšić et al., 2017; Goo et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 37,
      "context" : "Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkšić et al., 2017; Goo et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkšić et al., 2017; Goo et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkšić et al., 2017; Goo et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 181
    }, {
      "referenceID" : 34,
      "context" : "Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 25,
      "context" : "Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 13,
      "context" : "Recently, a new approach is proposed which formalizes the tasks in dialogue as sequence prediction problems using a unified language model (based on GPT-2) (Hosseini-Asl et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 183
    }, {
      "referenceID" : 20,
      "context" : "We note that very recently the effectiveness of the sequence to sequence approach has also been verified in other language understanding tasks (Paolini et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "(2) Representable: It employs BERT (Devlin et al., 2019) to learn and utilize better representations of not only the current utterance but also the previous utterances in the dialogue.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "(3) Scalable: It uses the pointer generation mechanism, as in the Pointer Network (Vinyals et al., 2015), to create representations of intents, slots, and slot-values, no matter whether the slots are categorical or non-categorical, and whether the schemas are unseen or not.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 32,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 37,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 26,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 23,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 21,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 31,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 35,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 11,
      "context" : "Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkšić et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 357
    }, {
      "referenceID" : 19,
      "context" : "FastSGD (Noroozi et al., 2020) BERT-based model, employs two carry-over procedures and multi-head attentions to model schema descriptions.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "SGD Baseline (Rastogi et al., 2019) BERT-based model, predictions are made over a dynamic set of intents and slots, using their descriptions.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "TripPy (Heck et al., 2020) BERT-based model, make use of various copy mechanisms to fill slots with values.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 31,
      "context" : "TRADE (Wu et al., 2019) Generate dialogue states from utterances using a copy mechanism, facilitating knowledge transfer for new schema elements.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "DS-DST (Zhang et al., 2019) BERT-based model, to classify over a candidate list or find values from text spans.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "BERT-DST (Chao and Lane, 2019) Use BERT as dialogue context encoder and makes parameter sharing across slots.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "StateNet (Ren et al., 2018) Independent of number of values, shares parameters across slots and uses pre-trained word vectors.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "GLAD (Zhong et al., 2018) Use global modules to share parameters across slots and uses local modules to retrain slot-specific parameters.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "Belief Tracking (Ramadan et al., 2018) Utilize semantic similarity between dialogue utterances and ontology, and information is shared across domains.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Neural Belief Tracker (Mrkšić et al., 2017) Conduct reasoning on pre-trained word vectors, and combines them into representations of user utterance and dialogue context.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "DST+LU (Rastogi et al., 2018) Select candidates for each slot, while candidates are generated by NLU.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "Joint BERT (Chen et al., 2019) A joint intent classification and slot filling model based on BERT.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "Slot-Gated (Goo et al., 2018) Use a slot gate, models relation between intent and slot vectors to create semantic frames.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "-BiRNN (Liu and Lane, 2016) Attention-based model, explores several strategies for alignment between intent classification and slot labeling.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "RNN-LSTM (Hakkani-Tür et al., 2016) Use RNN with LSTM cells to create complete semantic frames from user utterances.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Sequicity (Lei et al., 2018) Two-stage sequence-to-sequence model based on CopyNet, conducts both dialogue state tracking and response generation.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : "COMER (Ren et al., 2019) BERT-based hierarchical encoder-decoder model, generates state sequence based on user utterance WOZ2.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "CREDIT (Chen et al., 2020b) Hierarchical encoder-decoder model, views DST as a sequence generation problem.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "SimpleTOD (Hosseini-Asl et al., 2020) A unified sequence-to-sequence model based on GPT-2, conducts dialogue state tracking, dialogue action prediction, and response generation.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "As such, a number of methods are developed in the recent dialogue competition SGD (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020; Chen et al., 2020a).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 34,
      "context" : "As such, a number of methods are developed in the recent dialogue competition SGD (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020; Chen et al., 2020a).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "As such, a number of methods are developed in the recent dialogue competition SGD (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020; Chen et al., 2020a).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "As such, a number of methods are developed in the recent dialogue competition SGD (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020; Chen et al., 2020a).",
      "startOffset" : 82,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "Sequicity (Lei et al., 2018) is a two-step sequence to sequence model which first encodes the dialogue history and generates a belief span, and then generates a language response from the belief span.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : ", 2019) and CREDIT (Chen et al., 2020b) are hierarchical sequence-to-sequence models which represent the intents and slot-value pairs in a hierarchical way, and employ a multi-stage decoder.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "SimpleTOD (Hosseini-Asl et al., 2020) is a unified approach to task-oriented dialogue which employs a single and causal language model to perform sequence prediction in DST, Policy, and NLG.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "Traditionally the problem of NLU is decomposed into two independent issues, namely classification of intents and sequence labeling of slot-value pairs (Liu and Lane, 2016; Hakkani-Tür et al., 2016).",
      "startOffset" : 151,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : "Traditionally the problem of NLU is decomposed into two independent issues, namely classification of intents and sequence labeling of slot-value pairs (Liu and Lane, 2016; Hakkani-Tür et al., 2016).",
      "startOffset" : 151,
      "endOffset" : 197
    }, {
      "referenceID" : 33,
      "context" : "For example, deep neural network combined with conditional random field is employed for the task (Yao et al., 2014).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "Recently the pretrained language model BERT (Chen et al., 2019) is exploited to further enhance the accuracy.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "1718 The schema encoder in fact adopts the same approach of schema encoding as in (Rastogi et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "The state decoder is an LSTM using pointer (Vinyals et al., 2015) and attention (Bahdanau et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "We adopt the attention function in (Bahdanau et al., 2015) to calculate the context vectors as follows,",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 34,
      "context" : "2 (Zang et al., 2020) are datasets for DST; they include schemas with categorical slots and non-categorical slots in multiple domains and natural language descriptions on the schemas, as shown in Table 2.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "1 (Eric et al., 2020) is the previous version of MultiWOZ2.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : ", 2017) and DSTC2 (Henderson et al., 2014) are datasets for DST; they contain schemas with only categorical slots in a single domain.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 27,
      "context" : "M2M (Shah et al., 2018) is a dataset for DST and it has span annotations for slot values in multiple domains.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : ", 2010) and SNIPS (Coucke et al., 2018) are datasets for NLU in single-turn dialogues in a single domain.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "1, which utilize schema descriptions, span-based and candidate-based methods, unified seq2seq model and BERT: FastSGT (Noroozi et al., 2020), SGDbaseline (Rastogi et al.",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : ", 2020), SGDbaseline (Rastogi et al., 2019), TripPy (Heck et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : ", 2019), TripPy (Heck et al., 2020), SimpleTOD (Hosseini-Asl et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : ", 2020), SimpleTOD (Hosseini-Asl et al., 2020), TRADE (Wu et al.",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : ", 2020), TRADE (Wu et al., 2019), and DSDST (Zhang et al.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "0 and DSTC2, including those using a hierarchical seq2seq model and BERT: COMER (Ren et al., 2019), BERT-DST (Chao and Lane, 2019), StateNet (Ren et al.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : ", 2019), BERT-DST (Chao and Lane, 2019), StateNet (Ren et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : ", 2019), BERT-DST (Chao and Lane, 2019), StateNet (Ren et al., 2018), GLAD (Zhong et al.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 37,
      "context" : ", 2018), GLAD (Zhong et al., 2018), Belief Tracking (Ramadan et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : ", 2018), Belief Tracking (Ramadan et al., 2018), and Neural Belief Tracker (Mrkšić et al.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : ", 2018), and Neural Belief Tracker (Mrkšić et al., 2017).",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "M2M: We evaluate our approach and the stateof-the-art methods on M2M, which respectively employ a BERT-based architecture and a jointlytrained language understanding model, BERTDST (Chao and Lane, 2019) and DST+LU (Rastogi et al.",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : "M2M: We evaluate our approach and the stateof-the-art methods on M2M, which respectively employ a BERT-based architecture and a jointlytrained language understanding model, BERTDST (Chao and Lane, 2019) and DST+LU (Rastogi et al., 2018).",
      "startOffset" : 214,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "ATIS and SNIPS: We make comparison between our approach and the state-of-the-art methods on ATIS and SNIPS for NLU within the sequence labeling framework, including Joint BERT (Chen et al., 2019), Slot-Gated (Goo et al.",
      "startOffset" : 176,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "-BiRNN (Liu and Lane, 2016), and RNNLSTM (Hakkani-Tür et al.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "-BiRNN (Liu and Lane, 2016), and RNNLSTM (Hakkani-Tür et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate of 1e-4.",
      "startOffset" : 5,
      "endOffset" : 26
    } ],
    "year" : 2021,
    "abstractText" : "This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system. Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently. This paper proposes a new approach to dialogue state tracking, referred to as Seq2SeqDU, which formalizes DST as a sequence-tosequence problem. Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue. Seq2Seq-DU has the following advantages. It can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on BERT; it can effectively deal with categorical and non-categorical slots, and unseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural language understanding) module of a dialogue system. Experimental results on benchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the existing methods.",
    "creator" : "LaTeX with hyperref"
  }
}