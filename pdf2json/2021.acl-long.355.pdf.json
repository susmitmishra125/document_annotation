{
  "name" : "2021.acl-long.355.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Continuous Language Generative Flow",
    "authors" : [ "Zineng Tang", "Shiyue Zhang", "Hyounghun Kim", "Mohit Bansal" ],
    "emails" : [ "mbansal}@cs.unc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4609–4622\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4609"
    }, {
      "heading" : "1 Introduction",
      "text" : "Several generative models have been proposed for language generation, including sequence-tosequence models based on RNNs (Luong et al., 2015) and transformers (Vaswani et al., 2017), as well as variational autoencoders (VAEs) to generate diverse texts (Bowman et al., 2016; Jain\n1Our code and models are available at: https:// github.com/zinengtang/ContinuousFlowNLG\net al., 2017), plus generative adversarial networks (GANs) (Yu et al., 2017) to improve intended semantic fidelity. Another line of the generative model, normalizing flow (Rezende and Mohamed, 2015), is widely explored in computer vision and representation learning but less explored for NLG tasks. Flow models have been shown to be capable of improving probability density estimation, including variational inference (Rezende and Mohamed, 2015) and exact density estimation (Dinh et al., 2015). Generative flow is one type of flow model and first proposed by Dinh et al. (2015, 2017); Kingma and Dhariwal (2018). Taking advantage of its invertible structure, it can perform an exact density estimation of the input distribution. Thus, during generation, we can sample from its latent space and then generate new examples through its invertible decoder. Generative flow shows strong performance on image generation, attribute manipulation, and latent space inference (Kingma and Dhariwal, 2018). Considering these successful applications, we conjecture that the flow model should also have strong potential to be adapted for language generation tasks. Therefore, in this paper, we introduce a continuous language generative flow model that can deal with discrete language data in continuous latent space. We propose two variants, the non-autoregressive and autoregressive models, and show that they both can perform well on density estimation tasks.\nWe follow the architecture of one previous generative flow model, Glow (Kingma and Dhariwal, 2018), but make adaptions for language generation tasks. We first employ GloVe word embeddings (Pennington et al., 2014) to map the discrete token sequence to a continuous embedding matrix. Furthermore, we utilize two components: time-dimension permutation and affine coupling with RNN or Transformer non-linearity functions, which allow interaction between words in a se-\nquence and better contextualizes language semantics. Overall, these proposed components help generate texts in a non-autoregressive manner.\nHowever, even though the non-autoregressive model has attracted a lot of research attention because of its fast generation speed, it still hardly surpasses the generation quality of autoregressive models (Ren et al., 2020). Therefore, to make our language flow model learn language generation in a stronger autoregressive manner, we change the flow model’s affine coupling and permutation to a uni-directional structure, i.e., each timestep can only attend to previous timesteps. In this way, we enable our model to perform text generation autoregressively.\nSome recent works have developed density estimation models targeted on character-level discrete data (DiscreteFlow (Tran et al., 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive text generation (FlowSeq (Ma et al., 2019)). While our work shares some similar characteristics, we explore different directions: (1) DiscreteFlow develops a modulus calculation method to process discrete data. Instead, we use word embedding to transform the discrete input tokens to continuous features, which is simple yet effective. (2) FlowSeq essentially leverages the flow architecture in a typical encoder-decoder model to support non-autoregressive generation, whereas our models follow the standard generative flow framework and can directly generate texts via their invertible structure in both non-autoregressive or autoregressive manner. (3) Autoregressive flows were previously developed (Papamakarios et al., 2017; Huang et al., 2018) for stronger density estimation ability. However, the autoregressive language flow model we develop here aims for better text generation quality. For this, our model is autoregressive in both the forward stage (encoding an input to a latent feature ) and inverse stage (decoding the latent feature to the input ) with an uni-directional (i.e., the left-to-right direction) structure,\nWe evaluate the density estimation ability of our language flow models as well as their effectiveness for three downstream tasks: (1) sequenceto-sequence (Seq-to-Seq) generation that includes question generation (QG) and neural machine translation (NMT) and (2) data augmentation for Question Answering (QA). We test QG and QA data augmentation on two large-scale QA datasets: (a)\nSQuAD (Rajpurkar et al., 2016), a widely explored textual QA and QG dataset and (b) TVQA (Lei et al., 2018), a large-scale multimodal videodialogue QA task. We test machine translation on WMT16 (Cettolo et al., 2012), a commonly used NMT dataset.\nFor density estimation, we compare the negative likelihoods of our models against a baseline LSTM model. For QG, we use the non-autoregressive flow model to provide extra input features for a standard encoder-decoder text generation model. We show that it can significantly improve a baseline QG model for both SQuAD and TVQA on both automatic and human evaluation metrics. Aided by our flow model, we achieve strong improvements over a transformer baseline in the neural machine translation experiment. In addition to improving language generation quality, we also use the proposed autoregressive flow model for data augmentation. For this, we focus on generating diverse textual contexts for QA tasks. In particular, we inject noise into the latent features of our flow models (encoded from ground-truth contexts) and then generate new contexts from the noise-injected features. Experiments show that the generated contexts can be either a varied expression of the same subject or paraphrasing the original context, but, mostly keep the answerability of the original question (see examples in Table 3). Combined with data augmentation strategies (data filtering and training schema), we achieve statistically significant improvements on both SQuAD and TVQA over strong baselines.\nOverall, we have two contributions: (1) we propose two continuous language generative flow model variants that have better density estimation abilities than an LSTM baseline model, and can perform non-autoregressive and autoregressive generation respectively; (2) Our language flow model largely improves QG, NMT, and data augmentation for QA tasks."
    }, {
      "heading" : "2 Language Generative Flow",
      "text" : "In this section, we first review the generative flow model proposed in previous works (Dinh et al., 2015; Kingma and Dhariwal, 2018). Then, following it, we propose two variants of our continuous language generative flow model."
    }, {
      "heading" : "2.1 Background: Generative Flow",
      "text" : "Flow-based generative models transform simple latent distributions, p(z), into a complex data dis-\ntribution (language text in our case), p(x), through a chain of invertible transformations.\nWe first designate a true data distribution p(x) and a model pθ(x) with parameters θ to parameterize the true distribution p(x). The latent space inference is then defined as:\nxi ∼ p(x) (1) zi = fθ(xi) (2)\nwhere xi is a data point from the true data distribution and zi the latent features. This encoding x to z procedure is usually referred as the forward stage.\nThe transformation fθ is designed to be invertible and bijective. In previous flow-based generative models (Dinh et al., 2015, 2017; Kingma and Dhariwal, 2018), the generative process (or referred as the inverse stage) is defined as:\nzi ∼ pθ(z) (3) xi = gθ(zi) = f −1 θ (zi) (4)\nwhere zi is a sample from the latent space distribution, such as a standard Gaussian distribution.\nThe flow mapping fθ is composed of a chain of transformations: f = f1 ◦ f2 ◦ · · · ◦ fK with each representing one flow step. Then, the log-likelihood can be written as:\nlog pθ(x) = log pθ(z) + K∑ j=1 log ∣∣∣∣det( dhjdhj−1 )∣∣∣∣ (5)\nwhere hj is the output of each flow step. The value log |det(dhj/dhj−1)| is namely the logdeterminant: the log of the absolute value of the determinant of the Jacobian matrix (dhj/dhj−1). This value is the change in log-density from hj−1 to hj under transformation fj . This equation is namely the change of variable formula.\nThe objective for density estimation is formulated as:\nL(D) = 1 N N∑ i=1 − log pθ (x̃i)−M log d (6)\nx̃i = xi + u (7)\nwhere u is usually sampled from a Gaussian distribution, N the number of samples in a batch, and d (= 128) the discretization level of the data and M the dimension of xi.2\n2The change of variable formula, Eq.5, treats the data space as unbounded. However, the data we use is usually within range -1.0 to 1.0 and parameter d (the discretization) can reduce the impact of boundary effects according to Dinh et al. (2017).\nEach flow step in the generative flow model includes three parts: Normalization, Permutation, and Affine coupling. (1) Normalization is designed to scale each output to stabilize training. We follow Glow (Kingma and Dhariwal, 2018) to use actnorm. (2) Permutation makes sure after multiple flow steps, each channel can sufficiently affect other dimensions. The Glow model (Kingma and Dhariwal, 2018) proposes to use a (trainable) invertible 1 × 1 convolution. It is essentially a flexible generalization of a permutation operation. We follow Glow and also use its LU decomposition to reduce determinant computation cost. Different from all previous work, we apply 1× 1 convolution on the time dimension rather than the hidden dimension. This is because language data is sequential and temporal. This change is crucial to the proposed flow model’s performance, which will be shown in ablation studies (Table 4). (3) Affine coupling is designed to incorporate complex nonlinear mapping but still keep invertibility (see Figure 1).\nz1, z2 = Split(z0, dim : time) (8)\ns, t = Split(NN(z1),dim : hidden) (9)\nẑ2 = σ(s+ α) (t+ z2) (10)\nwhere NN refers to nonlinear function, σ is sigmoid activation. α is a hyperparameter that prevents small value (around 0) from resulting in large negative value by log. Note that, in the first equation, Glow (Kingma and Dhariwal, 2018) splits along the hidden dimension. However, we split along time dimension (first introduced in FlowSeq (Ma et al., 2019)) which has the same motivation as the permutation module."
    }, {
      "heading" : "2.2 Non-Autoregressive Language Flow",
      "text" : "We first present our non-autoregressive language flow which is based on the architecture introduced above. Besides the permutation/affine coupling structures changes introduced above, we use RNNs or Transformer as the nonlinear mapping, propose to use continuous input embedding, and introduce multi-scale architecture.\nAffine Coupling. We use a multihead selfattention module in transformer (Vaswani et al., 2017) or alternatively RNNs (a one-layer bidirectional LSTM (Schuster and Paliwal, 1997)) in the coupling layer by replacing the non-linear mapping of affine coupling, NN (see Eq.9).\nContinuous Input Embedding. The language flow model we propose operates on continuous inputs, which means the inputs are not discrete tokens but continuous word embeddings. We implement it through GloVe embeddings (Pennington et al., 2014). Therefore, the density estimation is performed for the distribution p(x), where x is the word embeddings of language tokens. Note that the word embeddings are frozen. In the inverse stage, we compute the cosine similarity between the embedding matrix and decoder output as the token generation probability distribution, so that all tokens can be generated in parallel, i.e., nonautoregressively.\nMulti-Scale Architecture. Following Dinh et al. (2017), we use a multi-scale architecture (see Figure 2) that contains multiple blocks while each block containing several flow steps. In our work, we denote the number of flow steps as K, and the number of blocks as L that each contains K flow steps. We denote the input shape as (batch size b, sequence length s, hidden dimension h). At the start of each block, the tensor is reshaped from (b, s, h) to (b, s2 , 2h), so the model can capture more local features; and at the end of each block (except the\nlast block), the latent feature is split into halves via channel dimension with one as the output, zl, and the other as the input of the next block. If we have 3 blocks, we will have three latent outputs, zl. Past works (Dinh et al., 2017; Kingma and Dhariwal, 2018; Ma et al., 2019) reshape in this manner for all blocks. However, we do not reshape in the first block but apply the same for the following blocks, which allows the model to better process the original input text with intact sentence structure."
    }, {
      "heading" : "2.3 Autoregressive Language Flow",
      "text" : "The model we developed in the previous subsection can properly operate on continuous word embeddings, have exact density estimation, and perform non-autoregressive generation, however, it lacks the autoregressive structure that is commonly used for text generation. Previous works have shown autoregressive generation usually performs better than non-autoregressive generation (Ren et al., 2020). Thus, we develop an autoregressive model that can generate text from left to right in the inverse stage. To achieve this, we change affine coupling and permutation in the flow step to be unidirectional, i.e., each timestep can only attend to timesteps that precede it. However, we have to remove the multi-scale architecture to fulfill the autoregressive requirement. See sample outputs in Table 1 for comparison to those from the nonautoregressive model.\nUni-directional Permutation. Since the permutation in each flow step designed in our nonautoregressive flow model is bidirectional, we mask the 1× 1 convolution to a lower triangular matrix. Therefore, each token can only attend to previous tokens in the permutation, i.e., uni-directional permutation.\nUni-directional Affine Coupling. We then introduce an autoregressive version of affine coupling, shown by the AC-cell in Figure 3. For each flow step, we denote the input sequence as ẑ (0):(T ) k+1 = [ẑ (0) k+1, ..., ẑ (T ) k+1], and then the autoregressive coupling is defined as:\nr(t−1) = NN([c(t−1); z(t−1)k+1 ]) (11)\nc(t) = ha(r (t−1), ẑ (t) k+1) (12)\nz (t) k+1 = hb(r (t−1), c(t)) (13)\nWe recurrently obtain the outputs, [z(1)k+1, ..., z (T ) k+1]. Note that z(0)k+1 = ẑ (0) k+1, so the computation starts from z(1)k+1. When computing z (1) k+1, we cannot get c(0), so we set it to be zero. ha and ha are both affine coupling structured, as shown in Figure 4. NN is either RNN or transformer.\nIn the inverse stage, to obtain ẑk+1 , we start from ẑ(0)k+1 = z (0) k+1 and c (0):\nr(t−1) = NN([c(t−1); ẑ(t−1)k+1 ]) (14)\nc(t) = h−1b (r (t−1), z (t) k+1) (15)\nẑ (t) k+1 = h −1 a (r (t−1), c(t)) (16)\nSince both decoded tokens z(t) and context c(t) only depend on previous tokens z(0):(t−1), we can perform autoregressive decoding and beam search\nwith cosine similarity as the probability distribution of output tokens.\nAutoregressive Flow Step. The changes of affine coupling and permutation to uni-directional allow the flow step to be autoregressive. And the whole autoregressive flow model will contain K such flow steps. At each flow step, the log-determinant is the summation of the log-determinant of all time steps:\nlog p(zk+1) = ∑ t log p(z (t) k+1) (17)\n= ∑ t log p(z (t) k ) + log ∣∣∣∣∣det ( dz (t) k+1 dz (t) k )∣∣∣∣∣ (18)"
    }, {
      "heading" : "3 Language Generation with Flow",
      "text" : "We next apply our flow model to several downstream tasks. Despite the flow’s rigid model structure, it has a strong potential in density estimation due to its complex transformation of inputs into a continuous latent space. We aim to use this property to improve standard encoder-decoder text generation models. Moreover, as the flow model has a strong ability in generating diverse text, we show that it has the capability for data augmentation to improve QA tasks."
    }, {
      "heading" : "3.1 Downstream Datasets",
      "text" : "SQuAD. SQuAD is a textual question answering dataset containing 100,000+ questions/answers with corresponding short articles as context. We use it to evaluate both question generation and data augmentation (by generating new articles) for question answering.\nTVQA. TVQA is a large-scale video QA dataset based on 6 TV shows. It consists of 152,545 QA pairs from 21,793 video clips with subtitle text. We use it to evaluate both question generation and data augmentation (by generating new subtitles) for question answering.\nWMT16 (RO-EN). WMT16 (RO-EN) is a machine translation dataset between English and Romanian with around 610k sentence pairs. We use it for our machine translation experiment and only test for the Romanian to English direction."
    }, {
      "heading" : "3.2 Seq-to-Seq Generation with Flow",
      "text" : "Similar to FlowSeq (Ma et al., 2019), we use flow as an extra module on top of a typical encoderdecoder language generation model and test on Question Generation (QG) and neural machine translation (NMT). As the flow model has the ability for exact density estimation, it provides the exact density components of context information and we assume that it provides a better hidden representation of context and thus helps with language generation. It can also be viewed as a self-supervised learning method that can provide new features for downstream tasks.\nConcretely, while the original QG model3 is formulated as ui = E(xi), q̂i = G(ui); the new QG model with flow is formulated as:\nui = E(xi), q̂i = G(hatt(ui, zi)) (19)\nwhere E refers to encoder and G decoder. zi refers to latent features of the non-autoregressive flow model. hatt is essentially a MLP with sigmoid activation.\nThe loss function has two parts:\nLgen = 1\nN N∑ i=1 − log p(qi) (20) L = λLnll + Lgen (21)\nwhere qi represents the target questions and λ is a hyperparameter for NLL loss (Eq. 6)\n3We replicate Zhang and Bansal (2019)’s standard encoderdecoder attention QG model with BERT features as input embeddings."
    }, {
      "heading" : "3.3 Context Generation for Data Augmentation",
      "text" : "Context Generation. We propose to use flow to generate diverse contexts for data augmentation as both TVQA and SQuAd are question answering tasks with textual context. We generate new context (video subtitles for TVQA; articles for SQuAD) by injecting noise to the hidden vector of the original context, zi, and reconstructing it to new sentences, x̂i. Note that, we can also do the same thing for questions, however, we find that changing one word in the question will dramatically change its meaning, so we limit this augmentation to the context and keep the original question unchanged.\nThe generation process is formulated as:\nzi = fθ(xi) (22) x̂i = f −1 θ (zi + z0) (23)\nwhere fθ refers to the flow model and xi the input text and zi the latent space. The transformation is performed by simply sampling a Gaussian noise z0, add it to zi, and reconstruct the new context x̂i in the reverse stage. In this task, we use the autoregressive flow model as this variant is designed for text generation. We also use the non-autoregressive flow model additionally leveraged by an additional autoregressive decoder, as an alternative approach.\nWhile the standard RNN-based language model does not have an explicit global sentence representation, our flow model is similar to Bowman et al. (2016)’s VAE framework that encodes the sentence into a continuous hidden vector, p(z|x). And sampling around the hidden vector can naturally be viewed as injecting noise without changing key information. Therefore, we do not aim for paraphrasing the original context because the flow model can reconstruct different information from random noise injection in latent space. Notably, this method has the risk of changing the context’s meaning and making the question unanswerable, however, empirically, we find that as long as we\nkeep the noise small enough, the generation will be either paraphrases or different expressions of the same subject without affecting the answerability.\nData Filtering. To better utilize the generated data, we design a data filter as filtering out the lowquality generated text is useful in helping improve the data augmentation (Zhang and Bansal, 2019). We use pretrained QA baseline models (see Table 8 Baseline TVQA+ and Table 9 Baseline BERT) to filter out the low-quality context. The generated context will be filtered out if the model performs worse on predicting correct answers when original context is replaced by its generated counterpart."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "We follow Zhang and Bansal (2019) to split the development set of SQuADv1.1 (Rajpurkar et al., 2016) into two halves and show the result on the test split. We generally follow previous work on evaluation metrics. For density estimation, we use negative log-likelihood (NLL) for comparison and bits per dimension to regularize the negative loglikelihood loss, formulated as LM log(2) , where M represents the dimension of input. We evaluate QG by BLEU4 (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation. We use the BLEU score to evaluate NMT. We use accuracy to evaluate the TVQA QA model and EM (exact match) and F1 score to evaluate the SQuAD QA model.\nWe replicate Zhang and Bansal (2019)’s baseline QG model. We use the STAGE model with\nGloVe embeddings developed by Lei et al. (2020) as the TVQA QA baseline and use BERT as the SQuAD QA baseline. See appendix A for more experiment/reproducibility details."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Negative Log-Likelihood Results",
      "text" : "First of all, to evaluate the density estimation ability, we compare the negative log-likelihood (NLL, Eq.6)4 of our different flow models on the context data of SQuAD and TVQA against a baseline model (a 3-layer bidirectional LSTM-RNN model with hidden size 300). As shown in Table 4, the flow model of time-dim coupling/permutation\n4Note that since our p(x) is over continuous word embeddings, so it is the probability density of a continuous variable which is not bounded by [0,1].\ngenerally outperforms the baseline LSTM model. The flow model of time-dim coupling/permutation largely outperforms the flow model of channeldim coupling/permutation. We also test our autoregressive model to check its density estimation ability, and we find it performs well and even sometimes slightly better than the non-autoregressive model. Note that we do not claim the autoregressive model is better at density estimation than the non-autoregressive version, instead, we aim to show that it can perform reasonably with the proposed autoregressive adaptation."
    }, {
      "heading" : "5.2 Seq-to-Seq Generation Results",
      "text" : "Question Generation. Through the ablation studies shown in Table 5 and Table 6, we demonstrate that the proposed flow-aided QG model significantly improves the QG performance. The statistical significances for all metric improvements (BLEU4, Rouge-L, Meteor) are p < 0.001 for both TVQA QG and SQuAD QG.5 We also conduct a human evaluation. We random sample 200 examples6, and we present the participants two questions per example generated by two different models and let them judge which question is better in terms of answerability and overall quality. See more human evaluation details in Appendix A.3. We compare our flow model to the pure encoder-decoder baseline as well as the FlowSeq model (Ma et al., 2019) in human evaluation. As shown in the last rows in Table 5 and Table 6, humans favor our model more than the baseline in both tasks, which indicates our flow model indeed provides useful latent features for better generation. Plus, our model also always outperforms FlowSeq. We conjecture that it is because FlowSeq is non-autoregressive whereas our QG model is autoregressive.\nNeural Machine Translation. We also test the effectiveness of our approach on a neural machine translation (NMT) task. We first replicate Lee et al. (2018)’s transformer autoregressive model baseline, and then we add our flow architecture on top of it. As shown in Table 7, our proposed flow-aided MT model can improve the machine translation performance over the strong transformer baseline on the WMT16 (Cettolo et al., 2012) Romanian to English translation task. See A.7 for more details. We hope\n5Statistical significance is computed using the bootstrap test (Efron and Tibshirani, 1994).\n6We exclude those examples where the two models generate identical questions.\nthat these promising initial NMT results will also encourage the community to use continuous flow models for other NMT and NLG tasks."
    }, {
      "heading" : "5.3 QA Data Augmentation Results",
      "text" : "As shown in Table 8 and Table 9, using the augmented data generated by our Language Flow model (refers to our autoregressive language flow model), we achieve significant performance improvements over strong baselines on both TVQA QA (Lei et al., 2020) (p < 0.0001) and SQuAD QA (Rajpurkar et al., 2016) (p < 0.0005) for both EM and F1. Furthermore, when we add an LSTM autoregressive decoder to our non-autoregressive encoder (referred to as Language Flow+) and use it to perform data augmentation, we observe even slightly better results. This may indicate the stronger encoding ability of our non-autoregressive model due to its multi-scale architecture. Mean-\nwhile, we compare to two other data augmentation techniques: paraphrasing (Niu and Bansal, 2018) and back-translation (Sennrich et al., 2016). Note that for a fair comparison, we apply the same data filter and training schema for all data augmentation methods. It can be seen that both methods perform worse than our Language Flow or Language Flow+ models."
    }, {
      "heading" : "6 Discussion",
      "text" : "We show some sample questions generated by our non-autoregressive and autoregressive flow models in Table 1. The autoregressive samples are better organized and grammatically sound, while non-autoregressive generation fails at the latter part of the sentence. It might because the nonautoregressive structure has a weaker ability to model the temporal dependency during generation, which is consistent with the observations from previous works (Ren et al., 2020). To show that our model generates samples from a continuous space, we generate interpolation samples from our autoregressive flow model shown in Table 2. Those samples are mostly grammatically sound and correctly reflect the intermediate content of the two interpolated sentences.\nWhile variational autoencoder has the issue of ignoring latent space (Li et al., 2019), our models do not suffer from this issue. We introduced two types of language generation models in the paper: (1) the autoregressive flow model (used in data augmentation tasks) and (2) the model that uses flow latent features as extra input (e.g., for QG tasks). Our autoregressive flow model’s decoder is\nthe inverted version of its encoder with the same weights, so it ensures the decoder uses the latent features. When we use flow latent features as extra inputs, it significantly improves QA performance (Table 5 and Table 6), which implies the latent features are usefully involved in generation."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have proposed a language generative flow model with non-autoregressive and autoregressive variants. The non-autoregressive flow model achieves strong performance on density estimation and helps improve question generation and machine translation by providing additional useful latent features to the decoder. Moreover, the autoregressive variant largely improves question answering by generating new contexts with noise injection."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their helpful feedback. This research is supported by NSF-CAREER Award 1846185, ONR Grant N00014-18-1-2871, and ARO-YIP Award #W911NF18-1-0336. The views contained in this article are those of the authors and not of the funding agency."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A Experimental Setup",
      "text" : "In this section, we introduce our experiment settings ranging from datasets usage, flow implementation details, question generation model, and data augmentation settings. We use a fixed seed 2020 for PyTorch random seed."
    }, {
      "heading" : "A.1 Software/Hardware Usage",
      "text" : "We use PyTorch 1.5 (Paszke et al., 2017) to build our model. We use Nvidia GeForce RTX 2080ti and Intel CPU (Intel(R) Xeon(R) Silver 4114 CPU\n@ 2.20GHz) built on Ubuntu 16.01 for each training or inference process."
    }, {
      "heading" : "A.2 Datasets",
      "text" : "We use SQuADv1.1 (Rajpurkar et al., 2016)7 and TVQA (Lei et al., 2018)8 to perform our experiments, since SQuAD is a widely explored QA and QG dataset, and TVQA is a video-based multimodal dataset with rich dialogue context. Therefore, question generation, context generation, and language density estimation, and data augmentation can be well performed and evaluated comprehensively on these two datasets and tasks.\nTVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. The subtitles in TVQA dataset has time-stamp annotations of localized clip in the full subtitle clip. The localized clip is the relevant interval of a clip for question answering. In both the video question generation task and the context generation task for data augmentation, we use localized subtitles. The TVQA context features are dialogues or video subtitles; hence data augmentation on this dataset should consider an additional frame-level dimension.\nSQuAD has over 100,000 questions and 23,215 paragraphs for the 536 articles covering a wide range of topics. We follow Zhang and Bansal (2019) to split the development set of SQuADv1.1 (Rajpurkar et al., 2016) into two splits and show the result on the second split."
    }, {
      "heading" : "A.3 Preprocessing",
      "text" : "We tokenize the data to be used for both GLoVe embedding and BERT features extraction, and we add the start of sentence token and the end of sentence token for every input."
    }, {
      "heading" : "A.4 Evaluation Metrics",
      "text" : "We generally follow previous work on evaluation metrics across density estimation, question generation, and question answering augmentation. Flow Model. For flow density estimation, we follow previous work (Kingma and Dhariwal, 2018; Dinh et al., 2017) to use negative log-likelihood for comparison. Question Generation Model. We evaluate the generation quality by BLEU4 (Papineni et al.,\n7Online link for SQuAD: rajpurkar.github.io/ SQuAD-explorer/explore/1.1/dev/\n8Online link for TVQA: tvqa.cs.unc.edu/\n2002), Meteor (Lavie and Agarwal, 2007), and Rouge-L (Lin, 2004) to provide an insight into the performance of our model. We also use Amazon Turk human evaluation that compares the baseline generation and the proposed model generation by proving a suitable QA context. For SQuAD QG, we present the article context, question pairs, and the answer for the users to select their preference in terms of answerability and overall quality of the question pair. For TVQA QG, we present the video clip, subtitle context, question pairs, and answer candidates for the users to select their preference in terms of answerability and overall quality of the questions pair.\nMachine Translation Model. We evaluate the generation quality by BLEU (Papineni et al., 2002) to provide an insight into the performance of our model.\nData Augmentation Model. We use accuracy scores to evaluate TVQA QA model, and follow previous work (Rajpurkar et al., 2016) to use EM (exact match) and F1 score to evaluate SQuAD QA model."
    }, {
      "heading" : "A.5 Flow Implementation Details",
      "text" : "The experiment on base flow models does not involve extensive hyperparameter search trials since flow models follow the principle: the deeper, the better. We use small-sized flow models across different versions of (K=8, L=3, parameter number: 128M for transformer module and 196M for RNN module) flow models for ablation study. The autoregressive flow model has K=24, L=1 with approximately the same parameter number, 130M, by changing the nonlinear functions complexity for a fair comparison.\nBased on the sequence length distribution of the dataset, we designate the maximum fixed flow sequence length for TVQA-subtitles as 64, SQuADparagraphs as 256. We set L=3 or 4 for all experiments while changing the number of flow steps K with a multiple of 8. While it follows that the more K, the better, setting L to a reasonable value is essential as each block will reduce sequence length by half. Therefore, L is set according to the length of the input.\nThe discretization, d, in the negative loglikelihood loss function (Eq.6) is set to 2n, where n=6. Noise, u, is set as Gaussian sample with α = 12m , where m=6. We follow previous work (Kingma and Dhariwal, 2018) to use bits per\ndimension to regularize the negative log-likelihood loss, formulated as L(M log(2)) , where M represents the dimension of input.\nWe use a learning rate between 1e-4 and 1e5, specifically 5e-5, to achieve stable and faster convergence (with Adam optimizer (Kingma and Ba, 2015), beta1=0.9, beta2=0.999). With prior knowledge of Adam optimizer, we perform 5 trials to test learning rate (1e-3, 5e-4. 1e-4. 5e-5, 1e-5) to find the fastest convergence rate.\nThe average training time is 50 epochs for a (k=8 L=3 parameter number: 128M for transformer module and 196M for RNN module) flow model, as each epoch takes 20 minutes. Inference for one sample takes around 0.01s.\nThe density estimation by the LSTM model we use for baseline comparison in NLL and QG models is designed to be well defined as a density estimation model. Flow density estimation models with no invertibility are not well-defined. Therefore, we mimic a model structure that the transformation is through only non-singular matrix weight to obtain an arithmetically invertible model."
    }, {
      "heading" : "A.6 Question Generation Implementation Details",
      "text" : "The experiment on question generation models does not involve extensive hyperparameter search trials, as the proposed model has stable convergence under varied circumstances. We take the last latent space output of the flow model as the features used for the QG model decoder or attention map.\nWe use (K=16, L=3 parameter number: 256M parameters for transformer module) flow models with transformer modules without autoregressive decoding for all the QG experiments. The loss weight of λ1 is set 1.0; the weight will not significantly affect the result as long as it is set to a reasonably large value. We use gradient descent with momentum optimizer (momentum = 0.8, lr = 1e-3) for both base model and flow model. With prior knowledge of the SGD optimizer, we perform four trials to test the learning rate (1e-2, 5e-3. 1e-3. 5e-4) to find the fastest convergence rate and stable training.\nWe employ Zhang and Bansal (2019)’s baseline QG model, which is a robust encoder-decoder attention generation network with a maxout pointer network and self-gated attention (Zhao et al., 2018) for both tasks.9 We use pretrained BERT (Devlin\n9Maxout pointer is not used in the TVQA QG model since\net al., 2019) hidden features with 768 dimensions by a small uncased BERT model to replace GLoVe embedding to make the baseline stronger to show that the flow model can still improve well on a strong baseline.\nThe average training time is 20 epochs for the joint training of the QG model and the (k=16 L=3) flow model, as each epoch takes 50 minutes. Inference for one sample takes around 0.03s."
    }, {
      "heading" : "A.7 Machine Translation Implementation Details",
      "text" : "For the machine translation dataset WMT16, the source and target languages share the same set of subword embeddings. The maximum text length is set to 64 and we filter out all data that is above this range. We use (K=4, L=3 with transformer module) non-autoregressive flow models with transformer modules for all the data augmentation experiments. We use Adam optimizer (Kingma and Ba, 2015) with beta1=0.9, beta2=0.999, and a learning rate 5e-5 for flow model training."
    }, {
      "heading" : "A.8 Data Augmentation Implementation Details",
      "text" : "The experiment on context generation models generally follows empirical hyperparameter settings.\nWe use (K=32, L=4 parameter number: 512M parameters for transformer module) autoregressive flow models with transformer modules for all the data augmentation experiments. We use Adam optimizer (Kingma and Ba, 2015) with beta1=0.9, beta2=0.999, and a learning rate 5e-5 for flow model training and an empirically stable learning rate 3e-4 for attention decoder training. We set z0 to a Gaussian noise sample with mean 0.0 and variance 1.0 during training and variance 0.5 during inference. For inference variance tuning, we start from variance 1.0 and gradually decrease by 0.1 until 0.1 to manually check which setting has generated samples with reliable quality and diversity suitable for robust data augmentation.\nThe average training time is 100 epochs for the (k=32 L=4 parameter number: 512M) augmentation flow model, as each epoch takes 30 minutes. Inference for one sample takes around 0.5s. Base QA model. We use model, backbone + Attn. Sup. + Temp. Sup. + local (STAGE) with GloVe embeddings, developed in TVQA+ dataset (Lei et al., 2020) as the QA baseline for TVQA data\nthe number of words out of vocabulary is small.\naugmentation model. We use the BERT baseline (Devlin et al., 2019) for SQuAD QA (Rajpurkar et al., 2016); this BERT Baseline is pretrained and uncased with 768 base dimension and finetuned on the SQuAD dataset. These two models are also used as data filters. Data Augmentation Strategies. The training schemes are crucial for context generation since the TVQA model has heavy dependence on the subtitles and SQuAD model on the paragraphs: similar to Zhang and Bansal (2019)’s strategies, we obtain approximately ten times the amount of augmented data than the original amount, and filter them to obtain approximately 40% of augmented data to be used for training. We set a probability, 0.5, for replacing the original data with newly generated filtered data for each batch in training. For TVQA data augmentation, we generate localized subtitles and replace the corresponding part in non-localized full-subtitles. For SQuAD data augmentation, We generate trunks of paragraphs that do not contain answers to replace the corresponding trunks in the original paragraphs."
    } ],
    "references" : [ {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Wit3: Web inventory of transcribed and translated talks",
      "author" : [ "Mauro Cettolo", "Christian Girardi", "Marcello Federico." ],
      "venue" : "Conference of european association for machine translation, pages 261–268.",
      "citeRegEx" : "Cettolo et al\\.,? 2012",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2012
    }, {
      "title" : "Nice: Non-linear independent components estimation",
      "author" : [ "Laurent Dinh", "David Krueger", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop",
      "citeRegEx" : "Dinh et al\\.,? 2015",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2015
    }, {
      "title" : "Density estimation using real NVP",
      "author" : [ "Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceed-",
      "citeRegEx" : "Dinh et al\\.,? 2017",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2017
    }, {
      "title" : "An introduction to the bootstrap",
      "author" : [ "Bradley Efron", "Robert J Tibshirani." ],
      "venue" : "CRC press.",
      "citeRegEx" : "Efron and Tibshirani.,? 1994",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1994
    }, {
      "title" : "Neural autoregressive flows",
      "author" : [ "Chin-Wei Huang", "David Krueger", "Alexandre Lacoste", "Aaron Courville." ],
      "venue" : "International Conference on Machine Learning, pages 2078–2087. PMLR.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Creativity: Generating diverse questions using variational autoencoders",
      "author" : [ "Unnat Jain", "Ziyu Zhang", "Alexander G Schwing." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6485–6494.",
      "citeRegEx" : "Jain et al\\.,? 2017",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Glow: Generative flow with invertible 1x1 convolutions",
      "author" : [ "Durk P Kingma", "Prafulla Dhariwal." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 10215–10224.",
      "citeRegEx" : "Kingma and Dhariwal.,? 2018",
      "shortCiteRegEx" : "Kingma and Dhariwal.",
      "year" : 2018
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231. Asso-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
      "author" : [ "Jason Lee", "Elman Mansimov", "Kyunghyun Cho." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "TVQA: localized, compositional video question answering",
      "author" : [ "Jie Lei", "Licheng Yu", "Mohit Bansal", "Tamara L. Berg." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, Oc-",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Tvqa+: Spatio-temporal grounding for video question answering",
      "author" : [ "Jie Lei", "Licheng Yu", "Tamara L Berg", "Mohit Bansal." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "A surprisingly effective fix for deep latent variable modeling of text",
      "author" : [ "Bohan Li", "Junxian He", "Graham Neubig", "Taylor Berg-Kirkpatrick", "Yiming Yang." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Flowseq: Nonautoregressive conditional sequence generation with generative flow",
      "author" : [ "Xuezhe Ma", "Chunting Zhou", "Xian Li", "Graham Neubig", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial over-sensitivity and over-stability strategies for dialogue models",
      "author" : [ "Tong Niu", "Mohit Bansal." ],
      "venue" : "Proceedings of the 22nd Conference on Computational Natural Language",
      "citeRegEx" : "Niu and Bansal.,? 2018",
      "shortCiteRegEx" : "Niu and Bansal.",
      "year" : 2018
    }, {
      "title" : "Masked autoregressive flow for density estimation",
      "author" : [ "George Papamakarios", "Theo Pavlakou", "Iain Murray." ],
      "venue" : "NeurlPS.",
      "citeRegEx" : "Papamakarios et al\\.,? 2017",
      "shortCiteRegEx" : "Papamakarios et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Automatic differentiation in PyTorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer." ],
      "venue" : "NIPS Autodiff Workshop.",
      "citeRegEx" : "Paszke et al\\.,? 2017",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A study of nonautoregressive model for sequence generation",
      "author" : [ "Yi Ren", "Jinglin Liu", "Xu Tan", "Zhou Zhao", "Sheng Zhao", "Tie-Yan Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of",
      "citeRegEx" : "Rezende and Mohamed.,? 2015",
      "shortCiteRegEx" : "Rezende and Mohamed.",
      "year" : 2015
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE transactions on Signal Processing, 45(11):2673– 2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Discrete flows: Invertible generative models of discrete data",
      "author" : [ "Dustin Tran", "Keyon Vafa", "Kumar Agrawal", "Laurent Dinh", "Ben Poole." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 14692–14701.",
      "citeRegEx" : "Tran et al\\.,? 2019",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Seqgan: Sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "Thirty-First AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Addressing semantic drift in question generation for semisupervised question answering",
      "author" : [ "Shiyue Zhang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Zhang and Bansal.,? 2019",
      "shortCiteRegEx" : "Zhang and Bansal.",
      "year" : 2019
    }, {
      "title" : "Paragraph-level neural question generation with maxout pointer and gated selfattention networks",
      "author" : [ "Yao Zhao", "Xiaochuan Ni", "Yuanyuan Ding", "Qifa Ke." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Several generative models have been proposed for language generation, including sequence-tosequence models based on RNNs (Luong et al., 2015) and transformers (Vaswani et al.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : ", 2015) and transformers (Vaswani et al., 2017), as well as variational autoencoders (VAEs) to generate diverse texts (Bowman et al.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : ", 2017), plus generative adversarial networks (GANs) (Yu et al., 2017) to improve intended semantic fidelity.",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "Another line of the generative model, normalizing flow (Rezende and Mohamed, 2015), is widely explored in computer vision and representation learning but less explored for NLG tasks.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Flow models have been shown to be capable of improving probability density estimation, including variational inference (Rezende and Mohamed, 2015) and exact density estimation (Dinh et al.",
      "startOffset" : 119,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "Flow models have been shown to be capable of improving probability density estimation, including variational inference (Rezende and Mohamed, 2015) and exact density estimation (Dinh et al., 2015).",
      "startOffset" : 176,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "Generative flow shows strong performance on image generation, attribute manipulation, and latent space inference (Kingma and Dhariwal, 2018).",
      "startOffset" : 113,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "We follow the architecture of one previous generative flow model, Glow (Kingma and Dhariwal, 2018), but make adaptions for language generation tasks.",
      "startOffset" : 71,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "We first employ GloVe word embeddings (Pennington et al., 2014) to map the discrete token sequence to a continuous embedding matrix.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "However, even though the non-autoregressive model has attracted a lot of research attention because of its fast generation speed, it still hardly surpasses the generation quality of autoregressive models (Ren et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 222
    }, {
      "referenceID" : 27,
      "context" : "Some recent works have developed density estimation models targeted on character-level discrete data (DiscreteFlow (Tran et al., 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive text generation (FlowSeq (Ma et al.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : ", 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive text generation (FlowSeq (Ma et al., 2019)).",
      "startOffset" : 159,
      "endOffset" : 176
    }, {
      "referenceID" : 18,
      "context" : "(3) Autoregressive flows were previously developed (Papamakarios et al., 2017; Huang et al., 2018) for stronger density estimation ability.",
      "startOffset" : 51,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "(3) Autoregressive flows were previously developed (Papamakarios et al., 2017; Huang et al., 2018) for stronger density estimation ability.",
      "startOffset" : 51,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "We test QG and QA data augmentation on two large-scale QA datasets: (a) SQuAD (Rajpurkar et al., 2016), a widely explored textual QA and QG dataset and (b) TVQA (Lei et al.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : ", 2016), a widely explored textual QA and QG dataset and (b) TVQA (Lei et al., 2018), a large-scale multimodal videodialogue QA task.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "We test machine translation on WMT16 (Cettolo et al., 2012), a commonly used NMT dataset.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "In this section, we first review the generative flow model proposed in previous works (Dinh et al., 2015; Kingma and Dhariwal, 2018).",
      "startOffset" : 86,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "In this section, we first review the generative flow model proposed in previous works (Dinh et al., 2015; Kingma and Dhariwal, 2018).",
      "startOffset" : 86,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "In previous flow-based generative models (Dinh et al., 2015, 2017; Kingma and Dhariwal, 2018), the generative process (or referred as the inverse stage) is defined as:",
      "startOffset" : 41,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "The Glow model (Kingma and Dhariwal, 2018) proposes to use a (trainable) invertible 1 × 1 convolution.",
      "startOffset" : 15,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "Note that, in the first equation, Glow (Kingma and Dhariwal, 2018) splits along the hidden dimension.",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "However, we split along time dimension (first introduced in FlowSeq (Ma et al., 2019)) which has the same motivation as the permutation module.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : "We use a multihead selfattention module in transformer (Vaswani et al., 2017) or alternatively RNNs (a one-layer bidirectional LSTM (Schuster and Paliwal, 1997)) in the coupling layer by replacing the non-linear mapping of affine coupling, NN (see Eq.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : ", 2017) or alternatively RNNs (a one-layer bidirectional LSTM (Schuster and Paliwal, 1997)) in the coupling layer by replacing the non-linear mapping of affine coupling, NN (see Eq.",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "We implement it through GloVe embeddings (Pennington et al., 2014).",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Past works (Dinh et al., 2017; Kingma and Dhariwal, 2018; Ma et al., 2019) reshape in this manner for all blocks.",
      "startOffset" : 11,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Past works (Dinh et al., 2017; Kingma and Dhariwal, 2018; Ma et al., 2019) reshape in this manner for all blocks.",
      "startOffset" : 11,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "Past works (Dinh et al., 2017; Kingma and Dhariwal, 2018; Ma et al., 2019) reshape in this manner for all blocks.",
      "startOffset" : 11,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "Previous works have shown autoregressive generation usually performs better than non-autoregressive generation (Ren et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Similar to FlowSeq (Ma et al., 2019), we use flow as an extra module on top of a typical encoderdecoder language generation model and test on Question Generation (QG) and neural machine translation (NMT).",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "To better utilize the generated data, we design a data filter as filtering out the lowquality generated text is useful in helping improve the data augmentation (Zhang and Bansal, 2019).",
      "startOffset" : 160,
      "endOffset" : 184
    }, {
      "referenceID" : 22,
      "context" : "1 (Rajpurkar et al., 2016) into two halves and show the result on the test split.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "We evaluate QG by BLEU4 (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : ", 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : ", 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation.",
      "startOffset" : 51,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "We compare our flow model to the pure encoder-decoder baseline as well as the FlowSeq model (Ma et al., 2019) in human evaluation.",
      "startOffset" : 92,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "As shown in Table 7, our proposed flow-aided MT model can improve the machine translation performance over the strong transformer baseline on the WMT16 (Cettolo et al., 2012) Romanian to English translation task.",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "Statistical significance is computed using the bootstrap test (Efron and Tibshirani, 1994).",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "Table 5: TVQA-QG Evaluation: comparison between FlowSeq (Ma et al., 2019), a BERT QG baseline, Flow aided QG model (Lang-Flow), and simple density estimation model (3-layer LSTM) aided QG baseline model (LSTM-Flow) on TVQA QG validation split.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "Table 6: SQuAD-QG Evaluation: comparison between FlowSeq (Ma et al., 2019), a BERT QG baseline, Flow aided QG baseline model (Lang-Flow) and simple density estimation model (3-layer LSTM) aided QG baseline model (LSTM-Flow) on the SQuAD-QG test split.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "As shown in Table 8 and Table 9, using the augmented data generated by our Language Flow model (refers to our autoregressive language flow model), we achieve significant performance improvements over strong baselines on both TVQA QA (Lei et al., 2020) (p < 0.",
      "startOffset" : 233,
      "endOffset" : 251
    }, {
      "referenceID" : 17,
      "context" : "while, we compare to two other data augmentation techniques: paraphrasing (Niu and Bansal, 2018) and back-translation (Sennrich et al.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "while, we compare to two other data augmentation techniques: paraphrasing (Niu and Bansal, 2018) and back-translation (Sennrich et al., 2016).",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "It might because the nonautoregressive structure has a weaker ability to model the temporal dependency during generation, which is consistent with the observations from previous works (Ren et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "While variational autoencoder has the issue of ignoring latent space (Li et al., 2019), our models do not suffer from this issue.",
      "startOffset" : 69,
      "endOffset" : 86
    } ],
    "year" : 2021,
    "abstractText" : "Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation. We also apply our framework to Sequence-to-Sequence generation, including textand video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.1",
    "creator" : "LaTeX with hyperref"
  }
}