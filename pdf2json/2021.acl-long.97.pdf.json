{
  "name" : "2021.acl-long.97.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Dataset and Baselines for Multilingual Reply Suggestion",
    "authors" : [ "Mozhi Zhang", "Wei Wang", "Budhaditya Deb", "Guoqing Zheng", "Milad Shokouhi", "Ahmed Hassan Awadallah" ],
    "emails" : [ "mozhi@cs.umd.edu", "wwang@qualtrics.com", "budeb@microsoft.com", "zheng@microsoft.com", "milads@microsoft.com", "hassanam@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1207"
    }, {
      "heading" : "1 Multilingual Reply Suggestion",
      "text" : "Automated reply suggestion is a useful feature for email and chat applications. Given an input message, the system suggests several replies, and users may click on them to save typing time (Figure 1). This feature is available in many applications including Gmail, Outlook, LinkedIn, Facebook Messenger, Microsoft Teams, and Uber.\nReply suggestion is related to but different from open-domain dialog systems or chatbots (Adiwardana et al., 2020; Huang et al., 2020). While both are conversational AI tasks (Gao et al., 2019), the goals are different: reply suggestion systems help the user quickly reply to a message, while chatbots aim to continue the conversation and focus more on multi-turn dialogues.\nIdeally, we want our model to generate replies in any language. However, reply suggestion models require large training sets, so previous work mostly\n∗Work mostly done as an intern at Microsoft Research. †Work done at Microsoft Research.\nfocuses on English (Kannan et al., 2016; Henderson et al., 2017; Deb et al., 2019). To investigate reply suggestion for other languages with possibly limited data, we build a multilingual dataset, dubbed MRS (Multilingual Reply Suggestion). From publicly available Reddit threads, we extract messagereply pairs, response sets, and machine-translated examples in ten languages (Table 1).\nOne interesting aspect of the reply suggestion problem is that there are two modeling approaches. Some models follow the retrieval framework and select the reply from a predetermined response set (Henderson et al., 2017). Others follow the generation framework and generate the reply from scratch (Kannan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output.\nThe two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer\nlearning in generation tasks, the tasks are extractive; i.e., the output often has significant overlap with the input. These tasks include news title generation, text summarization, and question generation (Chi et al., 2020; Liang et al., 2020; Scialom et al., 2020). Reply suggestion is more challenging because the reply often does not overlap with the message (Figure 1), so the model needs to address different cross-lingual generalization challenges (Section 5.2).\nWe build two baselines for MRS: a retrieval model and a generation model. We first compare the models in English, where we have abundant training data and human referees. We evaluate the models with both automatic metrics and human judgments. The two models have different strengths. The generation model has higher word overlap scores and is favored by humans on average, but inference is slower, and the output is sometimes contradictory or repetitive (Holtzman et al., 2020). In contrast, the retrieval model is faster and always produces coherent replies, but the replies are sometimes too generic or irrelevant due to the fixed response set.\nNext, we test models in other languages. We compare different training settings and investigate two cross-lingual generalization methods: initializing with pre-trained multilingual models (Wu and Dredze, 2019; Conneau et al., 2020; Liang et al., 2020) and training on machine-translated data (Banea et al., 2008). Interestingly, the two models prefer different methods: multilingual pretraining works better for the retrieval model, while the generation model prefers machine translation.\nIn summary, we present MRS, a multilingual\nreply suggestion dataset. We use MRS to provide the first systematic comparison between generation and retrieval models for reply suggestion in both monolingual and multilingual settings. MRS is also a useful benchmark for future research in reply suggestion and cross-lingual generalization.\nThe rest of the paper is organized as follows. Section 2 describes the data collection process for MRS. Section 3 introduces task formulations, experiment settings, and evaluation metrics. Section 4 describes the baseline generation and retrieval models. Section 5 presents our experiment results. Section 6 discusses how MRS can help future research."
    }, {
      "heading" : "2 Dataset Construction",
      "text" : "To study reply suggestion in multiple languages, we build MRS, a dataset with message-reply pairs based on Reddit comments. The dataset is available at https://github.com/zhangmozhi/mrs.\nWe download Reddit comments between January 2010 and December 2019 from the Pushshift Reddit dataset (Baumgartner et al., 2020).1 We extract message-reply pairs from each thread by considering the parent comment as an input message and the response to the comment as the reference reply. We remove comments starting with [removed] or [deleted], which are deleted messages. We also skip comments with a rating of less than one, since they are likely to contain inappropriate content.\nAfter extracting examples, we identify their languages with fastText language detector (Joulin et al., 2016). For each example, we run the model\n1https://files.pushshift.io/reddit/ comments\non the concatenation of the message and the reply. We discard low-confidence examples where none of the languages has a score higher than 0.7. For the remaining examples, we use the highest-scoring label as the language.\nWe only use English data from 2018 because English data is abundant on Reddit. Non-English examples are much more scarce, so we use data from the last ten years. We select the top ten languages with at least 100K examples. We create three splits for each language: 80% examples for training, 10% for validation, and 10% for testing.\nTable 1 shows some dataset statistics. MRS is heavily biased towards English. We have more than 48 million English examples, but fewer than one million examples for half of the languages. This gap reflects a practical challenge for reply suggestion—we do not have enough data for most languages in the world. Nevertheless, we can use MRS to test models in different multilingual settings, including cross-lingual transfer learning, where we build non-English reply suggestion models from English data (Section 3.2).\nWe also build response sets and filter out toxic examples. We describe these steps next."
    }, {
      "heading" : "2.1 Response Set",
      "text" : "We build a response set of 30K to 50K most frequent replies for each language, which are used in the retrieval model. We want the response set to cover generic responses, so we select replies that appear at least twenty times in the dataset. This simple criterion works well for English, but the set is too small for other languages. For non-English languages, we augment the response set by translating the English response set to other languages with Microsoft Translator. The non-English response set is sometimes smaller than the English set, because different English responses may have the same translation."
    }, {
      "heading" : "2.2 Filtering Toxic Examples",
      "text" : "Exchanges on Reddit are sometimes uncivil, inappropriate, or even abusive (Massanari, 2017; Mohan et al., 2017). We try to filter out toxic contents, as they are not desirable for reply suggestion systems.\nWe use two toxicity detection models. First, we use an in-house multilingual model. The model is initialized with multilingual BERT (Devlin et al., 2019, MBERT) and fine-tuned on a mixture of proprietary and public datasets with toxic and offen-\nsive language labels. The model outputs a score from zero to one, with a higher score corresponding to a higher level of toxicity. Second, we use Perspective API2, a publicly available model. Perspective API has limited free access (one query per second), so we only use the API on the English validation, test, and response set. For other languages, we rely on our in-house model. We filter message-reply pairs if it has greater than 0.9 score according to the in-house model, or greater than 0.5 score according to Perspective API (Gehman et al., 2020). About one percent of examples are filtered. After filtering the data, we manually validate three hundred random examples and do not find any toxic examples, which confirms that our filter method have a high recall.\nWhile we hope the filtered dataset leads to better reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). Therefore, models trained on all MRS data may still have undesirable behavior. MRS is intended to be used as a benchmark for testing cross-lingual generalization of generation and retrieval models. The dataset should not be directly used in production systems. To use the dataset in practice, additional work is required to address other possible biases and toxic or inappropriate content that may exist in the data."
    }, {
      "heading" : "3 Experiment Settings",
      "text" : "After presenting the dataset, we explain how we use MRS to compare reply suggestion models. We describe the two frameworks for reply suggestion, our experiment settings, and evaluation metrics."
    }, {
      "heading" : "3.1 Task Formulation",
      "text" : "In reply suggestion, the input is a message x, and the output is one or more suggested replies y. In practice, reply suggestion systems can choose to not suggest any replies. This decision is usually made by a separate trigger model (Kannan et al., 2016). In this paper, we focus on reply generation, so we assume that the models always need to suggest a fixed number of replies. Reply suggestion can be formulated as either a retrieval problem or a generation problem.\nRetrieval Model. A retrieval model selects the reply y from a fixed response set Y (Section 2.1).\n2https://www.perspectiveapi.com\nGiven an input message x, the model computes a relevance score Θxy for each candidate reply y ∈ Y . The model then selects the highestscoring replies as suggestions; e.g., the top-1 reply is arg maxy∈Y Θxy.\nGeneration Model. A generation model generates the reply y from scratch. Generation models usually follow the sequence-to-sequence framework (Sutskever et al., 2014, SEQ2SEQ), which generates y token by token. Given an input message x = (x1, x2, · · · , xn) of n tokens, a SEQ2SEQ model estimates the probability of a reply y = (y1, y2, · · · , ym) of m tokens as following:\np(y |x) = m∏ i=1 p(yi |x, y<i). (1)\nThe model computes probability for the next token p(yi |x, y<i) based on the input x and the first (i− 1) tokens of the output y. The model is trained to maximize the probability of reference replies in the training set. At test time, we find the top replies that approximately maximize (1) with beam search.\nThe two models have different strengths. The generation model is more flexible, but the retrieval model is faster (Henderson et al., 2017), and the output can be controlled by curating the response set (Kannan et al., 2016).\nWe compare a retrieval model and a generation model as baselines for MRS. To our knowledge, we are the first to systematically compare the two models in both monolingual and multilingual settings. We explain our training settings and metrics next."
    }, {
      "heading" : "3.2 Training Settings",
      "text" : "For each language in MRS, we train and compare models in four settings. Future work can experiment with other settings (discussed in Section 6).\nMonolingual. Here, we simply train and test models in a single language. This setting simulates the scenario where we have adequate training data for the target language. Previous reply suggestion models were only studied in the English monolingual setting.\nZero-Shot. Next, we train models in a zero-shot cross-lingual setting. We train the model on the English training set and use the model on the test set for another language. This setting simulates the scenario where we want to build models for a low-resource language using our large English set.\nTo generalize across languages, we initialize the models with pre-trained multilingual models (details in Section 4). These models work well in other tasks (Wu and Dredze, 2019; Liang et al., 2020). We test if they also work for reply suggestion, as different tasks often prefer different multilingual representations (Zhang et al., 2020b).\nMachine Translation (MT). Another strategy for cross-lingual generalization is to train on machine-translated data (Banea et al., 2008). We train models on nineteen million English training examples machine-translated to the target language with Microsoft Translator. We compare against the zero-shot setting to compare the two cross-lingual generalization strategies.\nMultilingual. Finally, we build a multilingual model by jointly training on the five languages with the most training data: English, Spanish, German, Portuguese, and French. We oversample nonEnglish training data to have the same number of training examples data across all languages (Johnson et al., 2017). We make two comparisons: 1) for the five training languages, we compare against the monolingual setting to test whether fitting multiple languages in a single model hurts performance; and 2) for other languages, we compare against the zero-shot setting to check if adding more training languages helps cross-lingual generalization."
    }, {
      "heading" : "3.3 Evaluation Metrics",
      "text" : "The goal of reply suggestion is to save user typing time, so the ideal metrics are click-through rate (CTR), how often the user chooses a suggested reply, and time reduction, how much time is saved by clicking the suggestion instead of typing. However, these metrics require deploying the model to test on real users, which is not feasible at full-scale while writing this paper. Instead, we focus on automated offline metrics that can guide research and model development before deploying production systems. Specifically, we evaluate models using a test set of message-reply pairs.\nTo identify a good metric, we compare several metrics in a pilot study by deploying an English system. We collect millions of user interactions and measure Pearson’s correlation between CTR and automated offline metrics. The next paragraph lists the metrics. Based on the study, we recommend weighted ROUGE F1 ensemble (ROUGE in tables), which has the highest correlation with CTR.\nFor the retrieval model, we follow previous work and consider mean reciprocal rank (Kannan et al., 2016, MRR) and precision at one (Henderson et al., 2017). These metrics test if the model can retrieve the reference response from a random set of responses. Alternatively, we compute MRR and precision on a subset of examples where the reference reply is in the response set so that we can directly measure the rank of the reference response in the response set. This set also allows us to compute MRR for individual responses, so we can compute macro-MRR, the average MRR over each response in the set. Higher macro-MRR can indicate diversity but has a worse correlation than computing MRR over the entire test set. For the generation model, we consider model perplexity (Adiwardana et al., 2020). Finally, we consider two word overlap scores, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which can be used for both retrieval and generation models.\nOur pilot study shows that ROUGE has the best correlation. However, individual ROUGE F1 scores (ROUGE-1/2/3) are sensitive to small changes in sequence lengths (more so because our responses are generally short). Therefore, we use a weighted average of the three scores:\nROUGE-1 6 + ROUGE-2 3 + ROUGE-3 2 . (2)\nThis weighted score leads to the highest correlation with CTR. Intuitively, the weights balance the differences in the average magnitude of each metric and thus reduce variance on short responses.\nPopular reply suggestion systems (such as Gmail and Outlook) suggest three replies for each message, while the user only selects one. To simulate this setting, we predict three replies for each message. For the retrieval model, we use the three highest-scoring replies from the response set. For the generation model, we use top-three results from beam search. Out of the three replies, we only use the reply with the highest ROUGE compared to the reference reply when computing the final metrics; i.e., the model only has to provide one “correct” reply to have a full score.\nWe compare models primarily with ROUGE, since the metric has the best correlation in the pilot study. Nevertheless, word overlap scores have known limitations (Liu et al., 2016), as there are different ways to reply to a message. We encourage future research to investigate other metrics to understand different aspects of the model.\nAs examples, we also report two diversity scores: the proportion of distinct unigrams (Dist-1) and bigrams (Dist-2) in the generated replies (Li et al., 2016). While ROUGE measures the relevance of the replies, higher diversity can also increase CTR (Deb et al., 2019). We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models (Deb et al., 2019), but we leave this direction to future work.\nFor our English monolingual experiments, we also complement automatic metrics with human judgments (Human in Figure 2). For each example, we display the input message and sets of three suggested replies from both generation and retrieval models to three human annotators (crowd workers). We then ask the annotators to select the set with more responses that they prefer to send as a reply. We leave evaluations for other languages to future work due to resource limitations."
    }, {
      "heading" : "4 Baseline Models",
      "text" : "This section introduces the two baseline models: a retrieval model and a generation model."
    }, {
      "heading" : "4.1 Retrieval Model",
      "text" : "For the retrieval model, we use the architecture from Henderson et al. (2017), except we replace the feedforward network encoders with Transformers (Vaswani et al., 2017). Given an input message x and candidate reply y, two Transformer encoders Φx and Φy map the message and the reply to two vectors Φx(x) and Φy(y). The relevance score Θxy between the message x and the reply y is the dot product of the two vectors:\nΘxy = Φx(x) >Φy(y). (3)\nHenderson et al. (2017) also adds a language model score to encourage more frequent replies. We do not use language model score for simplicity.\nWe train the model with the symmetric loss from Deb et al. (2019). Suppose the batch size is n. For a batch of training messages {xi}ni=1 and corresponding replies {yi}nj=1, we maximize:\nn∑ i=1 eΘxiyi∑n j=1 ( eΘxiyj + eΘxjyi ) − eΘxiyi . (4)\nIn a regular softmax loss, the denominator only sums over one variable. The denominator in the\nsymmetric loss sum over both variables to encourage bidirectional compatibility: the message should be predictive of the reply, and the reply should be predictive of the message. This encourages the model to select responses specific to the message, similar to the Maximum Mutual Information objective from Li et al. (2016).\nThe two encoders Φx and Φy are initialized with MBERT (Devlin et al., 2019), a Transformer with 110 million parameters pre-trained on multilingual corpora. Initializing with MBERT allows the model to generalize across languages (Wu and Dredze, 2019). In Appendix A, we experiment with another pre-trained multilingual Transformer, XLM-R (Conneau et al., 2020). We use the “base” version with 270 million parameters."
    }, {
      "heading" : "4.2 Generation Model",
      "text" : "For the generation model, we follow the SEQ2SEQ architecture (Section 3.1). We use a Transformer encoder to read the input x, and another Transformer decoder to estimate p(yi |x, y<i) in (1).\nWe cannot initialize the generation model with MBERT or XLM-R, because the model also has a decoder. Instead, we use Unicoder-XDAE (Liang et al., 2020), a pre-trained multilingual SEQ2SEQ model, which can generalize across languages in extractive generation tasks such as news title generation and question generation. We test if UnicoderXDAE also generalizes in the more challenging reply suggestion task. There are other generation models we can use, which we discuss as future work in Section 6."
    }, {
      "heading" : "4.3 Training Details",
      "text" : "We train the retrieval model using Adam optimizer (Kingma and Ba, 2015) with 1e-6 learning rate, default β, and 256 batch size. For monolingual and zero-shot settings, we use twenty epochs for English and fifty epochs for other languages. We use ten epochs for MT and multilingual settings. The first 1% training steps are warmup steps. During training, we freeze the embedding layers and the bottom two Transformer layers of both en-\nGray cells indicate when the model is trained on the target language training set. White cells indicate crosslingual settings where the target language training set is not used for training. For each language, we boldface the best ROUGE scores in cross-lingual settings (white cells). The zero-shot setting has better ROUGE scores than using MT data for most languages, and the results are sometimes close to monolingual training, confirming the effectiveness of MBERT. Multilingual training hurts training languages (gray cells compared to monolingual) but sometimes improves cross-lingual generalization (white cells compared to zero-shot).\ncoders, which preserves multilingual knowledge from the pre-trained model and improves crosslingual transfer learning (Wu and Dredze, 2019). All hyperparameters are manually tuned on the English validation set.\nWe use almost the same hyperparameters as Liang et al. (2020) to train generation models. Specifically, we use Adam optimizer with 1e-5 initial learning rate, default β, and 1024 batch size. For the monolingual and zero-shot setting, we use four epochs for English and 5000 steps for other languages (equivalent to two to nine epochs depending on the language). We use one epoch for the MT setting and 40,000 steps for the multilingual setting. The first 20% training steps are warmup steps. We freeze the embedding layer during training for faster training.\nAll models are trained with eight Tesla V100 GPU. It takes about an hour to train the generation model for 1000 steps (covering about one million examples). For the retrieval model, an epoch on the English training set (about 48 million examples) takes about seven hours."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "We experiment with the two baselines from Section 4 on MRS. We first compare the models in English, where we have enough training data and human referees. We then build models for other\nlanguages and compare training settings listed in Section 3.2."
    }, {
      "heading" : "5.1 Results on English",
      "text" : "Figure 2 compares the generation and retrieval models in the English monolingual setting. Generation model not only has higher relevance (ROUGE) score but also can generate more diverse replies (higher DIST scores). For English, we also ask three human referees to compare the model outputs on a subset of 500 test examples. Again, the referees prefer the generation model more often than the retrieval model (Figure 2).\nWe look at some generated responses to understand the models qualitatively. In the top two examples in Table 2, the generation model produces replies highly specific to the input message. In contrast, the retrieval model fails to find a relevant reply, because the response set does not cover these topics. This explains why the generation model has much higher ROUGE and distinct n-gram scores than the retrieval model.\nHowever, the expressiveness comes at the cost of a lack of control over the generated replies. The generation model sometimes produces incoherent replies that are repetitive and/or contradictory, as shown in the bottom two examples of Table 2. For the retrieval model, we can easily avoid these problems by curating the fixed response set. These degenerative behaviors are observed in other text\ngeneration tasks and can be mitigated by changing training and decoding objectives (Holtzman et al., 2020; Welleck et al., 2020). We leave these directions for future research."
    }, {
      "heading" : "5.2 Results on Other Languages",
      "text" : "After comparing English models, we experiment on other languages using the settings from Section 3.2.\nRetrieval Model. Table 3 shows results for the retrieval model when initialized with MBERT. The retrieval model can generalize fairly well across languages, as the ROUGE in the zero-shot setting is often close to the monolingual setting. This result confirms that initializing with MBERT is an effective strategy for cross-lingual generalization. Training on MT data is usually worse than training in the zero-shot setting. This is possible because the MT system may create artifacts that do not appear in organic data (Artetxe et al., 2020). For the multilingual model, the training language ROUGE scores are lower than monolingual training (gray cells in Table 3). However, multilingual training sometimes leads to better ROUGE on unseen languages compared to transferring from only English (zero-shot). Previous work observes similar results on other tasks, where multilingual training hurts training languages but helps generalization to unseen languages (Johnson et al., 2017; Con-\nneau et al., 2020; Wang et al., 2020). Finally, Appendix A shows similar results when initializing with XLM-R (Conneau et al., 2020).\nGeneration Model. Table 4 shows results for the generation model. In the monolingual setting, the generation model has higher scores than the retrieval model on most languages, consistent with the English result (Figure 2). However, unlike the retrieval model, the generation model fails to generalize across languages in the zero-shot setting, despite using Unicoder-XDAE for initialization. We do not show zero-shot results in Table 4, because ROUGE are close to zero for non-English languages. After training on English data, the model always produces English replies, regardless of the input language; i.e., the generation model “forgets” multilingual knowledge acquired during pre-training (Kirkpatrick et al., 2017). This result is surprising because Unicoder-XDAE works in the zero-shot setting for other generation tasks (Liang et al., 2020), which suggests that reply suggestion poses unique challenges for cross-lingual transfer learning. Interestingly, the multilingual model can generalize to unseen languages; perhaps training on multiple languages regularizes the model to produce replies in the input language. Overall, the best method to generalize the generation model across languages is to use machine-translated data."
    }, {
      "heading" : "6 Future Work",
      "text" : "MRS opens up opportunities for future research. Our experiments use four training settings (Section 3.2), but there are many other settings to explore. For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a). We are also interested in training on both organic data and MT data; i.e., mixing the zero-shot and MT setting.\nWe can also compare other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ2SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020).\nAnother idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017).\nOur experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present MRS, a multilingual dataset for reply suggestion. We compare a generation and a retrieval baseline on MRS. The two models have dif-\nferent strengths in the English monolingual setting and require different strategies to transfer across languages. MRS provides a benchmark for future research in both reply suggestion and cross-lingual transfer learning.\nEthical Considerations\nData Collection. No human annotators are involved while creating MRS. The examples and response sets of MRS come from publicly available Reddit dumps from Pushshift, which are used in more than a hundred peer-reviewed publications (Baumgartner et al., 2020).\nPrivacy. Examples in MRS do not have the username and are from publicly available data. Therefore, we do not anticipate any privacy issues. In the pilot study (Section 3.3), we measure the correlation of user CTR with different evaluation metrics. To protect user privacy, we only collect aggregated statistics (CTR) and use no other information.\nPotential Biased and Toxic Content. Despite our best effort to filter toxic contents (Section 2.2), the dataset may not be perfectly cleansed and may have other biases that are typical in open forums (Massanari, 2017; Mohan et al., 2017). Users should be aware of these issues. We will continue to improve the quality of the dataset.\nIntended Use of MRS. Because of the possible biases and inappropriateness in the data, MRS should not be directly used to build production systems (as mentioned in Section 2.2). The main use of MRS is to test cross-lingual generalization for text retrieval and generation models, and researchers should be aware of possible ethical issues of Reddit data before using MRS."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We appreciate the feedback from anonymous reviewers. MZ is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the BETTER Program contract #2019- 19051600005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
    }, {
      "heading" : "A Results for XLM-R",
      "text" : "tion 3.2. Gray cells indicate when the model is trained on the target language training set. White cells indicate cross-lingual settings where the target language training set is not used for training. For each language, we boldface the best ROUGE scores in cross-lingual settings (white cells). We observe similar trends as MBERT (Table 3)."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le." ],
      "venue" : "arXiv preprint arXiv:2001.09977.",
      "citeRegEx" : "Adiwardana et al\\.,? 2020",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing",
      "author" : [ "Wasi Ahmad", "Zhisong Zhang", "Xuezhe Ma", "Eduard Hovy", "Kai-Wei Chang", "Nanyun Peng." ],
      "venue" : "Conference of the North American Chapter of the",
      "citeRegEx" : "Ahmad et al\\.,? 2019",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2019
    }, {
      "title" : "Many languages, one parser",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:431–444.",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Translation artifacts in cross-lingual transfer learning",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Multilingual subjectivity analysis using machine translation",
      "author" : [ "Carmen Banea", "Rada Mihalcea", "Janyce Wiebe", "Samer Hassan." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Banea et al\\.,? 2008",
      "shortCiteRegEx" : "Banea et al\\.",
      "year" : 2008
    }, {
      "title" : "The Pushshift Reddit dataset",
      "author" : [ "Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn." ],
      "venue" : "International Conference on Weblogs and Social Media.",
      "citeRegEx" : "Baumgartner et al\\.,? 2020",
      "shortCiteRegEx" : "Baumgartner et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-source cross-lingual model transfer: Learning what to share",
      "author" : [ "Xilun Chen", "Ahmed Hassan Awadallah", "Hany Hassan", "Wei Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial deep averaging networks for cross-lingual sentiment classification",
      "author" : [ "Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:557–570.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual natural language generation via pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Wenhui Wang", "XianLing Mao", "Heyan Huang." ],
      "venue" : "Association for the Advancement of Artificial Intelligence.",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "Conference of the",
      "citeRegEx" : "Chi et al\\.,? 2021",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning cross-lingual sentence representations via a multi-task dual-encoder model",
      "author" : [ "Muthu Chidambaram", "Yinfei Yang", "Daniel Cer", "Steve Yuan", "Yunhsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of ACL Workshop on Representation",
      "citeRegEx" : "Chidambaram et al\\.,? 2019",
      "shortCiteRegEx" : "Chidambaram et al\\.",
      "year" : 2019
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Discriminative reranking for natural language parsing",
      "author" : [ "Michael Collins", "Terry Koo." ],
      "venue" : "Computational Linguistics, 31(1):25–70.",
      "citeRegEx" : "Collins and Koo.,? 2005",
      "shortCiteRegEx" : "Collins and Koo.",
      "year" : 2005
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Ruty Rinott", "Adina Williams", "Samuel R Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Crosslingual character-level neural morphological tagging",
      "author" : [ "Ryan Cotterell", "Georg Heigold." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Cotterell and Heigold.,? 2017",
      "shortCiteRegEx" : "Cotterell and Heigold.",
      "year" : 2017
    }, {
      "title" : "Diversifying reply suggestions using a matching-conditional variational autoencoder",
      "author" : [ "Budhaditya Deb", "Peter Bailey", "Milad Shokouhi." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics (Industry",
      "citeRegEx" : "Deb et al\\.,? 2019",
      "shortCiteRegEx" : "Deb et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of AAAI/ACM Conference on AI, Ethics, and Society.",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Languageagnostic BERT sentence embedding",
      "author" : [ "Fangxiaoyu Feng", "Yinfei Yang", "Daniel Cer", "Naveen Arivazhagan", "Wei Wang." ],
      "venue" : "arXiv preprint arXiv:2007.01852.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural approaches to conversational AI",
      "author" : [ "Jianfeng Gao", "Michel Galley", "Lihong Li" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "Gao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Discriminative reranking for semantic parsing",
      "author" : [ "Ruifang Ge", "Raymond J. Mooney." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Ge and Mooney.,? 2006",
      "shortCiteRegEx" : "Ge and Mooney.",
      "year" : 2006
    }, {
      "title" : "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP. Asso-",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient natural language response suggestion for Smart Reply",
      "author" : [ "Matthew Henderson", "Rami Al-Rfou", "Brian Strope", "YunHsuan Sung", "László Lukács", "Ruiqi Guo", "Sanjiv Kumar", "Balint Miklos", "Ray Kurzweil." ],
      "venue" : "arXiv preprint arXiv:1705.00652.",
      "citeRegEx" : "Henderson et al\\.,? 2017",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2017
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the International Confer-",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual multi-level adversarial transfer to enhance low-resource name tagging",
      "author" : [ "Lifu Huang", "Heng Ji", "Jonathan May." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Challenges in building intelligent open-domain dialog systems",
      "author" : [ "Minlie Huang", "Xiaoyan Zhu", "Jianfeng Gao." ],
      "venue" : "ACM Transactions on Information Systems, 38(3):1–32.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Social biases in NLP models as barriers for persons with disabilities",
      "author" : [ "Ben Hutchinson", "Vinodkumar Prabhakaran", "Emily Denton", "Kellie Webster", "Yu Zhong", "Stephen Denuyl." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Hutchinson et al\\.,? 2020",
      "shortCiteRegEx" : "Hutchinson et al\\.",
      "year" : 2020
    }, {
      "title" : "FastText.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Hérve Jégou", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Joulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Smart Reply: Automated response suggestion for email",
      "author" : [ "Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "Laszlo Lukacs", "Marina Ganea", "Peter Young" ],
      "venue" : null,
      "citeRegEx" : "Kannan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Pre-training via paraphrasing",
      "author" : [ "Mike Lewis", "Marjan Ghazvininejad", "Gargi Ghosh", "Armen Aghajanyan", "Sida Wang", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Lewis et al\\.,? 2020a",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Lewis et al\\.,? 2020b",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "Wu", "Shuguang Liu", "Fan Yang", "Daniel Campos", "Rangan Majumder", "Ming Zhou." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Choosing transfer languages for cross-lingual",
      "author" : [ "Yu-Hsiang Lin", "Chian-Yu Chen", "Jean Lee", "Zirui Li", "Yuyan Zhang", "Mengzhou Xia", "Shruti Rijhwani", "Junxian He", "Zhisong Zhang", "Xuezhe Ma", "Antonios Anastasopoulos", "Patrick Littell", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "Proceedings of Em-",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Gamergate and The Fappening: How Reddit’s algorithm, governance, and culture support toxic technocultures",
      "author" : [ "Adrienne Massanari." ],
      "venue" : "New Media & Society, 19(3):329–346.",
      "citeRegEx" : "Massanari.,? 2017",
      "shortCiteRegEx" : "Massanari.",
      "year" : 2017
    }, {
      "title" : "The impact of toxic language on the health of Reddit communities",
      "author" : [ "Shruthi Mohan", "Apala Guha", "Michael Harris", "Fred Popowich", "Ashley Schuster", "Chris Priebe." ],
      "venue" : "Canadian Conference on Artificial Intelligence.",
      "citeRegEx" : "Mohan et al\\.,? 2017",
      "shortCiteRegEx" : "Mohan et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal Dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajič", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "English intermediate-task training improves zero-shot cross-lingual transfer too",
      "author" : [ "Jason Phang", "Iacer Calixto", "Phu Mon Htut", "Yada Pruksachatkun", "Haokun Liu", "Clara Vania", "Katharina Kann", "Samuel R. Bowman." ],
      "venue" : "Conference of the",
      "citeRegEx" : "Phang et al\\.,? 2020",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2020
    }, {
      "title" : "AliMe chat: A sequence to sequence and rerank based chatbot engine",
      "author" : [ "Minghui Qiu", "Feng-Lin Li", "Siyu Wang", "Xing Gao", "Yan Chen", "Weipeng Zhao", "Haiqing Chen", "Jun Huang", "Wei Chu." ],
      "venue" : "Proceedings of the Association for Computational Lin-",
      "citeRegEx" : "Qiu et al\\.,? 2017",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2017
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "A corpus for multilingual document classification in eight languages",
      "author" : [ "Holger Schwenk", "Xian Li." ],
      "venue" : "Proceedings of the Language Resources and Evaluation Conference.",
      "citeRegEx" : "Schwenk and Li.,? 2018",
      "shortCiteRegEx" : "Schwenk and Li.",
      "year" : 2018
    }, {
      "title" : "MLSUM: The multilingual summarization corpus",
      "author" : [ "Thomas Scialom", "Paul-Alexis Dray", "Sylvain Lamprier", "Benjamin Piwowarski", "Jacopo Staiano." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Scialom et al\\.,? 2020",
      "shortCiteRegEx" : "Scialom et al\\.",
      "year" : 2020
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "LORELEI language packs: Data, tools, and resources for technology development in low resource languages",
      "author" : [ "Stephanie Strassel", "Jennifer Tracey." ],
      "venue" : "Proceedings of the Language Resources and Evaluation Conference.",
      "citeRegEx" : "Strassel and Tracey.,? 2016",
      "shortCiteRegEx" : "Strassel and Tracey.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "Conference on Computational Natural Language Learning.",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Cross-lingual retrieval for iterative self-supervised training",
      "author" : [ "Chau Tran", "Yuqing Tang", "Xian Li", "Jiatao Gu." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra." ],
      "venue" : "Association for the Advancement",
      "citeRegEx" : "Vijayakumar et al\\.,? 2018",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2018
    }, {
      "title" : "On negative interference in multilingual models: Findings and a meta-learning treatment",
      "author" : [ "Zirui Wang", "Zachary C. Lipton", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing, Online. Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Welleck et al\\.,? 2020",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "MetaXL: Meta representation transformation for low-resource cross-lingual learning",
      "author" : [ "Mengzhou Xia", "Guoqing Zheng", "Subhabrata Mukherjee", "Milad Shokouhi", "Graham Newbig", "Ahmed Hassan Awadallah." ],
      "venue" : "Conference of the North",
      "citeRegEx" : "Xia et al\\.,? 2021",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2021
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "arXiv preprint arXiv:2010.11934.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Interactive refinement of cross-lingual word embeddings",
      "author" : [ "Michelle Yuan", "Mozhi Zhang", "Benjamin Van Durme", "Leah Findlater", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting cross-lingual subword similarities in low-resource document classification",
      "author" : [ "Mozhi Zhang", "Yoshinari Fujinuma", "Jordan BoydGraber." ],
      "venue" : "Association for the Advancement of Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Why overfitting isn’t always bad: Retrofitting cross-lingual word embeddings to dictionaries",
      "author" : [ "Mozhi Zhang", "Yoshinari Fujinuma", "Michael J. Paul", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating informative and diverse conversational responses via adversarial information maximization",
      "author" : [ "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan." ],
      "venue" : "Proceedings of Advances in Neural Information",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "DialoGPT: Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the Association for",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "open-domain dialog systems or chatbots (Adiwardana et al., 2020; Huang et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 84
    }, {
      "referenceID" : 29,
      "context" : "open-domain dialog systems or chatbots (Adiwardana et al., 2020; Huang et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "While both are conversational AI tasks (Gao et al., 2019), the goals are different: reply suggestion systems help the user quickly reply to a message, while chatbots aim to continue the conversation and focus more on multi-turn dialogues.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "Some models follow the retrieval framework and select the reply from a predetermined response set (Henderson et al., 2017).",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 32,
      "context" : "generation framework and generate the reply from scratch (Kannan et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 45,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 56,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 16,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 53,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 13,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 27,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 36,
      "context" : "Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 92,
      "endOffset" : 263
    }, {
      "referenceID" : 10,
      "context" : "These tasks include news title generation, text summarization, and question generation (Chi et al., 2020; Liang et al., 2020; Scialom et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 147
    }, {
      "referenceID" : 54,
      "context" : "These tasks include news title generation, text summarization, and question generation (Chi et al., 2020; Liang et al., 2020; Scialom et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "The generation model has higher word overlap scores and is favored by humans on average, but inference is slower, and the output is sometimes contradictory or repetitive (Holtzman et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 64,
      "context" : "We compare different training settings and investigate two cross-lingual generalization methods: initializing with pre-trained multilingual models (Wu and Dredze, 2019; Conneau et al., 2020; Liang et al., 2020) and training on machine-translated data (Banea et al.",
      "startOffset" : 147,
      "endOffset" : 210
    }, {
      "referenceID" : 15,
      "context" : "We compare different training settings and investigate two cross-lingual generalization methods: initializing with pre-trained multilingual models (Wu and Dredze, 2019; Conneau et al., 2020; Liang et al., 2020) and training on machine-translated data (Banea et al.",
      "startOffset" : 147,
      "endOffset" : 210
    }, {
      "referenceID" : 5,
      "context" : ", 2020) and training on machine-translated data (Banea et al., 2008).",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "We download Reddit comments between January 2010 and December 2019 from the Pushshift Reddit dataset (Baumgartner et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 31,
      "context" : "After extracting examples, we identify their languages with fastText language detector (Joulin et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 43,
      "context" : "Exchanges on Reddit are sometimes uncivil, inappropriate, or even abusive (Massanari, 2017; Mohan et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 111
    }, {
      "referenceID" : 44,
      "context" : "Exchanges on Reddit are sometimes uncivil, inappropriate, or even abusive (Massanari, 2017; Mohan et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 111
    }, {
      "referenceID" : 24,
      "context" : "5 score according to Perspective API (Gehman et al., 2020).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 162
    }, {
      "referenceID" : 52,
      "context" : "reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 162
    }, {
      "referenceID" : 30,
      "context" : "reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 162
    }, {
      "referenceID" : 32,
      "context" : "This decision is usually made by a separate trigger model (Kannan et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : "generation model is more flexible, but the retrieval model is faster (Henderson et al., 2017), and the output can be controlled by curating the response set (Kannan et al.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 32,
      "context" : ", 2017), and the output can be controlled by curating the response set (Kannan et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 64,
      "context" : "These models work well in other tasks (Wu and Dredze, 2019; Liang et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 69,
      "context" : "We test if they also work for reply suggestion, as different tasks often prefer different multilingual representations (Zhang et al., 2020b).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "Another strategy for cross-lingual generalization is to train on machine-translated data (Banea et al., 2008).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : ", 2016, MRR) and precision at one (Henderson et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "For the generation model, we consider model perplexity (Adiwardana et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 46,
      "context" : "overlap scores, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which can be used for both retrieval and generation models.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 39,
      "context" : ", 2002) and ROUGE (Lin, 2004), which can be used for both retrieval and generation models.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 41,
      "context" : "Nevertheless, word overlap scores have known limitations (Liu et al., 2016), as there are different ways to reply to a message.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 37,
      "context" : "As examples, we also report two diversity scores: the proportion of distinct unigrams (Dist-1) and bigrams (Dist-2) in the generated replies (Li et al., 2016).",
      "startOffset" : 141,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "While ROUGE measures the relevance of the replies, higher diversity can also increase CTR (Deb et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 37,
      "context" : "We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models (Deb et al.",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 61,
      "context" : "We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models (Deb et al.",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 70,
      "context" : "We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models (Deb et al.",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : ", 2018) or latent variable models (Deb et al., 2019), but we leave this direction to future work.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 19,
      "context" : "The two encoders Φx and Φy are initialized with MBERT (Devlin et al., 2019), a Transformer with 110 million parameters pre-trained on multilingual corpora.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 64,
      "context" : "Initializing with MBERT allows the model to generalize across languages (Wu and Dredze, 2019).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "In Appendix A, we experiment with another pre-trained multilingual Transformer, XLM-R (Conneau et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 33,
      "context" : "We train the retrieval model using Adam optimizer (Kingma and Ba, 2015) with 1e-6 learning rate, default β, and 256 batch size.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "Table 3: Results for retrieval model initialized with MBERT (Devlin et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 64,
      "context" : "coders, which preserves multilingual knowledge from the pre-trained model and improves crosslingual transfer learning (Wu and Dredze, 2019).",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 26,
      "context" : "generation tasks and can be mitigated by changing training and decoding objectives (Holtzman et al., 2020; Welleck et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 128
    }, {
      "referenceID" : 63,
      "context" : "generation tasks and can be mitigated by changing training and decoding objectives (Holtzman et al., 2020; Welleck et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "This is possible because the MT system may create artifacts that do not appear in organic data (Artetxe et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Previous work observes similar results on other tasks, where multilingual training hurts training languages but helps generalization to unseen languages (Johnson et al., 2017; Conneau et al., 2020; Wang et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 216
    }, {
      "referenceID" : 62,
      "context" : "Previous work observes similar results on other tasks, where multilingual training hurts training languages but helps generalization to unseen languages (Johnson et al., 2017; Conneau et al., 2020; Wang et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "Finally, Appendix A shows similar results when initializing with XLM-R (Conneau et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 34,
      "context" : "“forgets” multilingual knowledge acquired during pre-training (Kirkpatrick et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 114,
      "endOffset" : 222
    }, {
      "referenceID" : 17,
      "context" : "For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 114,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 114,
      "endOffset" : 222
    }, {
      "referenceID" : 40,
      "context" : "For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 114,
      "endOffset" : 222
    }, {
      "referenceID" : 68,
      "context" : "For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 114,
      "endOffset" : 222
    }, {
      "referenceID" : 42,
      "context" : "For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ2SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 227
    }, {
      "referenceID" : 59,
      "context" : "For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ2SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 227
    }, {
      "referenceID" : 35,
      "context" : "For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ2SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 227
    }, {
      "referenceID" : 66,
      "context" : "For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ2SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 211
    }, {
      "referenceID" : 51,
      "context" : "For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 211
    }, {
      "referenceID" : 21,
      "context" : "For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 211
    }, {
      "referenceID" : 55,
      "context" : "Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al.",
      "startOffset" : 89,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al.",
      "startOffset" : 89,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al.",
      "startOffset" : 89,
      "endOffset" : 152
    }, {
      "referenceID" : 49,
      "context" : ", 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al.",
      "startOffset" : 52,
      "endOffset" : 97
    }, {
      "referenceID" : 47,
      "context" : ", 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 65,
      "context" : ", 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 48,
      "context" : ", 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 67,
      "context" : ", 2020), and bringing a human in the loop (Yuan et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "The examples and response sets of MRS come from publicly available Reddit dumps from Pushshift, which are used in more than a hundred peer-reviewed publications (Baumgartner et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 187
    }, {
      "referenceID" : 43,
      "context" : "2), the dataset may not be perfectly cleansed and may have other biases that are typical in open forums (Massanari, 2017; Mohan et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 141
    }, {
      "referenceID" : 44,
      "context" : "2), the dataset may not be perfectly cleansed and may have other biases that are typical in open forums (Massanari, 2017; Mohan et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 141
    } ],
    "year" : 2021,
    "abstractText" : "Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at https://github.com/zhangmozhi/mrs. 1 Multilingual Reply Suggestion Automated reply suggestion is a useful feature for email and chat applications. Given an input message, the system suggests several replies, and users may click on them to save typing time (Figure 1). This feature is available in many applications including Gmail, Outlook, LinkedIn, Facebook Messenger, Microsoft Teams, and Uber. Reply suggestion is related to but different from open-domain dialog systems or chatbots (Adiwardana et al., 2020; Huang et al., 2020). While both are conversational AI tasks (Gao et al., 2019), the goals are different: reply suggestion systems help the user quickly reply to a message, while chatbots aim to continue the conversation and focus more on multi-turn dialogues. Ideally, we want our model to generate replies in any language. However, reply suggestion models require large training sets, so previous work mostly ∗Work mostly done as an intern at Microsoft Research. †Work done at Microsoft Research. Figure 1: An example of reply suggestion system. User can click on the suggestions for a quick reply. focuses on English (Kannan et al., 2016; Henderson et al., 2017; Deb et al., 2019). To investigate reply suggestion for other languages with possibly limited data, we build a multilingual dataset, dubbed MRS (Multilingual Reply Suggestion). From publicly available Reddit threads, we extract messagereply pairs, response sets, and machine-translated examples in ten languages (Table 1). One interesting aspect of the reply suggestion problem is that there are two modeling approaches. Some models follow the retrieval framework and select the reply from a predetermined response set (Henderson et al., 2017). Others follow the generation framework and generate the reply from scratch (Kannan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer",
    "creator" : "LaTeX with hyperref"
  }
}