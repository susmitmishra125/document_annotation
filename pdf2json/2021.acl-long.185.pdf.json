{
  "name" : "2021.acl-long.185.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding",
    "authors" : [ "Dongyeop Kang", "Eduard Hovy" ],
    "emails" : [ "dongyeopk@berkeley.edu", "hovy@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2376–2387\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2376"
    }, {
      "heading" : "1 Introduction",
      "text" : "People often use style as a strategic choice for their personal or social goals in communication (Hovy,\n∗This work was done while DK was at CMU. 1https://github.com/dykang/xslue\n1987; Silverstein, 2003; Jaffe et al., 2009; Kang, 2020). Some stylistic choices implicitly reflect the author’s characteristics, like personality, demographic traits (Kang et al., 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al., 2013). In this work, we broadly call each individual linguistic phenomena as one specific type of style.\nStyle is not a single variable, but multiple variables have their own degrees of freedom and they co-vary together. Imagine an orchestra, as a metaphor of style. What we hear from the orchestra is the harmonized sound of complex combinations of individual instruments played. A conductor, on top of it, controls their combinatory choices among them, such as tempo or score. Some instruments under the same category, such as violin and cello for bowed string type, make a similar pattern of sound. Similarly, text reflects complex combination of multiple styles. Each has its own lexical and syntactic features and some are dependent on each other. Consistent combination of them by the author will produce stylistically appropriate text.\nTo the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preoţiuc-Pietro and Ungar, 2018), across emotions (Warriner et al., 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al., 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016).\nUnlike the prior works, this work proposes the first comprehensive understanding of cross-stylistic language variation, particularly focusing on how different styles co-vary together in written text, which styles are dependent on each other, and how they are systematically composed to generate text.\nOur work has following contributions: • Aggregate 15 different styles and 23 sentence-\nlevel classification tasks (§3). Based on their social goals, the styles are categorized into four groups (Table 1): figurative, affective, personal and interpersonal. • Collect a cross-style set by annotating 15 styles on the same text for valid evaluation of crossstylistic variation (§3.3). • Study cross-style variations in classification (§4), correlation (§5), and generation (§6): – our jointly trained classifier on multiple styles\nshows better performance than individuallytrained classifiers. – our correlation study finds statistically significant style inter-dependencies (e.g., impoliteness and offense) in written text. – our conditional stylistic generator shows that better style classifier enables stylistically better generation. Also, some styles (e.g., impoliteness and positive sentiment) are condtradictive in generation."
    }, {
      "heading" : "2 Related Work",
      "text" : "Definition of style. People may have different definitions in what they call ‘style’. Several sociolinguistic theories on styles have been developed focusing on their inter-personal perspectives, such as Halliday’s systemic functional linguistics (Halliday, 2006) or Biber’s theory on register, genre, and style (Biber and Conrad, 2019).\nIn sociolinguistics, indexicality (Silverstein, 2003; Coupland, 2007; Johnstone, 2010) is the phenomenon where a sign points to some object, but only in the context in which it occurs. Nonreferential indexicalities include the speaker’s gender, affect (Besnier, 1990), power, solidarity (Brown et al., 1960), social class, and identity (Ochs, 1990).\nBuilding on Silverstein’s notion of indexical order, Eckert (2008) built the notion that linguistic variables index a social group, which leads to the indexing of certain traits stereotypically associated with members of that group. Eckert (2000, 2019) argued that style change creates a new persona, impacting a social landscape and presented the expression of social meaning as a continuum of decreasing reference and increasing performativity.\nDespite the extensive theories, very little is known on extra-dependency across multiple styles. In this work, we empirically show evidence of extra-linguistic variations of styles, like a formal-\nGroups Styles\nINTERPERSONAL Formality, Politeness FIGURATIVE Humor, Sarcasm, Metaphor AFFECTIVE Emotion, Offense, Romance, Sentiment PERSONAL Age, Ethnicity, Gender, Education level,\nCountry, Political view\nTable 1: Style grouping in XSLUE.\nity, politeness, etc, but limited to styles only if we can obtain publicly available resources for computing. We call the individual phenomena a specific type of “style” in this work. We admit that there are many other kinds of styles not covered in this work, such as inter-linguistic variables in grammars and phonology, or high-level style variations like individual’s writing style or genres.\nCross-style analysis. Some recent works have provided empirical evidence of style interdependencies but in a very limited range: Warriner et al. (2013) analyzed emotional norms and their correlation in lexical features of text. Chhaya et al. (2018) studied a correlation of formality, frustration, and politeness but on small samples (i.e., 960 emails). Nguyen et al. (2014) focused on correlation across demographic information (e.g., gender, age) and with some other factors such as emotions (Preoţiuc-Pietro and Ungar, 2018). Dankers et al. (2019); Mohammad et al. (2016) studied the interplay of metaphor and emotion in text. Liu et al. (2010) studied sarcasm detection using sentiment as a sub-problem. Brooke and Hirst (2013) conducted a topical analysis of six styles: literary, abstract, objective, colloquial, concrete, and subjective, on different genres of text. Passonneau et al. (2014) conducted a detailed analysis of Biber’s genres and relationship between genres.\n3 XSLUE: A Benchmark for Cross-Style Language Understanding and Evaluation"
    }, {
      "heading" : "3.1 Style selection and groupings",
      "text" : "In order to conduct a comprehensive style research, one needs to collect a collection of different style datasets. We survey recent papers related to style research published in ACL venues and choose 15 widely-used styles that have publicly available annotated resources and feasible size of training dataset (Table 1). We plan to gradually increase the coverage of style kinds and make the benchmark more comprehensive in the future.\nWe follow the theoretical style grouping criteria based on their social goals in Kang (2020) that categorizes styles into four groups (Table 1): PERSONAL, INTERPERSONAL, FIGURATIVE, and AFFECTIVE group, where each group has its own social goals in communication. This grouping will be used in our case studies as a basic framework to detect their dependencies."
    }, {
      "heading" : "3.2 Individual style dataset",
      "text" : "For each style in the group, we pre-process existing style datasets or collect our own if there is no publicly available one (i.e., ShortRomance). We do not include datasets with small samples (e.g., ≤ 1K) due to its infeasibility of training a large model. We also limit our dataset to classify a single sentence, although there exists other types of datasets (e.g., document-level style classifications, classifying a sentence with respect to context given) which are out of scope of this work.\nIf a dataset has its own data split, we follow that. Otherwise, we randomly split it by 0.9/0.05/0.05 ra-\ntios for the train, valid, and test set, respectively. If a dataset has only positive samples (ShortHumor, ShortJoke, ShortRomance), we do negative sampling from literal text as in Khodak et al. (2017). We include the detailed pre-processing steps in Appendix §A."
    }, {
      "heading" : "3.3 Cross-style diagnostic set",
      "text" : "The individual datasets, however, have variations in domains (e.g., web, dialogue, tweets), label distributions, and data sizes (See domain, label, and #S columns in Table 2). Evaluating a system with these individual datasets’ test set is not an appropriate way to validate how multiple styles are used together in a mixed way, because samples from individual datasets are annotated only when a single style is considered.\nTo help researchers evaluate their systems in the cross-style setting, we collect an additional diagnostic set, called cross-set by annotating labels of 15 styles together on the same text from crowd workers. We collect total 500 sample texts from\npha). The degree of gray shading shows good ,\nmoderate , and fair agreements.\ntwo different sources: the first half is randomly chosen from test sets among the 15 style datasets in balance, and the second half is chosen from random tweets that have high variations across style prediction scores using our pre-trained style classifiers. Each sample text is annotated by five annotators, and the final label for each style is decided via majority voting over the five annotations. In case they are tied or all different from each other for multiple labels, we don’t include them. We also include Don’t Know option for personal styles and Neutral option for two opposing binary styles (e.g., sentiment, formality). The detailed annotation schemes are in Appendix §B.\nTable 3 shows annotator’s agreement on the cross-set. We find that annotator’s agreement varies a lot depending on style: sentiment and politeness with good agreement, and formality, emotion, and romance with moderate agreement. However, personal styles (e.g., age, education level, and political view), metaphor, and emotions (e.g., arousal and dominance), show fair agreements, indicating how difficult and subjective styles they are."
    }, {
      "heading" : "3.4 Contribution",
      "text" : "Most datasets in XSLUE except for Romance are collected from others’ work. Following the data statement (Bender and Friedman, 2018), we cite and introduce individual datasets with their data statistics in Table 2. Our main contribution is to make every dataset to have the same pre-processed format, and distribute them with accompanying code for better reproducibility and accessibility. Besides this engineering effort, XSLUE’s main goal is to invite NLP researchers to the field of crossstyle understanding and provide them a valid set of evaluation for further exploration. As the first step, using XSLUE, we study cross-style language variation in various applications such as classification (§4), correlation (§5), and generation (§6)."
    }, {
      "heading" : "4 Case #1: Cross-Style Classification",
      "text" : "We study how modeling multiple styles together, instead of modeling them individually, can be effective in style classification task. Particularly, the annotated cross-set in XSLUE will be used as a part of evaluation for cross-style classification.\nModels. We compare two types of models: single and cross model. The single model is trained on individual style dataset separately, whereas the cross model is trained on shuffled set of every dataset together. For single model, we use various baseline models, such as majority classifier by choosing the majority label in training data, Bidirectional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al., 2014), and variants of fine-tuned transformers; Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), robustly optimized BERT (RoBERTa) (Liu et al., 2019), and text-to-text transformer (T5) (Raffel et al., 2019).2\nFor cross model, we propose an encoder-decoder based model that learns cross-style patterns with the shared internal representation across styles (Figure 1). It encodes different styles of input as text (e.g., “STYLE: formality TEXT: would you please..”) and decodes output label as text (e.g., “formal”). We use the pretrained encoder-decoder model from T5 (Raffel et al., 2019), and finetune it using the combined, shuffled datasets in XSLUE. Due to the nature of encoder-decoder model, we can take any training instances for classification tasks into the same text-to-text format. We also trained the single model (e.g., RoBERTa) on the combined datasets via a multi-task setup (i.e., 15 different heads), but showing less significant result.\n2For a fair comparison, we restrict size of the pre-trained transformer models to ‘base‘ model only, although additional improvement from the larger models is possible.\nThe detailed hyper-parameters used in our model training are in Appendix §C.\nTasks. Our evaluation has two tasks: individualset evaluation for evaluating a classifier on individual dataset’s test set (left columns in Table 4) and cross-set evaluation for evaluating a classifier on the annotated cross-set collected in §3.3 (right columns in Table 4).\nDue to the label imbalance of datasets, we measure f-score (F1) for classification tasks and Pearson-Spearman correlation for regression tasks (i.e., EmoBank). For multi-labels, all scores are macro-averaged on each label.\nResults. In the individual-set evaluation, compared to the biLSTM classifier, the fine-tuned transformers show significant improvements (+8% points F1) on average, although the different transformer models have similar F1 scores. Our proposed cross model, significantly outperforms the single model, by +1.7 percentage points overall F1 score, showing the benefit of learning multiple styles together. Particularly, the cross model sig-\nnificantly improves F1 scores on personal styles such as gender, age, and education level, possibly because the personal styles may be beneficial from detecting other styles. Among the styles, all personal styles, figurative styles (e.g., sarcasm and metaphor), and emotions are the most difficult styles to predict, which is similarly observed in the annotator’s agreement in Table 3.\nIn cross-set evaluation, the overall performance significantly drops against the individual set evaluation, like from 65.9% to 40.7%, showing why it is important to have these annotated diagnostic set for valid evaluation of cross-style variation. Again, the cross-style model achieves +1.2% gain than the single models.\nFigure 2 shows F1 improvement by the cross model against the single model BERT. Most styles obtain performance gain from the cross-style modeling, whereas not in the two metaphor style datasets (VUA, TroFi) and ethnicity style. This is possibly because metaphor tasks prepend the target metaphor verb to the input text, which is different from other task setups. Thus, learning them\ntogether may harm the performance, although it is not significant."
    }, {
      "heading" : "5 Case #2: Style Dependencies",
      "text" : "In addition to the theoretical style grouping in §3.1, we empirically find how two styles are correlated in human-written text using silver predictions from the classifiers.\nSetup. We sample 1,000,000 tweets crawled using Twitter’s Gardenhose API. We choose tweets as the target domain, because of their stylistic diversity compared to other domains, such as news articles. Using the fine-tuned cross-style classifier in §4, we predict probability of 53 style attributes3 over the 1M tweets. We split a tweet into sentences and then average their prediction scores. We then produce a correlation matrix across the style attributes using Pearson correlation coefficients with Euclidean distance and finally output a 53×53 correlation matrix. We only show correlations that are statistical significant with p-value < 0.05 and cross out the rest.\nReliability. One may doubt about the classifier’s low performance on some styles, leading to unreliable interpretation of our analysis. Although we only show correlation on the predicted style values,\n3Attribute means labels of each style: positive and negative labels for sentiment style.\nwe also performed the same analysis on the humanannotated cross-set, showing similar correlation tendencies to the predicted ones. However, due to the small number of annotations, its statistical significance is not high enough. Instead, we decide to show the prediction-based correlation, possibly including noisy correlations but with statistical significance.\nResults. Figure 3 shows the full correlation matrix we found. From the matrix, we summarize some of the highly correlated style pairs in Table 5 For each pair of correlation, two annotators evaluate its validity of stylistic dependency using a Likert scale. Our prediction-based correlation shows 4.18 agreement on average, showing reasonable accuracy of correlations.\nWe also provide an empirical grouping of styles using Ward hierarchical clustering (Ward Jr, 1963) on the correlation matrix. Figure 4 shows some interpretable style clusters detected from text, like Asian ethnicities (SouthAsian, EastAsian), middle ages (35-44, 45-54, 55-74), positiveness (happiness, dominance, positive, polite), and bad emotions (anger, disgust, sadness, fear)."
    }, {
      "heading" : "6 Case #3: Cross-Style Generation",
      "text" : "We study the effect of combination of some styles in the context of generation. We first describe our\nstyle-conditioned generators that combine the style classifiers in §4 with pre-trained generators (§6.1), and then validate two hypothetical questions using the generators: does better identification of styles help better stylistic generation (§6.2)? and which combination of styles are more natural or contradictive in generation (§6.3)?"
    }, {
      "heading" : "6.1 Style-conditioned Generation",
      "text" : "Let x an input text and s a target style. Since we already have the fine-tuned style classifiers P(s|x) from §4, we can combine them with a genera-\ntor P(x), like a pre-trained language model, and then generate text conditioned on the target style P(x|s). We extend the plug-and-play language model (PPLM) (Dathathri et al., 2019) to combine our style classifiers trained on XSLUE with the pre-trained generator; GPT2 (Radford et al., 2019) without extra fine-tuning: P(x|s) ∝ P(x) · P(s|x). Table 6 shows example outputs from our styleconditioned generators given a prompt ‘Every natural text is’.\nWe evaluate quality of output text: given 20 frequent prompts randomly extracted from our\nOutput without style condition:\n‘Every natural text is’ a series of images. The images, as they are known within the text, are the primary means by which a text is read, and therefore are ..\nOutput conditioned on Formality (F1 = 89.9%) : Formal (left) and Informal (right)\n‘Every natural text is’ different. You may find that the word you wrote does not appear on the website of the author. If you have any queries, you can contact us.. ‘Every natural text is’ a bit of a hack. I don’t think of it as a hack, because this hack is the hack.. and if you don’t believe me then please don’t read this, I don’t care..\nOutput conditioned on Offense (F1 = 93.4%) : Non-offensive (left) and Offensive (right)\n‘Every natural text is’ a natural language, and every natural language is a language that we can speak. It is the language of our thoughts and of our lives.. ‘Every natural text is’ worth reading...I’m really going to miss the music of David Byrne, and that was so much fun to watch live. The guy is a *ucking *ick. ..\nTable 6: Given a prompt ‘Every natural text is’, output text predicted by our stylistic generator. The blue and red phrases are manually-labeled as reasonable features for each label. Offensive words are replaced with *.\ntraining data,4 we generate 10 continuation text for each prompt for each binary label of four styles (sentiment, politeness, offense, and formality)5 using the conditional style generator; total 20∗10∗2∗4=1600 continuations.\nWe evaluate using both automatic and human measures: In automatic evaluation, we calculate F1 score of generated text using the fine-tuned classifiers, to check whether the output text reflects stylistic factor of the target style given. In human\n4Some example prompts: “Meaning of life is”, “I am”, “I am looking for”, “Humans are”, “The virus is”, etc\n5We choose them by the two highest F1 scored styles each from inter-personal and affective groups, although we conduct experiments on other styles such as romance and emotions.\nSentiment Politeness Formality Offense\nXSLUE (F1) 96.5 71.2 89.8 93.3 Auto (F1) 73.7 70.1 60.0 63.7\nHuman(1st ) 3.4/3.5/2.8 3.6/3.6/3.3 3.4/3.7/3.1 4.0/3.9/3.3 Human(2nd ) 2.4/3.2/2.3 2.8/3.4/2.7 2.9/2.8/2.0 2.9/3.3/2.5\nTable 7: Automatic and human evaluation on generated text. 1st and 2nd labels correspond to positive and negative for sentiment, polite and impolite for politeness, formal and informal for formality, and nonoffensive and offensive for offense. Three numbers in human evaluation means stylistic appropriateness, consistency with prompt, and overall coherence in order.\nevaluation, scores (1-5 Likert scale) annotated by three crowd-workers are averaged on three metrics: stylistic appropriateness6, consistency with prompt, and overall coherence.\nIn Table 7, compared to F1 scores on individual test set in XSLUE, automatic scores on output from the generator are less by 20.5% on average, showing sub-optimality of the conditional style generator between classification and generation. Interestingly, in human evaluation, negative labels (2nd label for each style) for each style, like negative sentiment, impoliteness, informality, and offensiveness, show less stylistic appropriateness than positive or literal labels."
    }, {
      "heading" : "6.2 Better classification, better generation",
      "text" : "To further investigate the relationship between classifier’s performance and generation quality, we conduct a study by decreasing the training completion ratio (i.e., a fraction of epochs until completion; C%) of the classifiers; PC%(s|x) over the four styles and again evaluate the output continuation; PC%(x|s) ∝ P(x) · PC%(s|x) using the same\n6Stylistically appropriateness means the output text includes appropriate amount of target style given.\nhuman metrics. Figure 5 shows that the better style understanding (higher F1 scores in classification) yields the better stylistic generation (higher stylistic appropriateness and consistency scores)."
    }, {
      "heading" : "6.3 Contradictive styles in generation",
      "text" : "We have generated text conditioned on single styles. We now generate text conditioned on combination of multiple styles; P(x|s1..sk)& ∝ P(x) · P(s1|x) · · ·P(sk|x) where k is the number of styles. In our experiment, we set k=2 for sentiment and politeness styles, and generate text conditioned on all possible combinations between the labels of the two styles (e.g., positive and polite label, negative and impolite label). We again conduct human evaluation on the output text for measuring whether the generator produces stylistically appropriate text given the combination.\nTable 8 shows averaged human-measured stylistic appropriate scores over the four label combinations (left) and the correlation scores observed in the style correlation matrix on written text in Figure 3 (right). Some combinations, like positive and impolite or like negative and polite, show less stylistic appropriateness scores, because they are naturally contradictive in their stylistic variation. Moreover, the stylistic appropriateness scores look similar to the correlation score observed from written text, showing that there exists some natural or\nunnatural combination of styles in both classification on human-written text and output generated by the model."
    }, {
      "heading" : "7 Conclusion and Discussion",
      "text" : "We introduce a benchmark XSLUE of mostly existing datasets for studying cross-style language understanding and evaluation. Using XSLUE, we found interesting cross-style observations in classification, correlation, and generation case studies. We believe XSLUE helps other researchers develop more solid methods on various cross-style applications. We summarize other concerns we found from our case studies:\nStyle drift. The biggest challenge in collecting style datasets is to diversify the style of text but preserve the meaning, to avoid semantic drift. In the cross-style setting, we also faced a new challenge; style drift, where different styles are coupled so changing one style might affect the others.\nEthical consideration. Some styles, particularly on styles related to personal traits, are ethically sensitive, so require more careful interpretation of the results not to make any misleading points. Any follow-up research needs to consider such ethical issues as well as provides potential weaknesses of their proposed methods.\nFrom correlation to causality. Our analysis is based on correlation, not causality. In order to find causal relation between styles, more sophisticated causal analyses, such as propensity score (Austin, 2011), need to be considered for controlling the confounding variables. By doing so, we may resolve the biases driven from the specific domain of training data. For example, generated text with the politeness classifier (Danescu et al., 2013) contains many technical terms (e.g., 3D, OpenCV, bugs) because its training data is collected from StackExchange."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work would not have been possible without the efforts of the authors who kindly share the style language datasets publicly. We thank Edvisees members at CMU, Hearst lab members at UC Berkeley, and anonymous reviewers for their helpful comments."
    } ],
    "references" : [ {
      "title" : "An introduction to propensity score methods for reducing the effects of confounding in observational studies",
      "author" : [ "Peter C Austin." ],
      "venue" : "Multivariate behavioral research, 46(3):399–424.",
      "citeRegEx" : "Austin.,? 2011",
      "shortCiteRegEx" : "Austin.",
      "year" : 2011
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Language and affect",
      "author" : [ "Niko Besnier." ],
      "venue" : "Annual review of anthropology, 19(1):419–451.",
      "citeRegEx" : "Besnier.,? 1990",
      "shortCiteRegEx" : "Besnier.",
      "year" : 1990
    }, {
      "title" : "Register, genre, and style",
      "author" : [ "Douglas Biber", "Susan Conrad." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Biber and Conrad.,? 2019",
      "shortCiteRegEx" : "Biber and Conrad.",
      "year" : 2019
    }, {
      "title" : "A clustering approach for nearly unsupervised recognition of nonliteral language",
      "author" : [ "Julia Birke", "Anoop Sarkar." ],
      "venue" : "EACL.",
      "citeRegEx" : "Birke and Sarkar.,? 2006",
      "shortCiteRegEx" : "Birke and Sarkar.",
      "year" : 2006
    }, {
      "title" : "A multidimensional Bayesian approach to lexical style",
      "author" : [ "Julian Brooke", "Graeme Hirst." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Brooke and Hirst.,? 2013",
      "shortCiteRegEx" : "Brooke and Hirst.",
      "year" : 2013
    }, {
      "title" : "The pronouns of power and solidarity",
      "author" : [ "Roger Brown", "Albert Gilman" ],
      "venue" : null,
      "citeRegEx" : "Brown and Gilman,? \\Q1960\\E",
      "shortCiteRegEx" : "Brown and Gilman",
      "year" : 1960
    }, {
      "title" : "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "author" : [ "Sven Buechel", "Udo Hahn." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Buechel and Hahn.,? 2017",
      "shortCiteRegEx" : "Buechel and Hahn.",
      "year" : 2017
    }, {
      "title" : "Frustrated, polite, or formal: Quantifying feelings and tone in email",
      "author" : [ "Niyati Chhaya", "Kushal Chawla", "Tanya Goyal", "Projjal Chanda", "Jaya Singh." ],
      "venue" : "Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality,",
      "citeRegEx" : "Chhaya et al\\.,? 2018",
      "shortCiteRegEx" : "Chhaya et al\\.",
      "year" : 2018
    }, {
      "title" : "Politeness, politics and diplomacy",
      "author" : [ "Paul Chilton." ],
      "venue" : "Discourse & Society, 1(2):201–224.",
      "citeRegEx" : "Chilton.,? 1990",
      "shortCiteRegEx" : "Chilton.",
      "year" : 1990
    }, {
      "title" : "Polite responses to polite requests",
      "author" : [ "Herbert H Clark", "Dale H Schunk." ],
      "venue" : "Cognition, 8(2):111–143.",
      "citeRegEx" : "Clark and Schunk.,? 1980",
      "shortCiteRegEx" : "Clark and Schunk.",
      "year" : 1980
    }, {
      "title" : "Style: Language variation and identity",
      "author" : [ "Nikolas Coupland." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Coupland.,? 2007",
      "shortCiteRegEx" : "Coupland.",
      "year" : 2007
    }, {
      "title" : "text Emotion",
      "author" : [ "CrowdFlower." ],
      "venue" : "http: //www.crowdflower.com/wp-content/uploads/ 2016/07/text_emotion.csv. [Online; accessed 1-Oct-2019].",
      "citeRegEx" : "CrowdFlower.,? 2016",
      "shortCiteRegEx" : "CrowdFlower.",
      "year" : 2016
    }, {
      "title" : "Short Text Corpus For Humor Detection",
      "author" : [ "CrowdTruth." ],
      "venue" : "http://github.com/CrowdTruth/ Short-Text-Corpus-For-Humor-Detection. [Online; accessed 1-Oct-2019].",
      "citeRegEx" : "CrowdTruth.,? 2016",
      "shortCiteRegEx" : "CrowdTruth.",
      "year" : 2016
    }, {
      "title" : "A computational approach to politeness with application to social factors",
      "author" : [ "Niculescu-Mizil Cristian Danescu", "Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts." ],
      "venue" : "arXiv preprint arXiv:1306.6078.",
      "citeRegEx" : "Danescu et al\\.,? 2013",
      "shortCiteRegEx" : "Danescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Modelling the interplay of metaphor and emotion through multitask learning",
      "author" : [ "Verna Dankers", "Marek Rei", "Martha Lewis", "Ekaterina Shutova." ],
      "venue" : "IJCNLP 2019.",
      "citeRegEx" : "Dankers et al\\.,? 2019",
      "shortCiteRegEx" : "Dankers et al\\.",
      "year" : 2019
    }, {
      "title" : "Plug and play language models: a simple approach to controlled text generation",
      "author" : [ "Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu." ],
      "venue" : "arXiv preprint arXiv:1912.02164.",
      "citeRegEx" : "Dathathri et al\\.,? 2019",
      "shortCiteRegEx" : "Dathathri et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Eleventh international aaai conference on web and social media.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Language variation as social practice: The linguistic construction of identity in Belten High",
      "author" : [ "Penelope Eckert." ],
      "venue" : "Wiley-Blackwell.",
      "citeRegEx" : "Eckert.,? 2000",
      "shortCiteRegEx" : "Eckert.",
      "year" : 2000
    }, {
      "title" : "Variation and the indexical field 1",
      "author" : [ "Penelope Eckert." ],
      "venue" : "Journal of sociolinguistics, 12(4):453–476.",
      "citeRegEx" : "Eckert.,? 2008",
      "shortCiteRegEx" : "Eckert.",
      "year" : 2008
    }, {
      "title" : "The limits of meaning: Social indexicality, variation, and the cline of interiority",
      "author" : [ "Penelope Eckert." ],
      "venue" : "Language, 95(4):751–776.",
      "citeRegEx" : "Eckert.,? 2019",
      "shortCiteRegEx" : "Eckert.",
      "year" : 2019
    }, {
      "title" : "An argument for basic emotions",
      "author" : [ "Paul Ekman." ],
      "venue" : "Cognition & emotion, 6(3-4):169–200.",
      "citeRegEx" : "Ekman.,? 1992",
      "shortCiteRegEx" : "Ekman.",
      "year" : 1992
    }, {
      "title" : "Fracking sarcasm using neural network",
      "author" : [ "Aniruddha Ghosh", "Tony Veale." ],
      "venue" : "Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis, pages 161–169.",
      "citeRegEx" : "Ghosh and Veale.,? 2016",
      "shortCiteRegEx" : "Ghosh and Veale.",
      "year" : 2016
    }, {
      "title" : "Linguistic studies of text and discourse, volume 2",
      "author" : [ "Michael Alexander Kirkwood Halliday." ],
      "venue" : "A&C Black.",
      "citeRegEx" : "Halliday.,? 2006",
      "shortCiteRegEx" : "Halliday.",
      "year" : 2006
    }, {
      "title" : "Formality of language: definition, measurement and behavioral determinants",
      "author" : [ "Francis Heylighen", "Jean-Marc Dewaele." ],
      "venue" : "Interner Bericht, Center “Leo Apostel”, Vrije Universiteit Brüssel.",
      "citeRegEx" : "Heylighen and Dewaele.,? 1999",
      "shortCiteRegEx" : "Heylighen and Dewaele.",
      "year" : 1999
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Generating natural language under pragmatic constraints",
      "author" : [ "Eduard Hovy." ],
      "venue" : "Journal of Pragmatics, 11(6):689–719.",
      "citeRegEx" : "Hovy.,? 1987",
      "shortCiteRegEx" : "Hovy.",
      "year" : 1987
    }, {
      "title" : "Deep contextualized word representations for detecting sarcasm and irony",
      "author" : [ "Suzana Ilić", "Edison Marrese-Taylor", "Jorge A Balazs", "Yutaka Matsuo." ],
      "venue" : "arXiv preprint arXiv:1809.09795.",
      "citeRegEx" : "Ilić et al\\.,? 2018",
      "shortCiteRegEx" : "Ilić et al\\.",
      "year" : 2018
    }, {
      "title" : "Hate crimes: Criminal law & identity politics",
      "author" : [ "James B Jacobs", "Kimberly Potter" ],
      "venue" : null,
      "citeRegEx" : "Jacobs and Potter,? \\Q1998\\E",
      "shortCiteRegEx" : "Jacobs and Potter",
      "year" : 1998
    }, {
      "title" : "Stance: sociolinguistic perspectives",
      "author" : [ "Alexandra Jaffe" ],
      "venue" : "OUP USA.",
      "citeRegEx" : "Jaffe,? 2009",
      "shortCiteRegEx" : "Jaffe",
      "year" : 2009
    }, {
      "title" : "Locating language in identity",
      "author" : [ "Barbara Johnstone." ],
      "venue" : "Language and identities, 31:29–36.",
      "citeRegEx" : "Johnstone.,? 2010",
      "shortCiteRegEx" : "Johnstone.",
      "year" : 2010
    }, {
      "title" : "Earlier isn’t always better: Subaspect analysis on corpus and system biases in summarization",
      "author" : [ "Taehee Jung", "Dongyeop Kang", "Lucas Mentch", "Eduard Hovy." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP), Hong",
      "citeRegEx" : "Jung et al\\.,? 2019",
      "shortCiteRegEx" : "Jung et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistically Informed Language Generation: A Multifaceted Approach",
      "author" : [ "Dongyeop Kang." ],
      "venue" : "PhD dissertation, Carnegie Mellon University.",
      "citeRegEx" : "Kang.,? 2020",
      "shortCiteRegEx" : "Kang.",
      "year" : 2020
    }, {
      "title" : "male, bachelor) and (female, ph.d) have different connotations: Parallelly annotated stylistic language dataset with multiple personas",
      "author" : [ "Dongyeop Kang", "Varun Gangal", "Eduard Hovy" ],
      "venue" : "In Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Kang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2019
    }, {
      "title" : "A large self-annotated corpus for sarcasm",
      "author" : [ "Mikhail Khodak", "Nikunj Saunshi", "Kiran Vodrahalli." ],
      "venue" : "arXiv preprint arXiv:1704.05579.",
      "citeRegEx" : "Khodak et al\\.,? 2017",
      "shortCiteRegEx" : "Khodak et al\\.",
      "year" : 2017
    }, {
      "title" : "That’s what she said: Double entendre identification",
      "author" : [ "Chloe Kiddon", "Yuriy Brun." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 89–94. As-",
      "citeRegEx" : "Kiddon and Brun.,? 2011",
      "shortCiteRegEx" : "Kiddon and Brun.",
      "year" : 2011
    }, {
      "title" : "Determining the sentiment of opinions",
      "author" : [ "Soo-Min Kim", "Eduard Hovy." ],
      "venue" : "Proceedings of the 20th international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.",
      "citeRegEx" : "Kim and Hovy.,? 2004",
      "shortCiteRegEx" : "Kim and Hovy.",
      "year" : 2004
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "arXiv preprint arXiv:1710.03957.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Sentiment analysis and subjectivity",
      "author" : [ "Bing Liu" ],
      "venue" : "Handbook of natural language processing, 2(2010):627–666.",
      "citeRegEx" : "Liu,? 2010",
      "shortCiteRegEx" : "Liu",
      "year" : 2010
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphor as a medium for emotion: An empirical study",
      "author" : [ "Saif Mohammad", "Ekaterina Shutova", "Peter Turney." ],
      "venue" : "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 23–33, Berlin, Germany. Association for",
      "citeRegEx" : "Mohammad et al\\.,? 2016",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2016
    }, {
      "title" : "Introducing the lcc metaphor datasets",
      "author" : [ "Michael Mohler", "Mary Brunson", "Bryan Rink", "Marc T Tomlinson." ],
      "venue" : "LREC.",
      "citeRegEx" : "Mohler et al\\.,? 2016",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2016
    }, {
      "title" : "short jokes dataset",
      "author" : [ "Abhinav Moudgil." ],
      "venue" : "https:// github.com/amoudgl/short-jokes-dataset. [Online; accessed 1-Oct-2019].",
      "citeRegEx" : "Moudgil.,? 2017",
      "shortCiteRegEx" : "Moudgil.",
      "year" : 2017
    }, {
      "title" : "Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment",
      "author" : [ "Dong Nguyen", "Dolf Trieschnigg", "A. Seza Doğruöz", "Rilana Gravel", "Mariët Theune", "Theo Meder", "Franciska de Jong." ],
      "venue" : "Proceedings of COLING",
      "citeRegEx" : "Nguyen et al\\.,? 2014",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2014
    }, {
      "title" : "Indexicality and socialization",
      "author" : [ "Elinor Ochs." ],
      "venue" : "Cultural psychology: Essays on comparative human development.",
      "citeRegEx" : "Ochs.,? 1990",
      "shortCiteRegEx" : "Ochs.",
      "year" : 1990
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and Trends® in Information Retrieval,",
      "citeRegEx" : "Pang and Lee,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2008
    }, {
      "title" : "Biber redux: Reconsidering dimensions of variation in American English",
      "author" : [ "Rebecca J. Passonneau", "Nancy Ide", "Songqiao Su", "Jesse Stuart." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Passonneau et al\\.,? 2014",
      "shortCiteRegEx" : "Passonneau et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Userlevel race and ethnicity predictors from twitter text",
      "author" : [ "Daniel Preoţiuc-Pietro", "Lyle Ungar." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1534–1545.",
      "citeRegEx" : "Preoţiuc.Pietro and Ungar.,? 2018",
      "shortCiteRegEx" : "Preoţiuc.Pietro and Ungar.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer",
      "author" : [ "Sudha Rao", "Joel Tetreault." ],
      "venue" : "arXiv preprint arXiv:1803.06535.",
      "citeRegEx" : "Rao and Tetreault.,? 2018",
      "shortCiteRegEx" : "Rao and Tetreault.",
      "year" : 2018
    }, {
      "title" : "Indexical order and the dialectics of sociolinguistic life",
      "author" : [ "Michael Silverstein." ],
      "venue" : "Language & communication, 23(3-4):193–229.",
      "citeRegEx" : "Silverstein.,? 2003",
      "shortCiteRegEx" : "Silverstein.",
      "year" : 2003
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "A method for linguistic metaphor identification: From MIP to MIPVU, volume 14",
      "author" : [ "Gerard Steen." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Steen.,? 2010",
      "shortCiteRegEx" : "Steen.",
      "year" : 2010
    }, {
      "title" : "Hate speech: The history of an American controversy",
      "author" : [ "Samuel Walker." ],
      "venue" : "U of Nebraska Press.",
      "citeRegEx" : "Walker.,? 1994",
      "shortCiteRegEx" : "Walker.",
      "year" : 1994
    }, {
      "title" : "Hierarchical grouping to optimize an objective function",
      "author" : [ "Joe H Ward Jr." ],
      "venue" : "Journal of the American statistical association, 58(301):236–244.",
      "citeRegEx" : "Jr.,? 1963",
      "shortCiteRegEx" : "Jr.",
      "year" : 1963
    }, {
      "title" : "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "author" : [ "Amy Beth Warriner", "Victor Kuperman", "Marc Brysbaert." ],
      "venue" : "Behavior research methods, 45(4):1191–1207.",
      "citeRegEx" : "Warriner et al\\.,? 2013",
      "shortCiteRegEx" : "Warriner et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Some stylistic choices implicitly reflect the author’s characteristics, like personality, demographic traits (Kang et al., 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : ", 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : ", 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al., 2013).",
      "startOffset" : 203,
      "endOffset" : 225
    }, {
      "referenceID" : 44,
      "context" : "To the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preoţiuc-Pietro and Ungar, 2018), across emotions (Warriner et al.",
      "startOffset" : 149,
      "endOffset" : 203
    }, {
      "referenceID" : 49,
      "context" : "To the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preoţiuc-Pietro and Ungar, 2018), across emotions (Warriner et al.",
      "startOffset" : 149,
      "endOffset" : 203
    }, {
      "referenceID" : 58,
      "context" : ", 2014; Preoţiuc-Pietro and Ungar, 2018), across emotions (Warriner et al., 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : ", 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 47,
      "context" : ", 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al., 2014), or between metaphor and emotion (Dankers et al.",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : ", 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 86
    }, {
      "referenceID" : 41,
      "context" : ", 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Several sociolinguistic theories on styles have been developed focusing on their inter-personal perspectives, such as Halliday’s systemic functional linguistics (Halliday, 2006) or Biber’s theory on register, genre, and style (Biber and Conrad, 2019).",
      "startOffset" : 161,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "Several sociolinguistic theories on styles have been developed focusing on their inter-personal perspectives, such as Halliday’s systemic functional linguistics (Halliday, 2006) or Biber’s theory on register, genre, and style (Biber and Conrad, 2019).",
      "startOffset" : 226,
      "endOffset" : 250
    }, {
      "referenceID" : 53,
      "context" : "In sociolinguistics, indexicality (Silverstein, 2003; Coupland, 2007; Johnstone, 2010) is the phenomenon where a sign points to some object, but only in the context in which it occurs.",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "In sociolinguistics, indexicality (Silverstein, 2003; Coupland, 2007; Johnstone, 2010) is the phenomenon where a sign points to some object, but only in the context in which it occurs.",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "In sociolinguistics, indexicality (Silverstein, 2003; Coupland, 2007; Johnstone, 2010) is the phenomenon where a sign points to some object, but only in the context in which it occurs.",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Nonreferential indexicalities include the speaker’s gender, affect (Besnier, 1990), power, solidarity (Brown et al.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 45,
      "context" : ", 1960), social class, and identity (Ochs, 1990).",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 49,
      "context" : ", gender, age) and with some other factors such as emotions (Preoţiuc-Pietro and Ungar, 2018).",
      "startOffset" : 60,
      "endOffset" : 93
    }, {
      "referenceID" : 52,
      "context" : "Formality GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "Politeness StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "F IG U R A T IV E Humor ShortHumor (CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) Y web Y clsf.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 43,
      "context" : "ShortJoke (Moudgil, 2017) 463k random 2 humor (50%), non-humor (50%) Y web Y clsf.",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) Y tweet Y clsf.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 35,
      "context" : "SARC (Khodak et al., 2017) 321k given 2 sarcastic (50%), non-sarcastic (50%) Y reddit Y clsf.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 35,
      "context" : "SARC_pol (Khodak et al., 2017) 17k given 2 sarcastic (50%), non-sarcastic (50%) Y reddit Y clsf.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 55,
      "context" : "Metaphor VUA (Steen, 2010) 23k given 2 metaphor (28.",
      "startOffset" : 13,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "A FF E C T IV E Emotion EmoBankvalence (Buechel and Hahn, 2017) 10k random 1 negative, positive - misc.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "EmoBankarousal (Buechel and Hahn, 2017) 10k random 1 calm, excited - misc.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "EmoBankdominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control - misc.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 54,
      "context" : "Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 34,
      "context" : "P E R S O N A L Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 34,
      "context" : "Country PASTEL (Kang et al., 2019) 41k given 2 USA (97.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "Politics PASTEL (Kang et al., 2019) 41k given 3 LeftWing (42.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 34,
      "context" : "Education PASTEL (Kang et al., 2019) 41k given 10 Bachelor(30.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 34,
      "context" : "Ethnicity PASTEL (Kang et al., 2019) 41k given 10 Caucasian(75.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Following the data statement (Bender and Friedman, 2018), we cite and introduce individual datasets with their data statistics in Table 2.",
      "startOffset" : 29,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "For single model, we use various baseline models, such as majority classifier by choosing the majority label in training data, Bidirectional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al.",
      "startOffset" : 155,
      "endOffset" : 189
    }, {
      "referenceID" : 48,
      "context" : "For single model, we use various baseline models, such as majority classifier by choosing the majority label in training data, Bidirectional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al., 2014), and variants of fine-tuned transformers; Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al.",
      "startOffset" : 212,
      "endOffset" : 237
    }, {
      "referenceID" : 18,
      "context" : ", 2014), and variants of fine-tuned transformers; Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), robustly optimized BERT (RoBERTa) (Liu et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 40,
      "context" : ", 2019), robustly optimized BERT (RoBERTa) (Liu et al., 2019), and text-to-text transformer (T5) (Raffel et al.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 51,
      "context" : ", 2019), and text-to-text transformer (T5) (Raffel et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 51,
      "context" : "We use the pretrained encoder-decoder model from T5 (Raffel et al., 2019), and finetune it using the combined, shuffled datasets in XSLUE.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "We extend the plug-and-play language model (PPLM) (Dathathri et al., 2019) to combine our style classifiers trained on XSLUE with the pre-trained generator; GPT2 (Radford et al.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 50,
      "context" : ", 2019) to combine our style classifiers trained on XSLUE with the pre-trained generator; GPT2 (Radford et al., 2019) without extra fine-tuning: P(x|s) ∝ P(x) · P(s|x).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "In order to find causal relation between styles, more sophisticated causal analyses, such as propensity score (Austin, 2011), need to be considered for controlling the confounding variables.",
      "startOffset" : 110,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "For example, generated text with the politeness classifier (Danescu et al., 2013) contains many technical terms (e.",
      "startOffset" : 59,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One cannot form a complete understanding of a text without considering these factors. The factors combine and co-vary in complex ways to form styles. Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called crossstyle language understanding. This paper provides the benchmark corpus (XSLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. The benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups. For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text. Using XSLUE, we propose three interesting crossstyle applications in classification, correlation, and generation. First, our proposed crossstyle classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. Second, our study shows that some styles are highly dependent on each other in human-written text. Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for crossstyle research. The preprocessed datasets and code are publicly available.1",
    "creator" : "LaTeX with hyperref"
  }
}