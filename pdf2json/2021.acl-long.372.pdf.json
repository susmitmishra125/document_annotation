{
  "name" : "2021.acl-long.372.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition",
    "authors" : [ "Fei Li", "Zhichao Lin", "Meishan Zhang", "Donghong Ji" ],
    "emails" : [ "csnlp@whu.edu.cn", "dhji@whu.edu.cn", "mason.zms@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4814–4828\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4814"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019). Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001). However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec-\n∗Corresponding author. 1 We consider “nested” as a special case of “overlapped”.\nond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments.\nThere have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER could be conducted universally without any assumption to exclude overlapped or discontinuous entities, which could be more practical in real applications.\nThe hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020) are flexible to be adapted for different tasks, achieving great successes for overlapped or discontinuous NER. However, these models need to manually define graph nodes, edges and transition actions. Moreover, these models build graphs or generate transitions along the words in the sentences gradually, which may lead to error propagation (Zhang et al., 2016). In contrast, the spanbased scheme might be a good alternative, which is much simpler including only span-level classification. Thus, it needs less manual intervention and meanwhile span-level classification can be fully parallelized without error propagation. Recently, Luan et al. (2019) utilized the span-based model for information extraction effectively.\nIn this work, we propose a novel span-based joint model to recognize overlapped and discon-\ntinuous entities simultaneously in an end-to-end way. The model utilizes BERT (Devlin et al., 2019) to produce deep contextualized word representations, and then enumerates all candidate text spans (Luan et al., 2019), classifying whether they are entity fragments. Following, fragment relations are predicted by another classifier to determine whether two specific fragments involve a certain relation. We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively. In essence, the joint model can be regarded as one kind of relation extraction models, which is adapted for our goal. To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019).\nWe evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Straková et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner."
    }, {
      "heading" : "2 Related Work",
      "text" : "In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities.\nFor overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018). Other approaches include multi-grained detection (Xia et al., 2019), boundary detection (Zheng et al., 2019), anchorregion network (Lin et al., 2019a) and machine reading comprehension (Li et al., 2020). The stateof-the-art models for overlapped NER include the sequence-to-sequence (seq2seq) model (Straková et al., 2019), where the decoder predicts multiple\nlabels for a word and move to next word until it outputs the “end of word” label, and the span-based model (Luan et al., 2019; Wadden et al., 2019), where overlapped entities are recognized by classification for enumerated spans.\nCompared with the number of related work for overlapped NER, there are no related studies for only discontinuous NER, but several related studies for both overlapped and discontinuous NER. Early studies addressed such problem by extending the BIO label scheme (Tang et al., 2013; MetkeJimenez and Karimi, 2016). Muis and Lu (2016) first proposed a hypergraph-based model for recognizing overlapped and discontinuous entities, and then Wang and Lu (2019) utilized deep neural networks to enhance the model. Very recently, Dai et al. (2020) proposed a transition-based neural model with manually-designed actions for both overlapped and discontinuous NER. In this work, we also aim to design a competitive model for both overlapped and discontinuous NER. Our differences are that our model is span-based (Luan et al., 2019) and it is also enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al., 2018; Guo et al., 2019).\nTo our knowledge, syntax information is commonly neglected in most previous work for overlapped or discontinuous NER, except Finkel and Manning (2009). The work employs a constituency\nparser to transform a sentence into a nested entity tree, and syntax information is used naturally to facilitate NER. By contrast, syntax information has been utilized in some studies for traditional regular NER. Under the traditional statistical setting, syntax information is used by manually-crafted features (Hacioglu et al., 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al., 2006) for NER. Recently, Jie et al. (2017) build a semi-CRF model based on dependency information to optimize the research space of NER recognition. Jie and Lu (2019) stack the dependency-guided graph convolutional network (Zhang et al., 2018; Guo et al., 2019) on top of the BiLSTM layer. These studies have demonstrated that syntax information could be an effective feature source for NER."
    }, {
      "heading" : "3 Method",
      "text" : "The key idea of our model includes two mechanisms. First, our model enumerates all possible text spans in a sentence and then exploits a multiclassification strategy to determine whether one span is an entity fragment as well as the entity type. Based on this mechanism, overlapped entities could be recognized. Second, our model performs pairwise relation classifications over all entity fragments to recognize their relationships. We define three kinds of relation types:\n• Succession, indicating that the two entity fragments belong to one single named entity. • Overlapping, indicating that the two en-\ntity fragments have overlapped parts. • Other, indicating that the two entity frag-\nments have other relations or no relations.\nWith the Succession relation, we can recognize discontinuous entities. Through the Overlapping relation, we aim to improve the recognition of overlapped entities with double supervision. The proposed model is essentially a relation extraction model being adapted for our task. The architecture of our model is illustrated in Figure 2, where the main components include the following parts: (1) word representation, (2) graph convolutional network, (3) span representation, and (4) joint decoding, which are introduced by the following subsections, respectively."
    }, {
      "heading" : "3.1 Word Representation",
      "text" : "We exploit BERT (Devlin et al., 2019) as inputs for our model, which has demonstrated effective for a range of NLP tasks.2 Given an input sentence x = {x1, x2, ..., xN}, we convert each word xi into word pieces and then feed them into a pretrained BERT module. After the BERT calculation, each sentential word may involve vectorial representations of several pieces. Here we employ the representation of the beginning word piece as the final word representation following (Wadden et al., 2019). For instance, if “fevers” is split into “fever” and “##s”, the representation of “fever” is used as the whole word representation. Therefore, all the words in the sentence x correspond to a matrix H = {h1, h2, ..., hN} ∈ RN×dh , where dh denotes the dimension of hi."
    }, {
      "heading" : "3.2 Graph Convolutional Network",
      "text" : "Dependency syntax information has been demonstrated to be useful for NER previously (Jie and Lu, 2019). In this work, we also exploit it to enhance our proposed model.3 Graph convolutional network (GCN) (Kipf and Welling, 2017) is one representative method to encode dependency-based graphs, which has been shown effective in information extraction (Zhang et al., 2018). Thus, we choose it as one standard strategy to enhance our word representations. Concretely, we utilize the\n2We also investigate the effects of different word encoders in the experiments. Please refer to Appendix A.\n3Some cases are shown in Appendix B.\nattention-guided GCN (AGGCN) (Guo et al., 2019) to reach our goal, as it can bring better performance compared with the standard GCN.\nIn order to illustrate the network of AGGCN (Figure 3), we start with the standard GCN module. Given the word representations H = {h1, h2, ..., hN}, the standard GCN uses the following equation to update them:\nh (l) i = σ( N∑ j=1 AijW (l)h (l−1) j + b (l)), (1)\nwhere W (l) and b(l) are the weight and bias of the l-th layer. A ∈ RN×N is an adjacency matrix obtained from the dependency graph, where Aij = 1 indicates there is an edge between the word i and j in the dependency graph. Figure 2 offers an example of the matrix which is produced by the corresponding dependency syntax tree.\nIn fact, A can be considered as a form of hard attention in GCN, while AGGCN (Guo et al., 2019) aims to improve the method by using A in the lower layers and updating A at the higher layers via multi-head self-attention (Vaswani et al., 2017) as below:\nÃt = softmax( HtW tQ × (HtW tK)T√\ndhead ), (2)\nwhere W tQ and W t K are used to project the input Ht ∈ RN×dhead (dhead = dhNhead ) of the t-th head into a query and a key. Ãt ∈ RN×N is the updated adjacency matrix for the t-th head.\nFor each head t, AGGCN uses Ãt and a densely connected layer to update the word representations, which is similar to the standard GCN as shown in Equation 1. The output of the densely connected layer is H̃t ∈ RN×dh . Then a linear combination layer is used to merge the output of each head, namely H̃ = [H̃1, · · · , H̃Nhead ]W1, where W1 ∈ R(Nhead×dh)×dh is the weight and H̃ ∈ RN×dh is the final output of AGGCN.\nAfter that, H̃ is concatenated with the original word representations H to form final word representations H ′ ∈ RN×(dh+df ) = [H, H̃W2], where W2 ∈ Rdh×df indicates a linear transformation for dimensionality reduction.4"
    }, {
      "heading" : "3.3 Span Representation",
      "text" : "We employ span enumeration (Luan et al., 2019) to generate text spans. Take the sentence “The mitral valve leaflets are mildly thickened” in Figure 2 as an example, the generated text spans will be “The”, “The mitral”, “The mitral valve”, ..., “mildly”, “mildly thickened” and “thickened”. To represent a text span, we use the concatenation of word representations of its startpoint and endpoint. For example, given word representations H = {h1, h2, ..., hN} ∈ RN×dh (or H ′ = {h′1, h′2, ..., h′N}) and a span (i, j) that starts at the position i and ends at j, the span representation will be\nsi,j = [hi,hj ,w] or [h ′ i,h ′ j ,w], (3)\nwhere w is a 20-dimensional embedding to represent the span width following previous work (Luan et al., 2019; Wadden et al., 2019). Thus, the dimension ds of si,j is 2dh + 20 (or 2(dh + df ) + 20)."
    }, {
      "heading" : "3.4 Decoding",
      "text" : "Our decoding consists of two parts. First, we recognize all valid entity fragments, and then perform pairwise classifications over the fragments to uncover their relationships. Entity Fragment Recognition: Given a span (i, j) represented as si,j , we utilize one MLP to\n4We employ third-party tools to perform parsing for the corpora that do not contain gold syntax annotations. Since sometimes parsing may fail, dependency-guided GCN will be noneffective. Concatenation can remedy such problem since H still works even if H̃ is invalid.\nAlgorithm 1 Decoding algorithm. Input: An input sentence x = {x1, x2, ..., xN} Output: The recognized results R 1: S = ENUMERATESPAN(x) where S = {s1,1, s1,2, ...} 2: for si,j in S do 3: if ISENTITYFRAGMENT(si,j) then 4: V ← si,j 5: for each pair si,j , sĩ,j̃ in V do 6: if ISSUCCESSION(si,j , sĩ,j̃) then 7: E← < si,j , sĩ,j̃ > 8: Graph G = {V,E} 9: for g in FINDCOMPLETESUBGRAPHS(G) do 10: R← g 11: return R\nclassify whether the span is an entity fragment and what is the entity type, formalized as:\np1 = softmax(MLP1(si,j)), (4)\nwhere p1 indicates the probabilities of entity types such as Organization, Disease and None (i.e., not an entity fragment). Fragment Relation Prediction: Given two entity fragments (i, j) and (̃i, j̃) represented as si,j and sĩ,j̃ , we utilize another MLP to classify their relations:\np2 = softmax(MLP2([si,j , si,j ∗ sĩ,j̃ , sĩ,j̃ ])), (5) where p2 indicates the probabilities of three classes, namely Succession, Overlapping and Other, and the feature representations are mostly referred from Luan et al. (2019) and Wadden et al. (2019). Noticeably, although the overlapped entities can be recognized at the first step, here we use the Overlapping as one auxiliary strategy to further enhance the model.\nDuring decoding (Algorithm 1), our model recognizes entity fragments from text spans (lines 2-4) in the input sentence and selects each pair of these fragments to determine their relations (lines 5-7). Therefore, the prediction results can be considered as an entity fragment relation graph (line 8), where a node denotes an entity fragment and an edge denotes the relation between two entity fragments.5 The decoding object is to find all the subgraphs in which each node connects with any other node (line 9). Thus, each of such subgraph composes an entity (line 10). In particular, the entity fragment that has no edge with others composes an entity by itself.\n5We only use the Succession relations during decoding while ignore the Overlapping relations. The Overlapping relations are only used during training."
    }, {
      "heading" : "3.5 Training",
      "text" : "During training, we employ multi-task learning (Caruana, 1997; Liu et al., 2017) to jointly train different parts of our model.6 The loss function is defined as the negative log-likelihood of the two classification tasks, namely Entity Fragment Recognition and Fragment Relation Prediction:\nL = − ∑\nα log p1(yent) + β log p2(yrel), (6)\nwhere yent and yrel denote the corresponding goldstandard labels for text spans and span pairs, α and β are the weights to control the task importance. During training, we use the BertAdam algorithm (Devlin et al., 2019) with the learning rate 5× 10−5 to finetune BERT and 1× 10−3 to finetune other parts of our model. The training process would terminate if the performance does not increase by 15 epochs."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Datasets: To evaluate our model for simultaneously recognizing overlapped and discontinuous entities, we follow prior work (Muis and Lu, 2016; Wang and Lu, 2019; Dai et al., 2020) and employ the data, called CLEF, from the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al., 2013), which consists of 199 and 99 clinical notes for training and testing. Note that Dai et al. (2020) used the full CLEF dataset in their experiments (179 for training, 20 for development and 99 for testing), while Muis and Lu (2016) and Wang and Lu (2019) used a subset of the union of the CLEF dataset and SemEval 2014 Task 7 (Pradhan et al.,\n6Please refer to Appendix C for the effect of multi-task learning.\n2014). Concretely, they used the training set and test set of the ShARe/CLEF eHealth Evaluation Lab 2013 as the training and development set, and they also used the development set of the SemEval 2014 Task 7 as the test set. In addition, they selected only the sentences that contain at least one discontinuous entity. Finally, the training, development and test sets contain 534, 303 and 430 sentences, respectively. We call this dataset as CLEF-Dis in this paper. Moreover, we also follow Dai et al. (2020) to evaluate models using the CADEC dataset proposed by Karimi et al. (2015). We follow the setting of Dai et al. (2020) to split the dataset and conduct experiments.\nTo show our model is comparable with the stateof-the-art models for overlapped NER, we conduct experiments on GENIA (Kim et al., 2003) and ACE05. For the GENIA and ACE05 datasets, we employ the same experimental setting in previous works (Lu and Roth, 2015; Muis and Lu, 2017; Wang and Lu, 2018; Luan et al., 2019), where 80%, 10% and 10% sentences in 1,999 GENIA documents, and the sentences in 370, 43 and 51 ACE05 documents are used for training, development and test, respectively. The statistics of all the datasets we use in this paper is shown in Table 1.\nEvaluation Metrics: In terms of evaluation metrics, we follow prior work (Lu and Roth, 2015; Muis and Lu, 2016; Wang and Lu, 2018, 2019) and employ the precision (P), recall (R) and F1-score (F1). A predicted entity is counted as true-positive if its boundary and type match those of a gold entity. For a discontinuous entity, each span should match a span of the gold entity. All F1 scores reported in Section 5 are the mean values from five runs of the same setting.\nImplementation Details: For hyper-parameters and other details, please refer to Appendix D."
    }, {
      "heading" : "5 Results and Analyses",
      "text" : ""
    }, {
      "heading" : "5.1 Results on CLEF",
      "text" : "Table 2 shows the results on the CLEF dataset. As seen, Tang et al. (2013) and Tang et al. (2015) adapted the CRF model, which is usually used for flat NER, to overlapped and discontinuous NER. They modified the BIO label scheme to BIOHD and BIOHD1234, which use “H” to label overlapped entity segments and “D” to label discontinuous entity segments. Surprisingly, the recently-proposed transition-based model (Dai et al., 2020) does not perform better than the CRF model (Tang et al., 2015), which may be because Tang et al. (2015) have conducted elaborate feature engineering for their model. In contrast, our model outperforms all the strong baselines with at least about 5% margin in F1. Our model does not rely on feature engineering or manually-designed transitions, which is more suitable for modern end-to-end learning.\nWe further perform ablation studies to investigate the effect of dependency-guided GCN and the overlapping relation, which can be removed without influencing our major goal. As shown in Table 2, after removing either of them, the F1 scores\n7Dai et al. (2020) found that BERT did not perform better than ELMo in their experiments.\ngo down by 0.7% and 1.0%. The observation suggests that both dependency-guided GCN and the overlapping relation are effective for our model. Moreover, after we replace BERT with the word embeddings pretrained on PubMed (Chiu et al., 2016), the F1 score goes down by 4.6%, which demonstrates that BERT plays an important role in our model."
    }, {
      "heading" : "5.2 Results on CLEF-Dis",
      "text" : "Table 3 shows the results on the CLEF-Dis dataset. As seen, our model outperforms the previous best model (Dai et al., 2020) by 0.4% in F1, which indicates that our model is very competitive, leading to a new state-of-the-art result on the dataset. Similarly, we further perform ablation studies to investigate the effect of dependency-guided GCN, the overlapping relation and BERT on this dataset. As shown, after removing either of the GCN or overlapping relation, the F1 score decreases by 0.4% or 0.7%, which is consistent with the observations in Table 2. In addition, to fairly compare with Wang and Lu (2019), we also replace BERT with the word embeddings pretrained on PubMed (Chiu et al., 2016). As we can see, our model also outperforms their model by 0.3%."
    }, {
      "heading" : "5.3 Results on CADEC",
      "text" : "As shown in Table 4, Metke-Jimenez and Karimi (2016) employed the similar method in (Tang et al., 2013) by expanding the BIO label scheme to BIOHD. Tang et al. (2018) also experimented the BIOHD label scheme, but they found that the result of the BIOHD-based method was slightly worse than that of the “Multilabel” method (65.5% vs. 66.3% in F1). Compared with the method in (Metke-Jimenez and Karimi, 2016), the performance improvement might be mainly because they used deep neural networks (e.g., LSTM) instead of shallow non-neural models.\nCompared with the above baselines, the transition-based model Dai et al. (2020) is still the best. Our full model slightly outperforms the transition-based model by 0.5%. In this dataset, we do not observe mutual benefit between the dependency-guided GCN and overlapped relation prediction modules, since our model achieves better results when using them separately (69.9%) than using them jointly (69.5%). However, when using them separately, the F1 is still 0.6% higher than the one using neither of them. Without BERT, the performance of our model drops by about 3% but it is still comparable with the performances of the methods without contextualized representations."
    }, {
      "heading" : "5.4 Result Analysis based on Entity Types",
      "text" : "Comparing with BiLSTM-CRF To show the necessity of building one model to recognize regular, overlapped and discontinuous entities simultaneously, we analyze the predicted entities in the CLEF-Dis dataset and classify them based on their types, as shown in Figure 4. In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly. For a fair comparison, we replace BERT with Glove (Pennington et al., 2014) and keep the setting of our model the same with the setting of the BiLSTM-CRF model used in previous work (Yang et al., 2018).\nAs seen, if only considering regular entities, the\n8Many discontinuous entities are also overlapped, but we do not count them as overlapped entities in this figure.\nBiLSTM-CRF model can achieve a better performance compared with our model, especially the precision value is much higher. One likely reason might be that the BiLSTM-CRF model is capable of using the label dependence to detect entity boundaries accurately, ensuring the correctness of the recognized entities, which is closely related to the precision. Nevertheless, our model can lead to higher recall, which reduces the gap between the two models.\nIf considering both regular and overlapped entities, the recall of our model is greatly boosted, and thus the F1 increases concurrently. If both regular and discontinuous entities are included, the performance of our model rises significantly to 50.9% due to the large scale of discontinuous entities. When all types of entities are concerned, the F1 of our model further increases by 0.8%, indicating the effectiveness of our model in joint recognition of overlapped, discontinuous and regular entities.\nComparing with the Transition-Based Model As shown in Figure 5, we also compare our model with the transition-based model (Dai et al., 2020) based on entity types by analyzing the results from one run of experiments. Note that since we do not tune the hyper-parameters of the transition-based model elaborately, the performance is not as good as the one that they have reported. As seen, our model performs better in all of the four groups, namely regular, regular+overlapped, regular+discontinuous, regular+overlapped+discontinuous entity recognition. However, based on the observation on the bars in different groups, we find that the main superiority\nof our model comes from regular entity recognition. In recognizing overlapped entities, our model is comparable with the transition-based model, but in recognizing discontinuous entities, our model performs slightly worse than the transition-based model. This suggests that a combination of spanbased and transition-based models may be a potential method for future research."
    }, {
      "heading" : "5.5 Results on GENIA and ACE05",
      "text" : "Table 5 shows the results of the GENIA and ACE05 datasets, which include only regular and overlapped entities. Our final model achieves 77.8% and 83.0% F1s in the GENIA and ACE05 datasets, respectively. By removing the dependency-guided GCN, the model shows an averaged decrease of 0.4%, indicating the usefulness of dependency syntax information. The finding is consistent with that of the CLEF dataset. Interestingly, we note that the overlapping relation also brings a positive influence in this setting. Actually, the relation extraction architecture is not necessary for only regular and overlapped entities, because the decoding can be finished after the first entity fragment recognition step. The observation doubly demonstrates the advantage of our final model. We also compare our results with several state-of-the-art results of the previous work on the two datasets in Table 5. Only the studies with the same training, development and test divisions are listed. We can see that our model can achieve very competitive performances\non both datasets. Note that Luan et al. (2019) and Wadden et al. (2019) use extra coreference resolution information, and Straková et al. (2019) exploit much richer word representations by a combination of ELMo, BERT and Flair."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we proposed an efficient and effective model to recognize both overlapped and discontinuous entities simultaneously, which can be applied to any NER dataset theoretically, since no extra assumption is required to limit the type of named entities. First, we enumerate all spans in a given sentence to determine whether they are valid entity fragments, and then relation classifications are performed to check the relationships between all fragment pairs. The results show that our model is highly competitive to the state-of-the-art models for overlapped or discontinuous NER. We have conducted detailed studies to help comprehensive understanding of our model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their comments and recommendation. This work is supported by the National Natural Science Foundation of China (No. 61772378), the National Key Research and Development Program of China (No. 2017YFC1200500), the Research Foundation of Ministry of Education of China (No. 18JZD015)."
    }, {
      "heading" : "A Comparing Different Settings in the Word Representation Layer",
      "text" : "The word representation layer addresses the problem that how to transform a word into a vector for the usage of upper layers. In this paper, we investigate several common word encoders in recent NLP research to generate word representations, namely Word2Vec (Mikolov et al., 2013) (or its variants such as Glove (Pennington et al., 2014)), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Given an input sentence x = {x1, x2, ..., xN}, we use different methods to represent them as vectors based on which word encoders we utilize:\n• If Word2Vec is used, each word xi will be directly transformed into a vector hi according to the pretrained embedding lookup table. Therefore, all the words in the sentence x correspond to a matrix H = {h1, h2, ..., hN} ∈ RN×dh , where dh denotes the dimension of hi.\n• If ELMo is used, each word xi will first be split into characters and then input into character-level convolutional networks to obtain character-level word representations. Finally, all word representations in the sentence will be input into 3-layer BiLSTMs to generate contextualized word representations, which can also be denoted as H = {h1, h2, ..., hN}\n• If BERT is used, each word xi will be converted into word pieces and then fed into a pretrained BERT module. After the BERT calculation, each sentential word may involve vectorial representations of several pieces. Here we employ the representation of the beginning word piece as the final word representation following (Wadden et al., 2019). For instance,\nif “fevers” is split into “fever” and “##s”, the representation of “fever” is used as the whole word representation. Therefore, all the words in the sentence x can also be represented as a matrix H = {h1, h2, ..., hN}\nIn addition, a bidirectional LSTM (BiLSTM) layer can be stacked on word encoders to further capture contextual information in the sentence, which is especially helpful for non-contextualized word representations such as Word2Vec. Concretely, the word representations H = {h1, h2, ..., hN} will be input into the BiLSTM layer and consumed in the forward and backward orders. Assuming that the outputs of the forward and backward LSTMs are −→ H = { −→ h 1, −→ h 2, ..., −→ hN} and ←− H = {\n←− h 1, ←− h 2, ...,←− hN} respectively. Thus, they can be concatenated (e.g., ĥi = [ −→ h i, ←− h i]) to compose the final word representations Ĥ = {ĥ1, ĥ2, ..., ĥN}. We investigate the effects of different word encoders and the BiLSTM layer in the experiments. As shown in Table 6, we compare the effects of different word representation methods in the CLEF and CLEF-Dis datasets, where the size of the former one is much bigger than that of the latter, in order to also investigate the impact of the data size on word representations. From the table, the first observation is that BERT is the most effective word representation method. Surprisingly, Word2Vec is more effective than ELMo, which may be because ELMo is exclusively based on characters and cannot effectively capture the whole meanings of words. Therefore, this suggests that it is better to use ELMo with Word2Vec.\nSecond, we find that BiLSTM is helpful in all cases, especially for Word2Vec. This may be because Word2Vec is a kind of non-contexualized word representations, which particularly needs the help of BiLSTM to capture contexual information. In contrast, BERT is not very sensitive to the help of BiLSTM as Word2Vec and ELMo, which may be because the transformer in BERT has already captured contexual information.\nThird, we observe that the effect of BiLSTM is more obvious for the CLEF-Dis dataset. Considering the data sizes of the CLEF and CLEF-Dis datasets, it is more likely that small datasets need the help of BiLSTM, while big datasets are less sensitive to the BiLSTM and BERT is usually enough for them to build word representations."
    }, {
      "heading" : "B Case Studies",
      "text" : "To understand how syntax information helps our model to identify discontinuous or overlapped entities, we offer two examples in the CLEF dataset for illustration, as shown in Table 7. Both the two examples are failed in the model without using dependency information, but are correctly recognized in our final model. In the first example, the fragments “displaced” and “fracture” of the same entity are far away from each other in the original sentence, while they are directly connected in the dependency graph. Similarly, in the second example, the distance between “Tone” and “decreased” is 9 in the sentence, while their dependency distance is only 1. These dependency connections can be directly modeled in dependency-guided GCN, thus, resulting in strong clues for the NER, which makes our final model work."
    }, {
      "heading" : "C Effect of Joint Training",
      "text" : "As mentioned in Section 3.5, we employ multi-task learning to jointly train our model between two tasks, namely entity fragment recognition and fragment relation prediction. Therefore, it is interesting to show the effect of joint training by observing the performance changes of the entity fragment recognition (EFR) task before and after adding the fragment relation prediction (FRP) task. As seen in Table 8, the F1 of entity fragment recognition increases by 0.3% after adding the FRP task, which shows that the FRP task could improve the EFR\ntask. This suggests that the interaction between entity fragment recognition and fragment relation prediction could benefit our model, which also indicates that end-to-end modeling is more desirable.\nD Implementation Details\nOur model is implemented based on AllenNLP (Gardner et al., 2018). The number of parameters is about 117M plus BERT. We use one GPU of NVIDIA Tesla V100 to train the model, which occupies about 10GB memories. The training time for one epoch is between 2∼6 minutes on different datasets.\nTable 9 shows the main hyper-parameter values in our model. We tune the hyper-parameters based on the results of about 5 trials on development sets. Below are the ranges tried for the hyper-parameters: the GCN layer l (1, 2), the GCN head Nhead (2, 4), the GCN output size df (20, 48, 64), the MLP layer (1, 2), the MLP size (100, 150, 200), the loss weight α and β (0.6, 0.8, 1.0). Since we employ the BERTBASE , the dimension dh of word representations is 768 except in the CLEF and CADEC\ndatasets, where we use a BiLSTM layer on top of BERT to obtain word representations since we observe performance improvements. We try 200 and 400 hidden units for the BiLSTM layer.\nConsidering the domains of the datasets, we employ clinical BERT1 (Alsentzer et al., 2019), SciBERT2 (Beltagy et al., 2019) and Google BERT3 (Devlin et al., 2019) for the CLEF (and CADEC), GENIA and ACE05 datasets, respectively. In addition, since our model needs syntax information for dependency-guided GCN, but the datasets do not contain gold syntax annotations, we utilize the Stanford CoreNLP toolkit (Manning et al., 2014) to perform dependency parsing.\n1https://github.com/EmilyAlsentzer/clinicalBERT 2https://github.com/allenai/scibert 3https://github.com/google-research/bert"
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Publicly available clinical bert embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jindi", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "SciBERT: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Low-resource name tagging learned with weakly labeled data",
      "author" : [ "Yixin Cao", "Zikun Hu", "Tat-seng Chua", "Zhiyuan Liu", "Heng Ji." ],
      "venue" : "Proceedings of the 2019 Conference on EMNLP, pages 261–270.",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "How to train good word embeddings for biomedical nlp",
      "author" : [ "Billy Chiu", "Gamal Crichton", "Anna Korhonen", "Sampo Pyysalo." ],
      "venue" : "Proceedings of the 15th workshop on biomedical natural language processing, pages 166–174.",
      "citeRegEx" : "Chiu et al\\.,? 2016",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2016
    }, {
      "title" : "An effective transition-based model for discontinuous NER",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Ben Hachey", "Cecile Paris." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5860–5870, Online. Association",
      "citeRegEx" : "Dai et al\\.,? 2020",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating non-local information into information extraction systems by gibbs sampling",
      "author" : [ "Jenny Rose Finkel", "Trond Grenager", "Christopher Manning." ],
      "venue" : "Proceedings of the 43rd annual meeting on association for computational linguistics,",
      "citeRegEx" : "Finkel et al\\.,? 2005",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "Nested named entity recognition",
      "author" : [ "Jenny Rose Finkel", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2009 Conference on EMNLP, pages 141–150.",
      "citeRegEx" : "Finkel and Manning.,? 2009",
      "shortCiteRegEx" : "Finkel and Manning.",
      "year" : 2009
    }, {
      "title" : "Merge and label: A novel neural network architecture for nested ner",
      "author" : [ "Joseph Fisher", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the ACL, pages 5840–5850.",
      "citeRegEx" : "Fisher and Vlachos.,? 2019",
      "shortCiteRegEx" : "Fisher and Vlachos.",
      "year" : 2019
    }, {
      "title" : "Factorizing complex models: A case study in mention detection",
      "author" : [ "Radu Florian", "Hongyan Jing", "Nanda Kambhatla", "Imed Zitouni." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the As-",
      "citeRegEx" : "Florian et al\\.,? 2006",
      "shortCiteRegEx" : "Florian et al\\.",
      "year" : 2006
    }, {
      "title" : "Allennlp: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke S. Zettlemoyer." ],
      "venue" : "Proceedings of Workshop",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention guided graph convolutional networks for relation extraction",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Wei Lu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Detection of entity mentions occurring in english and chinese text",
      "author" : [ "Kadri Hacioglu", "Benjamin Douglas", "Ying Chen." ],
      "venue" : "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Hacioglu et al\\.,? 2005",
      "shortCiteRegEx" : "Hacioglu et al\\.",
      "year" : 2005
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Dependency-guided lstm-crf for named entity recognition",
      "author" : [ "Zhanming Jie", "Wei Lu." ],
      "venue" : "Proceedings of the 2019 Conference on EMNLP, pages 3853–3863.",
      "citeRegEx" : "Jie and Lu.,? 2019",
      "shortCiteRegEx" : "Jie and Lu.",
      "year" : 2019
    }, {
      "title" : "Efficient dependency-guided named entity recognition",
      "author" : [ "Zhanming Jie", "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Thirty-First AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Jie et al\\.,? 2017",
      "shortCiteRegEx" : "Jie et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural layered model for nested named entity recognition",
      "author" : [ "Meizhi Ju", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the ACL, pages 1446–1459.",
      "citeRegEx" : "Ju et al\\.,? 2018",
      "shortCiteRegEx" : "Ju et al\\.",
      "year" : 2018
    }, {
      "title" : "Cadec: A corpus of adverse drug event annotations",
      "author" : [ "Sarvnaz Karimi", "Alejandro Metke-Jimenez", "Madonna Kemp", "Chen Wang." ],
      "venue" : "Journal of biomedical informatics, 55:73–81.",
      "citeRegEx" : "Karimi et al\\.,? 2015",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2015
    }, {
      "title" : "Nested named entity recognition revisited",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the ACL, pages 861–871.",
      "citeRegEx" : "Katiyar and Cardie.,? 2018",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2018
    }, {
      "title" : "Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182",
      "author" : [ "J-D Kim", "Tomoko Ohta", "Yuka Tateisi", "Jun’ichi Tsujii" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling" ],
      "venue" : null,
      "citeRegEx" : "Kipf and Welling.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "Proceedings of the eighteenth international conference on machine learning, ICML,",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the NAACL, pages 260–270.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified MRC framework for named entity recognition",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849–",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-nuggets: Nested entity mention detection via anchor-region networks",
      "author" : [ "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5182–5192.",
      "citeRegEx" : "Lin et al\\.,? 2019a",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Reliability-aware dynamic feature composition for name tagging",
      "author" : [ "Ying Lin", "Liyuan Liu", "Heng Ji", "Dong Yu", "Jiawei Han." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the ACL, pages 165– 174.",
      "citeRegEx" : "Lin et al\\.,? 2019b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-grained entity recognition",
      "author" : [ "Xiao Ling", "Daniel S Weld." ],
      "venue" : "Twenty-Sixth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ling and Weld.,? 2012",
      "shortCiteRegEx" : "Ling and Weld.",
      "year" : 2012
    }, {
      "title" : "Empower sequence labeling with task-aware neural language model",
      "author" : [ "Liyuan Liu", "Jingbo Shang", "Xiang Ren", "Frank Fangzheng Xu", "Huan Gui", "Jian Peng", "Jiawei Han." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial multi-task learning for text classification",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the ACL, pages 1–10.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Recognizing named entities in tweets",
      "author" : [ "Xiaohua Liu", "Shaodian Zhang", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 359–367.",
      "citeRegEx" : "Liu et al\\.,? 2011",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2011
    }, {
      "title" : "Joint mention extraction and classification with mention hypergraphs",
      "author" : [ "Wei Lu", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on EMNLP, pages 857–867.",
      "citeRegEx" : "Lu and Roth.,? 2015",
      "shortCiteRegEx" : "Lu and Roth.",
      "year" : 2015
    }, {
      "title" : "A general framework for information extraction using dynamic span graphs",
      "author" : [ "Yi Luan", "Dave Wadden", "Luheng He", "Amy Shah", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the ACL,",
      "citeRegEx" : "Luan et al\\.,? 2019",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th ACL, pages 1064–1074.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "Proceedings of 52nd annual meeting of the ACL: system demonstrations, pages",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Concept identification and normalisation for adverse drug event discovery in medical forums",
      "author" : [ "Alejandro Metke-Jimenez", "Sarvnaz Karimi." ],
      "venue" : "BMDID@ ISWC. Citeseer.",
      "citeRegEx" : "Metke.Jimenez and Karimi.,? 2016",
      "shortCiteRegEx" : "Metke.Jimenez and Karimi.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to recognize discontiguous entities",
      "author" : [ "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Proceedings of the 2016 Conference on EMNLP, pages 75–84.",
      "citeRegEx" : "Muis and Lu.,? 2016",
      "shortCiteRegEx" : "Muis and Lu.",
      "year" : 2016
    }, {
      "title" : "Labeling gaps between words: Recognizing overlapping mentions with mention separators",
      "author" : [ "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Proceedings of the 2017 Conference on EMNLP, pages 2608–2618.",
      "citeRegEx" : "Muis and Lu.,? 2017",
      "shortCiteRegEx" : "Muis and Lu.",
      "year" : 2017
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the ACL, pages",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Semeval-2014 task 7: Analysis of clinical text",
      "author" : [ "Sameer Pradhan", "Wendy Chapman", "Suresh Man", "Guergana Savova." ],
      "venue" : "Proc. of the 8th International Workshop on Semantic Evaluation (SemEval 2014. Citeseer.",
      "citeRegEx" : "Pradhan et al\\.,? 2014",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Neural architectures for nested ner through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the ACL, pages 5326–5331.",
      "citeRegEx" : "Straková et al\\.,? 2019",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the share/clef ehealth evaluation lab",
      "author" : [ "Hanna Suominen", "Sanna Salanterä", "Sumithra Velupillai", "Wendy W Chapman", "Guergana Savova", "Noemie Elhadad", "Sameer Pradhan", "Brett R South", "Danielle L Mowery", "Gareth JF Jones" ],
      "venue" : null,
      "citeRegEx" : "Suominen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Suominen et al\\.",
      "year" : 2013
    }, {
      "title" : "Recognizing disjoint clinical concepts in clinical text using machine learning-based methods",
      "author" : [ "Buzhou Tang", "Qingcai Chen", "Xiaolong Wang", "Yonghui Wu", "Yaoyun Zhang", "Min Jiang", "Jingqi Wang", "Hua Xu." ],
      "venue" : "AMIA annual symposium proceedings,",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognizing continuous and discontinuous adverse drug reaction mentions from social media using lstm-crf",
      "author" : [ "Buzhou Tang", "Jianglu Hu", "Xiaolong Wang", "Qingcai Chen." ],
      "venue" : "Wireless Communications and Mobile Computing, 2018.",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "Recognizing and encoding discorder concepts in clinical text using machine learning and vector space model",
      "author" : [ "Buzhou Tang", "Yonghui Wu", "Min Jiang", "Joshua C Denny", "Hua Xu." ],
      "venue" : "CLEF (Working Notes), 665.",
      "citeRegEx" : "Tang et al\\.,? 2013",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on EMNLP, pages 5788–5793.",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural segmental hypergraphs for overlapping mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of the 2018 Conference on EMNLP, pages 204–214.",
      "citeRegEx" : "Wang and Lu.,? 2018",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2018
    }, {
      "title" : "Combining spans into entities: A neural two-stage approach for recognizing discontiguous entities",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of the 2019 Conference on EMNLP, pages 6217–6225.",
      "citeRegEx" : "Wang and Lu.,? 2019",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2019
    }, {
      "title" : "A neural transition-based model for nested mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu", "Yu Wang", "Hongxia Jin." ],
      "venue" : "Proceedings of the 2018 Conference on EMNLP, pages 1011–1017.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-grained named entity recognition",
      "author" : [ "Congying Xia", "Chenwei Zhang", "Tao Yang", "Yaliang Li", "Nan Du", "Xian Wu", "Wei Fan", "Fenglong Ma", "S Yu Philip." ],
      "venue" : "Proceedings of the ACL, pages 1430–1440.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Design challenges and misconceptions in neural sequence labeling",
      "author" : [ "Jie Yang", "Shuailong Liang", "Yue Zhang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3879–3889.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transition-based neural word segmentation",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 421–431.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Graph convolution over pruned dependency trees improves relation extraction",
      "author" : [ "Yuhao Zhang", "Peng Qi", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2018 Conference on EMNLP, pages 2205–2215.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "A boundary-aware neural model for nested named entity recognition",
      "author" : [ "Changmeng Zheng", "Yi Cai", "Jingyun Xu", "Ho-fung Leung", "Guandong Xu." ],
      "venue" : "Proceedings of the 2019 Conference on EMNLP, pages 357–366.",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "A neural probabilistic structuredprediction model for transition-based dependency parsing",
      "author" : [ "Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 229
    }, {
      "referenceID" : 3,
      "context" : "Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 229
    }, {
      "referenceID" : 24,
      "context" : "Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001).",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 33,
      "context" : "However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1.",
      "startOffset" : 85,
      "endOffset" : 123
    }, {
      "referenceID" : 39,
      "context" : "However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1.",
      "startOffset" : 85,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 33,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 40,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 21,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 52,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 19,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 54,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 11,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 34,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 53,
      "context" : "There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : "Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 85,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "The hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al.",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 53,
      "context" : "The hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al.",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "The hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020) are flexible to be adapted for different tasks, achieving great successes for overlapped or discontinuous NER.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 57,
      "context" : "Moreover, these models build graphs or generate transitions along the words in the sentences gradually, which may lead to error propagation (Zhang et al., 2016).",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "The model utilizes BERT (Devlin et al., 2019) to produce deep contextualized word representations, and then enumerates all candidate text spans (Luan et al.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : ", 2019) to produce deep contextualized word representations, and then enumerates all candidate text spans (Luan et al., 2019), classifying whether they are entity fragments.",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 201
    }, {
      "referenceID" : 58,
      "context" : "To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 201
    }, {
      "referenceID" : 17,
      "context" : "To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 201
    }, {
      "referenceID" : 14,
      "context" : "To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 201
    }, {
      "referenceID" : 39,
      "context" : "The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al.",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al.",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 34,
      "context" : "Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Straková et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 177
    }, {
      "referenceID" : 51,
      "context" : "Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Straková et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 177
    }, {
      "referenceID" : 45,
      "context" : "Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Straková et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 177
    }, {
      "referenceID" : 30,
      "context" : "In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 131
    }, {
      "referenceID" : 28,
      "context" : "In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011).",
      "startOffset" : 84,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011).",
      "startOffset" : 84,
      "endOffset" : 146
    }, {
      "referenceID" : 32,
      "context" : "With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011).",
      "startOffset" : 84,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 60,
      "context" : "Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 42,
      "context" : "Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : ", 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : ", 2018) and BERT (Devlin et al., 2019) have also achieved great success.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture.",
      "startOffset" : 58,
      "endOffset" : 117
    }, {
      "referenceID" : 35,
      "context" : "As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture.",
      "startOffset" : 58,
      "endOffset" : 117
    }, {
      "referenceID" : 56,
      "context" : "As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture.",
      "startOffset" : 58,
      "endOffset" : 117
    }, {
      "referenceID" : 40,
      "context" : "Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 52,
      "context" : "Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 19,
      "context" : "Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 54,
      "context" : "Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 55,
      "context" : "Other approaches include multi-grained detection (Xia et al., 2019), boundary detection (Zheng et al.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 59,
      "context" : ", 2019), boundary detection (Zheng et al., 2019), anchorregion network (Lin et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : ", 2019), anchorregion network (Lin et al., 2019a) and machine reading comprehension (Li et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : ", 2019a) and machine reading comprehension (Li et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 45,
      "context" : "The stateof-the-art models for overlapped NER include the sequence-to-sequence (seq2seq) model (Straková et al., 2019), where the decoder predicts multiple",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 34,
      "context" : "labels for a word and move to next word until it outputs the “end of word” label, and the span-based model (Luan et al., 2019; Wadden et al., 2019), where overlapped entities are recognized by classification for enumerated spans.",
      "startOffset" : 107,
      "endOffset" : 147
    }, {
      "referenceID" : 51,
      "context" : "labels for a word and move to next word until it outputs the “end of word” label, and the span-based model (Luan et al., 2019; Wadden et al., 2019), where overlapped entities are recognized by classification for enumerated spans.",
      "startOffset" : 107,
      "endOffset" : 147
    }, {
      "referenceID" : 49,
      "context" : "Early studies addressed such problem by extending the BIO label scheme (Tang et al., 2013; MetkeJimenez and Karimi, 2016).",
      "startOffset" : 71,
      "endOffset" : 121
    }, {
      "referenceID" : 34,
      "context" : "Our differences are that our model is span-based (Luan et al., 2019) and it is also enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 58,
      "context" : ", 2019) and it is also enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al., 2018; Guo et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and it is also enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al., 2018; Guo et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Under the traditional statistical setting, syntax information is used by manually-crafted features (Hacioglu et al., 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al.",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "Under the traditional statistical setting, syntax information is used by manually-crafted features (Hacioglu et al., 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al.",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : ", 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al., 2006) for NER.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 58,
      "context" : "Jie and Lu (2019) stack the dependency-guided graph convolutional network (Zhang et al., 2018; Guo et al., 2019) on top of the BiLSTM layer.",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "Jie and Lu (2019) stack the dependency-guided graph convolutional network (Zhang et al., 2018; Guo et al., 2019) on top of the BiLSTM layer.",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "We exploit BERT (Devlin et al., 2019) as inputs for our model, which has demonstrated effective for a range of NLP tasks.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 51,
      "context" : "Here we employ the representation of the beginning word piece as the final word representation following (Wadden et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "Dependency syntax information has been demonstrated to be useful for NER previously (Jie and Lu, 2019).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "3 Graph convolutional network (GCN) (Kipf and Welling, 2017) is one representative method to encode dependency-based graphs, which has been shown effective in information extraction (Zhang et al.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 58,
      "context" : "3 Graph convolutional network (GCN) (Kipf and Welling, 2017) is one representative method to encode dependency-based graphs, which has been shown effective in information extraction (Zhang et al., 2018).",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "attention-guided GCN (AGGCN) (Guo et al., 2019) to reach our goal, as it can bring better performance compared with the standard GCN.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "In fact, A can be considered as a form of hard attention in GCN, while AGGCN (Guo et al., 2019) aims to improve the method by using A in the lower layers and updating A at the higher layers via multi-head self-attention (Vaswani et al.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 50,
      "context" : ", 2019) aims to improve the method by using A in the lower layers and updating A at the higher layers via multi-head self-attention (Vaswani et al., 2017) as below:",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 34,
      "context" : "We employ span enumeration (Luan et al., 2019) to generate text spans.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : "where w is a 20-dimensional embedding to represent the span width following previous work (Luan et al., 2019; Wadden et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 51,
      "context" : "where w is a 20-dimensional embedding to represent the span width following previous work (Luan et al., 2019; Wadden et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "During training, we employ multi-task learning (Caruana, 1997; Liu et al., 2017) to jointly train different parts of our model.",
      "startOffset" : 47,
      "endOffset" : 80
    }, {
      "referenceID" : 31,
      "context" : "During training, we employ multi-task learning (Caruana, 1997; Liu et al., 2017) to jointly train different parts of our model.",
      "startOffset" : 47,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "During training, we use the BertAdam algorithm (Devlin et al., 2019) with the learning rate 5× 10−5 to finetune BERT and 1× 10−3 to finetune other parts of our model.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 39,
      "context" : "Datasets: To evaluate our model for simultaneously recognizing overlapped and discontinuous entities, we follow prior work (Muis and Lu, 2016; Wang and Lu, 2019; Dai et al., 2020) and employ the data, called CLEF, from the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al.",
      "startOffset" : 123,
      "endOffset" : 179
    }, {
      "referenceID" : 53,
      "context" : "Datasets: To evaluate our model for simultaneously recognizing overlapped and discontinuous entities, we follow prior work (Muis and Lu, 2016; Wang and Lu, 2019; Dai et al., 2020) and employ the data, called CLEF, from the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al.",
      "startOffset" : 123,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "Datasets: To evaluate our model for simultaneously recognizing overlapped and discontinuous entities, we follow prior work (Muis and Lu, 2016; Wang and Lu, 2019; Dai et al., 2020) and employ the data, called CLEF, from the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al.",
      "startOffset" : 123,
      "endOffset" : 179
    }, {
      "referenceID" : 46,
      "context" : ", 2020) and employ the data, called CLEF, from the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al., 2013), which consists of 199 and 99 clinical notes for training and testing.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "To show our model is comparable with the stateof-the-art models for overlapped NER, we conduct experiments on GENIA (Kim et al., 2003) and ACE05.",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 33,
      "context" : "For the GENIA and ACE05 datasets, we employ the same experimental setting in previous works (Lu and Roth, 2015; Muis and Lu, 2017; Wang and Lu, 2018; Luan et al., 2019), where 80%, 10% and 10% sentences in 1,999 GENIA documents, and the sentences in 370, 43 and 51 ACE05 documents are used for training, development and test, respectively.",
      "startOffset" : 92,
      "endOffset" : 168
    }, {
      "referenceID" : 40,
      "context" : "For the GENIA and ACE05 datasets, we employ the same experimental setting in previous works (Lu and Roth, 2015; Muis and Lu, 2017; Wang and Lu, 2018; Luan et al., 2019), where 80%, 10% and 10% sentences in 1,999 GENIA documents, and the sentences in 370, 43 and 51 ACE05 documents are used for training, development and test, respectively.",
      "startOffset" : 92,
      "endOffset" : 168
    }, {
      "referenceID" : 52,
      "context" : "For the GENIA and ACE05 datasets, we employ the same experimental setting in previous works (Lu and Roth, 2015; Muis and Lu, 2017; Wang and Lu, 2018; Luan et al., 2019), where 80%, 10% and 10% sentences in 1,999 GENIA documents, and the sentences in 370, 43 and 51 ACE05 documents are used for training, development and test, respectively.",
      "startOffset" : 92,
      "endOffset" : 168
    }, {
      "referenceID" : 34,
      "context" : "For the GENIA and ACE05 datasets, we employ the same experimental setting in previous works (Lu and Roth, 2015; Muis and Lu, 2017; Wang and Lu, 2018; Luan et al., 2019), where 80%, 10% and 10% sentences in 1,999 GENIA documents, and the sentences in 370, 43 and 51 ACE05 documents are used for training, development and test, respectively.",
      "startOffset" : 92,
      "endOffset" : 168
    }, {
      "referenceID" : 33,
      "context" : "Evaluation Metrics: In terms of evaluation metrics, we follow prior work (Lu and Roth, 2015; Muis and Lu, 2016; Wang and Lu, 2018, 2019) and employ the precision (P), recall (R) and F1-score (F1).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 39,
      "context" : "Evaluation Metrics: In terms of evaluation metrics, we follow prior work (Lu and Roth, 2015; Muis and Lu, 2016; Wang and Lu, 2018, 2019) and employ the precision (P), recall (R) and F1-score (F1).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "Surprisingly, the recently-proposed transition-based model (Dai et al., 2020) does not perform better than the CRF model (Tang et al.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 47,
      "context" : ", 2020) does not perform better than the CRF model (Tang et al., 2015), which may be because Tang et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "Moreover, after we replace BERT with the word embeddings pretrained on PubMed (Chiu et al., 2016), the F1 score goes down by 4.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "As seen, our model outperforms the previous best model (Dai et al., 2020) by 0.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "In addition, to fairly compare with Wang and Lu (2019), we also replace BERT with the word embeddings pretrained on PubMed (Chiu et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 49,
      "context" : "As shown in Table 4, Metke-Jimenez and Karimi (2016) employed the similar method in (Tang et al., 2013) by expanding the BIO label scheme to BIOHD.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "Compared with the method in (Metke-Jimenez and Karimi, 2016), the performance improvement might be mainly because they used deep neural networks (e.",
      "startOffset" : 28,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly.",
      "startOffset" : 50,
      "endOffset" : 109
    }, {
      "referenceID" : 35,
      "context" : "In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly.",
      "startOffset" : 50,
      "endOffset" : 109
    }, {
      "referenceID" : 56,
      "context" : "In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly.",
      "startOffset" : 50,
      "endOffset" : 109
    }, {
      "referenceID" : 41,
      "context" : "For a fair comparison, we replace BERT with Glove (Pennington et al., 2014) and keep the setting of our model the same with the setting of the BiLSTM-CRF model used in previous work (Yang et al.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 56,
      "context" : ", 2014) and keep the setting of our model the same with the setting of the BiLSTM-CRF model used in previous work (Yang et al., 2018).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Comparing with the Transition-Based Model As shown in Figure 5, we also compare our model with the transition-based model (Dai et al., 2020) based on entity types by analyzing the results from one run of experiments.",
      "startOffset" : 122,
      "endOffset" : 140
    } ],
    "year" : 2021,
    "abstractText" : "Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.",
    "creator" : "LaTeX with hyperref package"
  }
}