{
  "name" : "2021.acl-long.329.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Probing Toxic Content in Large Pre-Trained Language Models",
    "authors" : [ "Nedjma Ousidhoum", "Xinran Zhao", "Tianqing Fang", "Yangqiu Song", "Dit-Yan Yeung" ],
    "emails" : [ "nousidhoum@cse.ust.hk,", "xzhaoar@connect.ust.hk,", "tfangaa@connect.ust.hk,", "yqsong@cse.ust.hk,", "dyyeung@cse.ust.hk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4262–4274\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4262"
    }, {
      "heading" : "1 Introduction",
      "text" : "The recent gain in size of pre-trained language models (PTLMs) has had a large impact on state-of-theart NLP models. Although their efficiency and usefulness in different NLP tasks is incontestable, their shortcomings such as their learning and reproduction of harmful biases cannot be overlooked and ought to be addressed. Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al., 2021). Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al., 2019; Lin et al.,\n2020). However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups.\nIn this paper, we present an extensive study which examines the generation of harmful content by PTLMs. First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap et al., 2019b). Then, we use a PTLM to predict possible reasons for these actions. We look into how BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-2 (Radford et al., 2019) associate unrelated and detrimental causes to basic everyday actions and examine how frequently the predicted words relate to specific social groups. Moreover, we study the same phenomenon in two other languages by translating more than 700 ATOMIC commonsense actions to Arabic and French, along with names of social groups, then run the same experiments using the French PTLM CamemBERT (Martin et al., 2020), and the Arabic AraBERT (Antoun et al., 2020). We find that, overall, the predicted content can also be irrelevant and offensive especially when the subject of the sentence is part of a marginalized community in the predominant culture of the language.\nIn order to gauge the generated toxicity by different language models, we train simple toxicity classifiers based on logistic regression using available hate speech and offensive language datasets. We reduce the classification bias using a two-step approach to first, filter out examples with identity words which typically lead classifiers to predict a toxic label, then perform a second classification step on the remaining examples. We further con-\nduct a human evaluation step on 100 automatically labeled examples in each language. Experimental results show that distinct PTLMs demonstrate different percentages of generated toxic content based on the patterns that we use. The human annotations confirm the existence of toxicity in English, French, and Arabic PTLMS and show that, despite their imperfections, the classifiers can be used as toxicity pointers.\nOur main contributions can be summarized in the following.\n• We perform a large-scale extensible study on toxic content in PTLMs without relying on datasets which are specific to such a task.\n• We quantify common misconceptions and wrongly attributed designations to people from different communities. This assessment can be taken into account when using a PTLM for toxic language classification, and when adopting a mitigation strategy in NLP experiments.\n• We develop a large dataset based on structured patterns that can later be used for the evaluation of toxic language classification and harmful content within PTLMs. We make our data resources publicly available to the community. 1\nThe rest of the paper is organized as follows. We first introduce our methodology in Section 2.\n1The link to code and data is https://github.com/ HKUST-KnowComp/Probing_toxicity_in_PTLMs.\nPATTERN\nIn Section 3, we present our probing experiments using classifiers and show frequent words that are generated by different PTLMs in order to demonstrate the spread of the existing toxicity across different languages, both quantitatively and qualitatively. Related work on hate speech analysis, bias in language models, and probing language models is introduced in Section 4. Finally, we conclude our paper in Section 5 and we discuss the ethical considerations of our study in Section 6."
    }, {
      "heading" : "2 Methodology",
      "text" : "We adopt a rule-based methodology based on Masked Language Modeling (MLM) in order to probe the toxicity of the content generated by different PTLMs.\nAs shown in Figure 1, we use a PTLM on a one token masked cloze statement which starts with the name of a social group, followed by an everyday action, and ends by a predicted reason of the action. Our goal is to provide a set of tests and a process to assess toxicity in PTLMs with regard to various social groups."
    }, {
      "heading" : "2.1 Probing Patterns",
      "text" : "We use the ATOMIC atlas of everyday commonsense reasoning based on if-then relations (Sap et al., 2019b) to create cloze statements to fill in. Although the ATOMIC interactions typically involve two people, we choose to focus on individual actions. Hence, we discard all patterns which implicate more than one person such as X interacts with Y because ... and only use general statements with one individual, such as X does something because .... We prompt the statements by the name of a social group and use gendered pronouns to evoke\nthe effect of the action. For the sake of normalizing English, French, and Arabic patterns2, we do not consider the pronoun they.\nAs shown in Table 1, we adapt X to be either a person, a man, or a woman. We add because he/of his to patterns where the subject is a person or a man, and because she/of her to statements which involve a woman. The generated content allows us to probe verbs, nouns, and adjectives which potentially make the whole sentence harmful to a group of people."
    }, {
      "heading" : "2.2 Lists of Social Groups",
      "text" : "The original PersonX and PersonY contained in the original ATOMIC patterns are insufficient to probe a PTLM with respect to present social entities and constructs. Slightly modified patterns such as ManX or WomanX give us an idea about the disparities between men and women only. Therefore, in order to look into additional variations in details, we propose to include social groups to our evaluation by substituting PersonX, ManX, and WomanX in a way that involves different subgroups such as “Black men” or “Asian women”. The subgroups share a general social attribute or a value system. Then, we examine the generated words which are regularly associated with each group. Table 2 contains examples of these subgroups."
    }, {
      "heading" : "2.3 The Generated Data",
      "text" : "We use a total of 1,000 ATOMIC heads for each language, 6,000 patterns for English and French, and\n2We acknowledge not being aware of the equivalent of they as a singular pronoun in Arabic and French.\n4,000 patterns for Arabic. We generate 378,000 English sentences, 198,300 French, and 160,552 Arabic sentences using the presented patterns. We notice in the examples shown in Table 3 that, when using a PTLM to reason about the possible intentions related to basic actions, stereotypical, confusing, and harmful content can easily be generated.\nFor instance, one would think that the most obvious reason to prepare dinner or to join the basketball team would not be a person’s ethnicity or religious affiliation in contrast to what is generated in the first two examples. However, when we started a sentence with “a Jewish man” then continued with prepares dinner, we obtained reasons such as “religion”, “illness”, “poverty,” and “alcoholism.” Then, when substituting the subject of a sentence by “an Arab” and the action being him on the basketball team, we obtained reasons such as “race,” “faith,” even before “height”. The case of a refugee woman going hiking is even worse, since most of the generated content is related to death and diseases, and the PTLM produces syntactically incoherent sentences where nouns such as tuberculosis, and asthma appear after the pronoun she.\nGiven the frequency of the observed incoherent and harmful content, we come up with a way to quantify how often they tend to be generated."
    }, {
      "heading" : "2.4 Probing Classifiers",
      "text" : "We propose to use simple toxic language classifiers despite their bias towards slurs and identity words (Sap et al., 2019a; Park et al., 2018; Ousidhoum et al., 2020). Due to the trade-off between explainability and performance we train simple logistic regression (LR) models rather than deep learning ones.\nWe trained an LR classifier on four relatively different English datasets (Davidson et al., 2017; Founta et al., 2018; Ousidhoum et al., 2019; Zampieri et al., 2019), four others in Arabic (Ousidhoum et al., 2020; Albadi et al., 2018; Mulki et al., 2019; Zampieri et al., 2020), and the only one we know about in French (Ousidhoum et al., 2019). Table 4 shows the performance of the LR classifiers on the test splits of these datasets respectively. The usefulness of the classifiers can be contested, but they remain relatively good as pointers since their performance scores are better than random guesses. We use the three classifiers in order to assess different PTLMs, compare the extent to which toxicity"
    }, {
      "heading" : "TR drunk, singing, lying, old, a dog",
      "text" : "can be generated despite the benign commonsense actions and simple patterns we make use of."
    }, {
      "heading" : "2.5 Bias in Toxic Language Classifiers",
      "text" : "Toxic language classifiers show an inherent bias towards certain terms such as the names of some social groups which are part of our patterns (Sap et al., 2019a; Park et al., 2018; Hutchinson et al., 2020). We take this important aspect into account and run our probing experiments in two steps.\nIn the first step, we run the LR classifier on cloze statements which contain patterns based on different social groups and actions without using the generated content. Then, we remove all the patterns which have been classified as toxic. In the second step, we run our classifier over the full generated sentences with only patterns which were not labeled toxic. In this case, we consider the toxicity of a sentence given the newly PTLM-introduced con-\ntent. Finally, we compare counts of potentially incoherent associations produced by various PTLMs in English, French and Arabic."
    }, {
      "heading" : "3 Experiments",
      "text" : "We use the HuggingFace (Wolf et al., 2020) to implement our pipeline which, given a PTLM, outputs a list of candidate words and their probabilities. The PTLMs we use are BERT, RoBERTa, GPT-2, CamemBERT, and AraBERT."
    }, {
      "heading" : "3.1 Main Results",
      "text" : "We present the main results based on the proportions of toxic statements generated by different PTLMs in Table 5. In the first step, 9.55%, 83.55%, and 18.25% of the English, French, and Arabic sentences to be probed were filtered out by the toxic language classifiers.\nAs we only have one relatively small dataset on which we train our French LR classifier, the classifier shows more bias and is more sensitive to the existence of keywords indicating social groups. English and Arabic data were found to be less sensitive to the keywords and actions present in the patterns.\nAfter filtering out the toxic patterns that our classifier labeled as offensive, we fed the sentences generated from the remaining patterns to be labeled by the toxic language classifiers. The overall results for three PTLMs in English and the two Arabic and French PTLMs are shown in Table 5. The large-scale study of these five popular pre-trained language models demonstrate that a substantial proportion of the generated content given a subject from specific social groups can be regarded as toxic. Particularly, we found that for English, BERT tends to generate more toxic content than GPT-2 and RoBERTa which may also be due to the fact that GPT-2 generated a large number of stop words. Although the French PTLM Camem-\nBERT seems to produce more toxic content than the Arabic and English PTLMs, it may only be due to the fact that we are assessing less samples in French after the first filtering step. Hence, we need additional evidence to be more assertive.\nWe study the social groups to which PTLMs associate potential toxicity in Table 6. The outcome is consistent with the overall results in Table 5. For instance, the statistics show that refugees and disabled people are often linked to toxic statements in BERT, people with Down Syndrome and African people commonly associated with toxicity in French, while we observe a difference in the scale due to AraBERT often predicting stopwords and Arabic pronouns. Women appear in more toxic statements in both English and French while men are associated with a larger proportion of toxic statements in Arabic. Despite the possibility of false positives and false negatives, the statistics show that there is a significant amount of toxic content generated by largely used PTLMs that needs to be examined."
    }, {
      "heading" : "3.2 Human Evaluation",
      "text" : "We randomly sample 100 generated sentences and ask five domain experts to annotate them as insulting, stereotypical, confusing, or normal."
    }, {
      "heading" : "3.2.1 labels",
      "text" : "We ask the annotators to label the generated content as stereotypical, insulting, confusing, or normal.\nStereotypical A stereotype is an overgeneralized belief about a particular social group. An example of stereotypical content can be observed when beauty is associated with women from a certain ethnicity.\nInsulting A generated insulting statement can consist of a direct insult regardless of the context such as names of animals associated with social (X is a dog). Other indirect insulting statements depend on the context of the statement, such as saying that someone received a job offer because of their ethnicity, religion, or gender and not due to their abilities.\nConfusing A statement is labeled confusing when annotators cannot decide on whether the statement is problematic or not due to a lack of information. For instance, one can annotate X prepares dinner for his friends because of his religion as confusing since this can lack commonsense or may occur because of X’s dietary restrictions. However, the annotator could not decide due to the lack of context. Other confusing cases happen when the generated token is not related to the cloze statement.\nNormal When the generated content sounds normal."
    }, {
      "heading" : "3.2.2 Discussion",
      "text" : "As shown in Table 7, many instances were considered toxic by the annotators. Some examples of\nEnglish negative sentences that the five annotators agreed on are a black woman makes a snack because of her poverty and a disabled man tells mom because of his blindness.\nThis further demonstrates that there can be more toxic instances in PTLM-generated texts, even if our two-step approach for automatic evaluation tries to filter out patterns that are considered toxic by the classifiers.\nDespite prompting the generation task with simple statements, the relative bias of toxic language classifiers can still be observed.\nIn addition, harvesting the generated data by breaking a given sentence into a subject, action, and reason which corresponds to the unmasked token to guide the classification process, allowed us to counter a considerable portion of false positives. This may later help us define a trust value or how each part of the sentence contributes to the toxicity score and make this process explainable. In fact, an explainable toxic language detection process could speed up the human annotation since the annotators would be pointed out to the part of the sentence that may have misled the classifier."
    }, {
      "heading" : "3.3 Frequent Content in English",
      "text" : "We show examples of potentially harmful yet relatively informative descriptive nouns and adjectives which appear as Top-1 predictions in Table 8. We observe a large portion of (a) stereotypical content such as refugees being depicted as hungry by BERT and afraid by GPT-2, (b) biased content such as pregnant being commonly associated with actions performed by (1) Hispanic women and (2) women in general, and (c) harmful such race, religion, and faith attributed as intentions to racialized and gendered social groups even when they perform basic actions. This confirms that PTLM-generated content can be strongly associated with words biased towards social groups which can also help with an explanability component for toxic language analysis in PTLMs.\nIn fact, we can also use these top generated words coupled as strongly attached words as anchors to further probe other data collections or evaluate selection bias for existing toxic content analysis datasets (Ousidhoum et al., 2020)."
    }, {
      "heading" : "3.4 Frequent Content in French and Arabic",
      "text" : "Similarly to Table 8, Table 9 shows biased content generated by Arabic and French PTLMs. We observe similar biased content about women with the\nTop Social Groups Top Biased Top-1 Freq\nBERT\nHispanic women, women pregnant 22,546 Jewish, Muslim people religion 15,449\nBlack, white people race 14,889 Atheists, Buddhists faith 14,652\nRussian, Hindu women beauty 9,153 Leftists, Immigrants work 8,712\nImmigrants, Muslims poor 8,604 Disabled people, Buddhists illness 6,994\nDisabled, trans people disability 6,492 Refugees, Brown people hungry 6,361\nRoBERTa\nAtheists, Muslims religion 15,799 Refugees, Indian people hungry 13,564 Disabled, trans people disability 10,556\nEuropean, Russian people job 9,671 Atheists, Christians faith 8,604\nWomen, Men lonely 6,493 White, Black people race 5,780 African people, Immigrants poor 5,666 Refugees, Immigrants fear 3,089\nBuddhists, Hindus happy 5,100\nGPT-2\ncommon word pregnant in both French and Arabic, in addition to other stereotypical associations such as gay and Asian men being frequently depicted as drunk in Arabic, and Chinese and Russian men as rich in French. This confirms our previous findings in multilingual settings."
    }, {
      "heading" : "3.5 A Case Study On offensive Content Generated by PTLMs",
      "text" : "When generating Arabic data, in addition to stereotypical, biased, and generally harmful content, we have observed a significant number of names of animals often seen in sentences where the subject is a member of a commonly marginalized social group in the Arabic-speaking world such as foreign\nmigrants3. Table 10 shows names of animals with, usually, a bad connotation in the Arabic language.\nBesides showing a blatant lack of commonsense in Arabic cause-effect associations, we observe that such content is mainly coupled with groups involving people from East-Africa, South-East Asia, and the Asian Pacific region. Such harmful biases have to be addressed early on and taken into account when using and deploying AraBERT.\n3https://pewrsr.ch/3jbIkQm"
    }, {
      "heading" : "4 Related Work",
      "text" : "The large and incontestable success of BERT (Devlin et al., 2019) revolutionized the design and performance of NLP applications. However, we are still investigating the reasons behind this success with the experimental setup side (Rogers et al., 2020; Prasanna et al., 2020). Classification models are typically fine-tuned using PTLMs to boost their performance including hate speech and offensive language classifiers (Aluru et al., 2020; Ranasinghe and Zampieri, 2020). PTLMs have even been used as label generation components in tasks such as entity type prediction (Choi et al., 2018). This work aims to assess toxic content in large PTLMs in order to help with the examination of elements which ought to be taken into account when adapting the formerly stated strategies during the fine-tuning process.\nSimilarly to how long existing stereotypes are deep-rooted in word embeddings (Papakyriakopoulos et al., 2020; Garg et al., 2018), PTLMs have also been shown to recreate stereotypical content due to the nature of their training data (Sheng et al., 2019) among other reasons. Nadeem et al. (2020); Tay et al. (2020); Forbes et al. (2020); Sheng et al. (2019) have introduced datasets to evaluate the stereotypes they incorporate. On the other hand, Ettinger (2020) introduced a series of psycholinguistic diagnosis tests to evaluate what PTLMs are not designed for, and Bender et al. (2021) thoroughly surveyed their impact in the short and long terms.\nDifferent probing experiments have been proposed to study the drawbacks of PTLMs in areas such as the biomedical domain (Jin et al., 2019), syntax (Hewitt and Manning, 2019; Marvin and Linzen, 2018), semantic and syntactic sentence structures (Tenney et al., 2019), prenomial anaphora (Sorodoc et al., 2020), common-\nsense (Petroni et al., 2019), gender bias (Kurita et al., 2019), and typicality in judgement(Misra et al., 2021). Except for Hutchinson et al. (2020) who examine what words BERT generate in some fill-in-the-blank experiments with regard to people with disabilities, and more recently Nozza et al. (2019) who assess hurtful auto-completion by multilingual PTLMs, we are not aware of other strategies designed to estimate toxic content in PTLMs with regard to several social groups. In this work, we are interested in assessing how PTLMs encode bias towards different communities.\nBias in social data is a broad concept which involves several issues and formalism (Kiritchenko and Mohammad, 2018; Olteanu et al., 2019; Papakyriakopoulos et al., 2020; Blodgett et al., 2020). For instance, Shah et al. (2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019a), selection bias (Garimella et al., 2019; Ousidhoum et al., 2020), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al., 2018). Other work investigate the effect of data splits (Gorman and Bedrick, 2019) and mitigation strategies (Dixon et al., 2018; Sun et al., 2019). Bias in toxic language classification has been addressed through mitigation methods which focus on false positives caused by identity words and lack of context (Park et al., 2018; Davidson et al., 2019; Sap et al., 2019a). We take this issue into account in our experiments by looking at different parts of the generated statements.\nConsequently, there has been an increasing amount of work on explainability for toxic language classifiers (Aluru et al., 2020; Mathew et al., 2021). For instance, Aluru et al. (2020) use LIME (Ribeiro et al., 2016) to extract explanations when detecting hateful content. Akin to (Ribeiro et al., 2016), a more recent work on explainability by\nRibeiro et al. (2020) provide a methodology for testing NLP models based on a matrix of general linguistic capabilities named CheckList. Similarly, we present a set of steps in order to probe for toxicity in large PTLMs."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we present a methodology to probe toxic content in pre-trained language models using commonsense patterns. Our large scale study presents evidence that PTLMs tend to generate harmful biases towards minorities due to their spread within the pre-trained models. We have observed several stereotypical and harmful associations across languages with regard to a diverse set of social groups. We believe that the patterns we generated along with the predicted content can be adopted to build toxic language lexicons that have been noticed within PTLMs, and use the observed associations to mitigate implicit biases in order to build more robust systems. Furthermore, our methodology and predictions can help us define toxicity anchors that can be utilized to improve toxic language classification. The generated words can also be used to study socio-linguistic variations across languages by comparing stereotypical content with respect to professions, genders, religious groups, marginalized communities, and various demographics. In the future, we plan to revise our data by adding actions, more fluent and complex patterns, and longer generated statements which involve human interactions between people within the same social group, and people who belong to different ones."
    }, {
      "heading" : "6 Ethical Considerations",
      "text" : "Our research addresses the limitations of large pretrained language models which, despite their undeniable usefulness, are commonly used without further investigation on their impact on different communities around the world. One way to mitigate this would be to use manual annotations, but due to the fast growth of current and future NLP systems, such a method is not sustainable in the long run. Therefore, as shown in our paper, classifiers can be used to point us to potentially problematic statements.\nWe acknowledge the lack of naturalness and fluency in some of our generated sentences as well as the reliance of our approach on biased content which exists in toxic language classifiers. Hence,\nwe join other researchers in calling for and working toward building better toxic language datasets and detection systems. Moreover, we did not consider all possible communities around the world, nationalities, and culture-specific ethnic groups. Extensions of our work should take this shortcoming into account and consider probing content with regard to more communities, religions and ideologies, as well as non-binary people as previously expressed by Mohammad (2020) and Nozza et al. (2021).\nFinally, we mitigated the risk of biased annotations by working with annotators who come from different backgrounds, to whom we showed the original statements along with professional translations of the French and the Arabic statements. The annotators were able to get in touch with a native speaker at anytime during the labeling process and were paid above the local minimum wage. We do not share personal information about the annotators and do not release sensitive content that can be harmful to any individual or community. All our experiments can be replicated."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "We thank the annotators and anonymous reviewers and meta-reviewer for their valuable feedback.\nThis paper was supported by the Theme-based Research Scheme Project (T31-604/18-N), the NSFC Grant (No. U20B2053) from China, the Early Career Scheme (ECS, No. 26206717), the General Research Fund (GRF, No. 16211520), and the Research Impact Fund (RIF, No. R6020-19 and No. R6021-20) from the Research Grants Council (RGC) of Hong Kong."
    }, {
      "heading" : "A Appendix",
      "text" : "The full list of the social groups can be found on our GitHub page https: //github.com/HKUST-KnowComp/Probing_ toxicity_in_PTLMs. We show additional confusing examples in Table 11 and insulting ones in Table 12."
    } ],
    "references" : [ {
      "title" : "Are they our brothers? analysis and detection of religious hate speech in the arabic twittersphere",
      "author" : [ "Nuha Albadi", "Maram Kurdi", "Shivakant Mishra." ],
      "venue" : "Proceedings of ASONAM, pages 69–76. IEEE Computer Society.",
      "citeRegEx" : "Albadi et al\\.,? 2018",
      "shortCiteRegEx" : "Albadi et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep learning models for multilingual hate speech detection",
      "author" : [ "Sai Saketh Aluru", "Binny Mathew", "Punyajoy Saha", "Animesh Mukherjee." ],
      "venue" : "Proceedings of ECML/PKDD.",
      "citeRegEx" : "Aluru et al\\.,? 2020",
      "shortCiteRegEx" : "Aluru et al\\.",
      "year" : 2020
    }, {
      "title" : "Arabert: Transformer-based model for arabic language understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj." ],
      "venue" : "LREC 2020 Workshop Language Resources and Evaluation Conference.",
      "citeRegEx" : "Antoun et al\\.,? 2020",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "Language (technology) is power: A critical survey of ”bias” in nlp",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "arXiv preprint arXiv:2005.14050.",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Ultra-fine entity typing",
      "author" : [ "Eunsol Choi", "Omer Levy", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of ACL, pages 87–96.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael W. Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of ICWSM, pages 512–515.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’18, page 67–73, New",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Social chemistry 101: Learning to reason about social and moral norms",
      "author" : [ "Maxwell Forbes", "Jena D. Hwang", "Vered Shwartz", "Maarten Sap", "Yejin Choi." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Forbes et al\\.,? 2020",
      "shortCiteRegEx" : "Forbes et al\\.",
      "year" : 2020
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter",
      "author" : [ "Antigoni-Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "author" : [ "Nikhil Garg", "Londa Schiebinger", "Dan Jurafsky", "James Zou." ],
      "venue" : "Proceedings of the National Academy of Sciences, 115(16):E3635–E3644.",
      "citeRegEx" : "Garg et al\\.,? 2018",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2018
    }, {
      "title" : "Women’s syntactic resilience",
      "author" : [ "Aparna Garimella", "Carmen Banea", "Dirk Hovy", "Rada Mihalcea" ],
      "venue" : null,
      "citeRegEx" : "Garimella et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Garimella et al\\.",
      "year" : 2019
    }, {
      "title" : "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "EMNLP Findings.",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "We need to talk about standard splits",
      "author" : [ "Kyle Gorman", "Steven Bedrick." ],
      "venue" : "Proceedings of ACL. Association for Computational Linguistics.",
      "citeRegEx" : "Gorman and Bedrick.,? 2019",
      "shortCiteRegEx" : "Gorman and Bedrick.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4129–4138.",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Social biases in NLP models as barriers for persons with disabilities",
      "author" : [ "Ben Hutchinson", "Vinodkumar Prabhakaran", "Emily Denton", "Kellie Webster", "Yu Zhong", "Stephen Denuyl." ],
      "venue" : "Proceedings of ACL, pages 5491–5501. Association for Compu-",
      "citeRegEx" : "Hutchinson et al\\.,? 2020",
      "shortCiteRegEx" : "Hutchinson et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing biomedical embeddings from language models",
      "author" : [ "Qiao Jin", "Bhuwan Dhingra", "William Cohen", "Xinghua Lu." ],
      "venue" : "Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP at NAACL, pages 82–89.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Examining gender and race bias in two hundred sentiment analysis systems",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics *SEM, pages 43–53.",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2018",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2018
    }, {
      "title" : "Measuring bias in contextualized word representations",
      "author" : [ "Keita Kurita", "Nidhi Vyas", "Ayush Pareek", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166–172.",
      "citeRegEx" : "Kurita et al\\.,? 2019",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2019
    }, {
      "title" : "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models",
      "author" : [ "Bill Yuchen Lin", "Seyeon Lee", "Rahul Khanna", "Xiang Ren." ],
      "venue" : "Proceedings EMNLP, pages 6862–6868.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv: 1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "CamemBERT: a tasty French language model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro Javier Ortiz Suárez", "Yoann Dupont", "Laurent Romary", "Éric de la Clergerie", "Djamé Seddah", "Benoı̂t Sagot" ],
      "venue" : "In Proceedings of the 58th Annual Meeting",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted syntactic evaluation of language models",
      "author" : [ "Rebecca Marvin", "Tal Linzen." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Marvin and Linzen.,? 2018",
      "shortCiteRegEx" : "Marvin and Linzen.",
      "year" : 2018
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Mathew et al\\.,? 2021",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2021
    }, {
      "title" : "Do language models learn typicality judgments from text? arXiv preprint arXiv:2105.02987",
      "author" : [ "Kanishka Misra", "Allyson Ettinger", "Julia Taylor Rayz" ],
      "venue" : null,
      "citeRegEx" : "Misra et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2021
    }, {
      "title" : "Gender gap in natural language processing research: Disparities in authorship and citations",
      "author" : [ "Saif M. Mohammad." ],
      "venue" : "Proceedings of ACL, pages 7860– 7870.",
      "citeRegEx" : "Mohammad.,? 2020",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2020
    }, {
      "title" : "L-HSAB: A Levantine twitter dataset for hate speech and abusive language",
      "author" : [ "Hala Mulki", "Hatem Haddad", "Chedi Bechikh Ali", "Halima Alshabani." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 111–118. Association for",
      "citeRegEx" : "Mulki et al\\.,? 2019",
      "shortCiteRegEx" : "Mulki et al\\.",
      "year" : 2019
    }, {
      "title" : "Stereoset: Measuring stereotypical bias in pretrained language models",
      "author" : [ "Moin Nadeem", "Anna Bethke", "Siva Reddy." ],
      "venue" : "arXiv preprint arXiv:2004.09456.",
      "citeRegEx" : "Nadeem et al\\.,? 2020",
      "shortCiteRegEx" : "Nadeem et al\\.",
      "year" : 2020
    }, {
      "title" : "Unintended bias in misogyny detection",
      "author" : [ "D. Nozza", "C. Volpetti", "E. Fersini." ],
      "venue" : "2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI), pages 149–155.",
      "citeRegEx" : "Nozza et al\\.,? 2019",
      "shortCiteRegEx" : "Nozza et al\\.",
      "year" : 2019
    }, {
      "title" : "HONEST: Measuring Hurtful Sentence Completion in Language Models",
      "author" : [ "Debora Nozza", "Federico Bianchi", "Dirk Hovy." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Nozza et al\\.,? 2021",
      "shortCiteRegEx" : "Nozza et al\\.",
      "year" : 2021
    }, {
      "title" : "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "author" : [ "Alexandra Olteanu", "Carlos Castillo", "Fernando Diaz", "Emre Kıcıman." ],
      "venue" : "Frontiers in Big Data, 2:13.",
      "citeRegEx" : "Olteanu et al\\.,? 2019",
      "shortCiteRegEx" : "Olteanu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual and multi-aspect hate speech analysis",
      "author" : [ "Nedjma Ousidhoum", "Zizheng Lin", "Hongming Zhang", "Yangqiu Song", "Dit-Yan Yeung." ],
      "venue" : "Proceedings of EMNLP, Hong Kong, China.",
      "citeRegEx" : "Ousidhoum et al\\.,? 2019",
      "shortCiteRegEx" : "Ousidhoum et al\\.",
      "year" : 2019
    }, {
      "title" : "Comparative evaluation of labelagnostic selection bias in multilingual hate speech datasets",
      "author" : [ "Nedjma Ousidhoum", "Yangqiu Song", "Dit-Yan Yeung." ],
      "venue" : "Proceedings of EMNLP, pages 2532– 2542.",
      "citeRegEx" : "Ousidhoum et al\\.,? 2020",
      "shortCiteRegEx" : "Ousidhoum et al\\.",
      "year" : 2020
    }, {
      "title" : "Bias in word embeddings",
      "author" : [ "Orestis Papakyriakopoulos", "Simon Hegelich", "Juan Carlos Medina Serrano", "Fabienne Marco." ],
      "venue" : "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ’20, page 446–457. Association",
      "citeRegEx" : "Papakyriakopoulos et al\\.,? 2020",
      "shortCiteRegEx" : "Papakyriakopoulos et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing gender bias in abusive language detection",
      "author" : [ "Ji Ho Park", "Jamin Shin", "Pascale Fung." ],
      "venue" : "Proceedings of EMNLP, pages 2799–2804. Association for Computational Linguistics.",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of EMNLP-IJCNLP,",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "When BERT Plays the Lottery, All Tickets Are Winning",
      "author" : [ "Sai Prasanna", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings EMNLP, pages 3208– 3229, Online.",
      "citeRegEx" : "Prasanna et al\\.,? 2020",
      "shortCiteRegEx" : "Prasanna et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual offensive language identification with cross-lingual embeddings",
      "author" : [ "Tharindu Ranasinghe", "Marcos Zampieri." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Ranasinghe and Zampieri.,? 2020",
      "shortCiteRegEx" : "Ranasinghe and Zampieri.",
      "year" : 2020
    }, {
      "title" : "why should i trust you?”: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of ACM SIGKDD, KDD ’16, page 1135–1144. Association for Computing Machinery.",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of ACL, pages 4902–4912.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of ACL, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL, pages 1668–1678, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Sap et al\\.,? 2019a",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan LeBras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Sap et al\\.,? 2019b",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Predictive biases in natural language processing models: A conceptual framework and overview",
      "author" : [ "Deven Shah", "H. Andrew Schwartz", "Dirk Hovy." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Shah et al\\.,? 2020",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2020
    }, {
      "title" : "The woman worked as a babysitter: On biases in language generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Premkumar Natarajan", "Nanyun Peng." ],
      "venue" : "Proceedings of EMNLP, pages 3405–3410. Association for Computational Linguistics.",
      "citeRegEx" : "Sheng et al\\.,? 2019",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Probing for referential information in language models",
      "author" : [ "Ionut-Teodor Sorodoc", "Kristina Gulordava", "Gemma Boleda." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Sorodoc et al\\.,? 2020",
      "shortCiteRegEx" : "Sorodoc et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating gender bias in natural language processing: Literature review",
      "author" : [ "Tony Sun", "Andrew Gaut", "Shirlyn Tang", "Yuxin Huang", "Mai ElSherief", "Jieyu Zhao", "Diba Mirza", "Elizabeth Belding", "Kai-Wei Chang", "William Yang Wang." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Would you rather? a new benchmark for learning machine alignment with cultural values and social preferences",
      "author" : [ "Yi Tay", "Donovan Ong", "Jie Fu", "Alvin Chan", "Nancy Chen", "Anh Tuan Luu", "Chris Pal." ],
      "venue" : "Proceedings of ACL, pages 5369–5373.",
      "citeRegEx" : "Tay et al\\.,? 2020",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "What do you learn from context? probing for sentence structure",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of EMNLP, pages 38–45.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of The 13th International",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offen",
      "author" : [ "Marcos Zampieri", "Preslav Nakov", "Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Hamdy Mubarak", "Leon Derczynski", "Zeses Pitenis", "Çağrı Çöltekin" ],
      "venue" : null,
      "citeRegEx" : "Zampieri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2020
    }, {
      "title" : "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "Proceedings of EMNLP, pages 2979–2989.",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al.",
      "startOffset" : 143,
      "endOffset" : 203
    }, {
      "referenceID" : 50,
      "context" : "Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al.",
      "startOffset" : 143,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "Present work on evaluating the sensitivity of language models towards stereotypical content involves the construction of assessment benchmarks (Nadeem et al., 2020; Tay et al., 2020; Gehman et al., 2020) in addition to the study of the potential risks associated with the use and deployment of PTLMs (Bender et al.",
      "startOffset" : 143,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al.",
      "startOffset" : 83,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al.",
      "startOffset" : 83,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "Previous work on probing PTLMs focuses on their syntactic and semantic limitations (Hewitt and Manning, 2019; Marvin and Linzen, 2018), lack of domainspecific knowledge (Jin et al., 2019), and absence of commonsense (Petroni et al.",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : ", 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : ", 2019), and absence of commonsense (Petroni et al., 2019; Lin et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 31,
      "context" : "However, except for a recent evaluation process of hurtful sentence completion (Nozza et al., 2021), we notice a lack of large-scale probing experiments for quantifying toxic content in PTLMs or systemic methodologies to measure the extent to which they generate harmful content about different social groups.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 45,
      "context" : "First, we create cloze statements which are prompted by explicit names of social groups followed by benign and simple actions from the ATOMIC cause-effect knowledge graph patterns (Sap et al., 2019b).",
      "startOffset" : 180,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "We look into how BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), and GPT-2 (Radford et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : ", 2019), and GPT-2 (Radford et al., 2019) associate unrelated and detrimental causes to basic everyday actions and examine how frequently the predicted words relate to specific social groups.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "Moreover, we study the same phenomenon in two other languages by translating more than 700 ATOMIC commonsense actions to Arabic and French, along with names of social groups, then run the same experiments using the French PTLM CamemBERT (Martin et al., 2020), and the Arabic AraBERT (Antoun et al.",
      "startOffset" : 237,
      "endOffset" : 258
    }, {
      "referenceID" : 45,
      "context" : "We use the ATOMIC atlas of everyday commonsense reasoning based on if-then relations (Sap et al., 2019b) to create cloze statements to fill in.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 44,
      "context" : "We propose to use simple toxic language classifiers despite their bias towards slurs and identity words (Sap et al., 2019a; Park et al., 2018; Ousidhoum et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 166
    }, {
      "referenceID" : 36,
      "context" : "We propose to use simple toxic language classifiers despite their bias towards slurs and identity words (Sap et al., 2019a; Park et al., 2018; Ousidhoum et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "We propose to use simple toxic language classifiers despite their bias towards slurs and identity words (Sap et al., 2019a; Park et al., 2018; Ousidhoum et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "We trained an LR classifier on four relatively different English datasets (Davidson et al., 2017; Founta et al., 2018; Ousidhoum et al., 2019; Zampieri et al., 2019), four others in Arabic (Ousidhoum et al.",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 11,
      "context" : "We trained an LR classifier on four relatively different English datasets (Davidson et al., 2017; Founta et al., 2018; Ousidhoum et al., 2019; Zampieri et al., 2019), four others in Arabic (Ousidhoum et al.",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 33,
      "context" : "We trained an LR classifier on four relatively different English datasets (Davidson et al., 2017; Founta et al., 2018; Ousidhoum et al., 2019; Zampieri et al., 2019), four others in Arabic (Ousidhoum et al.",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 53,
      "context" : "We trained an LR classifier on four relatively different English datasets (Davidson et al., 2017; Founta et al., 2018; Ousidhoum et al., 2019; Zampieri et al., 2019), four others in Arabic (Ousidhoum et al.",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 34,
      "context" : ", 2019), four others in Arabic (Ousidhoum et al., 2020; Albadi et al., 2018; Mulki et al., 2019; Zampieri et al., 2020), and the only one we know about in French (Ousidhoum et al.",
      "startOffset" : 31,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : ", 2019), four others in Arabic (Ousidhoum et al., 2020; Albadi et al., 2018; Mulki et al., 2019; Zampieri et al., 2020), and the only one we know about in French (Ousidhoum et al.",
      "startOffset" : 31,
      "endOffset" : 119
    }, {
      "referenceID" : 28,
      "context" : ", 2019), four others in Arabic (Ousidhoum et al., 2020; Albadi et al., 2018; Mulki et al., 2019; Zampieri et al., 2020), and the only one we know about in French (Ousidhoum et al.",
      "startOffset" : 31,
      "endOffset" : 119
    }, {
      "referenceID" : 54,
      "context" : ", 2019), four others in Arabic (Ousidhoum et al., 2020; Albadi et al., 2018; Mulki et al., 2019; Zampieri et al., 2020), and the only one we know about in French (Ousidhoum et al.",
      "startOffset" : 31,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : ", 2020), and the only one we know about in French (Ousidhoum et al., 2019).",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 44,
      "context" : "Toxic language classifiers show an inherent bias towards certain terms such as the names of some social groups which are part of our patterns (Sap et al., 2019a; Park et al., 2018; Hutchinson et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 205
    }, {
      "referenceID" : 36,
      "context" : "Toxic language classifiers show an inherent bias towards certain terms such as the names of some social groups which are part of our patterns (Sap et al., 2019a; Park et al., 2018; Hutchinson et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "Toxic language classifiers show an inherent bias towards certain terms such as the names of some social groups which are part of our patterns (Sap et al., 2019a; Park et al., 2018; Hutchinson et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 205
    }, {
      "referenceID" : 34,
      "context" : "In fact, we can also use these top generated words coupled as strongly attached words as anchors to further probe other data collections or evaluate selection bias for existing toxic content analysis datasets (Ousidhoum et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 233
    }, {
      "referenceID" : 7,
      "context" : "The large and incontestable success of BERT (Devlin et al., 2019) revolutionized the design and per-",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 43,
      "context" : "However, we are still investigating the reasons behind this success with the experimental setup side (Rogers et al., 2020; Prasanna et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : "However, we are still investigating the reasons behind this success with the experimental setup side (Rogers et al., 2020; Prasanna et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Classification models are typically fine-tuned using PTLMs to boost their performance including hate speech and offensive language classifiers (Aluru et al., 2020; Ranasinghe and Zampieri, 2020).",
      "startOffset" : 143,
      "endOffset" : 194
    }, {
      "referenceID" : 40,
      "context" : "Classification models are typically fine-tuned using PTLMs to boost their performance including hate speech and offensive language classifiers (Aluru et al., 2020; Ranasinghe and Zampieri, 2020).",
      "startOffset" : 143,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "PTLMs have even been used as label generation components in tasks such as entity type prediction (Choi et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 35,
      "context" : "Similarly to how long existing stereotypes are deep-rooted in word embeddings (Papakyriakopoulos et al., 2020; Garg et al., 2018), PTLMs have also been shown to recreate stereotypical content due to the nature of their training data (Sheng et al.",
      "startOffset" : 78,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Similarly to how long existing stereotypes are deep-rooted in word embeddings (Papakyriakopoulos et al., 2020; Garg et al., 2018), PTLMs have also been shown to recreate stereotypical content due to the nature of their training data (Sheng et al.",
      "startOffset" : 78,
      "endOffset" : 129
    }, {
      "referenceID" : 47,
      "context" : ", 2018), PTLMs have also been shown to recreate stereotypical content due to the nature of their training data (Sheng et al., 2019) among other reasons.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "Different probing experiments have been proposed to study the drawbacks of PTLMs in areas such as the biomedical domain (Jin et al., 2019), syntax (Hewitt and Manning, 2019; Marvin and Linzen, 2018), semantic and syntactic sentence structures (Tenney et al.",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : ", 2019), syntax (Hewitt and Manning, 2019; Marvin and Linzen, 2018), semantic and syntactic sentence structures (Tenney et al.",
      "startOffset" : 16,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : ", 2019), syntax (Hewitt and Manning, 2019; Marvin and Linzen, 2018), semantic and syntactic sentence structures (Tenney et al.",
      "startOffset" : 16,
      "endOffset" : 67
    }, {
      "referenceID" : 51,
      "context" : ", 2019), syntax (Hewitt and Manning, 2019; Marvin and Linzen, 2018), semantic and syntactic sentence structures (Tenney et al., 2019), prenomial anaphora (Sorodoc et al.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 48,
      "context" : ", 2019), prenomial anaphora (Sorodoc et al., 2020), commonsense (Petroni et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : ", 2020), commonsense (Petroni et al., 2019), gender bias (Kurita et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : ", 2019), gender bias (Kurita et al., 2019), and typicality in judgement(Misra et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : ", 2019), and typicality in judgement(Misra et al., 2021).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 44,
      "context" : "(2020) present a framework to predict the origin of different types of bias including label bias (Sap et al., 2019a), selec-",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "tion bias (Garimella et al., 2019; Ousidhoum et al., 2020), model overamplification (Zhao et al.",
      "startOffset" : 10,
      "endOffset" : 58
    }, {
      "referenceID" : 34,
      "context" : "tion bias (Garimella et al., 2019; Ousidhoum et al., 2020), model overamplification (Zhao et al.",
      "startOffset" : 10,
      "endOffset" : 58
    }, {
      "referenceID" : 55,
      "context" : ", 2020), model overamplification (Zhao et al., 2017), and semantic bias (Garg et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "Other work investigate the effect of data splits (Gorman and Bedrick, 2019) and mitigation strategies (Dixon et al.",
      "startOffset" : 49,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "Other work investigate the effect of data splits (Gorman and Bedrick, 2019) and mitigation strategies (Dixon et al., 2018; Sun et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 49,
      "context" : "Other work investigate the effect of data splits (Gorman and Bedrick, 2019) and mitigation strategies (Dixon et al., 2018; Sun et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : "Bias in toxic language classification has been addressed through mitigation methods which focus on false positives caused by identity words and lack of context (Park et al., 2018; Davidson et al., 2019; Sap et al., 2019a).",
      "startOffset" : 160,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "Bias in toxic language classification has been addressed through mitigation methods which focus on false positives caused by identity words and lack of context (Park et al., 2018; Davidson et al., 2019; Sap et al., 2019a).",
      "startOffset" : 160,
      "endOffset" : 221
    }, {
      "referenceID" : 44,
      "context" : "Bias in toxic language classification has been addressed through mitigation methods which focus on false positives caused by identity words and lack of context (Park et al., 2018; Davidson et al., 2019; Sap et al., 2019a).",
      "startOffset" : 160,
      "endOffset" : 221
    }, {
      "referenceID" : 1,
      "context" : "Consequently, there has been an increasing amount of work on explainability for toxic language classifiers (Aluru et al., 2020; Mathew et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "Consequently, there has been an increasing amount of work on explainability for toxic language classifiers (Aluru et al., 2020; Mathew et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 148
    }, {
      "referenceID" : 41,
      "context" : "(2020) use LIME (Ribeiro et al., 2016) to extract explanations when detecting hateful content.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 41,
      "context" : "Akin to (Ribeiro et al., 2016), a more recent work on explainability by",
      "startOffset" : 8,
      "endOffset" : 30
    } ],
    "year" : 2021,
    "abstractText" : "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.",
    "creator" : "LaTeX with hyperref"
  }
}