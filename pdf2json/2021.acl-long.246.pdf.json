{
  "name" : "2021.acl-long.246.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fast and Accurate Neural Machine Translation with Translation Memory",
    "authors" : [ "Qiuxiang He", "Guoping Huang", "Qu Cui", "Li Li", "Lemao Liu" ],
    "emails" : [ "hqxiang@email.swu.edu.cn", "cuiq@smail.nju.edu.cn", "donkeyhuang@tencent.com", "redmondliu@tencent.com", "lily@swu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3170–3180\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3170"
    }, {
      "heading" : "1 Introduction",
      "text" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).\nRecently there are increasing interests in improving neural machine translation (NMT) with a\n∗Corresponding author.\nTM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a TM and then integrate it into the NMT architecture. Bulte and Tezcan (2019) and Xu et al. (2020) employ data augmentation to train an NMT model whose training instances are bilingual sentences augmented by their TMs. Despite their improvements on the TM-specialized translation tasks (aka JRC-Acquis corpora) where a TM is very similar to test sentences, they consume considerable computational overheads in either training or testing, and particularly it is unclear whether they can deliver gains over standard NMT on general tasks where a TM is not very similar to test sentences. Indeed, both Zhang et al. (2018) and Xu et al. (2020) reported their failures on WMT news translation tasks.\nIn this paper, we present a fast and accurate approach for TM-based NMT which can be applied to general translation tasks besides TM-specialized tasks. We first design a light-weight TM-based NMT model for efficiency: its TM includes a single bilingual sentence and we explore variant ways to encode the TM. Also, the designed model outperforms strong TM-based baselines. Second, we deeply analyze its translation performance and observe an issue of robustness: it decreases significantly for those input sentences which are not very similar to their TMs, although it obtains substantial improvements for other inputs. To address this issue, we propose a novel training criterion for optimizing the parameters of our model inspired by multiple-task learning (van Dyk and Meng, 2001;\nBen-David and Borbely, 2008; Qiu et al., 2013). The loss function includes two terms: the first term is induced by the bilingual corpus with a TM whereas the second term is induced by the bilingual corpus without any TM. In this way, the TM-based NMT model gains better performance and is robust to translate any input sentences no matter they are similar to their TM or not. Additionally, this makes it possible that a single unified model can handle both translation situations (with or without a TM), which is practical for online services.\nTo validate the effectiveness of the proposed approach, we conduct extensive experiments on eight translation tasks including both TM-specialized tasks and general tasks (WMT). Our experiments justify that the proposed approach is better than several strong TM-based baselines in speed, and it further delivers substantial gains (up to 4.7 BLUE points) over those baselines on TM-specialized tasks, leading to up to 8.5 BLEU points over standard Transformer-based NMT. In particular, it also outperforms strong baselines on two general translation tasks, i.e., with a gain of 0.7 BLEU points on WMT14 En→De task and 1.0 BLEU point on WMT17 Zh→En task.\nThis paper makes the following contributions:\n• It points out a critical issue about robustness when training TM-based NMT models and provides an elegant method to address this issue.\n• It proposes a simple TM-based NMT model that outperforms strong TM-based baselines in terms of both translation quality and speed.\n• It verifies that a well-designed TM-based translation model is able to advance strong MT baselines on general translation tasks where a TM is not very similar to input source sentences."
    }, {
      "heading" : "2 Preliminary on NMT",
      "text" : "Suppose x = {x1, ..., xn} is a source sentence and y = {y1, ..., ym} is the corresponding target sentence. From the probabilistic perspective, NMT models the conditional probability of the target sentence y given the source sentence x. Formally, for a given x, NMT aims to generate the output y according to the conditional probability P (y|x) defined by neural networks:\nP (y|x) = m∏ i=1 P (yi|x, y<i) (1)\nwhere y<i = {y1, . . . , yi−1} denotes a prefix of y, and each factor P (yi|x, y<i) is defined as follows:\nP (yi|x, y<i) = softmax ( φ(hD,Li ) ) (2)\nwhere hD,Li indicates the ith hidden unit at Lth layer in the Decoding phrase under the encoderdecoder framework (Bahdanau et al., 2016), and φ is a linear network that projects hidden units onto vectors with dimension of the target vocabulary.\nRecently, self-attention networks have attracted many interests due to their flexibility in parallel computation and modeling hD,Li . The state-of-theart NMT model is Transformer (Vaswani et al., 2017), which uses stacked self-attention and fully connected layers for its encoder and decoder. Selfattention relies on an attention mechanism to compute a representation of a sequence. In Transformer, there are three kinds of attention mechanisms, including encoder multi-head attention, decoder masked multi-head attention and encoderdecoder multi-head attention. Attention with H heads can be calculated by the equations:\nMH-Att(q,u) = [ Att(q, φj(u), ψj(u)) ]H j=1 ,\nAtt(q,u,v) = softmax ( qu>√ d ) v\n(3)\nwhere q is a query vector and u is a twodimensional matrix, [uj ]Hj=1 denotes concatenation of all vectors uj , φj and ψj stand for two linear projections from one matrix to another matrix, respectively. The 1√\nd is the scaling factor, and d is the\ndimension of q. And we refer enthusiastic readers to Vaswani et al. (2017) for detailed definitions."
    }, {
      "heading" : "3 Model Architecture",
      "text" : "In this section, in order to preferably bridge TM and NMT, we propose the architecture of TM-based NMT within the Transformer. To make our proposed model fast in running time and powerful in quality, at first, we present a configuration of TM to make the proposed model efficient. Then we explore three different methods to encode the TM into a sequence of vectors in a coarse-to-fine manner. Finally, we propose the architecture that decodes a target word given an input source sentence and its TM representation."
    }, {
      "heading" : "3.1 TM Configuration",
      "text" : "Following previous works (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), for each source\nsentence x we employ Apache Lucene (Bialecki et al., 2020) to retrieve top-100 similar bilingual sentences from the training data. Then we adopt the following similarity to re-rank the retrieved bilingual sentences and maintain top-K (K < 100) bilingual sentences as the TM for x:\nsim(x, xtm) = 1− dist(x, xtm)\nmax(|x|, |xtm|) (4)\nwhere dist denotes the edit-distance, and xtm is a retrieved source sentence from the training data and its reference is ytm.\nPrevious studies show that the best translation quality is achieved when the size K of the TM is larger than 1. For example, the optimized K is set to be 5 in Gu et al. (2018) and Xia et al. (2019), and it is even set to be 100 in Zhang et al. (2018). Unfortunately, such a large K significantly decreases the translation speed because the computational complexity is linear in the size of K. To make our inference as efficient as possible, we setK = 1 and employ the most similar bilingual sentence denoted by 〈xtm, ytm〉 as the TM for x.1"
    }, {
      "heading" : "3.2 Encoding TM",
      "text" : "In this subsection, we will describe how to encode the TM 〈xtm, ytm〉 into a sequence of vectors m.\n1We also did some experiments on K = 2 and K = 4 in our proposed model, but we did not observe significant gains.\nThree variant methods for encoding a TM are illustrated in the right part of Figure 1.\nMethod 1: sentence (TF-S) Given 〈xtm, ytm〉 for x, the first method utilizes word embedding and position embedding of ytm to represent m as follows:\nm = Etm = [Ew(y1tm) + Ep(y 1 tm),\n· · · , Ew(yJ ′ tm) + Ep(y J ′ tm)]\n(5)\nwhereEw andEp are word embedding and position embedding respectively, J ′ is the length of ytm and the symbol + denotes a simple addition operator.\nMethod 2: sentence with score (TF-SS) The first method is agnostic to the similarity score. Intuitively, if a TM 〈xtm, ytm〉 is with high similarity, ytm may be more helpful to predict a good translation. So, the second method takes the similarity score into account and it defines m as follows: m = stm × Etm (6) where stm = sim(x, xtm) is the similarity score and the symbol× denotes the scalar-multiplication.\nMethod 3: sentence with alignment (TF-SA) As shown in Figure 1, xtm consists of the matched parts (in orange color) and the unmatched parts (in dark color) to x. Since each word in the TM is not of the same importance to the source sentence x, we should pay more attention to the words that\nare in the matched parts. So, we further obtain the word alignment between xtm and ytm through fast-align toolkit (Dyer et al., 2013).2 Suppose Atm is the word alignment between xtm and ytm: Ajtm = 1 denotes yj is aligned to some xi otherwise Ajtm = 0, where xi is also in x . Therefore, the third method defines m as follows:\nm = Atm ◦ ( stm × Etm ) (7)\nwhere the symbol ◦ denotes an operator between a vector and a matrix such that\nmj =\n{ stm × Ejtm if A j tm = 0\nEjtm if A j tm = 1\n(8)"
    }, {
      "heading" : "3.3 TM Augmented NMT",
      "text" : "Suppose the encoded TM 〈xtm, ytm〉 is denoted by m, a sequence of vectors. We aim to build a model P (yi | x, y<i,m) for the source sentence x, given the m and prefix translation y<i at time step i, leading to the entire translation model: P (y | x, xtm, ytm; θ) =\n∏ i P (yi | x, y<i,m) (9)\nwhere θ denotes the parameter of our proposed model.3\nExample Layer The model architecture of P (yi | x, y<i,m) is illustrated at the left part of Figure 1, where its architecture is generally similar to standard Transformer and the core component is the Example Layer. Specifically, the Example Layer includes two multi-head attention operators: the left multi-head attention (i.e. MH-Att (y<i, y<i)) is the same as Transformer, and it is defined on the prefix translation y<i; the right multihead attention (i.e. MH-Att (y<i, ytm)) attempts to capture information from the TM, and its query is from y<i while key and value are from the representation of TM m. After the two parallel attention operators, two resulting sequences are passed to Add & Norm operator and a new sequence is obtained as the query for the next multi-head attention (i.e. MH-Att (y<i, x)). The following sub-layer is the same as Transformer and P (yi | x, y<i,m) can be obtained similar to the definition of standard NMT P (yi | x, y<i) as presented in Section 2. We skip those formal equations to rewrite P (yi | x, y<i,m) due to space limitation.\n2Although some advanced word alignment toolkits (Dou and Neubig, 2021; Chen et al., 2021; Jalili Sabet et al., 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al., 2018; Xia et al., 2019).\n3In the rest of this paper, we may drop θ in the model for easier notations.\nIn summary The entire model architecture is illustrated in Figure 1: the dashed box in the right part shows the memory encoder, and the left part shows how the memory representation is used in the NMT model similar to the Transformer. In our model architecture, the encoder block contains two sub-layers and the decoder block contains three sub-layers. The core sub-layer in the decoder block is our proposed Example Layer, which consists of multi-head attention and cross attention. By introducing the memory encoder and Example Layer, the parameters in our model are increased only by 8.96% compared to the standard NMT baseline."
    }, {
      "heading" : "4 Training",
      "text" : "Suppose the training corpus is D = {〈xi, yi, xitm, yitm〉 | i ∈ [1, N ]}, where 〈xi, yi〉 is a bilingual sentence, and 〈xitm, yitm〉 is the related TM which consists of a single bilingual sentence. Our goal is to learn the parameter θ of the TM-based NMT model P (y | x, xtm, ytm; θ) defined in Eq.(9) using D.\nThe common wisdom is to optimize the parameter under the maximum likelihood estimation (MLE), i.e. standard training. Formally, it minimizes the following criterion:\n− N∑ i logP (yi | xi, xitm, yitm; θ).\nRobustness issue Unfortunately, the model trained with MLE suffers from an issue about robustness even if its overall performance is much better than standard Transformer and outperforms TM-based baselines on the Es→En task. According to our experiments (see Table 4 later), our proposed model performs worse than the Transformer for those sentences which do not have a similar TM. As a result, it would be dangerous to use the model for online services because users may provide an input sentence whose TM is not similar to itself.\nThe possible reason for the above issue is explained as follows. On the average case, the reference y is strongly correlated to its TM target ytm in the training corpus D. For example, the average similarity score is about 0.58 for Es→En translation task, according to our statistics. Because of the powerful fitting ability of neural networks, the model parameters will be guided to heavily depend on the given TM target ytm during training. In this way, if an input source sentence x has a high similarity with its given TM, the model will output\nhigh-quality results, as we also observed in Table 5. On the contrary, once an input sentence is provided with a low similar TM 〈xtm, ytm〉 (for instance, the similarity between 0 and 0.3, as shown in Table 4), the translation quality of its output rapidly decreases.\nTraining criterion In order to avoid the TM over-fitting, we propose a simple yet elegant method, inspired by data augmentation (van Dyk and Meng, 2001; Li et al., 2019; Zhong et al., 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016). Specifically, we first construct another corpus D0 = {〈xi, yi, null, null〉 | i ∈ [1, N ]} from D = {〈xi, yi, xitm, yitm〉 | i ∈ [1, N ]}. In the constructed corpus, 〈null, null〉 plays a role of a TM, but both source and target sides of the TM are empty sentences.4 Then we train the model P (y | x, xtm, ytm; θ) using both D and D0, i.e. joint training, which is similar to multiple-task learning. Formally, we minimize the following joint loss function: `(D,D0; θ) = − N∑ i ( logP (yi | xi, xitm, yitm; θ)\n+ λ× logP (yi | xi, null, null; θ) )\n(10) where 0 < λ is a coefficient to trade off both loss terms. Intuitively, the first term induced by D guides the model to use the information from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services.\nNote that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is\n4In the experiments, we implement null as the sentence including a single word, i.e. “〈eos〉”.\nAlgorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration\nM , a learning rate schema η and two corpus: D = {〈xi, yi, xitm, yitm〉 | i ∈ [1, N ]} and D0 = {〈xi, yi, null, null〉 | i ∈ [1, N ]}\nOutput: The parameter θ. 1 for 1 ≤ t ≤M do 2 Sample a mini-batch B with size of b/2 from D 3 Sample a mini-batch B0 with size of b/2 from D0 4 Calculate gradient ∆ = ∇θ`(B,B0; θ) as defined in Eq.(10) 5 Update parameter: θ = θ − ηt∆\ndirectly taken from the original D in our scenario. Also, multiple-task learning in their works typically involves different models that share some partial parameters rather than all parameters. In contrast, both terms in our joint loss correspond to the same task, i.e. translation prediction given a source sentence and its TM; and both models are exactly the same.\nThe detailed joint training algorithm is presented in Algorithm 1. It follows the standard gradient descent method for optimization. Note that in line 2 and 3, it samples two mini-batches which do not share the same bilingual sentences to promote diversity, i.e., D and D0 are independently and randomly sampled. In our experiments, we employ Adam (Kingma and Ba, 2014) with default settings as the learning rate schema."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we validate the effectiveness of the proposed approach: robustness for handling both translation situations (with or without a TM), running efficiency compared with the previous TMbased NMT models, translation quality on both TM-specialized tasks and general MT tasks. We use the case-insensitive BLEU score as the automatic metric (Papineni et al., 2002) for the translation quality evaluation."
    }, {
      "heading" : "5.1 Setup",
      "text" : "TM-specialized tasks We evaluate our proposed models with the JRC-Acquis corpora, which include three language pairs and lead to six translation tasks in total: English↔German (En↔De),\nEnglish↔Spanish (En↔Es) and English↔French (En↔Fr). To compare with previous work, we adopt the same splitting of training/dev/test and pre-processing as Gu et al. (2018), Zhang et al. (2018), and Xia et al. (2019).\nGeneral tasks The proposed models are evaluated on the widely-used general WMT tasks: WMT14 English-to-German (En→De) and WMT17 Chinese-to-English (Zh→En) tasks. For the En→De task, we use newstest2013 as the development set, as well as employ newstest2014 and newstest2017 as the test sets. For the Zh→En task, we employ newsdev2017 and newstest2017 as the development and test set respectively.\nTable 1 summarizes the data statistics for both TM-specialized and general tasks. In addition, we employ Byte Pair Encoding (BPE) (Sennrich et al., 2016b) on all the tasks mentioned before.\nBaseline systems We compare our proposed model with the strong baselines as follows:\n• TF (Vaswani et al., 2017): it is the standard Transformer. • TF-P (Zhang et al., 2018): it is reimplemented on top of Transformer by ourselves. • TF-G (Xia et al., 2019) and TF-SEQ (Gu et al., 2018): TF-SEQ is a mimic implementation over Transformer by Xia et al. (2019). We report the results from Xia et al. (2019) since they were also implemented over Transformer as comparison. • FM+ (Xu et al., 2020): since Xu et al. (2020) adopt a different split on JRC corpus, the results are not comparable to ours. For a fair comparison, we re-implement a strong model FM+ as a baseline which makes use of the same metric to retrieve a TM as ours and is better than the method in Bulte and Tezcan (2019).\nOur models In the case of the three methods proposed in this paper, TF-S, TF-SS and TF-SA refer to the method encoding TM by the sentence, sentence with score, and sentence with alignment, respectively. We optimize their parameters through both standard training and joint training. For joint training, the hyperparameter λ is set to be 1 for all translation tasks.\nSystem configuration For a fair comparison, we employ the same settings to train all baselines and our models, and the learning rate for all models is Adam with the default hyper-parameters. The\ndetails of the settings are shown in Table 2."
    }, {
      "heading" : "5.2 Results and Analysis on Es→En Task",
      "text" : "Standard training and robustness issue We first evaluate the proposed models under the standard training criterion. Table 3 shows the comparison among different TM encoding methods for our models. From this table, we can see that our models achieve substantial improvements over Transformer (TF) which does not use any TM, even if our models are simple and only utilize a single bilingual sentence in the TM. TF-SA performs better than TF-S and TF-SS thanks to the fine-grained alignment information encoded in the TM. Also, TF-SA outperforms all TM-based baselines by at least 1.0 BLEU point, compared with Table 6.\nIn addition, we exploit the influence of our models on the similarity of a TM. We thereby divide the test dataset into ten subsets according to the similarity score and report the results in Table 4. We find that the gains of our models over the TF baseline are mainly from those sentences whose TMs are with relatively high similarity. To our surprise, our models perform worse than TF on the subset with relatively low similarity except the subset with the lowest similarity.5 This result demonstrates that our models with standard training are not robust to similarity scores, as deeply explained in the previous section.\nJoint training Luckily the robustness issue can be fixed well by joint training, as depicted in the right part of Table 4. We can see that our model is better than the baseline TF on the subset of [0, 0.3), and it substantially outperforms TF on the subset of [0.3, 1). With the help of joint training, TF-SA delivers gains of 1.2 BLEU points over standard training, and gains of 5.7 BLEU points over the strong TF baseline on the entire test set.\nTherefore, in the rest of the experiments, we employ joint training to set up all of our models because it is robust to the low similarity of TMs.\nWithout TM or with Ref as TM The situation without any TM and the situation with reference as a TM are more extreme cases of the robustness issue. As reported in Table 5, if a perfect TM is\n5We further check these two exceptional sentences and find that they are very short in length. In particular, their word alignment results from the fast-align toolkit are very good, which may be beneficial to our proposed model. This might be the reason why our proposed model advances the baseline Transformer.\nprovided to our models, they can yield excellent translation results. Besides, the proposed methods are not inferior to the standard Transformer when no TM is provided. As a result, the proposed model makes it possible that a single unified model can handle both translation situations (with or without a TM), which is practical for online services.\nNoisy TM To validate whether the model works well with noisy TMs, we also conduct a quick experiment by adding noises to TM for the test set by randomly replacing words in the target side of TM with incorrect words. After replacing one and two words, the proposed TF-SA achieves 68.17 BLEU points and 67.94 BLEU points, respectively. Both results are slightly worse than the noise-free TF-SA (68.49) but still better than the best TM baseline (66.21). Note that both results are obtained without retraining TF-SA model with noisy TM. This fact demonstrates our model is even robust to noisy TMs and thus it is useful for the online TM.\nComparison with baselines Table 6 illustrates the results between the proposed model TF-SA and the baselines. It is clearly shown that TF-SA surpasses all TM-based baselines with a substantial margin. In details, TF-SA outperforms TF-P and TF-SEQ by about 3.2 BLEU points, FM+ by about 2.6 BLEU points, and the strong baseline TF-G by about 2.2 BLEU points.\nRunning time Since all TM-based models employ the same retrieval metric and their retrieval\ntime is exactly the same, we only report the running time of all TM-based NMT models excluding retrieval time in Table 7. As reported in this table, our proposed model further saves significant running time over TF-SEQ and TF-G for both training and testing, besides achieving better translation performance. In addition, although it requires slight overhead in training, its testing is more efficient than TF-P; and our training is faster than FM+."
    }, {
      "heading" : "5.3 Overall Translation Quality",
      "text" : ""
    }, {
      "heading" : "5.3.1 On the TM-specialized Datasets",
      "text" : "The experimental results of all the systems on the six translation tasks of TM-specialized datasets are reported in Table 8. Several observations can be made from the results. First, the baseline TF-P and TF-G achieve substantial gains over the strong baseline TF, outperforming by [1.1, 4.1] BLEU points. This result is in line with the finding in Zhang et al. (2018) and Xia et al. (2019). Second, on the basis of that, compared with the strongest baseline TF-G, our proposed TF-S, TF-SS and TFSA can obtain further gains up to 4.9 BLEU points, at least 1.2 BLEU points."
    }, {
      "heading" : "5.3.2 On the General WMT Datasets",
      "text" : "It is important to mention that all previous TMbased approaches failed in getting notable improvements on the general WMT datasets. Since Xia et al. (2019) did not conduct experiments on the WMT datasets and their implementation is not released, we compare our models with two baselines: TF and TF-P. Our experimental results on the general WMT datasets are reported in Table 9. As we\ncan see, the method TF-P is only comparable to the baseline NMT, which is in line with the observation in Zhang et al. (2018). In contrast, our models perform well on these tasks. Our best model gains about 0.7 BLEU points on the En→De and 1.0 BLEU point on the Zh→En task, over both baselines on average. The experimental results demonstrate that a TM based translation model can advance strong MT baselines on general translation tasks where a TM is not very similar to input source sentences. What’s more, as shown in Table 5, our models can get excellent translation results while a perfect TM is provided.\nIn a summary, based on the above extensive experimental results, our proposed models substantially surpass several baselines on TM-specialized tasks and general tasks, in terms of BLEU and running time."
    }, {
      "heading" : "6 Related Work",
      "text" : "In the statistical machine translation (SMT) diagram, Koehn and Senellart (2010a) extract bilingual segments from a TM which matches the source sentence to be translated, and employ a heuristic score to decide whether the extracted segments should be used as decoding constraints or not, then hardly constrain SMT to decode for those unmatched parts of the source sentence. Ma et al. (2011) design a fine-grained classifier, rather than the heuristic score, to predict the score for making more reliable decisions. Simard and Isabelle (2009), Wang et al. (2013) and Wang et al. (2014) add the extracted bilingual segments to the translation table of SMT, and then bias the decoder in a\nsoft constraint manner when decoding the source sentence with the augmented translation table. Liu et al. (2012) use the retrieved bilingual sentences to update the parameters for the log-linear model based SMT.\nIn recent years, many efforts are made on neural machine translation (NMT) associated with a TM. Li et al. (2016) and Farajian et al. (2017) make full use of the retrieved TM sentence pairs to fine-tune the pre-trained NMT model on-the-fly. The most obvious drawback of fine-tuning is that the delay is too long for testing sentences. To avoid the online tuning process, Zhang et al. (2018) and He et al. (2019) dynamically integrate translation pieces, based on n-grams extracted from the matched segments in the TM target, into the beam search stage. The second type of approach is efficient but heavily depends on the global hyper-parameter λ, which is sensitive to the development set, leading to inferior performance.\nRecently, there are notable approaches for the sake of further excavation on TM-based NMT. Bulte and Tezcan (2019) and Xu et al. (2020) propose data augmentation approaches by augmenting input sentences with a TM which do not modify the NMT model architecture. Gu et al. (2018) and Xia et al. (2019) employ an auxiliary network to encode TMs and integrate it into the NMT architecture. Our model architecture is simpler than Gu et al. (2018) and Xia et al. (2019) and we encode a single TM target sentence and utilize simple attention mechanisms on the TM. And the architecture is more efficient and leads to a faster translation speed compared with Gu et al. (2018) and Xia et al. (2019). In particular, we propose a novel training criterion to make the TM-based NMT model more robust in different translation situations (with or without a TM). In parallel with our work, Cai et al. (2021) extend the translation memory from the bilingual setting to the monolingual setting through a cross-lingual retrieval technique, and Khandelwal et al. (2021) report significant improvements in quality on general translation tasks as ours, but their inference speed is two orders of magnitude slower than Transformer because they perform contextual word retrieval whose search space is much larger than that of sentence retrieval."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper presents a simple TM-based NMT model that employs a single bilingual sentence as\nits TM and thus is fast in training and inference. Although the presented model with the standard training outperforms strong TM-based baselines, it suffers from a robustness issue: its performance highly depends on the similarity of a TM. To address this issue, we propose a novel training criterion inspired by multiple-task learning and data augmentation. Experiments on TM-specialized tasks demonstrate its superiority over strong baselines in terms of running time and BLEU. Also, it is shown that a TM-based NMT model can advance the strong Transformer on general translation tasks like WMT."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by NSFC (grant No. 61877051). We thank Jiatao Gu and Mengzhou Xia for providing their preprocessed datasets. We also thank the anonymous reviewers for providing valuable suggestions and feedbacks."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ArXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2016",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "A notion of task relatedness yielding provable multiple-task learning guarantees",
      "author" : [ "Shai Ben-David", "Reba Schuller Borbely." ],
      "venue" : "Mach. Learn., 73(3):273–287.",
      "citeRegEx" : "Ben.David and Borbely.,? 2008",
      "shortCiteRegEx" : "Ben.David and Borbely.",
      "year" : 2008
    }, {
      "title" : "Apache lucene 4",
      "author" : [ "Andrzej Bialecki", "Robert Muir", "Grant Ingersoll." ],
      "venue" : "Proceedings of the SIGIR 2012 Workshop on Open Source Information Retrieval, OSIR@SIGIR 2012, pages 17–24.",
      "citeRegEx" : "Bialecki et al\\.,? 2020",
      "shortCiteRegEx" : "Bialecki et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural fuzzy repair: Integrating fuzzy matches into neural machine translation",
      "author" : [ "Bram Bulte", "Arda Tezcan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800–1809, Florence, Italy.",
      "citeRegEx" : "Bulte and Tezcan.,? 2019",
      "shortCiteRegEx" : "Bulte and Tezcan.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation with monolingual translation memory",
      "author" : [ "Deng Cai", "Yan Wang", "Huayang Li", "Wai Lam", "Lemao Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Cai et al\\.,? 2021",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2021
    }, {
      "title" : "Maskalign: Self-supervised neural word alignment",
      "author" : [ "Chi Chen", "Maosong Sun", "Yang Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Word alignment by fine-tuning embeddings on parallel corpora",
      "author" : [ "Zi-Yi Dou", "Graham Neubig." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2112–2128.",
      "citeRegEx" : "Dou and Neubig.,? 2021",
      "shortCiteRegEx" : "Dou and Neubig.",
      "year" : 2021
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association of Computational Linguistics (NAACL 2013),",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "The art of data augmentation",
      "author" : [ "David A van Dyk", "Xiao-Li Meng." ],
      "venue" : "Journal of Computational and Graphical Statistics, 10(1):1–50.",
      "citeRegEx" : "Dyk and Meng.,? 2001",
      "shortCiteRegEx" : "Dyk and Meng.",
      "year" : 2001
    }, {
      "title" : "Data augmentation for low-resource neural machine translation",
      "author" : [ "Marzieh Fadaee", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567–",
      "citeRegEx" : "Fadaee et al\\.,? 2017",
      "shortCiteRegEx" : "Fadaee et al\\.",
      "year" : 2017
    }, {
      "title" : "Backtranslation sampling by targeting difficult words in neural machine translation",
      "author" : [ "Marzieh Fadaee", "Christof Monz." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 436–446.",
      "citeRegEx" : "Fadaee and Monz.,? 2018",
      "shortCiteRegEx" : "Fadaee and Monz.",
      "year" : 2018
    }, {
      "title" : "Multi-domain neural machine translation through unsupervised adaptation",
      "author" : [ "M. Amin Farajian", "Marco Turchi", "Matteo Negri", "Marcello Federico." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 127–137.",
      "citeRegEx" : "Farajian et al\\.,? 2017",
      "shortCiteRegEx" : "Farajian et al\\.",
      "year" : 2017
    }, {
      "title" : "Beyond translation memory: Computers and the professional translator",
      "author" : [ "Ignacio Garcia." ],
      "venue" : "The Journal of Specialised Translation, 12(12):199–214.",
      "citeRegEx" : "Garcia.,? 2009",
      "shortCiteRegEx" : "Garcia.",
      "year" : 2009
    }, {
      "title" : "Search engine guided nonparametric neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018), pages 5133–5140.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Word position aware translation memory for neural machine translation",
      "author" : [ "Qiuxiang He", "Guoping Huang", "Lemao Liu", "Li Li." ],
      "venue" : "Proceedings of the 8th CCF International Conference on Natural Language Processing and Chinese Computing, pages",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Transmart: A practical interactive machine translation system",
      "author" : [ "Guoping Huang", "Lemao Liu", "Xing Wang", "Longyue Wang", "Huayang Li", "Zhaopeng Tu", "Chengyan Huang", "Shuming Shi." ],
      "venue" : "ArXiv preprint arXiv:2105.13072.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
      "author" : [ "Masoud Jalili Sabet", "Philipp Dufter", "François Yvon", "Hinrich Schütze." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Sabet et al\\.,? 2020",
      "shortCiteRegEx" : "Sabet et al\\.",
      "year" : 2020
    }, {
      "title" : "Nearest neighbor machine translation",
      "author" : [ "Urvashi Khandelwal", "Angela Fan", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis." ],
      "venue" : "Proceedings of the 2021 International Conference on Learning Representations.",
      "citeRegEx" : "Khandelwal et al\\.,? 2021",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ArXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Scheduled multi-task learning: From syntax to translation",
      "author" : [ "Eliyahu Kiperwasser", "Miguel Ballesteros." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:225–240.",
      "citeRegEx" : "Kiperwasser and Ballesteros.,? 2018",
      "shortCiteRegEx" : "Kiperwasser and Ballesteros.",
      "year" : 2018
    }, {
      "title" : "Convergence of translation memory and statistical machine translation",
      "author" : [ "Philipp Koehn", "Jeaf Senellart." ],
      "venue" : "Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21–31.",
      "citeRegEx" : "Koehn and Senellart.,? 2010a",
      "shortCiteRegEx" : "Koehn and Senellart.",
      "year" : 2010
    }, {
      "title" : "Convergence of translation memory and statistical machine translation",
      "author" : [ "Philipp Koehn", "Jean Senellart." ],
      "venue" : "Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21–31.",
      "citeRegEx" : "Koehn and Senellart.,? 2010b",
      "shortCiteRegEx" : "Koehn and Senellart.",
      "year" : 2010
    }, {
      "title" : "Understanding data augmentation in neural machine translation: Two perspectives towards generalization",
      "author" : [ "Guanlin Li", "Lemao Liu", "Guoping Huang", "Conghui Zhu", "Tiejun Zhao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "One sentence one model for neural machine translation",
      "author" : [ "Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "ArXiv preprint arXiv:1609.06490.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Locally training the log-linear model for SMT",
      "author" : [ "Lemao Liu", "Hailong Cao", "Taro Watanabe", "Tiejun Zhao", "Mo Yu", "Conghui Zhu." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-",
      "citeRegEx" : "Liu et al\\.,? 2012",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Neural machine translation with supervised attention",
      "author" : [ "Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified framework and models for integrating translation memory into phrase-based statistical machine translation",
      "author" : [ "Yang Liu", "Kun Wang", "Chengqing Zong", "Keh-Yih Su." ],
      "venue" : "Comput. Speech Lang., 54:176–206.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Consistent translation using discriminative learning: A translation memory-inspired approach",
      "author" : [ "Yanjun Ma", "Yifan He", "Andy Way", "Josef van Genabith." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ma et al\\.,? 2011",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2011
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ACL 2002, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Joint chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning",
      "author" : [ "Xipeng Qiu", "Jiayi Zhao", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Qiu et al\\.,? 2013",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2013
    }, {
      "title" : "Becoming a Translator: An Introduction to the Theory and Practice of Translation",
      "author" : [ "Douglas Robinson." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Robinson.,? 2012",
      "shortCiteRegEx" : "Robinson.",
      "year" : 2012
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Phrase-based machine translation in a computer-assisted translation environment",
      "author" : [ "Michel Simard", "Pierre Isabelle." ],
      "venue" : "Proceedings of the Twelfth Machine Translation Summit (MT Summit XII), pages 120–127.",
      "citeRegEx" : "Simard and Isabelle.,? 2009",
      "shortCiteRegEx" : "Simard and Isabelle.",
      "year" : 2009
    }, {
      "title" : "Searching translation memories for paraphrases",
      "author" : [ "Masao Utiyama", "Graham Neubig", "Takashi Onishi", "Eiichiro Sumita." ],
      "venue" : "Machine Translation Summit, volume 13, pages 325–331.",
      "citeRegEx" : "Utiyama et al\\.,? 2011",
      "shortCiteRegEx" : "Utiyama et al\\.",
      "year" : 2011
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Integrating translation memory into phrase-based machine translation during decoding",
      "author" : [ "Kun Wang", "Chengqing Zong", "Keh-Yih Su." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11–21.",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamically integrating cross-domain translation memory into phrase-based machine translation during decoding",
      "author" : [ "Kun Wang", "Chengqing Zong", "Keh-Yih Su." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "SwitchOut: an efficient data augmentation algorithm for neural machine translation",
      "author" : [ "Xinyi Wang", "Hieu Pham", "Zihang Dai", "Graham Neubig." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task learning for multilingual neural machine translation",
      "author" : [ "Yiren Wang", "ChengXiang Zhai", "Hany Hassan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1022–1034.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph based translation memory for neural machine translation",
      "author" : [ "Mengzhou Xia", "Guoping Huang", "Lemao Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), pages 7297–7304.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Boosting neural machine translation with similar translations",
      "author" : [ "Jitao Xu", "Josep Crego", "Jean Senellart." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Guiding neural machine translation with retrieved translation pieces",
      "author" : [ "Jingyi Zhang", "Masao Utiyama", "Eiichro Sumita", "Graham Neubig", "Satoshi Nakamura." ],
      "venue" : "Proceedings of the 16th Annual Conference of the North American Chapter of the",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Random erasing data augmentation",
      "author" : [ "Zhun Zhong", "Liang Zheng", "Guoliang Kang", "Shaozi Li", "Yi Yang." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, pages 13001–13008.",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021).",
      "startOffset" : 208,
      "endOffset" : 308
    }, {
      "referenceID" : 22,
      "context" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021).",
      "startOffset" : 208,
      "endOffset" : 308
    }, {
      "referenceID" : 35,
      "context" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021).",
      "startOffset" : 208,
      "endOffset" : 308
    }, {
      "referenceID" : 31,
      "context" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021).",
      "startOffset" : 208,
      "endOffset" : 308
    }, {
      "referenceID" : 16,
      "context" : "A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021).",
      "startOffset" : 208,
      "endOffset" : 308
    }, {
      "referenceID" : 34,
      "context" : "A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 349
    }, {
      "referenceID" : 21,
      "context" : "A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 349
    }, {
      "referenceID" : 28,
      "context" : "A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 349
    }, {
      "referenceID" : 37,
      "context" : "A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 349
    }, {
      "referenceID" : 27,
      "context" : "A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 349
    }, {
      "referenceID" : 0,
      "context" : "where h i indicates the ith hidden unit at Lth layer in the Decoding phrase under the encoderdecoder framework (Bahdanau et al., 2016), and φ is a linear network that projects hidden units onto vectors with dimension of the target vocabulary.",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 36,
      "context" : "The state-of-theart NMT model is Transformer (Vaswani et al., 2017), which uses stacked self-attention and fully connected layers for its encoder and decoder.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Following previous works (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), for each source",
      "startOffset" : 25,
      "endOffset" : 80
    }, {
      "referenceID" : 43,
      "context" : "Following previous works (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), for each source",
      "startOffset" : 25,
      "endOffset" : 80
    }, {
      "referenceID" : 41,
      "context" : "Following previous works (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), for each source",
      "startOffset" : 25,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "sentence x we employ Apache Lucene (Bialecki et al., 2020) to retrieve top-100 similar bilingual sentences from the training data.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "So, we further obtain the word alignment between xtm and ytm through fast-align toolkit (Dyer et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "Although some advanced word alignment toolkits (Dou and Neubig, 2021; Chen et al., 2021; Jalili Sabet et al., 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al.",
      "startOffset" : 47,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "Although some advanced word alignment toolkits (Dou and Neubig, 2021; Chen et al., 2021; Jalili Sabet et al., 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al.",
      "startOffset" : 47,
      "endOffset" : 115
    }, {
      "referenceID" : 43,
      "context" : ", 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al., 2018; Xia et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 158
    }, {
      "referenceID" : 41,
      "context" : ", 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al., 2018; Xia et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "Training criterion In order to avoid the TM over-fitting, we propose a simple yet elegant method, inspired by data augmentation (van Dyk and Meng, 2001; Li et al., 2019; Zhong et al., 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al.",
      "startOffset" : 128,
      "endOffset" : 189
    }, {
      "referenceID" : 44,
      "context" : "Training criterion In order to avoid the TM over-fitting, we propose a simple yet elegant method, inspired by data augmentation (van Dyk and Meng, 2001; Li et al., 2019; Zhong et al., 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al.",
      "startOffset" : 128,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : ", 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : ", 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : ", 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 100
    }, {
      "referenceID" : 32,
      "context" : "Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al.",
      "startOffset" : 84,
      "endOffset" : 171
    }, {
      "referenceID" : 10,
      "context" : "Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al.",
      "startOffset" : 84,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al.",
      "startOffset" : 84,
      "endOffset" : 171
    }, {
      "referenceID" : 39,
      "context" : "Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al.",
      "startOffset" : 84,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : ", 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : ", 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 40,
      "context" : ", 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research.",
      "startOffset" : 35,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "In our experiments, we employ Adam (Kingma and Ba, 2014) with default settings as the learning rate schema.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "We use the case-insensitive BLEU score as the automatic metric (Papineni et al., 2002) for the translation quality evaluation.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 33,
      "context" : "In addition, we employ Byte Pair Encoding (BPE) (Sennrich et al., 2016b) on all the tasks mentioned before.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : "Baseline systems We compare our proposed model with the strong baselines as follows: • TF (Vaswani et al., 2017): it is the standard Transformer.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : "• TF-P (Zhang et al., 2018): it is reimplemented on top of Transformer by ourselves.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and TF-SEQ (Gu et al., 2018): TF-SEQ is a mimic implementation over Transformer by Xia et al.",
      "startOffset" : 19,
      "endOffset" : 36
    } ],
    "year" : 2021,
    "abstractText" : "It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TMbased neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh→En and En→De).",
    "creator" : "LaTeX with hyperref"
  }
}