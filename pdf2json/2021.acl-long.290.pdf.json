{
  "name" : "2021.acl-long.290.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Surprisal Estimators for Human Reading Times Need Character Models",
    "authors" : [ "Byung-Doh Oh", "Christian Clark", "William Schuler" ],
    "emails" : [ "schuler.77}@osu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3746–3757\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3746"
    }, {
      "heading" : "1 Introduction and Related Work",
      "text" : "Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context. In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013). However, surprisal itself makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model.\nIn natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively\nused to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at the word level (e.g. Futrell et al. 2019; Goodkind and Bicknell 2018; Hale et al. 2018; Hao et al. 2020). This raises the question of whether incorporating character-level information into an incremental processing model will result in surprisal estimates that better characterize predictability in context.\nTo answer this question, this paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.1 The proposed model defines a process of generating a word from an underlying lemma and a morphological rule, which allows the processing model to capture the predictability of a given word form in a fine-grained manner. Regression analyses on self-paced reading, eye-tracking, and fMRI data demonstrate that surprisal estimates calculated from this character-based structural processing model contribute to substantially better fits compared to those calculated from large-scale language models, despite the fact that these other models are trained on much more data and show lower perplexities on test data. This finding deviates from the monotonic relationship between test perplexity and predictive power observed in previous studies (Goodkind and Bicknell, 2018; Wilcox et al., 2020). Furthermore, it suggests that the character-based structural processing model may provide a more humanlike account of processing difficulty and may suggest a larger role of morphology, phonotactics, and orthographic complexity than was previously\n1Code for model and experiments is available at https: //github.com/byungdoh/acl21_semproc.\nthought."
    }, {
      "heading" : "2 Background",
      "text" : "The experiments presented in this paper use surprisal predictors (Shannon, 1948) calculated by an incremental processing model based on a leftcorner parser (Johnson-Laird, 1983; van Schijndel et al., 2013). This incremental processing model provides a probabilistic account of sentence processing by making a single lexical attachment decision and a single grammatical attachment decision for each input word.\nSurprisal. Surprisal can be defined as the negative log ratio of prefix probabilities of word sequences w1..t at consecutive time steps t − 1 and t:\nS(wt) def = − log P(w1..t)\nP(w1..t−1) (1)\nThese prefix probabilities can be calculated by marginalizing over the hidden states qt of the forward probabilities of an incremental processing model:\nP(w1..t) = ∑\nqt\nP(w1..t qt) (2)\nThese forward probabilities are in turn defined recursively using a transition model:\nP(w1..t qt) def = ∑ qt−1 P(wt qt | qt−1) · P(w1..t−1 qt−1)\n(3) Left-corner parsing. The transition model presented in this paper is based on a probabilistic leftcorner parser (Johnson-Laird, 1983; van Schijndel et al., 2013). Left-corner parsers have been used to model human sentence processing because they define a fixed number of decisions at every time step and also require only a bounded amount of working memory, in keeping with experimental observations of human memory limits (Miller and Isard, 1963). The transition model maintains a distribution over possible working memory store states qt at every time step t, each of which consists of a bounded number D of nested derivation fragments adt /b d t . Each derivation fragment spans a part of a derivation tree from some apex node adt lacking a base node bdt yet to come. Previous work has shown that large annotated corpora such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 of such fragments (Schuler et al., 2010).\nAt each time step, a left-corner parsing model generates a new word wt and a new store state qt\nin two phases (see Figure 1). First, it makes a lexical decision `t regarding whether to use the word to complete the most recent derivation fragment (match), or to use the word to create a new preterminal node a`t (no-match). Subsequently, the model makes a grammatical decision gt regarding whether to use a predicted grammar rule to combine the node constructed in the lexical phase a`t with the next most recent derivation fragment (match), or to use the grammar rule to convert this node into a new derivation fragment agt/bgt (no-match): 2\nP(wt qt | qt−1) = ∑ `t ,gt\nP(`t | qt−1) · P(wt | qt−1 `t) · P(gt | qt−1 `t wt) · P(qt | qt−1 `t wt gt) (4)\nThus, the parser creates a hierarchically organized sequence of derivation fragments and joins these fragments up whenever expectations are satisfied.\nIn order to update the store state based on the lexical and grammatical decisions, derivation fragments above the most recent nonterminal node are carried forward, and derivation fragments below it are set to null (⊥):\nP(qt | . . .) def = D∏ d′=1  Jad′t , bd ′ t = a d′ t−1, b d′ t−1K if d′ < d Jad′t , bd ′\nt = agt , bgtK if d′ = d Jad′t , bd ′ t = ⊥,⊥K if d′ > d\n(5) where the indicator function JϕK = 1 if ϕ is true and 0 otherwise, and d = argmaxd′{ad ′\nt−1,⊥} + 1 − m`t − mgt . Together, these probabilistic decisions generate the n unary branches and n − 1 binary branches of a parse tree in Chomsky normal form for an n-word sentence."
    }, {
      "heading" : "3 Model",
      "text" : ""
    }, {
      "heading" : "3.1 Processing Model",
      "text" : "The processing model extends the above left-corner parser to maintain lemmatized predicate information by augmenting each preterminal, apex, and base node to consist not only of a syntactic category label cpt , cadt , or cbdt , but also of a binary predicate context vector hpt , hadt , or hbdt ∈ {0, 1}\nK+V ·K , where K is the size of the set of predicate contexts and V is the maximum valence of any syntactic\n2Johnson-Laird (1983) refers to lexical and grammatical decisions as ‘shift’ and ‘predict’ respectively.\ncategory.3 Each 0 or 1 element of this vector represents a unique predicate context, which consists of a 〈predicate, role〉 pair that specifies the content constraints of a node in a predicate-argument structure. These predicate contexts are obtained by reannotating the training corpus using a generalized categorial grammar of English (Nguyen et al., 2012),4 which is sensitive to syntactic valence and non-local dependencies.\nLexical decisions. Each lexical decision of the parser includes a match decision m`t and decisions about a syntactic category c`t and a predicate context vector h`t that together specify a preterminal node p`t . The probability of generating the match decision and the predicate context vector depends on the base node bdt−1 of the previous derivation fragment (i.e. its syntactic category and predicate context vector). The first term of Equation 4 can therefore be decomposed into the following:\nP(`t | qt−1) = SOFTMAX m`t h`t ( FFθL[δd >, [δ>cbdt−1 ,h> bdt−1 ] EL] ) ·\nP(c`t | qt−1 m`t h`t ) (6)\nwhere FF is a feedforward neural network, and δi is a Kronecker delta vector consisting of a one at element i and zeros elsewhere. Depth d = argmaxd′{ad ′\nt−1,⊥} is the number of non-null derivation fragments at the previous time step, and EL is a matrix of jointly trained dense embeddings for each syntactic category and predicate context. The syntactic category and predicate context vector\n3The valence of a category is the number of unsatisfied syntactic arguments it has. Separate vectors for syntactic arguments are needed in order to correctly model cases such as passives where syntactic arguments do not align with predicate arguments.\n4The predicates in this annotation scheme come from words that have been lemmatized by a set of rules that have been manually written and corrected in order to account for common irregular inflections.\ntogether define a complete preterminal node p`t for use in the word generation model:\np`t def = cbdt−1,hbdt−1+ h`t if m`t = 1c`t ,h`t if m`t = 0 (7) and a new apex node a`t for use in the grammatical decision model:\na`t def = adt−1 if m`t = 1p`t if m`t = 0 (8) Grammatical decisions. Each grammatical decision includes a match decision mgt and decisions about a pair of syntactic category labels cgt and c ′ gt , as well as a predicate context composition operator ogt , which governs how the newly generated predicate context vector h`t is propagated through its new derivation fragment agt/bgt . The probability of generating the match decision and the composition operators depends on the base node bd−m`tt−1 of the previous derivation fragment and the apex node a`t from the current lexical decision (i.e. their syntactic categories and predicate context vectors). The third term of Equation 4 can accordingly be decomposed into the following:\nP(gt | qt−1 `t wt) = SOFTMAX\nmgt ogt ( FFθG[δd >, [δ>c b\nd−m`t t−1\n,h> b\nd−m`t t−1\n, δ>ca`t ,h>a`t ] EG] ) ·\nP(cgt | qt−1 `t wt mgt ogt ) · P(c′gt | qt−1 `t wt mgt ogt cgt ) (9)\nwhere EG is a matrix of jointly trained dense embeddings for each syntactic category and predicate context. The composition operators are associated with sparse composition matrices Aogt which can be used to compose predicate context vectors associated with the apex node agt :\nagt def = ad−m`tt−1 if mgt = 1cgt ,Aogt ha`t if mgt = 0 (10)\nand sparse composition matrices Bogt which can be used to compose predicate context vectors associated with the base node bgt :\nbgt def = c ′ gt ,Bogt [hbd−m`tt−1 >,ha`t >]> if mgt=1\nc′gt ,Bogt [0 >,ha`t >]> if mgt=0 (11)"
    }, {
      "heading" : "3.2 Character-based Word Model",
      "text" : "The baseline version of the word model P(wt | qt−1 `t) uses relative frequency estimation with backoff probabilities for out-of-vocabulary words trained using hapax legomena. A character-based test version of this model instead applies a morphological rule rt to a lemma xt to generate an inflected form wt. The set of rules model affixation through string substitution and are inverses of lemmatization rules that are used to derive predicates in the generalized categorial grammar annotation (Nguyen et al., 2012). For example, the rule %ay→%aid can apply to the word say to derive its past tense form said. There are around 600 such rules that account for inflection in Sections 02 to 21 of the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993), which includes an identity rule for words in bare form and a ‘no semantics’ rule for generating certain function words.\nFor an observed input word wt, the model first generates a list of 〈xt, rt〉 pairs that deterministically generate wt. This allows the model to capture morphological regularity and estimate how expected a word form is given its predicted syntactic category and predicate context, which have been generated as part of the preceding lexical decision. In addition, this lets the model hypothesize the underlying morphological structure of out-of-vocabulary words and assign probabilities to them. The second term of Equation 4 can thus be decomposed into the following:\nP(wt | qt−1 `t) = ∑ xt ,rt\nP(xt | qt−1 `t) · P(rt | qt−1 `t xt) · P(wt | qt−1 `t xt rt) (12)\nThe probability of generating the lemma sequence depends on the syntactic category cp`t and predicate context h`t resulting from the preceding lexical decision `t:\nP(xt | qt−1 `t) = ∏\ni\nSOFTMAX xt,i ( WX xt,i + bX )\n(13)\nwhere xt,1, xt,2, ..., xt,I is the character sequence of lemma xt, with xt,1 = 〈s〉 and xt,I = 〈e〉 as special start and end characters. WX and bX are respectively a weight matrix and bias vector of a softmax classifier. A recurrent neural network (RNN) calculates a hidden state xt,i for each character from an input vector at that time step and the hidden state after the previous character xt,i−1:\nxt,i = RNNθX( [δ > cp`t ,h>`t , δ > xt,i] EX, x > t,i−1 ) (14)\nwhere EX is a matrix of jointly trained dense embeddings for each syntactic category, predicate context, and character.\nSubsequently, the probability of applying a particular morphological rule to the generated lemma depends on the syntactic category cp`t and predicate context h`t from the preceding lexical decision as well as the character sequence of the lemma:\nP(rt | qt−1 `t xt) = SOFTMAX rt ( WR rt,I + bR ) (15)\nwhere WR and bR are respectively a weight matrix and bias vector of a softmax classifier. rt,I is the last hidden state of an RNN that takes as input the syntactic category, predicate context, and character sequence of the lemma xt,2, xt,3, ..., xt,I−1 without the special start and end characters:\nrt,i = RNNθR( [δ > cp`t ,h>`t , δ > xt,i] ER, r > t,i−1 ) (16)\nwhere ER is a matrix of jointly trained dense embeddings for each syntactic category, predicate context, and character.\nFinally, as the model calculates probabilities only for 〈xt, rt〉 pairs that deterministically generate wt, the word probability conditioned on these variables P(wt | qt−1 `t xt rt) is deterministic."
    }, {
      "heading" : "4 Experiment 1: Effect of Character Model",
      "text" : "In order to assess the influence of the characterbased word generation model over the baseline word generation model on the predictive quality of surprisal estimates, linear mixed-effects models containing common baseline predictors and one or more surprisal predictors were fitted to self-paced reading times. Subsequently, a series of likelihood ratio tests were conducted in order to evaluate the relative contribution of each surprisal predictor to regression model fit."
    }, {
      "heading" : "4.1 Response Data",
      "text" : "The first experiment described in this paper used the Natural Stories Corpus (Futrell et al., 2018), which contains self-paced reading times from 181 subjects that read 10 naturalistic stories consisting of 10,245 tokens. The data were filtered to exclude observations corresponding to sentenceinitial and sentence-final words, observations from subjects who answered fewer than four comprehension questions correctly, and observations with durations shorter than 100 ms or longer than 3000 ms. This resulted in a total of 768,584 observations, which were subsequently partitioned into an exploratory set of 383,906 observations and a held-out set of 384,678 observations. The partitioning allows model selection (e.g. making decisions about predictors and random effects structure) to be conducted on the exploratory set and a single hypothesis test to be conducted on the held-out set, thus eliminating the need for multiple trials correction. All observations were log-transformed prior to model fitting."
    }, {
      "heading" : "4.2 Predictors",
      "text" : "The baseline predictors commonly included in all regression models are word length measured in characters and index of word position within each sentence.5 In addition to the baseline predictors, surprisal predictors were calculated from two variants of the processing model in which word generation probabilities P(wt | qt−1 `t) are calculated using relative frequency estimation (FreqWSurp) and using the character-based model described in Section 3.2 (CharWSurp). Both variants of the processing model were trained on a generalized categorial grammar (Nguyen et al., 2012) reannotation of Sections 02 to 21 of the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). Beam search decoding with a beam size of 5,000 was used to estimate prefix probabilities and surprisal predictors for both variants.\nTo account for the time the brain takes to process and respond to linguistic input, it is standard practice in psycholinguistic modeling to include ‘spillover’ variants of predictors from preceding words (Rayner et al., 1983; Vasishth, 2006). However, as including multiple spillover variants of predictors leads to identifiability issues in mixed-\n5Although unigram surprisal or 5-gram surprisal is also commonly included as a baseline predictor, it was not included in this experiment due to convergence issues.\neffects modeling (Shain and Schuler, 2019), CharWSurp and FreqWSurp were both spilled over by one position. All predictors were centered and scaled prior to model fitting, and all regression models included by-subject random slopes for all fixed effects as well as random intercepts for each word and subject-sentence interaction, following the convention of keeping the random effects structure maximal in psycholinguistic modeling (Barr et al., 2013)."
    }, {
      "heading" : "4.3 Likelihood Ratio Testing",
      "text" : "A total of three linear mixed-effects models were fitted to reading times in the held-out set using lme4 (Bates et al., 2015); the full model included the fixed effects of both CharWSurp and FreqWSurp, and the two ablated models included the fixed effect of either CharWSurp or FreqWSurp. This resulted in two pairs of nested models whose fit could be compared through a likelihood ratio test (LRT). The first LRT tested the contribution of CharWSurp by comparing the fit of the full regression model to that of the regression model without the fixed effect of CharWSurp. Similarly, the second LRT tested the contribution of FreqWSurp by comparing the fit of the full regression model to that of the regression model without its fixed effect."
    }, {
      "heading" : "4.4 Results",
      "text" : "The results in Table 1 show that the contribution of CharWSurp in predicting reading times is statistically significant over and above that of FreqWSurp (p < 0.0001), while the converse is not significant (p = 0.8779). This demonstrates that incorporating a character-based word generation model to the structural processing model better captures predictability in context, subsuming the effects of the processing model without it."
    }, {
      "heading" : "5 Experiment 2: Comparison to Other Models",
      "text" : "To further examine the impact of the characterbased word generation model, CharWSurp and Fre-\nqWSurp were evaluated against surprisal predictors calculated from a number of other large-scale pretrained language models and smaller parser-based models. To compare the predictive power of surprisal estimates from different language models on equal footing, we calculated the increase in loglikelihood (∆LL) to a baseline regression model as a result of including a surprisal predictor, following recent work (Goodkind and Bicknell, 2018; Hao et al., 2020)."
    }, {
      "heading" : "5.1 Surprisal Estimates from Other Models",
      "text" : "A total of three pretrained language models were used to calculate surprisal estimates at each word.6\n• GLSTMSurp (Gulordava et al., 2018): A twolayer LSTM model trained on ∼80M tokens of the English Wikipedia.\n• JLSTMSurp (Jozefowicz et al., 2016): A twolayer LSTM model with CNN character inputs trained on ∼800M tokens of the 1B Word Benchmark (Chelba et al., 2014).\n• GPT2Surp (Radford et al., 2019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents).\nIn addition, three incremental parsing models were used to calculate surprisal estimates:\n• RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the WSJ corpus.\n• vSLCSurp (van Schijndel et al., 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus.\n• JLCSurp (Jin and Schuler, 2020): A neural leftcorner parser based on stack LSTMs (Dyer et al., 2015), trained on Sections 02 to 21 of the WSJ corpus."
    }, {
      "heading" : "5.2 Procedures",
      "text" : "The set of self-paced reading times from the Natural Stories Corpus after applying the same data exclusion criteria as Experiment 1 provided the response variable for the regression models. In addition to the full dataset, regression models were\n6Please refer to the appendix for surprisal calculation, outof-vocabulary handling, and re-initialization procedures.\nalso fitted to a ‘no out-of-vocabulary (No-OOV)’ version of the dataset, in which observations corresponding to out-of-vocabulary words for the LSTM language model with the smallest vocabulary (i.e. Gulordava et al., 2018) were also excluded. This exclusion criterion was included in order to avoid putting the LSTM language models that may have unreliable surprisal estimates for out-of-vocabulary words at an unfair disadvantage. This resulted in a total of 744,607 observations in the No-OOV dataset, which were subsequently partitioned into an exploratory set of 371,937 observations and a held-out set of 372,670 observations. All models were fitted to the held-out set, and all observations were log-transformed prior to model fitting.\nThe predictors included in the baseline linear mixed-effects model were word length, word position in sentence, and unigram surprisal. Unigram surprisal was calculated using the KenLM toolkit (Heafield et al., 2013) with parameters trained on the Gigaword 4 corpus (Parker et al., 2009). In order to calculate the increase in log-likelihood (∆LL) attributable to each surprisal predictor, a ‘full’ linear-mixed effects model, which includes one surprisal predictor on top of the baseline model, was fitted for each surprisal predictor. As with Experiment 1, the surprisal predictors were spilled over by one position. All predictors were centered and scaled prior to model fitting, and all regression models included by-subject random slopes for all fixed effects and random intercepts for each word and subject-sentence interaction.\nAdditionally, in order to examine whether any of the models fail to generalize across domains, their perplexity on the entire Natural Stories Corpus was also calculated."
    }, {
      "heading" : "5.3 Results",
      "text" : "The results show that surprisal from the characterbased structural model (CharWSurp) made the biggest contribution to model fit compared to surprisal from other models on both full and No-OOV sets of self-paced reading times (Figure 2; the difference between the model with CharWSurp and other models is significant with p < 0.001 by a paired permutation test using by-item errors). The exclusion of OOV words did not make a notable difference in the overall trend of ∆LL across models. This finding, despite the fact that the pretrained language models were trained on much\nlarger datasets and also show lower perplexities on test data,7 suggests that this model may provide a more humanlike account of processing difficulty. In other words, accurately predicting the next word alone does not fully explain humanlike processing costs that manifest in self-paced reading times. The analysis of residuals grouped by the lowest base category of the previous time step (cbdt−1) from manual annotations (Shain et al., 2018) shows that the improvement of CharWSurp over GPT2Surp was broad-based across categories (see Figure 3)."
    }, {
      "heading" : "6 Experiment 3: Eye-tracking Data",
      "text" : "In order to examine whether these results generalize to other latency-based measures, linear-mixed effects models were fitted on the Dundee eyetracking corpus (Kennedy et al., 2003) to test the contribution of each surprisal predictor, following similar procedures to Experiment 2."
    }, {
      "heading" : "6.1 Procedures",
      "text" : "The set of go-past durations from the Dundee Corpus (Kennedy et al., 2003) provided the response\n7Perplexity of the parsing models is higher partly because they optimize for a joint distribution over words and trees.\nvariable for the regression models. The Dundee Corpus contains gaze durations from 10 subjects that read 20 newspaper editorials consisting of 51,502 tokens. The data were filtered to exclude unfixated words, words following saccades longer than four words, and words at starts and ends of sentences, screens, documents, and lines. This resulted in the full set with a total of 195,296 observations, which were subsequently partitioned into an exploratory set of 97,391 observations and a held-out set of 97,905 observations. As with Experiment 2, regression models were also fitted to a No OOV version of the dataset, in which observations corresponding to out-of-vocabulary words for the Gulordava et al. (2018) model were also excluded. This resulted in a subset with a total of 184,894 observations (exploratory set of 92,272 observations, held-out set of 92,622 observations). All models were fitted to the held-out set, and all observations were log-transformed prior to model fitting.\nThe predictors included in the baseline linear mixed-effects models were word length, word position, and saccade length. In order to calculate the increase in log-likelihood from including each surprisal predictor, a full model including one sur-\nprisal predictor on top of the baseline model was fitted for each surprisal predictor. All surprisal predictors were spilled over by one position, and all predictors were centered and scaled prior to model fitting. All regression models included by-subject random slopes for all fixed effects and random intercepts for each word and sentence."
    }, {
      "heading" : "6.2 Results",
      "text" : "The results in Figure 4 show that as with Experiment 2, surprisal from the character-based structural model (CharWSurp) made the biggest contribution to model fit on both full and No-OOV sets of go-past durations (the difference between model with CharWSurp and other models is significant with p < 0.001 by a paired permutation test using by-item errors). In contrast to Natural Stories, surprisal from the two left-corner parsing models (i.e. vSLCSurp and JLCSurp) did not contribute to as much model fit compared to other models. The exclusion of OOV words again did not make a notable difference in the general trend across different models, although it led to an increase in ∆LL for GLSTMSurp and RNNGSurp. Residuals grouped by the lowest base category from the previous time\nstep show that, similarly to Natural Stories, the improvement of CharWSurp over GPT2Surp was broad-based across different categories (see Figure 5). These results provide further support for the observation that language models that are trained to predict the next word accurately do not fully explain processing cost in the form of latency-based measures."
    }, {
      "heading" : "7 Experiment 4: fMRI Data",
      "text" : "Finally, to examine whether a similar tendency is observed in brain responses, we analyzed the time series of blood oxygenation level-dependent (BOLD) signals in the language network, which were identified using functional magnetic resonance imaging (fMRI). To this end, the novel statistical framework of continuous-time deconvolutional regression (CDR; Shain and Schuler, 2019) was employed. As CDR allows the data-driven estimation of continuous impulse response functions from variably spaced linguistic input, it is more appropriate for modeling fMRI responses, which are typically measured in fixed time intervals. Similarly to the previous experiments, the increase in CDR model log-likelihood as a result of including a\nsurprisal predictor on top of a baseline CDR model was calculated for evaluation."
    }, {
      "heading" : "7.1 Procedures",
      "text" : "This experiment used the same fMRI data used by Shain et al. (2019), which were collected from 78 subjects that listened to a recorded version of the Natural Stories Corpus. The functional regions of interest (fROI) corresponding to the domainspecific language network were identified for each subject based on the results of a localizer task that they conducted. This resulted in a total of 202,295 observations, which were subsequently partitioned into an exploratory set of 100,325 observations and a held-out set of 101,970 observations by assigning alternate 60-second intervals of BOLD series to different partitions for each participant. All models were fitted to the BOLD signals in the held-out set.\nThe predictors included in the baseline CDR model were the index of current fMRI sample within the current scan, unigram surprisal, and the deconvolutional intercept which captures the influence of stimulus timing. Following Shain et al. (2019), the CDR models assumed the twoparameter HRF based on the double-gamma canonical HRF (Lindquist et al., 2009). Furthermore, the two parameters of the HRF were tied across predictors, modeling the assumption that the shape of the blood oxygenation response to neural activity is identical in a given region. However, to allow the HRFs to have differing amplitudes, a coefficient that rescales the HRF was estimated for each predictor. The models also included a by-fROI random effect for the amplitude coefficient and a by-subject random intercept.\nTo calculate the increase in log-likelihood from including each predictor, a full CDR model including the fixed effects of one surprisal predictor was also fitted for each surprisal predictor. All surprisal predictors were included without spillover,8 and all predictors were centered prior to model fitting."
    }, {
      "heading" : "7.2 Results",
      "text" : "The results in Figure 6 show that surprisal from GPT-2 (GPT2Surp) made the biggest contribution to model fit in comparison to surprisal from other models (difference between model with GPT2Surp and other models significant with p < 0.001 by a paired permutation test using by-item errors). Most\n8As CDR estimates continuous HRFs from variably spaced linguistic input, consideration of spillover variants of surprisal predictors was not necessary.\nnotably, in contrast to self-paced reading times and eye-gaze durations, CharWSurp did not contribute as much to model fit on fMRI data, with a ∆LL lower than those of the LSTM language models. This differential contribution of CharWSurp across datasets suggests that latency-based measures and blood oxygenation levels may capture different aspects of online processing difficulty."
    }, {
      "heading" : "8 Conclusion",
      "text" : "This paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model. Experiments demonstrate that surprisal estimates calculated from this processing model generally contribute to substantially better fits to human response data than those calculated from large-scale pretrained language models or other incremental parsers. These results add a new nuance to the relationship between perplexity and predictive power reported in previous work (Goodkind and Bicknell, 2018; Wilcox et al., 2020). In addition, they suggest that structural parser-based processing models may provide a more humanlike account of sentence processing, and may suggest a larger role of morphology, phonotactics, and orthographic complexity than was previously thought."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported by the National Science Foundation grant #1816891. All views expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation.\nEthical Considerations\nExperiments presented in this work used datasets from previously published research (Futrell et al., 2018; Kennedy et al., 2003; Marcus et al., 1993; Shain et al., 2019), in which the procedures for data collection and validation are outlined."
    }, {
      "heading" : "A Procedures for Surprisal Calculation",
      "text" : "• GLSTMSurp, JLSTMSurp: These models directly estimate P(wt | w1..t−1), which can be used to calculate S(wt) = − log P(wt | w1..t−1).\n• GPT2Surp: Since GPT-2 relies on byte-pair encoding (Sennrich et al., 2016), negative log probabilities of word pieces corresponding to wt were added together to calculate S(wt) = − log P(wt | w1..t−1).\n• RNNGSurp: Since the generative RNNG model defines a joint distribution over words and trees, we marginalize over trees to calculate P(wt | w1..t−1). To keep this tractable, a wordsynchronous beam search (Stern et al., 2017) was used with beam size 100, fast-track beam size 5, and word beam size 10.\n• vSLCSurp, JLCSurp: Beam search decoding with a beam size of 5,000 and 2,000 respectively was used to estimate prefix probabilities and surprisal predictors."
    }, {
      "heading" : "B Procedures for Out-of-vocabulary Handling",
      "text" : "• GLSTMSurp, JLSTMSurp, JLCSurp: Out-ofvocabulary (OOV) words in the test corpus were replaced with a corresponding “UNK” symbol prior to surprisal estimation.\n• GPT2Surp: Special OOV handling was not necessary because GPT-2 uses byte-pair encoding (Sennrich et al., 2016).\n• RNNGSurp, vSLCSurp: Mapping rules from the Berkeley parser9 were used to replace OOV words with a set of unknown word classes (e.g. “UNK-LC-ing”). 9https://github.com/slavpetrov/ berkeleyparser"
    }, {
      "heading" : "C Procedures for Hidden State Re-initialization",
      "text" : "• GLSTMSurp, JLSTMSurp, GPT2Surp: The hidden states of these models were re-initialized at the end of every article before making predictions on the next article.\n• RNNGSurp, vSLCSurp, JLCSurp: Since these models predict parsing operations while making word predictions, their hidden states were reinitialized after each sentence."
    } ],
    "references" : [ {
      "title" : "Recurrent neural network",
      "author" : [ "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Smith.,? \\Q2016\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2016
    }, {
      "title" : "A probabilistic Earley parser as a psy",
      "author" : [ "John Hale" ],
      "venue" : null,
      "citeRegEx" : "Hale.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hale.",
      "year" : 2001
    }, {
      "title" : "Finding syntax in human",
      "author" : [ "Jonathan Brennan" ],
      "venue" : null,
      "citeRegEx" : "Brennan.,? \\Q2018\\E",
      "shortCiteRegEx" : "Brennan.",
      "year" : 2018
    }, {
      "title" : "Mental models: Towards a cognitive science of language, inference, and consciousness",
      "author" : [ "Philip N. Johnson-Laird." ],
      "venue" : "Harvard University Press, Cambridge, MA.",
      "citeRegEx" : "Johnson.Laird.,? 1983",
      "shortCiteRegEx" : "Johnson.Laird.",
      "year" : 1983
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection",
      "author" : [ "Katharina Kann", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-",
      "citeRegEx" : "Kann and Schütze.,? 2016",
      "shortCiteRegEx" : "Kann and Schütze.",
      "year" : 2016
    }, {
      "title" : "The Dundee Corpus",
      "author" : [ "Alan Kennedy", "Robin Hill", "Joël Pynte." ],
      "venue" : "Proceedings of the 12th European conference on eye movement.",
      "citeRegEx" : "Kennedy et al\\.,? 2003",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2003
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2741– 2749.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:365–378.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Expectation-based syntactic comprehension",
      "author" : [ "Roger Levy." ],
      "venue" : "Cognition, 106(3):1126–1177.",
      "citeRegEx" : "Levy.,? 2008",
      "shortCiteRegEx" : "Levy.",
      "year" : 2008
    }, {
      "title" : "Modeling the hemodynamic response function in fMRI: Efficiency, bias and mismodeling",
      "author" : [ "Martin A. Lindquist", "Ji Meng Loh", "Lauren Y. Atlas", "Tor D. Wager." ],
      "venue" : "NeuroImage, 45(1, Supplement 1):S187– S198.",
      "citeRegEx" : "Lindquist et al\\.,? 2009",
      "shortCiteRegEx" : "Lindquist et al\\.",
      "year" : 2009
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Some perceptual consequences of linguistic rules",
      "author" : [ "George A. Miller", "Stephen Isard." ],
      "venue" : "Journal of Verbal Learning and Verbal Behavior, 2(3):217– 228.",
      "citeRegEx" : "Miller and Isard.,? 1963",
      "shortCiteRegEx" : "Miller and Isard.",
      "year" : 1963
    }, {
      "title" : "Accurate unbounded dependency recovery using generalized categorial grammars",
      "author" : [ "Luan Nguyen", "Marten van Schijndel", "William Schuler." ],
      "venue" : "Proceedings of the 24th International Conference on Computational Linguistics, pages 2125–2140.",
      "citeRegEx" : "Nguyen et al\\.,? 2012",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning accurate, compact, and interpretable tree annotation",
      "author" : [ "Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for",
      "citeRegEx" : "Petrov et al\\.,? 2006",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2006
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Technical Report.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences",
      "author" : [ "Keith Rayner", "Marcia Carlson", "Lyn Frazier." ],
      "venue" : "Journal of verbal learning and verbal behavior, 22(3):358–374.",
      "citeRegEx" : "Rayner et al\\.,? 1983",
      "shortCiteRegEx" : "Rayner et al\\.",
      "year" : 1983
    }, {
      "title" : "A model of language processing as hierarchic sequential prediction",
      "author" : [ "Marten van Schijndel", "Andy Exley", "William Schuler." ],
      "venue" : "Topics in Cognitive Science, 5(3):522–540.",
      "citeRegEx" : "Schijndel et al\\.,? 2013",
      "shortCiteRegEx" : "Schijndel et al\\.",
      "year" : 2013
    }, {
      "title" : "Broad-coverage incremental parsing using human-like memory constraints",
      "author" : [ "William Schuler", "Samir AbdelRahman", "Tim Miller", "Lane Schwartz." ],
      "venue" : "Computational Linguistics, 36(1):1–30.",
      "citeRegEx" : "Schuler et al\\.,? 2010",
      "shortCiteRegEx" : "Schuler et al\\.",
      "year" : 2010
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "A large-scale study of the effects of word frequency and predictability in naturalistic reading",
      "author" : [ "Cory Shain." ],
      "venue" : "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Shain.,? 2019",
      "shortCiteRegEx" : "Shain.",
      "year" : 2019
    }, {
      "title" : "fMRI reveals language-specific predictive coding during naturalistic sentence comprehension",
      "author" : [ "Cory Shain", "Idan Asher Blank", "Marten van Schijndel", "William Schuler", "Evelina Fedorenko." ],
      "venue" : "Neuropsychologia, 138.",
      "citeRegEx" : "Shain et al\\.,? 2019",
      "shortCiteRegEx" : "Shain et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep syntactic annotations for broad-coverage psycholinguistic modeling",
      "author" : [ "Cory Shain", "Marten van Schijndel", "William Schuler." ],
      "venue" : "Workshop on Linguistic and Neuro-Cognitive Resources (LREC 2018).",
      "citeRegEx" : "Shain et al\\.,? 2018",
      "shortCiteRegEx" : "Shain et al\\.",
      "year" : 2018
    }, {
      "title" : "ContinuousTime Deconvolutional Regression for Psycholinguistic Modeling",
      "author" : [ "Cory Shain", "William Schuler." ],
      "venue" : "PsyArXiv.",
      "citeRegEx" : "Shain and Schuler.,? 2019",
      "shortCiteRegEx" : "Shain and Schuler.",
      "year" : 2019
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "Claude Elwood Shannon." ],
      "venue" : "Bell System Technical Journal, 27:379–423.",
      "citeRegEx" : "Shannon.,? 1948",
      "shortCiteRegEx" : "Shannon.",
      "year" : 1948
    }, {
      "title" : "The effect of word predictability on reading time is logarithmic",
      "author" : [ "Nathaniel J. Smith", "Roger Levy." ],
      "venue" : "Cognition, 128:302–319.",
      "citeRegEx" : "Smith and Levy.,? 2013",
      "shortCiteRegEx" : "Smith and Levy.",
      "year" : 2013
    }, {
      "title" : "Effective inference for generative neural parsing",
      "author" : [ "Mitchell Stern", "Daniel Fried", "Dan Klein." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1695–1700.",
      "citeRegEx" : "Stern et al\\.,? 2017",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2017
    }, {
      "title" : "On the proper treatment of spillover in real-time reading studies: Consequences for psycholinguistic theories",
      "author" : [ "Shravan Vasishth." ],
      "venue" : "Proceedings of the International Conference on Linguistic Evidence, pages 96–100.",
      "citeRegEx" : "Vasishth.,? 2006",
      "shortCiteRegEx" : "Vasishth.",
      "year" : 2006
    }, {
      "title" : "On the predictive power of neural language models for human realtime comprehension behavior",
      "author" : [ "Ethan Gotlieb Wilcox", "Jon Gauthier", "Jennifer Hu", "Peng Qian", "Roger P. Levy." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Cognitive Science Soci-",
      "citeRegEx" : "Wilcox et al\\.,? 2020",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 9,
      "context" : "In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 20,
      "context" : "In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 25,
      "context" : "In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 7,
      "context" : "In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al.",
      "startOffset" : 82,
      "endOffset" : 127
    }, {
      "referenceID" : 28,
      "context" : "This finding deviates from the monotonic relationship between test perplexity and predictive power observed in previous studies (Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 178
    }, {
      "referenceID" : 24,
      "context" : "The experiments presented in this paper use surprisal predictors (Shannon, 1948) calculated by an incremental processing model based on a leftcorner parser (Johnson-Laird, 1983; van Schijndel et al.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "The experiments presented in this paper use surprisal predictors (Shannon, 1948) calculated by an incremental processing model based on a leftcorner parser (Johnson-Laird, 1983; van Schijndel et al., 2013).",
      "startOffset" : 156,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "The transition model presented in this paper is based on a probabilistic leftcorner parser (Johnson-Laird, 1983; van Schijndel et al., 2013).",
      "startOffset" : 91,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "Left-corner parsers have been used to model human sentence processing because they define a fixed number of decisions at every time step and also require only a bounded amount of working memory, in keeping with experimental observations of human memory limits (Miller and Isard, 1963).",
      "startOffset" : 260,
      "endOffset" : 284
    }, {
      "referenceID" : 11,
      "context" : "Previous work has shown that large annotated corpora such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 of such fragments (Schuler et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : ", 1993) do not require more than D = 4 of such fragments (Schuler et al., 2010).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "reannotating the training corpus using a generalized categorial grammar of English (Nguyen et al., 2012),4 which is sensitive to syntactic valence and non-local dependencies.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "The set of rules model affixation through string substitution and are inverses of lemmatization rules that are used to derive predicates in the generalized categorial grammar annotation (Nguyen et al., 2012).",
      "startOffset" : 186,
      "endOffset" : 207
    }, {
      "referenceID" : 11,
      "context" : "There are around 600 such rules that account for inflection in Sections 02 to 21 of the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993), which includes an identity rule for words in bare form and a ‘no semantics’ rule for generating certain function words.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "Both variants of the processing model were trained on a generalized categorial grammar (Nguyen et al., 2012) reannotation of Sections 02 to 21 of the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : ", 2012) reannotation of Sections 02 to 21 of the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "To account for the time the brain takes to process and respond to linguistic input, it is standard practice in psycholinguistic modeling to include ‘spillover’ variants of predictors from preceding words (Rayner et al., 1983; Vasishth, 2006).",
      "startOffset" : 204,
      "endOffset" : 241
    }, {
      "referenceID" : 27,
      "context" : "To account for the time the brain takes to process and respond to linguistic input, it is standard practice in psycholinguistic modeling to include ‘spillover’ variants of predictors from preceding words (Rayner et al., 1983; Vasishth, 2006).",
      "startOffset" : 204,
      "endOffset" : 241
    }, {
      "referenceID" : 23,
      "context" : "effects modeling (Shain and Schuler, 2019), CharWSurp and FreqWSurp were both spilled over by",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "• JLSTMSurp (Jozefowicz et al., 2016): A twolayer LSTM model with CNN character inputs trained on ∼800M tokens of the 1B Word Bench-",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "• GPT2Surp (Radford et al., 2019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents).",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : ", 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "The analysis of residuals grouped by the lowest base category of the previous time step (cbdt−1) from manual annotations (Shain et al., 2018) shows that the improvement of CharWSurp over GPT2Surp was broad-based across categories (see Figure 3).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "In order to examine whether these results generalize to other latency-based measures, linear-mixed effects models were fitted on the Dundee eyetracking corpus (Kennedy et al., 2003) to test the contribution of each surprisal predictor, following similar procedures to Experiment 2.",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 6,
      "context" : "The set of go-past durations from the Dundee Corpus (Kennedy et al., 2003) provided the response",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "To this end, the novel statistical framework of continuous-time deconvolutional regression (CDR; Shain and Schuler, 2019) was employed.",
      "startOffset" : 91,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "(2019), the CDR models assumed the twoparameter HRF based on the double-gamma canonical HRF (Lindquist et al., 2009).",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "These results add a new nuance to the relationship between perplexity and predictive power reported in previous work (Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "Experiments presented in this work used datasets from previously published research (Futrell et al., 2018; Kennedy et al., 2003; Marcus et al., 1993; Shain et al., 2019), in which the procedures for data collection and validation are outlined.",
      "startOffset" : 84,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Experiments presented in this work used datasets from previously published research (Futrell et al., 2018; Kennedy et al., 2003; Marcus et al., 1993; Shain et al., 2019), in which the procedures for data collection and validation are outlined.",
      "startOffset" : 84,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "Experiments presented in this work used datasets from previously published research (Futrell et al., 2018; Kennedy et al., 2003; Marcus et al., 1993; Shain et al., 2019), in which the procedures for data collection and validation are outlined.",
      "startOffset" : 84,
      "endOffset" : 169
    } ],
    "year" : 2021,
    "abstractText" : "While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.",
    "creator" : "LaTeX with hyperref"
  }
}