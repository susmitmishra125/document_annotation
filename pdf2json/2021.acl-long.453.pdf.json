{
  "name" : "2021.acl-long.453.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER",
    "authors" : [ "Linlin Liu", "Bosheng Ding", "Lidong Bing", "Shafiq Joty", "Luo Si", "Chunyan Miao" ],
    "emails" : [ "luo.si}@alibaba-inc.com", "ascymiao}@ntu.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5834–5846\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5834"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) aims to identify and classify entities in a text into predefined types, which is an essential tool for information extraction. It has also been proven to be useful in various downstream natural language processing (NLP) tasks, including information retrieval (Banerjee et al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages\n∗Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University.\n1Our code is available at https://ntunlpsg. github.io/project/mulda/.\n(e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021).\nExisting approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation.\nThese recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection. For model-based transfer, although the large-scale pretrained multilingual language models (LM) (Conneau et al., 2020; Liu et al., 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020).\nTo address the above problems under the setting of low-resource cross-lingual NER, we propose a multilingual data augmentation (MulDA) framework to make better use of the cross-lingual\ngeneralization ability of the pretrained multilingual LMs. Specifically, we consider a low-resource setting for cross-lingual NER, where there is very limited source-language training data and no targetlanguage train/dev data. Such setting is practical and useful in many real scenarios.\nOur proposed framework seeks the initial help from the instance-based transfer (i.e., translate train) paradigm (Li et al., 2020; Fang et al., 2020). We first introduce a novel labeled sequence translation method to translate the training data to the target language as well as to other languages. This allows us to finetune the LM based NER model on multilingual data rather than on the sourcelanguage data only, which helps prevent over-fitting on the language-specific features. One commonly used tool for translation is the off-the-shelf Google translate system2, which supports more than 100 languages. Alternatively, there are also many pretrained MT models conveniently accessible, e.g., more than 1,000 MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models have been released on the Hugging Face model hub.3\nNote that the instance-based transfer methods add limited semantic variety to the training set, since they only translate entities and the corresponding contexts to a different language. In contrast, data augmentation has been proven to be a successful method for tackling the data scarcity problem. Inspired by a recent monolingual data augmentation method (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to increase the diversity, where LMs are trained on multilingual labeled data and then used to generate more synthetic training data.\nWe conduct extensive experiments and analysis to verify the effectiveness of our methods. Our main contributions can be summarized as follows:\n• We propose a simple but effective labeled sequence translation method to translate the source training data to a desired language. Compared with exiting methods, our labeled sentence translation approach leverages placeholders for label projection, which effectively avoids many issues faced during word alignment, such as word order change, entity span determination, noisesensitive similarity metrics and so on.\n• We propose a generation-based multilingual data 2https://cloud.google.com/translate 3https://huggingface.co/transformers/model doc/marian.html\naugmentation method for NER, which leverages the multilingual language models to add more diversity to the training data.\n• Through empirical experiments, we observe that when fine-tuning pretrained multilingual LMs for low-resource cross-lingual NER, translations to more languages can also be used as an effective data augmentation method, which helps improve performance of both the source and the target languages."
    }, {
      "heading" : "2 MulDA: Our Multilingual Data Augmentation Framework",
      "text" : "We propose a multilingual data augmentation framework that leverages the advantages of both instance-based and model-based transfer for crosslingual NER. In our framework, a novel labeled sequence translation method is first introduced to translate the annotated training data from the source language S to a set of target languages T = {T1, . . . , Tn}. Then language models are trained on {DS ,DT1 , ...,DTn} to generate multilingual synthetic data, where DS is the sourcelanguage training data, and DTi is the translated data in language Ti. Finally, we post-process and filter the augmented data to train multilingual NER models for inference on target-language test sets."
    }, {
      "heading" : "2.1 Labeled Sequence Translation",
      "text" : "We leverage labeled sequence translation for the training data of the source language to generate multilingual NER training data, which can also be viewed a method for data augmentation. Prior methods (Jain et al., 2019; Li et al., 2020) usually perform translation and label projection in two separate steps: 1) translate source-language training sentences to the target language; 2) propagate labels from the source training data to the translated sentences via word-to-word/phrase-to-phrase mapping with alignment models or algorithms. However, these methods suffer from a few label projection problems, such as word order change, wordspan determination (Li et al., 2020), and so on. An alternative to avoid the label projection problems is word-by-word translation (Xie et al., 2018), but often at the sacrifice of the translation quality.\nWe address the problems identified above by first replacing named entities with contextual placeholders before sentence translation, and then after translation, we replace placeholders in translated\nLabeled sentence in the source language: [PER Jamie Valentine] was born in [LOC London]."
    }, {
      "heading" : "1. Translate sentence with placeholders:",
      "text" : "src: PER0 was born in LOC1. tgt: PER0 nació en LOC1."
    }, {
      "heading" : "2. Translate entities with context:",
      "text" : "PER0 src: [Jamie Valentine] was born in London. tgt: [Jamie Valentine] nació en Londres.\nLOC1 src: Jamie Valentine was born in [London]. tgt: Jamie Valentine nació en [Londres]."
    }, {
      "heading" : "3. Replace placeholders with translated entities:",
      "text" : "[PER Jamie Valentine] nació en [LOC Londres].\nFigure 1: An example of labeled sentence translation, where src and tgt are the translation model inputs and outputs, respectively. For the example shown in this figure, Google translation system and the MarianMT model generate the same translations in step 1 and 2.\nsentences with the corresponding translated entities. An illustration of the method is shown in Figure 1.\nAssume a sentence XS = {x1, . . . , xM} ∈ DS and the corresponding NER tags {y1, . . . , yM} are given, where xi’s are the sentence tokens and M is the sentence length. Let {E1, . . . , En} denote the predefined named entity types. Our method first replaces all entities in {x1, . . . , xM} with placeholders (src of step 1 in Figure 1). Placeholders Ek are reconstructed tokens with the corresponding entity type E as prefix and the index of the entity k as suffix. Assume {xi, . . . , xj} is the kth entity in the source sentence, and the corresponding type is Ez , then we can replace the entity with the placeholderEzk to get {. . . , xi−1, Ezk, xj+1, . . .}. We use XS∗ to denote the generated sentence after replacing all entities with placeholders. XS∗ is fed into an MT model to get the translation XT∗ in the target language T . With such design, the placeholder prefix E can provide the MT model4 with relevant contextual information about the entities, so that the model can translate the sentence with reasonably good quality. Besides, we observe most of placeholders are unchanged after translation,5 which can be used to help locate the position of entities.\nIn the second step, we translate each entity\n4When the MT model use subword vocabularies. 5See Appendix for more examples.\nwith the corresponding context. More specifically, we use brackets to mark the span of each entity and translate it to the target language successively, one at a time (src of step 2 in Figure 1). For example, to translate entity {xi, . . . , xj}, we feed {. . . , xi−1, [xi, . . . , xj ], xj+1, . . .} into the MT model. Then we can get entity translations by extracting the square bracket marked tokens from the translated sentences. We translate the entities directly if the square brackets are not found.\nFinally, we can replace placeholders in XT∗ (obtained from the first step) with the corresponding entity translations (obtained from the second step) and copy placeholder prefix as entity labels to generate the synthetic training data in the target language (step 3 in Figure 1). We tested the proposed method with Google translate and the MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models, and we found that both produce high quality synthetic data as we had expected."
    }, {
      "heading" : "2.2 Synthetic Data Generation with Language Models",
      "text" : "Although labeled sequence translation generates high quality multilingual NER training data, it adds limited variety since translation does not introduce new entities or contexts. Inspired by DAGA (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to add more diversity to the training data. DAGA is a monolingual data augmentation method designed for sequence labeling tasks, which has been shown to\nbe able to add significant diversity to the training data. As the example shown in Figure 2, it first linearizes labeled sequences by adding the entity type before sentence tokens. Then an LSTM-based LM (LSTM-LM) is trained on the linearized sequences in an autoregressive way, after which the begin-ofsentence token [BOS] is fed into the LSTM-LM to generate synthetic training data autoregressively. The monolingual LSTM-LM of DAGA is trained in a similar way as the example shown in Figure 3, except that there is no language tag [en].\nTo extend this method for multilingual data augmentation, we add special tokens at the beginning of each sentence to indicate the language that it belongs to. The source-language data and the multilingual data obtained via translation are concatenated to train/finetune multilingual LMs with a shared vocabulary (as shown in Figure 5). Given a labeled sequence {x1, . . . , xM} from the multilingual training data, the LMs are trained to maximize the probability p(x1, . . . , xM ) in Eq. 1:\np(x1, . . . , xM ) = M∏ t=1 pθ(xt|x<t) (1)\nwhere θ is the parameter to optimize, and pθ(xt|x<t) is the probability of the next token given the previous tokens in the sequence, which is usually computed with the softmax function. Figure 3 shows an example of how the multilingual LSTM-LM is trained in the autoregressive way. After training the LSTM-LM, we can feed the [BOS] token and a language token to the model to generate synthetic training data for the specified language.\nBesides, to leverage the cross-lingual generalization ability of large scale pretrained multilingual LMs, we also finetune a recent state-of-the-art seq2seq model mBART (Liu et al., 2020), which is pretrained with multilingual denoising tasks. Sentence permutation and word-span masking are the two noise injection methods used to add noise to original sentence X = {x1, . . . , xM} to output g(X), where g(.) is used to denote the noise injection function. After encoding g(X) with the Transformer encoder, the Transformer decoder is trained to generate the original sequence X autoregressively by maximizing Eq. 1.\nDenoising word-span masked sequences is the most relevant to our data augmentation method, since only small modifications are required to make our finetuning task as consistent to the pretraining task as possible. More specifically, we design\nour finetuning task with the following changes: 1) use the linearized labeled sequences (as shown in Figure 5) as input X; 2) modify g(.) to mask random trailing sub-sequences such that g(X) = {x1, . . . , xz, [mask]}, where 1 ≤ z ≤ |X| is a random integer. After finetuning with such task, we can conveniently feed a randomly masked sequence {x1, . . . , xz, [mask]} into mBART to generate synthetic data. Figure 4 shows a more concrete example to illustrate how mBART is finetuned with the linearized sequences in our work."
    }, {
      "heading" : "2.3 Semi-supervised Method",
      "text" : "Unlabeled multilingual sentences are usually easy to get, for example, data from the Wikimedia6. To make better use of these unlabeled multilingual data, we propose a semi-supervised method to prepare more pseudo labeled data for finetuning multilingual LMs. Inspired by self-training (Zoph et al., 2020; Xie et al., 2020), we use the NER model trained on the multilingual translated data to annotate the unlabeled sentences. After that, we use two additional NER models trained with different random seeds to filter the annotated data by removing those with different tag predictions."
    }, {
      "heading" : "2.4 Post-Processing",
      "text" : "We also design several straightforward methods to post-process and filter the augmented data generated by the LMs:\n• Delete sequences that contain only O (other) tags.\n• Convert the generated labeled sequences to the same format as gold data by separating sentence tokens and NER tags.\n• Use the NER model trained on the multilingual translated data to label the generated sequences (after tag removal). Then compare the tags generated by the LM and NER model predictions, and remove the sentences with inconsistencies."
    }, {
      "heading" : "3 Experiments",
      "text" : "We conduct experiments to evaluate the effectiveness of the proposed multilingual data augmentation framework. Firstly, we compare our labeled sequence translation method with the previous instance-based transfer (i.e., translate train) methods. Following that, we show the benefit of adding multilingual translations. Then we continue\n6https://dumps.wikimedia.org/\nto evaluate the generation-based multilingual data augmentation method by comparing cross-lingual NER performance of the models trained on monolingual, bilingual, and multilingual augmented data, respectively. Finally, we further evaluate our methods on a wider range of distant languages.\nWe use the most typical Transformer-based NER model7 in our experiments, which is implemented by adding a randomly initialized feed forward layer to the Transformer final layer for label classification. Specifically, to demonstrate that our framework can help achieve additional performance gain even on the top of the state-of-the-art multilingual LMs, the checkpoint of the pretrained XLM-R large (Conneau et al., 2020) model is used to initialize our NER models."
    }, {
      "heading" : "3.1 Labeled Sequence Translation",
      "text" : "We finetune the NER model on the translated targetlanguage data to compare our labeled sequence translation method (§2.1) with the existing instancebased transfer methods.\nExperimental settings The CoNLL02/03 NER dataset (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) is used for evaluation, which contains data in four different languages: English, German, Dutch and Spanish. All of the data are annotated with the same set of NER tags. We follow the steps described in §2.1 to translate En-\n7Similar to the token classification model in https://github.com/huggingface/transformers.\nglish train data to the other three languages. Following Jain et al. (2019) and Li et al. (2020), Google translation system is used in the experiments. Since our NER model is more powerful than those used by Jain et al. (2019) and Li et al. (2020), we reproduce their results with XLM-R large for a fair comparison. All of the NER models are finetuned on the translated target-language sentences only for 10 epochs with the best model selected using English dev data, and then evaluated on the targetlanguage original test data.\nResults We present the results in Table 1. As we can see, our method outperforms the best baseline method by 2.90 and 2.97 on German and Dutch respectively, and by 2.23 on average. Since our models are only finetuned with the data generated by the labeled sequence translation method, the results directly demonstrate the effectiveness of our method. Moreover, compared with the two recent baseline methods (Jain et al., 2019; Li et al., 2020), our method does not rely on complex label projection algorithms and is much easier to implement."
    }, {
      "heading" : "3.2 Multilingual Translation as Data Augmentation",
      "text" : "After showing that our labeled sequence translation method can generate high quality labeled data in the target language, in this section, we run ex-\nperiments to verify the hypothesis that multilingual translation may help improve the cross-lingual transfer performance of multilingual LMs in low resource scenarios.\nExperimental settings We use the same NER dataset as above. In order to simulate low resource scenarios, we randomly sample 500, 1k and 2k sentences from the gold English train set. Our labeled sequence translation method is used to translate the sampled data to pseudo labeled data in the three target languages, German, Spanish and Dutch. To better demonstrate how the training data affects cross-lingual NER performance, we train the NER model on four different conditions: 1) En: train the models on English data only; 2) Tgt-Tran: train the models on the pseudo labeled data in a certain target language only; 3) En + Tgt-Tran: train the models on the combination of English data and pseudo labeled target-language data; 4) En + Multi-Tran: train one single model on the combination of English data and pseudo labeled data in all three target languages. We find filtering the translated sentences can further improve cross-lingual transfer performance, so we use an NER model trained on the sampled English data to label the translated sentences, count the number of entities in each sentence different from NER model predictions, and then remove the top 20% sentences with the most inconsistent entities. This is similar to the third step described in §2.4, except that we remove all the inconsistent sentences from the augmented data, since the LMs can be used to generate a large number of candidate sentences. We set max number of epochs to 10 and use 500 sentences randomly sampled from the English dev data to select the best models for each setting. Then the best models are evaluated on the original target language test sets.\nResults Table 2 compares the cross-lingual NER performance of the models trained on the different training sets. Although the performances of En and Tgt-Tran are relatively bad in most of the cases, combining them can always boost the performance significantly, especially when the dataset size is small. Adding multilingual translated data further improves cross-lingual performance by more than 1% on average when English data size is 1k or less. Therefore, multilingual translation can be used as an effective data augmentation approach in the low resource scenarios of cross-lingual NER. Moreover,\nthe trained single model with En + Multi-Tran can be applied to all target languages.\nBesides, we also observe that multilingual translated data can even help improve NER performance of the source language. Table 3 summarizes English test data results for the above settings. TgtTran (avg) is the average English results of the models trained on three different Tgt-Tran of German, Spanish and Dutch respectively. En + TgtTran (avg) is the average for combining En with each of the three different Tgt-Tran. As we can see, adding additional translated data consistently improves English NER performance. Particularly, En + Multi-Tran achieves the best performance. Therefore, we can also use multilingual translated data to improve low-resource monolingual NER performance."
    }, {
      "heading" : "3.3 Generation-based Multilingual Data Augmentation",
      "text" : "In this section, we run experiments to verify whether applying generation-based data augmentation methods to the multilingual translated data can further improve cross-lingual performance in the low resource scenarios.\nExperimental settings We follow the steps described in §2.2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al., 2020) sep-\n500 1k 2k Method de es nl avg de es nl avg de es nl avg\narately, and then use them to augment the data processed in §3.2. We concatenate English gold data and the filtered multilingual translated data to train/finetune the modified LMs, where LSTM-LM is trained from scratch and mBART is intialized with the mBART CC25 checkpoint8 for finetuning. mBART CC25 is a model with 12 encoder and decoder layers trained on 25 languages. We follow the steps described in §2.4 to post-process the augmented data, and concatenate them with the corresponding English gold and translated multilingual data to train the NER models. The size of the augmented data used in each setting is the same as the size of the corresponding English gold data. MulDA-LSTM and MulDA-mBART are used to denote the methods that use LSTM-LM and mBART augmented data respectively. In addition, we also report a bilingual version of our method, denoted with BiDA-LSTM, which performs data augmentation on English and the translated target-language data only. We follow the same settings as above to evaluate cross-lingual performance of the NER models trained on different data.\nResults Average results of 5 runs are reported in Table 4. Note that MulDA-LSTM and MulDAmBART train a single model for all the target languages in each setting, while BiDA-LSTM trains one model for each target language in each setting. Therefore, we compare BiDA-LSTM with\n8https://github.com/pytorch/fairseq/blob/master/ examples/mbart/README.md\nEn + Tgt-Tran only. As we can see, the proposed multilingual data augmentation methods further improve cross-lingual NER performance consistently. For the 1k and 2k setting, MulDA-LSTM achieves comparable average performance as BiDA-LSTM."
    }, {
      "heading" : "3.4 Evaluation on More Distant Languages",
      "text" : "We evaluate the proposed method on a wider range of target languages in this section.\nExperimental settings The Wikiann NER data (Pan et al., 2017) processed by Hu et al. (2020) is used in these experiments. 1k English sentences (DS1k) are sampled from the gold train data to simulate the low resource scenarios. We also assume MT models are not available for all of the target languages, so we only translate the sampled English sentences to 6 target languages: ar, fr, it, ja, tr and zh. DTtrans is used to denote the translated target-language sentences by following steps described in §2.1. The low quality translated sentences are filtered out in the same way as §3.2. To evaluate our method in the semi-supervised setting, we also sample 5,000 sentences from the training data of the 6 target languages and then remove the NER tags to create unlabeled data DTunlabeled. We follow the steps described in §2.3 to annotate DTunlabeled with one NER model trained on {DS1k,DTtrans}, and then filter the pseudo labeled data with two other NER models trained on the same data but with different random seeds. We use DTsemi to denote the data generated with this\nsemi-supervised approach. Finally, we concatenate {DS1k,DTtrans,DTsemi} to generate augmented data DTaug following the steps in §2.2 and §2.4. With the augmented data above, we train NER models on the concatenated data of {DS1k,DTtrans,DTaug} for cross-lingual NER evaluation. We also train an NER model on {DS1k,DTtrans,DTsemi} for comparison, denoted as Weak Tagger. The other settings are same as the above experiments.\nResults We summarize the results in Table 6. Tran-Train is the average performance of the 6 languages that have corresponding training data translated from English. Zero Shot is the average performance of the other target languages. MulDALSTM demonstrates promising performance improvements on both the Tran-Train and Zero Shot languages. The performance of MulDA-mBART is slightly lower, one possible reason is the noise introduced by the sentences labeled at character level. We follow the gold data format to label translated zh and ja sequences at character level, which is inconsistent with how mBART is pretrained. Please refer to Table 5 for the detailed cross-lingual NER results of each language."
    }, {
      "heading" : "3.5 Case Study",
      "text" : ""
    }, {
      "heading" : "3.5.1 Effectiveness in Label Projection",
      "text" : "The label projection step of the previous methods needs to locate the entities and determine their boundaries, which is vulnerable to many problems, such as word order change, long entities, etc. Our method effectively avoids these problems with placeholders. In the two examples shown in Figure 6, Jain et al. (2019) either labeled only part of the whole entity or incorrectly split the entity into two, Li et al. (2020) incorrectly split the entities into two in both examples, while our method can correctly map the labels."
    }, {
      "heading" : "3.5.2 Multilingual Data Augmentation",
      "text" : "We look into the data generated by our multilingual data augmentation method. During LM training,\nthe NER tags can be viewed as a shared vocabulary between different languages. As a result, we find that some generated sentences contain tokens from multiple languages, which are useful to help improve cross-lingual transfer (Tan and Joty, 2021). Two examples are shown in Figure 7."
    }, {
      "heading" : "4 Related Work",
      "text" : "Cross-lingual NER There has been growing interest in cross-lingual NER. Prior approaches can be grouped into two main categories, instancebased transfer and model-based transfer. Instancebased transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality\nwith additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on.\nModel-based transfer directly applies the model trained on the source language to the targetlanguage test data (Täckström et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data.\nData augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We have proposed a multilingual data augmentation framework for low resource cross-lingual\nNER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging placeholders during MT. Our generation-based multilingual data augmentation method generates high quality synthetic training data to add more diversity. The proposed framework has demonstrated encouraging performance improvement in various low-resource settings and across a wide range of target languages."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is partly supported by the AlibabaNTU Singapore Joint Research Institute, Nanyang Technological University. Linlin Liu would like to thank the support from Interdisciplinary Graduate School, Nanyang Technological University. We would like to thank the help from our Alibaba colleagues, Ruidan He and Qingyu Tan in this work as well."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Translation with Placeholders Figure 8 shows more examples of translating the sequence “PER0 was born in LOC1.” to different languages. We can see that the placeholders are all well kept. Meanwhile, the translation quality is also good.\nSource sentence: en: PER0 was born in LOC1.\nTranslations: de: PER0 wurde in LOC1 geboren. es: PER0 nació en LOC1. nl: PER0 is geboren in LOC1. vi: PER0 được sinh ra ở LOC1. fr: PER0 est né en LOC1. zh: PER0出生于LOC1。\nFigure 8: Translations of “PER0 was born in LOC1.” to different languages with Google translation system.\nA.2 Number of Entities in Translated Data We count the total number of entities in gold EN data and the translated data. As shown in Table 7, the number of entities in our translated data is the most close to that of the gold EN data.\nA.3 Visualization of Entity Representations We visualize the last layer transformer outputs of the finetuned NER model with t-SNE. We finetune two XLM-R initialized NER models on English and MulDA-LSTM respectively, and generate last layer representations with Chinese test data. Only the token representations corresponding to the B and I tags are saved. The two dimensional t-SNE visualizations are shown in Figures 9 and 10. As we can see, the representation clusters corresponding to different NER entities in Figure 10 (MulDALSTM) are further separated than that in Figure 9 (English).\nA.4 Parameters The parameters used for NER model fine-tuning are shown in Table 8."
    } ],
    "references" : [ {
      "title" : "A information retrieval based on question and answering and ner for unstructured information without using sql",
      "author" : [ "Partha Sarathy Banerjee", "Baisakhi Chakraborty", "Deepak Tripathi", "Hardik Gupta", "Sourabh S Kumar." ],
      "venue" : "Wireless Personal",
      "citeRegEx" : "Banerjee et al\\.,? 2019",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-Resource Cross-Lingual Named Entity Recognition",
      "author" : [ "M Saiful Bari", "Shafiq Joty", "Prathyusha Jwalapuram." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artifical Intelligence, AAAI ’20, New York, USA. AAAI.",
      "citeRegEx" : "Bari et al\\.,? 2020",
      "shortCiteRegEx" : "Bari et al\\.",
      "year" : 2020
    }, {
      "title" : "UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP",
      "author" : [ "M Saiful Bari", "Tasnim Mohiuddin", "Shafiq Joty." ],
      "venue" : "Proceedings of The Joint Conference of the 59th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Bari et al\\.,? 2021",
      "shortCiteRegEx" : "Bari et al\\.",
      "year" : 2021
    }, {
      "title" : "Multilingual language models for named entity recognition in German and English",
      "author" : [ "Antonia Baumann." ],
      "venue" : "Proceedings of the Student Research Workshop Associated with RANLP 2019, pages 21–27, Varna, Bulgaria. INCOMA Ltd.",
      "citeRegEx" : "Baumann.,? 2019",
      "shortCiteRegEx" : "Baumann.",
      "year" : 2019
    }, {
      "title" : "Adapting word embeddings to new languages with morphological and phonological subword representations",
      "author" : [ "Aditi Chaudhary", "Chunting Zhou", "Lori Levin", "Graham Neubig", "David R Mortensen", "Jaime G Carbonell." ],
      "venue" : "arXiv preprint arXiv:1808.09500.",
      "citeRegEx" : "Chaudhary et al\\.,? 2018",
      "shortCiteRegEx" : "Chaudhary et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–",
      "citeRegEx" : "Guzmán et al\\.,? 2020",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation for deep neural network acoustic modeling",
      "author" : [ "Xiaodong Cui", "Vaibhava Goel", "Brian Kingsbury." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(9):1469– 1477.",
      "citeRegEx" : "Cui et al\\.,? 2015",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DAGA: Data augmentation with a generation approach for low-resource tagging tasks",
      "author" : [ "Bosheng Ding", "Linlin Liu", "Lidong Bing", "Canasai Kruengkrai", "Thien Hai Nguyen", "Shafiq Joty", "Luo Si", "Chunyan Miao." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Ding et al\\.,? 2020",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "Template-based question generation from retrieved sentences for improved unsupervised question answering",
      "author" : [ "Alexander Fabbri", "Patrick Ng", "Zhiguo Wang", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Fabbri et al\\.,? 2020",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2020
    }, {
      "title" : "Filter: An enhanced fusion method for cross-lingual language understanding",
      "author" : [ "Yuwei Fang", "Shuohang Wang", "Zhe Gan", "Siqi Sun", "Jingjing Liu" ],
      "venue" : null,
      "citeRegEx" : "Fang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating chinese named entity data from parallel corpora",
      "author" : [ "Ruiji Fu", "Bing Qin", "Ting Liu." ],
      "venue" : "Frontiers of Computer Science, 8(4):629–641.",
      "citeRegEx" : "Fu et al\\.,? 2014",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2014
    }, {
      "title" : "Xtreme: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "International Conference on Machine",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Entity projection via machine translation for cross-lingual NER",
      "author" : [ "Alankar Jain", "Bhargavi Paranjape", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-language learning with adversarial neural networks: Application to community question answering",
      "author" : [ "Shafiq Joty", "Preslav Nakov", "Lluı́s Màrquez", "Israa Jaradat" ],
      "venue" : "In Proceedings of The SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Joty et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER",
      "author" : [ "Phillip Keung", "Yichao Lu", "Vikas Bhardwaj." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Keung et al\\.,? 2019",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2019
    }, {
      "title" : "From research to production and back: Ludicrously fast neural machine translation",
      "author" : [ "Young Jin Kim", "Marcin Junczys-Dowmunt", "Hany Hassan", "Alham Fikri Fikri Aji", "Kenneth Heafield", "Roman Grundkiewicz", "Nikolay Bogoychev." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Improving bert with self-supervised attention",
      "author" : [ "Xiaoyu Kou", "Yaming Yang", "Yujing Wang", "Ce Zhang", "Yiren Chen", "Yunhai Tong", "Yan Zhang", "Jing Bai" ],
      "venue" : null,
      "citeRegEx" : "Kou et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kou et al\\.",
      "year" : 2020
    }, {
      "title" : "Better exploiting latent variables in text modeling",
      "author" : [ "Canasai Kruengkrai." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5527–5532, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Kruengkrai.,? 2019",
      "shortCiteRegEx" : "Kruengkrai.",
      "year" : 2019
    }, {
      "title" : "Improving low-resource named entity recognition using joint sentence and token labeling",
      "author" : [ "Canasai Kruengkrai", "Thien Hai Nguyen", "Sharifah Mahani Aljunied", "Lidong Bing." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Kruengkrai et al\\.,? 2020",
      "shortCiteRegEx" : "Kruengkrai et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation using pre-trained transformer models",
      "author" : [ "Varun Kumar", "Ashutosh Choudhary", "Eunah Cho." ],
      "venue" : "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18–26, Suzhou, China. Association for Com-",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual adaptation for sequence tagging and beyond",
      "author" : [ "Xin Li", "Lidong Bing", "Wenxuan Zhang", "Zheng Li", "Wai Lam" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Cheap translation for cross-lingual named entity recognition",
      "author" : [ "Stephen Mayhew", "Chen-Tse Tsai", "Dan Roth." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2536–2545, Copenhagen, Denmark. As-",
      "citeRegEx" : "Mayhew et al\\.,? 2017",
      "shortCiteRegEx" : "Mayhew et al\\.",
      "year" : 2017
    }, {
      "title" : "Augvic: Exploiting bitext vicinity for lowresource nmt",
      "author" : [ "Tasnim Mohiuddin", "M Saiful Bari", "Shafiq Joty." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Mohiuddin et al\\.,? 2021",
      "shortCiteRegEx" : "Mohiuddin et al\\.",
      "year" : 2021
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Çağlar GuÌ‡lçehre", "Bing Xiang" ],
      "venue" : "In Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
      "author" : [ "Jian Ni", "Georgiana Dinu", "Radu Florian." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ni et al\\.,? 2017",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "author" : [ "Daniel S. Park", "William Chan", "Yu Zhang", "ChungCheng Chiu", "Barret Zoph", "Ekin D. Cubuk", "Quoc V. Le." ],
      "venue" : "Proc. Interspeech 2019, pages 2613–",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Cosda-ml: Multi-lingual code-switching",
      "author" : [ "Libo Qin", "Minheng Ni", "Yue Zhang", "Wanxiang Che" ],
      "venue" : null,
      "citeRegEx" : "Qin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Synthetic data augmentation for zero-shot crosslingual question answering",
      "author" : [ "Arij Riabi", "Thomas Scialom", "Rachel Keraron", "Benoı̂t Sagot", "Djamé Seddah", "Jacopo Staiano" ],
      "venue" : null,
      "citeRegEx" : "Riabi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Riabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Students need more attention: Bert-based attention model for small data with application to automatic patient message triage",
      "author" : [ "Shijing Si", "Rui Wang", "Jedrek Wosik", "Hao Zhang", "David Dov", "Guoyin Wang", "Lawrence Carin." ],
      "venue" : "Proceedings of the 5th",
      "citeRegEx" : "Si et al\\.,? 2020",
      "shortCiteRegEx" : "Si et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformation Invariance in Pattern Recognition — Tangent Distance and Tangent Propagation, pages 239–274",
      "author" : [ "Patrice Y. Simard", "Yann A. LeCun", "John S. Denker", "Bernard Victorri." ],
      "venue" : "Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Simard et al\\.,? 1998",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 1998
    }, {
      "title" : "XLDA}: Cross-lingual data augmentation for natural language inference and question answering",
      "author" : [ "Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Singh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "Mixuptransformer: Dynamic data augmentation for NLP tasks",
      "author" : [ "Lichao Sun", "Congying Xia", "Wenpeng Yin", "Tingting Liang", "Philip Yu", "Lifang He." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3436–",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "Oscar Täckström", "Ryan McDonald", "Jakob Uszkoreit." ],
      "venue" : "The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Täckström et al\\.,? 2012",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Code-mixing on sesame street: Dawn of the adversarial polyglots",
      "author" : [ "Samson Tan", "Shafiq Joty." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL’21, Mexico City,",
      "citeRegEx" : "Tan and Joty.,? 2021",
      "shortCiteRegEx" : "Tan and Joty.",
      "year" : 2021
    }, {
      "title" : "Treebank translation for cross-lingual parser induction",
      "author" : [ "Jörg Tiedemann", "Željko Agić", "Joakim Nivre." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Tiedemann et al\\.,? 2014",
      "shortCiteRegEx" : "Tiedemann et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual named entity recognition via wikification",
      "author" : [ "Chen-Tse Tsai", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 219–228.",
      "citeRegEx" : "Tsai et al\\.,? 2016",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2016
    }, {
      "title" : "The galactic dependencies treebanks: Getting more data by synthesizing new languages",
      "author" : [ "Dingquan Wang", "Jason Eisner." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:491–505.",
      "citeRegEx" : "Wang and Eisner.,? 2016",
      "shortCiteRegEx" : "Wang and Eisner.",
      "year" : 2016
    }, {
      "title" : "Cross-lingual alignment vs joint training: A comparative study and a simple unified framework",
      "author" : [ "Zirui Wang", "Jiateng Xie", "Ruochen Xu", "Yiming Yang", "Graham Neubig", "Jaime G. Carbonell." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unitrans: Unifying model transfer and data transfer for cross-lingual named entity recognition with unlabeled data",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje F. Karlsson", "Biqing Huang", "Jian-Guang Lou" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural crosslingual named entity recognition with minimal resources",
      "author" : [ "Jiateng Xie", "Zhilin Yang", "Graham Neubig", "Noah A. Smith", "Jaime Carbonell" ],
      "venue" : null,
      "citeRegEx" : "Xie et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-training with noisy student improves imagenet classification",
      "author" : [ "Qizhe Xie", "Minh-Thang Luong", "Eduard Hovy", "Quoc V. Le." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end slot alignment and recognition for crosslingual NLU",
      "author" : [ "Weijia Xu", "Batool Haider", "Saab Mansour." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5052–5063, Online. As-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Inducing multilingual text analysis tools via robust projection across aligned corpora",
      "author" : [ "David Yarowsky", "Grace Ngai", "Richard Wicentowski." ],
      "venue" : "Proceedings of the First International Conference on Human Language Technology Research.",
      "citeRegEx" : "Yarowsky et al\\.,? 2001",
      "shortCiteRegEx" : "Yarowsky et al\\.",
      "year" : 2001
    }, {
      "title" : "mixup: Beyond empirical risk minimization",
      "author" : [ "Hongyi Zhang", "Moustapha Cisse", "Yann N. Dauphin", "David Lopez-Paz." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual dependency parsing using code-mixed TreeBank",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking pre-training and self-training",
      "author" : [ "Barret Zoph", "Golnaz Ghiasi", "Tsung-Yi Lin", "Yin Cui", "Hanxiao Liu", "Ekin D Cubuk", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:2006.06882.",
      "citeRegEx" : "Zoph et al\\.,? 2020",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It has also been proven to be useful in various downstream natural language processing (NLP) tasks, including information retrieval (Banerjee et al., 2019), question answering (Fabbri et al.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : ", 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 199
    }, {
      "referenceID" : 14,
      "context" : "zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 199
    }, {
      "referenceID" : 2,
      "context" : "zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021).",
      "startOffset" : 121,
      "endOffset" : 199
    }, {
      "referenceID" : 24,
      "context" : "Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with",
      "startOffset" : 164,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with",
      "startOffset" : 164,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 180
    }, {
      "referenceID" : 44,
      "context" : "aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 180
    }, {
      "referenceID" : 23,
      "context" : "For model-based transfer, although the large-scale pretrained multilingual language models (LM) (Conneau et al., 2020; Liu et al., 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al.",
      "startOffset" : 96,
      "endOffset" : 136
    }, {
      "referenceID" : 46,
      "context" : ", 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 33,
      "context" : ", 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 18,
      "context" : ", 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "more than 1,000 MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models have been released on the Hugging Face model hub.",
      "startOffset" : 25,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "Inspired by a recent monolingual data augmentation method (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to increase the diversity, where LMs",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "Prior methods (Jain et al., 2019; Li et al., 2020) usually perform translation and label projection in two separate steps: 1) translate source-language training sentences to the target language; 2) propagate labels from the source training data to the translated sentences via word-to-word/phrase-to-phrase mapping with alignment models or algorithms.",
      "startOffset" : 14,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "Prior methods (Jain et al., 2019; Li et al., 2020) usually perform translation and label projection in two separate steps: 1) translate source-language training sentences to the target language; 2) propagate labels from the source training data to the translated sentences via word-to-word/phrase-to-phrase mapping with alignment models or algorithms.",
      "startOffset" : 14,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "However, these methods suffer from a few label projection problems, such as word order change, wordspan determination (Li et al., 2020), and so on.",
      "startOffset" : 118,
      "endOffset" : 135
    }, {
      "referenceID" : 47,
      "context" : "An alternative to avoid the label projection problems is word-by-word translation (Xie et al., 2018), but often at the sacrifice of the translation quality.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "We tested the proposed method with Google translate and the MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models, and we found that both produce high quality synthetic data as we had expected.",
      "startOffset" : 69,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "Inspired by DAGA (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to add more diversity to the training data.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "Besides, to leverage the cross-lingual generalization ability of large scale pretrained multilingual LMs, we also finetune a recent state-of-the-art seq2seq model mBART (Liu et al., 2020), which is pretrained with multilingual denoising tasks.",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 53,
      "context" : "Inspired by self-training (Zoph et al., 2020; Xie et al., 2020), we use the NER model trained on the multilingual translated data to anno-",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 48,
      "context" : "Inspired by self-training (Zoph et al., 2020; Xie et al., 2020), we use the NER model trained on the multilingual translated data to anno-",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "Moreover, compared with the two recent baseline methods (Jain et al., 2019; Li et al., 2020), our method does not rely on complex label projection algorithms and is much easier to implement.",
      "startOffset" : 56,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "Moreover, compared with the two recent baseline methods (Jain et al., 2019; Li et al., 2020), our method does not rely on complex label projection algorithms and is much easier to implement.",
      "startOffset" : 56,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al., 2020) sep-",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "Experimental settings The Wikiann NER data (Pan et al., 2017) processed by Hu et al.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : "that some generated sentences contain tokens from multiple languages, which are useful to help improve cross-lingual transfer (Tan and Joty, 2021).",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 39,
      "context" : "based transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019).",
      "startOffset" : 140,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "based transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019).",
      "startOffset" : 140,
      "endOffset" : 183
    }, {
      "referenceID" : 50,
      "context" : "Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 49,
      "context" : "Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 127
    }, {
      "referenceID" : 45,
      "context" : "Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 127
    }, {
      "referenceID" : 51,
      "context" : ", 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al.",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : ", 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016;",
      "startOffset" : 16,
      "endOffset" : 53
    }, {
      "referenceID" : 29,
      "context" : ", 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016;",
      "startOffset" : 16,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al.",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 36,
      "context" : "Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al.",
      "startOffset" : 103,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al.",
      "startOffset" : 103,
      "endOffset" : 141
    }, {
      "referenceID" : 52,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    }, {
      "referenceID" : 35,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    }, {
      "referenceID" : 31,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    }, {
      "referenceID" : 30,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    }, {
      "referenceID" : 25,
      "context" : "Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.",
      "startOffset" : 82,
      "endOffset" : 203
    } ],
    "year" : 2021,
    "abstractText" : "Named Entity Recognition (NER) for lowresource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of sourcelanguage training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.1",
    "creator" : "LaTeX with hyperref"
  }
}