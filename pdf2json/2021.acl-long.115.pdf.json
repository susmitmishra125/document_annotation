{
  "name" : "2021.acl-long.115.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Towards Table-to-Text Generation with Numerical Reasoning",
    "authors" : [ "Lya Hulliyyatus Suadaa", "Hidetaka Kamigaito", "Kotaro Funakoshi", "Manabu Okumura", "Hiroya Takamura" ],
    "emails" : [ "lya@stis.ac.id", "kamigaito@lr.pi.titech.ac.jp", "funakoshi@lr.pi.titech.ac.jp", "oku@lr.pi.titech.ac.jp", "takamura@pi.titech.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1451–1465\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1451"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent data-to-text generation studies have shown significant improvement in generating faithful text aligned with data sources. A copy mechanism has been widely explored to improve faithfulness in various ways. Wiseman et al. (2017) used joint probabilities to let models choose between copying records from data sources or generating from a vocabulary. Puduppully et al. (2019) improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources.\nHowever, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti-\nModel Precision Recall F1\nOur full model 89.6 82.2 85.7\nLee et al. (2018) 86.2 83.7 84.9\nTable 2: The overall mention detection results on the test set of OntoNotes.\nTarget Header\nOur full model\nDescription\nTable 2 shows the mention detection results on the test set. Similar to coreference linking results, our model achieves higher precision and F1 score, which indicates that our model can significantly reduce false positive mentions while it can still find a reasonable number of mentions.\nFigure 1: Example of table and description in numeric NLG dataset.\ncal text with richer inference, including numerical reasoning. Making inferences beyond texts is still an open question due to the limitation of language models in handling numeric operations. In this study, we further encourage research by elaborating numerical tables to initialize the ability to inject reasoning while maintaining high fluency.\nOur contributions are summarized as follows.\n• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg.\n• We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representations in pre-trained models.\n• We propose a copy mechanism for pre-trained models, that uses general placeholders covering table contents and results of pre-executed numerical operations to avoid fact hallucination.\n• We conduct experiments with current state-ofthe-art neural generation models and a simple template-based system to demonstrate the challenges and opportunities for future research on text generation with numerical reasoning."
    }, {
      "heading" : "2 Related Work",
      "text" : "The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is LOGICNLG, by Chen et al. (2020a), who first introduced logical text generation using open-domain tables with unknown schemas. Different from our target text for generation, which consists of several sentences in a paragraph, they proposed a task of generating only one sentence from selected table contents."
    }, {
      "heading" : "3 Numerical Table-to-Text Dataset",
      "text" : "We created numericNLG, a new table-to-text dataset focusing on a text generation task with numerical reasoning. We collected table descriptions from scientific papers, that are naturally produced by experts with richer inference."
    }, {
      "heading" : "3.1 Dataset Creation",
      "text" : "Data Acquisition We constructed a table-to-text dataset based on numerical tables of experimental results, extracted from PDF files of scientific papers on the ACL Anthology website,1 introduced\n1https://www.aclweb.org/anthology/\nby Suadaa et al. (2021). Then, we collected candidates for corresponding descriptions from the source files using PDFMiner.2 We used table numbers in their captions as keywords for the collection. An example of a table and its description is shown in Figure 1.\nData Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps:\n• Examine tables and their corresponding descriptions and then recommend only the descriptions that have at least one sentence representing numerical facts in the table.\n• Categorize each sentence of the recommended description into three fact-checking classes: data description, supporting description, and not-related-to-table description. As a final dataset, we used only sentences classified as belonging to the data description category to reduce fact hallucination.\n• Identify a content plan of table description by selecting part of table headers which directly stated or logically inferred in the description, called target header. For example, refer to the table description shown in Figure 1, “Our full model” is selected as the target header.\nWe used the same split of training, validation, and test sets as the source table dataset (Suadaa et al., 2021)."
    }, {
      "heading" : "3.2 Dataset Comparison",
      "text" : "Table 1 provides a comparison of numericNLG with other related table-to-text datasets. The ROTOWIRE (Wiseman et al., 2017) dataset consists of summaries of NBA basketball games containing several paragraphs, paired with their corresponding box-score tables. Since ROTOWIRE has only 39 record types, each table contains similar record types with limited schemas. Although most of the ROTOWIRE table contents are in numerical values, the summaries contain only a few numerical-reasoning sentences, such as a comparison of scores between two basketball teams. While our dataset consists of closed domain articles as\n2http://pypi.python.org/pypi/pdfminer/\nwith ROTOWIRE, it is of shorter text (a paragraph) and with unlimited table schemas.\nChen et al. (2020a) introduced the LOGICNLG dataset to facilitate the study of table-to-text generation tasks with richer inference. The dataset contains unlimited schemas of open-domain tables crawled from Wikipedia, paired with five annotated sentences covering different logical inferences. Although most inferences are numerical reasoning, the table contents are not fully numeric.\nSimilar in motivation to LOGICNLG in generating text that can be logically entailed by facts in tables, numericNLG consists of collections of paragraphs that are naturally produced by human experts in scientific papers, paired with their corresponding numerical tables. Our dataset has fewer tables than LOGICNLG, focusing on numericalreasoning text in the scientific domain."
    }, {
      "heading" : "4 Table Representation",
      "text" : "Due to ROTOWIRE’s limited schemas, Wiseman et al. (2017) viewed a table input as a set of records (entity, value, type), where the entity and the type are the extracted row and column names, respectively. Because of the unlimited table schemas in our dataset, by capturing the original table structure in real-world tables, this paper uses the representations which consist of captions, row headers, column headers, cell values, and metrics, called a data table. Using only descriptive facts from the data table as input representations is sufficient to generate descriptive texts that explicitly mention facts in the table. However, since we intend to produce more analytical text with numerical reasoning, we propose adding inferred facts to the input representation by computing a set of arithmetic operations on the data table beforehand, defined as a pre-executed operation table.\nData Table We view T as a set of cells with their corresponding row header (rh), column header (ch), numerical value (val), and metric-type (m), defined as a data table (TD). A data table for the example in Figure 1 consists of rh: ((model, our full model), (model, lee et al. (2018))); ch: (); val: ((89.6, 82.2, 85.7), (86.2, 83.7, 84.9); and m:\n(precision, recall, f1). Since our tables are annotated with a targeted header as a content plan for table descriptions, we mark cells corresponding to the targeted header with a target flag (tgt) to highlight the marked cells in text generation. We set tgt = 1 for targeted cells and tgt = 0 for non-targeted cells. In this study, we preprocess the header name by concatenating the row and column headers (h = [rh; ch]) and keep information about the header category by extracting overlapping tokens of row and column headers as th. As a result, we define TD = (hij , thij , valij ,mij , tgtij), where 1 ≤ i ≤ nr, 1 ≤ j ≤ nc; nr and nc are the numbers of rows and columns, respectively.\nPre-executed Operation Table We provide a table of pre-executed cell operations (TOP ) by doing mathematical operations only on targeted cells to limit the calculation. In this study, we cover maximum, minimum, and difference operations. Examples of a preprocessed table, data table, and pre-executed operation table are shown in Figure 2.\nLinearized Table Supporting transfer learning of pre-trained transformers to our table-to-text generation task, we prepare a linearized table PT as an input representation so that it similar to the representation that encoder has seen during pre-training. T is converted to a flat string PT = w1, ..., w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |. In this study, we adopt the template-based input representation, introduced by Kale and Rastogi (2020a), to handle representation bias between a structured data T and a natural language utterance PT , where PT is generated using a manually defined template. We propose not only covering data table TD in the template but also injecting the pre-executed numerical operations of table T through TOP to guide numerical-reasoningbased text generation. We consider four different methods3 for converting T into sequences, the last two being our contributions.\n3An example is shown in Table 6 in the appendix.\n1. Naive Representation T is simply flattened into a sequence ignoring its table structure by concatenating captions, headers, metrics, and targeted cell values:\ncaption: <table id> <caption>. row name: <rh1> . . . <rhnr>. column name: <ch1> . . . <chnc>. metric:<m1>, ..., <mnr/nc>. value: <val1.1> . . . <valnr.nc> .\nThis naive representation omits the relation between rows and columns. Note that <table id> is extracted from the caption to support table mentioning in generating table descriptions.\n2. Data-based Template (TD temp) T is transformed into a natural language sentence by scanning each row of TD with tgt = 1 to fill a manually defined template:\n<table id> shows <caption>. <m1.1> of <h1.1> is <val1.1> . . . <mnr.nc> of <hnr.nc> is <valnr.nc>.\nThis representation covers the semantics of data in the original table.\n3. Reasoning-based Template (TOP temp) Mathematical operation arguments and results from TOP are injected in this representation to cover the numerical reasoning of data in the\noriginal table. We define hop and valop as a header and a value of an operation result respectively, where op ={max, min, diff}. Specific to the difference operation, hdiff1 and hdiff2 refer to the first and second header arguments, respectively. Then, T is represented by concatenating the templatized representation for each row of TOP :\n<table id> shows <caption>. <hmax> has the largest <mmax> (<valmax>) of <thmax>. <hmin> has the smallest <mmin> (<valmax>) of <thmin>. <mdiff> of <hdiff1> is larger/smaller than <hdiff2>.\n4. Data and Reasoning-based Template (TD + TOP temp) T is converted by combining templatized sentences of TD and TOP . This representation covers both data and their numerical reasoning."
    }, {
      "heading" : "5 Generation Models",
      "text" : "The task is to generate text by translating table representation PT into table description Y = y1, y2, ..., yn. We apply a series of generation models to solve the proposed task. While our focus is primarily on pre-trained models since they have been most widely used for limited data settings,\nlike ours, we also include a template-based generator and a pointer-generator network as baselines."
    }, {
      "heading" : "5.1 Non-pre-trained Models",
      "text" : "Template-based Generator We design a domain-specific template-based generator covering two types of sentences in producing table descriptions: table referring sentences and data description sentences. Since our task focuses on numerical-reasoning descriptions, we define templatized sentences using maximum records in table TOP :\n<table id> shows <caption>. we can see that <hmax> outperforms other <thmax> with <valmax> of <mmax>.\nPointer-Generator Pointer-generator (See et al., 2017) is a sequence-to-sequence model with attention and a copy mechanism. This model copes with the out-of-vocabulary problem in data-to-text generation by jointly copying from source texts and generating from a vocabulary."
    }, {
      "heading" : "5.2 Pre-trained Models",
      "text" : "Fine-tuned GPT2 GPT2 (Radford et al., 2019) is a pre-trained language model with a decoder-only transformer architecture. We fine-tuned the GPT2 model by using table representation PT as a prefix of our input. Specifically, we fed the concatenation of table representation PT and table description Y to the model and generated Y . In the inference phase, we used only PT as the input to generate Ŷ starting after the last token of PT .\nFine-tuned T5 T5 (Raffel et al., 2020) is a pretrained transformer model with an encoder-decoder architecture, that solves natural language tasks by converting into a text-to-text format. We fine-tuned the T5 model in our dataset by adding a “summarize” prefix to table representation PT producing output Ŷ .\nCopy Mechanism Pre-trained language models have proven their effectiveness in handling the open vocabulary problem through subword tokenization. Supported by attention layers of the transformer in their architecture, the models learn to attend to source inputs while generating target texts in subword units. However, pre-trained generators often produce texts that are not aligned to table sources. In this study, we propose strengthening their copying ability by incorporating a copy mechanism into the pre-trained models. Although a copy mechanism based on pointer-generator (See et al., 2017) was used for pre-trained models (Chen et al., 2020c) and is well-known in the community, it cannot maintain the global logical structure of sentences with richer inference. We instead employed a simpler copy mechanism based on placeholders (Murakami et al., 2017) with more specific tags than in Chen et al. (2020a). We further propose a ranking-based placeholder alignment algorithm, as illustrated in Figure 3.\nFirst, we align entities and numbers in Y with the data tables TD and pre-executed arithmetic operation results TOP by using string matching. The alignment starts from the first row to the last row of TOP . If no matched token is found, it continues\nto the rows of TD. We set a higher rank to TOP than TD in the alignment since we focus on logical text generation. Then, we replace the matched tokens with corresponding placeholders4 in a templatized description Ytemp. As depicted in Figure 3, since “our full model” in sentence Y is matched with the header result of the maximum operation, we replace it with <header max> placeholder. During the fine-tuning phase, instead of directly generating Y , the models learn to produce a templatized description Ytemp including placeholders as well as words.\nIn the inference phase, we design a ranking algorithm with a placeholder memory to select the best-replaced tokens for placeholders of a predicted templatized description Ŷtemp in producing a generated description Ŷ . We define a set of values in the same row of source tables as a content set and prioritize replacing placeholders in one sentence with the same content set, ensuring sentence coherence. A content set of TD is a tuple of header, metric, and value. For TOP , a content set consists of header, metric, and value of the operation results. Specific to the difference operation, we add the header of the first and second arguments to the content set since the header arguments are important to capture entity comparison in a sentence.\nWe utilize a placeholder memory to temporarily save prioritized placeholder candidates from the same content set that is previously chosen. For\n4Details of placeholders and their definition are in Tables 7 and 8 in the appendix.\nexample, as shown in Figure 3, after replacing the header max placeholder with the header result from the first row of maximum records of TOP in Step 1, the related placeholders from the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Ŷtemp and the alignment starts again from the next content set of table sources."
    }, {
      "heading" : "6 Experiments",
      "text" : "We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations."
    }, {
      "heading" : "6.1 Automatic Evaluation Metrics",
      "text" : "We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTSCORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only\ntargeted table contents for table sources."
    }, {
      "heading" : "6.2 Implementation Details",
      "text" : "We trained a pointer-generator model using the Adagrad optimizer with a batch size of 8 and a learning rate of 0.15. For fine-tuning the GPT2 model, the Adam optimizer set weight decay to 3 × 10−5. Following Raffel et al. (2020), the T5 model was fine-tuned with a constant learning rate of 0.001. We trained all models for a maximum of ten epochs with early stopping based on the loss score on the validation set (patience of 3). At the time of decoding, the generated text was produced through a beam search of size 5."
    }, {
      "heading" : "7 Results",
      "text" : ""
    }, {
      "heading" : "7.1 Automatic Evaluation",
      "text" : "Table 2 shows our experimental results. The finetuned T5 models performed better than the others in terms of BLEU, ROUGE-L, METEOR, and BERTSCORE. The slightly lower PARENT of the best fine-tuned T5 model than the template-based generator implies that the fine-tuned T5 model was also comparable in terms of generating related table descriptions. The pointer-generator model had the lowest score since our dataset consists of limited table collections with a broad vocabulary and challenging target texts.\nEffect of table representation Comparing the performance between table representation types in the pre-trained models, we can see a different tendency between GPT2 and T5. The more similar the table representation used as an input, the higher the score of GPT2. Since GPT2 had only a decoder, the inputs including reasoning-based templates (TOP and TD + TOP ), which are more similar to our target with numerical reasoning, performed the best for several metrics with more than 1 point improvement. In T5 with an encoder-decoder architecture, on the contrary, there was only a slight margin between different table representations. This indicates that the encoder part of T5 can capture table contexts from various input templates. For variants without a copy mechanism, T5 with only data representation (TD) outperformed the other representation types with longer sentences for all metrics. Because of the gap between the encoder and decoder, T5 still had difficulty aligning the information of longer inputs and outputs.\nEffect of copy mechanism The worst scores of the fine-tuned GPT2+copy models indicate that our proposed copy mechanism failed to learn the templatized target patterns in the fine-tuning step. The decoder-only GPT2 could not handle the sparse distributions of target texts with placeholders. Conversely, the copy-based fine-tuned T5 models achieved a better BLEU score due to their encoder and decoder ability in handling output texts with placeholders."
    }, {
      "heading" : "7.2 Qualitative Analysis",
      "text" : "Table 3 shows table descriptions generated by the template-based, pointer-generator, and fine-tuned pre-trained models (GPT2 and T5), using data and reasoning-based templates5 for our table example in Figure 2. We marked sentences related to table captions in green, correct facts based on table contents in blue, and incorrect facts in red. In this study, since we had a limited training set with a broader vocabulary, the pointer-generator model tended to result in repetitive words and failed to generate well-described descriptions. The pre-trained models, GPT2 and T5, generated more natural descriptions. While several pieces of text generated by GPT2 included numerical facts, they used numbers that were not extracted from table contents. The T5 models produced descriptions that were more related to table contents than GPT2.\nConsidering our lengthy output examples in Table 3, unlike the fine-tuned GPT2 model, which generated longer sentences, the fine-tuned T5 model generated shorter sentences than the references.6 The length gap between the references and outputs of the fine-tuned T5 model affected the F1-based metrics of ROUGE-L, METEOR, BERTSCORE, and PARENT. Note that BLEU is a precision-based metric that can handle shorter outputs through a brevity penalty (Papineni et al., 2002). Therefore, we assume that BLEU better represents the performance of the fine-tuned T5 model than the other metrics."
    }, {
      "heading" : "7.3 Human Evaluation",
      "text" : "We conducted a human evaluation7 to better assess the quality of the generated text. We compared our copy-based fine-tuned T5 model with\n5Examples using other table representations are shown in Table 9 in the appendix.\n6Average token length of references: 80.57, GPT2: 87.39, GPT2+copy: 73.58, T5: 39.81, T5+copy: 41.81.\n7The interfaces are shown in Figures 4 and 5 in the appendix.\nthe template-based, pointer-generator, fine-tuned GPT2, and fine-tuned T5 models. We did not compare it against the copy-based fine-tuned GPT2 since GPT2 failed to incorporate our proposed copy mechanism. We used the best table representation with majority metrics for each model on the basis of the experimental results in Table 2.\nIn the first study, we evaluated the correctness of the generated text on the basis of facts in tables. We randomly selected 30 tables in the test set and elicited responses from three graduate students per table. Following Wiseman et al. (2017), the raters were asked to count how many facts in the\ndescriptions were supported by numerical data in the tables and how many were contradicted. Since our task covers numerical-reasoning text, we distinguished descriptive numerical facts from inferred numerical facts. We also measured the level of relevance of the generated text to the table captions by using a four-point Likert scale (highly relevant, relevant, somewhat relevant, and irrelevant).\nThe results are shown in Table 4. The pointergenerator failed to reflect facts due to the wide variety of our table schemas. While the fine-tuned GPT2 model generated sentences with a larger number of descriptive and inferred facts than the others on average, most of the facts were contradictive. The fine-tuned T5 model generated fewer sentences than GPT2, with the average number of inferred facts being larger than that of descriptive facts. Our model based on the fine-tuned T5 model with a copy mechanism reduced the ratio of contradictive facts for both descriptive and inferred facts.\nFollowing earlier work (Puduppully et al., 2019),\nwe also evaluated text fluency in terms of grammaticality, coherence, and conciseness by using best-worst scaling (BWS) (Louviere and Woodworth, 1991; Louviere et al., 2015). We divided the outputs of the five models into ten pairs of descriptions. We presented workers with two descriptions and asked them to decide which one is best for each fluency category.\nThe score of each model was calculated by using the MaxDiff approach (Orme, 2009): the number of times a description was chosen as the best minus the number of times it was chosen as the worst. Scores range from −100 (absolutely worst) to 100 (absolutely best). We elicited judgments with Amazon Mechanical Turk for the 30 descriptions, rated by 3 participants. The results are shown in Table 5. Most of the pre-trained models achieved better scores than the others. The fine-tuned GPT2 model achieved the highest score in terms of grammaticality and coherence. The fine-tuned T5 model achieved the highest score in terms of conciseness. Adding a copy mechanism to the T5 slightly decreased the grammaticality and conciseness but improved the coherence."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We proposed numericNLG, a new dataset for tableto-text generation using a table and its corresponding description from scientific papers, focusing on numerical-reasoning texts. Even though our proposed dataset is not a large-scale table collection, we provided pairs of a table and its rich inference description, that are naturally written by experts in scientific papers, supporting further research on table-to-text generation with numerical reasoning.\nWe conducted experiments with fine-tuned pretrained models by using several types of table linearization as input representations, comparing with a template-based generator and pointer-generator. The experiments showed that transfer-learning of pre-trained language models leads to an improvement in our settings, that resulted in more fluent text while it still lacked fidelity to table contents. We then proposed incorporating a copy mechanism by using general placeholders to avoid the production of hallucinated phrases, that are not supported by tables while preserving high fluency. Even though our proposed copy mechanism failed to learn to generate better outputs in the decoder-only pre-trained models, we showed that a copy-based pre-trained model with an encoder-decoder archi-\ntecture leads to a better BLEU score and improves correctness."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Lya Hulliyyatus Suadaa is supported by the Indonesian Endowment Fund for Education (LPDP) and the Okumura-Takamura-Funakoshi Laboratory, Tokyo Institute of Technology. This work is partially supported by JST PRESTO (Grant Number JPMJPR1655). We thank the anonymous reviewers for their helpful discussion on this work and comments on the previous draft of the paper."
    }, {
      "heading" : "A Table Representation",
      "text" : "An example of table representation for Figure 2 is shown in Table 6."
    }, {
      "heading" : "B Placeholders of Copy-based Pre-trained Models",
      "text" : "Tables 7 and 8 describe placeholders of our proposed copy-based pre-trained models."
    }, {
      "heading" : "C System Output Examples",
      "text" : "Table 9 shows table descriptions generated by the fine-tuned GPT2 and fine-tuned T5 models with and without a copy mechanism, using different types of table representations for our table example in Figure 2."
    }, {
      "heading" : "D Human Evaluation",
      "text" : "Figures 4 and 5 show the user interface for evaluating correctness and relevance and for evaluating grammaticality, coherence, and conciseness, respectively."
    } ],
    "references" : [ {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Logical natural language generation from open-domain tables",
      "author" : [ "Wenhu Chen", "Jianshu Chen", "Yu Su", "Zhiyu Chen", "William Yang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7929–",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Tabfact : A large-scale dataset for table-based fact verification",
      "author" : [ "Wenhu Chen", "Hongmin Wang", "Jianshu Chen", "Yunkai Zhang", "Hong Wang", "Shiyang Li", "Xiyou Zhou", "William Yang Wang." ],
      "venue" : "International Conference on Learning Representations",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot NLG with pre-trained language model",
      "author" : [ "Zhiyu Chen", "Harini Eavani", "Wenhu Chen", "Yinyin Liu", "William Yang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 183–190, Online.",
      "citeRegEx" : "Chen et al\\.,? 2020c",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Handling divergent reference texts when evaluating table-to-text generation",
      "author" : [ "Bhuwan Dhingra", "Manaal Faruqui", "Ankur Parikh", "Ming-Wei Chang", "Dipanjan Das", "William Cohen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Dhingra et al\\.,? 2019",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2019
    }, {
      "title" : "INFOTABS: Inference on tables as semi-structured data",
      "author" : [ "Vivek Gupta", "Maitrey Mehta", "Pegah Nokhiz", "Vivek Srikumar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309–2324, Online. Association",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Template guided text generation for task-oriented dialogue",
      "author" : [ "Mihir Kale", "Abhinav Rastogi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6505–6520, Online. Association for Computa-",
      "citeRegEx" : "Kale and Rastogi.,? 2020a",
      "shortCiteRegEx" : "Kale and Rastogi.",
      "year" : 2020
    }, {
      "title" : "Text-to-text pre-training for data-to-text tasks",
      "author" : [ "Mihir Kale", "Abhinav Rastogi." ],
      "venue" : "Proceedings of the 13th International Conference on Natural Language Generation, pages 97–102, Dublin, Ireland. Association for Computational Linguistics.",
      "citeRegEx" : "Kale and Rastogi.,? 2020b",
      "shortCiteRegEx" : "Kale and Rastogi.",
      "year" : 2020
    }, {
      "title" : "Neural text generation from structured data with application to the biography domain",
      "author" : [ "Rémi Lebret", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203–1213,",
      "citeRegEx" : "Lebret et al\\.,? 2016",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning semantic correspondences with less supervision",
      "author" : [ "Percy Liang", "Michael Jordan", "Dan Klein." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language",
      "citeRegEx" : "Liang et al\\.,? 2009",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2009
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Best-worst scaling: A model for the largest difference judgments",
      "author" : [ "Jordan J. Louviere", "Terry N. Flynn", "A.A.J. Marley." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Louviere et al\\.,? 2015",
      "shortCiteRegEx" : "Louviere et al\\.",
      "year" : 2015
    }, {
      "title" : "Best-worst scaling: A model for the largest difference judgments",
      "author" : [ "Jordan J. Louviere", "George G. Woodworth." ],
      "venue" : "University of Alberta: Working Paper.",
      "citeRegEx" : "Louviere and Woodworth.,? 1991",
      "shortCiteRegEx" : "Louviere and Woodworth.",
      "year" : 1991
    }, {
      "title" : "Learning to generate market comments from stock prices",
      "author" : [ "Soichiro Murakami", "Akihiko Watanabe", "Akira Miyazawa", "Keiichi Goshima", "Toshihiko Yanase", "Hiroya Takamura", "Yusuke Miyao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the",
      "citeRegEx" : "Murakami et al\\.,? 2017",
      "shortCiteRegEx" : "Murakami et al\\.",
      "year" : 2017
    }, {
      "title" : "Operation-guided neural networks for high fidelity data-to-text generation",
      "author" : [ "Feng Nie", "Jinpeng Wang", "Jin-Ge Yao", "Rong Pan", "Chin-Yew Lin." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Nie et al\\.,? 2018",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2018
    }, {
      "title" : "Maxdiff analysis: Simple counting, individual-level logit, and hb",
      "author" : [ "Bryan Orme." ],
      "venue" : "Sawtooth Software, Inc.",
      "citeRegEx" : "Orme.,? 2009",
      "shortCiteRegEx" : "Orme.",
      "year" : 2009
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "ToTTo: A controlled table-totext generation dataset",
      "author" : [ "Ankur Parikh", "Xuezhi Wang", "Sebastian Gehrmann", "Manaal Faruqui", "Bhuwan Dhingra", "Diyi Yang", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Parikh et al\\.,? 2020",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2020
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Pasupat and Liang.,? 2015",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "Data-to-text generation with entity modeling",
      "author" : [ "Ratish Puduppully", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Puduppully et al\\.,? 2019",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural question answering model based on semistructured tables",
      "author" : [ "Hao Wang", "Xiaodong Zhang", "Shuming Ma", "Xu Sun", "Houfeng Wang", "Mengxiang Wang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards faithful neural table-to-text generation with content-matching constraints",
      "author" : [ "Zhenyi Wang", "Xiaoyang Wang", "Bang An", "Dong Yu", "Changyou Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "rec. of model our full model is smaller than model lee et al",
      "author" : [ "lee" ],
      "venue" : null,
      "citeRegEx" : "lee,? \\Q2018\\E",
      "shortCiteRegEx" : "lee",
      "year" : 2018
    }, {
      "title" : "2018) has the largest rec",
      "author" : [ "lee" ],
      "venue" : null,
      "citeRegEx" : "lee,? \\Q2018\\E",
      "shortCiteRegEx" : "lee",
      "year" : 2018
    }, {
      "title" : "Example of table representation. B Placeholders of Copy-based Pre-trained Models Tables 7 and 8 describe placeholders",
      "author" : [ "lee" ],
      "venue" : null,
      "citeRegEx" : "lee,? \\Q2018\\E",
      "shortCiteRegEx" : "lee",
      "year" : 2018
    }, {
      "title" : "<metric other> metric of non",
      "author" : [ "lee" ],
      "venue" : null,
      "citeRegEx" : "lee,? \\Q2018\\E",
      "shortCiteRegEx" : "lee",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "• We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation.",
      "startOffset" : 43,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al.",
      "startOffset" : 33,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al.",
      "startOffset" : 33,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al.",
      "startOffset" : 33,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al.",
      "startOffset" : 33,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : ", 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : "The ROTOWIRE (Wiseman et al., 2017) dataset consists of summaries of NBA basketball games containing several paragraphs, paired with their corresponding box-score tables.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |.",
      "startOffset" : 50,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : ", w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |.",
      "startOffset" : 50,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : ", w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |.",
      "startOffset" : 50,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "Pointer-Generator Pointer-generator (See et al., 2017) is a sequence-to-sequence model with attention and a copy mechanism.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "Fine-tuned GPT2 GPT2 (Radford et al., 2019) is a pre-trained language model with a decoder-only transformer architecture.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "Fine-tuned T5 T5 (Raffel et al., 2020) is a pretrained transformer model with an encoder-decoder architecture, that solves natural language tasks by converting into a text-to-text format.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "Although a copy mechanism based on pointer-generator (See et al., 2017) was used for pre-trained models (Chen et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : ", 2017) was used for pre-trained models (Chen et al., 2020c) and is well-known in the community, it cannot maintain the global logical structure of sentences with richer inference.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "a simpler copy mechanism based on placeholders (Murakami et al., 2017) with more specific tags than in Chen et al.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie,",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : ", 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie,",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 27,
      "context" : "We computed the BERTSCORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : ", 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection.",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "Note that BLEU is a precision-based metric that can handle shorter outputs through a brevity penalty (Papineni et al., 2002).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "maticality, coherence, and conciseness by using best-worst scaling (BWS) (Louviere and Woodworth, 1991; Louviere et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "maticality, coherence, and conciseness by using best-worst scaling (BWS) (Louviere and Woodworth, 1991; Louviere et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "The score of each model was calculated by using the MaxDiff approach (Orme, 2009): the number of times a description was chosen as the best minus the number of times it was chosen as the worst.",
      "startOffset" : 69,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning.",
    "creator" : "LaTeX with hyperref"
  }
}