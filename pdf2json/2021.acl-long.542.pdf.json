{
  "name" : "2021.acl-long.542.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dynamic Contextualized Word Embeddings",
    "authors" : [ "Valentin Hofmann", "Janet B. Pierrehumbert", "Hinrich Schütze" ],
    "emails" : [ "valentin.hofmann@ling-phil.ox.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6970–6984\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6970"
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the last decade, word embeddings have revolutionized the field of NLP. Traditional methods such as LSA (Deerwester et al., 1990), word2vec (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017) compute static word embeddings, i.e., they represent words as a single vector. From a theoretical standpoint, this way of modeling lexical semantics is problematic since it ignores the variability of word meaning in different linguistic contexts (e.g., polysemy) as well as different extralinguistic contexts (e.g., temporal and social variation).\nThe first shortcoming was addressed by the introduction of contextualized word embeddings that represent words as vectors varying across linguistic contexts. This allows them to capture more complex characteristics of word meaning, including polysemy. Contextualized word embeddings are widely used in NLP, constituting the semantic backbone of pretrained language models (PLMs) such as ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet\n(Yang et al., 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al., 2020).\nA concurrent line of work focused on the second shortcoming of static word embeddings, resulting in various types of dynamic word embeddings. Dynamic word embeddings represent words as vectors varying across extralinguistic contexts, in particular time (e.g., Rudolph and Blei, 2018) and social space (e.g., Zeng et al., 2018).\nIn this paper, we introduce dynamic contextualized word embeddings that combine the strengths of contextualized word embeddings with the flexibility of dynamic word embeddings. Dynamic contextualized word embeddings mark a departure from existing contextualized word embeddings (which are not dynamic) as well as existing dynamic word embeddings (which are not contextualized). Furthermore, as opposed to all existing dynamic word embedding types, they represent time and social space jointly.\nWhile our general framework for training dynamic contextualized word embeddings is modelagnostic (Figure 1), we present a version using a PLM (BERT) as the contextualizer, which allows for an easy integration within existing architectures. Dynamic contextualized word embeddings can serve as an analytical tool (e.g., to track the emergence and spread of semantic changes in online communities) or be employed for downstream tasks (e.g., to build temporally and socially aware text classification models), making them beneficial for various areas in NLP that face semantic variability. We illustrate application scenarios by performing exploratory experiments on English data from ArXiv, Ciao, Reddit, and YELP.\nContributions. We introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a PLM, dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks. We showcase potential applications by means of qualitative and quantitative analyses.1"
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Contextualized Word Embeddings",
      "text" : "The distinction between the non-contextualized core meaning of a word and the senses that are realized in specific linguistic contexts lies at the heart of lexical-semantic scholarship (Geeraerts, 2010), going back to at least Paul (1880). In NLP, this is reflected by contextualized word embeddings that map type-level representations to token-level representations as a function of the linguistic context (McCann et al., 2017). As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2017).\nSince their introduction, several studies have analyzed the linguistic properties of contextualized word embeddings (Peters et al., 2018b; Goldberg, 2019; Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Liu et al., 2019; Tenney et al., 2019; Edmiston, 2020; Ettinger, 2020; Hof-\n1We make our code publicly available at https:// github.com/valentinhofmann/dcwe.\nmann et al., 2020; Rogers et al., 2020). Regarding lexical semantics, this line of research has shown that contextualized word embeddings are more context-specific in the upper layers of a contextualizer (Ethayarajh, 2019; Mickus et al., 2020; Vulić et al., 2020) and represent different word senses as separated clusters (Peters et al., 2018a; Coenen et al., 2019; Wiedemann et al., 2019)."
    }, {
      "heading" : "2.2 Dynamic Word Embeddings",
      "text" : "The meaning of a word can also vary across extralinguistic contexts such as time (Bybee, 2015; Koch, 2016) and social space (Robinson, 2010, 2012; Geeraerts, 2018). To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al., 2017, 2018; Oba et al., 2019; Welch et al., 2020a,b; Yao et al., 2020). Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).\nThe relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020). Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.g., the alignment of static word embedding spaces (Hamilton et al., 2016b). However, such approaches come at the cost of modeling disadvantages (Bamler and Mandt, 2017).\nSociolinguistics has shown that temporal and social variation in language are tightly interwoven: innovations such as a new word sense in the case of lexical semantics spread through the language community along social ties (Milroy, 1980, 1992;\nLabov, 2001; Pierrehumbert, 2012). However, most proposed dynamic word embedding types cannot capture more than one dimension of variation. Recently, a few studies have taken first steps in this direction by using genre information within a Bayesian model of semantic change (Frermann and Lapata, 2016; Perrone et al., 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019). In addition, to capture the full range of lexical-semantic variability, dynamic word embeddings should also be contextualized. Crucially, while contextualized word embeddings have been used to investigate semantic change (Giulianelli, 2019; Hu et al., 2019; Giulianelli et al., 2020; Kutuzov and Giulianelli, 2020; Martinc et al., 2020a,b), the word embeddings employed in these studies are not dynamic, i.e., they represent a word in a specific linguistic context by the same contextualized word embedding independent of extralinguistic context or are fit to different time periods as separate models.2"
    }, {
      "heading" : "3 Model",
      "text" : ""
    }, {
      "heading" : "3.1 Model Overview",
      "text" : "Given a sequence of words X = [ x(1), . . . , x(K) ] and corresponding non-contextualized embeddings E = [ e(1), . . . , e(K) ] , contextualizing language models compute the contextualized embedding of a particular word x(k), h(k), as a function c of its non-contextualized embedding, e(k), and the noncontextualized embeddings of words in the left context X(<k) and the right context X(>k),3\nh(k) = c ( e(k), E(<k), E(>k) ) . (1)\nCrucially, while h(k) is a token-level representation, e(k) is a type-level representation and is modeled as a simple embedding look-up. Here, in order to take the variability of word meaning in different extralinguistic contexts into account, we depart from this practice and model e(k) as a function d that depends not only on the identity of x(k) but also on the social context si and the temporal context tj in which the sequence X occurred,\ne (k) ij = d ( x(k), si, tj ) . (2)\n2It is interesting to notice that contextualized word embeddings so far have performed worse than non-contextualized word embeddings on the task of lexical semantic change detection (Kaiser et al., 2020; Schlechtweg et al., 2020).\n3Some contextualizing language models such as GPT-2 (Radford et al., 2019) only operate on X(<k).\nnent ( ), which are then contextualized by the contextualizer ( ). The output of the contextualizer is used to compute the task-specific loss Ltask.\nDynamic contextualized word embeddings are hence computed in two stages: words are first mapped to dynamic type-level representations by d and then to contextualized token-level representations by c (Figures 1 and 2). This two-stage structure follows work in cognitive science and linguistics that indicates that extralinguistic information is processed before linguistic information by human speakers (Hay et al., 2006).\nSince many words in the core vocabulary are semantically stable across social and temporal contexts, we place a Gaussian prior on e(k)ij ,\ne (k) ij ∼ N ( ẽ(k), λ−1a I ) , (3)\nwhere ẽ(k) denotes a non-dynamic representation of x(k). Combining Equations 2 and 3, we write the function d as\nd ( x(k), si, tj ) = ẽ(k) + o (k) ij , (4)\nwhere o(k)ij denotes the vector offset from x (k)’s non-dynamic embedding ẽ(k), which is stable across social and temporal contexts, to its dynamic embedding e(k)ij , which is specific to si and tj . The distribution of o(k)ij then follows a Gaussian with\no (k) ij ∼ N ( 0, λ−1a I ) . (5)\nWe enforce Equation 5 by including a regularization term in the objective function (Section 3.4)."
    }, {
      "heading" : "3.2 Contextualizing Component",
      "text" : "We leverage a PLM for the function c, specifically BERT (Devlin et al., 2019). Denoting with Eij the sequence of dynamic embeddings corresponding to X in si and tj , the dynamic version of Equation 1 becomes\nh (k) ij = BERT ( e (k) ij , E (<k) ij , E (>k) ij ) . (6)\nWe also use BERT, specifically its pretrained input embeddings, to initialize the non-dynamic embeddings ẽ(k), which are summed with the vector offsets o(k)ij (Equation 4) and fed into BERT.\nUsing a PLM for c has the advantage of making it easy to employ dynamic contextualized word embeddings for downstream tasks by adding a taskspecific layer on top of the PLM."
    }, {
      "heading" : "3.3 Dynamic Component",
      "text" : "We model the vector offset o(k)ij as a function of the word x(k), which we represent by its non-dynamic embedding ẽ(k), as well as the social context si, which we represent by a time-specific embedding sij . We use BERT’s pretrained input embeddings for ẽ(k).4 We combine these representations in a time-specific feed-forward network,\no (k) ij = FFNj ( ẽ(k)‖ sij ) , (7)\nwhere ‖ denotes concatenation. To compute the social embedding sij , we follow common practice in the computational social sciences and represent the social community as a graph G = (S, E), where S is the set of social units si, and E is the set of edges between them (Section 4). We use a timespecific graph attention network (GAT) as proposed by Veličković et al. (2018) to encode G,5\nsij = GATj (s̃i,G) . (8)\nWe initialize s̃i with node2vec (Grover and Leskovec, 2016) embeddings.\nTo model the temporal drift of the dynamic embeddings e(k)ij , we follow previous work on dynamic word embeddings (Bamler and Mandt, 2017; Rudolph and Blei, 2018) and impose a random walk prior over o(k)ij ,\no (k) ij ∼ N ( o (k) ij′ , λ −1 w I ) , (9)\n4We also tried to learn separate embeddings in the dynamic component, but this led to worse performance.\n5We also tried a model with a feed-forward network instead of graph attention, but it consistently performed worse.\nwith j′ = j − 1. This type of Gaussian process is known as Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930) and is commonly used to model time series (Roberts et al., 2013). The random walk prior enforces that the dynamic embeddings e(k)ij change smoothly over time."
    }, {
      "heading" : "3.4 Model Training",
      "text" : "The combination with BERT makes dynamic contextualized word embeddings easily applicable to different tasks by adding a task-specific layer on top of the contextualizing component. For training the model, the overall loss is\nLtotal = Ltask + Lpriora + Lpriorw , (10)\nwhere Ltask is the task-specific loss, and Lpriora andLpriorw are the regularization terms that impose the anchoring and random walk priors on the typelevel offset vectors,\nLpriora = λa K K∑ k=1 ‖o(k)ij ‖ 2 2 (11)\nLpriorw = λw K K∑ k=1 ‖o(k)ij − o (k) ij′ ‖ 2 2 . (12)\nIt is common practice to set λa λw (Bamler and Mandt, 2017; Rudolph and Blei, 2018). Here, we set λa = 10−3 · λw, which reduces the number of tunable hyperparameters. We place the priors only on frequent words in the vocabulary (Section 5.1), taking into account the observation that the vocabulary core constitutes the best basis for dynamic word embeddings (Hamilton et al., 2016b)."
    }, {
      "heading" : "4 Data",
      "text" : "We fit dynamic contextualized word embeddings to four datasets with different linguistic, social, and temporal characteristics, which allows us to investigate factors impacting their utility. Each dataset D consists of a set of texts (e.g., reviews) written by a set of social units S (e.g., users) over a sequence of time periods T (e.g., years). Furthermore, the social units are connected by a set of edges E within a social network G. Table 1 provides summary statistics of the four datasets.\nArXiv. ArXiv is an open-access distribution service for scientific articles. Recently, a dataset of all papers published on ArXiv with corresponding metadata was released.6 For this study, we\n6https://www.kaggle.com/ Cornell-University/arxiv\nuse ArXiv’s subject classes (e.g., cs.CL) as social units and extract the abstracts of papers published between 2001 and 2020 for subjects with at least 100 publications in that time.7 To create the network, we measure the overlap in authors between subject classes as the Jaccard similarity of corresponding author sets, resulting in a similarity matrix S. Based on S, we define the adjacency matrix G of G, whose elements are\nGij = ⌈ Sij − θ ⌉ , (13)\ni.e., there is an edge between subject classes i and j if the Jaccard similarity of author sets is greater than θ. We set θ to 0.01.8\nCiao. Ciao is a product review site on which users can mark explicit trust relations towards other users (e.g., if they find their reviews helpful). A dataset containing reviews covering the time period from 2000 to 2011 has been made publicly available (Tang et al., 2012).9 We use the trust relations to create a directed graph. Since we also perform sentiment analysis on the dataset, we follow Yang and Eisenstein (2017) in converting the five-star rating range into two classes by discarding threestar reviews and treating four/five stars as positive and one/two stars as negative.\nReddit. Reddit is a social media platform hosting discussions about a variety of topics. It is divided into smaller communities, so-called subreddits, which have been shown to be highly conducive to linguistic dynamics (del Tredici and Fernández, 2018; del Tredici et al., 2019a). A full dump of public Reddit posts is available online.10 We retrieve all comments between September 2019 and April\n7We treat subject class combinations passing the frequency threshold (e.g., cs.CL&cs.AI) as individual units.\n8We tried other values of θ, but the results were similar. 9https://www.cse.msu.edu/˜tangjili/\ntrust.html 10https://files.pushshift.io/reddit/ comments\n2020, which allows us to examine the effects of the rising Covid-19 pandemic on lexical usage patterns. We remove subreddits with fewer than 10,000 comments in the examined time period and sample 20 comments per subreddit and month. For each subreddit, we compute the set of users with at least 10 comments in the examined time period. Based on this, we use the same strategy as for ArXiv to create a network based on user overlap.\nYELP. Similarly to Ciao, YELP is a product review site on which users can mark explicit friendship relations. A subset of the data has been released online.11 We use the friendship relations to create a directed graph between users. Since we also use the dataset for sentiment analysis, we again discard three-star reviews and convert the five-star rating range into two classes.\nThe fact that the datasets differ in terms of their social and temporal characteristics allows us to examine which factors impact the utility of dynamic contextualized word embeddings. We highlight, e.g., that the datasets differ in the nature of their social units, cover different time periods, and exhibit different levels of temporal granularity. We randomly split all datasets into 70% training, 10% development, and 20% test. We apply stratified sampling to make sure the model sees data from all time points during training. See Appendix A.1 for details about data preprocessing."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Embedding Training",
      "text" : "We fit dynamic contextualized word embeddings to all four datasets, using BERTBASE (uncased) as the contextualizer and masked language modeling as the training objective (Devlin et al., 2019), i.e., we\n11https://www.yelp.com/dataset\nadd a language modeling head on top of BERT.12 To estimate the goodness of fit, we measure masked language modeling perplexity and compare against finetuned (non-dynamic) contextualized word embeddings, specifically BERTBASE (uncased). See Appendix A.2 for details about implementation, hyperparameter tuning, and runtime.\nDynamic contextualized word embeddings (DCWE) yield fits to the data similar to and (sometimes significantly) better than non-dynamic contextualized word embeddings (CWE), which indicates that they successfully combine extralinguistic with linguistic information (Table 2).13"
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "To examine the relative importance of temporal and social information for dynamic contextualized word embeddings, we perform two experiments in which we ablate social context and time (Figure 3). In social ablation (SA), we train dynamic contextualized word embeddings where the vector offset depends only on word identity and time, not social context, keeping the random walk prior between subsequent time slices. In temporal ablation (TA), we use one social component for all time slices. See Appendix A.3 for details about implementation, hyperparameter tuning, and runtime.\nTemporal ablation has more severe consequences than social ablation (Table 3). On Ciao, the social component does not yield better fits on the data at all, which might be related to the fact that many users in this dataset only have one review, and that its social network has the lowest density as well as the smallest average node degree out of all considered datasets (Table 1).\n12For a given dataset, we only compute dynamic embeddings for tokens in BERT’s input vocabulary that are among the 100,000 most frequent words. For less frequent tokens, we input the non-dynamic BERT embedding.\n13Statistical significance is tested with a Wilcoxon signedrank test (Wilcoxon, 1945; Dror et al., 2018)."
    }, {
      "heading" : "5.3 Qualitative Analysis",
      "text" : "Do dynamic contextualized word embeddings indeed capture interpretable dynamics in word meaning? To examine this question qualitatively, we define as sim(k)ij the cosine similarity between the non-dynamic embedding of x(k), ẽ(k), and the dynamic embeddings of x(k) given social and temporal contexts si and tj , e (k) ij ,\nsim (k) ij = cosφ (k) ij , (14)\nwhere φ(k)ij is the angle between ẽ (k) and e(k)ij (Figure 1).14 To find words with a high degree of variability, we compute the standard deviation of sim\n(k) ij based on all si and tj in which a given word\nx(k) occurs in the data,\nσ (k) sim = σ ( {sim(k)ij |(x (k), si, tj) ∈ D} ) , (15)\nwhere we take the development set for D. Looking at the top-ranked words according to σ (k) sim, we observe that they exhibit pronounced\n14In cases where x(k) is split into several WordPiece tokens by BERT, we follow previous work (Pinter et al., 2020; Sia et al., 2020) and average the subword embeddings.\nContext for sim(k)ij > µ (k) sim Context for sim (k) ij < µ (k) sim\nWord Extralinguistic Linguistic Extralinguistic Linguistic\n“isolating” r/SAHP 12/19\nIt’s really hard to explain to other people how isolating and exhausting being a SAHP can be. r/Asthma 03/20\nI wish I knew if I’d had covid so that I could stop self isolating and instead volunteer in my community.\n“testing” r/VJoeShows 04/20 Testing a photocell light fixture during the day is easy when you know how. This is what this DIY video is about.\nr/vancouver 03/20 Testing is not required if a patient has no symptoms, mild symptoms, or is a returning traveller and is isolating at home.\nTable 4: Examples of dynamics in word meaning during the Covid-19 pandemic. The table lists example words with top-ranked values of σ(k)sim, i.e., they exhibit a high degree of extralinguistically-driven semantic dynamics.\nextralinguistically-driven semantic dynamics in the data. For Reddit, e.g., many of the top-ranked words have experienced a sudden shift in their dominant sense during the Covid-19 pandemic such as “isolating” and “testing” (Table 4). Social and temporal contexts in which the sense related to Covid19 is dominant have smaller values of sim(k)ij (i.e., the cosine distance is larger) than the ones in which the more general sense is dominant. Such shortterm semantic shifts, which have attracted growing interest in NLP recently (Stewart et al., 2017; del Tredici et al., 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).\nThus, the qualitative analysis suggests that the dynamic component indeed captures extralinguistically-driven variability in word meaning. In Sections 5.4 and 5.5, we will demonstrate by means of two example applications how this property can be beneficial in practice."
    }, {
      "heading" : "5.4 Exploration 1: Semantic Diffusion",
      "text" : "We will now provide a more in-depth analysis of social and temporal dynamics in word meaning to showcase the potential of dynamic contextualized word embeddings as an analytical tool. Specifically, we will analyze how changes in the dominant sense of a word diffuse through the social networks of ArXiv and Reddit. For ArXiv, we will examine the deep learning sense of the word “network”. For Reddit, we will focus on the medical sense of the word “mask”. We know that these senses have become more widespread over the last few years (ArXiv) and months (Reddit), but we want to test if dynamic contextualized word embeddings can capture this spread, and if they allow us to gain new insights about the spread of semantic associations through social networks in general.\nTo perform this analysis, let r(k,k ′)\nij be the rank of x(k\n′)’s embedding among the N nearest neighbors of x(k)’s embedding, given social and temporal contexts si and tj . We then define as\nr̂ (k,k′) ij = N − r (k,k′) ij + 1 (16)\na semantic similarity score between x(k) and x(k ′). r̂ (k,k′) ij is maximal when x (k′)’s embedding is closest to x(k)’s embedding. We set r̂(k,k ′)\nij = 0 if x (k′)\nis not among the N nearest neighbors of x(k). We set N = 100.\nUsing r̂(k,k ′)\nij , we measure dynamics in the semantic similarity between “network” and “learning” (representing the deep learning sense of “network”) as well as “mask” and “vaccine” (representing the medical sense of “mask”). For all social and temporal contexts in which “network” and “mask” occur, we compute r̂(k,k\n′) ij between their socially and\ntemporally dynamic embeddings on the one hand and time-specific centroids of “learning” and “vaccine” averaged over social contexts on the other, employing contextualized versions of the dynamic embeddings.15 In cases where “network” or “mask” occur more than once in a certain social and temporal context, we take the mean of r̂(k,k\n′) ij .\nThe dynamics of r̂(k,k ′)\nij reflect how the changes in the dominant sense of “network” and “mask” spread through the social networks (Figure 4). For “network”, we see that the deep learning sense was already present in computer science and physics in 2013, where neural networks have been used since the 1980s. It then gradually spread from these two epicenters, with a major intensification after 2016. For “mask”, we also see a gradual diffusion, with a major intensification after 03/2020.\n15We average the first six layers of the contextualizer since they have been shown to contain the core of lexical and semantic information (Vulić et al., 2020).\nr̂ (k,k′) ij , a score for semantic similarity between 0 (no similarity) and 100 (very similar), for “network” and “learning” in ArXiv as well as “mask” and “vaccine” in Reddit. The different node shapes in the ArXiv network represent the three major ArXiv subject classes: computer science (square), mathematics (triangle), and physics (circle). For “network”, the change towards the deep learning sense spread gradually from computer science and physics. For “mask”, the change towards the medical sense also spread gradually, with a major intensification after 03/2020.\nOn what paths do new semantic associations spread through the social network? In complex systems theory, there are two basic types of random motion on networks: random walks, which consist of a series of consecutive random steps, and random flights, where step lengths are drawn from the Lévy distribution (Masuda et al., 2017). To probe whether there is a dominant type of spread for the two examples, we compute for each time slice tj what proportion of nodes that have r̂(k,k\n′) ij > 0 for\nthe first time at tj (i.e., the change in the dominant sense has just arrived) are neighbors of nodes that already had r̂(k,k\n′) ij > 0 before tj . This anal-\nysis shows that random walks are the dominant type of spread for “network”, but random flights for “mask” (Figure 5). Intuitively, it makes sense that a technical concept such as neural networks spreads through the direct contact of collaborating scientists rather than through more distant forms of reception (e.g., the reading of articles). In the case of facial masks, on the other hand, the exogenous factor of the worsening Covid-19 pandemic and the accompanying publicity was a driver of semantic dynamics irrespective of node position."
    }, {
      "heading" : "5.5 Exploration 2: Sentiment Analysis",
      "text" : "As a second testbed, we apply dynamic contextualized word embeddings on a task for which social and temporal information is known to be important (Yang and Eisenstein, 2017): sentiment analysis. We use the Ciao and YELP datasets and train dynamic contextualized word embeddings by adding a two-layer feed-forward network on top of BERTBASE (uncased) and finetuning it for the task of sentiment classification.16 We again compare\n16We finetune directly on sentiment analysis without prior finetuning on masked language modeling.\nagainst contextualized word embeddings, specifically BERTBASE (uncased), which is finetuned without the dynamic component. See Appendix A.4 for details about implementation, hyperparameter tuning, and runtime.\nDynamic contextualized word embeddings achieve slight but significant improvements over the already strong performance of non-dynamic BERT (Table 5).17 This provides further evidence that infusing social and temporal information on the lexical level can be useful for NLP tasks."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have introduced dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a PLM, specifically BERT, dynamic contextualized word embeddings model time and social space jointly, which makes them advantageous for various areas in NLP. We have trained dynamic contextualized word embeddings on four datasets and showed that they are capable of tracking social and temporal variability in word meaning. Besides serving as an analytical tool, dynamic contextualized word embeddings can also be of benefit for downstream tasks such as sentiment analysis."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was funded by the European Research Council (#740516) as well as the Engineering and Physical Sciences Research Council (EP/T023333/1). The first author was also supported by the German Academic Scholarship Foundation and the Arts and Humanities Research Council. We thank the anonymous reviewers for their detailed and extremely helpful comments.\n17Statistical significance is tested with a McNemar’s test for binary data (McNemar, 1947; Dror et al., 2018)"
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Data Preprocessing For each dataset, we remove duplicates as well as texts with less than 10 words. For the Ciao dataset, we further remove reviews rated as not helpful. We lowercase all words. Since BERT’s input is limited to 512 tokens, we truncate longer texts by taking the first and last 256 tokens.\nA.2 Embedding Training: Hyperparameters DCWE. The hyperparameters of the contextualizer are as for BERTBASE (uncased). In particular, the dimensionality of the input embeddings ẽ(k) is 768. For the dynamic component, the social vectors sij and s̃i have a dimensionality of 50. The node2vec vectors for the initialization of s̃i are trained on 10 sampled walks of length 80 per node with a window size of 2. The GAT has two layers with four attention heads, respectively (activation function: tanh). The feed-forward network has two layers (activation function: tanh). We apply dropout\nwith a rate of 0.2 after each layer of the dynamic component. The number of trainable parameters varies between models trained on different datasets due to differences in |T | and is 134,914,570 for ArXiv, 124,990,698 for Ciao, 120,028,762 for Reddit, and 122,509,730 for YELP. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 7}, the learning rate l ∈ {1× 10−6, 3× 10−6}, and the regularization constant λa ∈ {1× 10−2, 1× 10−1}, thereby also determining λw (Section 3.4).\nCWE. All hyperparameters are as for BERTBASE (uncased). The number of trainable parameters is 110,104,890. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 7} and the learning rate l ∈ {1× 10−6, 3× 10−6}.\nFor both DCWE and CWE, we tune hyperparameters except for the number of epochs on the Ciao dataset (selection criterion: masked language modeling perplexity) and use the best configuration for ArXiv, Reddit, and YELP. Models are trained with categorical cross-entropy as the loss function and Adam (Kingma and Ba, 2015) as the optimizer. Experiments are performed on a GeForce GTX 1080 Ti GPU (11GB).\nTable 6 lists statistics of the validation performance over hyperparameter search trials and provides information about best hyperparameter configurations.18 We also report the number of hyperparameter search trials as well as runtimes for the hyperparameter search.\nA.3 Ablation Study: Hyperparameters\nSA. Words are mapped to offsets using timespecific two-layer feed-forward networks (activation function: tanh). Both layers have a dimensionality of 768. All other hyperparameters are\n18Since expected validation performance (Dodge et al., 2019) may not be correct for grid search, we report mean and standard deviation of the performance instead.\nas for DCWE with a full dynamic component (Appendix A.2). The number of trainable parameters again varies between models trained on different datasets due to differences in |T | and is 133,728,570 for ArXiv, 124,279,098 for Ciao, 119,554,362 for Reddit, and 121,916,730 for YELP. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 7}, the learning rate l ∈ {1× 10−6, 3× 10−6}, and the regularization constant λa ∈ {1× 10−2, 1× 10−1}, thereby also determining λw (Section 3.4).\nTA. All hyperparameters are as for DCWE with a full dynamic component (Appendix A.2), with the difference that we only use one social component (consisting of a two-layer GAT and a two-layer feed-forward network) for all time units. The number of trainable parameters is 111,345,374. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 7}, the learning rate l ∈ {1× 10−6, 3× 10−6}, and the regularization constant λa ∈ {1× 10−2, 1× 10−1}.\nFor both SA and TA, we tune hyperparameters except for the number of epochs on the Ciao dataset (selection criterion: masked language modeling perplexity) and use the best configuration for ArXiv, Reddit, and YELP. Models are trained with categorical cross-entropy as the loss function and Adam as the optimizer. Experiments are performed on a GeForce GTX 1080 Ti GPU (11GB).\nTable 7 lists statistics of the validation performance over hyperparameter search trials and provides information about best hyperparameter configurations. We also report the number of hyperparameter search trials as well as runtimes for the hyperparameter search.\nA.4 Sentiment Analysis: Hyperparameters\nDCWE. The mid layer of the feed-forward network on top of BERT has a dimensionality of 100. All other hyperparameters are as for DCWE trained on masked language modeling (Appendix A.2).\nThe number of trainable parameters again varies between models trained on different datasets due to differences in |T | and is 124,445,049 for Ciao and 121,964,081 for YELP. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 5}, the learning rate l ∈ {1× 10−6, 3× 10−6}, and the regularization constant λa ∈ {1× 10−2, 1× 10−1}, thereby also determining λw (Section 3.4).\nCWE. The mid layer of the feed-forward network on top of BERT has a dimensionality of 100. All other hyperparameters are as for BERTBASE (uncased). The number of trainable parameters is 109,559,241. We use a batch size of 4 and perform grid search for the number of epochs ne ∈ {1, . . . , 5} and the learning rate l ∈ {1× 10−6, 3× 10−6}.\nFor both DCWE and CWE, we tune hyperparameters except for the number of epochs on the Ciao dataset (selection criterion: F1 score) and use the best configuration for YELP. Models are trained with binary cross-entropy as the loss function and Adam as the optimizer. Experiments are performed on a GeForce GTX 1080 Ti GPU (11GB).\nTable 8 lists statistics of the validation performance over hyperparameter search trials and provides information about best hyperparameter con-\nfigurations. We also report the number of hyperparameter search trials as well as runtimes for the hyperparameter search."
    } ],
    "references" : [ {
      "title" : "Modelling context with user embeddings for sarcasm detection in social media",
      "author" : [ "Silvio Amir", "Byron C. Wallace", "Hao Lyu", "Paula Carvalho", "Mário J. Silva." ],
      "venue" : "Conference on Computational Natural Language Learning (CoNLL) 20.",
      "citeRegEx" : "Amir et al\\.,? 2016",
      "shortCiteRegEx" : "Amir et al\\.",
      "year" : 2016
    }, {
      "title" : "Historical and comparative linguistics",
      "author" : [ "Raimo Anttila." ],
      "venue" : "John Benjamins, Amsterdam.",
      "citeRegEx" : "Anttila.,? 1989",
      "shortCiteRegEx" : "Anttila.",
      "year" : 1989
    }, {
      "title" : "EmbLexChange at SemEval-2020 task 1: Unsupervised embedding-based detection of lexical semantic changes",
      "author" : [ "Ehsaneddin Asgari", "Christoph Ringlstetter", "Hinrich Schütze." ],
      "venue" : "International Workshop on Semantic Evaluation (SemEval) 2020.",
      "citeRegEx" : "Asgari et al\\.,? 2020",
      "shortCiteRegEx" : "Asgari et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic word embeddings",
      "author" : [ "Robert Bamler", "Stephan Mandt." ],
      "venue" : "International Conference on Machine Learning (ICML) 34.",
      "citeRegEx" : "Bamler and Mandt.,? 2017",
      "shortCiteRegEx" : "Bamler and Mandt.",
      "year" : 2017
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Language change",
      "author" : [ "Joan Bybee." ],
      "venue" : "Cambridge University Press, Cambridge, UK.",
      "citeRegEx" : "Bybee.,? 2015",
      "shortCiteRegEx" : "Bybee.",
      "year" : 2015
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations (ICLR) 8.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing and measuring the geometry of BERT",
      "author" : [ "Andy Coenen", "Emily Reif", "Ann Yuan", "Been Kim", "Adam Pearce", "Fernanda Viégas", "Martin Wattenberg." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS) 33.",
      "citeRegEx" : "Coenen et al\\.,? 2019",
      "shortCiteRegEx" : "Coenen et al\\.",
      "year" : 2019
    }, {
      "title" : "Explaining language change: An evolutionary approach",
      "author" : [ "William Croft." ],
      "venue" : "Pearson, Harlow, UK.",
      "citeRegEx" : "Croft.,? 2000",
      "shortCiteRegEx" : "Croft.",
      "year" : 2000
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T. Dumais", "George Furnas", "Thomas Landauer", "Richard Harshman." ],
      "venue" : "Journal of the American Society for Information Science, 41(6):391–407.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "The road to success: Assessing the fate of linguistic innovations in online communities",
      "author" : [ "Marco del Tredici", "Raquel Fernández." ],
      "venue" : "International Conference on Computational Linguistics (COLING)",
      "citeRegEx" : "Tredici and Fernández.,? 2018",
      "shortCiteRegEx" : "Tredici and Fernández.",
      "year" : 2018
    }, {
      "title" : "Short-term meaning shift: A distributional exploration",
      "author" : [ "Marco del Tredici", "Raquel Fernández", "Gemma Boleda." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Tredici et al\\.,? 2019a",
      "shortCiteRegEx" : "Tredici et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. You shall know a user by the company it keeps: Dynamic representations for social media users in nlp",
      "author" : [ "Marco del Tredici", "Diego Marcheggiani", "Sabine Schulte im Walde", "Raquel Fernández" ],
      "venue" : null,
      "citeRegEx" : "Tredici et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tredici et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Show your work: Improved reporting of experimental results",
      "author" : [ "Jesse Dodge", "Suchin Gururangan", "Dallas Card", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019.",
      "citeRegEx" : "Dodge et al\\.,? 2019",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2019
    }, {
      "title" : "The hitchhiker’s guide to testing statistical significance in natural language processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 56.",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "Time-out: Temporal referencing for robust modeling of lexical semantic change",
      "author" : [ "Haim Dubossarsky", "Simon Hengchen", "Nina Tahmasebi", "Dominik Schlechtweg." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 57.",
      "citeRegEx" : "Dubossarsky et al\\.,? 2019",
      "shortCiteRegEx" : "Dubossarsky et al\\.",
      "year" : 2019
    }, {
      "title" : "A systematic analysis of morphological content in BERT models for multiple languages",
      "author" : [ "Daniel Edmiston." ],
      "venue" : "arXiv 2004.03032.",
      "citeRegEx" : "Edmiston.,? 2020",
      "shortCiteRegEx" : "Edmiston.",
      "year" : 2020
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019.",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "A Bayesian model of diachronic meaning change",
      "author" : [ "Lea Frermann", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:31–45.",
      "citeRegEx" : "Frermann and Lapata.,? 2016",
      "shortCiteRegEx" : "Frermann and Lapata.",
      "year" : 2016
    }, {
      "title" : "Theories of lexical semantics",
      "author" : [ "Dirk Geeraerts." ],
      "venue" : "Oxford University Press, Oxford, UK.",
      "citeRegEx" : "Geeraerts.,? 2010",
      "shortCiteRegEx" : "Geeraerts.",
      "year" : 2010
    }, {
      "title" : "Ten lectures on cognitive sociolinguistics",
      "author" : [ "Dirk Geeraerts." ],
      "venue" : "Brill, Leiden.",
      "citeRegEx" : "Geeraerts.,? 2018",
      "shortCiteRegEx" : "Geeraerts.",
      "year" : 2018
    }, {
      "title" : "Lexical semantic change analysis with contextualised word representations",
      "author" : [ "Mario Giulianelli." ],
      "venue" : "University of Amsterdam, Amsterdam.",
      "citeRegEx" : "Giulianelli.,? 2019",
      "shortCiteRegEx" : "Giulianelli.",
      "year" : 2019
    }, {
      "title" : "Analysing lexical semantic change with contextualised word representations",
      "author" : [ "Mario Giulianelli", "Marco del Tredici", "Raquel Fernández." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 58.",
      "citeRegEx" : "Giulianelli et al\\.,? 2020",
      "shortCiteRegEx" : "Giulianelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing BERT’s syntactic abilities",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "arXiv 1901.05287.",
      "citeRegEx" : "Goldberg.,? 2019",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Enriching word embeddings with temporal and spatial information",
      "author" : [ "Hongyu Gong", "Suma Bhat", "Pramod Viswanath." ],
      "venue" : "Conference on Computational Natural Language Learning (CoNLL) 24.",
      "citeRegEx" : "Gong et al\\.,? 2020",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2020
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "Aditya Grover", "Jure Leskovec." ],
      "venue" : "International Conference on Knowledge Discovery and Data Mining (KDD) 22.",
      "citeRegEx" : "Grover and Leskovec.,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "Inducing domain-specific sentiment lexicons from unlabeled corpora",
      "author" : [ "William Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016.",
      "citeRegEx" : "Hamilton et al\\.,? 2016a",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "Diachronic word embeddings reveal statistical laws of semantic change",
      "author" : [ "William Hamilton", "Jure Leskovec", "Dan Jurafsky." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Hamilton et al\\.,? 2016b",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "Factors influencing speech perception in the context of a merger-in-progress",
      "author" : [ "Jennifer Hay", "Paul Warren", "Katie Drager." ],
      "venue" : "Journal of Phonetics, 34(4):458–484.",
      "citeRegEx" : "Hay et al\\.,? 2006",
      "shortCiteRegEx" : "Hay et al\\.",
      "year" : 2006
    }, {
      "title" : "CASCADE: Contextual sarcasm detection in online discussion forums",
      "author" : [ "Devamanyu Hazarika", "Soujanya Poria", "Sruthi Gorantla", "Erik Cambria", "Roger Zimmermann", "Rada Mihalcea." ],
      "venue" : "International Conference on Computational Linguistics",
      "citeRegEx" : "Hazarika et al\\.,? 2018",
      "shortCiteRegEx" : "Hazarika et al\\.",
      "year" : 2018
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "DagoBERT: Generating derivational morphology with a pretrained language model",
      "author" : [ "Valentin Hofmann", "Janet B. Pierrehumbert", "Hinrich Schütze." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Hofmann et al\\.,? 2020",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Diachronic sense modeling with deep contextualized word embeddings: An ecological view",
      "author" : [ "Renfen Hu", "Shen Li", "Shichen Liang." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 57.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Diachronic degradation of language models: Insights from social media",
      "author" : [ "Kokil Jaidka", "Niyati Chhaya", "Lyle H. Ungar." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Jaidka et al\\.,? 2018",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2018
    }, {
      "title" : "What does BERT learn about the structure of language? In Annual Meeting of the Association for Computational Linguistics (ACL) 57",
      "author" : [ "Ganesh Jawahar", "Benoit Sagot", "Djamé Seddah" ],
      "venue" : null,
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextualized diachronic word representations",
      "author" : [ "Ganesh Jawahar", "Djamé Seddah." ],
      "venue" : "International Workshop on Computational Approaches to Historical Language Change 1.",
      "citeRegEx" : "Jawahar and Seddah.,? 2019",
      "shortCiteRegEx" : "Jawahar and Seddah.",
      "year" : 2019
    }, {
      "title" : "OP-IMS @ DIACR-Ita: Back to the roots: SGNS+OP+CD still rocks semantic change detection",
      "author" : [ "Jens Kaiser", "Dominik Schlechtweg", "Sabine Schulte im Walde." ],
      "venue" : "Evaluation Campaign of Natural Language Processing and Speech Tools for Ital-",
      "citeRegEx" : "Kaiser et al\\.,? 2020",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2020
    }, {
      "title" : "Temporal analysis of language through neural language models",
      "author" : [ "Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov." ],
      "venue" : "Workshop on Language Technologies and Computational Social Science.",
      "citeRegEx" : "Kim et al\\.,? 2014",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy L. Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR) 3.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Meaning change and semantic shifts",
      "author" : [ "Peter Koch." ],
      "venue" : "Päivi Juvonen and Maria Koptjevskaja-Tamm, editors, The lexical typology of semantic shifts, pages 21–66. De Gruyter, Berlin.",
      "citeRegEx" : "Koch.,? 2016",
      "shortCiteRegEx" : "Koch.",
      "year" : 2016
    }, {
      "title" : "Statistically significant detection of linguistic change",
      "author" : [ "Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "The Web Conference (WWW) 24.",
      "citeRegEx" : "Kulkarni et al\\.,? 2015",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "UiOUvA at SemEval-2020 task 1: Contextualised embeddings for lexical semantic change detection",
      "author" : [ "Andrey Kutuzov", "Mario Giulianelli." ],
      "venue" : "International Workshop on Semantic Evaluation (SemEval) 2020.",
      "citeRegEx" : "Kutuzov and Giulianelli.,? 2020",
      "shortCiteRegEx" : "Kutuzov and Giulianelli.",
      "year" : 2020
    }, {
      "title" : "Diachronic word embeddings and semantic shifts: A survey",
      "author" : [ "Andrey Kutuzov", "Lilja Øvrelid", "Terrence Szymanski", "Erik Velldal." ],
      "venue" : "International Conference on Computational Linguistics (COLING) 27.",
      "citeRegEx" : "Kutuzov et al\\.,? 2018",
      "shortCiteRegEx" : "Kutuzov et al\\.",
      "year" : 2018
    }, {
      "title" : "Principles of linguistic change: Social Factors",
      "author" : [ "William Labov." ],
      "venue" : "Blackwell, Malden, MA.",
      "citeRegEx" : "Labov.,? 2001",
      "shortCiteRegEx" : "Labov.",
      "year" : 2001
    }, {
      "title" : "Encoding social information with graph convolutional networks for political perspective detection in news media",
      "author" : [ "Chang Li", "Dan Goldwasser." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 57.",
      "citeRegEx" : "Li and Goldwasser.,? 2019",
      "shortCiteRegEx" : "Li and Goldwasser.",
      "year" : 2019
    }, {
      "title" : "Open sesame: Getting inside BERT’s linguistic knowledge",
      "author" : [ "Yongjie Lin", "Yi C. Tan", "Robert Frank." ],
      "venue" : "Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP) 2.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew Peters", "Noah A. Smith." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis under temporal shift",
      "author" : [ "Jan Lukes", "Anders Søgaard." ],
      "venue" : "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA) 9.",
      "citeRegEx" : "Lukes and Søgaard.,? 2018",
      "shortCiteRegEx" : "Lukes and Søgaard.",
      "year" : 2018
    }, {
      "title" : "Leveraging contextual embeddings for detecting diachronic semantic shift",
      "author" : [ "Matej Martinc", "Petra Kralj Novak", "Senja Pollak." ],
      "venue" : "International Conference on Language Resources and Evaluation (LREC) 12.",
      "citeRegEx" : "Martinc et al\\.,? 2020a",
      "shortCiteRegEx" : "Martinc et al\\.",
      "year" : 2020
    }, {
      "title" : "Capturing evolution in word usage: Just add more clusters",
      "author" : [ "Matej Martinc", "Syrielle Montariol", "Elaine Zosa", "Lidia Pivovarova" ],
      "venue" : "In The Web Conference (WWW)",
      "citeRegEx" : "Martinc et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martinc et al\\.",
      "year" : 2020
    }, {
      "title" : "Random walks and diffusion on networks",
      "author" : [ "Naoki Masuda", "Mason A. Porter", "Renaud Lambiotte." ],
      "venue" : "Physics Reports, 716-717:1–58.",
      "citeRegEx" : "Masuda et al\\.,? 2017",
      "shortCiteRegEx" : "Masuda et al\\.",
      "year" : 2017
    }, {
      "title" : "Learned in translation: Contextualized word vectors",
      "author" : [ "Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 31.",
      "citeRegEx" : "McCann et al\\.,? 2017",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2017
    }, {
      "title" : "Note on the sampling error of the difference between correlated proportions or percentages",
      "author" : [ "Quinn McNemar." ],
      "venue" : "Psychometrika, 12(2):153–157.",
      "citeRegEx" : "McNemar.,? 1947",
      "shortCiteRegEx" : "McNemar.",
      "year" : 1947
    }, {
      "title" : "What do you mean, BERT? Assessing BERT as a distributional semantics model",
      "author" : [ "Timothee Mickus", "Denis Paperno", "Mathieu Constant", "Kees van Deemter." ],
      "venue" : "Society for Computation in Linguistics (SCiL) 3.",
      "citeRegEx" : "Mickus et al\\.,? 2020",
      "shortCiteRegEx" : "Mickus et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv 1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 26.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic variation and change: On the historical sociolinguistic of English",
      "author" : [ "James Milroy." ],
      "venue" : "Blackwell, Oxford, UK.",
      "citeRegEx" : "Milroy.,? 1992",
      "shortCiteRegEx" : "Milroy.",
      "year" : 1992
    }, {
      "title" : "Language and social networks",
      "author" : [ "Lesley Milroy." ],
      "venue" : "Blackwell, Oxford, UK.",
      "citeRegEx" : "Milroy.,? 1980",
      "shortCiteRegEx" : "Milroy.",
      "year" : 1980
    }, {
      "title" : "Author profiling for abuse detection",
      "author" : [ "Pushkar Mishra", "Marco del Tredici", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "International Conference on Computational Linguistics (COLING)",
      "citeRegEx" : "Mishra et al\\.,? 2018",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2018
    }, {
      "title" : "Abusive language detection with graph convolutional networks",
      "author" : [ "Pushkar Mishra", "Marco del Tredici", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Mishra et al\\.,? 2019",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling personal biases in language use by inducing personalized word embeddings",
      "author" : [ "Daisuke Oba", "Naoki Yoshinaga", "Shoetsu Sato", "Satoshi Akasaki", "Masashi Toyoda." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Oba et al\\.,? 2019",
      "shortCiteRegEx" : "Oba et al\\.",
      "year" : 2019
    }, {
      "title" : "Principien der Sprachgeschichte",
      "author" : [ "Hermann Paul." ],
      "venue" : "Tübingen, Niemeyer.",
      "citeRegEx" : "Paul.,? 1880",
      "shortCiteRegEx" : "Paul.",
      "year" : 1880
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2014.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "GASC: Genre-aware semantic change for Ancient Greek",
      "author" : [ "Valerio Perrone", "Palma", "Marco", "Simon Hengchen", "Alessandro Vatri", "Jim Q. Smith", "Barbara McGillivray." ],
      "venue" : "International Workshop on Computational Approaches to Historical",
      "citeRegEx" : "Perrone et al\\.,? 2019",
      "shortCiteRegEx" : "Perrone et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Peters et al\\.,? 2018a",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Dissecting contextual word embeddings: Architecture and representation",
      "author" : [ "Matthew Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2018.",
      "citeRegEx" : "Peters et al\\.,? 2018b",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "The dynamic lexicon",
      "author" : [ "Janet Pierrehumbert." ],
      "venue" : "Abigail Cohn, Cécile Fougeron, and Marie Huffman, editors, The Oxford handbook of laboratory phonology, pages 173–183. Oxford University Press, Oxford.",
      "citeRegEx" : "Pierrehumbert.,? 2012",
      "shortCiteRegEx" : "Pierrehumbert.",
      "year" : 2012
    }, {
      "title" : "Will it unblend? In Findings of Empirical Methods in Natural Language Processing (EMNLP) 2020",
      "author" : [ "Yuval Pinter", "Cassandra L. Jacobs", "Jacob Eisenstein" ],
      "venue" : null,
      "citeRegEx" : "Pinter et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pinter et al\\.",
      "year" : 2020
    }, {
      "title" : "CIRCE at SemEval-2020 task 1: Ensembling context-free and context-dependent word representations",
      "author" : [ "Martin Pömsl", "Roman Lyapin." ],
      "venue" : "International Workshop on Semantic Evaluation (SemEval) 2020.",
      "citeRegEx" : "Pömsl and Lyapin.,? 2020",
      "shortCiteRegEx" : "Pömsl and Lyapin.",
      "year" : 2020
    }, {
      "title" : "Tracking shortterm temporal linguistic dynamics to characterize candidate therapeutics for COVID-19 in the CORD19 corpus",
      "author" : [ "James Powell", "Kari Sentz." ],
      "venue" : "Conference on Embedded Networked Sensor Systems (SenSys) 18.",
      "citeRegEx" : "Powell and Sentz.,? 2020",
      "shortCiteRegEx" : "Powell and Sentz.",
      "year" : 2020
    }, {
      "title" : "UWB at SemEval-2020 task 1: Lexical semantic change detection",
      "author" : [ "Ondřej Pražák", "Pavel Přibáň", "Stephen Taylor", "Jakub Sido." ],
      "venue" : "International Workshop on Semantic Evaluation (SemEval) 2020.",
      "citeRegEx" : "Pražák et al\\.,? 2020",
      "shortCiteRegEx" : "Pražák et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Gaussian processes for time-series modelling",
      "author" : [ "Stephen Roberts", "Michael Osborne", "Mark Ebden", "Steven Reece", "Neale Gibson", "Suzanne Aigrain." ],
      "venue" : "Philosophical Transactions of the Royal Society A, 371(1984):20110550.",
      "citeRegEx" : "Roberts et al\\.,? 2013",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2013
    }, {
      "title" : "Awesome insights into semantic variation",
      "author" : [ "Justyna Robinson." ],
      "venue" : "Dirk Geeraerts, Gitte Kristiansen, and Yves Peirsman, editors, Advances in cognitive sociolinguistics, pages 85–109. De Gruyter, Berlin.",
      "citeRegEx" : "Robinson.,? 2010",
      "shortCiteRegEx" : "Robinson.",
      "year" : 2010
    }, {
      "title" : "A sociolinguistic approach to semantic change",
      "author" : [ "Justyna Robinson." ],
      "venue" : "Kathryn Allan and Justyna Robinson, editors, Current methods in historical semantics, pages 199–231. De Gruyter, Berlin.",
      "citeRegEx" : "Robinson.,? 2012",
      "shortCiteRegEx" : "Robinson.",
      "year" : 2012
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "arXiv 2002.12327.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep neural models of semantic shift",
      "author" : [ "Alex Rosenfeld", "Katrin Erk." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT) 2018, pages 474–484.",
      "citeRegEx" : "Rosenfeld and Erk.,? 2018",
      "shortCiteRegEx" : "Rosenfeld and Erk.",
      "year" : 2018
    }, {
      "title" : "Dynamic embeddings for language evolution",
      "author" : [ "Maja Rudolph", "David Blei." ],
      "venue" : "The Web Conference (WWW) 27.",
      "citeRegEx" : "Rudolph and Blei.,? 2018",
      "shortCiteRegEx" : "Rudolph and Blei.",
      "year" : 2018
    }, {
      "title" : "A wind of change: Detecting and evaluating lexical semantic change across times and domains",
      "author" : [ "Dominik Schlechtweg", "Anna Hätty", "Marco del Tredici", "Sabine Schulte im Walde." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Schlechtweg et al\\.,? 2019",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2020 task 1: Unsupervised lexical semantic change detection",
      "author" : [ "Dominik Schlechtweg", "Barbara McGillivray", "Simon Hengchen", "Haim Dubossarsky", "Nina Tahmasebi." ],
      "venue" : "International Workshop on Semantic Evaluation (SemEval) 2020.",
      "citeRegEx" : "Schlechtweg et al\\.,? 2020",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2020
    }, {
      "title" : "Simulating lexical semantic change from sense-annotated data",
      "author" : [ "Dominik Schlechtweg", "Sabine Schulte im Walde." ],
      "venue" : "International Conference on the Evolution of Language (EvoLang) 13.",
      "citeRegEx" : "Schlechtweg and Walde.,? 2020",
      "shortCiteRegEx" : "Schlechtweg and Walde.",
      "year" : 2020
    }, {
      "title" : "Diachronic usage relatedness (DURel): A framework for the annotation of lexical semantic change",
      "author" : [ "Dominik Schlechtweg", "Sabine Schulte im Walde", "Stefanie Eckmann." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Schlechtweg et al\\.,? 2018",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2018
    }, {
      "title" : "Tired of topic models? Clusters of pretrained word embeddings make for fast and good topics too",
      "author" : [ "Suzanna Sia", "Ayush Dalmia", "Sabrina J. Mielke" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Sia et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sia et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring, predicting and visualizing short-term change in word representation and usage in VKontakte social network",
      "author" : [ "Ian Stewart", "Dustin Arendt", "Eric Bell", "Svitlana Volkova." ],
      "venue" : "International AAAI Conference on Weblogs and Social Me-",
      "citeRegEx" : "Stewart et al\\.,? 2017",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 2017
    }, {
      "title" : "Survey of computational approaches to lexical semantic change detection",
      "author" : [ "Nina Tahmasebi", "Lars Borin", "Adam Jatowt." ],
      "venue" : "arXiv 1811.06278.",
      "citeRegEx" : "Tahmasebi et al\\.,? 2018",
      "shortCiteRegEx" : "Tahmasebi et al\\.",
      "year" : 2018
    }, {
      "title" : "mTrust: Discerning multi-faceted trust in a connected world",
      "author" : [ "Jiliang Tang", "Huiji Gao", "Huan Liu." ],
      "venue" : "International Conference on Web Search and Data Mining (WSDM) 5.",
      "citeRegEx" : "Tang et al\\.,? 2012",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2012
    }, {
      "title" : "What do you learn from context? probing for sentence structure",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R. Thomas McCoy", "Najoung Kim", "Benjamin van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "On the theory of the Brownian motion",
      "author" : [ "George Uhlenbeck", "Leonard Ornstein." ],
      "venue" : "Physical Review, 36:823–841.",
      "citeRegEx" : "Uhlenbeck and Ornstein.,? 1930",
      "shortCiteRegEx" : "Uhlenbeck and Ornstein.",
      "year" : 1930
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR) 6.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo M. Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "Compositional demographic word embeddings",
      "author" : [ "Charles Welch", "Jonathan Kummerfeld", "Verónica PérezRosas", "Rada Mihalcea." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Welch et al\\.,? 2020a",
      "shortCiteRegEx" : "Welch et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the value of personalized word embeddings",
      "author" : [ "Charles Welch", "Jonathan Kummerfeld", "Verónica PérezRosas", "Rada Mihalcea." ],
      "venue" : "International Conference on Computational Linguistics (COLING) 28.",
      "citeRegEx" : "Welch et al\\.,? 2020b",
      "shortCiteRegEx" : "Welch et al\\.",
      "year" : 2020
    }, {
      "title" : "Does BERT make any sense? interpretable word sense disambiguation with contextualized embeddings",
      "author" : [ "Gregor Wiedemann", "Steffen Remus", "Avi Chawla", "Chris Biemann." ],
      "venue" : "arXiv 1909.10430.",
      "citeRegEx" : "Wiedemann et al\\.,? 2019",
      "shortCiteRegEx" : "Wiedemann et al\\.",
      "year" : 2019
    }, {
      "title" : "Individual comparisons by ranking methods",
      "author" : [ "Frank Wilcoxon." ],
      "venue" : "Biometrics Bulletin, 1(6):80–83.",
      "citeRegEx" : "Wilcoxon.,? 1945",
      "shortCiteRegEx" : "Wilcoxon.",
      "year" : 1945
    }, {
      "title" : "Toward socially-infused information extraction: Embedding authors, mentions, and entities",
      "author" : [ "Yi Yang", "Ming-Wei Chang", "Jacob Eisenstein." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Overcoming language variation in sentiment analysis with social attention",
      "author" : [ "Yi Yang", "Jacob Eisenstein." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:295–307.",
      "citeRegEx" : "Yang and Eisenstein.,? 2017",
      "shortCiteRegEx" : "Yang and Eisenstein.",
      "year" : 2017
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS) 33.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Employing personal word embeddings for personalized search",
      "author" : [ "Jing Yao", "Zhicheng Dou", "Ji-Rong Wen." ],
      "venue" : "International Conference on Research and Development in Information Retrieval (SIGIR)",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic word embeddings for evolving semantic discovery",
      "author" : [ "Zijun Yao", "Yifan Sun", "Weicong Ding", "Nikhil Rao", "Hui Xiong." ],
      "venue" : "International Conference on Web Search and Data Mining (WSDM)",
      "citeRegEx" : "Yao et al\\.,? 2018",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2018
    }, {
      "title" : "Biased random walk based social regularization for word embeddings",
      "author" : [ "Ziqian Zeng", "Xin Liu", "Yangqiu Song." ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI) 27.",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Socialized word embeddings",
      "author" : [ "Ziqian Zeng", "Yichun Yin", "Yangqiu Song", "Ming Zhang." ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI) 26.",
      "citeRegEx" : "Zeng et al\\.,? 2017",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Traditional methods such as LSA (Deerwester et al., 1990), word2vec (Mikolov et al.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 64,
      "context" : ", 2013a,b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : ", 2014), and fastText (Bojanowski et al., 2017) compute static word embeddings, i.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 66,
      "context" : "Contextualized word embeddings are widely used in NLP, constituting the semantic backbone of pretrained language models (PLMs) such as ELMo (Peters et al., 2018a), BERT (Devlin et al.",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : ", 2018a), BERT (Devlin et al., 2019), GPT-2 (Radford et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : ", 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "The distinction between the non-contextualized core meaning of a word and the senses that are realized in specific linguistic contexts lies at the heart of lexical-semantic scholarship (Geeraerts, 2010), going back to at least Paul (1880).",
      "startOffset" : 185,
      "endOffset" : 202
    }, {
      "referenceID" : 53,
      "context" : "In NLP, this is reflected by contextualized word embeddings that map type-level representations to token-level representations as a function of the linguistic context (McCann et al., 2017).",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 66,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 73,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 99,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 74,
      "context" : "As part of PLMs (Peters et al., 2018a; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Clark et al., 2020; Raffel et al., 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al.",
      "startOffset" : 16,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : ", 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2017).",
      "startOffset" : 181,
      "endOffset" : 281
    }, {
      "referenceID" : 64,
      "context" : ", 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2017).",
      "startOffset" : 181,
      "endOffset" : 281
    }, {
      "referenceID" : 4,
      "context" : ", 2020), contextualized word embeddings have led to substantial performance gains on a variety of tasks compared to static word embeddings that only have type-level representations (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014; Bojanowski et al., 2017).",
      "startOffset" : 181,
      "endOffset" : 281
    }, {
      "referenceID" : 18,
      "context" : "Regarding lexical semantics, this line of research has shown that contextualized word embeddings are more context-specific in the upper layers of a contextualizer (Ethayarajh, 2019; Mickus et al., 2020; Vulić et al., 2020) and represent different word senses as separated clusters (Peters et al.",
      "startOffset" : 163,
      "endOffset" : 222
    }, {
      "referenceID" : 55,
      "context" : "Regarding lexical semantics, this line of research has shown that contextualized word embeddings are more context-specific in the upper layers of a contextualizer (Ethayarajh, 2019; Mickus et al., 2020; Vulić et al., 2020) and represent different word senses as separated clusters (Peters et al.",
      "startOffset" : 163,
      "endOffset" : 222
    }, {
      "referenceID" : 92,
      "context" : "Regarding lexical semantics, this line of research has shown that contextualized word embeddings are more context-specific in the upper layers of a contextualizer (Ethayarajh, 2019; Mickus et al., 2020; Vulić et al., 2020) and represent different word senses as separated clusters (Peters et al.",
      "startOffset" : 163,
      "endOffset" : 222
    }, {
      "referenceID" : 66,
      "context" : ", 2020) and represent different word senses as separated clusters (Peters et al., 2018a; Coenen et al., 2019; Wiedemann et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : ", 2020) and represent different word senses as separated clusters (Peters et al., 2018a; Coenen et al., 2019; Wiedemann et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 133
    }, {
      "referenceID" : 95,
      "context" : ", 2020) and represent different word senses as separated clusters (Peters et al., 2018a; Coenen et al., 2019; Wiedemann et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "The meaning of a word can also vary across extralinguistic contexts such as time (Bybee, 2015; Koch, 2016) and social space (Robinson, 2010, 2012; Geeraerts, 2018).",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 41,
      "context" : "The meaning of a word can also vary across extralinguistic contexts such as time (Bybee, 2015; Koch, 2016) and social space (Robinson, 2010, 2012; Geeraerts, 2018).",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "The meaning of a word can also vary across extralinguistic contexts such as time (Bybee, 2015; Koch, 2016) and social space (Robinson, 2010, 2012; Geeraerts, 2018).",
      "startOffset" : 124,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al.",
      "startOffset" : 145,
      "endOffset" : 255
    }, {
      "referenceID" : 79,
      "context" : "To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al.",
      "startOffset" : 145,
      "endOffset" : 255
    }, {
      "referenceID" : 80,
      "context" : "To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al.",
      "startOffset" : 145,
      "endOffset" : 255
    }, {
      "referenceID" : 101,
      "context" : "To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al.",
      "startOffset" : 145,
      "endOffset" : 255
    }, {
      "referenceID" : 26,
      "context" : "To capture these phenomena, various types of dynamic word embeddings have been proposed: diachronic word embeddings for temporal semantic change (Bamler and Mandt, 2017; Rosenfeld and Erk, 2018; Rudolph and Blei, 2018; Yao et al., 2018; Gong et al., 2020) and personalized word embeddings for social semantic variation (Zeng et al.",
      "startOffset" : 145,
      "endOffset" : 255
    }, {
      "referenceID" : 62,
      "context" : ", 2020) and personalized word embeddings for social semantic variation (Zeng et al., 2017, 2018; Oba et al., 2019; Welch et al., 2020a,b; Yao et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 155
    }, {
      "referenceID" : 100,
      "context" : ", 2020) and personalized word embeddings for social semantic variation (Zeng et al., 2017, 2018; Oba et al., 2019; Welch et al., 2020a,b; Yao et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 155
    }, {
      "referenceID" : 35,
      "context" : "Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al.",
      "startOffset" : 114,
      "endOffset" : 160
    }, {
      "referenceID" : 49,
      "context" : "Other studies have demonstrated that performance on a diverse set of tasks can be increased by including temporal (Jaidka et al., 2018; Lukes and Søgaard, 2018) and social information (Amir et al.",
      "startOffset" : 114,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 28,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 97,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 98,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 31,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 60,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 46,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 61,
      "context" : ", 2018; Lukes and Søgaard, 2018) and social information (Amir et al., 2016; Hamilton et al., 2016a; Yang et al., 2016; Yang and Eisenstein, 2017; Hazarika et al., 2018; Mishra et al., 2018; del Tredici et al., 2019b; Li and Goldwasser, 2019; Mishra et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 262
    }, {
      "referenceID" : 44,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 84,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 87,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 16,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 81,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 2,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 70,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 72,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 82,
      "context" : "The relevance of dynamic (specifically diachronic) word embeddings is also reflected by the emergence of lexical semantic change detection as an established task in NLP (Kutuzov et al., 2018; Schlechtweg et al., 2018; Tahmasebi et al., 2018; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Asgari et al., 2020; Pömsl and Lyapin, 2020; Pražák et al., 2020; Schlechtweg and Schulte im Walde, 2020; Schlechtweg et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 425
    }, {
      "referenceID" : 39,
      "context" : "Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.",
      "startOffset" : 127,
      "endOffset" : 168
    }, {
      "referenceID" : 42,
      "context" : "Besides dynamic word embeddings, many studies on lexical semantic change detection use methods based on static word embeddings (Kim et al., 2014; Kulkarni et al., 2015), e.",
      "startOffset" : 127,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : ", the alignment of static word embedding spaces (Hamilton et al., 2016b).",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "However, such approaches come at the cost of modeling disadvantages (Bamler and Mandt, 2017).",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "Recently, a few studies have taken first steps in this direction by using genre information within a Bayesian model of semantic change (Frermann and Lapata, 2016; Perrone et al., 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 65,
      "context" : "Recently, a few studies have taken first steps in this direction by using genre information within a Bayesian model of semantic change (Frermann and Lapata, 2016; Perrone et al., 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 37,
      "context" : ", 2019) and including social variables in training diachronic word embeddings (Jawahar and Seddah, 2019).",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "It is interesting to notice that contextualized word embeddings so far have performed worse than non-contextualized word embeddings on the task of lexical semantic change detection (Kaiser et al., 2020; Schlechtweg et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 228
    }, {
      "referenceID" : 82,
      "context" : "It is interesting to notice that contextualized word embeddings so far have performed worse than non-contextualized word embeddings on the task of lexical semantic change detection (Kaiser et al., 2020; Schlechtweg et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 228
    }, {
      "referenceID" : 73,
      "context" : "(3)Some contextualizing language models such as GPT-2 (Radford et al., 2019) only operate on X.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "This two-stage structure follows work in cognitive science and linguistics that indicates that extralinguistic information is processed before linguistic information by human speakers (Hay et al., 2006).",
      "startOffset" : 184,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "We leverage a PLM for the function c, specifically BERT (Devlin et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "We initialize s̃i with node2vec (Grover and Leskovec, 2016) embeddings.",
      "startOffset" : 32,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "To model the temporal drift of the dynamic embeddings e ij , we follow previous work on dynamic word embeddings (Bamler and Mandt, 2017; Rudolph and Blei, 2018) and impose a random walk prior over o ij ,",
      "startOffset" : 112,
      "endOffset" : 160
    }, {
      "referenceID" : 80,
      "context" : "To model the temporal drift of the dynamic embeddings e ij , we follow previous work on dynamic word embeddings (Bamler and Mandt, 2017; Rudolph and Blei, 2018) and impose a random walk prior over o ij ,",
      "startOffset" : 112,
      "endOffset" : 160
    }, {
      "referenceID" : 90,
      "context" : "This type of Gaussian process is known as Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930) and is commonly used to model time series (Roberts et al.",
      "startOffset" : 69,
      "endOffset" : 99
    }, {
      "referenceID" : 75,
      "context" : "This type of Gaussian process is known as Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930) and is commonly used to model time series (Roberts et al., 2013).",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "It is common practice to set λa λw (Bamler and Mandt, 2017; Rudolph and Blei, 2018).",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 80,
      "context" : "It is common practice to set λa λw (Bamler and Mandt, 2017; Rudolph and Blei, 2018).",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 29,
      "context" : "1), taking into account the observation that the vocabulary core constitutes the best basis for dynamic word embeddings (Hamilton et al., 2016b).",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 88,
      "context" : "A dataset containing reviews covering the time period from 2000 to 2011 has been made publicly available (Tang et al., 2012).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "We fit dynamic contextualized word embeddings to all four datasets, using BERTBASE (uncased) as the contextualizer and masked language modeling as the training objective (Devlin et al., 2019), i.",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 96,
      "context" : "(13)Statistical significance is tested with a Wilcoxon signedrank test (Wilcoxon, 1945; Dror et al., 2018).",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "(13)Statistical significance is tested with a Wilcoxon signedrank test (Wilcoxon, 1945; Dror et al., 2018).",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 69,
      "context" : "In cases where x is split into several WordPiece tokens by BERT, we follow previous work (Pinter et al., 2020; Sia et al., 2020) and average the subword embeddings.",
      "startOffset" : 89,
      "endOffset" : 128
    }, {
      "referenceID" : 85,
      "context" : "In cases where x is split into several WordPiece tokens by BERT, we follow previous work (Pinter et al., 2020; Sia et al., 2020) and average the subword embeddings.",
      "startOffset" : 89,
      "endOffset" : 128
    }, {
      "referenceID" : 86,
      "context" : "Such shortterm semantic shifts, which have attracted growing interest in NLP recently (Stewart et al., 2017; del Tredici et al., 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 86,
      "endOffset" : 159
    }, {
      "referenceID" : 71,
      "context" : "Such shortterm semantic shifts, which have attracted growing interest in NLP recently (Stewart et al., 2017; del Tredici et al., 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 86,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : ", 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : ", 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 77,
      "context" : ", 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 5,
      "context" : ", 2019a; Powell and Sentz, 2020), can result in lasting semantic narrowing if speakers become reluctant to use the word outside of the more specialized sense (Anttila, 1989; Croft, 2000; Robinson, 2012; Bybee, 2015).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 92,
      "context" : "We average the first six layers of the contextualizer since they have been shown to contain the core of lexical and semantic information (Vulić et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 52,
      "context" : "On what paths do new semantic associations spread through the social network? In complex systems theory, there are two basic types of random motion on networks: random walks, which consist of a series of consecutive random steps, and random flights, where step lengths are drawn from the Lévy distribution (Masuda et al., 2017).",
      "startOffset" : 306,
      "endOffset" : 327
    }, {
      "referenceID" : 98,
      "context" : "As a second testbed, we apply dynamic contextualized word embeddings on a task for which social and temporal information is known to be important (Yang and Eisenstein, 2017): sentiment analysis.",
      "startOffset" : 146,
      "endOffset" : 173
    } ],
    "year" : 2021,
    "abstractText" : "Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability. We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets.",
    "creator" : "LaTeX with hyperref"
  }
}