{
  "name" : "2021.acl-long.308.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation",
    "authors" : [ "Fuli Luo", "Wei Wang", "Jiahao Liu", "Yijia Liu", "Bin Bi", "Songfang Huang", "Fei Huang" ],
    "emails" : [ "lfl259702@alibaba-inc.com", "hebian.ww@alibaba-inc.com", "glacier.ljh@alibaba-inc.com", "yanshan.lyj@alibaba-inc.com", "b.bi@alibaba-inc.com", "songfang.hsf@alibaba-inc.com", "f.huang@alibaba-inc.com", "luo.si@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3980–3994\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3980\nAs a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-theart Transformer variants on WMT14 Englishto-German and English-to-French translation datasets, with gains of up to 1∼2 BLEU. 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-lingual pre-trained models like mBERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) that target providing contextualized representations for the inputs across languages, have shown large poten-\n*Equal contribution. 1Code and model are available at https://github.\ncom/alibaba/AliceMind/tree/main/VECO\nchrome-extension://mbniclmhobmnbdlbpiphghaielnnpgdp/screenshot.html?id=screenshot_0.0007918474100581108 1/1\ntial on a variety of cross-lingual understanding and generation tasks.\nBehind the great success, two major factors play the role of aligning the contextual representations between languages: 1) build the shared vocabulary across languages through subword tokenization, which supports the simple extension of masked language modeling (MLM) from English corpus to multilingual corpus; 2) capture the alignment in parallel data via concatenating two sentences as input, called translation language modeling (TLM). However, both of these two mechanisms rely on the self-attention module (query=key/value) of the Transformer encoder to implicitly enhance the interdependence between languages, which may lead to few attention patterns across languages. Taking Figure 1 as an example, even though inputting a pair of parallel sentences, both models only attend to the English context to build the representation of English tokens, while ignoring the se-\nmantically related Chinese tokens. That is, the self-attention module captures little communication across languages, which is crucial for learning universal cross-lingual representations.\nBased on the above observation, we propose to plug a cross-attention module (query!=key/value) into the Transformer encoder and design a crossattention MLM task to explicitly capture the interdependence between languages. As illustrated in Figure 2 (c), the cross-attention module takes the representation of x as query and y as key/value (purple lines) to build the representations of x in the next layer, thus explicitly aligning the representations across languages (purple attention matrices). It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. Moreover, what distinguishes our work from pre-training an encoderdecoder model (Liu et al., 2020b) is that we also keep the good nature (i.e., bidirectional contextual modeling) of the original encoder by unplugging the cross-attention from the model to predicting the masked words (e.g., x2 and y3).\nFurthermore, when fine-tuning on various downstream tasks, we can choose either plug-in or plugout the cross-attention module on-demand, thus making it suitable for both cross-lingual language understanding (NLU) and generation tasks (NLG). For cross-lingual NLU tasks, if plugging the crossattention module out, we can adopt the same fine-\ntuning methods as an encoder-only model like XLM. However, we find that plugging the crossattention module in fine-tuning can better utilize the bilingual context to boost the performance. For cross-lingual NLG like machine translation (MT), the cross attention is already jointly pre-trained with the whole network. Therefore, the parameters of the decoder do not need to be re-adjusted substantially in the following tuning process, thus fundamentally solving the main drawback of utilizing pre-trained encoders like XLM for initializing encoder-decoder models.\nWe call our approach VECO for “Variable and Flexible Cross-lingual Pre-training”. We validate VECO on a variety of representative cross-lingual understanding and generation benchmarks. Regrading cross-lingual understanding tasks, we conduct experiments on the XTREME benchmark consisting of 9 cross-lingual tasks, including text classification, sequence labeling, question answering, and sentence retrieval. VECO ranks first at the XTREME leaderboard 2 at the submission deadline. Regrading cross-lingual generation tasks, we validate VECO on the widely used WMT14 EnglishGerman and English-French machine translation benchmarks. VECO obtains 44.5 and 31.7 BLEU scores, consistently outperforming existing crosslingual pre-training approaches and state-of-the-art Transformer variants by around 1∼2 BLEU.\n2https://sites.research.google/xtreme"
    }, {
      "heading" : "2 Pre-training of VECO",
      "text" : ""
    }, {
      "heading" : "2.1 Overview of VECO",
      "text" : "VECO extends from a multi-layer Transformer encoder and plugs a cross-attention module in each layer. Given a pair of input (x,y) and its corrupted version (x̂, ŷ) via randomly masking part of its tokens, the model builds two types of contextualized vector representation for each token:\n• One suit of contextual representations H, denoted as green blocks and yellow blocks in Figure 2 (c), are only build on self-attention module (i.e., unpluging the cross-attention module) in each layer.\n• Another suit of contextual representations S, denoted as mixed color blocks in Figure 2 (c), are build on both the self-attention and cross-attention modules 3.\nThe model is trained to predict the masked tokens via two corresponding representations, conditioning on both its own context and paired context, respectively. Take predicting the masked words in sequence x as an example, the training objective is the cross-entropy of the gold distribution and predicted distribution P (x|x̂) and P (x|ŷ, x̂) computed via the above two suits of contextual representations. Thus, the training objective of crossattention masked language modeling (CA-MLM) can be formulated as\nL(x,y) = − logP (x|x̂; θs)− logP (x|ŷ, x̂; θs, θc) − logP (y|ŷ; θs)− logP (y|x̂, ŷ; θs, θc) (1)\nwhere θs and θc are the parameters of self-attention and cross-attention modules."
    }, {
      "heading" : "2.2 Architecture",
      "text" : "The backbone network of VECO is composed of a stack ofN Transformer layers. Each layer has three modules: a required self-attention module, a plugand-play cross-attention module, and a required feed-forward linear module. Both self-attention and cross-attention modules are based on the multihead attention (Vaswani et al., 2017). An attention function can be described as mapping a query (Q) and a set of key-value (K-V) pairs to an output.\n3For simplicity of illustration, we only show the mixed representations S of x3 and y2 in Figure 2 (c).\nFor the self-attention module, all the queries, keys and values are the same representations from the previous layer. Specifically, for the l-th Transformer layer, the output of a self-attention head Asl is computed via:\nQ = Hl−1WQl (2) K = Hl−1WKl (3) V = Hl−1WVl (4)\nAsl = softmax( QKT√ dk )V (5)\nwhere Hl−1 are the previous layer’s outputs, WQl ,W K l ,W V l are the parameter matrices of selfattention modules. For the cross-attention module, the queries come from the previous layer, and the keys and values come from the last layer’s representations of paired input. Specifically, for the l-th layer, the output of a cross-attention head Acl is computed via:\nQ = Sl−1UQl (6) K = HLUKl (7) V = HLUVl (8)\nAcl = softmax( QKT√ dk )V (9)\nwhere Sl−1 are the previous layer’s outputs, UQl ,U K l ,U V l are the parameter matrices of crossattention modules. Finally, the output HL of the last layer is used to recover the masked tokens of x, conditioning on its own context.\nP (x|x̂) = softmax(f(HLx )) (10) P (y|ŷ) = softmax(f(HLy )) (11)\nwhere f is the feed-forward network that maps the output vectors into the dictionary. HLx and H L y are computed via Eq 2∼5 when H0x and H0y are the word embeddings of x and y, respectively.\nMeanwhile, SL, conditioning on the context of the paired sequence x̂ and ŷ, is used to predict the masked tokens of y.\nP (x|ŷ, x̂) = softmax(f(SLx )) (12) P (y|x̂, ŷ) = softmax(f(SLy )) (13)\nwhere SLx and S L y are computed via Eq 6∼9 with the corresponding word embeddings and HL.\nVECO Fine-tuning: Flexible for NLU and NLG tasks\n12\nNote that when optimizing the objectives based on Eq 12 and Eq 13, we apply a stop-gradients operation (Chen and He, 2020) to HL (i.e., HL is treated as a constant in this term). This operation can largely speed up the training by avoiding the backpropagation on a 2L-layer network. Moreover, it even stabilizes the training of deep postlayernorm Transformer, which requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (Liu et al., 2020a; Bachlechner et al., 2020)."
    }, {
      "heading" : "3 Fine-tuning VECO for Downstream Cross-lingual Understanding and Generation Tasks",
      "text" : "As Figure 3 illustrated, when fine-tuning on various downstream tasks, one advantage of VECO is its flexibility for initializing both the encoder-only Transformer for understanding tasks and encoderdecoder Transformer for generation tasks. Beyond it, we also explore a fine-tuning approach combined with the characteristics of VECO ."
    }, {
      "heading" : "3.1 VECO for Cross-lingual Understanding",
      "text" : "Due to the plug-and-play cross-attention module, we explore two fine-tuning approaches:\n• Plug-Out fine-tuning is to unplug the crossattention module from the pre-trained model. In other words, the architecture of the finetuned model is almost the same as mBERT or XLM. Specifically, the contextual representations from the last layer HLx is used to predict the label of input x.\n• Plug-In fine-tuning is to plug the crossattention module into the fine-tuned model, if\nthe bilingual or automatically translated training data y is available in the downstream task. Specifically, we concatenated the two representations [HLx : S L x ] to predict the label of x, [HLy : S L y ] to predict the label of y. 4."
    }, {
      "heading" : "3.2 VECO for Cross-lingual Generation",
      "text" : "For pre-trained encoders like XLM, it is not a trivial problem to incorporate them into the sequenceto-sequence architecture – the mainstream backbone model of generation tasks (Zhu et al., 2020). One of the drawbacks or challenges could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted along with the encoder in the following fine-tuning process (Ren et al., 2019).\nHowever, under the framework of VECO , the cross-attention is jointly pre-trained along with the whole network, making it easy to provide full initialization for sequence-to-sequence models. Specifically, the self-attention module is used to initialize both the corresponding modules in the encoder and decoder for contextual modeling, while the cross-attention module is used to initialize the encoder-to-decoder attention. It’s okay whether you continue to tie the self-attention parameters during fine-tuning. Directly pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b) could be another solution for NLG tasks, but we found mBART is not so effective in cross-lingual NLU tasks. We refer the reader to the Section 7 for detailed experiments and analysis.\n4Plug-In fine-tuning is not suitable for the zero-shot setting (also called cross-lingual transfer) due to the lack of bilingual or translated pair (x,y)"
    }, {
      "heading" : "4 Pre-training Setup",
      "text" : "Model Configuration We pre-train a 24-layer model with 1024 embedding/hidden size and 4096 feed-forward size. We do not use language embeddings to allow our model to better deal with downstream tasks of unseen languages. We adopt the same 250K vocabulary that is also used by XLM-R (Conneau et al., 2019). Table 1 shows the other details of baselines and VECO .\nPre-Training Data We collect monolingual and bilingual corpus covering 50 languages. For monolingual training datasets, we reconstruct CommonCrawl Corpus used in XLM-R (Conneau et al., 2019). We extract 1.36TB data in 50 languages, which contains 6.5G sentences and 0.4G documents. We up/down-sample the monolingual text like XLM from each language with a smoothing parameter α = 0.5. For bilingual data, we collect from the OPUS website 5 like previous works (Lample and Conneau, 2019; Chi et al., 2020b). There are 6.4G parallel sentences, covering 879 language pairs across 50 languages. See more statistics of training data in Appendix A.\nOptimization Settings For each iteration, we alternately sample a batch of adjacent segments from the monolingual corpus and a batch of parallel sentences from bilingual datasets to conduct a pair of masked input (x̂, ŷ). We adopt the translation language modeling (TLM) when the inputs are parallel bilingual sentences. Thus the overall training objective is the sum of TLM and the proposed CA-MLM objectives. During training, the model parameters except for cross-attention are initialized by XLM-R. We first freeze the parameters of XLM-R and only update the cross-attention parameters for faster convergence. Then, we jointly train the whole model. We pre-train our model with mixed-precision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details.\n5http://opus.nlpl.eu/"
    }, {
      "heading" : "5 Experiments on Cross-lingual Understanding Tasks",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets.\nFine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and directly performs inference on the test data of different target languages; (2) TranslateTrain-All fine-tunes a multilingual model on the concatenation of all data (golden training corpus in English and translated training corpus in other languages). Note that for two sequence-labeling tasks (POS, NER), the position of token labels in the translated text generally differs from that in the source text. Following FILTER (Fang et al., 2020), we use the model trained only on the English training dataset as a teacher, to label the translated text. To have a fair comparison with the strong baseline XLM-R (Conneau et al., 2019)\nunder the translate-train-all setting, we also show the results of XLM-R using the same fine-tuning hyperparameters as VECO ."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : "The detailed test results of nine tasks on the XTREME benchmark are shown in Table 2. It demonstrates that the proposed VECO outperforms previous cross-lingual models on all datasets. Compared to XLM-R, it averagely scores 5.0 and 6.6 points higher under the cross-lingual transfer and translation-train-all settings, respectively.\nIn the cross-lingual transfer setting, VECO delivers a large improvement compared to XLM-R, especially on zero-shot sentence retrieval tasks (BUCC, Tatoeba). This phenomenon reflects that our model can better build the interdependence between languages. Thus it can better mine parallel sentences in a multilingual corpus.\nUnder the translation-train-all setting, it can be observed that VECO with Plug-In fine-tuning (VECOin) is better than Plug-Out fine-tuning (VECOout). We conclude the reasons as two-fold. On the input side, the Plug-Out fine-tuning individually takes multilingual instances as input, while the Plug-In fine-tuning considers the bilingual instances 6 at each run. On the model side, the Plug-In fine-tuning can encourage correspondence across language via the cross-attention module. Note that the Plug-In fine-tuning method also outperforms FILTER (Fang et al., 2020), an enhanced cross-lingual fine-tuning method that also takes the\n6English instance with its translated one.\nbilingual instance as the input of XLM-R. It further demonstrates the effectiveness of VECO and its specialized fine-tuning method.\nWe conclude the reasons for the above performance improvement as two-fold: 1) the introduction of bilingual data during pre-training, which is a direct way to enhance the cross-lingual ability of the model; 2) Stronger ability to enhance the interdependence and fusion among languages via the proposed CA-MLM pre-training tasks. To analyze which plays a leading role, we conduct a set of more fair experiments in Section 7."
    }, {
      "heading" : "6 Experiments on Cross-lingual Generation Tasks",
      "text" : ""
    }, {
      "heading" : "6.1 Experimental Setup",
      "text" : "Datasets We choose the machine translation (MT) task, a typical cross-lingual generation scenario. In order to illustrate the generality of our approach and have a fair comparison with the most recent state-of-the-art Transformer work (Liu et al., 2020a), we choose two most widely used datasets: WMT14 English→German (En-De) and English→French (En-Fr) translation. WMT14 EnDe is a medium-resource dataset that provides 4.5M pairs for training and validation. We adopt standard newstest2014 as the test set. WMT14 En-Fr is a high-resource dataset that contains 36M pairs of parallel sentences. We use newstest2012+newstest2013 for validation and newstest2016 for test. We measure case-insensitive tokenized BLEU with multi-bleu.perl and de-\ntokenized SacreBLEU 7 to avoid the influence of different tokenization and normalization between models (Post, 2018).\nFine-tuning Setting We fine-tune our model using fairseq 8 toolkit and adopt comparable training settings with baselines. We run WMT 14 EnDe and En-Fr MT experiments on 16 and 32 V100 GPUs, respectively. The batch size is 64k for EnDe and 256k for En-Fr. The total training updates are set to 100k. The learning rate is 1e-4/2e-4, with linear warm-up over the first 16k steps and linear decay. We average the last 10 checkpoints and use beam search with a beam size of 5.\nBaselines We consider two types of Transformer baselines: randomly initialized and cross-lingual models initialized. For random initialization, we reproduce a Transformer baseline that adopts the same architecture and fine-tuning hyperparameters as VECO but with random initialization. Besides, we compare to the state-of-the-art Deep Transformer (Liu et al., 2020a). For cross-lingual encoder-decoder models, we include mBART (Liu et al., 2020b) and mRASP (Lin et al., 2020), which show impressive results on MT. Note that since we tied the self-attention weights of each encoder layer with each decoder layer, the whole parameters of mBART and VECO are comparable. We also conduct the WMT experiments for XLM-R, following the totally same fine-tuning settings as VECO , but leaving the encoder-to-decoder attention un-initialized.\n7Hash: BLEU+case.mixed+lang.en-{de,fr}+numrefs.1+ smooth.exp+test.wmt14/full+tok.13a+version.1.4.9\n8https://github.com/pytorch/fairseq"
    }, {
      "heading" : "6.2 Experimental Results",
      "text" : "Table 3 (left) shows the results on the machine translation. We can observe that VECO can largely outperform the randomly initialized same-sized Transformer baseline by 2.3 BLEU points. Moreover, it even beats the (randomly initialized) stateof-the-art Deep-Transformer (Liu et al., 2020a), which is three times deep as VECO . Among the cross-lingual models, VECO can consistently outperform the best models, averaged on two datasets, by 0.8 BLEU points.\nTable 3 (right) displays the BLEU scores of same-sized models during training. We find that VECO initialized model can get a surprising more than 28 SacreBLEU score just after 10 epochs, which is better than the final score of the randomly initialized model at 35 epochs. It reveals that VECO can provide a fairly good initialization for the machine translation model, which can converge quickly and further boost the results.\nOne might suspect that the main reason for the performance improvement is leveraging parallel corpus during pre-training. To figure it out, we conduct a more comparable experiment. We first train an out-of-domain Transformer model using the whole En-De parallel data (∼ 68M) used in VECO pre-training, and then continue to train the model on the in-domain WMT14 En-De training dataset. Results are shown in Table 3 (left) marked with *. Under this set of a totally fair comparison, VECO still maintains a lead of 1.1 BLEU score. This directly confirms that the improvement in MT is not only due to the use of bilingual data. More importantly, CA-MLM ensures better use of bilingual and large-scale unlabeled multilingual corpus."
    }, {
      "heading" : "6.3 Potential of Initializing Shallow Decoder",
      "text" : "Online translation applications usually have a restriction of inference time. The most direct way is to reduce the decoder layers since previous MT works (Liu et al., 2020a) have shown that deeper encoders are more worthwhile than deeper decoders. Based on this, we also explore the potential of the VECO to initialize deep encoder and shallow decoder Transformers, which is a blank in the crosslingual pre-training works.\nTable 4 contrasts two ways of initializing a Transformer with n decoder layers (n < 24) via selecting: (1) the first n layers; (2) the last n layers from a 24-layer pre-trained VECO model. We consider n = {3, 6} to conduct experiments. We find that selecting the last n layers exhibits better performance than selecting the first n layers. It reveals that the last several layers play a more important role in making predictions over the whole vocabulary. Moreover, we can find that there is 0.2∼0.3 BLEU gain when increasing the decoder layers from 3 to 6. However, we observe that only marginal improvement can be gained when further increasing the decoder layers from 6 to 24, which is also in line with the findings in Liu et al. (2020a). Regardless of the initialization method, the VECO initialized model can gain consistent 1∼2 BLEU improvement over the randomly initialized model."
    }, {
      "heading" : "7 Analysis and Ablation Study",
      "text" : "We perform an ablation study to investigate where the improvement in cross-lingual NLU and NLG tasks mainly comes from. Specifically, there are three main aspects we have studied:\n1. How much performance improvement comes from the parallel translation corpus used in pre-training?\n2. How effective of the CA-MLM pre-training\ntask, especially compared to the MLM and TLM pre-training tasks?\n3. How about pre-training a sequence-tosequence model like mBART for NLU and NLG tasks?\nTo figure out these questions, we train XLM, mBART and VECO model from scratch using the same datasets and parameter settings (see Appendix A for more details). All of them is pre-trained via MLM and TLM tasks. Note that the MLM task generally refers to predict the masked words of source language, while the TLM task generally refers to predict the words of the target language. Specifically for mBART that is under the framework of encoder-decoder, the input of encoder is masked sequence x̂, and the target of decoder is the masked words of source input x (for MLM task), or the parallel sentence y (for TLM task).\nTable 5 shows the results of two representative datasets of cross-lingual NLU and NLG. We can observe that, when using monolingual corpus only, VECO can outperform XLM by 0.8 points on the XNLI dataset and 0.3 BLEU scores on the IWSLT14 De-En translation dataset. It suggests that the CA-MLM can still benefit from adjacent sentences in monolingual corpus 9, to be equipped with a stronger ability of contextual modeling. Moreover, when pre-training both on the monolingual and bilingual corpus, VECO can even achieve a larger improvement compared to XLM, with 3.2 and 2.1 points improvement on two datasets, respectively. It reveals that CA-MLM objective of VECO can better utilize the bilingual corpus, compared to only optimized by TLM and MLM of XLM.\nMoreover, we find that pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b)\n9As noted in Section 4, we take two adjacent sentences in the monolingual corpus as (x,y).\nperforms worst on NLU tasks like XNLI 10, almost 6 points worse than VECO and near 2 points worse than XLM. One possible explanation could be that the unidirectional language modeling in the decoder might be sub-optimal for NLU tasks. And even on the machine translation task, mBART still performs worse than VECO when pre-training on the same bilingual datasets. We conclude that it is because that VECO can do better in the contextual modeling of source input x via a explicit masked language modeling objective in Eq 10 applied to x2 in Figure 2 (c)."
    }, {
      "heading" : "8 Related Work",
      "text" : "mBERT (Devlin et al., 2019) is a key step towards building a unified contextual language representation over multiple languages. It simply shares all languages’ vocabulary and trains a bidirectional Transformer encoder, achieving promising results in various cross-lingual NLU tasks. There have been several extensions that follow the same encoder-only backbone as mBERT. The main difference is the introduction of more training corpus (e.g., bilingual data) and pre-training tasks. XLM (Lample and Conneau, 2019) utilizes both monolingual and bilingual corpus to perform the masked language modeling. XLM-R (Conneau et al., 2019) extends to be built on RoBERTa (Liu et al., 2019) using larger monolingual training data. Other works (Huang et al., 2019; Yang et al., 2020; Chi et al., 2020b) propose new pre-training tasks to utilize the bilingual data better. However, there are two main drawbacks of these works. First, they mainly rely on the self-attention module in the Transformer encoder to implicitly build the interdependence between languages, leading to few attention patterns across languages due to the “lazy” network. Second, even though they show impressive performance improvement on cross-lingual understanding tasks like XNLI, only marginal improvement has been gained on cross-lingual generation tasks like machine translation, especially on high-resource languages.\nA feasible solution for cross-language generation is to pre-train a denoising auto-encoder like mBART (Liu et al., 2020b). It extends BART (Lewis et al., 2019) to the multilingual setting, demonstrating significant gains in low/medium-resource machine translation, but\n10We follow BART (Lewis et al., 2019) by utilizing the final representation from the decoder for classification tasks.\nwith a decrease in high resource languages. Unlike mBART, Chi et al. (2020a) first trains an encoder via MLM and then frozen the encoder to train the decoder only via two generative tasks. A similar approach is also proposed in Liang et al. (2020) and Lin et al. (2020), with the main difference in the joint training of encoder-decoder with code-switch tricks. However, all these cross-lingual models emphasize training a dedicated model for NLG. Thus they may hurt the NLU capabilities of the model. The ablation study in Section 7 also validates that it is sub-optimal to train an encoder-encoder network for NLU tasks.\nThis paper endeavors to build a unified crosslingual model for NLU and NLG tasks via a plugand-play cross-attention module. More importantly, the cross-attention module plays a role in the explicit alignment of encoded representations of different languages, thus largely contributing to building a unified cross-lingual model."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We present VECO, a variable and flexible crosslingual pre-training model, targets at explicitly capturing the interdependence between languages via a plug-and-play cross-attention module. Based on the flexible characteristics, VECO can initialize both NLU preferred encoder-only and NLG specialized encoder-decoder Transformer. Moreover, we also introduce a Plug-In fine-tuning approach to encourage the fusion between languages, combining the feature of VECO and cross-language downstream tasks.\nTaken together, VECO achieves consistent improvements on various language understanding and generation tasks, broadening the way of thinking about pre-trained backbone architecture and finetuning methods under the cross-lingual scenario."
    }, {
      "heading" : "A Pre-Training Details",
      "text" : "For monolingual data, following XLM-R (Conneau et al., 2019), we build a clean CommonCrawl Corpus using an open-source tool CCNet (Wenzek\net al., 2019). There are 1.36TB monolingual data in 50 languages before up/down-sampling. Table 6 reports the language codes and statistics of pretraining data. We collect bilingual corpus in 50 languages from the OPUS website11, including MultiUN, UNPC, Bombay, EU-bookshop, OpenSubtitles2018, Tanzil, GlobalVoices, ParaCrawl, MultiParaCrawl, DGT, Tilde, Europarl, Wikipedia, ECB, TED2013, News-Commentary, Ubuntu, Books, UN, infopankki-v1, EUconst, and Bianet. In total, there are 1TB bilingual training data before pre-processing, covering 879 language pairs. Table 7 lists the statistics for each language pair. We then apply subword tokenization directly on raw text data using Sentence Piece Model (Kudo and Richardson, 2018) without any additional preprocessing.\nWe use the whole corpus to train VECO and a subset (∼ 1/4) that contains 33 languages to train small-sized XLM, mBART and VECO . The full set of pre-training hyperparameters for smallsized and large-sized VECO (default) are listed in Table 8."
    }, {
      "heading" : "B More details about Illustrated Attention",
      "text" : "The models illustrated with attention patterns in Figure 1 of main paper (not appendix), are the base-sized XLM 12 and XLM-R 13. We show the attention scores averaged on all heads in the middle layer."
    }, {
      "heading" : "C Fine-Tuning Details on XTERME",
      "text" : "We select the model with the best average result over all the languages on the dev sets, by searching the learning rate over [5e-6,8e-6,1e-5,2e-5,3e-5] for the Cross-lingual Transfer setting and [5e-6,6e6,7e-6,8e-6,9e-6] for Translate-Train-All setting, training epoch over [3,5,10], and batch size over [16,32,64]."
    }, {
      "heading" : "D Detailed Results on XTREME",
      "text" : "The detailed results of each XTREME task under the cross-lingual transfer and translate-train-all settings on all languages are listed in the following tables.\n11http://opus.nlpl.eu/ 12https://huggingface.co/ xlm-mlm-tlm-xnli15-1024 13https://huggingface.co/ xlm-roberta-base\nModel en de es fr ja ko zh Avg.\nCross-lingual Transfer XLM-R 94.7 89.7 90.1 90.4 78.7 79.0 82.3 86.4 VECOout 96.2 91.3 91.4 92.0 81.8 82.9 85.1 88.7\nTranslate-Train-All VECOout 96.4 93.0 93.0 93.5 87.2 86.8 87.9 91.1 VECOin 96.5 94.4 94.3 94.0 89.0 90.3 91.0 92.8\nTable 10: PAWS-X accuracy scores.\nModel de fr ru zh Avg.\nCross-lingual Transfer XLM-R 67.5 66.5 73.5 56.7 66.0 VECOout 89.6 84.6 87.4 78.5 85.0\nTranslate-Train-All VECOout 93.0 88.7 89.9 85.7 89.3 VECOin 95.4 91.9 93.1 89.9 92.6\nTable 11: BUCC F1 results."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Rezero is all you need: Fast convergence at large depth",
      "author" : [ "Thomas Bachlechner", "Bodhisattwa Prasad Majumder", "Huanru Henry Mao", "Garrison W Cottrell", "Julian McAuley." ],
      "venue" : "arXiv preprint arXiv:2003.04887.",
      "citeRegEx" : "Bachlechner et al\\.,? 2020",
      "shortCiteRegEx" : "Bachlechner et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He." ],
      "venue" : "CoRR, abs/2011.10566.",
      "citeRegEx" : "Chen and He.,? 2020",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual natural language generation via pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Wenhui Wang", "XianLing Mao", "Heyan Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Chi et al\\.,? 2020a",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "arXiv",
      "citeRegEx" : "Chi et al\\.,? 2020b",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transac-",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of EMNLP 2018, pages 2475–2485.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "FILTER: An enhanced fusion method for cross-lingual language understanding",
      "author" : [ "Yuwei Fang", "Shuohang Wang", "Zhe Gan", "Siqi Sun", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:2009.05166.",
      "citeRegEx" : "Fang et al\\.,? 2020",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "arXiv preprint arXiv:2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks",
      "author" : [ "Haoyang Huang", "Yaobo Liang", "Nan Duan", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1909.00964.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "arXiv preprint arXiv:1808.06226.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "MLQA: Evaluating Cross-lingual Extractive Question Answering",
      "author" : [ "Patrick Lewis", "Barlas Oğuz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of ACL 2020.",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "XGLUE: A new benchmark dataset for cross-lingual pretraining, understanding and generation",
      "author" : [ "Yaobo Liang", "Nan Duan", "Yeyun Gong", "Ning Wu", "Fenfei Guo", "Weizhen Qi", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Guihong Cao" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretraining multilingual neural machine translation by leveraging alignment information",
      "author" : [ "Zehui Lin", "Xiao Pan", "Mingxuan Wang", "Xipeng Qiu", "Jiangtao Feng", "Hao Zhou", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Very deep transformers for neural machine translation",
      "author" : [ "Xiaodong Liu", "Kevin Duh", "Liyuan Liu", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2008.07772.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2001.08210.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal dependencies 2.2",
      "author" : [ "Joakim Nivre", "Mitchell Abrams", "Željko Agić", "Lars Ahrenberg", "Lene Antonsen", "Maria Jesus Aranzabe", "Gashaw Arutie", "Masayuki Asahara", "Luma Ateyah", "Mohammed Attia" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2018
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of ACL 2017, pages 1946–1958.",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "A call for clarity in reporting",
      "author" : [ "Matt Post" ],
      "venue" : null,
      "citeRegEx" : "Post.,? \\Q2018\\E",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Explicit cross-lingual pre-training",
      "author" : [ "Shuai Ma" ],
      "venue" : null,
      "citeRegEx" : "Ma.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ma.",
      "year" : 2019
    }, {
      "title" : "Evaluating the cross-lingual",
      "author" : [ "Karthik Raman" ],
      "venue" : null,
      "citeRegEx" : "Raman.,? \\Q2020\\E",
      "shortCiteRegEx" : "Raman.",
      "year" : 2020
    }, {
      "title" : "Attention is all",
      "author" : [ "Kaiser", "Illia Polosukhin" ],
      "venue" : null,
      "citeRegEx" : "Kaiser and Polosukhin.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kaiser and Polosukhin.",
      "year" : 2017
    }, {
      "title" : "Overview of the second BUCC shared",
      "author" : [ "Rapp" ],
      "venue" : null,
      "citeRegEx" : "2017.,? \\Q2017\\E",
      "shortCiteRegEx" : "2017.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Cross-lingual pre-trained models like mBERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : ", 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : ", 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) that target providing contextualized representations for the inputs across languages, have shown large poten-",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "Moreover, what distinguishes our work from pre-training an encoderdecoder model (Liu et al., 2020b) is that we also keep the good nature (i.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Note that when optimizing the objectives based on Eq 12 and Eq 13, we apply a stop-gradients operation (Chen and He, 2020) to HL (i.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "Moreover, it even stabilizes the training of deep postlayernorm Transformer, which requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (Liu et al., 2020a; Bachlechner et al., 2020).",
      "startOffset" : 195,
      "endOffset" : 240
    }, {
      "referenceID" : 2,
      "context" : "Moreover, it even stabilizes the training of deep postlayernorm Transformer, which requires non-trivial efforts regarding carefully designing learning rate schedulers and cutting-edge optimizers (Liu et al., 2020a; Bachlechner et al., 2020).",
      "startOffset" : 195,
      "endOffset" : 240
    }, {
      "referenceID" : 20,
      "context" : "Directly pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b) could be another solution for NLG tasks, but we found mBART is not so effective in cross-lingual NLU tasks.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "mBERT (Devlin et al., 2019) Encoder-only 110M 12 - 104 110k Wikipedia XLM (Lample and Conneau, 2019) Encoder-only 570M 24 - 100 200k Wikipedia XLM-R (Conneau et al.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : ", 2019) Encoder-only 110M 12 - 104 110k Wikipedia XLM (Lample and Conneau, 2019) Encoder-only 570M 24 - 100 200k Wikipedia XLM-R (Conneau et al.",
      "startOffset" : 54,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2019) Encoder-only 110M 12 - 104 110k Wikipedia XLM (Lample and Conneau, 2019) Encoder-only 570M 24 - 100 200k Wikipedia XLM-R (Conneau et al., 2019) Encoder-only 550M 24 - 100 250k CommonCrawl mRASP (Lin et al.",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : ", 2019) Encoder-only 550M 24 - 100 250k CommonCrawl mRASP (Lin et al., 2020) Encoder-decoder 375M 6 6 32 64k Translation MMTE (Siddhant et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : ", 2020) Encoder-decoder 375M 6 6 103 64k Translation mBART (Liu et al., 2020b) Encoder-decoder 680M 12 12 25 250k CommonCrawl",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "We adopt the same 250K vocabulary that is also used by XLM-R (Conneau et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "For monolingual training datasets, we reconstruct CommonCrawl Corpus used in XLM-R (Conneau et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "For bilingual data, we collect from the OPUS website 5 like previous works (Lample and Conneau, 2019; Chi et al., 2020b).",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "For bilingual data, we collect from the OPUS website 5 like previous works (Lample and Conneau, 2019; Chi et al., 2020b).",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : ", 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : ", 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : ", 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : ", 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : ", 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "Fine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and directly performs inference on the test data of different target languages; (2) TranslateTrain-All fine-tunes a multilingual model on the concatenation of all data (golden training corpus in English and translated training corpus in other languages).",
      "startOffset" : 45,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Fine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and directly performs inference on the test data of different target languages; (2) TranslateTrain-All fine-tunes a multilingual model on the concatenation of all data (golden training corpus in English and translated training corpus in other languages).",
      "startOffset" : 45,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Following FILTER (Fang et al., 2020), we use the model trained only on the English training dataset as a teacher, to label the translated text.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "To have a fair comparison with the strong baseline XLM-R (Conneau et al., 2019)",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Note that the Plug-In fine-tuning method also outperforms FILTER (Fang et al., 2020), an enhanced cross-lingual fine-tuning method that also takes the",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "In order to illustrate the generality of our approach and have a fair comparison with the most recent state-of-the-art Transformer work (Liu et al., 2020a), we choose two most widely used datasets: WMT14 English→German (En-De) and English→French (En-Fr) translation.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : "tokenized SacreBLEU 7 to avoid the influence of different tokenization and normalization between models (Post, 2018).",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "Besides, we compare to the state-of-the-art Deep Transformer (Liu et al., 2020a).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "For cross-lingual encoder-decoder models, we include mBART (Liu et al., 2020b) and mRASP (Lin et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : ", 2020b) and mRASP (Lin et al., 2020), which show impressive results on MT.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "Moreover, it even beats the (randomly initialized) stateof-the-art Deep-Transformer (Liu et al., 2020a), which is three times deep as VECO .",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "The most direct way is to reduce the decoder layers since previous MT works (Liu et al., 2020a) have shown that deeper encoders are more worthwhile than deeper decoders.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 20,
      "context" : "Moreover, we find that pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b)",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "mBERT (Devlin et al., 2019) is a key step towards building a unified contextual language representation over multiple languages.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "XLM (Lample and Conneau, 2019) utilizes both monolingual and bilingual corpus to perform the masked language modeling.",
      "startOffset" : 4,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "XLM-R (Conneau et al., 2019) extends to be built on RoBERTa (Liu et al.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : ", 2019) extends to be built on RoBERTa (Liu et al., 2019) using larger monolingual training data.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Other works (Huang et al., 2019; Yang et al., 2020; Chi et al., 2020b) propose new pre-training tasks to utilize the bilingual data better.",
      "startOffset" : 12,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "Other works (Huang et al., 2019; Yang et al., 2020; Chi et al., 2020b) propose new pre-training tasks to utilize the bilingual data better.",
      "startOffset" : 12,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "A feasible solution for cross-language generation is to pre-train a denoising auto-encoder like mBART (Liu et al., 2020b).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "It extends BART (Lewis et al., 2019) to the multilingual setting, demonstrating significant gains in low/medium-resource machine translation, but",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "We follow BART (Lewis et al., 2019) by utilizing the final representation from the decoder for classification tasks.",
      "startOffset" : 15,
      "endOffset" : 35
    } ],
    "year" : 2021,
    "abstractText" : "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-theart Transformer variants on WMT14 Englishto-German and English-to-French translation datasets, with gains of up to 1∼2 BLEU. 1",
    "creator" : "LaTeX with hyperref"
  }
}