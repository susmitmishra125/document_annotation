{
  "name" : "2021.acl-long.420.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Syntax-Enhanced Pre-trained Model",
    "authors" : [ "Zenan Xu", "Daya Guo", "Duyu Tang", "Qinliang Su", "Linjun Shou", "Ming Gong", "Wanjun Zhong", "Xiaojun Quan", "Daxin Jiang", "Nan Duan" ],
    "emails" : [ "zhongwj25}@mail2.sysu.edu.cn", "quanxj3}@mail.sysu.edu.cn", "dutang@microsoft.com", "lisho@microsoft.com", "migon@microsoft.com", "djiang@microsoft.com", "nanduan@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5412–5422\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5412\nSyntax-Enhanced Pre-trained Model Zenan Xu1∗ †, Daya Guo1∗, Duyu Tang2†, Qinliang Su1,4,5‡, Linjun Shou3,\nMing Gong3, Wanjun Zhong1∗, Xiaojun Quan1, Daxin Jiang3, and Nan Duan2 1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\n2Microsoft Research Asia, Beijing, China 3Microsoft Search Technology Center Asia, Beijing, China\n4Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China 5Key Lab. of Machine Intelligence and Advanced Computing, Ministry of Education, China\n{xuzn, guody5, zhongwj25}@mail2.sysu.edu.cn {suqliang, quanxj3}@mail.sysu.edu.cn\n{dutang,lisho,migon,djiang,nanduan}@microsoft.com Abstract\nWe study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018), and RoBERTa (Liu et al., 2019) have advanced the state-of-the-art performances of various natural language processing tasks. The successful recipe is that a model is first pre-trained on a huge volume of unsupervised\n∗ Work is done during internship at Microsoft. † For questions, please contact D. Tang and Z. Xu. ‡ Corresponding author.\n1The source data is available at https://github.com/HiZenanXu/Syntax-Enhanced Pre-trained Model.\ndata with self-supervised objectives, and then is fine-tuned on supervised data with the same data scheme. Dominant pre-trained models represent a text as a sequence of tokens2. The merits are that such basic text representations are available from vast amounts of unsupervised data, and that models pre-trained and fine-tuned with the same paradigm usually achieve good accuracy in practice (Guu et al., 2020). However, an evident limitation of these methods is that richer syntactic structure of text is ignored.\nIn this paper, we seek to enhance pre-trained models with syntax of text. Related studies attempt to inject syntax information either only in the finetuning stage (Nguyen et al., 2020; Sachan et al., 2020), or only in the pre-training stage (Wang et al., 2020), which results in discrepancies. When only fusing syntax information in the fine-tuning phase, Sachan et al. (2020) finds that there is no performance boost unless high quality human-annotated dependency parses are available. However, this requirement would limit the application of the model to broader scenarios where human-annotated dependency information is not available.\nTo address this, we conduct a large-scale study on injecting automatically produced syntax of text in both the pre-training and fine-tuning stages. We construct a pre-training dataset by applying an offthe-shelf dependency parser (Qi et al., 2020) to one billion sentences from common crawl news. With these data, we introduce a syntax-aware pretraining task, called dependency distance prediction, which predicts the syntactic distance between tokens in the dependency structure. Compared with the pre-training task of dependency head prediction (Wang et al., 2020) that only captures local syntactic relations among words, dependency distance prediction leverages global syntax of the text. In\n2Such tokens can be words or word pieces. We use token for clarity.\naddition, we developed a syntax-aware attention layer, which can be conveniently integrated into Transformer (Vaswani et al., 2017) to allow tokens to selectively attend to contextual tokens based on their syntactic distance in the dependency structure.\nWe conduct experiments on entity typing, question answering and relation classification on six benchmark datasets. Experimental results show that our method achieves state-of-the-art performance on all six datasets. Further analysis shows that our model can indicate the importance of syntactic information on downstream tasks, and that the newly introduced dependency distance prediction task could capture the global syntax of the text, performs better than dependency head prediction. In addition, compared with experimental results of injecting syntax information in either the pre-training or fine-tuning stage, injecting syntax information in both stages achieves the best performance.\nIn summary, the contribution of this paper is threefold. (1) We demonstrate that infusing automatically produced dependency structures into the pre-trained model shows superior performance over downstream tasks. (2) We propose a syntax-aware attention layer and a pre-training task for infusing syntactic information into the pre-trained model. (3) We find that the newly introduced dependency distance prediction task performs better than the dependency head prediction task."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work involves injecting syntax information into pre-trained models. First, we will review recent studies on analyzing the knowledge presented in pre-trained models, and then we will introduce the existing methods that enhance pre-trained models with syntax information."
    }, {
      "heading" : "2.1 Probing Pre-trained Models",
      "text" : "With the huge success of pre-trained models (Devlin et al., 2019; Radford et al., 2018) in a wide range of NLP tasks, lots of works study to what extent pre-trained models inherently. Here, we will introduce recent works on probing linguistic information, factual knowledge, and symbolic reasoning ability from pre-trained models respectively. In terms of linguistic information, Hewitt and Manning (2019) learn a linear transformation to predict the depth of each word on a syntax tree based on their representation, which indicates that the\nsyntax information is implicitly embedded in the BERT model. However, Yaushian et al. (2019) find that the attention scores calculated by pre-trained models seem to be inconsistent with human intuitions of hierarchical structures, and indicate that certain complex syntax information may not be naturally embedded in BERT. In terms of probing factual knowledge, Petroni et al. (2019) find that pretrained models are able to answer fact-filling cloze tests, which indicates that the pre-trained models have memorized factual knowledge. However, Poerner et al. (2019) argue that BERT’s outstanding performance of answering fact-filling cloze tests is partly due to the reasoning of the surface form of name patterns. In terms of symbolic reasoning, Talmor et al. (2020) test the pre-trained models on eight reasoning tasks and find that the models completely fail on half of the tasks. Although probing knowledge from pre-trained model is a worthwhile area, it runs perpendicular to infusing knowledge into pre-trained models."
    }, {
      "heading" : "2.2 Integrating Syntax into Pre-trained Models",
      "text" : "Recently, there has been growing interest in enhancing pre-trained models with syntax of text. Existing methods attempt to inject syntax information in the fine-tuning stage or only in the pre-training stage. We first introduce related works that inject syntax in the fine-tuning stage. Nguyen et al. (2020) incorporate a tree-structured attention into the Transformer framework to help encode syntax information in the fine-tuning stage. Zhang et al. (2020) utilize the syntax to guide the Transformer model to pay no attention to the dispensable words in the fine-tuning stage and improve the performance in machine reading comprehension. Sachan et al. (2020) investigate two distinct strategies for incorporating dependency structures in the fine-tuning stage and obtain state-of-the-art results on the semantic role labeling task. Meanwhile, Sachan et al. (2020) argue that the performance boost is mainly contributed to the high-quality human-annotated syntax. However, human annotation is costly and difficult to extend to a wide range of applications. Syntax information can also be injected in the pretraining stage. Wang et al. (2020) introduce head prediction tasks to inject syntax information into the pre-trained model, while syntax information is not provided during inference. Note that the head prediction task in Wang et al. (2020) only focuses\non the local relationship between two related tokens, which prevents each token from being able to perceive the information of the entire tree. Despite the success of utilizing syntax information, existing methods only consider the syntactic information of text in the pre-training or the fine-tuning stage so that they suffer from discrepancy between the pre-training and the fine-tuning stage. To bridge this gap, we conduct a large-scale study on injecting automatically produced syntax information in both the two stages. Compared with the head prediction task (Wang et al., 2020) that captures the local relationship, we introduce the dependency distance prediction task that leverages the global relationship to predict the distance of two given tokens."
    }, {
      "heading" : "3 Data Construction",
      "text" : "In this paper, we adopt the dependency tree to express the syntax information. Such a tree structure is concise and only expresses necessary information for the parse (Jurafsky, 2000). Meanwhile, its head-dependent relation can be viewed as an approximation to the semantic relationship between tokens, which is directly useful for capturing semantic information. The above advantages help our model make more effective use of syntax information. Another available type of syntax information is the constituency tree, which is used in Nguyen et al. (2020). However, as pointed out in Jurafsky (2000), the relationships between the tokens in dependency tree can directly reflect important syntax information, which is often buried in the more complex constituency trees. This property requires extra techniques to extracting relation among the words from a constituency tree (Jurafsky, 2000)3.\nThe dependency tree takes linguistic words as one of its basic units. However, most pre-trained models take subwords (also known as the word pieces) instead of the entire linguistic words as the input unit, and this necessitates us to extend the definition of the dependency tree to include subwords. Following Wang et al. (2020), we will add edges\n3https://web.stanford.edu/˜jurafsky/slp3/\nfrom the first subword of v to all subwords of u, if there exists a relationship between linguistic word v and word u.\nBased on the above extended definition, we build a pre-training dataset from open-domain sources. Specifically, we randomly collect 1B sentences from publicly released common crawl news datasets (Zellers et al., 2019) that contain English news articles crawled between December 2016 and March 2019. Considering its effectiveness and ability to expand to multiple languages, we adopt offthe-shelf Stanza4 to automatically generate the syntax information for each sentence. The average token length of each sentence is 25.34, and the average depth of syntax trees is 5.15."
    }, {
      "heading" : "4 Methodology",
      "text" : "In this section, we present the proposed SyntaxEnhanced PRE-trained Model (SEPREM). We first define the syntax distance between two tokens. Based on the syntax distance, we then introduce a syntax-aware attention layer to learn syntax-aware representation and a pre-training task to enable model to capture global syntactic relations among tokens."
    }, {
      "heading" : "4.1 Syntax Distance over Syntactic Tree",
      "text" : "Intuitively, the distance between two tokens on the syntactic tree may reflect the strength of their linguistic correlation. If two tokens are far away from each other on the syntactic tree, the strength of their linguistic correlation is likely weak. Thus, we define the distance of two tokens over the dependency tree as their syntactic distance. Specifically, we define the distance between the token v and token u as 1, i.e. d(v, u) = 1, if v is the head of u. If two tokens are not directly connected in the dependency graph, their distance is the summation of the distances between adjacent nodes on the path. If two tokens are separated in the graph, their distance is set to infinite. Taking the sentence “My dog is playing frisbee outside the room.” in Fig 1 as\n4https://github.com/stanfordnlp/stanza\nan example, d(playing, frisbee) equals 1 since the token “playing” is the head of the token “frisbee”."
    }, {
      "heading" : "4.2 Syntax-Aware Transformer",
      "text" : "We follow BERT (Devlin et al., 2019) and use the multi-layer bidirectional Transformer (Vaswani et al., 2017) as the model backbone. The model takes a sequence X as the input and applies N transformer layers to produce contextual representation:\nHn = transformern((1−α)Hn−1 +αĤn−1) (1) where n ∈ [1, N ] denotes the n-th layer of the model, Ĥ is the syntax-aware representation which will be described in Section 4.3, H0 is embeddings of the sequence input X , and α is a learnable variable.\nHowever, the introduction of syntax-aware representation Ĥ in the Equation 1 changes the architecture of Transformer, invalidating the original weights from pre-trained model, such as BERT and RoBERTa. Instead, we introduce a learnable importance score α that controls the proportion of integration between contextual and syntax-aware representation. When α is equal to zero, the syntaxaware representation is totally excluded and the model is architectural identical to vanilla Transformer. Therefore, we initialize the parameter α as the small but not zero value, which can help better fuse syntactic information into existing pre-trained models. We will discuss importance score α in detailed in Section 5.6.\nEach transformer layer transformern contains an architecturally identical transformer block, which is composed of a multi-headed self-attention MultiAttn (Vaswani et al., 2017) and a followed feed forward layer FFN . Formally, the output Hn of the transformer block transformern(H ′n−1) is computed as:\nG′n = LN(MultiAttn(H ′ n−1) +H ′ n−1) Hn = LN(FFN(G′n) +G ′ n) (2)\nwhere the input H ′n−1 is (1− α)Hn−1 + αĤn−1 and LN represents a layer normalization operation."
    }, {
      "heading" : "4.3 Syntax-aware Attention Layer",
      "text" : "In this section, we will introduce how to obtain the syntax-aware representation Ĥ used in syntaxaware transformer.\nTree Structure Encoding We adopt a distance matrix D to encode the tree structure. The advantages of distance matrix D are that it can well preserve the hierarchical syntactic structure of text and can directly reflect the distance of two given tokens. Meanwhile, its uniqueness property guarantees the one-to-one mapping of the tree structure. Given a dependency tree, the element Di,j of distance matrix D in i-th row and j-th column is defined as:\nDi,j =\n{ d(i, j), if exists a path from vi to vj ,\n0, if i = j and otherwise. (3)\nwhere vi and vj are tokens on the dependency tree. Based on the concept that distance is inversely proportional to importance, we normalize the matrix D and obtain the normalized correlation strength matrix D̃ as follows:\nD̃i,j =\n{ 1/Di,j∑\nz∈{y|Di,y 6=0} (1/Di,z)\n, if Di,j 6= 0,\n0, otherwise. (4)\nSyntax-aware Representation Given the tree structure representation D̃ and the contextual representation Hn, we fuse the tree structure into the contextual representation as:\nĤn = σ(W 1nH n +W 2nD̃H n) (5)\nwhere σ is the activation function, W 1n and W 2 n ∈ Rdh×dh are model parameters. We can see that D̃Hn allows one to aggregate information from others along the tree structure. The closer they are on the dependency tree, the larger the attention weight, and thus more information will be propagated to each other, and vice verse."
    }, {
      "heading" : "4.4 Syntax-aware Pre-training Task",
      "text" : "To better understand the sentences, it is beneficial for model to be aware of the underlying syntax. To this end, a new pre-training task, named dependency distance prediction task (DP), is designed to enhance the model’s ability of capturing global syntactic relations among tokens. Specifically, we first randomly mask some elements in the distance matrix D, e.g., supposed Di,j . Afterwards, the representations of tokens i and j from SEPREM are concatenated and fed into a linear classifier, which outputs the probabilities over difference distances. In all of our experiments, 15% of distance are masked at random.\nSimilar to BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), we conduct the following operations to boost the robustness. The distance in matrix D will be masked at 80% probability or replaced by a random integer with a probability of 10%. For the rest 10% probability, the distance will be maintained.\nDuring pre-training, in addition to the DP pretraining task, we also use the dependency head prediction (HP) task, which is used in Wang et al. (2020) to capture the local head relation among words, and the dynamic masked language model (MLM), which is used in Liu et al. (2019) to capture contextual information. The final loss for the pretraining is the summation of the training loss of DP, HP and MLM tasks."
    }, {
      "heading" : "4.5 Implementation Details",
      "text" : "The implementation of SEPREM is based on HuggingFace’s Transformer (Wolf et al., 2019). To accelerate the training process, we initialize parameters from RoBERTa model released by HuggingFace5, which contains 24 layers, with 1024 hidden states in each layer. The number of parameters of our model is 464M. We pre-train our model with 16 32G NVIDIA V100 GPUs for approximately two weeks. The batch size is set to 2048, and the total steps are 500000, of which 30000 is the warm up steps.\nIn both pre-training and fine-tuning stages, our model takes the syntax of the text as the additional input, which is pre-processed in advance. Specially, we obtain the dependency tree of each sentence via Stanza and then generate the normalized distance matrix."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we evaluate the proposed SEPREM on six benchmark datasets over three downstream tasks, i.e., entity typing, question answering and relation classification."
    }, {
      "heading" : "5.1 Entity Typing",
      "text" : "The entity typing task requires the model to predict the type of a given entity based on its context. Two fine-grained public datasets, Open Entity (Choi et al., 2018) and FIGER (Ling et al., 2015), are employed to evaluate our model. The statistics of the aforementioned datasets are shown in Table 1. Following Wang et al. (2020), special token\n5https://huggingface.co/transformers/\n“@” is added before and after a certain entity, then the representation of the first special token “@” is adopted to predict the type of the given entity. To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and F1 to evaluate model performance on Open Entity datasets. As for FIGER datasets, we utilize strict accuracy, loose macro-F1, and loose micro-F1 as evaluation metrics.\nBaselines NFGEC (Shimaoka et al., 2016) recursively composes representation of entity context and further incorporates an attention mechanism to capture fine-grained category memberships of an entity. KEPLER (Wang et al., 2019) infuses knowledge into the pre-trained models and jointly learns the knowledge embeddings and language representation. RoBERTa-large (continue training) learns on the proposed pre-training dataset under the same settings with SEPREM but only with dynamic MLM task. In addition, we also report the results of BERT-base (Devlin et al., 2019), ERNIE (Zhang et al., 2019), KnowBERT (Peters et al., 2019), WKLM (Xiong et al., 2020), RoBERTalarge, and K-adapter (Wang et al., 2020) for a full comparison.\nExperimental Results As we can see in Table 2, our SEPREM outperforms all other baselines on both entity typing datasets. In the Open Entity dataset, with the utility of the syntax of text, SEPREM achieves an improvement of 3.6% in micro-F1 score comparing with RoBERTa-large (continue training) model. The result demonstrates that the proposed syntax-aware pre-training tasks and syntax-aware attention layer help to capture the syntax of text, which is beneficial to predict the types more accurately. As for the FIGER dataset, which contains more labels about the type of entity, SEPREM still brings an improvement in strict accuracy, macro-F1, and micro-F1. This demonstrates\nthe effectiveness of leveraging syntactic information in tasks with more fine-grained information. Specifically, compared with the K-adapter model, our SEPREM model brings an improvement of 2.6% F1 score on Open Entity dataset. It is worth noting that SEPREM model is complementary to the K-adapter model, both of which inject syntactic information into model during pre-training stage. This improvement indicates that injecting syntactic information in both the pre-training and fine-tuning stages can make full use of the syntax of the text, thereby benefiting downstream tasks."
    }, {
      "heading" : "5.2 Question Answering",
      "text" : "We use open-domain question answering (QA) task and commonsense QA task to evaluate the proposed model. Open-domain QA requires models to answer open-domain questions with the help of external resources such as materials of collected documents and webpages. We use SearchQA (Dunn et al., 2017) and QuasarT (Dhingra et al., 2017) for this task, and adopt ExactMatch (EM) and loose F1 scores as evaluation metrics. In this task, we first retrieve related paragraphs according to the question from external materials via the information retrieval system, and then a\nreading comprehension technique is adopted to extract possible answers from the above retrieved paragraphs. Following previous work (Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017b) for the two datasets. For fair comparison, we follow Wang et al. (2020) to use [<sep>, quesiton,</sep>, paragraph,</sep>] as the input, where <sep> is a special token in front of two segmants and </sep> is a special symbol to split two kinds of data types. We take the task as a multi-classification to fine-tune the model and use two linear layers over the last hidden features from models to predict the start and end positions of the answer span.\nCommonsense QA aims to answer questions which require commonsense knowledge that is not explicitly expressed in the question. We use the public CosmosQA dataset (Huang et al., 2019) for this task, and the accuracy scores are used as evaluation metrics. The data statistics of the above three datasets are shown in Table 3. In CosmosQA, each question has 4 candidate answers, and we concatenate the question together with each answer separately as [<sep>, context,</sep>, paragraph,</sep>] for input. The representation of the first token is adopted to calculate a score for this answer, and the answer with the highest score is regarded as the prediction answer for this question.\nBaselines BiDAF (Seo et al., 2017) is a bidirectional attention network to obtain query-aware context representation. AQA (Buck et al., 2018) adopts a reinforce-guide questions re-write system and generates answers according to the re-written questions. Rˆ3 (Wang et al., 2017a) selects the most\nconfident paragraph with a designed reinforcement ranker. DSQA (Lin et al., 2018) employs a paragraph selector to remove paragraphs with noise and a paragraph reader to extract the correct answer from denoised paragraphs. Evidence Agg. (Wang et al., 2018) makes use of multiple passages to generate answers. BERT-FTRACE+SWAG (Huang et al., 2019) sequentially fine-tunes the BERT model on the RACE and SWAG datasets for knowledge transfer. Besides the aforementioned models, we also report the results of BERT (Xiong et al., 2020), WKLM (Xiong et al., 2020), WKLM + Ranking (Xiong et al., 2020), RoBERTa-large, RoBERTa-large (continue training), and K-Adapter (Wang et al., 2020) for a detailed comparison.\nExperimental Results The results of the opendomain QA task are shown in Table 4. We can see that the proposed SEPREM model brings significant gains of 3.1% and 8.4% in F1 scores, compared with RoBERTa-large (continue training) model. This may be partially attributed to the fact that, QA task requires a model to have reading comprehension ability (Wang et al., 2020), and\nthe introduced syntax information can guide the model to avoid concentrating on certain dispensable words and improve its reading comprehension capacity (Zhang et al., 2020). Meanwhile, SEPREM achieves state-of-the-art results on the CosmosQA dataset, which demonstrates the effectiveness of the proposed SEPREM model. It can be also seen that the performance gains observed in CosmosQA are not as substantial as those in the open-domain QA tasks. We speculate that CosmosQA requires capacity for contextual commonsense reasoning and the lack of explicitly injection of commonsense knowledge into SEPREM model limits its improvement."
    }, {
      "heading" : "5.3 Relation Classification",
      "text" : "A relation classification task aims to predict the relation between two given entities in a sentence. We use a large-scale relation classification dataset TACRED (Zhang et al., 2017) for this task, and adopt Micro-precision, recall, and F1 scores as evaluation metrics. The statistics of the TACRED datasets are shown in Table 1. Following Wang et al. (2020), we add special tokens “@” and “#” before and after the first and second entity respectively. Then, the representations of the former token “@” and “#” are concatenated to perform relation classification.\nBaselines C-GCN (Zhang et al., 2018) encodes the dependency tree via graph convolutional networks for relation classification. BERT+MTB (Baldini Soares et al., 2019) trains relation representation by matching the blanks. We also include the baseline models of BERT-base (Zhang et al., 2019), ERNIE (Zhang et al., 2019), BERT-large (Baldini Soares et al., 2019), KnowBERT (Peters et al., 2019), KEPLER (Wang et al., 2019), RoBERTa-\nlarge, RoBERTa-large (continue training), and KAdapter (Wang et al., 2020) for a comprehensive comparison.\nExperimental Results Table 5 shows the performances of baseline models and the proposed SEPREM on TACRED. As we can see that the proposed syntax-aware pre-training tasks and syntaxaware attention mechanism can continuously bring gains in relation classification task and SEPREM outperforms baseline models overall. This further confirms the outstanding generalization capacity of our proposed model. It can be also seen that compared with K-Adapter model, the performance gains of SEPREM model observed in the TACRED dataset are not as substantial as that in Open Entity dataset. This may be partially due to the fact that K-Adapter also injects factual knowledge into the model, which may help in identifying relationships."
    }, {
      "heading" : "5.4 Ablation Study",
      "text" : "To investigate the impacts of various components in SEPREM, experiments are conducted for en-\ntity typing, question answering and relation classification tasks under the different corresponding benchmarks, i .e., Open Entity, CosmosQA, and TACRED, respectively. Note that due to the timeconsuming issue of training the models on entire data, we randomly sample 10 million sentences from the whole data to build a small dataset in this ablation study.\nThe results are illustrated in Figure 2, in which we eliminate two syntax-aware pre-training tasks (i.e., HP and DP) and syntax-aware attention layer to evaluate their effectiveness. It can be seen that without using the syntax-aware attention layer, immediate performance degradation is observed, indicating that leveraging syntax-aware attention layer to learn syntax-aware representation could benefit the SEPREM. Another observation is that for all three experiments, eliminating DP pre-training task leads to worse empirical results. In other words, compared with existing method (i.e., head prediction task), the proposed dependency distance prediction task is more advantageous to various downstream tasks. This observation may be attributed\nto the fact that leveraging global syntactic correlation is more beneficial than considering local correlation. Moreover, significant performance gains can be obtained by simultaneously exploiting the two pre-training tasks and syntax-aware attention layer, which further confirms superiority of our pre-training architecture."
    }, {
      "heading" : "5.5 Case Study",
      "text" : "We conduct a case study to empirically explore the effectiveness of utilizing syntax information. In the case of relation classification task, we need to predict the relationship of two tokens in a sentence. As the three examples shown in Figure 3, SEPREM can capture the syntax information by the dependency tree and make correct predictions. However, without utilizing syntax information, RoBERTa fails to recognize the correct relationship. To give further insight of how syntax information affects prediction, we also take case 1 for detailed analysis. The extracted dependency tree captures the close correlation of “grew” and “Jersey”, which indicates that “New Jersey” is more likely to be a residence place. These results reflects that our model can better understand the global syntax relations among tokens by utilizing dependency tree."
    }, {
      "heading" : "5.6 Analysis of Importance Score α",
      "text" : "Under the syntax-enhanced pre-trained framework introduced here, the contextual representation (Hn) and syntax-aware representation (Ĥn) are jointly optimized to abstract semantic information from sentences. An interesting question concerns how much syntactic information should be leveraged for our pre-trained model. In this regard, we further investigate the effect of the importance score α on the aforementioned six downstream tasks, and the learned weights α after fine-tuning SEPREM model are shown in Table 6. We observe that the values of α are in the range of 13% and 15% on six downstream datasets, which indicates that those downstream tasks require syntactic information to obtain the best performance and once again confirms the effectiveness of utilizing syntax information.\nTo have a further insight of the effect brought by importance score α, we conduct experiments on SEPREM w/o α, which eliminates the α in Equation 1 and equally integrates the syntaxaware and contextual representation, i.e., Hn = transformern(H\nn−1+Ĥn−1). The pre-training settings of the SEPREM w/o α model are the same\nwith the proposed SEPREM model. It can be seen in Table 6 that, the performances drop 1%∼3% on the six datasets when excluding the α. This observation indicates the necessity of introducing the α to better integrate the syntax-aware and contextual representation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we present SEPREM that leverage syntax information to enhance pre-trained models. To inject syntactic information, we introduce a syntax-aware attention layer and a newly designed pre-training task are proposed. Experimental results show that our method achieves state-of-theart performance over six datasets. Further analysis shows that the proposed dependency distance prediction task performs better than dependency head prediction task."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to Yeyun Gong, Ruize Wang and Junjie Huang for fruitful comments. We are obliged to Zijing Ou and Wenxuan Li for perfecting this article. We appreciate Genifer Zhao for beautifying the figures of this article. Zenan Xu and Qinliang Su are supported by the National Natural Science Foundation of China (No. 61806223, 61906217, U1811264), Key R&D Program of Guangdong Province (No. 2018B010107005), National Natural Science Foundation of Guangdong Province (No. 2021A1515012299). Zenan Xu and Qinliang Su are also supported by Huawei MindSpore."
    } ],
    "references" : [ {
      "title" : "Matching the Blanks: Distributional Similarity for Relation Learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "ACL, pages 2895–2905.",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
      "author" : [ "Christian Buck", "Jannis Bulian", "Massimiliano Ciaramita", "Andrea Gesmundo", "Neil Houlsby", "Wojciech Gajewski", "Wei Wang." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Buck et al\\.,? 2018",
      "shortCiteRegEx" : "Buck et al\\.",
      "year" : 2018
    }, {
      "title" : "Ultra-fine entity typing",
      "author" : [ "Eunsol Choi", "Omer Levy", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "ACL, pages 87–96.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Quasar: Datasets for question answering by search and reading",
      "author" : [ "Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:1707.03904.",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Güney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "ArXiv preprint arXiv:1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:2002.08909.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
      "author" : [ "Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "EMNLP, pages 2391–2401.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Speech & language processing",
      "author" : [ "Dan Jurafsky." ],
      "venue" : "Pearson Education India.",
      "citeRegEx" : "Jurafsky.,? 2000",
      "shortCiteRegEx" : "Jurafsky.",
      "year" : 2000
    }, {
      "title" : "Denoising distantly supervised open-domain question answering",
      "author" : [ "Yankai Lin", "Haozhe Ji", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "ACL, pages 1736–1745.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Design challenges for entity linking",
      "author" : [ "Xiao Ling", "Sameer Singh", "Daniel S. Weld." ],
      "venue" : "TACL, 3:315– 328.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Tree-structured attention with hierarchical accumulation",
      "author" : [ "Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge enhanced contextual word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "IV Logan", "L Robert", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A Smith." ],
      "venue" : "EMNLP, pages 43–",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models as knowledge bases? ArXiv, abs/1909.01066",
      "author" : [ "F. Petroni", "Tim Rocktäschel", "Patrick Lewis", "A. Bakhtin", "Y. Wu", "Alexander H. Miller", "S. Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert is not a knowledge base (yet): Factual knowledge vs",
      "author" : [ "Nina Poerner", "Ulli Waltinger", "Hinrich Schütze." ],
      "venue" : "name-based reasoning in unsupervised qa. ArXiv, abs/1911.03681.",
      "citeRegEx" : "Poerner et al\\.,? 2019",
      "shortCiteRegEx" : "Poerner et al\\.",
      "year" : 2019
    }, {
      "title" : "Stanza: A Python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Do syntax trees help pretrained transformers extract information? arXiv preprint arXiv:2008.09084",
      "author" : [ "Devendra Singh Sachan", "Yuhao Zhang", "Peng Qi", "William Hamilton" ],
      "venue" : null,
      "citeRegEx" : "Sachan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2020
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "An attentive neural architecture for fine-grained entity type classification",
      "author" : [ "Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 5th Workshop on Automated Knowledge Base Construction(AKBC), pages 69–",
      "citeRegEx" : "Shimaoka et al\\.,? 2016",
      "shortCiteRegEx" : "Shimaoka et al\\.",
      "year" : 2016
    }, {
      "title" : "olmpics-on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743–758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Cuihong Cao", "Daxin Jiang", "Ming Zhou" ],
      "venue" : "arXiv preprint arXiv:2002.01808",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reinforced reader-ranker for open-domain question answering",
      "author" : [ "Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerald Tesauro", "Bowen Zhou", "Jing Jiang." ],
      "venue" : "arXiv preprint arXiv:1709.00023.",
      "citeRegEx" : "Wang et al\\.,? 2017a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Evidence aggregation for answer re-ranking in opendomain question answering",
      "author" : [ "Shuohang Wang", "Mo Yu", "Jing Jiang", "Wei Zhang", "Xiaoxiao Guo", "Shiyu Chang", "Zhiguo Wang", "Tim Klinger", "Gerald Tesauro", "Murray Campbell." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "ACL, pages 189–198.",
      "citeRegEx" : "Wang et al\\.,? 2017b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Kepler: A unified model for knowledge embedding and pretrained language representation",
      "author" : [ "Xiaozhi Wang", "Tianyu Gao", "Zhaocheng Zhu", "Zhiyuan Liu", "Juanzi Li", "Jian Tang." ],
      "venue" : "arXiv preprint arXiv:1911.06136.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Huggingface’s transformers: Stateof-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
      "author" : [ "Wenhan Xiong", "Jingfei Du", "William Yang Wang", "Veselin Stoyanov." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Xiong et al\\.,? 2020",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitree transformer: Integrating tree structures into self-attention",
      "author" : [ "Wang Yaushian", "Lee Hung-Yi", "Chen Yun-Nung." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Yaushian et al\\.,? 2019",
      "shortCiteRegEx" : "Yaushian et al\\.",
      "year" : 2019
    }, {
      "title" : "Defending against neural fake news",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi." ],
      "venue" : "H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction",
      "author" : [ "Yuhao Zhang", "Peng Qi", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 2205–2215.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Positionaware Attention and Supervised Data Improve Slot Filling",
      "author" : [ "Yuhao Zhang", "Victor Zhong", "Danqi Chen", "Gabor Angeli", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 35–45.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "ERNIE: Enhanced Language Representation with Informative Entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "ACL, pages 1441–1451.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Sg-net: Syntax-guided machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Junru Zhou", "Sufeng Duan." ],
      "venue" : "Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-2020).",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Pre-trained models such as BERT (Devlin et al., 2019), GPT (Radford et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : ", 2019), GPT (Radford et al., 2018), and RoBERTa (Liu et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : ", 2018), and RoBERTa (Liu et al., 2019) have advanced the state-of-the-art performances of various natural language processing tasks.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "The merits are that such basic text representations are available from vast amounts of unsupervised data, and that models pre-trained and fine-tuned with the same paradigm usually achieve good accuracy in practice (Guu et al., 2020).",
      "startOffset" : 214,
      "endOffset" : 232
    }, {
      "referenceID" : 13,
      "context" : "Related studies attempt to inject syntax information either only in the finetuning stage (Nguyen et al., 2020; Sachan et al., 2020), or only in the pre-training stage (Wang et al.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "Related studies attempt to inject syntax information either only in the finetuning stage (Nguyen et al., 2020; Sachan et al., 2020), or only in the pre-training stage (Wang et al.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : ", 2020), or only in the pre-training stage (Wang et al., 2020), which results in discrepancies.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "We construct a pre-training dataset by applying an offthe-shelf dependency parser (Qi et al., 2020) to one billion sentences from common crawl news.",
      "startOffset" : 82,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "Compared with the pre-training task of dependency head prediction (Wang et al., 2020) that only captures local syntactic relations among words, dependency distance prediction leverages global syntax of the text.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "5413 addition, we developed a syntax-aware attention layer, which can be conveniently integrated into Transformer (Vaswani et al., 2017) to allow tokens to selectively attend to contextual tokens based on their syntactic distance in the dependency structure.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "With the huge success of pre-trained models (Devlin et al., 2019; Radford et al., 2018) in a wide range of NLP tasks, lots of works study to what extent pre-trained models inherently.",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "With the huge success of pre-trained models (Devlin et al., 2019; Radford et al., 2018) in a wide range of NLP tasks, lots of works study to what extent pre-trained models inherently.",
      "startOffset" : 44,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "Compared with the head prediction task (Wang et al., 2020) that captures the local relationship, we introduce the dependency distance prediction task that leverages the global relationship to predict the distance of two given tokens.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "This property requires extra techniques to extracting relation among the words from a constituency tree (Jurafsky, 2000)3.",
      "startOffset" : 104,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : "Specifically, we randomly collect 1B sentences from publicly released common crawl news datasets (Zellers et al., 2019) that contain English news articles crawled between December 2016 and March 2019.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "We follow BERT (Devlin et al., 2019) and use the multi-layer bidirectional Transformer (Vaswani et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and use the multi-layer bidirectional Transformer (Vaswani et al., 2017) as the model backbone.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "Each transformer layer transformern contains an architecturally identical transformer block, which is composed of a multi-headed self-attention MultiAttn (Vaswani et al., 2017) and a followed feed forward layer FFN .",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "5416 Similar to BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019), we conduct the following operations to boost the robustness.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : "The implementation of SEPREM is based on HuggingFace’s Transformer (Wolf et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Two fine-grained public datasets, Open Entity (Choi et al., 2018) and FIGER (Ling et al.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : ", 2018) and FIGER (Ling et al., 2015), are employed to evaluate our model.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and",
      "startOffset" : 63,
      "endOffset" : 166
    }, {
      "referenceID" : 35,
      "context" : "To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and",
      "startOffset" : 63,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and",
      "startOffset" : 63,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and",
      "startOffset" : 63,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020), we adopt loose micro precision, recall, and",
      "startOffset" : 63,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "Baselines NFGEC (Shimaoka et al., 2016) recursively composes representation of entity context and further incorporates an attention mechanism to capture fine-grained category memberships of",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 28,
      "context" : "KEPLER (Wang et al., 2019) infuses knowledge into the pre-trained models and jointly learns the knowledge embeddings and language representation.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "In addition, we also report the results of BERT-base (Devlin et al., 2019), ERNIE (Zhang et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : ", 2019), ERNIE (Zhang et al., 2019), KnowBERT (Peters et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : ", 2019), KnowBERT (Peters et al., 2019), WKLM (Xiong et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : ", 2019), WKLM (Xiong et al., 2020), RoBERTalarge, and K-adapter (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : ", 2020), RoBERTalarge, and K-adapter (Wang et al., 2020) for a full comparison.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "We use SearchQA (Dunn et al., 2017) and QuasarT (Dhingra et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : ", 2017) and QuasarT (Dhingra et al., 2017) for this task, and adopt ExactMatch (EM) and loose F1 scores as evaluation metrics.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Following previous work (Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "We use the public CosmosQA dataset (Huang et al., 2019) for this task, and the accuracy scores are used as evaluation metrics.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "Baselines BiDAF (Seo et al., 2017) is a bidirectional attention network to obtain query-aware context representation.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "AQA (Buck et al., 2018) adopts a reinforce-guide questions re-write system and generates answers according to the re-written questions.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "DSQA (Lin et al., 2018) employs a paragraph selector to remove paragraphs with noise",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 26,
      "context" : "(Wang et al., 2018) makes use of multiple passages to generate answers.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "BERT-FTRACE+SWAG (Huang et al., 2019) sequentially fine-tunes the BERT model on the RACE and SWAG datasets for knowledge transfer.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 30,
      "context" : "Besides the aforementioned models, we also report the results of BERT (Xiong et al., 2020), WKLM (Xiong et al.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 30,
      "context" : ", 2020), WKLM (Xiong et al., 2020), WKLM + Ranking (Xiong et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : ", 2020), WKLM + Ranking (Xiong et al., 2020), RoBERTa-large, RoBERTa-large (continue training), and K-Adapter (Wang et al.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : ", 2020), RoBERTa-large, RoBERTa-large (continue training), and K-Adapter (Wang et al., 2020) for a detailed comparison.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "This may be partially attributed to the fact that, QA task requires a model to have reading comprehension ability (Wang et al., 2020), and the introduced syntax information can guide the model to avoid concentrating on certain dispensable words and improve its reading comprehension capacity (Zhang et al.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : ", 2020), and the introduced syntax information can guide the model to avoid concentrating on certain dispensable words and improve its reading comprehension capacity (Zhang et al., 2020).",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 34,
      "context" : "We use a large-scale relation classification dataset TACRED (Zhang et al., 2017) for this task, and adopt Micro-precision, recall, and F1 scores as evaluation metrics.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 33,
      "context" : "Baselines C-GCN (Zhang et al., 2018) encodes the dependency tree via graph convolutional networks for relation classification.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 35,
      "context" : "We also include the baseline models of BERT-base (Zhang et al., 2019), ERNIE (Zhang et al.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : ", 2019), ERNIE (Zhang et al., 2019), BERT-large (Baldini Soares et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : ", 2019), KnowBERT (Peters et al., 2019), KEPLER (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "large, RoBERTa-large (continue training), and KAdapter (Wang et al., 2020) for a comprehensive comparison.",
      "startOffset" : 55,
      "endOffset" : 74
    } ],
    "year" : 2021,
    "abstractText" : "and the 11th International Joint Conference on Natural Language Processing, pages 5412–5422 August 1–6, 2021. ©2021 Association for Computational Linguistics 5412 Syntax-Enhanced Pre-trained Model Zenan Xu1∗ †, Daya Guo1∗, Duyu Tang2†, Qinliang Su1,4,5‡, Linjun Shou, Ming Gong, Wanjun Zhong1∗, Xiaojun Quan, Daxin Jiang, and Nan Duan School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China Microsoft Research Asia, Beijing, China Microsoft Search Technology Center Asia, Beijing, China Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China Key Lab. of Machine Intelligence and Advanced Computing, Ministry of Education, China {xuzn, guody5, zhongwj25}@mail2.sysu.edu.cn {suqliang, quanxj3}@mail.sysu.edu.cn {dutang,lisho,migon,djiang,nanduan}@microsoft.com Abstract",
    "creator" : "LaTeX with hyperref package"
  }
}