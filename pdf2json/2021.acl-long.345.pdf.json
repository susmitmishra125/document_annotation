{
  "name" : "2021.acl-long.345.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP",
    "authors" : [ "Anthony Chen", "Pallavi Gudipati", "Shayne Longpre", "Xiao Ling", "Sameer Singh" ],
    "emails" : [ "sameer}@uci.edu", "xiaoling}@apple.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4472–4485\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4472"
    }, {
      "heading" : "1 Introduction",
      "text" : "Substantial progress in NLP has been made on “closed” tasks, where queries are paired with relevant documents (Rajpurkar et al., 2016; Dua et al., 2019). However, there is growing interest in “opendomain” tasks, where relevant documents need to be retrieved from a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021). The open-domain setting better reflects real-world usage for tasks where relevant information is generally not provided (e.g., fact checking).\n∗Work started during an internship at Apple. 1The AmbER sets used in this paper and the code to generate them are available at https://github.com/ anthonywchen/AmbER-Sets.\nBecause success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b).\nA crucial challenge when interacting with a large knowledge source (e.g., Wikipedia) is entity ambiguity, the phenomenon where a single name can map to multiple entities. Resolving this ambiguity is referred to as entity disambiguation and is an important step for effective retrieval. For example, given the query “What musical instrument does Abe Lincoln play?”, documents about the musician should rank higher than other entities with the same name (Figure 1). Although entity disambiguation has been extensively studied in entity linking (Hoffart et al., 2011; Rao et al., 2013; Sevgili et al.,\n2For example, replacing the BM25 retriever with DPR on Natural Questions increases exact match by 15 points.\n2020) and search (Balog et al., 2010, 2011), in the context of open-domain NLP, it is unclear how good retrieval systems are when faced with queries with ambiguous entities. Evaluating entity ambiguity is challenging because the popularity of entities follows a long-tail (Figure 2) and rare entities are seldom covered in naturally-occurring datasets.\nIn this paper we introduce AmbER sets, a benchmark for evaluating the entity disambiguation capabilities of retrievers across multiple NLP tasks. Each AmbER set is a collection of Wikidata entities that share a name, and their corresponding queries for specific NLP tasks. For each set, we define the head entity as the most popular entity and tail entities as the less popular ones. By creating queries for multiple entities that share a name, AmbER sets provide an accurate test of entity disambiguation capabilities of retrievers and help assess the role of entity popularity in disambiguation. We show examples of AmbER sets for the question answering task in Table 1. We automatically create AmbER sets by mining the Wikidata knowledge graph (Vrandecic and Krötzsch, 2014) for relevant names and entities, and leveraging task-specific templates to generate inputs for three tasks: fact checking, slot filling, and question answering (Figure 3). In total, our AmbER sets contain 80k task-specific queries which we align to the Wikipedia snapshot from KILT (Petroni et al., 2021).\nWe use AmbER sets to conduct a systematic study of various retrieval systems that operate under different principles, such as token overlap and dense embedding similarity. Retrievers perform very differently on AmbER sets in terms of absolute retrieval numbers, with Bootleg (Orr et al., 2020), an entity-linking-based retriever, performing best. Despite these differences, all retrievers exhibit a large degree of popularity bias, underperforming on inputs concerning tail entities. TFIDF, a token-based retriever, performs about four times worse on tail entity inputs compared to head entity inputs. Even with Bootleg, the best performing retriever, performance on tail entities is still 1.5 times lower than on head entities. Our results on AmbER sets demonstrate that there is significant work to be done on making retrievers robust in handling entity disambiguation."
    }, {
      "heading" : "2 AmbER Sets",
      "text" : "Retrieving relevant documents from large knowledge sources such as Wikipedia is an important\nfirst step in the open-domain pipeline. An inherent problem in working with such sources is entity disambiguation: resolving a name (mention) to an entity in the knowledge source. Entity disambiguation can be challenging because many entities share a name, and the popularity of entities follows a long-tail distribution (Figure 2). Despite the importance of entity disambiguation, it remains an understudied problem for open-domain NLP. We introduce AmbER sets for evaluating entity disambiguation capabilities of retrievers and analyze the role of entity popularity in disambiguation."
    }, {
      "heading" : "2.1 What is an AmbER Set?",
      "text" : "We first provide an intuition for an AmbER set before concretely defining one. Consider two entities, a president and a musician, both of which have the name “Abe Lincoln” (Figure 1). Now, consider the query “Which battle did Abe Lincoln fight in?” and assume a retriever correctly returns the article about the president for this query. Simply because the correct document was retrieved does not mean a retriever has the ability to disambiguate between the president and the musician, as the president is much more popular. We should only be confident in its ability to disambiguate entities if we also pose a query about the less popular musician and the retriever again returns the correct document (as opposed to the document about the president).\nBased on this intuition, we define an AmbER set as a collection of queries that satisfy the following: • Criteria 1: Polysemous Name: The queries in\nan AmbER set are all about entities that share a common name (e.g., Abe Lincoln).\n• Criteria 2: Disparity in Popularity: An AmbER set contains queries about both the most popular entity for a name (the head entity), e.g., the president, and the less popular entities (the tail entities), e.g., the musician.\n• Criteria 3: Resolvable Ambiguity: The content of the query should be sufficient to resolve to the correct entity. The query “Which battle did Abe Lincoln fight in?” satisfies this criteria, because there is only one Abe Lincoln that fought in a war, while “Where was Abe Lincoln born?” does not since it applies to all Abe Lincolns.\nWe provide examples of AmbER sets for the task of question answering in Table 1."
    }, {
      "heading" : "2.2 Open-Domain Tasks",
      "text" : "In this work, we create AmbER sets for three tasks: fact checking, slot filling, and question answering (Table 2). We consider these three tasks for three reasons. First, these three set of tasks are diverse in nature. In this work, slot filling is a generation task, question answering is a span selection task, and fact checking is a classification task. Second, the training sets available for each task are quite disparate. The largest fact checking training set, FEVER (Thorne et al., 2018), has 80k instances, while the slot filling dataset, T-REx (Elsahar et al., 2018), has over 2 million instances. The final reason we study these three tasks is that their inputs are short and easy to create."
    }, {
      "heading" : "3 Creating AmbER Sets",
      "text" : "While AmbER sets can be manually created, doing so can be time-consuming, requiring a human to manually scour a knowledge base for polysemous\nnames and related entities before manually writing queries for those entities. Instead, we present a pipeline for automatically creating AmbER sets using the Wikidata knowledge graph (Vrandecic and Krötzsch, 2014). In this section, we describe two different collections of AmbER sets, and discuss our automatic pipeline for creating AmbER sets."
    }, {
      "heading" : "3.1 Two Collections of AmbER Sets",
      "text" : "A natural question is “How do retrievers handle entity ambiguity when two entities have the same entity type as opposed when they have different types?”. To answer this question, we create two collections of AmbER sets. The first is AmbERH, a collection of AmbER sets where all entities are humans. The choice to restrict AmbER-H to humans is motivated by the fact that humans have properties that help distinguish themselves from other humans, generally based on occupation. The second is AmbER-N, a collection of AmbER sets where all entities contained are non-humans, and disambiguation of a name is between non-human entities with different entity types. This is because a non-human entity, like a movie, does not generally have a single distinguishing property to distinguish from other movies. This makes it natural to compare non-human entities to other non-human entities with different types. We specify the entity types in each collection in Table 3."
    }, {
      "heading" : "3.2 Automatic Creation of AmbER Sets",
      "text" : "We now describe a pipeline to automatically create AmbER sets for three tasks: fact checking, slot filling, and question answering. We provide a visualization of the pipeline in Figure 3.\nCollecting Names and Entities We begin by collecting all entity aliases3 in Wikidata. From these aliases, we filter for those that are shared by multiple Wikidata entities. Each entity in Wikidata is represented by a unique QID. The entities must have an entity type from Table 3 depending on the collection we are collecting AmbER sets for. Each alias and associated entities form the basis for an AmbER set. Within each set, we define the head and tail entities based on the number of Wikipedia page views for the month of October 2019. We filter out AmbER sets where the percentage gap in popularity between the head entity and the most popular tail entity is less than 10% to account for noise in the monthly page views.\nCollecting Distinguishing Properties We gather properties and associated values for each entity from Wikidata. We only retain properties that are in a specified list (Table 3), as they are useful for resolving ambiguity (Criteria 3). We also filter a property if two entities within an AmbER set have that property, ensuring that the remaining properties can be used to disambiguate between entities with the same name. These properties are used to instantiate the queries.\nAligning Entities to Wikipedia We use the KILT Wikipedia snapshot (Petroni et al., 2021) as\n3Aliases are all possible names for an entity.\nthe knowledge source for AmbER sets for better reproducibility. Each Wikipedia document in KILT has an associated QID. For each entity, we find all Wikipedia documents with that associated QID. After this alignment, we apply a round of filtering on the tuples. For each tuple, we check that the value of the tuple is within the first 350 tokens of the aligned Wikipedia article. If not, we remove\nthe tuple.4 Aligned Wikipedia articles that contain the tuple value serve as gold documents.\nInstantiating AmbER Instances Recall that our goal was to create AmbER sets for three tasks: fact checking, slot filling, and question answering. We are able to create queries for all three tasks simultaneously using the collected Wikidata tuples. For question answering and fact checking, we use templates based on properties to instantiate inputs. Three of the authors wrote a template each for each property for the two tasks. Duplicate templates are removed, resulting in an average of 3 question answering templates per property and 2.7 fact checking templates per property. See Appendix B for the complete list of templates.\nFor slot filling, we create a single input from each Wikidata tuple by concatenating the AmbER set name with the property name, and using the value of the tuple as the answer. For question answering, we also create a single input for each tuple by filling in the template with the AmbER set name and using the value of the tuple as the answer. For fact checking, we create two inputs for each tuple, one claim that is true using the tuple value and one claim that is false. The false claim is created by finding the most popular value for the tuple property that does not match the tuple value5."
    }, {
      "heading" : "3.3 Dataset Statistics",
      "text" : "We provide statistics for AmbER sets in Table 4. On average, each AmbER set has about three entities that share the same name. Of these three entities, on average, only two have properties after filtering. In total, our AmbER sets contain about 80k task-specific input queries.\n4This reduces the number of tuples for AmbER-H from 17,079 to 5,942 and for AmbER-N from 22,219 to 13,804.\n5 The most popular instrument in Wikidata is piano. Therefore, given the true claim “Abe Lincoln played the trombone.”, the false claim would be “Abe Lincoln played the piano.”."
    }, {
      "heading" : "3.4 Limitations",
      "text" : "Since our pipeline is automated and relies on Wikipedia and Wikidata, there are a few limitations worth noting. AmbER sets will be affected by incompleteness of the knowledge source, sometimes resulting ambiguous queries if a property is missing from Wikidata, but answerable from Wikipedia text. For this reason, we only select a few properties for each type (Table 3). Second, even though we author multiple templates for each property, the reliance on these templates limits the syntactic diversity in the queries (not a critical concern, since we are only evaluating existing models). Also, we use Wikipedia page views as a proxy for real-world popularity of entities. Defining popularity in this way may be problematic, as page views for an entity can fluctuate, and may make our pipeline difficult to generalize to other knowledge sources, where this information may not be available.\nSeveral design choices in creating AmbER sets are worth further investigation. We limit AmbER sets to a pre-specified list of entity types and properties to ensure that entities in an AmbER set are distinguishable. This precludes other properties that may be useful in distinguishing entities, reducing the diversity in AmbER sets. Another design choice is we allow any alias in Wikidata to form an AmbER sets, however, not all aliases are canonical ways to refer to the entity. For instance, Shaquille O’Neal has the unusual alias “The Big Cactus”, potentially leading to a somewhat unrealistic query “What sport did The Big Cactus play?”. We plan to revisit the these design choices in future work."
    }, {
      "heading" : "4 Evaluation Setup",
      "text" : "Retrieval Systems The primary focus of this work is to evaluate entity ambiguity of retrieval systems. We consider four retrievers based on different retrieval paradigms. The first three are TF-IDF, a token-based retriever using sparse embeddings, DPR (Karpukhin et al., 2020), a dense embedding based retriever, and BLINK (Wu et al., 2020), a linker-based retriever which ranks documents based on input entities. These three retrievers have been thoroughly evaluated on a number of open-domain tasks in Petroni et al. (2021) with no obvious winner across tasks. Encouraged by the disambiguation success on rare entities by Orr et al. (2020), we also evaluate a retriever based on Bootleg, another entity linker. We provide additional details about these retrievers in Appendix D.\nDownstream Models The dominant approach to open-domain tasks is a two-stage process where a retriever first finds relevant documents, followed by a downstream model that processes these documents to produce an answer. We evaluate the end-to-end performance on AmbER sets by training downstream NLP models on our tasks of interest. For fact checking, we fine-tune a BERT classifier (Devlin et al., 2019) on FEVER (Thorne et al., 2018). For question answering, we fine-tune a RoBERTa model (Liu et al., 2019) on Natural Questions (Kwiatkowski et al., 2019). For slot filling, a generation task, we fine-tune a BART model (Lewis et al., 2020a) on T-Rex (Elsahar et al., 2018). We provide example training instances in Table 2 and additional details on the models in Appendix E. We use the AllenNLP and HuggingFace Transformers library to finetune our downstream models (Gardner et al., 2018; Wolf et al., 2020)."
    }, {
      "heading" : "5 Results",
      "text" : "In this section, we evaluate existing open-domain NLP pipelines using AmbER sets. We also conduct\na user study to evaluate the quality of the queries in the AmbER sets.\nTop Document Retrieval We report retrieval performance in Table 5 in terms of retriever accuracy@1 (the % of instances where the first retrieved document is the gold document). For each task, we report values on the entire AmbER set (“All”), as well as instances corresponding only to “Head” entities or to “Tail” entities. We also report a metric we call all correct (∀), the fraction\nof AmbER sets in which all queries had the correct document retrieved. All retrievers do better on head entities compared to tail entities. Since BLINK, Bootleg, and DPR are initialized using pre-trained language models, they may have a predisposition towards being biased to more popular entities. However, we find TF-IDF also does better on head entities, perhaps because more popular entities have longer Wikipedia pages, possibly increasing term-frequency scores. Second, there are large discrepancies between a retriever’s performance on different tasks for an AmbER collection. For instance, DPR does substantially worse on slot filling compared to its performance on question answering. This is surprising since queries for all tasks are created from the same set of Wikidata tuples. Finally, we find that retrievers are mostly incorrect on getting all the queries in a set correct, with some receiving a ∀ score of 0 on some tasks. Overall, we find that the Bootleg retriever on average does the best across tasks, however there is significant scope for improvement.\nEntity Confusion To explicitly evaluate whether retrievers get confused by entities in the same AmbER set, we compute entity confusion for retrievers defined as the percentage of queries where the retriever ranks a document for an incorrect entity from the same AmbER set over the gold document (Table 6). We find that across retrievers, tasks, and AmbER collections, entity confusion is twice as high for tail entity inputs. This result indicates that the popularity of an entity for a given name plays a significant role in retrieval performance.\nEffect of Popularity Gap Since the difference in popularity between the head and tail entities can vary considerably, these results obfuscate the effect of the size of the popularity gap. We explore how the gap in popularity between head and tail entities translates to the gaps in performance on their associated queries. For a head entity with popularity ph and a tail entity with popularity pt from the same AmbER set, we calculate popularity gap, ph−pt pt\n, and bin associated head/tail inputs based on the gap6. For each bin, we calculate the difference in accuracy@1 between the head and tail entity queries. Results for QA AmbER sets (Figure 4) show that there is a strong correlation between the popularity gap and the difference in performance.\nEnd to End Results We evaluate end to end performance in several evaluation settings with all results provided in Table 7. The metrics used are F1 for slot filling and question answering and accuracy for fact checking. In the “oracle” setting, we directly provide the downstream NLP model the gold document, and find that the gap between head entities and tail entities is fairly small. This suggests that in closed NLP settings, where the gold document is known, entity disambiguation is not a major concern.\nIn the regular retrieval setting, we provide the model the top 20 documents as ranked by a retrieval system (BLINK and DPR), and find that retrievers still perform better on head entity queries (see Appendix A). The downstream systems that use retrieved documents display a noticeable gap in end-to-end performance between head and tail entity inputs. This is expected, as retrieval systems perform worse on tail entities.\nUser Study AmbER sets are created in a largely automatic process, raising questions about data quality. To address these questions, we conduct a small user study on AmbER sets to evaluate whether the queries are resolvable by humans. We present a query from a QA AmbER set along with three documents for the entities from the same AmbER set, one of which is the gold document. We first ask the user to select the relevant document, then we ask the user to select an answer span from the selected document. In total, we asked 7 subjects to examine about 120 queries across AmbERH and AmbER-N, and computed their accuracy in\n6Bin width of 20%. Queries with a popularity gap higher than 100% are binned into the highest bin.\nselecting the correct document and answer (Table 8). We also compare retrievers for this task, i.e. select from 3 documents for the same queries, and find that humans perform very well on the document selection task compared to retrievers on both sets. We also compare the accuracy of answer selection, and see that the closed domain NLP model (fine-tuned BERT) is as almost accurate as humans on the same set of queries7. This further confirms that closed NLP models are not the source of bias towards head entities, but the retrievers are."
    }, {
      "heading" : "6 Related Work",
      "text" : "Entity Ambiguity As previously mentioned, entity ambiguity is when a single name can match multiple entities in a knowledge source. Entity ambiguity has been most studied in the context of entity linking (Rao et al., 2013). To improve disambiguation, entity linkers have included auxiliary information such as entity types (Onoe and Durrett, 2020) and entity descriptions (Logeswaran et al., 2019). A recent thread of work aims to study how language models recall and leverage information about names and entities. Prabhakaran et al. (2019) shows that names can have a measurable effect on the prediction of sentiment analysis systems. Shwartz et al. (2020) demonstrates that pre-trained language models implicitly resolve entity ambiguity by grounding names to entities based on the pretraining corpus. The problem of entity ambiguity also appears implicitly in entity-centric tasks such as determining the semantic relatedness between entities (Hoffart et al., 2012) and entity-oriented\n7The relatively low answer score is due to artifacts in using EM for QA evaluation, and is consistent with human performance on span selection (Rajpurkar et al., 2016)).\nsearch (Balog et al., 2010, 2011). We draw inspiration from these works by studying entity ambiguity in the context of open-domain NLP.\nPopularity Bias System’s that perform worse on the long-tail suffer from what is known as popularity bias. This problem has been studied extensively in the recommendation systems literature, where recommendation systems are known to often ignore the long-tail of products and instead recommend very popular items (Abdollahpouri et al., 2017; Chen et al., 2020). This has the effect of unfairly hurting users who would prefer these less-popular items (Abdollahpouri et al., 2019; Ciampaglia et al., 2018). We explore popularity bias from the angle of retrieval as opposed to recommendation, and find popularity bias exists in retrieval systems.\nOpen-Domain Ambiguity Ambiguity is an inherent problem when it comes to open-domain reasoning. Min et al. (2020) showed that half of instances sampled from Natural Questions are ambiguous, with multiple correct answers. AmbER sets are similar in that the ambiguity is in terms of the entity in the query, however, in contrast to Natural Questions, AmbER set inputs have been constructed such that the ambiguity is resolvable.\nChallenge Sets There have been many evaluation sets specifically designed to assess a model’s ability to handle a specific phenomenon (Naik et al., 2018; Zhao et al., 2018; McCoy et al., 2019; Warstadt et al., 2020; Richardson et al., 2020; Jeretic et al., 2020; Ribeiro et al., 2019). Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020). AmbER sets can be viewed as a challenge set for assessing opendomain systems’ ability to handle entity ambiguity."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Entity ambiguity is an inherent problem in retrieval, as many entities can share a name. For evaluating disambiguation capabilities of retrievers, we introduce AmbER sets; an AmbER set is a collection of task-specific queries about entities that share a name, but the queries have sufficient content to resolve the correct entity. We create a broad range of AmbER sets, covering many entity types, with input queries for three open-domain NLP tasks: fact checking, slot filling, and question answering. Our experiments demonstrate the struggles of current\nretrievers in handling entity ambiguity. In particular, we find that the popularity of an entity in relation to other entities that share a name plays a significant role during disambiguation. For instance, we find that all tested retrievers are about twice as likely to retrieve erroneous documents when dealing with less popular entities than the most popular entity with the same name. Future goals include improving entity disambiguation capabilities of retrievers, perhaps more directly incorporating ideas from entity linking and coreference resolution. The AmbER sets and the code for the generation pipeline is available at https: //github.com/anthonywchen/AmbER-Sets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Jo Daiber, Michael Tu, Russ Webb, Matt Gardner, Robert Logan, Sherry Tongshuang Wu, and the anonymous reviewers for providing valuable feedback for our work. This work is funded in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research."
    }, {
      "heading" : "A Top-20 Retrieval Results",
      "text" : "We provide results for top-20 retrieval in Table 9. Top-20 retrieval is used for providing documents in the end-to-end evaluation setting. In this setting, retrieval accuracy measures whether a gold document appears in one of the top-20 retrieved documents. Similar to top-1 retrieval, retrievers continue to perform better on head queries."
    }, {
      "heading" : "B Task Specific Templates",
      "text" : "Table 10 contains the templates used to instantiate the task-specific inputs. Templates were written on a per-property basis. We note that many of the properties share templates that are very similar."
    }, {
      "heading" : "C Computational Resources",
      "text" : "All experiments (e.g., training baselines, generating AmbER sets, etc.) were conducted on a machine with 500 GB of RAM, 64 CPUs, and using an NVIDIA TitanRTX with 24 GB of RAM. Retrieval on a collection of AmbER sets takes about 12 hours for the most time-consuming retriever, BLINK. Training a downstream model takes roughly 5 hours and inference on a collection of AmbER sets takes less than 30 minutes."
    }, {
      "heading" : "D Retriever Details",
      "text" : "For BLINK, DPR, and TF-IDF, we use the retriever code in the KILT repository released by Facebook8. For Bootleg, we use the code provided by the Hazy Research group9."
    }, {
      "heading" : "E Downstream Model Details",
      "text" : "For question answering, we train a RoBERTa-Large model on Natural Questions. We use the negative documents in Natural Questions to train a “noanswer” classifier using the [CLS] token. During inference, we take the highest-scoring span where the answer is not classified as “no-answer”. For slot filling, we train a BART-base model. For each slot filling instance, we train with the top non-gold document retrieved by TF-IDF as a negative document. For this negative document, we train the model to generate a “none” token, and during inference, we take the highest scoring answer that is\n8https://github.com/facebookresearch/ KILT\n9https://github.com/HazyResearch/ bootleg\nnot “none”. For fact checking, we train a three-way (i.e., SUPPORTS, REFUTES, NEUTRAL) BERTbase classifier. Similar to slot filling, we train with the top non-gold document retrieved by TF-IDF as a negative document and train the model to classify this negative document as NEUTRAL. During inference, we take the highest scoring prediction that is not NEUTRAL. When training baselines models, we do not tune over hyperparameters and train with a batch size of 32 for 3 epochs.\nProperty Question Answering Template Fact Checking Template A m bE R -H instrument Which musical instrument did $name play? What musical instrument does $name play? What instrument does $name play? $name plays the $object. $name plays the musical instrument $object. The $object is played by $name. movement What movement did $name participate in? Which movement is $name associated with? What movement is $name associated with? $name was a member of the $object movement. $name participated in the $object movement. $name was a part of the $object movement. appears in What works does the fictional entity $name appear in? What work is the character $name present in? Which work was the character $name in? $name is a character in $object. $name is a fictional character in $object. $object features the fictional character $name. doctoral student Who were the doctoral students of $name? Who are $name’s doctoral students? Who did $name advise? $name has a doctoral student named $object. $name’s doctoral student is $object. $name advised their student $object. military branch What branch of the military does $name belong to? Which military branch does $name belong to? What military branch is $name affiliated with? $name is a member of the $object. $name belongs to the military branch $object. $name belongs to the $object branch of the military.\nsports position What is the position that $name plays? What position does $name play? Which position does $name play?\n$name plays the $object position. $name plays as a $object.\nsports team $name plays for which team? What team does $name play for? Which team does $name play for?\n$name is a player on the $object. $name plays for the $object team. $name plays for the $object.\nbattles or wars What were the wars that $name participated in? Which battle did $name fight in? Which war did $name fight? $name fought in the $object. $name fought in $object.\nsport Which sport does $name participate in? Which sport does $name play? What sport does $name play?\n$name plays $object. $name plays the sport $object.\nA m\nbE R\n-N\nperformer Who performs $name? Who is the performer of $name? Who performed $name?\n$object performs in $name. $object is the performer of $name . $name was performed by $object.\nrecord label What is the record label of $name.? What is the record label for $name? $name belongs to which record label?\n$object is the record label for $name. $name’s record label is $object.\ntracklist What song appears in the album $name? What song appears on $name? What are the tracks in $name?\n$name belongs to $object tracklist. $object is on the release of $name . $object is a song in the $name tracklist.\nindustry Which industry is $name in? In what industry is $name? What is $name’s industry?\n$name is in the industry of $object. The company $name is in the $object industry. $name’s industry is $object.\npopulation What is the total population of $name? What is the population of $name? How many people live in $name?\nThe population of $name is $object. $name’s population is $object. $name has a population of $object.\ncast member Who acted in $name? Who is a cast member on $name? Who starred in $name?\n$object was a cast member in $name. $object appeared in $name. $object acted in $name.\nscreenwriter Who was the screenwriter for $name? Who was screenwriter for $name? Who is $name’s screenwriter?\n$name’s screenwriter is $object. $object wrote the screenplay of $name. $object screenwrote $name.\n# seasons How many seasons are there in $name? How many seasons does $name have? How many seasons were there in $name?\nThere were $object seasons in $name. $name has $object seasons."
    } ],
    "references" : [ {
      "title" : "Controlling popularity bias in learning-to-rank recommendation",
      "author" : [ "Himan Abdollahpouri", "Robin Burke", "Bamshad Mobasher." ],
      "venue" : "Proceedings of the Eleventh ACM Conference on Recommender Systems, RecSys 2017, Como, Italy, August 27-31,",
      "citeRegEx" : "Abdollahpouri et al\\.,? 2017",
      "shortCiteRegEx" : "Abdollahpouri et al\\.",
      "year" : 2017
    }, {
      "title" : "The unfairness of popularity bias in recommendation",
      "author" : [ "Himan Abdollahpouri", "Masoud Mansoury", "Robin Burke", "Bamshad Mobasher." ],
      "venue" : "arXiv preprint arXiv:1907.13286.",
      "citeRegEx" : "Abdollahpouri et al\\.,? 2019",
      "shortCiteRegEx" : "Abdollahpouri et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the trec 2010 entity track",
      "author" : [ "K. Balog", "Pavel Serdyukov", "Arjen P. de Vries." ],
      "venue" : "TREC.",
      "citeRegEx" : "Balog et al\\.,? 2010",
      "shortCiteRegEx" : "Balog et al\\.",
      "year" : 2010
    }, {
      "title" : "Overview of the trec 2011 entity track",
      "author" : [ "K. Balog", "Pavel Serdyukov", "Arjen P. de Vries." ],
      "venue" : "TREC.",
      "citeRegEx" : "Balog et al\\.,? 2011",
      "shortCiteRegEx" : "Balog et al\\.",
      "year" : 2011
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Bias and debias in recommender system: A survey and future directions",
      "author" : [ "J. Chen", "Hande Dong", "Xiao lei Wang", "Fuli Feng", "MingChieh Wang", "X. He." ],
      "venue" : "arXiv preprint arXiv:2010.03240.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "How algorithmic popularity bias hinders or promotes quality",
      "author" : [ "Giovanni Luca Ciampaglia", "Azadeh Nematzadeh", "Filippo Menczer", "Alessandro Flammini." ],
      "venue" : "Scientific Reports, 8.",
      "citeRegEx" : "Ciampaglia et al\\.,? 2018",
      "shortCiteRegEx" : "Ciampaglia et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "T-REx: A large scale alignment of natural language with knowledge base triples",
      "author" : [ "Hady Elsahar", "Pavlos Vougiouklis", "Arslen Remaci", "Christophe Gravier", "Jonathon Hare", "Frederique Laforest", "Elena Simperl." ],
      "venue" : "Proceedings of the Eleventh Interna-",
      "citeRegEx" : "Elsahar et al\\.,? 2018",
      "shortCiteRegEx" : "Elsahar et al\\.",
      "year" : 2018
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of Workshop for",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "KORE: keyphrase overlap relatedness for entity disambiguation",
      "author" : [ "Johannes Hoffart", "Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum." ],
      "venue" : "21st ACM International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Hoffart et al\\.,? 2012",
      "shortCiteRegEx" : "Hoffart et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust disambiguation of named entities in text",
      "author" : [ "Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen Fürstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 2011 Conference",
      "citeRegEx" : "Hoffart et al\\.,? 2011",
      "shortCiteRegEx" : "Hoffart et al\\.",
      "year" : 2011
    }, {
      "title" : "Are natural language inference models IMPPRESsive? Learning IMPlicature",
      "author" : [ "Paloma Jeretic", "Alex Warstadt", "Suvrat Bhooshan", "Adina Williams" ],
      "venue" : null,
      "citeRegEx" : "Jeretic et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jeretic et al\\.",
      "year" : 2020
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Retrieval-augmented generation",
      "author" : [ "Patrick S.H. Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot entity linking by reading entity descriptions",
      "author" : [ "Lajanugen Logeswaran", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova", "Jacob Devlin", "Honglak Lee." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Logeswaran et al\\.,? 2019",
      "shortCiteRegEx" : "Logeswaran et al\\.",
      "year" : 2019
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "AmbigQA: Answering ambiguous open-domain questions",
      "author" : [ "Sewon Min", "Julian Michael", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783–",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353,",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-grained entity typing for domain independent entity linking",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Onoe and Durrett.,? 2020",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2020
    }, {
      "title" : "Bootleg: Chasing the tail with self-supervised named entity disambiguation",
      "author" : [ "Laurel Orr", "Megan Leszczynski", "Simran Arora", "Sen Wu", "Neel Guha", "Xiao Ling", "Christopher Ré." ],
      "venue" : "arXiv preprint arXiv:2010.10363.",
      "citeRegEx" : "Orr et al\\.,? 2020",
      "shortCiteRegEx" : "Orr et al\\.",
      "year" : 2020
    }, {
      "title" : "KILT: a benchmark",
      "author" : [ "Fabio Petroni", "Aleksandra Piktus", "Angela Fan", "Patrick Lewis", "Majid Yazdani", "Nicola De Cao", "James Thorne", "Yacine Jernite", "Vladimir Karpukhin", "Jean Maillard", "Vassilis Plachouras", "Tim Rocktäschel", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2021
    }, {
      "title" : "Perturbation sensitivity analysis to detect unintended model biases",
      "author" : [ "Vinodkumar Prabhakaran", "Ben Hutchinson", "Margaret Mitchell." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Prabhakaran et al\\.,? 2019",
      "shortCiteRegEx" : "Prabhakaran et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Entity linking: Finding extracted entities in a knowledge base",
      "author" : [ "Delip Rao", "Paul McNamee", "Mark Dredze." ],
      "venue" : "Multi-source, Multilingual Information Extraction and Summarization.",
      "citeRegEx" : "Rao et al\\.,? 2013",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2013
    }, {
      "title" : "Are red roses red? evaluating consistency of question-answering models",
      "author" : [ "Marco Tulio Ribeiro", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6174–6184,",
      "citeRegEx" : "Ribeiro et al\\.,? 2019",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2019
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing natural language inference models through semantic fragments",
      "author" : [ "Kyle Richardson", "H. Hu", "L. Moss", "A. Sabharwal." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Richardson et al\\.,? 2020",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural entity linking: A survey of models based on deep learning",
      "author" : [ "Ozge Sevgili", "Artem Shelmanov", "Mikhail V. Arkhipov", "Alexander Panchenko", "Christian Biemann." ],
      "venue" : "arXiv preprint arXiv:2006.00575.",
      "citeRegEx" : "Sevgili et al\\.,? 2020",
      "shortCiteRegEx" : "Sevgili et al\\.",
      "year" : 2020
    }, {
      "title" : "you are grounded!”: Latent name artifacts in pre-trained language models",
      "author" : [ "Vered Shwartz", "Rachel Rudinger", "Oyvind Tafjord." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6850–6861,",
      "citeRegEx" : "Shwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Wikidata: a free collaborative knowledgebase",
      "author" : [ "Denny Vrandecic", "M. Krötzsch." ],
      "venue" : "Commun. ACM, 57:78–85.",
      "citeRegEx" : "Vrandecic and Krötzsch.,? 2014",
      "shortCiteRegEx" : "Vrandecic and Krötzsch.",
      "year" : 2014
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:377–392.",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-shot entity linking with dense entity retrieval",
      "author" : [ "Ledell Yu Wu", "F. Petroni", "Martin Josifoski", "Sebastian Riedel", "Luke Zettlemoyer." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Substantial progress in NLP has been made on “closed” tasks, where queries are paired with relevant documents (Rajpurkar et al., 2016; Dua et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "Substantial progress in NLP has been made on “closed” tasks, where queries are paired with relevant documents (Rajpurkar et al., 2016; Dua et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "However, there is growing interest in “opendomain” tasks, where relevant documents need to be retrieved from a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021).",
      "startOffset" : 193,
      "endOffset" : 234
    }, {
      "referenceID" : 26,
      "context" : "However, there is growing interest in “opendomain” tasks, where relevant documents need to be retrieved from a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021).",
      "startOffset" : 193,
      "endOffset" : 234
    }, {
      "referenceID" : 16,
      "context" : "Because success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "Because success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 198
    }, {
      "referenceID" : 36,
      "context" : "We automatically create AmbER sets by mining the Wikidata knowledge graph (Vrandecic and Krötzsch, 2014) for relevant names and entities, and leveraging task-specific templates",
      "startOffset" : 74,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "In total, our AmbER sets contain 80k task-specific queries which we align to the Wikipedia snapshot from KILT (Petroni et al., 2021).",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "Retrievers perform very differently on AmbER sets in terms of absolute retrieval numbers, with Bootleg (Orr et al., 2020), an entity-linking-based retriever, performing best.",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : "The largest fact checking training set, FEVER (Thorne et al., 2018), has 80k instances, while the slot filling dataset, T-REx (Elsahar et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : ", 2018), has 80k instances, while the slot filling dataset, T-REx (Elsahar et al., 2018), has over 2 million instances.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "Aligning Entities to Wikipedia We use the KILT Wikipedia snapshot (Petroni et al., 2021) as",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "The first three are TF-IDF, a token-based retriever using sparse embeddings, DPR (Karpukhin et al., 2020), a dense embedding based retriever, and BLINK (Wu et al.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 39,
      "context" : ", 2020), a dense embedding based retriever, and BLINK (Wu et al., 2020), a linker-based retriever which ranks documents based on input entities.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "For fact checking, we fine-tune a BERT classifier (Devlin et al., 2019) on FEVER (Thorne et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "For question answering, we fine-tune a RoBERTa model (Liu et al., 2019) on Natural Questions (Kwiatkowski et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "We use the AllenNLP and HuggingFace Transformers library to finetune our downstream models (Gardner et al., 2018; Wolf et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "Entity ambiguity has been most studied in the context of entity linking (Rao et al., 2013).",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "To improve disambiguation, entity linkers have included auxiliary information such as entity types (Onoe and Durrett, 2020) and entity descriptions (Logeswaran et al.",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "To improve disambiguation, entity linkers have included auxiliary information such as entity types (Onoe and Durrett, 2020) and entity descriptions (Logeswaran et al., 2019).",
      "startOffset" : 148,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "The problem of entity ambiguity also appears implicitly in entity-centric tasks such as determining the semantic relatedness between entities (Hoffart et al., 2012) and entity-oriented",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : "The relatively low answer score is due to artifacts in using EM for QA evaluation, and is consistent with human performance on span selection (Rajpurkar et al., 2016)).",
      "startOffset" : 142,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "This problem has been studied extensively in the recommendation systems literature, where recommendation systems are known to often ignore the long-tail of products and instead recommend very popular items (Abdollahpouri et al., 2017; Chen et al., 2020).",
      "startOffset" : 206,
      "endOffset" : 253
    }, {
      "referenceID" : 5,
      "context" : "This problem has been studied extensively in the recommendation systems literature, where recommendation systems are known to often ignore the long-tail of products and instead recommend very popular items (Abdollahpouri et al., 2017; Chen et al., 2020).",
      "startOffset" : 206,
      "endOffset" : 253
    }, {
      "referenceID" : 1,
      "context" : "This has the effect of unfairly hurting users who would prefer these less-popular items (Abdollahpouri et al., 2019; Ciampaglia et al., 2018).",
      "startOffset" : 88,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "This has the effect of unfairly hurting users who would prefer these less-popular items (Abdollahpouri et al., 2019; Ciampaglia et al., 2018).",
      "startOffset" : 88,
      "endOffset" : 141
    }, {
      "referenceID" : 32,
      "context" : "Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 188
    } ],
    "year" : 2021,
    "abstractText" : "Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.1",
    "creator" : "LaTeX with hyperref package"
  }
}