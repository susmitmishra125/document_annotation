{
  "name" : "2021.acl-long.429.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding",
    "authors" : [ "Hidetaka Kamigaito", "Katsuhiko Hayashi" ],
    "emails" : [ "kamigaito@lr.pi.titech.ac.jp", "khayashi0201@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Negative Sampling (NS) (Mikolov et al., 2013) is an approximation of softmax cross-entropy (SCE). Due to its efficiency in computation cost, NS is now a fundamental loss function for various Natural Language Processing (NLP) tasks such as used in word embedding (Mikolov et al., 2013), language modeling (Melamud et al., 2017), contextualized embedding (Clark et al., 2020b,a), and knowledge graph embedding (KGE) (Trouillon et al., 2016). Specifically, recent KGE models commonly use NS for training. Considering the current usages of NS, we investigated the characteristics of NS by mainly focusing on KGE from theoretical and empirical aspects.\nFirst, we introduce the task description of KGE. A knowledge graph is a graph that describes the relationships between entities. It is an indispensable resource for knowledge-intensive NLP applications such as dialogue (Moon et al., 2019) and questionanswering (Lukovnikov et al., 2017) systems. However, to create a knowledge graph, it is necessary to consider a large number of entity combinations and their relationships, making it difficult to construct a complete graph manually. Therefore, the\nprediction of links between entities is an important task.\nCurrently, missing relational links between entities are predicted using a scoring method based on KGE (Bordes et al., 2011). With this method, a score for each link is computed on vector space representations of embedded entities and relations. We can train these representations through various loss functions. The SCE (Kadlec et al., 2017) and NS (Trouillon et al., 2016) loss functions are commonly used for this purpose.\nSeveral studies (Ruffinelli et al., 2020; Ali et al., 2020) have shown that link-prediction performance can be significantly improved by choosing the appropriate combination of loss functions and scoring methods. However, the relationship between the SCE and NS loss functions has not been investigated in KGE. Without a basis for understanding the relationships among different loss functions, it is difficult to make a fair comparison between the SCE and NS results.\nWe attempted to solve this problem by using the Bregman divergence (Bregman, 1967) to provide a unified interpretation of the SCE and NS loss functions. Under this interpretation, we can understand the relationships between SCE and NS in terms of the model’s predicted distribution at the optimal solution, which we called the objective distribution. By deriving the objective distribution for a loss function, we can analyze different loss functions, the objective distributions of which are identical under certain conditions, from a unified viewpoint.\nWe summarize our theoretical findings not restricted to KGE as follows:\n• The objective distribution of NS with uniform noise (NS w/ Uni) is equivalent to that of SCE.\n• The objective distribution of self-adversarial negative sampling (SANS) (Sun et al., 2019)\nis quite similar to SCE with label smoothing (SCE w/ LS) (Szegedy et al., 2016).\n• NS with frequency-based noise (NS w/ Freq) in word2vec1 has a smoothing effect on the objective distribution.\n• SCE has a property wherein it more strongly fits a model to the training data than NS.\nTo check the validity of the theoretical findings in practical settings, we conducted experiments on the FB15k-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018) datasets. The experimental results indicate that\n• The relationship between SCE and SCE w/ LS is also similar to that between NS and SANS in practical settings.\n• NS is prone to underfitting because it weakly fits a model to the training data compared with SCE.\n• SCE causes underfitting of KGE models when their score function has a bound.\n• Both SANS and SCE w/ LS perform well as pre-training methods.\nThe structure of this paper is as follows: Sec. 2 introduces SCE and Bregman divergence; Sec. 3 induces the objective distributions for NS; Sec. 4 analyzes the relationships between SCE and NS loss functions; Sec. 5 summarizes and discusses our theoretical findings; Sec. 6 discusses empirically investigating the validity of the theoretical findings in practical settings; Sec. 7 explains the differences between this paper and related work; and Sec. 8 summarizes our contributions. Our code will be available at https://github.com/kamigaito/ acl2021kge"
    }, {
      "heading" : "2 Softmax Cross Entropy and Bregman Divergence",
      "text" : ""
    }, {
      "heading" : "2.1 SCE in KGE",
      "text" : "We denote a link representing a relationship rk between entities ei and e j in a knowledge graph as (ei,rk,e j). In predicting the links from given queries (ei,rk,?) and (?,rk,e j), the model must predict entities corresponding to each ? in the queries. We denote such a query as x and the entity to be\n1The word2vec uses unigram distribution as the frequencybased noise.\npredicted as y. By using the softmax function, the probability pθ (y|x) that y is predicted from x with the model parameter θ given a score function fθ (x,y) is expressed as follows:\npθ (y|x) = exp( fθ (x,y))\n∑y′∈Y exp( fθ (x,y′)) , (1)\nwhere Y is the set of all predictable entities. We further denote the pair of an input x and its label y as (x,y). Let D = {(x1,y1), · · · ,(x|D|,y|D|)} be observed data that obey a distribution pd(x,y)."
    }, {
      "heading" : "2.2 Bregman Divergence",
      "text" : "Next, we introduce the Bregman divergence. Let Ψ(z) be a differentiable function; the Bregman divergence between two distributions f and g is defined as follows:\ndΨ(z)( f ,g)=Ψ( f )−Ψ(g)−∇Ψ(g)T ( f−g). (2)\nWe can express various divergences by changing Ψ(z). To take into account the divergence on the entire observed data, we consider the expectation of dΨ( f ,g): BΨ(z)( f ,g) = ∑x,y dΨ(z)( f (y|x),g(y|x))pd(x,y). To investigate the relationship between a loss function and learned distribution of a model at an optimal solution of the loss function, we need to focus on the minimization of BΨ(z). Gutmann and Hirayama (2011) showed that BΨ(z)( f ,g) = 0 means that f equals g almost everywhere when Ψ(z) is a differentiable strictly convex function in its domain. Note that all Ψ(z) in this paper satisfy this condition. Accordingly, by fixing f , minimization of BΨ(z)( f ,g) with respect to g is equivalent to minimization of\nBΨ(z)( f ,g)\n=∑ x,y\n[ −Ψ(g)+∇Ψ(g)T g−∇Ψ(g)T f ] pd(x,y) (3)\nWe use BΨ( f ,g) to reveal a learned distribution of a model at optimal solutions for the SCE and NS loss functions."
    }, {
      "heading" : "2.3 Derivation of SCE",
      "text" : "For the latter explanations, we first derive the SCE loss function from Eq. (3). We denote a probability for a label y as p(y), vector for all y as y, vector of probabilities for y as p(y), and dimension size of z as len(z). In Eq. (3), by setting f as pd(y|x) and g as pθ (y|x) with Ψ(z) = ∑ len(z) i=1 zi logzi (Banerjee\net al., 2005), we can derive the SCE loss function as follows:\nBΨ(z)(pd(y|x), pθ (y|x))\n=−∑ x,y\n[ |Y |\n∑ i=1 pd(yi|x) log pθ (yi|x)\n] pd(x,y) (4)\n=− 1 |D| ∑\n(x,y)∈D log pθ (y|x). (5)\nThis derivation indicates that pθ (y|x) converges to the observed distribution pd(y|x) through minimizing BΨ(z)(pd(y|x), pθ (y|x)) in the SCE loss function. We call the distribution of pθ (y|x) when BΨ(z) equals zero an objective distribution."
    }, {
      "heading" : "3 Objective Distribution for Negative Sampling Loss",
      "text" : "We begin by providing a definition of NS and its relationship to the Bregman divergence, following the induction of noise contrastive estimation (NCE) from the Bregman divergence that was established by Gutmann and Hirayama (2011). We denote pn(y|x) to be a known non-zero noise distribution for y of a given x. Given ν noise samples from pn(y|x) for each (x,y) ∈ D, NS estimates the model parameter θ for a distribution G(y|x;θ) = exp(− fθ (x,y)).\nBy assigning to each (x,y) a binary class label C: C = 1 if (x,y) is drawn from observed data D following a distribution pd(x,y) and C = 0 if (x,y) is drawn from a noise distribution pn(y|x), we can model the posterior probabilities for the classes as follows:\np(C = 1,y|x;θ) = 1 1+exp(− fθ (x,y))\n= 1\n1+G(y|x;θ) ,\np(C = 0,y|x;θ) = 1−p(C = 1,y|x;θ)\n= G(y|x;θ)\n1+G(y|x;θ) .\nThe objective function `NS(θ) of NS is defined as follows:\n`NS(θ) =− 1 |D| ∑\n(x,y)∈D\n[ log(P(C = 1,y|x;θ))\n+ ν\n∑ i=1,yi∼pn\nlog(P(C = 0,yi|x;θ)) ] . (6)\nBy using the Bregman divergence, we can induce the following propositions for `NS(θ).\nProposition 1. `NS(θ) can be induced from Eq. (3) by setting Ψ(z) as:\nΨ(z) = z log(z)− (1+ z) log(1+ z). (7)\nProposition 2. When `NS(θ) equals 0, the following equation is satisfied:\nG(y|x;θ) = ν pn(y|x) pd(y|x) . (8)\nProposition 3. The objective distribution of Pθ (y|x) for `NS(θ) is\npd(y|x) pn(y|x) ∑\nyi∈Y\npd(yi|x) pn(yi|x)\n. (9)\nProof. We give the proof of Props. 1, 2, and 3 in Appendix A of the supplemental material.\nWe can also investigate the validity of Props. 1, 2, and 3 by comparing them with the previously reported result. For this purpose, we prove the following proposition:\nProposition 4. When Eq. (8) satisfies ν = 1 and pn(y|x) = pd(y), fθ (x,y) equals point-wise mutual information (PMI).\nProof. This is described in Appendix B of the supplemental material.\nThis observation is consistent with that by Levy and Goldberg (2014). The differences between their representation and ours are as follows. (1) Our noise distribution is general in the sense that its definition is not restricted to a unigram distribution; (2) we mainly discuss pθ (y|x) not fθ (x,y); and (3) we can compare NS- and SCE-based loss functions through the Bregman divergence."
    }, {
      "heading" : "3.1 Various Noise Distributions",
      "text" : "Different from the objective distribution of SCE, Eq. (9) is affected by the type of noise distribution pn(y|x). To investigate the actual objective distribution for `NS(θ), we need to consider separate cases for each type of noise distribution. In this subsection, we further analyze Eq. (9) for each separate case."
    }, {
      "heading" : "3.1.1 NS with Uniform Noise",
      "text" : "First, we investigated the case of a uniform distribution because it is one of the most common noise distributions for `NS(θ) in the KGE task. From Eq. (9), we can induce the following property.\nProposition 5. When pn(y|x) is a uniform distribution, Eq. (9) equals pd(y|x).\nProof. This is described in Appendix C of the supplemental material.\nDyer (2014) indicated that NS is equal to NCE when ν = |Y | and Pn(y|x) is uniform. However, as we showed, in terms of the objective distribution, the value of ν is not related to the objective distribution because Eq. (9) is independent of ν ."
    }, {
      "heading" : "3.1.2 NS with Frequency-based Noise",
      "text" : "In the original setting of NS (Mikolov et al., 2013), the authors chose as pn(y|x) a unigram distribution of y, which is independent of x. Such a frequencybased distribution is calculated in terms of frequencies on a corpus and independent of the model parameter θ . Since in this case, different from the case of a uniform distribution, pn(y|x) remains on the right side of Eq. (9), pθ (y|x) decreases when pn(y|x) increases. Thus, we can interpret frequency-based noise as a type of smoothing for pd(y|x). The smoothing of NS w/ Freq decreases the importance of high-frequency labels in the training data for learning more general vector representations, which can be used for various tasks as pretrained vectors. Since we can expect pre-trained vectors to work as a prior (Erhan et al., 2010) that prevents models from overfitting, we tried to use NS w/ Freq for pre-training KGE models in our experiments."
    }, {
      "heading" : "3.1.3 Self-Adversarial NS",
      "text" : "Sun et al. (2019) recently proposed SANS, which uses pθ (y|x) for generating negative samples. By replacing pn(y|x) with pθ (y|x), the objective distribution when using SANS is as follows:\npθ (y|x) = pd(y|x)\npθ̂ (y|x) ∑ yi∈Y pd(yi|x) pθ̂ (yi|x)\n, (10)\nwhere θ̂ is a parameter set updated in the previous iteration. Because both the left and right sides of Eq. (10) include pθ (y|x), we cannot obtain an analytical solution of pθ (y|x) from this equation. However, we can consider special cases of pθ (y|x) to gain an understanding of Eq. (10). At the beginning of the training, pθ (y|x) follows a discrete uniform distribution u{1, |Y |} because θ is randomly initialized. In this situation, when we set pθ̂ (y|x) in\nEq. (10) to a discrete uniform distribution u{1, |Y |}, pθ (y|x) becomes\npθ (y|x) = pd(y|x). (11)\nNext, when we set pθ̂ (y|x) in Eq. (10) as pd(y|x), pθ (y|x) becomes\npθ (y|x) = u{1, |Y |}. (12)\nIn actual mini-batch training, θ is iteratively updated for every batch of data. Because pθ (y|x) converges to u{1, |Y |} when pθ̂ (y|x) is close to pd(y|x) and pθ (y|x) converges to pd(y|x) when pθ̂ (y|x) is close to u{1, |Y |}, we can approximately regard the objective distribution of SANS as a mixture of pd and u{1, |Y |}. Thus, we can represent the objective distribution of pθ (y|x) as\npθ (y|x)≈ (1−λ )pd(y|x)+λu{1, |Y |} (13)\nwhere λ is a hyper-parameter to determine whether pθ (y|x) is close to pd(y|x) or u{1, |Y |}. Assuming that pθ (y|x) starts from u{1, |Y |}, λ should start from 0 and gradually increase through training. Note that λ corresponds to a temperature α for pθ̂ (y|x) in SANS, defined as\npθ̂ (y|x) = exp(α fθ (x,y))\n∑y′∈Y exp(α fθ (x,y′)) , (14)\nwhere α also adjusts pθ̂ (y|x) to be close to pd(y|x) or u{1, |Y |}."
    }, {
      "heading" : "4 Theoretical Relationships among Loss Functions",
      "text" : ""
    }, {
      "heading" : "4.1 Corresponding SCE form to NS with Frequency-based Noise",
      "text" : "We induce a corresponding cross entropy loss from the objective distribution for NS with frequencybased noise. We set Tx,y = pn(y|x) ∑\nyi∈Y\npd(yi|x) pn(yi|x) ,\nq(y|x) = T−1x,y pd(y|x), and Ψ(z) = ∑ len(z) i=1 zi logzi. Under these conditions, following induction from Eq. (4) to Eq. (5), we can reformulate BΨ(z)(q(y|x), p(y|x)) as follows:\nBΨ(z)(q(y|x), pθ (y|x))\n=−∑ x,y\n[ |Y |\n∑ i=1 T−1x,y pd(yi|x) log pθ (yi|x)\n] pd(x,y)\n=− 1 |D| ∑ (x,y)∈D T−1x,y log pθ (y|x). (15)\nExcept that Tx,y is conditioned by x and not normalized for y, we can interpret this loss function as SCE with backward correction (SCE w/ BC) (Patrini et al., 2017). Taking into account that backward correction can be a smoothing method for predicting labels (Lukasik et al., 2020), this relationship supports the theoretical finding that NS can adopt a smoothing to the objective distribution.\nBecause the frequency-based noise is used in word2vec as unigram noise, we specifically consider the case in which pn(y|x) is set to unigram noise. In this case, we can set pn(y|x) = pd(y). Since relation tuples do not appear twice in a knowledge graph, we can assume that pd(x,y) is uniform. Accordingly, we can change T−1x,y to\n1 pd(y) ∑\nyi∈Y\npd (yi|x) pd (yi)\n= 1 pd(y) ∑\nyi∈Y\npd (yi ,x) pd (yi)pd (x)\n= pd(x)pd(y)C , where C\nis a constant value, and we can reformulate Eq. (15) as follows:\n− 1 |D| ∑\n(x,y)∈D\npd(x) pd(y)C log pθ (y|x)\n∝− 1 |D| ∑\n(x,y)∈D\n#x #y log pθ (y|x), (16)\nwhere #x and #y respectively represent frequencies for x and y in the training data. We use Eq. (16) to pre-train models for SCE-based loss functions."
    }, {
      "heading" : "4.2 Corresponding SCE form to SANS",
      "text" : "We induce a corresponding cross entropy loss from the objective distribution for SANS by setting q(y|x) = (1−λ )pd(y|x)+λu{1, |Y |} and Ψ(z) = ∑len(z)i=1 zi logzi. Under these conditions, on the basis of induction from Eq. (4) to Eq. (5), we can\nreformulate BΨ(z)(q(y|x), pθ (y|x)) as follows:\nBΨ(z)(q(y|x), pθ (y|x))\n=−∑ x,y [ |Y | ∑ i=1 (1−λ )pd(yi|x) log pθ (yi|x)\n+ |Y |\n∑ i=1\nλu{1, |Y |} log pθ (yi|x) ] pd(x,y)\n=− 1 |D| ∑\n(x,y)∈D\n[ (1−λ ) log pθ (y|x)\n+ |Y |\n∑ i=1 λ |Y |\nlog pθ (yi|x) ] .\n(17) The equation in the brackets of Eq. (17) is the cross entropy loss that has a corresponding objective distribution to that of SANS. This loss function is similar in form to SCE with label smoothing (SCE w/ LS) (Szegedy et al., 2016). This relationship also accords with the theoretical finding that NS can adopt a smoothing to the objective distribution."
    }, {
      "heading" : "5 Understanding Loss Functions for Fair Comparisons",
      "text" : "We summarize the theoretical findings from Sections 2, 3, and 4 in Table 1. To compare the results from the theoretical findings, we need to understand the differences in their objective distributions and divergences."
    }, {
      "heading" : "5.1 Objective Distributions",
      "text" : "The objective distributions for NS w/ Uni and SCE are equivalent. We can also see that the objective distribution for SANS is quite similar to that for SCE w/ LS. These theoretical findings will be important for making a fair comparison between scoring methods trained with the NS and SCE loss functions. When a dataset contains low-frequency\nentities, SANS and SCE w/ LS can improve the link-prediction performance through their smoothing effect, even if there is no performance improvement from the scoring method itself. For comparing the SCE and NS loss functions fairly, therefore, it is necessary to use the vanilla SCE against NS w/ Uni and use SCE w/ LS against SANS.\nHowever, we still have room to discuss the relationship between SANS and SCE w/ LS because λ in SANS increases from zero during training, whereas λ in SCE w/ LS is fixed. To introduce the behavior of λ in SANS to SCE w/ LS, we tried a simple approach in our experiments that trains KGE models via SCE w/ LS using pre-trained embeddings from SCE as initial parameters. Though this approach is not exactly equivalent to SANS, we expected it to work similarly to increasing λ from zero in training.\nWe also discuss the relationship between NS w/ Freq and SCE w/ BC. While NS w/ Freq is often used for learning word embeddings, neither NS w/ Freq nor SCE w/ BC has been explored in KGE. We investigated whether these loss functions are effective in pre-training KGE models2. Because SANS and SCE w/ LS are similar methods to NS w/ Freq and SCE w/ BC in terms of smoothing, in our experiments, we also compared NS w/ Freq with SANS and SCE w/ BC with SCE w/ LS as pre-training methods."
    }, {
      "heading" : "5.2 Divergences",
      "text" : "Comparing Ψ(z) for NS and SCE losses is as important as focusing on their objective distributions. The Ψ(z) determines the distance between model-\n2As a preliminary experiment, we also trained KGE models via NS w/ Freq and SCE w/ BC. However, these methods did not improve the link-prediction performance because frequency-based noise changes the data distribution drastically.\npredicted and data distributions in the loss. It has an important role in determining the behavior of the model. Figure 1 shows the distance in Eq. (3) between the probability p and probability 0.5 for each Ψ in Table 13. As we can see from the example, dΨ(z)(0.5, p) of the SCE loss has a larger distance than that of the NS loss. In fact, Painsky and Wornell (2020) proved that the upper bound of the Bregman divergence for binary labels when Ψ(z) = ∑len(z)i=1 zi logzi. This means that the SCE loss imposes a larger penalty on the same predicted value than the NS loss when the value of the learning target is the same between the two losses4.\nHowever, this does not guarantee that the distance of SCE is always larger than NS. This is because the values of the learning target between the two losses are not always the same. To take into account the generally satisfied property, we also focus on the convexity of the functions. In each training instance, the first-order and second-order derivatives of these loss functions indicate that SCE is convex, but NS is not in their domains5. Since this property is independent of the objective distribution, we can consider SCE fits the model more strongly to the training data in general. Because of these features, SCE can be prone to overfitting.\nWhether the overfitting is a problem depends on how large the difference between training and test data is. To measure the difference between training and test data in a KG dataset, we calculated the Kullback-Leibler (KL) divergence for p(y|x) between the training and test data of commonly used KG datasets. To compute p(y|x), we first calculated\n3In this setting, we can expand Ψ(z) = ∑len(z)i=1 zi logzi to Ψ(z) = z logz+(1− z) log(1− z).\n4See Appendix. E for the further details. 5Goldberg and Levy (2014) discuss the convexity of the inner product in NS. Different from theirs, our discussion is about the convexity of the loss functions itself.\np(ei|rk,e j) = p(ei|rk)+ p(ei|e j) on the basis of frequencies in the data then calculated p(e j|rk,ei) in the same manner. We treated both p(ei|rk,e j) and p(e j|rk,ei) as p(y|x). We denote p(y|x) in the training data as P and in the test data as Q. With these notations, we calculated DKL(P||Q) as the KL divergence for p(y|x) between the test and training data. Figure 2 shows the results. There is a large difference in the KL divergence between FB15k237 and WN18RR. We investigated how this difference affects the SCE and NS loss functions for learning KGE models.\nIn a practical setting, the loss function’s divergence is not the only factor to affect the fit to the training data. Model selection also affects the fitting. However, understanding a model’s behavior is difficult due to the complicated relationship between model parameters. For this reason, we experimentally investigated which combinations of models and loss functions are suitable for link prediction."
    }, {
      "heading" : "6 Experiments and Discussion",
      "text" : "We conducted experiments to investigate the validity of what we explained in Section 5 through a\ncomparison of the NS and SCE losses."
    }, {
      "heading" : "6.1 Experimental Settings",
      "text" : "We evaluated the following models on the FB15k237 and WN18RR datasets in terms of the Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 metrics: TuckER (Balazevic et al., 2019); RESCAL (Bordes et al., 2011); ComplEx (Trouillon et al., 2016); DistMult (Yang et al., 2015); TransE (Bordes et al., 2013); RotatE (Sun et al., 2019). We used LibKGE (Broscheit et al., 2020)6 as the implementation. For each model to be able to handle queries in both directions, we also trained a model for the reverse direction that shares the entity embeddings with the model for the forward direction.\nTo determine the hyperparameters of these models, for RESCAL, ComplEx, DistMult, and TransE with SCE and SCE w/ LS, we used the settings that achieved the highest performance in a previous study (Ruffinelli et al., 2020) for each loss function as well as the settings from the original papers for TuckER and RotatE. In TransE with NS and SANS, we used the settings used by Sun\n6https://github.com/uma-pi1/kge\net al. (2019). When applying SANS, we set α to an initial value of 1.0 for LibKGE for all models except TransE and RotatE, and for TransE and RotatE, where we followed the settings of the original paper since SANS was used in it. When applying SCE w/ LS, we set λ to the initial value of LibKGE, 0.3, except on TransE and RotatE. In the original setting of RotatE, because the values of SANS for TransE and RotatE were tuned, we also selected λ from {0.3, 0.1, 0.01} using the development data in TransE and RotatE for fair comparison. Appendix D in the supplemental material details the experimental settings."
    }, {
      "heading" : "6.2 Characteristics of Loss functions",
      "text" : "Table 2 shows the results for each loss and model combination. In the following subsections, we discuss investigating whether our findings work in a practical setting on the basis of the results."
    }, {
      "heading" : "6.2.1 Objective Distributions",
      "text" : "In terms of the objective distribution, when SCE w/ LS improves performance, SANS also improves performance in many cases. Moreover, it accords with our finding that SCE w/ LS and SANS have similar effects. For TransE and RotatE, the relationship does not hold, but as we will see later, this is probably because TransE with SCE and RotatE with SCE did not fit the training data. If the SCE does not fit the training data, the effect of SCE w/ LS is suppressed as it has the same effect as smoothing."
    }, {
      "heading" : "6.2.2 Divergences",
      "text" : "Next, let us focus on the distance of the loss functions. A comparison of the results of WN18RR and FB15k-237 shows no performance degradation of SCE compared with NS. This indicates that the difference between the training and test data in WN18RR is not so large to cause overfitting problems for SCE.\nIn terms of the combination of models and loss functions, the results of NS are worse than those of SCE in TuckER, RESCAL, ComplEx, and DistMult. Because the four models have no constraint to prevent fitting to the training data, we consider that the lower scores are caused by underfitting. This conjecture is on the basis that the NS loss weakly fits model-predicted distributions to training-data distributions compared with the SCE loss in terms of divergence and convexity.\nIn contrast, the performance gap between NS\nFB15k-237\nMethod Pre-train MRR Hits@1 Hits@3 Hits@10\n- 0.363 0.269 0.400 0.548 RESCAL SCE 0.363 0.268 0.400 0.552 +SCE w/ LS SCE w/ BC 0.361 0.266 0.398 0.547\nSCE w/ LS 0.364 0.269 0.402 0.550\n- 0.339 0.249 0.372 0.520 RESCAL NS 0.342 0.251 0.376 0.524 +SANS NS w/ Freq 0.343 0.251 0.378 0.524\nSANS 0.345 0.254 0.380 0.525\nWN18RR\nand SCE is smaller in TransE and RotatE. This is because the score functions of TransE and RotatE have bounds and cannot express minus values. Since SCE has a normalization term, it is difficult to represent values close to 1 when the score function cannot represent negative values. This feature prevents TransE and RotatE from completely fitting to the training data. Therefore, we can assume that NS can be a useful loss function when the score function is bounded."
    }, {
      "heading" : "6.3 Effectiveness of Pre-training Methods",
      "text" : "We also explored pre-training for learning KGE models. We selected the methods in Table 2 that achieved the best MRR for each NS-based loss and each SCE-based loss in each dataset. In accordance with the success of word2vec, we chose unigram noise for both NS w/ Freq and SCE w/ BC.\nTable 3 shows the results. Contrary to our expectations, SCE w/ BC does not work well as a pre-training method. Because the unigram noise for SCE w/ BC can drastically change the original data distribution, SCE w/ BC is thought to be effective when the difference between training and test data is large. However, since the difference is not so large in the KG datasets, as discussed in the previous subsection, we believe that the unigram noise may be considered unsuitable for these datasets.\nCompared with SCE w/ BC, both SCE w/ LS and SANS are effective for pre-training. This is because the hyperparameters of SCE w/ LS and\nSANS are adjusted for KG datasets. When using vanilla SCE as a pre-training method, there is little improvement in prediction performance, compared with other methods. This result suggests that increasing λ in training is not as important for improving task performance.\nFor RotatE, there is no improvement in pretraining. Because RotatE has strict constraints on its relation representation, we believe it may degrade the effectiveness of pre-training."
    }, {
      "heading" : "7 Related Work",
      "text" : "Mikolov et al. (2013) proposed the NS loss function as an approximation of the SCE loss function to reduce computational cost and handle a large vocabulary for learning word embeddings. NS is now used in various NLP tasks, which must handle a large amount of vocabulary or labels. Melamud et al. (2017) used the NS loss function for training a language model. Trouillon et al. (2016) introduced the NS loss function to KGE. In contextualized pretrained embeddings, Clark et al. (2020a) indicated that ELECTRA (Clark et al., 2020b), a variant of BERT (Devlin et al., 2019), follows the same manner of the NS loss function.\nNS is frequently used to train KGE models. KGE is a task to complement a knowledge graph that describes relationships between entities. Knowledge graphs are used in various important downstream tasks because of its convenience in incorporating external knowledge, such as in a language model (Logan et al., 2019), dialogue (Moon et al., 2019), question-answering (Lukovnikov et al., 2017), natural language inference (K M et al., 2018), and named entity recognition (He et al., 2020). Thus, current KGE is important in NLP.\nDue to the importance of KGE, various scoring methods including RESCAL (Bordes et al., 2011), TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), TuckER (Balazevic et al., 2019), and RotatE (Sun et al., 2019) used in our experiment, have been proposed. However, the relationship between these score functions and loss functions is not clear. Several studies (Ruffinelli et al., 2020; Ali et al., 2020) have investigated the best combinations of scoring method, loss function, and their hyperparameters in KG datasets. These studies differ from ours in that they focused on empirically searching for good combinations rather than theoretical investigations.\nAs a theoretical study, Levy and Goldberg (2014) showed that NS is equivalent to factorizing a matrix for PMI when a unigram distribution is selected as a noise distribution. Dyer (2014) investigated the difference between NCE (Gutmann and Hyvärinen, 2010) and NS. Gutmann and Hirayama (2011) revealed that NCE is derivable from Bregman divergence. Our derivation for NS is inspired by their work. Meister et al. (2020) proposed a framework to jointly interpret label smoothing and confidence penalty (Pereyra et al., 2017) through investigating their divergence. Yang et al. (2020) theoretically induced that a noise distribution that is close to the true distribution behind the training data is suitable for training KGE models in NS. They also proposed a variant of SANS in the basis of their investigation.\nDifferent from these studies, we investigated the distributions at optimal solutions of SCE and NS loss functions while considering several types of noise distribution in NS."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We revealed the relationships between SCE and NS loss functions in KGE. Through theoretical analysis, we showed that SCE and NS w/ Uni are equivalent in objective distribution, which is the predicted distribution of a model at an optimal solution, and that SCE w/ LS and SANS have similar objective distributions. We also showed that SCE more strongly fits a model to the training data than NS due to the divergence and convexity of SCE.\nThe experimental results indicate that the differences in the divergence of the two losses were not large enough to affect dataset differences. The results also indicate that SCE works well with highly flexible scoring methods, which do not have any bound of the scores, while NS works well with RotatE, which cannot express minus values due to its bounded scoring. Moreover, they indicate that SCE and SANS work better in pre-training than NS w/ Uni, commonly used for learning word embeddings.\nFor future work, we will investigate the properties of loss functions in out-of-domain data."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by JSPS Kakenhi Grant nos. 19K20339, 21H03491, and 21K17801."
    }, {
      "heading" : "A Proof of Proposition 1, 2, and 3",
      "text" : "We can reformulate `NS as follows:\n`NS(θ) =− 1 |D| ∑\n(x,y)∈D\n( log(P(C = 1,y|x;θ))+\nν\n∑ i=1,yi∼pn log(P(C = 0,yi|x;θ))\n)\n=− 1 |D| ∑ (x,y)∈D log(P(C = 1,y|x;θ))− 1 |D| ∑ (x,y)∈D\nν\n∑ i=1,yi∼pn log(P(C = 0,yi|x;θ))\n=− 1 |D| ∑\n(x,y)∈D log(\n1 1+G(y|x;θ) )− 1 |D| ∑\n(x,y)∈D\nν\n∑ i=1,yi∼pn\nlog( G(yi|x;θ)\n1+G(yi|x;θ) )\n= 1 |D| ∑ (x,y)∈D log(1+G(y|x;θ))+ ν ν |D| ∑ (x,y)∈D\nν\n∑ i=1,yi∼pn\nlog(1+ 1\nG(yi|x;θ) )\n= ∑ x,y pd(y|x) log(1+G(y|x;θ))pd(x)+∑ x,y\nν pn(y|x) log(1+ 1\nG(y|x;θ) )pd(x) (18)\nLetting u = (x,y), f (u) = ν pn(y|x)pd(y|x) , g(u) = G(y|x;θ), and pd(x) = 1\npd(y|x) pd(x,y), we can reformulate Eq. (18) as:\n`NS(θ) = ( ∑ x,y pd(y|x) log(1+g(u)) 1 pd(y|x) pd(x,y)+∑ x,y ν pn(y|x) log(1+ 1 g(u) ) 1 pd(y|x) pd(x,y) )\n=∑ x,y\n[ log(1+g(u))+ log(1+\n1 g(u)\n) f (u) ]\npd(x,y)\n=∑ x,y\n[ log(1+g(u))− log(g(u)) f (u)+ log(1+g(u)) f (u) ] pd(x,y)\n=∑ x,y\n[ −g(u) log(1+g(u))+(1+g(u)) log(1+g(u))\n+ log(g(u))g(u)+ log(1+g(u))g(u)− log(g(u)) f (u)+ log(1+g(u)) f (u) ]\npd(x,y) (19)\nWith Ψ(g(u)) = g(u) log(g(u))− (1+ g(u)) log(1+ g(u)) and ∇Ψ(g(u)) = log(g(u))− log(1+ g(u)), we can reformulate Eq. (19) as:\n`NS(θ) =∑ x,y\n[ −Ψ(g(u))+∇Ψ(g(u))g(u)−∇Ψ(g(u)) f (u) ] pd(x,y)\n=BΨ(z)( f (u),g(u)). (20)\nFrom Eq. (20), when `NS(θ) is minimized, g(u) = f (u) is satisfied. In this condition, G(y|x;θ) becomes ν pn(y|x) pd(y|x) , and exp( fθ (x,y)) becomes pd(y|x) ν pn(y|x) as follows:\ng(u) = f (u)⇔ G(y|x;θ) = ν pn(y|x) pd(y|x) ⇔ exp( fθ (x,y)) = pd(y|x) ν pn(y|x) . (21)\nBased on the Eq. (1) and Eq. (21), the objective distribution for pθ (y|x) is as follows:\npθ (y|x) = pd(y|x)\npn(y|x) ∑ yi∈Y\npd(yi|x) pn(yi|x)\n. (22)"
    }, {
      "heading" : "B Proof of Proposition 4",
      "text" : "PMI is induced by multiplying pd(x) to the right-hand side of Eq. (8) and then computing logarithm for both sides as follows: G(y|x;θ) = pn(y|x) pd(y|x) ⇔ exp( fθ (x,y)) = pd(y|x) pn(y|x) = pd(y|x) pd(y) = pd(x,y) pd(x)pd(y) ⇔ fθ (x,y) = log pd(x,y) pd(y)pd(y)\n(23)"
    }, {
      "heading" : "C Proof of Proposition 5",
      "text" : "When pn(y|x) is a uniform distribution, pn(y|x) ∑ yi∈Y pd(yi|x) pn(yi|x) = ∑yi∈Y pd(yi|x) = 1, and thus, Eq. (9) becomes pd(y|x)."
    }, {
      "heading" : "D Experimental Details",
      "text" : "Dataset: We use FB15k-237 (Toutanova and Chen, 2015)7 and WN18RR (Dettmers et al., 2018)8 datasets in the experiments. We followed the standard split in the original papers for each dataset. Table 4 lists the statistics for each dataset.\nMetric: We evaluated the link prediction performance of models with MRR, Hits@1, Hits@3, and Hits@10 by ranking test triples against all other triples not appeared in the training, valid, and test datasets. We used LibKGE for calculating these metric scores. Model: We compared the following models: TuckER (Balazevic et al., 2019); RESCAL (Bordes et al., 2011); ComplEx (Trouillon et al., 2016); DistMult (Yang et al., 2015); TransE (Bordes et al., 2013); RotatE (Sun et al., 2019). For each model, we also trained a model for the reverse direction that shares the entity embeddings with the model for the forward direction. Thus, the dimension size of subject and object embeddings are the same in all models. Implementation: We used LibKGE (Broscheit et al., 2020)9 as the implementation. We used its 1vsAll setting for SCE-based loss functions and negative sampling setting for NS-based loss functions. We modified LibKGE to be able to use label smoothing on the 1vsAll setting. We also incorporated NS w/ Freq and SCE w/ BC into the implementation. Hyper-parameter: Table 5 and 6 show the hyper-parameter settings of each method for each dataset. In RESCAL, ComplEx, and DistMult we used the settings that achieved the highest performance for each loss function in the previous study (Ruffinelli et al., 2020)10. In TuckER and RotatE, we follow the settings from the original paper. When applying SANS, we set α to an initial value of 1.0 for LibKGE for all models except TransE and RotatE, and for TransE and RotatE, where we followed the settings of the original paper of SANS since SANS was used in it. When applying SCE w/ LS, we set λ to the initial value of LibKGE, 0.3, except on TransE and RotatE. In the original setting of TransE and RotatE, because the value of SANS was tuned for comparison, for fairness, we selected λ from {0.3, 0.1, 0.01} by using the development data through a single run for each value. We set the maximum epoch to 800. We calculated MRR every five epochs on the developed data, and the training was terminated when the highest value was not updated ten times. We chose the best model by using the MRR score on the development data. These hyperparameters were also used in the pre-training step. Validation Score Table 7, 8, and 9 show the best MRR scores of each loss for each model on the validation dataset. Device: In all models, we used a single NVIDIA RTX2080Ti for training. Except for RotetE with SCE-based loss functions, all models finished the training in one day. The RotetE with SCE-based loss function finished the training in at most one week.\n7https://www.microsoft.com/en-us/download/confirmation.aspx?id=52312 8https://github.com/TimDettmers/ConvE 9https://github.com/uma-pi1/kge\n10https://github.com/uma-pi1/kge-iclr20"
    }, {
      "heading" : "E Note: the divergence between the NS and SCE loss functions",
      "text" : "Painsky and Wornell (2020) proved that the upper bound of the Bregman divergence for binary labels when Ψ(z) = ∑len(z)i=1 zi logzi. However, to compare the SCE and NS loss functions in general, we need to consider the divergence of multi labels in SCE. When Ψ(z) = ∑len(z)i=1 zi logzi, we can derive the following inequality by using the log sum inequality:\ndΨ(z)(pd(y|x), pθ (y|x))\n= |Y |\n∑ i=1 pd(yi|x) log pd(yi|x) pθ (yi|x)\n=pd(y j|x) log pd(y j|x) pθ (y j|x) + |Y | ∑ i 6= j pd(yi|x) log pd(yi|x) pθ (yi|x)\n=pd(y j|x) log pd(y j|x) pθ (y j|x) +( |Y | ∑ i 6= j pd(yi|x)) log (∑|Y |i6= j pd(yi|x)) (∑|Y |i6= j pθ (yi|x)) =pd(y j|x) log pd(y j|x) pθ (y j|x) +(1− pd(y j|x)) log (1− pd(y j|x)) (1− pθ (y j|x)) . (24)\nEq. (24) shows that the divergence of multi labels is larger than that of binary labels in SCE. As we explained, dΨ(z)( f ,g) of SCE is larger than dΨ(z)( f ,g) of NS in binary labels. Therefore, the SCE loss imposes a larger penalty on the same predicted value than the NS loss when the value of the learning target is the same between the two losses."
    } ],
    "references" : [ {
      "title" : "Pykeen 1.0: A python library for training and evaluating knowledge graph emebddings",
      "author" : [ "Mehdi Ali", "Max Berrendorf", "Charles Tapley Hoyt", "Laurent Vermue", "Sahand Sharifzadeh", "Volker Tresp", "Jens Lehmann" ],
      "venue" : "arXiv preprint arXiv:2007.14175",
      "citeRegEx" : "Ali et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2020
    }, {
      "title" : "TuckER: Tensor factorization for knowledge graph completion",
      "author" : [ "Ivana Balazevic", "Carl Allen", "Timothy Hospedales." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Balazevic et al\\.,? 2019",
      "shortCiteRegEx" : "Balazevic et al\\.",
      "year" : 2019
    }, {
      "title" : "Clustering with bregman divergences",
      "author" : [ "Arindam Banerjee", "Srujana Merugu", "Inderjit S. Dhillon", "Joydeep Ghosh." ],
      "venue" : "Journal of Machine Learning Research, 6(58):1705–1749.",
      "citeRegEx" : "Banerjee et al\\.,? 2005",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2005
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 26, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI’11, page 301–306. AAAI Press.",
      "citeRegEx" : "Bordes et al\\.,? 2011",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming",
      "author" : [ "L.M. Bregman." ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics, 7(3):200 – 217.",
      "citeRegEx" : "Bregman.,? 1967",
      "shortCiteRegEx" : "Bregman.",
      "year" : 1967
    }, {
      "title" : "LibKGE - A knowledge graph embedding library for reproducible research",
      "author" : [ "Samuel Broscheit", "Daniel Ruffinelli", "Adrian Kochsiek", "Patrick Betz", "Rainer Gemulla." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Broscheit et al\\.,? 2020",
      "shortCiteRegEx" : "Broscheit et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training transformers as energy-based cloze models",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc Le", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Clark et al\\.,? 2020a",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020b",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional 2d knowledge graph embeddings",
      "author" : [ "Tim Dettmers", "Minervini Pasquale", "Stenetorp Pontus", "Sebastian Riedel." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Dettmers et al\\.,? 2018",
      "shortCiteRegEx" : "Dettmers et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Notes on noise contrastive estimation and negative sampling",
      "author" : [ "Chris Dyer." ],
      "venue" : "arXiv preprint arXiv:1410.8251.",
      "citeRegEx" : "Dyer.,? 2014",
      "shortCiteRegEx" : "Dyer.",
      "year" : 2014
    }, {
      "title" : "Why does unsupervised pre-training help deep learning? J",
      "author" : [ "Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio." ],
      "venue" : "Mach. Learn. Res., 11:625–660.",
      "citeRegEx" : "Erhan et al\\.,? 2010",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2010
    }, {
      "title" : "word2vec explained: deriving mikolov et al.’s negative-sampling word-embedding method. CoRR, abs/1402.3722",
      "author" : [ "Yoav Goldberg", "Omer Levy" ],
      "venue" : null,
      "citeRegEx" : "Goldberg and Levy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goldberg and Levy.",
      "year" : 2014
    }, {
      "title" : "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen." ],
      "venue" : "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297–304.",
      "citeRegEx" : "Gutmann and Hyvärinen.,? 2010",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2010
    }, {
      "title" : "Bregman divergence as general framework to estimate unnormalized statistical models",
      "author" : [ "Michael U. Gutmann", "Jun-ichiro Hirayama." ],
      "venue" : "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI’11, page",
      "citeRegEx" : "Gutmann and Hirayama.,? 2011",
      "shortCiteRegEx" : "Gutmann and Hirayama.",
      "year" : 2011
    }, {
      "title" : "Knowledge-graph augmented word representations for named entity recognition",
      "author" : [ "Qizhen He", "Liang Wu", "Yida Yin", "Heming Cai." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7919–7926.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning beyond datasets: Knowledge graph augmented neural networks for natural language processing",
      "author" : [ "Annervaz K M", "Somnath Basu Roy Chowdhury", "Ambedkar Dukkipati." ],
      "venue" : "Proceedings of the 2018 Conference of the North Ameri-",
      "citeRegEx" : "M et al\\.,? 2018",
      "shortCiteRegEx" : "M et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowledge base completion: Baselines strike back",
      "author" : [ "Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 69–74, Vancouver, Canada. Association for Computational Linguis-",
      "citeRegEx" : "Kadlec et al\\.,? 2017",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14, page 2177–2185, Cambridge, MA, USA.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling",
      "author" : [ "Robert Logan", "Nelson F. Liu", "Matthew E. Peters", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Logan et al\\.,? 2019",
      "shortCiteRegEx" : "Logan et al\\.",
      "year" : 2019
    }, {
      "title" : "Does label smoothing mitigate label noise",
      "author" : [ "Michal Lukasik", "Srinadh Bhojanapalli", "Aditya Menon", "Sanjiv Kumar" ],
      "venue" : "In Proceedings of the 37th International Conference on Machine Learning,",
      "citeRegEx" : "Lukasik et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lukasik et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network-based question answering over knowledge graphs on word and character level",
      "author" : [ "Denis Lukovnikov", "Asja Fischer", "Jens Lehmann", "Sören Auer." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, WWW ’17, page",
      "citeRegEx" : "Lukovnikov et al\\.,? 2017",
      "shortCiteRegEx" : "Lukovnikov et al\\.",
      "year" : 2017
    }, {
      "title" : "Generalized entropy regularization or: There’s nothing special about label smoothing",
      "author" : [ "Clara Meister", "Elizabeth Salesky", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6870–",
      "citeRegEx" : "Meister et al\\.,? 2020",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple language model based on PMI matrix approximations",
      "author" : [ "Oren Melamud", "Ido Dagan", "Jacob Goldberger." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1860–1865, Copenhagen,",
      "citeRegEx" : "Melamud et al\\.,? 2017",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2017
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs",
      "author" : [ "Seungwhan Moon", "Pararth Shah", "Anuj Kumar", "Rajen Subba." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Moon et al\\.,? 2019",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2019
    }, {
      "title" : "Bregman divergence bounds and universality properties of the logarithmic loss",
      "author" : [ "A. Painsky", "G.W. Wornell." ],
      "venue" : "IEEE Transactions on Information Theory, 66(3):1658–1673.",
      "citeRegEx" : "Painsky and Wornell.,? 2020",
      "shortCiteRegEx" : "Painsky and Wornell.",
      "year" : 2020
    }, {
      "title" : "Making deep neural networks robust to label noise: A loss correction approach",
      "author" : [ "Giorgio Patrini", "Alessandro Rozza", "Aditya Krishna Menon", "Richard Nock", "Lizhen Qu." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "Patrini et al\\.,? 2017",
      "shortCiteRegEx" : "Patrini et al\\.",
      "year" : 2017
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey E. Hinton." ],
      "venue" : "CoRR, abs/1701.06548.",
      "citeRegEx" : "Pereyra et al\\.,? 2017",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "You can teach an old dog new tricks! on training knowledge graph embeddings",
      "author" : [ "Daniel Ruffinelli", "Samuel Broscheit", "Rainer Gemulla." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ruffinelli et al\\.,? 2020",
      "shortCiteRegEx" : "Ruffinelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Rotate: Knowledge graph embedding by relational rotation in complex space",
      "author" : [ "Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Observed versus latent features for knowledge base and text inference",
      "author" : [ "Kristina Toutanova", "Danqi Chen." ],
      "venue" : "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 57–66, Beijing, China. Association",
      "citeRegEx" : "Toutanova and Chen.,? 2015",
      "shortCiteRegEx" : "Toutanova and Chen.",
      "year" : 2015
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard." ],
      "venue" : "ICML, pages 2071–2080.",
      "citeRegEx" : "Trouillon et al\\.,? 2016",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding negative sampling in graph representation learning",
      "author" : [ "Zhen Yang", "Ming Ding", "Chang Zhou", "Hongxia Yang", "Jingren Zhou", "Jie Tang." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Negative Sampling (NS) (Mikolov et al., 2013) is an approximation of softmax cross-entropy (SCE).",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : "Due to its efficiency in computation cost, NS is now a fundamental loss function for various Natural Language Processing (NLP) tasks such as used in word embedding (Mikolov et al., 2013), language modeling (Melamud et al.",
      "startOffset" : 164,
      "endOffset" : 186
    }, {
      "referenceID" : 24,
      "context" : ", 2013), language modeling (Melamud et al., 2017), contextualized embedding (Clark et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 34,
      "context" : ", 2020b,a), and knowledge graph embedding (KGE) (Trouillon et al., 2016).",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "It is an indispensable resource for knowledge-intensive NLP applications such as dialogue (Moon et al., 2019) and questionanswering (Lukovnikov et al.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and questionanswering (Lukovnikov et al., 2017) systems.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Currently, missing relational links between entities are predicted using a scoring method based on KGE (Bordes et al., 2011).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : ", 2017) and NS (Trouillon et al., 2016) loss functions are commonly used for this purpose.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "Several studies (Ruffinelli et al., 2020; Ali et al., 2020) have shown that link-prediction performance can be significantly improved by choosing the appropriate combination of loss functions and scoring methods.",
      "startOffset" : 16,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Several studies (Ruffinelli et al., 2020; Ali et al., 2020) have shown that link-prediction performance can be significantly improved by choosing the appropriate combination of loss functions and scoring methods.",
      "startOffset" : 16,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "We attempted to solve this problem by using the Bregman divergence (Bregman, 1967) to provide a unified interpretation of the SCE and NS loss functions.",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "• The objective distribution of self-adversarial negative sampling (SANS) (Sun et al., 2019)",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 32,
      "context" : "is quite similar to SCE with label smoothing (SCE w/ LS) (Szegedy et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 33,
      "context" : "To check the validity of the theoretical findings in practical settings, we conducted experiments on the FB15k-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "To check the validity of the theoretical findings in practical settings, we conducted experiments on the FB15k-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018) datasets.",
      "startOffset" : 153,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : "In the original setting of NS (Mikolov et al., 2013), the authors chose as pn(y|x) a unigram distribution of y, which is independent of x.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Since we can expect pre-trained vectors to work as a prior (Erhan et al., 2010) that prevents models from overfitting, we tried to use NS w/ Freq for pre-training KGE models in our experiments.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Except that Tx,y is conditioned by x and not normalized for y, we can interpret this loss function as SCE with backward correction (SCE w/ BC) (Patrini et al., 2017).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "Taking into account that backward correction can be a smoothing method for predicting labels (Lukasik et al., 2020), this relationship supports the theoretical finding that NS can adopt a smoothing to the objective distribution.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "This loss function is similar in form to SCE with label smoothing (SCE w/ LS) (Szegedy et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "We evaluated the following models on the FB15k237 and WN18RR datasets in terms of the Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 metrics: TuckER (Balazevic et al., 2019); RESCAL (Bordes et al.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : ", 2019); RESCAL (Bordes et al., 2011); ComplEx (Trouillon et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : ", 2011); ComplEx (Trouillon et al., 2016); DistMult (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 35,
      "context" : ", 2016); DistMult (Yang et al., 2015); TransE (Bordes et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : ", 2015); TransE (Bordes et al., 2013); RotatE (Sun et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "We used LibKGE (Broscheit et al., 2020)6 as the implementation.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "To determine the hyperparameters of these models, for RESCAL, ComplEx, DistMult, and TransE with SCE and SCE w/ LS, we used the settings that achieved the highest performance in a previous study (Ruffinelli et al., 2020) for each loss function as well as the settings from the original papers for TuckER and RotatE.",
      "startOffset" : 195,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "(2020a) indicated that ELECTRA (Clark et al., 2020b), a variant of BERT (Devlin et al.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : ", 2020b), a variant of BERT (Devlin et al., 2019), follows the same manner of the NS loss function.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Knowledge graphs are used in various important downstream tasks because of its convenience in incorporating external knowledge, such as in a language model (Logan et al., 2019), dialogue (Moon et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 26,
      "context" : ", 2019), dialogue (Moon et al., 2019), question-answering (Lukovnikov et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : ", 2019), question-answering (Lukovnikov et al., 2017), natural language inference (K M et al.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : ", 2018), and named entity recognition (He et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Due to the importance of KGE, various scoring methods including RESCAL (Bordes et al., 2011), TransE (Bordes et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : ", 2011), TransE (Bordes et al., 2013), DistMult (Yang et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : ", 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : ", 2015), ComplEx (Trouillon et al., 2016), TuckER (Balazevic et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : ", 2016), TuckER (Balazevic et al., 2019), and RotatE (Sun et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and RotatE (Sun et al., 2019) used in our experiment, have been proposed.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "Several studies (Ruffinelli et al., 2020; Ali et al., 2020) have investigated the best combinations of scoring method, loss function, and their hyperparameters in KG datasets.",
      "startOffset" : 16,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Several studies (Ruffinelli et al., 2020; Ali et al., 2020) have investigated the best combinations of scoring method, loss function, and their hyperparameters in KG datasets.",
      "startOffset" : 16,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Dyer (2014) investigated the difference between NCE (Gutmann and Hyvärinen, 2010) and NS.",
      "startOffset" : 52,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "(2020) proposed a framework to jointly interpret label smoothing and confidence penalty (Pereyra et al., 2017) through investigating their divergence.",
      "startOffset" : 88,
      "endOffset" : 110
    } ],
    "year" : 2021,
    "abstractText" : "In knowledge graph embedding, the theoretical relationship between the softmax crossentropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",
    "creator" : "LaTeX with hyperref"
  }
}