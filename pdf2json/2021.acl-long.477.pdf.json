{
  "name" : "2021.acl-long.477.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering",
    "authors" : [ "Nan Yang", "Furu Wei", "Binxing Jiao", "Daxin Jiang", "Linjun Yang" ],
    "emails" : [ "nanya@microsoft.com", "fuwei@microsoft.com", "binxjia@microsoft.com", "djiang@microsoft.com", "linjya@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6120–6129\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6120"
    }, {
      "heading" : "1 Introduction",
      "text" : "Retrieving relevant passages given certain query from a large collection of documents is a crucial component in many information retrieval systems such as web search and open domain question answering (QA). Current QA systems often employ a two-stage pipeline: a retriever is firstly used to find relevant passages, and then a fine-grained reader tries to locate the answer in the retrieved passages. As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages (Wang et al., 2017), the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever.\nTraditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25\n(Robertson and Zaragoza, 2009), which can be efficiently implemented with an inverted index. With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction (Karpukhin et al., 2020). In this approach, a dual-encoder model is learned to encode questions and passages into a dense, low-dimensional vector space, where the relevance between questions and passages can be calculated by the inner product of their respective vectors. As the vectors of all passages can be pre-computed and indexed, dense passage retrieval can also be done efficiently with vector space search methods during inference time (Shrivastava and Li, 2014).\nDense retrieval models are usually trained with contrastive objectives between positive and negative question-passage pairs. As the positive pairs are often given by the training data, one challenge in contrastive learning is how to select negative examples to avoid mismatch between training and inference. During inference time, the model needs to find the correct passages from a very large set of pre-computed candidate vectors, but during training, both positive and negative examples need to be encoded from scratch, thus severely limiting the number of negative examples due to computational cost. One promising way to reduce the discrepancy is momentum constrastive learning (MoCo) proposed by He et al. (2020). In this method, a pair of fast/slow encoders are used to encode questions and passages, respectively. The slow encoder is updated as a slow moving average of the fast encoder, which reduces the inconsistency of encoded passage vectors between subsequent training steps, enabling the encoded passages to be stored in a large queue and reused in later steps as negative examples. Unfortunately, directly applying MoCo in question-passage matching is problematic. Unlike the image matching tasks in original MoCo paper, the questions and passages are distinct from\neach other and not interchangeable. Furthermore, the passages are only encoded by the slow encoder, but the slow encoder is only updated with momentum from the fast encoder, not directly affected by the gradients. As the fast encoder only sees the questions, the training becomes insensitive to the passage representations and fails to learn properly. To solve this problem, we propose a new contrastive learning method called Cross Momentum Contrastive Learning (xMoCo). xMoCo employs two sets of fast/slow encoders and jointly optimizes the question-passage and passage-question matching tasks. It can be applied to scenarios where the questions and passages require different encoders, while retaining the advantage of efficiently maintaining a large number of negative examples. We test our method on several open-domain QA tasks, and the experimental results show the effectiveness of the proposed approach.\nTo summarize, the main contributions of this work are as follows:\n• We proposes a new momentum contrastive learning method, Cross Momentum Contrast (xMoCo), which can learn question-passage matching where questions and passages require different encoders.\n• We demonstrate the effectiveness of xMoCo in learning a dense passage retrieval model for various open domain question answering datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : "There are mainly two threads of research work related to this paper."
    }, {
      "heading" : "2.1 Passage Retrieval for QA",
      "text" : "Retrieving relevance passages is usually the first step in the most QA pipelines. Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 (Chen et al., 2017). Keyword-based approach enjoys its simplicity, but often suffers from term mismatch between questions and passages. Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019).\nThe challenge in training a dense retriever often lies in how to select negative question-passage pairs. As a small number of randomly generated negative pairs are considered too easy to differentiate, previous work has mainly focused on how to generate “hard” negatives. Karpukhin et al. (2020) selects one negative pair from the top results retrieved by BM25 as hard examples, in addition to one randomly sampled pair. Xiong et al. (2020) uses an iterative approach to gradually produce harder negatives by periodically retrieving top passages for each question using the trained model. In addition to finding hard negatives, Ding et al. (2020) also address the problem of false negatives by filtering them out using a more accurate, fused input model. Different from the above works, our approach aims to address this problem by enlarging the pool of negative samples using momentum contrastive learning, and can be adapted to incorporate harder, cleaner negative samples by other methods."
    }, {
      "heading" : "2.2 Momentum Contrastive Learning",
      "text" : "Momentum contrastive learning (MoCo) is originally proposed by He et al. (2020). He et al. (2020) learns image representations by training the model to find the heuristically altered images among a large set of other images. It is later improved by constructing better positive pairs (Chen et al., 2020). Different from the image counterpart, many NLP tasks has readily available positive pairs such question-passage pairs. Here the main benefit of momentum contrastive learning is to efficiently maintain a large set of negative samples, thus making the learning process more consistent with the inference. One example of applying momentum contrastive learning in NLP is Chi et al. (2020). In their work, momentum contrastive learning is employed to optimize the InfoNCE lower bound between parallel sentence pairs from different languages. Different from the above works, the questions and passages in our work are not interchangeable and require different encoders, which renders the original MoCo not directly applicable."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Task description",
      "text" : "In this paper, we deal with the task of retrieving relevant passages given certain natural language questions. Given a question q and a collection of N passages {q1, q2, . . . , qN}, a passage retriever aims to return a list of passages {qi1 , qi2 , . . . , qiM }\nranked by their relevance to q. While the number of retrieved passages M is usually in the magnitude of hundreds or thousands, the number of total passages N is typically very large, possibly in millions or billions. Such practical concern places constraints in model choices of the passage retrievers."
    }, {
      "heading" : "3.2 Dual-encoder framework for dense passage retrieval",
      "text" : "The de-facto “go-to” choice for dense passage retrieval is the dual-encoder approach. In this framework, a pair of encoders Eq and Ep, usually implemented as neural networks, are used to map the question q and the passage p into their lowdimensional vectors separately. The relevance or similarity score between q and p is calculated as the inner product of the two vectors:\ns(q, p) = Eq(q) · Ep(p)\nThe advantage of this approach is that the vectors of all passages can be pre-computed and stored. During inference, we only need to compute the vector for the question, and the maximum inner product search (MIPS) (Shrivastava and Li, 2014) can be used to efficiently retrieve most relevant passages from a large collection of candidates. It is possible to train a more accurate matching model if the q and p are fused into one input sequence, or if a more sophisticated similarity model is used instead of the simple inner-product, but those changes would no longer permit efficient retrieval, thus can only be used in a later “re-ranking” stage.\nThe training data D for passage retrieval consists of a collection of positive question-passage pairs {(p1, q1), (p2, q2), . . . , (pn, qn)}, and an additional m passages {pn+1, . . . , pn+m} without their corresponding questions. The encoders are trained to optimize the negative log-likelihood of all positive pairs:\nL(D, Eq, Ep) = − n∑\ni=1\nlog exp s(qi, pi)∑n+m\nj=1 exp s(qi, pj)\nAs the number of negative pairs (n + m − 1) is very large, it is infeasible to optimize the loss directly. Instead, only a subset of the negative samples will be selected to compute the denominator in the above equation. The selection of the negative samples is critical to the performance of trained model. Previous works such as Xiong et al. (2020)\nand Ding et al. (2020) mainly focus on selecting a few “hard” examples, which hve higher similarity scores with the question and thus contribute more to the sum in the denominator. In this work, we will explore how to use a large set of negative samples to better approximate the sum in the denominator."
    }, {
      "heading" : "4 Method",
      "text" : ""
    }, {
      "heading" : "4.1 Momentum contrast for passage retrieval",
      "text" : "We briefly review momentum contrast and explain why directly applying momentum contrast for passage retrieval is problematic.\nMomentum contrast method employs a pair of encoders Eq and Ep. For each training step, the training pair of qi and pi is encoded as Eq(qi) and Ep(pi) respectively, which is identical to other training method. The key difference is that momentum contrast maintains a queue Q of passage vectors {Ep(pi−k)}k encoded in previous training steps. The passage vectors in the queue serve as negative candidates for the current question qi. The process is computationally efficient since the vectors for negative samples are not re-computed, but it also brings the problem of staleness: the vectors in the queue are computed by the previous, not up-to-date models. To reduce the inconsistency, momentum contrast uses momentum update on the encoder Ep, making Ep a slow moving-average copy of the question encoder Eq. The gradient from the loss function is only directly applied to the question encoder Eq, not the passage encoder Ep. After each training step, the newly encoded Epi is pushed into the queue and the oldest vector is discarded, keeping the queue size constant during training. Such formulation poses no problem for the original MoCo paper (He et al., 2020), because their “questions” and “passages” are both images and are interchangeable. Unfortunately, in our passage retrieval problem, the questions and passages are distinct, and it is desirable to use different encoders Eq and Ep. Even in scenarios where the parameters of the two encoders can be shared, the passages are only encoded by the passage encoder Ep, but the gradient from the loss is not applied on the passage encoder. It makes the training process insensitive to the input passages, thus unable to learn reasonable representations."
    }, {
      "heading" : "4.2 xMoCo: Cross momentum contrast",
      "text" : "To solve the problems mentioned above, we propose a new momentum contrastive learning method,\ncalled cross momentum contrast (xMoCo). xMoCo employs two pairs of encoders: Efastq and Eslowq for questions; Efastp and Eslowp for passages. In addition, two separate queues Qq and Qp store previous encoded vectors for questions and passages, respectively. In one training step, given a positive pair q and p, the question encoders map q into Efastq (q) and Eslowq (q), while the passage encoders map p intoEfastp (p) andEslowp (p). The two vectors encoded by slow encoders are then pushed into their respective queues Qq and Qp. We jointly optimize the question-to-passage and passage-toquestion tasks by pitting q against all vectors inQq and p against all vectors in Qp:\nLqp = − log exp (Efastq (q) · Eslowp (p))∑\np′∈Qp expE fast q (q) · Eslowp (p′)\nLpq = − log exp (Efastp (p) · Eslowq (q))∑\nq′∈Qq expE fast p (p) · Eslowq (q′)\nL = λLqp + (1− λ)Lpq\nwhere λ is a weight parameter and simply set to 0.5 in all experiments in this paper. Like the original MoCo, the gradient update from the loss is only applied to the fast encodersEfastq andE fast p , while the slow encoders Eslowq and E slow p are updated with momentum from the fast encoders:\nEslowp ← αEfastp + (1− α)Eslowp\nEslowq ← αEfastq + (1− α)Eslowq\nwhere α controls the update speed of the slow encoders and is typically set to a small positive value. When training is finished, both slow encoders are discarded, and only the fast encoders are used in inference. Hence, the number of parameters for xMoCo is comparable to other dual-encoder methods when employing similar-sized encoders.\nIn this framework, the two fast encoders Efastq andEfastp are not tightly coupled in the gradient update, but instead influence other through the slow encoders. Efastp updates Eslowp through momentum updates, which in turn influences Efastq by gradient updates from optimizing the loss Lqp. Efastq can also influence Efastp through similar path. See Fig. 1 for illustration."
    }, {
      "heading" : "4.3 Adaption for Batch Training",
      "text" : "Batch training is the standard training protocol for deep learning models due to efficiency and performance reasons. For xMoCo, we also expect our model to be trained in batches. Under the batch training setting, a batch of positive examples are processed together in one training step. The only adaption we need here is to push all vectors computed by slow encoders in one batch into the queues together. It effectively mimics the behavior of the “in-batch negative” strategy employed by previous works such as Karpukhin et al. (2020), where the passages in one batch will serve as negatives examples for their questions."
    }, {
      "heading" : "4.4 Encoders",
      "text" : "We use pre-trained uncased BERT-base (Devlin et al., 2019) models as our encoders following Karpukhin et al. (2020). The question and passage encoders utilize two sets of different parameters but are initialized from the same BERT-base model. For both question and passage, we use the vectors of the sequence start tokens in the last layer as their representations. Better pre-trained models such as Liu et al. (2019) can lead to better retrieval performance, but we choose the uncased BERT-base model for easier comparison with previous work."
    }, {
      "heading" : "4.5 Incorporating hard negative examples",
      "text" : "Previous work has shown selecting hard examples can be helpful for training passage retrieval models. Our method can easily incorporate hard negative examples by simply adding an additional loss under the multitask framework:\nLhard\n=− log exp (E fast q (q) · Efastp (p))∑\np′∈P− ⋃ {p} expE fast q (q) · Efastp (p′)\nwhere P is a set of hard negative examples. The loss only involves the two fast encoders, not the slow encoders. We only add hard negatives for the question-to-passage matching tasks, not the passage-to-question matching tasks. In addition, we also encode these negative passages using the slow passage encoder Eslowp and enqueue them to serve as negative passages in calculating loss Lqp.\nIn this work, we only implement a simple method of generating hard examples following Karpukhin et al. (2020): for each positive pair, we add one hard negative example by randomly sampling from top retrieval results using a BM25 retriever. More elaborate methods of finding hard examples such as Xiong et al. (2020) and Ding et al. (2020) can also be included, but we leave it to future work."
    }, {
      "heading" : "4.6 Removing false negative examples",
      "text" : "False negative examples are passages that can match the given question but are falsely labeled as negative examples. In xMoCo formulation, false negatives can arise if a previous encoded passage p in the queue can answer current question q. It can happen if the some questions share the same passage as answer, or if the same question-passage pair is sampled another time when its previous encoded vector is still in the queue because the queue\nsize can be quite large. This is especially important for datasets with small number of positive pairs. To fix the problem, we keep track of the passage ids in the queue and mask out those passages identical to the current passage when calculating the loss.\nLabeling issues can also be the source of false negative examples as pointed out in Ding et al. (2020). In their work, an additional model with fused input is trained to reduce the false negatives. We plan to incorporate such model-based approach in the future."
    }, {
      "heading" : "5 Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Wikipedia Data as Passage Retrieval Candidates",
      "text" : "As many question answering datasets only provide positive pairs of questions and passages, we need to create a large collection of passages for passage retrieval tasks. Following Lee et al. (2019), we extract the passage candidate set from the English Wikipedia dump from Dec. 20, 2018. Following the pre-processing steps in Karpukhin et al. (2020), we first extract clean texts using pre-processing code from DrQA (Chen et al., 2017), and then split each article into non-overlapping chunks of 100 tokens as the passages for our retrieval task. After pre-processing, we get 20,914,125 passages in total."
    }, {
      "heading" : "5.2 Question Answering Datasets",
      "text" : "We use the five QA datasets from Karpukhin et al. (2020) and follow their training/dev/test splits. Here is a brief description of the datasets.\nNatural Questions (NQ) (Kwiatkowski et al., 2019) is a question answer dataset where the questions were real Google search queries and answers were text spans of Wikipedia articles manually selected by annotators.\nTriviaQA (Joshi et al., 2017) is a set of trivia questions with their answers. We use the unfiltered version of TriviaQA.\nWebQuestions (WQ) (Berant et al., 2013) is a collection of questions from Google Suggest API with answers from Freebase.\nCuratedTREC (TREC) (Baudiš and Šedivý, 2015) composes of questions from both TREC QA tracks and Web sources.\nSQuAD v1.1 (Rajpurkar et al., 2016) is original used as a benchmark for reading comprehension.\nWe follow the same procedure in Karpukhin et al. (2020) to create positive passages for all datasets.\nFor TriviaQA, WQ and TREC, we use the highestranked passage from BM25 which contains the answer as positive passage, because these three datasets do not provide answer passages. We discard questions if answer cannot be found at the top 100 BM25 retrieval results. For NQ and SQuAD, we replace the gold passage with the matching passage in our passage candidate set and discard unmatched questions due to differences in processing. Table 1 shows the number of questions in the original training/dev/test sets and the number of questions in training sets after discarding unmatched questions. Note that our numbers are slightly different from Karpukhin et al. (2020) due to small differences in the candidate set or the filtering process."
    }, {
      "heading" : "5.3 Settings",
      "text" : "Following Karpukhin et al. (2020), we test our model on two settings: a “single” setting where each dataset is trained separately, and a “multi” setting where the training data is combined from NQ, TriviaQA, WQ and TREC (excluding SQuAD).\nWe compare our model against two baselines. The first baseline is the classic BM25 baseline. The second baseline is the Deep Passage Retrieval (DPR) model from Karpukhin et al. (2020). We also implement the setting where the candidates are re-ranked using a linear combination of BM25 and the model similarity score from either DPR or our xMoCo model.\nThe evaluation metric for passage retrieval is top-K retrieval accuracy. Here the top-K accuracy means the percentage of questions which have at least one passage containing the answer in the top K retrieved passages. In our experiments, we evaluate the results on both Top-20 and Top-100 retrieval accuracy."
    }, {
      "heading" : "5.4 Implementation details",
      "text" : "For training, we used batch size of 128 for our models. For the two small datasets TREC and WQ, we trained the model for 100 epochs; for other datasets, we trained the model for 40 epochs. We used the dev set results to select the final checkpoint for testing. The dropout is 0.1 for all encoders. The queue size of negative examples in our model is 16, 384. The momentum co-efficient α in the momentum update is set to 0.001. We used Adam optimizer with a learning rate of 3e − 5, linear scheduling with 5% warm-up. We didn’t do hyperparameter search. We follow their specification in Karpukhin\net al. (2020) when re-implementing DPR baselines. Training was done on 16 32GB Nvidia GPUs, and took less than 12 hours to train each model.\nFor inference, we use FAISS (Johnson et al., 2017) for indexing and retrieving passage vectors. For BM25, we use Lucene implementation with b = 0.4 (length normalization) and k1 = 0.9 (term frequency scaling) following Karpukhin et al. (2020)."
    }, {
      "heading" : "5.5 Main Results",
      "text" : "We compare our xMoCo model with both BM25 and DPR baselines over the five QA datasets. As shown in Table 2, our model out-performs both BM25 and DPR baselins in most settings when evaluating on top-20 and top-100 accuracy, except SQuAD where xMoCo does slightly worse than BM25. The lower performance on SQuAD than BM25 is consistent with previous observation in Karpukhin et al. (2020). All the baseline numbers are our re-implementations and are comparable but slightly different from the numbers reported in Karpukhin et al. (2020) due to the difference in the pre-processing and random variations in training. The results empirically demonstrate that using a large number of negative samples in xMoCo indeed leads to a better retrieval model. The improvement of top-20 accuracy is larger than that of top-100 accuracy, since top-100 accuracy is already reasonably high for the DPR baselines. Linearly adding BM25 and model scores does not bring consistent improvement, as xMoCo’s performance is significantly better than BM25 except for SQuAD dataset. Furthermore, combining training data only brings improvement on smaller datasets and hurts results on larger datasets due to domain differences."
    }, {
      "heading" : "5.6 Ablation Study",
      "text" : "We perform all ablation experiments on NQ dataset except for the end-to-end QA result evaluation."
    }, {
      "heading" : "5.6.1 Size of the queue of negative samples",
      "text" : "One main assumption of xMoCo is that using a larger size of negative samples will lead to a better model for passage retrieval. Here we empirically study the assumption by varying the size of the queues of negative samples. The queue size cannot be reduced to zero because we need at least one negative sample to compute the contrastive loss. Instead, we use the two times the batch size as the minimal queue size, when the strategy essentially reverses to “in-batch negatives” used in previous\nworks. As shown in Fig. 2, the model performance increases as the queue size increases initially, but tapers off past 16k. This is different from previous work Chi et al. (2020), where they observe performance gains with queue size up to 130k. One possible explanation is that the number of training pairs is relatively small, thus limiting the effectiveness of the larger queue sizes. As for computational efficiency, the size of the queue has little impact on both training speed and memory cost, because both are dominated by the computation of the encoders."
    }, {
      "heading" : "5.6.2 Effect of using two set of encoders",
      "text" : "xMoCo formulation expands on the original momentum contrastive learning framework MoCo by enabling two different set of encoders for questions and passages respectively. For open-domain QA, it is unclear whether it is beneficial to use two different encoders for questions and passages because both questions and passages are texts. To empirically answer this question, we perform another ablation experiment where the parameters in the question and passage encoders are tied. As can be seen in Table 3, the model with tied encoders gives reasonable results, but still under-performs the model with two different encoders. Furthermore, the flexibility of xMoCo is necessary for tasks such as text-to-image matching where “ques-\ntions” and “passages” are drastically different."
    }, {
      "heading" : "5.6.3 End-to-end QA results",
      "text" : "For some open domain QA tasks, after the relevant passages are fetched by the retriever, a “reader” is then applied to the retrieval results to extract finegrained answer spans. While improving retrieval accuracy is an important goal, it is interesting to see how the improvement would translate into the end-to-end QA results. Following Karpukhin et al. (2020), we implement a simple BERT based reader to predict the answer spans. Give a question Q and N retrieved passages {P1, . . . , PN}, the reader first concatenates the question Q to each passage Pi and predicts the probability of span (P si , P e i as the answer as:\np(i, s, e|Q,P1, . . . , PN ) = pr(i|Q,P1, . . . , PN ) × pstart(s|Q,Pi) × pend(e|Q,Pi)\nwhere pr is the probability of selecting the ith passage, and pstart, pend are the probabilities of the sth and eth tokens being the answer start and end position respectively. pstart and pend is computed by the standard formula in the original BERT paper (Devlin et al., 2019), and the pr is computed by applying softmax over a linear transformation over the vectors of the start tokens of all passages. We follow the training strategy of Karpukhin et al. (2020), and sample one positive passages and 23 negative passages from the top-100 retrieval results during training. Please refer to their paper for the details.\nThe results are shown in Table 4. While the results from xMoCo are generally better in most cases, the improvements are marginal compared to the results of DPR models. The reason might be that the improvement of xMoCo over DPR on top-100 accuracy is not very large, and it might require better reader to find out the answer spans."
    }, {
      "heading" : "6 Discussion",
      "text" : "How to select/create negative examples is an essential aspect of passage retrieval model training. xMoCo improves passage retrieval model by efficiently maintaining a large set of negative examples, while previous works mainly focus on finding a few hard examples. It is desirable to design a method to take the best from both worlds. As described in Section 4.5, we can combine the two approaches under a simple multitask framework. But this multitask framework also has its drawbacks. Firstly, it loses the computational efficiency of xMoCo, especially if the method of generating the hard examples is expensive. Secondly, the large set of negative examples in xMoCo and the set of hard examples are two separate sets, while ideally, we want to maintain a large set of hard negative examples. To this end, one possible direction is to employ curriculum learning (Bengio et al., 2009). Assuming the corresponding passages for similar questions can serve as hard examples for each other, we can schedule the order of training examples so that similar questions are trained in adjacent steps, resulting more hard examples to be kept in the queue. We plan to explore this possibility in future work."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose cross momentum contrastive learning (xMoCo), for passage retrieval task in open domain QA. xMoCo jointly optimizes question-to-passage and passage-to-question matching, enabling using separate encoders for questions and passages, while efficiently maintains a large pool of negative samples like the original MoCo. We verify the effectiveness of the proposed method on various open domain QA datasets. For future work, we plan to investigate how to better integrate hard negative examples into xMoCo."
    } ],
    "references" : [ {
      "title" : "Modeling of the question answering task in the yodaqa system",
      "author" : [ "Petr Baudiš", "Jan Šedivý." ],
      "venue" : "Proceedings of the 6th International Conference on Experimental IR Meets Multilinguality, Multimodality, and Interaction - Volume 9283, CLEF’15, page",
      "citeRegEx" : "Baudiš and Šedivý.,? 2015",
      "shortCiteRegEx" : "Baudiš and Šedivý.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, page 41–48, New York, NY, USA. Association for Com-",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Semantic parsing on Freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Wash-",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "A survey of automatic query expansion in information retrieval",
      "author" : [ "Claudio Carpineto", "Giovanni Romano." ],
      "venue" : "ACM Comput. Surv., 44(1).",
      "citeRegEx" : "Carpineto and Romano.,? 2012",
      "shortCiteRegEx" : "Carpineto and Romano.",
      "year" : 2012
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved baselines with momentum contrastive learning",
      "author" : [ "Xinlei Chen", "Haoqi Fan", "Ross Girshick", "Kaiming He" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Chi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Rocketqa: An optimized training approach to dense passage retrieval for opendomain question answering",
      "author" : [ "Yingqi Qu Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang" ],
      "venue" : null,
      "citeRegEx" : "Ding et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "K. He", "H. Fan", "Y. Wu", "S. Xie", "R. Girshick." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9726–9735.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Billion-scale similarity search with gpus",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Joshi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:453–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Document expansion by query prediction",
      "author" : [ "Rodrigo Nogueira", "Wei Yang", "Jimmy Lin", "Kyunghyun Cho." ],
      "venue" : "CoRR, abs/1904.08375.",
      "citeRegEx" : "Nogueira et al\\.,? 2019",
      "shortCiteRegEx" : "Nogueira et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "The probabilistic relevance framework: Bm25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Found. Trends Inf. Retr., 3(4):333–389.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)",
      "author" : [ "Anshumali Shrivastava", "Ping Li." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 27, pages 2321–2329. Curran Associates, Inc.",
      "citeRegEx" : "Shrivastava and Li.,? 2014",
      "shortCiteRegEx" : "Shrivastava and Li.",
      "year" : 2014
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
      "author" : [ "Lee Xiong", "Chenyan Xiong", "Ye Li", "Kwok-Fung Tang", "Jialin Liu", "Paul Bennett", "Junaid Ahmed", "Arnold Overwijk" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages (Wang et al., 2017), the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever.",
      "startOffset" : 142,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 (Robertson and Zaragoza, 2009), which can be efficiently implemented with an inverted index.",
      "startOffset" : 93,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction (Karpukhin et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "passages can be pre-computed and indexed, dense passage retrieval can also be done efficiently with vector space search methods during inference time (Shrivastava and Li, 2014).",
      "startOffset" : 150,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "One promising way to reduce the discrepancy is momentum constrastive learning (MoCo) proposed by He et al. (2020). In this method, a pair of fast/slow encoders are used to encode questions and passages, respectively.",
      "startOffset" : 97,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 (Chen et al., 2017).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019).",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019). The challenge in training a dense retriever often lies in how to select negative question-passage pairs. As a small number of randomly generated negative pairs are considered too easy to differentiate, previous work has mainly focused on how to generate “hard” negatives. Karpukhin et al. (2020) selects one negative pair from the top results retrieved by BM25 as hard examples, in addition to one randomly sampled pair.",
      "startOffset" : 69,
      "endOffset" : 656
    }, {
      "referenceID" : 3,
      "context" : "Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019). The challenge in training a dense retriever often lies in how to select negative question-passage pairs. As a small number of randomly generated negative pairs are considered too easy to differentiate, previous work has mainly focused on how to generate “hard” negatives. Karpukhin et al. (2020) selects one negative pair from the top results retrieved by BM25 as hard examples, in addition to one randomly sampled pair. Xiong et al. (2020) uses an iterative approach to gradually produce harder negatives by periodically retrieving top passages for each question using the trained model.",
      "startOffset" : 69,
      "endOffset" : 801
    }, {
      "referenceID" : 3,
      "context" : "Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019). The challenge in training a dense retriever often lies in how to select negative question-passage pairs. As a small number of randomly generated negative pairs are considered too easy to differentiate, previous work has mainly focused on how to generate “hard” negatives. Karpukhin et al. (2020) selects one negative pair from the top results retrieved by BM25 as hard examples, in addition to one randomly sampled pair. Xiong et al. (2020) uses an iterative approach to gradually produce harder negatives by periodically retrieving top passages for each question using the trained model. In addition to finding hard negatives, Ding et al. (2020) also address the problem of false negatives by filtering them out using a more accurate, fused input model.",
      "startOffset" : 69,
      "endOffset" : 1007
    }, {
      "referenceID" : 9,
      "context" : "Momentum contrastive learning (MoCo) is originally proposed by He et al. (2020). He et al.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Momentum contrastive learning (MoCo) is originally proposed by He et al. (2020). He et al. (2020) learns image representations by training",
      "startOffset" : 63,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "It is later improved by constructing better positive pairs (Chen et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "One example of applying momentum contrastive learning in NLP is Chi et al. (2020). In their work, momentum contrastive learning is employed to optimize the InfoNCE lower bound between parallel sentence pairs from different languages.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "for the question, and the maximum inner product search (MIPS) (Shrivastava and Li, 2014) can be used to efficiently retrieve most relevant passages from a large collection of candidates.",
      "startOffset" : 62,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "Previous works such as Xiong et al. (2020) and Ding et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "(2020) and Ding et al. (2020) mainly focus on selecting a few “hard” examples, which hve higher similarity scores with the question and thus contribute more to the sum in the denominator.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "Such formulation poses no problem for the original MoCo paper (He et al., 2020), because their “questions” and “passages” are both images and are interchangeable.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "It effectively mimics the behavior of the “in-batch negative” strategy employed by previous works such as Karpukhin et al. (2020), where the passages in one batch will serve as negatives examples for their questions.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "We use pre-trained uncased BERT-base (Devlin et al., 2019) models as our encoders following Karpukhin et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "We use pre-trained uncased BERT-base (Devlin et al., 2019) models as our encoders following Karpukhin et al. (2020). The question and passage encoders utilize two sets of different parameters",
      "startOffset" : 38,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Better pre-trained models such as Liu et al. (2019) can lead to better retrieval performance, but we choose the uncased BERT-base model for easier comparison with previous work.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "In this work, we only implement a simple method of generating hard examples following Karpukhin et al. (2020): for each positive pair, we add one hard negative example by randomly sampling from top retrieval results using a BM25 retriever.",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "In this work, we only implement a simple method of generating hard examples following Karpukhin et al. (2020): for each positive pair, we add one hard negative example by randomly sampling from top retrieval results using a BM25 retriever. More elaborate methods of finding hard examples such as Xiong et al. (2020) and Ding et al.",
      "startOffset" : 86,
      "endOffset" : 316
    }, {
      "referenceID" : 8,
      "context" : "(2020) and Ding et al. (2020) can also be included, but we leave it to future work.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "Labeling issues can also be the source of false negative examples as pointed out in Ding et al. (2020). In their work, an additional model with fused input is trained to reduce the false negatives.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Following Lee et al. (2019), we extract the passage candidate set from the English Wikipedia dump from Dec.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "Following the pre-processing steps in Karpukhin et al. (2020), we first extract clean texts using pre-processing",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "code from DrQA (Chen et al., 2017), and then split each article into non-overlapping chunks of 100 tokens as the passages for our retrieval task.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "TriviaQA (Joshi et al., 2017) is a set of trivia questions with their answers.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "WebQuestions (WQ) (Berant et al., 2013) is a collection of questions from Google Suggest API with answers from Freebase.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "CuratedTREC (TREC) (Baudiš and Šedivý, 2015) composes of questions from both TREC QA tracks and Web sources.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "1 (Rajpurkar et al., 2016) is original used as a benchmark for reading comprehension.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "We follow the same procedure in Karpukhin et al. (2020) to create positive passages for all datasets.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Note that our numbers are slightly different from Karpukhin et al. (2020) due to small differences in the candidate set or the filtering process.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "Following Karpukhin et al. (2020), we test our model on two settings: a “single” setting where each dataset is trained separately, and a “multi” setting where the training data is combined from NQ, TriviaQA, WQ and TREC (excluding SQuAD).",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "The second baseline is the Deep Passage Retrieval (DPR) model from Karpukhin et al. (2020). We also implement the setting where the candidates",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "We follow their specification in Karpukhin et al. (2020) when re-implementing DPR baselines.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "For inference, we use FAISS (Johnson et al., 2017) for indexing and retrieving passage vectors.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "This is different from previous work Chi et al. (2020), where they observe performance gains with queue size up to 130k.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "Following Karpukhin et al. (2020), we implement a simple BERT based reader",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "pstart and pend is computed by the standard formula in the original BERT paper (Devlin et al., 2019), and the pr is computed by applying softmax over a linear transformation over the vectors of the start tokens of all passages.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "pstart and pend is computed by the standard formula in the original BERT paper (Devlin et al., 2019), and the pr is computed by applying softmax over a linear transformation over the vectors of the start tokens of all passages. We follow the training strategy of Karpukhin et al. (2020), and sample one positive passages and 23 negative passages from the top-100 retrieval results during training.",
      "startOffset" : 80,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "To this end, one possible direction is to employ curriculum learning (Bengio et al., 2009).",
      "startOffset" : 69,
      "endOffset" : 90
    } ],
    "year" : 2021,
    "abstractText" : "Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called cross momentum contrastive learning (xMoCo), for learning a dualencoder model for query-passage matching. Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching, enables using separate encoders for questions and passages. We evaluate our method on various open domain QA datasets, and the experimental results show the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref"
  }
}