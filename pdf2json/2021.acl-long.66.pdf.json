{
  "name" : "2021.acl-long.66.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data",
    "authors" : [ "Wei-Jen Ko", "Ahmed El-Kishky", "Adithya Renduchintala", "Vishrav Chaudhary", "Naman Goyal", "Francisco Guzmán", "Pascale Fung", "Philipp Koehn", "Mona Diab" ],
    "emails" : [ "wjko@utexas.edu,", "aelkishky@twitter.com", "adirendu@fb.com", "vishrav@fb.com", "naman@fb.com", "fguzman@fb.com", "mdiab@fb.com", "pascale@ece.ust.hk,", "phi@jhu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 802–812\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n802"
    }, {
      "heading" : "1 Introduction",
      "text" : "While machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) models, this improvement has been shown to be primarily in well-resourced languages with large available parallel training data.\nHowever with the growth of internet communication and the rise of social media, individuals worldwide have begun communicating and producing content in their native low-resource languages. Many of these low-resource languages are closely related to a high-resource language. One such example are “dialects”: variants of a language traditionally considered oral rather than written. Machine translating dialects using models trained on\n∗This work was conducted while author was working at Facebook AI\nthe formal variant of a language (typically the highresource variant which is sometimes considered the “standardized form”) can pose a challenge due to the prevalence of non standardized spelling as well significant slang vocabulary in the dialectal variant. Similar issues arise from translating a low-resource language using a related high-resource model (e.g., translating Catalan with a Spanish MT model).\nWhile an intuitive approach to better translating low-resource related languages could be to obtain high-quality parallel data. This approach is often infeasible due to lack specialized expertise or bilingual translators. The problems are exacerbated by issues that arise in quality control for low-resource languages (Guzmán et al., 2019). This scarcity motivates our task of learning machine translation models for low-resource languages while leveraging readily available data such as parallel data from a closely related language or monolingual data in the low-resource language.1\nThe use of monolingual data when little to no parallel data is available has been investigated for machine translation. A few approaches involve synthesising more parallel data from monolingual data using backtranslation (Sennrich et al., 2015) or mining parallel data from large multilingual corpora (Tran et al., 2020; El-Kishky et al., 2020b,a; Schwenk et al., 2019). We introduce NMT-Adapt, a zero resource technique that does not need parallel data of any kind on the low resource language.\nWe investigate the performance of NMT-Adapt at translating two directions for each low-resource language: (1) low-resource to English and (2) English to low-resource. We claim that translating into English can be formulated as a typical unsupervised domain adaptation task, with the highresource language as the source domain and the\n1We use low-resource language and dialect or variant interchangeably.\nrelated low-resource, the target domain. We then show that adversarial domain adaptation can be applied to this related language translation task. For the second scenario, translating into the lowresource language, the task is more challenging as it involves unsupervised adaptation of the generated output to a new domain. To approach this task, NMT-Adapt jointly optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation.\nWe test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and (3) Semitic languages, specifically Arabic dialects. We make our code and resources publicly available.2"
    }, {
      "heading" : "2 Related Work",
      "text" : "Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019). However, while zero-shot translation translates between a language pair with no parallel data, there is an assumption that both languages in the target pair have some parallel data with other languages. As such, the system can learn to process both languages. In one work, Currey and Heafield (2019) improved zero-shot translation using monolingual data on the pivot language. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could help zero shot training. We adopt a similar philosophy in our multi-task training to ensure our low-resource target is in the same latent space as the higher-resource language.\nUnsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019).\n2https://github.com/wjko2/NMT-Adapt\nDifferent approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx,Sy), they translate Sx into language Z with the current model to get S′z . Then use (Sy,S ′ z) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervised NMT, our work looks at the problem from a domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z.\nLeveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT.\nSimilar language translation Similar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method assumes that the vocabulary in the main language could be mapped word by word into the dialect vocabulary, and they calculate the corresponding word for substitution using\nlocalized projection. This approach differs from our work in that it relies on the existence of a seed bilingual lexicon to the dialect/similar language. Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination."
    }, {
      "heading" : "3 Method",
      "text" : "We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data. In Section 3.1, we describe how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe how we perform source-domain adaptation to translate a low-resource language into English. Finally, in Section 3.3, we demonstrate how we can leverage these two domain adaptations, to perform iterative backtranslation – further improving translation quality in both directions."
    }, {
      "heading" : "3.1 English to Low-resource",
      "text" : "To translate from English into a low-resource language, NMT-Adapt is initialized with a pretrained mBART model whose pretraining is described in (Liu et al., 2020). Then, as shown in Figure 1, we continue to train the model simultaneously with four tasks inspired by (Lample et al., 2018a) and update the model with a weighted sum of the gradients from different tasks.\nThe language identifying tokens are placed at the same position as in mBART. For the encoder, both high and low-resource language source text, with and without noise, use the language token of the high-resource language [HRL] in the pre-\ntrained mBART. For the decoder, the related high and low-resource languages use their own, different, language tokens. We initialize the language token embedding of the low-resource language with the embedding from the high-resource language token. Task 1: Translation The first task is translation from English into the high-resource language (HRL) which is trained using readily available highresource parallel data. This task aims to transfer high-resource translation knowledge to aid in translating into the low-resource language. We use the cross entropy loss formulated as follows:\nLt = LCE(D(ZEn, [HRL]), XHRL) (1)\n, where ZEn = E(XEn, [En]). (XEn, XHRL) is a parallel sentence pair. E ,D denotes the encoder and decoder functions, which take (input, language token) as parameters. LCE denotes the cross entropy loss.\nTask 2: Denoising Autoencoding For this task, we leverage monolingual text by introducing noise to each sentence, feeding the noised sentence into the encoder, and training the model to generate the original sentence. The noise we use is similar to (Lample et al., 2018a), which includes a random shuffling and masking of words. The shuffling is a random permutation of words, where the position of words is constrained to shift at most 3 words from the original position. Each word is masked with a uniform probability of 0.1. This task aims to learn a feature space for the languages, so that the encoder and decoder could transform between the features and the sentences. This is especially necessary for the low-resource language if it is not already pretrained in mBART. Adding noise was shown to be crucial to translation performance in (Lample et al., 2018a), as it forces the learned feature space to be more robust and contain highlevel semantic knowledge.\nWe train the denoising autoencoding on both the low-resource and related high-resource languages and compute the loss as follows:\nLda = ∑\ni=LRL,HRL\nLCE(D(Zi, [i]), Xi) (2)\n, where Zi = E(N (Xi), [HRL]). Xi is from the monolingual corpus.\nTask 3: Backtranslation For this task, we train on\nEnglish→LRL\nEnglish to low-resource backtranslation data. The aim of this task is to capture a language-modeling effect in the low-resource language. We describe how we obtain this data using the high-resource translation model to bootstrap backtranslation in Section 3.3.\nThe objective used is,\nLbt = LCE(D(Z ′En, [LRL]), XLRL) (3)\n, where Z ′En = E(YEn, [En]). (YEn, XLRL) is an English to low-resource backtranslation pair.\nTask 4: Adversarial Training The final task aims to make the encoder output language-agnostic features. The representation is language agnostic to the noised high and low-resource languages as well as English. Ideally, the encoder output should contain the semantic information of the sentence and little to no language-specific information. This way, any knowledge learned from the English to high-resource parallel data can be directly applied to generating the low-resource language by simply switching the language token during inference, without capturing spurious correlations (Gu et al., 2019a).\nTo adversarially mix the latent space of the encoder among the three languages, we use two critics\n(discriminators). The critics are recurrent networks to ensure that they can handle variable-length text input. Similar to Gu et al. (2019b), the adversarial component is trained using a Wasserstein loss, which is the difference of expectations between the two types of data. This loss minimizes the earth mover’s distance between the distributions of different languages. We compute the loss function as follows:\nLadv1 = E[Disc(ZHRL)]−E[Disc(ZLRL)] (4)\nLadv2 = E[Disc(ZHRL ∪ ZLRL)] −E[Disc(ZEn ∪ Z ′En)] (5)\nAs shown in Equation 4, the first critic is trained to distinguish between the high and low-resource languages. Similarly, in Equation 5, the second critic is trained to distinguish between English and nonEnglish (both high, and low-resource languages).\nFine-tuning with Backtranslation: Finally, we found that after training with the four tasks concurrently, it is beneficial to fine-tune solely using backtranslation for one pass before inference. We posit that this is because while spurious correlations are reduced by the adversarial training, they are not completely eliminated and using solely the\nlanguage tokens to control the output language is not sufficient. By fine-tuning on backtranslation, we are further adapting to the target side and encouraging the output probability distribution of the decoder to better match the desired output language."
    }, {
      "heading" : "3.2 Low-resource to English",
      "text" : "We propose to model translating from the lowresource language to English as a domain adaptation task and design our model based on insights from domain-adversarial neural network (DANN) (Ganin et al., 2017), a domain adaptation technique widely used in many NLP tasks. This time, we train three tasks simultaneously:\nTask 1: Translation We train high-resource to English translation on parallel data with the goal of adapting this knowledge to translate low-resource sentences. We compute this loss as follows:\nLt = LCE(D(ZHRL, [En]), XEn) (6)\n, where ZHRL = E(XHRL, [HRL]).\nTask 2: Backtranslation Low-resource to English backtranslation translation, which we describe in Section 3.3. The objective is as follows:\nLt = LCE(D(Z ′LRL, [En]), XEn) (7)\n, where Z ′LRL = E(YLRL, [HRL]).\nTask 3: Adversarial Training We feed the sentences from the monolingual corpora of the highand low-resource corpora into the encoder, and the encoder output is trained so that its input language cannot be distinguished by a critic. The goal is to encode the low-resource data into a shared space with the high-resource, so that the decoder trained on the translation task can be directly used. No noise was added to the input, since we did not observe an improvement. There is only one recurrent critic, which uses the Wasserstein loss and is computed as follows:\nLadv = E[Disc(ZHRL)]− E[Disc(ZLRL)] (8)\n, where ZLRL = E(XLRL, [HRL]). Similar to the reverse direction, we initialize NMT-Adapt with a pretrained mBART, and use the same language token for high-resource and lowresource in the encoder."
    }, {
      "heading" : "3.3 Iterative Training",
      "text" : "We describe how we can alternate training into/outof English models to create better backtranslation data improving overall quality.\nAlgorithm 1 Iterative training 1: MLRL→En0 ← Train HRL to En model 2: Xmono ← Monolingual LRL corpus 3: XEn ← English sentences in the En-HRL parallel corpus 4: for k in 1,2... do 5: // Generate backtranslation pairs 6: ComputeMLRL→Enk−1 (Xmono) 7: 8: // Train model as in Sec 3.1 9: MEn→LRLk ← trained En to LRL model 10: 11: // Generate backtranslation pairs 12: ComputeMEn→LRLk (XEn) 13: 14: // Train model as in Sec 3.2 15: MLRL→Enk ← trained LRL to En model 16: 17: if Converged then break;\nThe iterative training process is described in Algorithm 1. We first create English to low-resource backtranslation data by fine-tuning mBART on the high-resource to English parallel data. Using this model, we translate monolingual low-resource text into English treating the low-resource sentences as if they were in the high-resource language. The resulting sentence pairs are used as backtranslation data to train the first iteration of our English to low-resource model.\nAfter training English to low-resource, we use the model to translate the English sentences in the English-HRL parallel data into the low-resource language, and use those sentence pairs as backtranslation data to train the first iteration of our low-resource to English model.\nWe then use the first low-resource to English model to generate backtranslation pairs for the second English to low-resource model. We iteratively repeat this process of using our model of one direction to improve the other direction."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We experiment on three groups of languages. In each group, we have a large quantity of parallel training data for one language(high-resource) and no parallel for the related languages to simulate a low-resource scenario.\nOur three groupings include (i) Iberian languages, where we treat Spanish as the high-\nresource and Portuguese and Catalan as related lower-resource languages. (ii) Indic languages where we treat Hindi as the high-resource language, and Marathi, Nepali, and Urdu as lower-resource related languages (iii) Arabic, where we treat Modern Standard Arabic (MSA) as the high-resource, and Egyptian and Levantine Arabic dialects as lowresource. Among the languages, the relationship between Urdu and Hindi is a special setting; while the two languages are mutually intelligible as spoken languages, they are written using different scripts. Additionally, in our experimental setting, all low-resource languages except for Nepali were not included in the original mBART pretraining.\nThe parallel corpus for each language is described in Table 1. Due to the scarcity of any parallel data for a few low-resource languages, we are not able to match the training and testing domains. For monolingual data, we randomly sample 1M sentences for each language from the CC-100 corpus3 (Conneau et al., 2020; Wenzek et al., 2020). For quality control, we filter out sentences if more than 40% of characters in the sentence do not belong to the alphabet set of the language. For quality and memory constraints, we only use sentences with length between 30 and 200 characters.\nCollecting Dialectical Arabic Data While obtaining low-resource monolingual data is relatively straightforward, as language identifiers are often readily available for even low-resource text (Jauhiainen et al., 2019), identifying dialectical data is often less straightforward. This is because many dialects have been traditionally considered oral rather than written, and often lack standardized spelling, significant slang, or even lack of mutual intelligibility from the main language. In general, dialectical data has often been grouped in with the main lan-\n3http://data.statmt.org/cc-100/\nguage in language classifiers. We describe the steps we took to obtain reliable dialectical Arabic monolingual data. As the CC100 corpus does not distinguish between Modern Standard Arabic (MSA) and its dialectical variants, we train a finer-grained classifier that distinguishes between MSA and specific colloquial dialects. We base our language classifier on a BERT model pretrained for Arabic (Safaya et al., 2020) and finetune it for six-way classification: (i) Egyptian, (ii) Levantine, (iii) Gulf, (iv) Maghrebi, (v) Iraqi dialects as well as (vi) the literary Modern Standard Arabic (MSA). We use the data from (Bouamor et al., 2018) and (Zaidan and Callison-Burch, 2011) as training data, and the resulting classifier has an accuracy of 91% on a held-out set. We take our trained Arabic dialect classifier and further classify Arabic monolingual data from CC-100 and select MSA, Levantine and Egyptian sentences as Arabic monolingual data for our experiments."
    }, {
      "heading" : "4.2 Training Details",
      "text" : "We use the RMSprop optimizer with learning rate 0.01 for the critics and the Adam optimizer for the rest of the model. We train our model using eight GPUs and a batch size of 1024 tokens per GPU. We update the parameters once per eight batches. For the adversarial task, the generator is trained once per three updates, and the critic is trained every update.\nEach of the tasks of (i) translation, (ii) backtranslation as well as (iii) LRL and HRL denoising (only for En→LRL direction), have the same number of samples and their cross entropy loss has equal weight. The adversarial loss, Ladv, has the same weight on the critic, while it has a multiplier of −60 on the generator (encoder). This multiplier was tuned to ensure convergence and is negative as it’s opposite to the discriminator loss.\nFor the first iteration, we train 128 epochs from\nEnglish to the low-resource language and 64 iterations from low-resource language to English. For the second iteration we train 55 epochs for both directions. We follow the setting of (Liu et al., 2020) for all other settings and training parameters.\nThe critics consist of four layers: the third layer is a bidirectional GRU and the remaining three are fully connected layers. The hidden layer sizes are 512, 512 and 128 and we use an SELU activation function.\nWe ran experiments on 8-GPUs. Each iteration took less than 3 days and we used publicly available mBART-checkpoints for initialization. GPU memory usage of our method is only slightly larger than mBART. While we introduce additional parameters in discriminators, these additional parameters are insignificant compared to the size of the mBART model."
    }, {
      "heading" : "4.3 Results",
      "text" : "We present results of applying NMT-Adapt to lowresource language translation."
    }, {
      "heading" : "4.3.1 English to Low-Resource",
      "text" : "We first evaluate performance of translating into the low-resource language. We compare the first iteration of NMT-Adapt to the following baseline systems: (i) En→HRL Model: directly using the model trained for En→HRL translation. (ii) Adversarial: Our full model without using the backtranslation objective and without the final fine-tuning.\n(iii) Backtranslation: mBART fine-tuned on backtranslation data created using the HRL→En model. (iv) BT+Adv: Our full model without the final finetuning. (v) BT+Adv+fine-tune: Our full model (NMT-Adapt) as described in Section 3.\nAs seen in Table 2, using solely the adversarial component only, we generally see improvement in the BLEU scores over using the high-resource translate model. This suggests that our proposed method of combining denoising autoencoding with adversarial loss is effective in adapting to a new target output domain.\nAdditionally, we observe a large improvement using only backtranslation data. This demonstrates that using the high-resource translation model to create LRL-En backtranslation data is highly effective for adapting to the low-resource target.\nWe further see that combining adversarial and backtranslation tasks further improve over each individually, showing that the two components are complementary. We also experimented on En-HRL translation with backtranslation but without adversarial loss. However, this yielded much worse results, showing that the improvement is not simply due to multitask learning.\nFor Arabic, backtranslation provides most of the gain, while for Portuguese and Nepali, the adversarial component is more important. For some languages like Marathi, the two components provides small gains individually, but shows a large\nimprovement while combined. For Urdu, we found that backtranslation only using the Hindi model completely fails; this is intuitive as Hindi and Urdu are in completely different scripts and using a Hindi model to translate Urdu results in effectively random backtranslation data. When we attempt to apply models trained with the adversarial task, the model generates sentences with mixed Hindi, Urdu, and English. To ensure our model solely outputs Urdu, we restricted the output tokens by banning all tokens containing English or Devanagari (Hindi) characters. This allowed our model to output valid and semantically meaningful translations. This is an interesting result as it shows that our adversarial mixing allows translating similar languages even if they’re written in different scripts. We report the BLEU score with the restriction. Since the tokens are already restricted, we skip the final fine-tuning step."
    }, {
      "heading" : "4.3.2 Low-resource to English",
      "text" : "Table 3 shows the results of the first iteration from translating from a low-resource language into English. We compare the following systems (i) HRL→En model: directly using the model trained for HRL→En translation. (ii) Adversarial: similar to our full model, but without using the backtranslation objective. (iii) Backtranslation: mBART finetuned on backtranslation data from our full model in the English-LRL direction. (iv) BT+Adv: Our full model.\nFor this direction, we can see that both the backtranslation and the adversarial domain adaptation components are generally effective. The exception is Arabic which may be due to noisiness of our dialect classification compared to low-resource language classification. Another reason could be due to the lack of written standardization for spoken dialects in comparison to low-resource, but standardized languages.\nFor these experiments, we did not apply any special precautions for Urdu on this direction despite it being in a different script from Hindi."
    }, {
      "heading" : "4.3.3 Iterative Training",
      "text" : "Table 4 shows the results of two iterations of training. For languages other than Arabic dialects, the second iteration generally shows improvement over the first iteration, showing that we can leverage an improved model in one direction to further improve the reverse direction. We found that the improvement after the third iteration is marginal.\nWe compare our results with a baseline using the HRL language as a pivot. The baseline uses a fine tuned mBART (Liu et al., 2020) to perform supervised translation between English and the HRL, and uses MASS (Song et al., 2019) to perform unsupervised translation between the HRL and the LRL. The mBART is tuned on the same parallel data used in our method, and the MASS uses the same monolingual data as in our method. For all languages and directions, our method significantly outperforms the pivot baseline."
    }, {
      "heading" : "4.3.4 Comparison with Other Methods",
      "text" : "In table 5, we compare a cross translation method using parallel corpora with multiple languages as auxiliary data (Garcia et al., 2020b) as well as results reported in (Guzmán et al., 2019) and (Liu et al., 2020). All methods use the same test set, English-Hindi parallel corpus, and tokenization for fair comparison. For English to Nepali, NMTAdapt outperforms previous unsupervised methods using Hindi or multilingual parallel data, and is competitive with supervised methods. For Nepali to English direction, our method achieves similar performance to previous unsupervised methods. Note that we use a different tokenization than in table 3 and 4, to be consistent with previous work."
    }, {
      "heading" : "4.3.5 Monolingual Data Ablation",
      "text" : "Table 6 shows the first iteration English to Marathi results while varying the amount of monolingual data used. We see that the BLEU score increased from 11.3 to 16.1 as the number of sentences increased from 10k to 1M showing additional monolingual data significantly improves performance."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented NMT-Adapt, a novel approach for neural machine translation of low-resource languages which assumes zero parallel data or bilingual lexicon in the low-resource language. Utilizing parallel data in a similar high resource language as well as monolingual data in the low-resource language, we apply unsupervised adaptation to facilitate translation to and from the low-resource language. Our approach combines several tasks including adversarial training, denoising language modeling, and iterative back translation to facilitate the adaptation. Experiments demonstrate that this combination is more effective than any task on its own and generalizes across many different language groups."
    } ],
    "references" : [ {
      "title" : "Consistency by agreement in zero-shot neural machine translation",
      "author" : [ "Maruan Al-Shedivat", "Ankur P. Parikh." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Al.Shedivat and Parikh.,? 2019",
      "shortCiteRegEx" : "Al.Shedivat and Parikh.",
      "year" : 2019
    }, {
      "title" : "Tico-19: the translation initiative for covid-19",
      "author" : [ "Eric Paquin", "Grace Tang", "Sylwia Tur." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Paquin et al\\.,? 2020",
      "shortCiteRegEx" : "Paquin et al\\.",
      "year" : 2020
    }, {
      "title" : "The missing ingredient in zero-shot neural machine translation",
      "author" : [ "Naveen Arivazhagan", "Ankur Bapna", "Orhan Firat", "Roee Aharoni", "Melvin Johnson", "Wolfgang Macherey." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Arivazhagan et al\\.,? 2019",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised neural machine translation",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre", "Kyunghyun Cho." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Artetxe et al\\.,? 2018",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation by jointly",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "The madar arabic dialect corpus and lexicon",
      "author" : [ "Houda Bouamor", "Nizar Habash", "Mohammad Salameh", "Wajdi Zaghouani", "Owen Rambow", "Dana Abdulrahim", "Ossama Obeid", "Salam Khalifa", "Fadhl Eryani", "Alexander Erdmann", "Kemal Oflazer." ],
      "venue" : "RECL.",
      "citeRegEx" : "Bouamor et al\\.,? 2018",
      "shortCiteRegEx" : "Bouamor et al\\.",
      "year" : 2018
    }, {
      "title" : "Bolt arabic discussion forum parallel training data",
      "author" : [ "Song Chen", "Jennifer Tracey", "Christopher Walker", "Stephanie Strassel." ],
      "venue" : "LDC2019T01.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "A teacher-student framework for zeroresource neural machine translation",
      "author" : [ "Yun Chen", "Yang Liu", "Yong Cheng", "Victor O.K. Li." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Zeroresource neural machine translation with monolingual pivot data",
      "author" : [ "Anna Currey", "Kenneth Heafield." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation.",
      "citeRegEx" : "Currey and Heafield.,? 2019",
      "shortCiteRegEx" : "Currey and Heafield.",
      "year" : 2019
    }, {
      "title" : "A massive collection of cross-lingual web-document pairs",
      "author" : [ "Ahmed El-Kishky", "Vishrav Chaudhary", "Francisco Guzmán", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "El.Kishky et al\\.,? 2020a",
      "shortCiteRegEx" : "El.Kishky et al\\.",
      "year" : 2020
    }, {
      "title" : "Searching the web for crosslingual parallel data",
      "author" : [ "Ahmed El-Kishky", "Philipp Koehn", "Holger Schwenk." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages",
      "citeRegEx" : "El.Kishky et al\\.,? 2020b",
      "shortCiteRegEx" : "El.Kishky et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond english-centric multilingual machine translation",
      "author" : [ "Angela Fan", "Shruti Bhosale", "Holger Schwenk", "Zhiyi Ma", "Ahmed El-Kishky", "Siddharth Goyal", "Mandeep Baines", "Onur Celebi", "Guillaume Wenzek", "Vishrav Chaudhary" ],
      "venue" : null,
      "citeRegEx" : "Fan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2020
    }, {
      "title" : "A multilingual view of unsupervised machine translation",
      "author" : [ "Xavier Garcia", "Pierre Foret", "Thibault Sellam", "Ankur Parikh." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Garcia et al\\.,? 2020a",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2020
    }, {
      "title" : "Harnessing multilinguality in unsupervised machine translation for rare languages",
      "author" : [ "Xavier Garcia", "Aditya Siddhant", "Orhan Firat", "Ankur P. Parikh." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Garcia et al\\.,? 2020b",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved zero-shot neural machine translation via ignoring spurious correlations",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor O.K. Li." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gu et al\\.,? 2019a",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. DialogWAE: Multimodal response generation with conditional wasserstein autoencoder",
      "author" : [ "Xiaodong Gu", "Kyunghyun Cho", "Jung-Woo Ha", "Sunghun Kim" ],
      "venue" : null,
      "citeRegEx" : "Gu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "The flores evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english",
      "author" : [ "Francisco Guzmán", "Peng-Jen Chen", "Myle Ott", "Juan Pino", "Guillaume Lample", "Philipp Koehn", "Vishrav Chaudhary", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Guzmán et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2019
    }, {
      "title" : "The amara corpus: Building resources for translating the web’s educational content",
      "author" : [ "Francisco Guzman", "Hassan Sajjad", "A Abdelali", "S Vogel." ],
      "venue" : "IWSLT.",
      "citeRegEx" : "Guzman et al\\.,? 2013",
      "shortCiteRegEx" : "Guzman et al\\.",
      "year" : 2013
    }, {
      "title" : "Synthetic data for neural machine translation of spoken-dialects",
      "author" : [ "Hany Hassan", "Mostafa Elaraby", "Ahmed Tawfik." ],
      "venue" : "Proceedings of the 14th International Workshop on Spoken Language Translation.",
      "citeRegEx" : "Hassan et al\\.,? 2017",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic language identification in texts: A survey",
      "author" : [ "Tommi Jauhiainen", "Marco Lui", "Marcos Zampieri", "Timothy Baldwin", "Krister Lindén." ],
      "venue" : "Journal of Artificial Intelligence Research, 65:675–782.",
      "citeRegEx" : "Jauhiainen et al\\.,? 2019",
      "shortCiteRegEx" : "Jauhiainen et al\\.",
      "year" : 2019
    }, {
      "title" : "The iit bombay english-hindi parallel corpus",
      "author" : [ "Anoop Kunchukuttan", "Pratik Mehta", "Pushpak Bhattacharyya." ],
      "venue" : "LREC.",
      "citeRegEx" : "Kunchukuttan et al\\.,? 2018",
      "shortCiteRegEx" : "Kunchukuttan et al\\.",
      "year" : 2018
    }, {
      "title" : "Adapting multilingual neural machine translation to unseen languages",
      "author" : [ "Surafel M. Lakew", "Alina Karakanta", "Marcello Federico", "Matteo Negri", "Marco Turchi." ],
      "venue" : "IWSLT.",
      "citeRegEx" : "Lakew et al\\.,? 2019",
      "shortCiteRegEx" : "Lakew et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation into language varieties",
      "author" : [ "Surafel Melaku Lakew", "Aliia Erofeeva", "Marcello Federico." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation.",
      "citeRegEx" : "Lakew et al\\.,? 2018",
      "shortCiteRegEx" : "Lakew et al\\.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Unsupervised machine translation using monolingual corpora only",
      "author" : [ "Guillaume Lample", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "In ICLR",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "2018b. Phrase-based & neural unsupervised machine translation",
      "author" : [ "Guillaume Lample", "Myle Ott", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "Reference language based unsupervised neural machine translation",
      "author" : [ "Zuchao Li", "Hai Zhao", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TACL.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Rapid adaptation of neural machine translation to new languages",
      "author" : [ "Graham Neubig", "Junjie Hu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Neubig and Hu.,? 2018",
      "shortCiteRegEx" : "Neubig and Hu.",
      "year" : 2018
    }, {
      "title" : "Deciphering related languages",
      "author" : [ "Nima Pourdamghani", "Kevin Knight." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pourdamghani and Knight.,? 2017",
      "shortCiteRegEx" : "Pourdamghani and Knight.",
      "year" : 2017
    }, {
      "title" : "When and why are pre-trained word embeddings useful for neural machine translation",
      "author" : [ "Ye Qi", "Sachan Devendra", "Felix Matthieu", "Padmanabhan Sarguna", "Neubig Graham." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Qi et al\\.,? 2018",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2018
    }, {
      "title" : "Bolt arabic discussion forum parallel training data",
      "author" : [ "Raytheon." ],
      "venue" : "LDC2012T09.",
      "citeRegEx" : "Raytheon.,? 2012",
      "shortCiteRegEx" : "Raytheon.",
      "year" : 2012
    }, {
      "title" : "Kuisail at semeval-2020 task 12: Bert-cnn for offensive speech identification in social media",
      "author" : [ "Ali Safaya", "Moutasem Abdullatif", "Deniz Yuret." ],
      "venue" : "14th International Workshop on Semantic Evaluation (SemEval).",
      "citeRegEx" : "Safaya et al\\.,? 2020",
      "shortCiteRegEx" : "Safaya et al\\.",
      "year" : 2020
    }, {
      "title" : "Ccmatrix: Mining billions of high-quality parallel sentences on the web",
      "author" : [ "Holger Schwenk", "Guillaume Wenzek", "Sergey Edunov", "Edouard Grave", "Armand Joulin." ],
      "venue" : "arXiv preprint arXiv:1911.04944.",
      "citeRegEx" : "Schwenk et al\\.,? 2019",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1511.06709.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Leveraging monolingual data with self-supervision for multilingual neural machine translation",
      "author" : [ "Aditya Siddhant", "Ankur Bapna", "Yuan Cao", "Orhan Firat", "Mia Chen", "Sneha Kudungunta", "Naveen Arivazhagan", "Yonghui Wu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Siddhant et al\\.,? 2020",
      "shortCiteRegEx" : "Siddhant et al\\.",
      "year" : 2020
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "ICML.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Parallel data, tools and interfaces in opus",
      "author" : [ "Jorg Tiedemann." ],
      "venue" : "LREC.",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Cross-lingual retrieval for iterative self-supervised training",
      "author" : [ "Chau Tran", "Yuqing Tang", "Xian Li", "Jiatao Gu." ],
      "venue" : "arXiv preprint arXiv:2006.09526.",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised neural dialect translation with commonality and diversity modeling",
      "author" : [ "Yu Wan", "Baosong Yang", "Derek F. Wong", "Lidia S. Chao", "Haihua Du", "Ben C.H. Ao." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual supervision improves unsupervised neural machine translation",
      "author" : [ "Mingxuan Wang", "Hongxiao Bai", "Hai Zhao", "Lei Li." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "CCNet: Extracting high quality monolingual datasets from web crawl data",
      "author" : [ "Guillaume Wenzek", "Marie-Anne Lachaux", "Alexis Conneau", "Vishrav Chaudhary", "Francisco Guzmán", "Armand Joulin", "Edouard Grave." ],
      "venue" : "Proceedings of the 12th Lan-",
      "citeRegEx" : "Wenzek et al\\.,? 2020",
      "shortCiteRegEx" : "Wenzek et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalized data augmentation for low-resource translation",
      "author" : [ "Mengzhou Xia", "Xiang Kong", "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "ACL.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content",
      "author" : [ "Omar F. Zaidan", "Chris Callison-Burch." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zaidan and Callison.Burch.,? 2011",
      "shortCiteRegEx" : "Zaidan and Callison.Burch.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "While machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) models, this improvement has been shown to be primarily in well-resourced languages with large available parallel training data.",
      "startOffset" : 118,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "While machine translation (MT) has made incredible strides due to the advent of deep neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) models, this improvement has been shown to be primarily in well-resourced languages with large available parallel training data.",
      "startOffset" : 118,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "The problems are exacerbated by issues that arise in quality control for low-resource languages (Guzmán et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 35,
      "context" : "A few approaches involve synthesising more parallel data from monolingual data using backtranslation (Sennrich et al., 2015) or mining parallel data from large multilingual corpora (Tran et al.",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 40,
      "context" : ", 2015) or mining parallel data from large multilingual corpora (Tran et al., 2020; El-Kishky et al., 2020b,a; Schwenk et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : ", 2015) or mining parallel data from large multilingual corpora (Tran et al., 2020; El-Kishky et al., 2020b,a; Schwenk et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019).",
      "startOffset" : 83,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019).",
      "startOffset" : 83,
      "endOffset" : 154
    }, {
      "referenceID" : 24,
      "context" : "In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 180
    }, {
      "referenceID" : 37,
      "context" : "In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 197
    }, {
      "referenceID" : 27,
      "context" : "com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 197
    }, {
      "referenceID" : 42,
      "context" : "com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "To translate from English into a low-resource language, NMT-Adapt is initialized with a pretrained mBART model whose pretraining is described in (Liu et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 15,
      "context" : "This way, any knowledge learned from the English to high-resource parallel data can be directly applied to generating the low-resource language by simply switching the language token during inference, without capturing spurious correlations (Gu et al., 2019a).",
      "startOffset" : 241,
      "endOffset" : 259
    }, {
      "referenceID" : 18,
      "context" : "Spanish Iberian QED (Guzman et al., 2013) 694k N/A - CC-100 1M Catalan Iberian N/A - Global Voices (Tiedemann, 2012) 15k CC-100 1M Portuguese Iberian N/A - TED (Qi et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 39,
      "context" : ", 2013) 694k N/A - CC-100 1M Catalan Iberian N/A - Global Voices (Tiedemann, 2012) 15k CC-100 1M Portuguese Iberian N/A - TED (Qi et al.",
      "startOffset" : 65,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : ", 2013) 694k N/A - CC-100 1M Catalan Iberian N/A - Global Voices (Tiedemann, 2012) 15k CC-100 1M Portuguese Iberian N/A - TED (Qi et al., 2018) 8k CC-100 1M",
      "startOffset" : 126,
      "endOffset" : 143
    }, {
      "referenceID" : 21,
      "context" : "Hindi Indic IIT Bombay (Kunchukuttan et al., 2018) 769k N/A - CC-100 1M Marathi Indic N/A - TICO-19 (Anastasopoulos et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : ", 2020) 2k CC-100 1M Nepali Indic N/A - FLoRes (Guzmán et al., 2019) 3k CC-100 1M Urdu Indic N/A - TICO-19 (Anastasopoulos et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "MSA Arabic QED (Guzman et al., 2013) 465k N/A - CC-100 1M Egyptian Ar.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "Arabic N/A - Web text (Raytheon, 2012) 11k CC-100 1M",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "For monolingual data, we randomly sample 1M sentences for each language from the CC-100 corpus3 (Conneau et al., 2020; Wenzek et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 139
    }, {
      "referenceID" : 43,
      "context" : "For monolingual data, we randomly sample 1M sentences for each language from the CC-100 corpus3 (Conneau et al., 2020; Wenzek et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Collecting Dialectical Arabic Data While obtaining low-resource monolingual data is relatively straightforward, as language identifiers are often readily available for even low-resource text (Jauhiainen et al., 2019), identifying dialectical data is often less straightforward.",
      "startOffset" : 191,
      "endOffset" : 216
    }, {
      "referenceID" : 33,
      "context" : "We base our language classifier on a BERT model pretrained for Arabic (Safaya et al., 2020) and finetune it for six-way classification: (i) Egyptian, (ii) Levantine, (iii) Gulf, (iv) Maghrebi, (v) Iraqi dialects as well as (vi) the literary Modern Standard Arabic (MSA).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "We use the data from (Bouamor et al., 2018) and (Zaidan and Callison-Burch, 2011) as training data, and the resulting classifier has an accuracy of 91% on a held-out set.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 45,
      "context" : ", 2018) and (Zaidan and Callison-Burch, 2011) as training data, and the resulting classifier has an accuracy of 91% on a held-out set.",
      "startOffset" : 12,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "The baseline uses a fine tuned mBART (Liu et al., 2020) to perform supervised translation between English and the HRL, and uses MASS (Song et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : ", 2020) to perform supervised translation between English and the HRL, and uses MASS (Song et al., 2019) to perform unsupervised translation between the HRL and the LRL.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "In table 5, we compare a cross translation method using parallel corpora with multiple languages as auxiliary data (Garcia et al., 2020b) as well as results reported in (Guzmán et al.",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : ", 2020b) as well as results reported in (Guzmán et al., 2019) and (Liu et al.",
      "startOffset" : 40,
      "endOffset" : 61
    } ],
    "year" : 2021,
    "abstractText" : "The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a lowresource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for lowresource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.",
    "creator" : "LaTeX with hyperref"
  }
}