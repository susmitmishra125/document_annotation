{
  "name" : "2021.acl-long.353.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "authors" : [ "Xiang Lisa Li", "Percy Liang" ],
    "emails" : [ "xlisali@stanford.edu", "pliang@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4582–4597\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4582"
    }, {
      "heading" : "1 Introduction",
      "text" : "Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform downstream tasks (e.g., summarization), but it requires updating and storing all the parameters of the LM. Consequently, to build and deploy NLP systems that rely on large pretrained LMs, one currently needs to store a modified copy of all the LM parameters for each task. This can be prohibitively expensive given the size of current LMs; for example, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al., 2020).\nA natural approach to this problem is lightweight fine-tuning, which freezes most of the pretrained parameters and only tunes a smaller set of parameters. For example, adapter-tuning (Rebuffi et al.,\n2017; Houlsby et al., 2019) inserts additional taskspecific layers between the layers of pretrained language models. Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attaining comparable performance with fine-tuning while adding only around 2–4% task-specific parameters (Houlsby et al., 2019; Lin et al., 2020).\nAt the limit, GPT-3 (Brown et al., 2020) can be deployed using in-context learning, which is a form of prompting, without modifying any LM parameters. In in-context learning, Brown et al. (2020) prepend a natural language task instruction (e.g., TL;DR for summarization) and a few examples to the task input, and then generate the task output from the LM. However, since Transformers can only condition on a bounded-length context (e.g., 2048 tokens for GPT-3), in-context learning is restricted to very small training sets.\nIn this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting. Consider the task of generating a textual description of a data table, as shown in Figure 1, where the task input is a linearized table (e.g., “name: Starbucks | type: coffee shop”) and the output is a textual description (e.g., “Starbucks serves coffee.”). Prefix-tuning prepends a sequence of continuous task-specific vectors to the input, which we call a prefix, depicted by red blocks in Figure 1 (bottom). To generate each token, the LM can attend to the prefix as if it were a sequence of “virtual tokens”, but unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens. In contrast to fine-tuning in Figure 1 (top), which updates all LM parameters and thus requires storing a tuned copy of the model for each task, prefix-tuning only optimizes the prefix. Consequently, we only need to store one copy of the large LM and a learned task-specific prefix, yielding a very small overhead for each additional task (e.g., 250K parameters for table-to-text).\nIn contrast to full fine-tuning, prefix-tuning is also modular: we train an upstream prefix which steers an unmodified LM, and therefore, a single LM can support many tasks at once. In the context of personalization where the tasks correspond to users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we would have a separate prefix for each user trained only on that user’s data, thereby avoiding data cross-contamination. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch, something that is not possible with other lightweight fine-tuning approaches like adaptertuning.\nWe evaluate prefix-tuning on table-to-text generation using GPT-2 and abstractive summarization using BART. In terms of storage, prefix-tuning stores 1000x fewer parameters than full fine-tuning. In terms of performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text (§6.1), while prefix-tuning suffers a small degradation for summarization (§6.2). In low-data settings, prefix-tuning outperforms finetuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning in principle can be applied to other generation tasks and pretrained models, such as masked LMs.\nLightweight fine-tuning. Prefix-tuning falls under the broad class of lightweight fine-tuning methods, which freeze most of the pretrained parameters and only tune a smaller set of parameters. The key question is how to augment the LM architecture and decide which subset of pretrained parameters to tune. One line of research learns a task-specific parameter mask (Zhao et al., 2020; Radiya-Dixit and Wang, 2020). Another line of research inserts new modules with trainable parameters. For example, Zhang et al. (2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020). Compared to this line of work, which tunes around 3.6% of the LM parameters, our method obtains a further 30x reduction in task-specific parameters, tuning only 0.1% while maintaining comparable performance on table-to-text tasks.\nPrompting. Prompting is a way of leveraging a pretrained LM by prepending instructions and a few examples to the task input and generating the task output from the LM. For autoregressive LMs, the most successful form of prompting is GPT-3’s in-context learning (Brown et al., 2020), which uses manually designed prompts to adapt its generation for different tasks in few-shot settings. For masked LMs like BERT and RoBERTa (Liu et al.,\n2019), prompt engineering has been explored for natural language understanding tasks (Jiang et al., 2020; Schick and Schütze, 2020). For example, AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from BERT and RoBERTa. In contrast with AutoPrompt, our method optimizes continuous prefixes, which are more expressive (§7.2); moreover, we focus on language generation tasks.\nContinuous vectors have been used to steer LMs; for example, Subramani et al. (2020) showed that a pretrained LSTM language model can reconstruct arbitrary sentences by optimizing a continuous vector for each sentence, making the vector inputspecific. In contrast, prefix-tuning optimizes a taskspecific prefix that applies to all instances of that task. As a result, unlike the previous work whose application is limited to sentence reconstruction, prefix-tuning can be applied to NLG tasks.\nControllable generation. Controllable generation aims to steer a pretrained language model to match a sentence-level attribute (e.g., positive sentiment or sports). Such control can happen at training time: Keskar et al. (2019) pretrains the language model (CTRL) to condition on metadata such as keywords or URLs. The control can also happen at decoding time, by weighted decoding (GeDi, Krause et al., 2020) or iteratively updating the past activations (PPLM, Dathathri et al., 2020). However, there is no straightforward way to apply these controllable generation techniques to enforce fine-grained control over generated contents, as demanded by tasks like table-to-text and summarization.\nP*-tuning. Prefix tuning is an instance of a new class of methods that has emerged, which we call p*-tuning (since the other prominent instances, ptuning and prompt-tuning, also start with p), all based on the idea of optimizing a continuous prefix or prompt. Concurrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves\nGPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes as the model size grows."
    }, {
      "heading" : "3 Problem Statement",
      "text" : "Consider a conditional generation task where the input x is a context and the output y is a sequence of tokens. We focus on two tasks, shown in Figure 2 (right): In table-to-text, x corresponds to a linearized data table and y is a textual description; in summarization, x is an article and y is a summary."
    }, {
      "heading" : "3.1 Autoregressive LM",
      "text" : "Assume we have an autoregressive neural language model pφ(y | x) parametrized by φ (e.g., GPT-2; Radford et al., 2019). As shown in Figure 2 (top), let z = [x; y] be the concatenation of x and y; let Xidx denote the sequence of indices that corresponds to x, and Yidx denote the same for y.\nThe activation vector at time step i is hi ∈ Rd, where hi = [h (1) i ; · · · ;h (n) i ] is a concatenation of all activation layers at this time step, and h(j)i is the activation vector of the j-th layer at time step i.1\nAn autoregressive neural LM computes hi as a function of zi and the past activations in its left context, as follows:\nhi = LMφ(zi, h<i), (1)\nwhere the last layer of hi is used to compute the distribution for the next token: pφ(zi+1 | h≤i) = softmax(Wφ h (n) i ) and Wφ is a matrix that maps h (n) i to logits over the vocabulary."
    }, {
      "heading" : "3.2 Encoder-Decoder Architecture",
      "text" : "We can also use an encoder-decoder architecture (e.g., BART; Lewis et al., 2020) to model pφ(y | x), where x is encoded by the bidirectional encoder, and the decoder predicts y autoregressively (conditioned on the encoded x and its left context). We use the same indexing and activation notation, as shown in Figure 2 (bottom): each hi for i ∈ Xidx is computed by the a bidirectional encoder; each hi for i ∈ Yidx is computed by an autoregressive decoder using the same equation (1).\n1In GPT-2, h(n)i consists of a key-value pair, and the dimension of each key and value is 1024."
    }, {
      "heading" : "3.3 Fine-tuning",
      "text" : "In the full fine-tuning framework, we initialize with the pretrained parameters φ. Here pφ is a trainable language model distribution and we perform gradient updates on the following log-likelihood objective:\nmax φ log pφ(y | x) = max φ ∑ i∈Yidx log pφ(zi | h<i).\n(2)"
    }, {
      "heading" : "4 Prefix-Tuning",
      "text" : "We propose prefix-tuning as an alternative to full fine-tuning for conditional generation tasks. We first provide intuition in §4.1 before defining our method formally in §4.2."
    }, {
      "heading" : "4.1 Intuition",
      "text" : "Prompting has demonstrated that conditioning on a proper context can steer the LM without changing its parameters. For example, if we want the LM to generate a word (e.g., Obama), we can prepend its common collocations as context (e.g., Barack), and the LM will assign much higher probability to the desired word. Extending this intuition beyond generating a single word or sentence, we want to find a context that steers the LM to solve an NLG task. Intuitively, the context could influence the encoding of the task input x by guiding what to extract from x, and it could influence the generation of the task output y by steering the next token distribution. However, it’s non-obvious whether such a context exists. Using natural language task instructions (e.g., “summarize the following table in one sentence”) for the context might guide a human to\nsolve the task, but this fails for moderately-sized pretrained LMs.2 Optimizing over the discrete instructions might help, but discrete optimization is computationally challenging.\nInstead of optimizing over discrete tokens, we can optimize the instruction as continuous word embeddings, whose effects will be propagated upward to all Transformer activation layers and rightward to subsequent tokens. This is strictly more expressive than a discrete prompt which is constrained to the embeddings of real words. Prefix-tuning goes one step further in increasing expressivity by optimizing the activations of all the layers, not just the embedding layer. As another benefit, prefixtuning can directly modify representations deeper in the network, therefore, avoiding long computation paths across the depth of the network."
    }, {
      "heading" : "4.2 Method",
      "text" : "Prefix-tuning prepends a prefix for an autoregressive LM to obtain z = [PREFIX;x; y], or prepends prefixes for both encoder and decoder to obtain z = [PREFIX;x; PREFIX′; y], as shown in Figure 2. Here, Pidx denotes the sequence of prefix indices, and we use |Pidx| to denote the length of the prefix.\nWe follow the recurrence relation in equation (1), except that the activations of the prefix indices are free parameters, given by a matrix Pθ (parametrized by θ) of dimension |Pidx| × dim(hi).\nhi = { Pθ[i, :], if i ∈ Pidx, LMφ(zi, h<i), otherwise.\n(3)\n2In our preliminary experiments, GPT-2 and BART fail in this setting; the only exception is GPT-3.\nThe training objective is the same as equation (2), but the set of trainable parameters changes: the language model parameters φ are fixed and the prefix parameters θ are the only trainable parameters.\nHere, each hi is a function of the trainable Pθ. When i ∈ Pidx, this is clear because hi copies directly from Pθ. When i 6∈ Pidx, hi still depends on Pθ, because the prefix activations are always in the left context and will therefore affect any activations to the right."
    }, {
      "heading" : "4.3 Parametrization of Pθ",
      "text" : "Empirically, directly updating the Pθ parameters leads to unstable optimization and a slight drop in performance.3 So we reparametrize the matrix Pθ[i, :] = MLPθ(P ′θ[i, :]) by a smaller matrix (P ′ θ) composed with a large feedforward neural network (MLPθ). Now, the trainable parameters include P ′θ and the parameters of MLPθ. Note that Pθ and P ′θ has the same number of rows (i.e., the prefix length), but different number of columns.4\nOnce training is complete, these reparametrization parameters can be dropped, and only the prefix (Pθ) needs to be saved."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Metrics",
      "text" : "We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1).\nFor the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L."
    }, {
      "heading" : "5.2 Methods",
      "text" : "For table-to-text generation, we compare prefixtuning with three other methods: full fine-tuning\n3We find in preliminary experiments that directly optimizing the prefix is very sensitive to initialization.\n4Pθ has dimensions |Pidx| × dim(hi) while Pθ has dimensions |Pidx| × k. We choose k = 512 for table-to-text and 800 for summarization. MLPθ maps from k to dim(hi).\n(FT-FULL), fine-tuning only the top 2 layers (FTTOP2), and adapter-tuning (ADAPTER).5 We also report the current state-of-the-art results on these datasets: On E2E, Shen et al. (2019) uses a pragmatically informed model without pretraining. On WebNLG, Kale (2020) fine-tunes T5-large. On DART, no official models trained on this dataset version are released.6 For summarization, we compare against fine-tuning BART (Lewis et al., 2020)."
    }, {
      "heading" : "5.3 Architectures and Hyperparameters",
      "text" : "For table-to-text, we use GPT-2MEDIUM and GPT2LARGE. For summarization, we use BARTLARGE. Our implementation is based on the Hugging Face Transformers (Wolf et al., 2020).\nAt training time, we use the AdamW optimizer (Loshchilov and Hutter, 2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup. The hyperparameters we tune include the number of epochs, batch size, learning rate, and prefix length. Hyperparameter details are in the appendix. The default setting is 10 epochs, batch size 5, learning rate 5 ·10−5 and prefix length 10. The table-to-text models are trained on TITAN Xp or GeForce GTX TITAN X machines. Prefixtuning takes 0.2 hours per epoch to train on 22K examples, whereas fine-tuning takes around 0.3 hours per epoch. The summarization models are trained on Tesla V100 machines, taking 1.25 hours per epoch on the XSUM dataset. For time efficiency, prefix-tuning is around 30% faster than fine-tuning. For GPU memory efficiency, prefixtuning with batchsize 1 takes 18% of the total GPU memory, whereas fine-tuning takes 50%.\nAt decoding time, for table-to-text, we use beam search with beam size 5. For summarization, we use beam size 6 and length normalization 0.8. Decoding takes 1.2 seconds per sentence (without\n5Same implementation as Lin et al. (2020). 6The official benchmark model is trained on v.1.0.0 while\nthe release dataset is v1.1.1.\nbatching) for table-to-text, and 2.6 seconds per batch (using a batch size of 10) for summarization."
    }, {
      "heading" : "6 Main Results",
      "text" : ""
    }, {
      "heading" : "6.1 Table-to-text Generation",
      "text" : "We find that by updating only 0.1% task-specific parameters,7 prefix-tuning is effective in table-to-text generation, outperforming other lightweight baselines (ADAPTER and FT-TOP2) even by updating 30x fewer parameters and achieving a comparable performance with (full) fine-tuning. This trend holds for all datasets: E2E, WebNLG,8 and DART.\nIf we match the number of parameters for prefixtuning and adapter-tuning to be 0.1%, Table 2 shows that prefix-tuning is significantly better than ADAPTER (0.1%), attaining 4.1 BLEU improvement per dataset on average. Even when we compare with fine-tuning (100%) and adapter-tuning (3.0%), which update significantly more parameters than prefix-tuning, prefix-tuning still achieves results comparable or better than those two systems. This demonstrates that prefix-tuning is more Pareto efficient than adapter-tuning, significantly reducing parameters while improving generation quality.\nAdditionally, attaining good performance on DART suggests that prefix-tuning can generalize to tables with diverse domains and a large number of relations. We will delve deeper into extrapolation performance (i.e., generalization to unseen categories or topics) in §6.4.\nIn summary, prefix-tuning is an effective and space-efficient method to adapt GPT-2 to table-totext generation. It also maintains the performance gains when scaling up to GPT-2LARGE, suggesting it has the potential to scale to even larger models with a similar architecture, like GPT-3."
    }, {
      "heading" : "6.2 Summarization",
      "text" : "As shown in Table 3, with 2% parameters, prefixtuning obtains slightly lower performance than finetuning (36.05 vs. 37.25 in ROUGE-L). With only 0.1% parameters, prefix-tuning underperforms full fine-tuning (35.05 vs. 37.25). There are several differences between XSUM and the three table-totext datasets which could account for why prefixtuning has comparative advantage in table-to-text:\n7250K for E2E, 250K for WebNLG, and 500K for DART versus 345M GPT-2 parameters.\n8The S,U,A columns in WebNLG represents SEEN, UNSEEN, and ALL respectively; SEEN categories appear at training time; UNSEEN categories only appears at test time; and ALL is the combination of the two.\n(1) XSUM contains 4x more examples than the three table-to-text datasets on average; (2) the input articles are 17x longer than the linearized table input of table-to-text datasets on average; (3) summarization is more complex than table-to-text because it requires selecting key contents from an article."
    }, {
      "heading" : "6.3 Low-data Setting",
      "text" : "Based on the results from table-to-text (§6.1) and summarization (§6.2), we observe that prefixtuning has a comparative advantage when the number of training examples is smaller. To explore the low-data setting more systematically, we subsample the full dataset (E2E for table-to-text and XSUM for summarization) to obtain small datasets of size {50, 100, 200, 500}. For each size, we sample 5 different datasets and average over 2 training random seeds. Thus, we average over 10 models for each low-data setting.9\nFigure 3 (right) shows that prefix-tuning outperforms fine-tuning in low-data regimes by 2.9 BLEU on average, in addition to requiring much fewer parameters, but the gap narrows as the dataset size increases.\nQualitatively, Figure 3 (left) shows 8 examples generated by both prefix-tuning and fine-tuning models trained on different data levels. While both methods tend to undergenerate (missing table contents) in low data regimes, prefix-tuning tends to be more faithful than fine-tuning. For example, finetuning (100, 200)10 falsely claims a low customer rating while the true rating is average, whereas prefix-tuning (100, 200) generates a description that is faithful to the table."
    }, {
      "heading" : "6.4 Extrapolation",
      "text" : "We now investigate extrapolation performance to unseen topics for both table-to-text and summarization. In order to construct an extrapolation setting, we split the existing datasets so that training and test cover different topics. For table-to-text, the WebNLG dataset is labeled with table topics. There are 9 categories that appear in training and dev, denoted as SEEN and 5 categories that only appear at test time, denoted as UNSEEN. So we evaluate extrapolation by training on the SEEN categories and testing on the UNSEEN categories. For summarization, we construct two extrapolation data splits:\n9We also sample a dev split (with dev size = 30% × training size) for each training set. We use the dev split to choose hyperparameters and perform early stopping.\n10The number in the parenthesis refers to the training size.\nSource name : The Eagle | type : coffee shop | food : Chinese | price : cheap | customer rating : average | area : riverside | family friendly : no | near : Burger King Prefix (50) The Eagle is a cheap Chinese coffee shop located near Burger King. Prefix (100) The Eagle is a cheap coffee shop located in the riverside near Burger King. It has average customer ratings. Prefix (200) The Eagle is a cheap Chinese coffee shop located in the riverside area near Burger King. It has average customer ratings. Prefix (500) The Eagle is a coffee shop that serves Chinese food. It is located in the riverside\narea near Burger King. It has an average customer rating and is not family friendly.\nFT (50) The Eagle coffee shop is located in the riverside area near Burger King. FT (100) The Eagle is a cheap coffee shop near Burger King in the riverside area. It has a low customer rating and is not family friendly. FT (200) The Eagle is a cheap Chinese coffee shop with a low customer rating. It is located near Burger King in the riverside area. FT (500) The Eagle is a cheap Chinese coffee shop with average customer ratings. It is\nlocated in the riverside area near Burger King.\n100 200 300 400 500 training data size\n32\n33\n34\n35\n36\n37\nRO UG\nE1\nFT-full Prefix\n100 200 300 400 500 training data size\n10\n11\n12\n13\n14\n15\nRO UG\nE2\nFT-full Prefix\n100 200 300 400 500 training data size\n0.50\n0.55\n0.60\nBL EU\nFT-full Prefix\n100 200 300 400 500 training data size\n0.60\n0.62\n0.64\n0.66\nRO UG\nE\nFT-full Prefix\nFigure 3: (Left) qualitative examples in lowdata settings. (Right) prefix-tuning (orange) outperforms fine-tuning (blue) in low-data regimes in addition to requiring many fewer parameters. The top two plots correspond to summarization, measured by ROUGE-1 and ROUGE-2. The bottom two plots correspond to table-to-text, measured by BLEU and ROUGE-L. The x-axis is the training size and the y-axis is the evaluation metric (higher is better).\nIn news-to-sports, we train on news articles and test on sports articles. In within-news, we train on {world, UK, business} news and test on the remaining news categories (e.g., health, tech).\nOn both table-to-text and summarization, prefixtuning extrapolates better than fine-tuning under all metrics, as shown in Table 4 and the ‘U’ columns of Table 2 (middle).\nWe also find that adapter-tuning achieves good extrapolation performance, comparable with prefix-\ntuning, as shown in Table 2. This shared trend suggests that preserving LM parameters indeed has a positive impact on extrapolation. However, how prefix-tuning improves extrapolation is an open question and we will discuss this further in §8."
    }, {
      "heading" : "7 Intrinsic Evaluation",
      "text" : "We compare different variants of prefix-tuning to study the impact of various design decisions. §7.1 studies the impact of the prefix length. §7.2 studies tuning only the embedding layer, which is more akin to tuning a discrete prompt. §7.3 compares prefixing and infixing, which inserts trainable activations between x and y. §7.4 studies the impact of various prefix initialization strategies. §7.5 further studies the data efficiency of prefix-tuning."
    }, {
      "heading" : "7.1 Prefix Length",
      "text" : "A longer prefix means more trainable parameters, and therefore more expressive power.11 Figure 4 shows that performance increases as the prefix\n11Empirically, longer prefixes have a negligible impact on training and inference speed per batch, because attention computation over the entire prefix is parallellized on GPUs.\nlength increases up to a threshold (200 for summarization, 10 for table-to-text) and then a slight performance drop occurs. Prefixes longer than the threshold lead to lower training loss, but slightly worse test performance, suggesting that they tend to overfit the training data."
    }, {
      "heading" : "7.2 Full vs Embedding-only",
      "text" : "Recall in §4.1, we discussed optimizing the continuous embeddings of the “virtual tokens.” We instantiate that idea and call it embedding-only. The word embeddings are free parameters, and the remaining activation layers are computed by the Transformer. Table 5 (top) shows that the performance drops significantly, suggesting that tuning only the embedding layer is not sufficiently expressive.\nEmbedding-only upper bounds the performance of discrete prompt optimization (Shin et al., 2020), because discrete prompt restricts the embedding layer to exactly match the embedding of a real word. Consequently, we have this chain of increasing expressive power: discrete prompting < embeddingonly < prefix-tuning."
    }, {
      "heading" : "7.3 Prefix-tuning vs Infix-tuning",
      "text" : "We also investigate how the trainable activations’ position in the sequence affects performance. In\nprefix-tuning, we place them at the beginning [PREFIX;x; y]. We can also place the trainable activations between x and y (i.e. [x; INFIX; y]) and call this infix-tuning. Table 5 (bottom) shows that infix-tuning slightly underperforms prefix-tuning. We believe this is because prefix-tuning can affect the activations of x and y whereas infix-tuning can only influence the activations of y."
    }, {
      "heading" : "7.4 Initialization",
      "text" : "We find that how the prefix is initialized has a large impact in low-data settings. Random initialization leads to low performance with high variance. Initializing the prefix with activations of real words significantly improves generation, as shown in Figure 5. In particular, initializing with task relevant words such as “summarization” and “table-to-text” obtains slightly better performance than task irrelevant words such as “elephant” and “divide”, but using real words is still better than random. Moreover, in full data settings, the initialization trick has no impact, and random initialization leads to equally good performance.\nSince we initialize the prefix with activations of real words computed by the LM, this initialization strategy is concordant with prefix-tuning’s philosophy, which preserves the pretrained LM as much as possible."
    }, {
      "heading" : "7.5 Data Efficiency",
      "text" : "We also investigate the data efficiency of prefixtuning (without initialization trick, a.k.a random initialization) and full fine-tuning by comparing their performance on 5 different data scales of the E2E task (10%, 20%, 40%, 60%, and 80%). Figure 6 shows that prefix-tuning has better performance than fine-tuning when using more than 20% of the data. For data scale of 10%, prefix-tuning with random initialization yields comparable or slightly lower performance than full fine-tuning,\n20 40 60 80 percentage of training data\n65\n66\n67\n68 69 BL EU\nFT-full Prefix\n20 40 60 80 percentage of training data\n68\n69\n70\n71\nRO UG\nEL\nFT-full Prefix\nFigure 6: Data efficiency curves: percentage of training set vs. performance on table-to-text (E2E). Prefixtuning (without the initialization trick) is more dataefficient than fine-tuning when using more than 20% of the data.\nnecessitating the initialization trick (§6.3) to improve the performance in this low-data regime."
    }, {
      "heading" : "8 Discussion",
      "text" : "We will discuss several favorable properties of prefix-tuning and some open problems.\nPersonalization. As we note in §1, prefix-tuning is advantageous when there are a large number of tasks that needs to be trained independently. One practical setting is user privacy (Shokri and Shmatikov, 2015; McMahan et al., 2016). In order to preserve user privacy, each user’s data needs to be separated and a personalized model needs to be trained independently for each user. Consequently, each user can be regarded as an independent task. If there are millions of users, prefix-tuning can scale to this setting and maintain modularity, enabling flexible addition or deletion of users by adding or deleting their prefixes without cross-contamination.\nBatching across users. Under the same personalization setting, prefix-tuning allows batching different users’ queries even though they are backed by different prefixes. When multiple users query a cloud GPU device with their inputs, it is computationally efficient to put these users in the same batch. Prefix-tuning keeps the shared LM intact; consequently, batching requires a simple step of prepending the personalized prefix to user input, and all the remaining computation is unchanged. In contrast, we can’t batch across different users in adapter-tuning, which has personalized adapters between shared Transformer layers.\nThis batching benefit could also help create efficient ensembles of multiple prefixes trained on the same task (Lester et al., 2021).\nInductive bias of prefix-tuning. Recall that finetuning updates all pretrained parameters, whereas prefix-tuning and adapter-tuning preserve them.\nSince the language models are pretrained on general purpose corpora, preserving the LM parameters might help generalization to domains unseen during training. In concordance with this intuition, we observe that both prefix-tuning and adaptertuning have significant performance gain in extrapolation settings (§6.4); however, how these methods improve extrapolation is an open question.\nWhile prefix-tuning and adapter-tuning both freeze the pretrained parameters, they tune different sets of parameters to affect the activation layers of the Transformer. Recall that prefix-tuning keeps the LM intact and uses the prefix and the pretrained attention blocks to affect the subsequent activations; adapter-tuning inserts trainable modules between LM layers, which directly add residual vectors to the activations. Moreover, we observe that prefixtuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance. We think this gain in parameter efficiency is because prefix-tuning keeps the pretrained LM intact as much as possible, and therefore exploits the LM more than adapter-tuning.\nRecent work by Aghajanyan et al. (2020) uses intrinsic dimension to show that there exists a lowdimensional reparameterization that is as effective for fine-tuning as the full parametrization. This explains why good accuracy on downstream tasks can be obtained by updating only a small number of parameters. Our work echoes this finding by showing that good generation performance can also be attained by updating a very small prefix. However, prefix-tuning is not just about the size of trainable parameters, but more importantly, which subset of parameters to modify. Therefore, it would be interesting future work to explore other lightweight fine-tuning methods that achieve an even better accuracy-size tradeoff."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the members of p-lambda group as well as anonymous reviewers for valuable feedback. We gratefully acknowledge the support of a PECASE award. XLL is supported by a Stanford Graduate Fellowship.\nReproducibility Our code is available at https://github.com/ XiangLi1999/PrefixTuning. Experiments and data are available at https: //worksheets.codalab.org/worksheets/ 0x16e0c8e7ab1f4b22aaccddc8b586541f."
    }, {
      "heading" : "A Supplementary Material",
      "text" : "A.1 Datasets and Metrics\nWe evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020).\nThe E2E dataset contains approximately 50K examples with 8 distinct fields; it contains multiple test references for one source table, and the average output length is 22.9. We use the official evaluation script,12 which reports BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015).\nThe WebNLG (Gardent et al., 2017) dataset consists of 22K examples, and the input x is a sequence of (subject, property, object) triples. The average output length is 22.5. In the training and validation splits, the input describes entities from 9 distinct DBpedia categories (e.g., Monument). The test split consists of two parts: the first half contains DB categories seen in training data, and the second half contains 5 unseen categories. These unseen categories are used to evaluate extrapolation. We use the official evaluation script, which reports BLEU, METEOR and TER (Snover et al., 2006).\nDART (Radev et al., 2020) is an open domain table-to-text dataset, with similar input format (entity-relation-entity triples) as WebNLG. The average output length is 21.6. It consists of 82K examples from WikiSQL, WikiTableQuestions, E2E, and WebNLG and applies some manual or automated conversion. We use the official evaluation script13 and report BLEU, METEOR, TER, MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) and BLEURT (Sellam et al., 2020).\nFor the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. There are 225K examples. The average length of the articles is 431 words and the average length of the summaries is 23.3. We report ROUGE-1, ROUGE2 and ROUGE-L, computed by the python package rouge-score.\nData pre-processing. For table-to-text, we linearize a table x in order to fit into a language model context. In the E2E dataset, for example, “(field A,\n12https://github.com/tuetschek/ e2e-metrics\n13https://github.com/Yale-LILY/dart\nvalue A), (field B, value B)” is linearized to “field A : value A | field B : value B”. Also, in WebNLG and DART, a sequence of triple “(entity1.1, relation1, entity1.2), (entity2.1, relation2, entity2.2)” is linearlized as “entity1.1 : relation1 : entity1.2 | entity2.1 : relation2 : entity2.2”.\nFor summarization, we truncate the articles x to 512 BPE tokens.\nExtrapolation data splits. We construct two extrapolation data splits news-to-sports and within-news from the original XSUM dataset. XSUM dataset is drawn from BBC news, and we identify the topic of each article based on its URL. Since “news” and “sports” are the two domains with the most articles, we create our first train/test split. Additionally, “news” has subdomains such as “UK”, “world”, and “technology”. Consequently, we create a second data split, using the top 3 news subdomains (i.e. {world, UK, business }) as training data and the rest as test data.\nA.2 Hyperparameters\nIn Table 6, we report the hyperparameters used to train the best-performing models documented in the experiment section.\nAs for the search range of each hyperparameters: the learning rates are selected from {1e-5, 5e-05, 8e-05}; the number of epochs are selected from {5, 10} for table-to-text and {5, 25, 30 } for summarization; We select the largest batch size that can fit into GPU memory and didn’t explicitly tune for an optimal batch size. Prefix length are selected from {1, 5, 10, 20, 40} for table-to-text and {1, 10, 20, 50, 80, 100, 200, 300} for summarization. We use perplexity and automatic generation metrics on the validation set to select the best-performing models.\nFor table-to-text in the low data settings, we use a learning rate of 5e-5, and a batch size of 10. We use a prefix length of 6, since we apply the initialization trick and initialize the prefix with “table-to-text:”, which contains 6 BPE tokens. Instead of tuning the number of epochs, we tune the max steps of updates in {100, 200, 400, 600 }, as shown in Table 8. We apply early stopping based on the performance of validation set, where the validation size =30% training size.\nFor summarization in the low data settings, we use a learning rate of 5e-5 and a warmup step of 100. We use a batch size of 5 for prefix-tuning and 6 for fine-tuning. We apply the initialization trick and use the word “summarize” to initialize\nthe prefix, resulting in a prefix length of 1. We tune the number of epochs in {3, 5, 10, 20, 30}, shown in Table 8. We also apply early stopping based on validation performance.\nFor the extrapolation setting, the hyperparameters for our table-to-text model is the same as the hyperparameters of WebNLG. The hyperparameters for summarization is shown in the last block of Table 6.\nA.3 Validation Performance\nTable 9 shows the validation performance on the three table-to-text datasets. Table 7 shows the validation performance on XSUM.\nA.4 Additional Results for Low-data Settings Figure 7 supplements the low-data performance curves in Figure 3 by plotting the relationship between training size and generation metrics for both prefix-tuning and fine-tuning.\nA.5 Additional Results for the Initialization Experiment\nFigure 8 supplements Figure 3 by plotting additional metrics for our initialization technique §7.4. It validates that random initialization (from a uniform (0,1) distirbution) significantly underperforms initializing with real words; Additionally, initializing with task-relevant words (e.g., “summarization” and “table-to-text”) attains slightly better generation scores than initializing with task-irrelevant words (e.g., “elephant” and “banana”).\nA.6 Qualitative Examples for Extrapolation Table 10 contains qualitative examples from both seen and unseen categories in WebNLG. We find that for unseen categories, both prefix-tuning and fine-tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated output is inconsistent with table contents). In particular, prefix-tuning tends to undergenerate whereas fine-tuning tends to generate untruthfully. For seen categories, both perform fairly well in terms of coverage and truthfulness."
    } ],
    "references" : [ {
      "title" : "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
      "author" : [ "Armen Aghajanyan", "Luke Zettlemoyer", "Sonal Gupta" ],
      "venue" : null,
      "citeRegEx" : "Aghajanyan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2020
    }, {
      "title" : "Comparing automatic and human evaluation of NLG systems",
      "author" : [ "Anja Belz", "Ehud Reiter." ],
      "venue" : "11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Belz and Reiter.,? 2006",
      "shortCiteRegEx" : "Belz and Reiter.",
      "year" : 2006
    }, {
      "title" : "Plug and play language models: A simple approach to controlled text generation",
      "author" : [ "Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu." ],
      "venue" : "International Conference on Learning Represen-",
      "citeRegEx" : "Dathathri et al\\.,? 2020",
      "shortCiteRegEx" : "Dathathri et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The WebNLG challenge: Generating text from RDF data",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 10th International Conference on Natural Language Generation, pages 124–133, San-",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "WARP: word-level adversarial reprogramming",
      "author" : [ "Karen Hambardzumyan", "Hrant Khachatrian", "Jonathan May." ],
      "venue" : "CoRR, abs/2101.00121.",
      "citeRegEx" : "Hambardzumyan et al\\.,? 2021",
      "shortCiteRegEx" : "Hambardzumyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "Proceedings of the 36th International Conference",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "How can we know what language",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Text-to-text pre-training for data-totext tasks",
      "author" : [ "Mihir Kale" ],
      "venue" : null,
      "citeRegEx" : "Kale.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kale.",
      "year" : 2020
    }, {
      "title" : "Ctrl: A conditional transformer language model for controllable generation",
      "author" : [ "N. Keskar", "B. McCann", "L.R. Varshney", "Caiming Xiong", "R. Socher." ],
      "venue" : "ArXiv, abs/1909.05858.",
      "citeRegEx" : "Keskar et al\\.,? 2019",
      "shortCiteRegEx" : "Keskar et al\\.",
      "year" : 2019
    }, {
      "title" : "GeDi: Generative Discriminator Guided Sequence Generation",
      "author" : [ "Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "Richard Socher", "Nazneen Fatema Rajani." ],
      "venue" : "arXiv preprint arXiv:2009.06367.",
      "citeRegEx" : "Krause et al\\.,? 2020",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2020
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 228–231, Strouds-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant" ],
      "venue" : null,
      "citeRegEx" : "Lester et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Exploring versatile generative language model via parameter-efficient transfer learning",
      "author" : [ "Zhaojiang Lin", "Andrea Madotto", "Pascale Fung." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 441–459, Online. As-",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Gpt understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Federated learning of deep networks using model averaging",
      "author" : [ "H. Brendan McMahan", "Eider Moore", "Daniel Ramage", "Blaise Agüera y Arcas." ],
      "venue" : "Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017,",
      "citeRegEx" : "McMahan et al\\.,? 2016",
      "shortCiteRegEx" : "McMahan et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "The E2E dataset: New challenges for end-toend generation",
      "author" : [ "Jekaterina Novikova", "Ondrej Dusek", "Verena Rieser." ],
      "venue" : "CoRR, abs/1706.09254.",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Adapterfusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych" ],
      "venue" : null,
      "citeRegEx" : "Pfeiffer et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning how to ask: Querying LMs with mixtures of soft prompts",
      "author" : [ "Guanghui Qin", "Jason Eisner." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Qin and Eisner.,? 2021",
      "shortCiteRegEx" : "Qin and Eisner.",
      "year" : 2021
    }, {
      "title" : "Dart: Open-domain structured data record to text generation",
      "author" : [ "Mutuma", "Yasin Tarabar", "Ankit Gupta", "Tao Yu", "Yi Chern Tan", "Xi Victoria Lin", "Caiming Xiong", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Mutuma et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mutuma et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "A. Radford", "Jeffrey Wu", "R. Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "How fine can fine-tuning be? learning efficient language models",
      "author" : [ "Evani Radiya-Dixit", "Xin Wang." ],
      "venue" : "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Radiya.Dixit and Wang.,? 2020",
      "shortCiteRegEx" : "Radiya.Dixit and Wang.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning multiple visual domains with residual adapters",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Hakan Bilen", "Andrea Vedaldi." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 506– 516. Curran Associates, Inc.",
      "citeRegEx" : "Rebuffi et al\\.,? 2017",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploiting cloze questions for few shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Schick and Schütze.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Pragmatically informative text generation",
      "author" : [ "Sheng Shen", "Daniel Fried", "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV au", "Eric Wallace", "Sameer Singh" ],
      "venue" : null,
      "citeRegEx" : "Shin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Privacypreserving deep learning",
      "author" : [ "Reza Shokri", "Vitaly Shmatikov." ],
      "venue" : "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, page 1310–1321, New York, NY, USA. Association for",
      "citeRegEx" : "Shokri and Shmatikov.,? 2015",
      "shortCiteRegEx" : "Shokri and Shmatikov.",
      "year" : 2015
    }, {
      "title" : "A study of translation error rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "Ralph Weischedel." ],
      "venue" : "In Proceedings of the Association for Machine Transaltion in the Americas (AMTA 2006.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Recipes for adapting pre-trained monolingual and multilingual models to machine translation",
      "author" : [ "Asa Cooper Stickland", "Xian Li", "Marjan Ghazvininejad" ],
      "venue" : null,
      "citeRegEx" : "Stickland et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Stickland et al\\.",
      "year" : 2020
    }, {
      "title" : "Can unconditional language models recover arbitrary sentences",
      "author" : [ "Nishant Subramani", "Samuel R. Bowman", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Subramani et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Subramani et al\\.",
      "year" : 2020
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "CVPR, pages 4566–4575. IEEE Computer Society.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "2020a. Sidetuning: A baseline for network adaptation via additive side networks",
      "author" : [ "Jeffrey O Zhang", "Alexander Sax", "Amir Zamir", "Leonidas Guibas", "Jitendra Malik" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "BERTScore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Masking as an efficient alternative to finetuning for pretrained language models",
      "author" : [ "Mengjie Zhao", "Tao Lin", "Fei Mi", "Martin Jaggi", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M. Meyer", "Steffen Eger." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Extractive summarization as text matching",
      "author" : [ "Ming Zhong", "Pengfei Liu", "Yiran Chen", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208, On-",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating bert into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tieyan Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform downstream tasks (e.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform downstream tasks (e.",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : "This can be prohibitively expensive given the size of current LMs; for example, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attaining comparable performance with fine-tuning while adding only around 2–4% task-specific parameters (Houlsby et al., 2019; Lin et al., 2020).",
      "startOffset" : 207,
      "endOffset" : 247
    }, {
      "referenceID" : 15,
      "context" : "Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attaining comparable performance with fine-tuning while adding only around 2–4% task-specific parameters (Houlsby et al., 2019; Lin et al., 2020).",
      "startOffset" : 207,
      "endOffset" : 247
    }, {
      "referenceID" : 36,
      "context" : "In the context of personalization where the tasks correspond to users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we would have a separate prefix for each user trained only on that user’s data, thereby avoiding data cross-contamination.",
      "startOffset" : 70,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "In the context of personalization where the tasks correspond to users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we would have a separate prefix for each user trained only on that user’s data, thereby avoiding data cross-contamination.",
      "startOffset" : 70,
      "endOffset" : 120
    }, {
      "referenceID" : 30,
      "context" : "For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.",
      "startOffset" : 91,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al.",
      "startOffset" : 34,
      "endOffset" : 66
    }, {
      "referenceID" : 44,
      "context" : "For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 209
    }, {
      "referenceID" : 38,
      "context" : "For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 209
    }, {
      "referenceID" : 48,
      "context" : "For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 209
    }, {
      "referenceID" : 45,
      "context" : "One line of research learns a task-specific parameter mask (Zhao et al., 2020; Radiya-Dixit and Wang, 2020).",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "One line of research learns a task-specific parameter mask (Zhao et al., 2020; Radiya-Dixit and Wang, 2020).",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "(2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020).",
      "startOffset" : 182,
      "endOffset" : 267
    }, {
      "referenceID" : 15,
      "context" : "(2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020).",
      "startOffset" : 182,
      "endOffset" : 267
    }, {
      "referenceID" : 31,
      "context" : "(2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020).",
      "startOffset" : 182,
      "endOffset" : 267
    }, {
      "referenceID" : 25,
      "context" : "(2020a) trains a “side” network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020).",
      "startOffset" : 182,
      "endOffset" : 267
    }, {
      "referenceID" : 7,
      "context" : "4584 2019), prompt engineering has been explored for natural language understanding tasks (Jiang et al., 2020; Schick and Schütze, 2020).",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 32,
      "context" : "4584 2019), prompt engineering has been explored for natural language understanding tasks (Jiang et al., 2020; Schick and Schütze, 2020).",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 35,
      "context" : "For example, AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from BERT and RoBERTa.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves GPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : ", 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes as the model size grows.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "Assume we have an autoregressive neural language model pφ(y | x) parametrized by φ (e.g., GPT-2; Radford et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "We can also use an encoder-decoder architecture (e.g., BART; Lewis et al., 2020) to model pφ(y | x), where x is encoded by the bidirectional encoder, and the decoder predicts y autoregressively (conditioned on the encoded x and its left context).",
      "startOffset" : 48,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : ", 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "6 For summarization, we compare against fine-tuning BART (Lewis et al., 2020).",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "At training time, we use the AdamW optimizer (Loshchilov and Hutter, 2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup.",
      "startOffset" : 45,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "Embedding-only upper bounds the performance of discrete prompt optimization (Shin et al., 2020), because discrete prompt restricts the embedding layer to exactly match the embedding of a real word.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 36,
      "context" : "One practical setting is user privacy (Shokri and Shmatikov, 2015; McMahan et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "One practical setting is user privacy (Shokri and Shmatikov, 2015; McMahan et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "This batching benefit could also help create efficient ensembles of multiple prefixes trained on the same task (Lester et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 132
    } ],
    "year" : 2021,
    "abstractText" : "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
    "creator" : "LaTeX with hyperref"
  }
}