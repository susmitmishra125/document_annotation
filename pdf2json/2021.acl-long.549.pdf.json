{
  "name" : "2021.acl-long.549.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "TIMEDIAL: Temporal Commonsense Reasoning in Dialog",
    "authors" : [ "Lianhui Qin", "Aditya Gupta", "Shyam Upadhyay", "Luheng He", "Yejin Choi", "Manaal Faruqui" ],
    "emails" : [ "mfaruqui}@google.com", "yejin}@cs.washington.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7066"
    }, {
      "heading" : "1 Introduction",
      "text" : "Humans can effortlessly reason about temporal concepts of everyday events such as their duration, frequency, or relative ordering (Allen, 1984; Radvansky and Zacks, 2014) based on rich commonsense knowledge about how the world works, especially in relation to time. However, reasoning about such concepts has been challenging for machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations\n∗Work done during an internship at Google.\n(UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns.\nFor instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference.\nAlthough previous works have studied temporal reasoning in natural language, they have either focused on specific time-related concepts in\nisolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016).\nIn this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge.\nWe design a new task for dialog-based temporal reasoning and present a new challenge set in English, called TIMEDIAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et al., 2017). Given a dialog with one temporal span masked out, the model is asked to find all correct answers from a list of four options to fill in the blank (Table 1).\nThe challenge set requires the models to demonstrate understanding of the context and use temporal commonsense to make right choices. Our final challenge set consists of 1.1K carefully curated dialog instances.\nWe then study the performance of several stateof-the-art pre-trained language models on TIMEDIAL along several dimensions including modeling paradigms (classification, mask filling, and generation), the scope of dialog contexts, in-domain vs. out-of-domain training, dependence on shallow text matching for reasoning, and the types of reasoning required. Our experiments demonstrate that offthe-shelf, pre-trained language models cannot effectively reason about temporal aspects in a dialog, even with domain-specific finetuning. Our findings indicate that large-scale pre-trained models even after fine-tuning may not be sufficient for robust temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them."
    }, {
      "heading" : "2 Task: Temporal Reasoning in Dialog",
      "text" : "We formulate the dialog-based temporal commonsense reasoning problem as a cloze task (Taylor, 1953). Formally, given a multi-turn dialog context of n conversational turns between two speakers A\nand B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018).\nHaving a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options are selected to be spuriously correlated with the dialog context. For example, we include temporal spans in the dialog context as negative options, which will challenge models that rely primarily only on shallow pattern matching without correct temporal reasoning. We present more information in §3 about how the negative options were created by human annotators."
    }, {
      "heading" : "3 Dataset: TIMEDIAL",
      "text" : "The TIMEDIAL dataset is derived from DailyDialog data (Li et al., 2017), which is a multi-turn dialog corpus containing over 13K English dialogs. Dialogs in this dataset consist of turn-taking between two people on topics over 10 broad categories, ranging from daily lives to financial topics."
    }, {
      "heading" : "3.1 Data Collection",
      "text" : "Our data collection process involves two steps: (1) identifying dialogs that are rich in temporal expressions, and (2) asking human annotators to provide correct and incorrect options for cloze instances derived from these dialogs. We now describe these steps in detail.\nTemporal expression identification. Here, we select dialogs that are rich with temporal information, in order to focus on complex temporal reasoning that arises in natural dialogs. Temporal expressions are automatically identified with SUTime (Chang and Manning, 2012), an off-the-shelf\ntemporal expression detector.1 We keep only the dialogs with more than 3 temporal expressions and at least one expression that contains numerals like “two weeks” (as opposed to non-numeric spans, like “summer”, “right now”, and “later”). In our initial experiment, we observe that language models can often correctly predict these non-numerical temporal phrases.\nWe note that temporal expressions containing numerals serve as more challenging sets of options than non-numerical ones. This filtering step results in 1,127 unique dialogs for further processing.\nHuman annotated options. Next, we make spans in the dialogs. For a dialog, we mask out each temporal expression that contains numerals, each resulting in a cloze question that is then sent for human annotation.\nThis resulted in 1,526 instances for annotation. For each masked span in each dialog, we obtain human annotation to derive a fixed set of correct and incorrect options given the context. Concretely, given a masked dialog and a seed correct answer (i.e., the original text) for the masked span, the\n1https://nlp.stanford.edu/software/ sutime.shtml\nannotators2 were asked to (1) come up with an alternative correct answer that makes sense in the dialog adhering to commonsense, and (2) formulate two incorrect answers that have no possibility of making sense in the dialog context. We highlight all time expressions in the context to make it easier for annotators to select reasonable time expressions.\nTo ensure that the annotated incorrect options are not too trivially distinguishable by the models (as discussed in §2), we define three rules for the annotators to follow.\n• Rule 1: Phrase Matching. The rater should first try to pick another temporal span from the dialog context that makes syntactic/semantic sense (e.g., when the span is of the appropriate type, such as duration, for the masked span) but is still incorrect according to commonsense. • Rule 2: Numeral Matching. If Rule 1 does not apply, raters should follow a relaxed version of Rule 1, whereby the incorrect option should contain any numeral occurring in the dialog context.\n2who are English linguists."
    }, {
      "heading" : "Incorrect Options",
      "text" : "• Rule 3: Open-ended. If neither of the above rules is applicable, then raters can come up with an incorrect option using their own judgment. The two incorrect options are required to differ from each other as much as possible.\nRules-1&2 are designed to confuse models that rely on shallow pattern matching. Finally, to ensure the quality of the human-annotated options, we perform a subsequent round of human validation on the gathered data. The validators identify and fix issues such as duplicate options, unreasonable or obscure annotations w.r.t natural usage, or ungrammatical annotations that do not fit in the context."
    }, {
      "heading" : "3.2 Properties of TIMEDIAL",
      "text" : "Table 3 shows statistics of TIMEDIAL. The dataset contains over 1.1K test instances. Each dialog contains 11.7 turns and 3 temporal expressions on average, presenting richer and more complex context compared to the recent single-sentence-based temporal question answering benchmarks (e.g., Zhou et al., 2019; Vashishtha et al., 2020). As above, each test instance contains two correct answers and two incorrect ones.3 Over half of the incorrect options are annotated based on phrase and numeral matching from context, which pose a significant challenge for models relying on shallow text matching, as we show in our experimental analysis (§5).\nAnswering different instances in the dataset requires different types of core reasoning abilities, such as comparison, arithmetic inference, or reasoning based on world knowledge or general commonsense. To facilitate fine-grained analysis, we also annotate the reasoning categories for a randomly sampled set of 100 dialogs. Though each\n3We also collected 342 extra instances for which the annotators deem there is only one unique correct answer for the context. Thus, each of those instances contains one correct option and two incorrect ones. We release those instances along with the dataset, though we did not include them in empirical study in this paper.\ninstance can involve multiple reasoning types, we associate it with one predefined category label that indicates the primary type of reasoning it requires. Table 2 shows the category distribution and examples in each of the category. We observe that the dataset requires general commonsense for 60% of the dialogs, making it the most common reasoning type."
    }, {
      "heading" : "4 Modeling",
      "text" : "We consider a broad set of methods and evaluate their performance on our challenge TIMEDIAL dataset. These methods vary in terms of the modeling paradigms, the scope of the dialog contexts, and training settings. In particular, they encompass the major ways pre-trained LMs are currently used in downstream tasks (§4.1) which often outperform earlier specialized non-pretrained models. We also consider different lengths of context used in reasoning, varying by their vicinity to the masked span (§4.2). Finally, we study different training settings, including zero-shot, in-domain, and out-of-domain training (§4.3)."
    }, {
      "heading" : "4.1 Modeling Paradigms",
      "text" : "We experiment across three major modeling paradigms: (i) Binary Classification, (ii) Mask Filling, and (iii) Generation. Figure 1 shows the different architectures. For each test instance, the model takes as input a pair of (masked dialog context, candidate), and outputs a score measuring how likely the candidate being a correct answer. Based on the prediction scores of all options, the model then chooses the top two positive candidates as the predicted answer for the instance. Each paradigm of models is finetuned using training data from different domains, as discussed in §4.3."
    }, {
      "heading" : "4.1.1 Binary Classification",
      "text" : "In this setting, we formulate the task as a binary classification problem, i.e., we use a classifier to measure the probability of the candidate in the (masked dialog context, candidate) pair being a correct answer. Any powerful LM — e.g., BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), ROBERTA (Liu et al., 2019), etc. can be used to build the classifier.\nThis method’s key challenge is the lack of annotated training data for direct supervision. We generate weak supervision training data as follows. In an unlabeled corpus, we use the SUTime tool\nto annotate temporal spans. We mask each temporal span in this corpus and use the masked text as one positive example for binary classification. To generate negative example, we randomly sample another temporal span from the dialog context and use it as a negative example for the masked temporal span. The resulting data is noisy because the randomly sampled temporal span can also logically fit in the masked span in the given context; however, we assume the likelihood of that happening is low. We leave drawing harder negative instances using heuristics to future work."
    }, {
      "heading" : "4.1.2 Mask Filling",
      "text" : "We also use the mask filling approach of BERTlike mask language models (MLMs). For each dialog context and a candidate temporal span of m tokens, we replace the blank in the dialog context with m masked tokens. We then evaluate the likelihood of predicting the temporal span tokens for those masked positions, and make average across the positions. A key advantage of this method is that we can directly apply a BERT model in the zero-shot manner since the model was pretrained in the same way, as for accommodating for [MASK] fillings. Additionally, we also finetune BERT’s MLM for learning task specific properties."
    }, {
      "heading" : "4.1.3 Generation",
      "text" : "The third method is a fully generative approach using the text-to-text paradigm of T5 (Raffel et al., 2020). Given a masked dialog context, the model is trained to generate the masked text in an encoderdecoder framework. As a result, evaluating the likelihood of generating the given temporal span (normalized with the length of the span) is used as the probability of it being correct. Similar to mask\nfilling, we use T5 either in a zero-shot manner or with additional fine-tuning."
    }, {
      "heading" : "4.2 Dialog Context",
      "text" : "We aim to study the influence of context on a model’s temporal reasoning in dialog by incorporating varying scopes of dialog context based on their vicinity to the target span. Since the dialogs in TIMEDIAL are rich in temporal concepts, we want to evaluate LMs’ dependence on shallow text matching vs. the ability to accurately understand the causal relations between those concepts (see Table 6). We use the following three settings:\n• Full context, where the model is presented with the complete available dialog to reason on. Due to our design of challenging negatives, the full context can often confuse models that rely on shallow cues. • Local context, where we provide only with the utterances that immediately precede and follow the target utterance. • Target context, where the context is restricted to only the particular utterance that contains the masked span."
    }, {
      "heading" : "4.3 Training Details",
      "text" : "For all models, we consider two common training settings, e.g., in-domain data, which is typically small, and out-of-domain training where a large amount of data is available. Table 4 shows training data statistics. For mask-filling and generation, we also evaluate in a zero-shot setup with no finetuning.\nIn-domain training. Our challenge TIMEDIAL test set is derived from contextually rich dialogs"
    }, {
      "heading" : "Classification",
      "text" : "from the DailyDialog dataset, based on the number of temporal spans. However, this still leaves remaining data with less than 3 temporal spans or with no numeric span. By masking each temporal span in each dialog, we obtain 14.5K training instances to use in our domain specific fine-tuning.\nOut-of-domain training. In this setting, we consider a much larger corpus from a general domain. Specifically, we use the large scale training set based on the Meena dataset Adiwardana et al. (2020), which is mined and filtered from public domain social media conversations over 341GB of text (40B words).4 Compared to the above indomain data from DailyDialog which were manually written by human annotators in a clean and consistent way, the dialogs in the Meena corpus tend to be noisy, casual, and usually short. Like our DailyDialog processing, we identify all temporal expressions for dialogs in Meena using SUTime."
    }, {
      "heading" : "5 Experiments and Analyses",
      "text" : "Using the proposed TIMEDIAL challenge set, we next conduct extensive experiments and analyses on the different model variants and context settings. We use either 4x4 or 8x8 Cloud TPUs V3 pod slices5 for fine-tuning and one V100 GPU for inference. We provide more details of the experiment configurations in the appendix.\nEvaluation. Since each example of TIMEDIAL contains two correct answers, we report the metric 2-best accuracy, which measures whether both of the model’s top-ranked answers are correct. In\n4We acquired a trimmed down version of the Meena dataset by contacting the authors.\n5https://cloud.google.com/tpu"
    }, {
      "heading" : "Classification (BERT)",
      "text" : ""
    }, {
      "heading" : "Mask Filling (BERT)",
      "text" : ""
    }, {
      "heading" : "Generation (T5)",
      "text" : "other words, if the model erroneously ranks an incorrect answer over a correct one, we consider it to be an error case. Note that we use the rankingbased metric as opposed to classification-based ones (for example, by asking the model to classify whether each individual candidate answer is correct or not (e.g., Zhou et al., 2019)) and because it presents a stricter measure that penalizes any incorrect answers being ranked over correct answers, and the ranking metric is not influenced by specific choices of the threshold hyperparameter that cuts off positive and negative predictions."
    }, {
      "heading" : "5.1 Model Performance",
      "text" : "Table 5 shows model results and human performance. Human performance achieves a nearperfect level (97.80, with Cohen’s kappa score of 0.86 showing almost perfect inter-rater agreement (Landis and Koch, 1977)).\nOverall. The generation model based on T5LARGE and finetuned on the in-domain DailyDialog data achieves the best performance. However, its 2-best accuracy (74.8) lagged far behind the human performance, demonstrating the difficulty of the TIMEDIAL challenge set.\nZero-shot vs. out-of-domain vs. in-domain. When comparing the different training data setup, we observe that models with in-domain training using the DailyDialog data (e.g., LARGE-IN) consistently outperforms those trained on the large out-ofdomain Meena dataset (e.g., LARGE-OUT). Both setups outperform the zero-shot models (without any fine-tuning) (e.g., LARGE-ZERO). The results show that the large LMs still highly depend on indomain or at least dialog data to grasp and enhance their temporal reasoning ability in dialog context. Further, we see increasing performance with increasing model size, which is not unexpected given the complexity of the task."
    }, {
      "heading" : "5.2 Error Analysis",
      "text" : "Next, we analyze the different types of errors based on different rules for negative option creation in the annotation process. In particular, the phrase matching rule picks an exact time span from the dialog context, and numeral matching picks numerals from the dialog context. Thus, models picking those incorrect options imply reliance on spurious shallow text matching features.\nFigure 2 shows the percentage of errors in terms of the different rules. For example, the BERTbased classification model CLS-IN erroneously picks 52% of negative options created by the phrase matching rule as correct answers (i.e., by ranking those negative options over the true correct options). We observe that the various models are all most vulnerable to the phrase matching options compared\n1\nto other types of negative options, showing that they rely on spurious text matching to a significant extent. Between BERT and T5, we find T5 being more robust to shallow text matching.\nTable 6 provides further examples of prediction errors, illustrating confusions due to shallow text matching. In the first dialog, both incorrect answers already partially occur in the context or are related to preexisting concepts (i.e., “three” to “three o’clock”, and “nine” to “September”). All the three models were confused and chose either of the two as the top prediction for the blank, even though the options clearly violate the context. Interestingly, the mask filling model was completely confused and ranked both incorrect answers over the correct ones. Similarly in the second example,\nSize BASE LARGE Training IN OUT IN OUT"
    }, {
      "heading" : "Classification (BERT)",
      "text" : "TARGET 50.5 40.0 50.5 47.5 LOCAL + 3.4 + 3.3 + 7.5 + 2.0 FULL − 0.6 − 0.1 + 2.7 + 1.2"
    }, {
      "heading" : "Mask Filling (BERT)",
      "text" : "TARGET 57.8 44.3 60.3 46.8 LOCAL + 5.4 + 3.0 + 8.1 + 4.9 FULL + 9.6 + 3.1 + 9.6 + 8.0"
    }, {
      "heading" : "Generation (T5)",
      "text" : "TARGET 55.5 45.9 66.7 56.1 LOCAL + 3.7 + 2.7 + 6.1 + 3.7 FULL + 3.7 + 4.7 + 8.2 + 5.8\nTable 7: Impact of dialog context on reasoning accuracy. IN and OUT denote in-domain and out-of-domain training, respectively. We use 2-best accuracy of target context as reference and report the absolute changes in performance of local and full context, respectively. Local dialog context results in better performance to full dialog context on 5 of the 12 cases, which are highlighted in the table.\nthe models fail to capture the contextual semantics."
    }, {
      "heading" : "5.3 Influence of Dialog Context",
      "text" : "Table 7 shows how different scopes of dialog context (§4.2) affect model performance. First, the most restrictive target-only context is insufficient for accurate reasoning, by producing the weakest performance of most models. This highlights the importance of context information for temporal commonsense reasoning in dialog, which differs from previous temporal reasoning studies based on limited context (e.g., single-sentence question answering). Second, we note that the full dialog context does not always lead to the best performance. In 5 out of the 12 cases, using the local context yields equal or higher reasoning accuracy. The results show that the LMs still fall short of properly modeling the rich dialog contexts and making effective use of all information to do reasoning."
    }, {
      "heading" : "5.4 Errors of Reasoning Categories",
      "text" : "Figure 3 shows the percentage of errors in each reasoning category. We observe that the models tend to make non-trivial portions of errors on commonsense/world knowledge questions. For example, the strongest model, T5 GEN-IN, failed on 18% of the instances that require commonsense or world knowledge, while BERT CLS-IN made errors on 48% of such instances. The performance\nTable 1 Commonsense/ World knowledge Comparison Arithmetic/Others\nCLS-IN 0.4769230769 0.5416666667 0.6363636364 CLS-OUT 0.4153846154 0.4583333333 0.2727272727 MF-IN 0.2461538462 0.2083333333 0.09090909091 MF-OUT 0.3538461538 0.3333333333 0.3636363636 GEN-IN 0.1846153846 0.1666666667 0.1818181818 GEN-OUT 0.2307692308 0.4166666667 0.2727272727"
    }, {
      "heading" : "Commonsense/World knowledge Comparison Arithmetic/Others",
      "text" : "1\non comparison-based instances seems similar."
    }, {
      "heading" : "6 Related Work",
      "text" : "Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018).\nSome recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018).\nCommonsense reasoning with LMs. With the recent success of large pre-trained language models\n(LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We introduced TIMEDIAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog. The dataset is carefully curated to evaluate a models’ ability to do temporal commonsense/numerical reasoning over dialog context. In order to establish strong baselines and provide information on future model development, we conducted extensive experiments with state-of-the-art language models with different settings: the scope of context, weak supervision strategies, and learning objectives. While humans can easily answer these questions (97.8% accuracy), even our best model variant (T5-large with in-domain training) struggles on this challenge set (73%). Moreover, our qualitative error analyses show that these large language models often rely on shallow, spurious features (particularly text matching) when answering these questions, instead of truly doing reasoning over the context."
    } ],
    "references" : [ {
      "title" : "Towards a human-like open-domain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu" ],
      "venue" : "arXiv preprint arXiv:2001.09977",
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Maintaining knowledge about temporal intervals",
      "author" : [ "James F Allen." ],
      "venue" : "Communications of the ACM, 26(11):832–843.",
      "citeRegEx" : "Allen.,? 1983",
      "shortCiteRegEx" : "Allen.",
      "year" : 1983
    }, {
      "title" : "Towards a general theory of action and time",
      "author" : [ "James F Allen." ],
      "venue" : "Artificial intelligence, 23(2):123– 154.",
      "citeRegEx" : "Allen.,? 1984",
      "shortCiteRegEx" : "Allen.",
      "year" : 1984
    }, {
      "title" : "Parsing time: Learning to interpret time expressions",
      "author" : [ "Gabor Angeli", "Christopher D Manning", "Dan Jurafsky." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Angeli et al\\.,? 2012",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2012
    }, {
      "title" : "Inducing relational knowledge from bert",
      "author" : [ "Zied Bouraoui", "José Camacho-Collados", "S. Schockaert." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Bouraoui et al\\.,? 2020",
      "shortCiteRegEx" : "Bouraoui et al\\.",
      "year" : 2020
    }, {
      "title" : "A model for temporal references and its application in a question answering program",
      "author" : [ "Bertram C Bruce." ],
      "venue" : "Artificial intelligence, 3:1–25.",
      "citeRegEx" : "Bruce.,? 1972",
      "shortCiteRegEx" : "Bruce.",
      "year" : 1972
    }, {
      "title" : "Classifying temporal relations between events",
      "author" : [ "Nathanael Chambers", "Shan Wang", "Dan Jurafsky." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Chambers et al\\.,? 2007",
      "shortCiteRegEx" : "Chambers et al\\.",
      "year" : 2007
    }, {
      "title" : "Sutime: A library for recognizing and normalizing time expressions",
      "author" : [ "Angel X Chang", "Christopher D Manning." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Chang and Manning.,? 2012",
      "shortCiteRegEx" : "Chang and Manning.",
      "year" : 2012
    }, {
      "title" : "Commonsense knowledge mining from pretrained models",
      "author" : [ "Joe Davison", "Joshua Feldman", "Alexander Rush." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Davison et al\\.,? 2019",
      "shortCiteRegEx" : "Davison et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Dinan et al\\.,? 2018",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2018
    }, {
      "title" : "Joint inference for event timeline construction",
      "author" : [ "Quang Do", "Wei Lu", "Dan Roth." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Do et al\\.,? 2012",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2012
    }, {
      "title" : "Shortcut learning in deep neural networks",
      "author" : [ "Robert Geirhos", "Jörn-Henrik Jacobsen", "Claudio Michaelis", "Richard Zemel", "Wieland Brendel", "Matthias Bethge", "Felix A Wichmann." ],
      "venue" : "Nature Machine Intelligence, 2(11):665–673.",
      "citeRegEx" : "Geirhos et al\\.,? 2020",
      "shortCiteRegEx" : "Geirhos et al\\.",
      "year" : 2020
    }, {
      "title" : "Injecting numerical reasoning skills into language models",
      "author" : [ "Mor Geva", "Ankit Gupta", "Jonathan Berant." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Geva et al\\.,? 2020",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2020
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A Smith." ],
      "venue" : "Proc. of NAACL, pages 107–112.",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Mechanizing temporal knowledge",
      "author" : [ "Kenneth Kahn", "G.Anthony Gorry." ],
      "venue" : "Artificial Intelligence, 9(1):87 – 108.",
      "citeRegEx" : "Kahn and Gorry.,? 1977",
      "shortCiteRegEx" : "Kahn and Gorry.",
      "year" : 1977
    }, {
      "title" : "Learning temporal information for states and events",
      "author" : [ "Z. Kozareva", "E. Hovy." ],
      "venue" : "2011 IEEE Fifth International Conference on Semantic Computing, pages 424–429.",
      "citeRegEx" : "Kozareva and Hovy.,? 2011",
      "shortCiteRegEx" : "Kozareva and Hovy.",
      "year" : 2011
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "biometrics, pages 159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Learning sentence-internal temporal relations",
      "author" : [ "Mirella Lapata", "Alex Lascarides." ],
      "venue" : "Journal of Artificial Intelligence Research, 27:85–117.",
      "citeRegEx" : "Lapata and Lascarides.,? 2006",
      "shortCiteRegEx" : "Lapata and Lascarides.",
      "year" : 2006
    }, {
      "title" : "Adversarial filters of dataset biases",
      "author" : [ "Ronan Le Bras", "Swabha Swayamdipta", "Chandra Bhagavatula", "Rowan Zellers", "Matthew Peters", "Ashish Sabharwal", "Yejin Choi." ],
      "venue" : "Proc. of ICML, pages 1078–1088. PMLR.",
      "citeRegEx" : "Bras et al\\.,? 2020",
      "shortCiteRegEx" : "Bras et al\\.",
      "year" : 2020
    }, {
      "title" : "Context-dependent semantic parsing for time expressions",
      "author" : [ "Kenton Lee", "Yoav Artzi", "Jesse Dodge", "Luke Zettlemoyer." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Lee et al\\.,? 2014",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "Temporal information extraction by predicting relative time-lines",
      "author" : [ "Artuur Leeuwenberg", "Marie Francine Moens." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Leeuwenberg and Moens.,? 2018",
      "shortCiteRegEx" : "Leeuwenberg and Moens.",
      "year" : 2018
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "arXiv preprint arXiv:1710.03957.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models",
      "author" : [ "Bill Yuchen Lin", "Seyeon Lee", "Rahul Khanna", "Xiang Ren." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
      "author" : [ "Todor Mihaylov", "Anette Frank." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Mihaylov and Frank.,? 2018",
      "shortCiteRegEx" : "Mihaylov and Frank.",
      "year" : 2018
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint reasoning for temporal and causal relations",
      "author" : [ "Qiang Ning", "Zhili Feng", "Hao Wu", "Dan Roth." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Ning et al\\.,? 2018a",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "TORQUE: A reading comprehension dataset of temporal ordering questions",
      "author" : [ "Qiang Ning", "Hao Wu", "Rujun Han", "Nanyun Peng", "Matt Gardner", "Dan Roth." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Ning et al\\.,? 2020",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving temporal relation extraction with a globally acquired statistical resource",
      "author" : [ "Qiang Ning", "Hao Wu", "Haoruo Peng", "Dan Roth." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Ning et al\\.,? 2018b",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "Who did what: A large-scale person-centered cloze dataset",
      "author" : [ "Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Onishi et al\\.,? 2016",
      "shortCiteRegEx" : "Onishi et al\\.",
      "year" : 2016
    }, {
      "title" : "Iso-timeml and the annotation of temporal information",
      "author" : [ "James Pustejovsky." ],
      "venue" : "Handbook of Linguistic Annotation, pages 941–968. Springer.",
      "citeRegEx" : "Pustejovsky.,? 2017",
      "shortCiteRegEx" : "Pustejovsky.",
      "year" : 2017
    }, {
      "title" : "Temporal and event information in natural language text. Language resources and evaluation, 39(2):123–164",
      "author" : [ "James Pustejovsky", "Robert Knippen", "Jessica Littman", "Roser Saurı" ],
      "venue" : null,
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2005
    }, {
      "title" : "Counterfactual story reasoning and generation",
      "author" : [ "Lianhui Qin", "Antoine Bosselut", "Ari Holtzman", "Chandra Bhagavatula", "Elizabeth Clark", "Yejin Choi." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Qin et al\\.,? 2019a",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Conversing by reading: Contentful neural conversation with on-demand machine reading",
      "author" : [ "Lianhui Qin", "Michel Galley", "Chris Brockett", "Xiaodong Liu", "Xiang Gao", "William B Dolan", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Qin et al\\.,? 2019b",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Back to the future: Backpropagation-based decoding for unsupervised counterfactual and abductive reasoning",
      "author" : [ "Lianhui Qin", "Vered Shwartz", "Peter West", "Chandra Bhagavatula", "Jena D Hwang", "Ronan Le Bras", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "Proc. of",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Event cognition",
      "author" : [ "Gabriel A Radvansky", "Jeffrey M Zacks." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Radvansky and Zacks.,? 2014",
      "shortCiteRegEx" : "Radvansky and Zacks.",
      "year" : 2014
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Annotating events and temporal information in newswire texts",
      "author" : [ "Andrea Setzer", "Robert J Gaizauskas." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Setzer and Gaizauskas.,? 2000",
      "shortCiteRegEx" : "Setzer and Gaizauskas.",
      "year" : 2000
    }, {
      "title" : "cloze procedure”: A new tool for measuring readability",
      "author" : [ "Wilson L Taylor." ],
      "venue" : "Journalism quarterly, 30(4):415–433.",
      "citeRegEx" : "Taylor.,? 1953",
      "shortCiteRegEx" : "Taylor.",
      "year" : 1953
    }, {
      "title" : "SemEval-2013 task 1: TempEval-3: Evaluating time expressions, events, and temporal relations",
      "author" : [ "Naushad UzZaman", "Hector Llorens", "Leon Derczynski", "James Allen", "Marc Verhagen", "James Pustejovsky." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "UzZaman et al\\.,? 2013",
      "shortCiteRegEx" : "UzZaman et al\\.",
      "year" : 2013
    }, {
      "title" : "Temporal reasoning in natural language inference",
      "author" : [ "Siddharth Vashishtha", "Adam Poliak", "Yash Kumar Lal", "Benjamin Van Durme", "Aaron Steven White." ],
      "venue" : "Proc. of Findings of EMNLP.",
      "citeRegEx" : "Vashishtha et al\\.,? 2020",
      "shortCiteRegEx" : "Vashishtha et al\\.",
      "year" : 2020
    }, {
      "title" : "Do language embeddings capture scales? In Proc",
      "author" : [ "Xikun Zhang", "Deepak Ramachandran", "Ian Tenney", "Yanai Elazar", "Dan Roth." ],
      "venue" : "of Findings of EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "going on a vacation” takes longer than “going for a walk”: A study of temporal commonsense understanding",
      "author" : [ "Ben Zhou", "Daniel Khashabi", "Qiang Ning", "Dan Roth." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Temporal common sense acquisition with minimal supervision",
      "author" : [ "Ben Zhou", "Qiang Ning", "Daniel Khashabi", "Dan Roth." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Humans can effortlessly reason about temporal concepts of everyday events such as their duration, frequency, or relative ordering (Allen, 1984; Radvansky and Zacks, 2014) based on rich commonsense knowledge about how the world works, especially in relation to time.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 37,
      "context" : "Humans can effortlessly reason about temporal concepts of everyday events such as their duration, frequency, or relative ordering (Allen, 1984; Radvansky and Zacks, 2014) based on rich commonsense knowledge about how the world works, especially in relation to time.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 15,
      "context" : "However, reasoning about such concepts has been challenging for machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations",
      "startOffset" : 73,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "However, reasoning about such concepts has been challenging for machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations",
      "startOffset" : 73,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "7067 isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al.",
      "startOffset" : 66,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "7067 isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al.",
      "startOffset" : 66,
      "endOffset" : 115
    }, {
      "referenceID" : 44,
      "context" : ", 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 42,
      "context" : ", 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : ", 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 40,
      "context" : "We formulate the dialog-based temporal commonsense reasoning problem as a cloze task (Taylor, 1953).",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 27,
      "context" : "Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 31,
      "context" : "Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : "The TIMEDIAL dataset is derived from DailyDialog data (Li et al., 2017), which is a multi-turn dialog corpus containing over 13K English dialogs.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "Temporal expressions are automatically identified with SUTime (Chang and Manning, 2012), an off-the-shelf",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 42,
      "context" : "7 turns and 3 temporal expressions on average, presenting richer and more complex context compared to the recent single-sentence-based temporal question answering benchmarks (e.g., Zhou et al., 2019; Vashishtha et al., 2020).",
      "startOffset" : 174,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : ", 2019), ALBERT (Lan et al., 2019), ROBERTA (Liu et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 38,
      "context" : "The third method is a fully generative approach using the text-to-text paradigm of T5 (Raffel et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "86 showing almost perfect inter-rater agreement (Landis and Koch, 1977)).",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al.",
      "startOffset" : 93,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al.",
      "startOffset" : 93,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al.",
      "startOffset" : 93,
      "endOffset" : 157
    }, {
      "referenceID" : 39,
      "context" : ", 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al.",
      "startOffset" : 52,
      "endOffset" : 179
    }, {
      "referenceID" : 33,
      "context" : ", 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al.",
      "startOffset" : 52,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : ", 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al.",
      "startOffset" : 52,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : ", 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al.",
      "startOffset" : 52,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : ", 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al.",
      "startOffset" : 52,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : ", 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018).",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : ", 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018).",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 44,
      "context" : "Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 35,
      "context" : "Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "7074 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge.",
      "startOffset" : 11,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al.",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al.",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : ", 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 45,
      "context" : "Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al.",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : ", 2020) and numerical reasoning abilities (Geva et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 61
    } ],
    "year" : 2021,
    "abstractText" : "Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TIMEDIAL. We formulate TIMEDIAL as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. The dataset is publicly available at: https://github.com/ google-research-datasets/timedial.",
    "creator" : "LaTeX with hyperref"
  }
}