{
  "name" : "2021.acl-long.376.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks",
    "authors" : [ "Pengfei Cao", "Xinyu Zuo", "Yubo Chen", "Kang Liu", "Jun Zhao", "Yuguang Chen", "Weihua Peng" ],
    "emails" : [ "jzhao}@nlpr.ia.ac.cn,", "pengweihua}@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4862–4872\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4862"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event causality identification (ECI) aims to identify causal relation of events in texts. For example, in the sentence “The earthquake generated a tsunami.”, an ECI model should be able to identify a causal relationship that holds between the two mentioned events, i.e., earthquake cause−−−→ tsunami. ECI is an important task in natural language processing (NLP) area and can support many NLP applications, such as machine reading comprehension (Berant et al., 2014), process extraction (Thalappillil Scaria et al., 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014).\nIdentifying event causal relation is inherently\nchallenging, because event causality is usually expressed in diverse forms that often lack explicit clues indicating its existence. For example in Figure 1, the sentence has no explicit clue indicating the causal relation between “global warming” and “tsunami”. In this scenario, models can resort to a large amount of labeled data to learn diverse causal expressions. However, existing ECI datasets are very small. For example, the largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, which is not sufficient to train neural network models (Liu et al., 2020). Consequently, models cannot thoroughly understand the text and possibly make a wrong prediction. Nonetheless, humans could make a correct judgement, because humans have the background knowledge about the two events. To be more specific, humans not only know what the two events are, but also know the connection between them. Fortunately, existing knowledge bases (KBs) usually contain the Descriptive Knowledge of events and Relational Knowledge between events, which can be regarded as the background knowledge to enhance ECI models. In this paper, we focus on how to incorporate these two kinds of external knowledge into the task.\nDescriptive Knowledge: The external knowl-\nedge base contains the descriptive or explanatory information about events, which can be called the descriptive knowledge of events. It usually consists of one-hop neighbors of events. This kind of knowledge is able to help the model better understand what the mentioned event is. For example in Figure 1, the descriptive knowledge associated with “global warming” includes (global warming, IsA, temperature change), (global warming, CreatedBy, greenhouse gas) and so on. If the model can make use of such knowledge, it is obvious that the model can better understand the meaning of the event itself than using only the given text. Therefore, incorporating the descriptive knowledge is very helpful for this task. However, when leveraging this kind of knowledge, we find two critical challenges: (1) As shown in Figure 1, the descriptive knowledge forms a sub-graph. How to effectively encode the graph-structured knowledge is a very challenging problem; (2) The knowledge base is incomplete (Wang et al., 2020), which will inevitably cause the descriptive knowledge of some events cannot be obtained from the KB. Thus, the model should have the ability to obtain and encode such knowledge, even if it does not exist in the KB.\nRelational Knowledge: The external knowledge base contains connections between events, which can be referred as the relational knowledge between events. It is usually defined by the multi-hop path between two events. This kind of knowledge can provide useful information for event causality reasoning, especially when the text lacks causal clues. For example in Figure 1, the relational knowledge between the two events is “global warming” Causes−−−−→ “glacier melting” CapableOf−−−−−−→ “sea-level rising” AtLocation−−−−−−→ “ocean” AtLocation←−−−−−− “tsunami”. Apparently, compared with only using text information, utilizing the relational knowledge can provide ample evidence for the model to judge the causality between “global warming” and “tsunami”. However, two challenges exist when using the relational knowledge: (1) The multi-hop path may miss some potentially useful relations. For example in Figure 1, the fact (sea-level rising, Causes, tsunami) is described in the wikipedia page of “sea-level rising”1, while it is not annotated in the KB; (2) Not all the knowledge on the path is related to causality, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc-\n1https://en.wikipedia.org/wiki/Sea_ level_rise\nture may not be optimal. The model should be able to learn a more reasonable structure for capturing potentially useful information and reducing the impact of irrelevant knowledge.\nIn this paper, we propose a novel method termed as Latent Structure Induction Network (LSIN) to overcome aforementioned challenges. Specifically, we devise a Descriptive Graph Induction module to make use of the descriptive knowledge. The module first adopts a hybrid method of retrieval and generation to obtain the descriptive knowledge, and then utilizes the information aggregation technique to encode the graph-structured knowledge. Meanwhile, we propose a Relational Graph Induction module to leverage the relational knowledge. The module first treats the reasoning structure as a latent variable and learns it in an end-to-end fashion. Then, the module performs event causality reasoning based on the induced structure. Experimental results on two widely used datasets demonstrate that our model substantially outperforms previous state-of-the-art methods.\nOur contributions are summarized as follows:\n• We propose a novel Latent Structure Induction Network (LSIN) to leverage the external structural knowledge. To our knowledge, we are the first to use both the descriptive knowledge and relational knowledge for this task.\n• To exploit the descriptive knowledge, we devise a descriptive graph induction module. To utilize the relational knowledge, we propose a relational graph induction module.\n• Experimental results on two widely used datasets indicate that our proposed approach significantly outperforms previous state-ofthe-art methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural\nnetwork-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020). Liu et al. (2020) propose a mention masking generalization method and also consider the external structural knowledge. The very recent work (Zuo et al., 2020) propose a data augmentation method to alleviate the data lacking problem for the task. Regarding datasets construction, Mirza (2014) annotates the CausalTimeBank dataset about event causal relations in the TempEval-3 corpus. Caselli and Vossen (2017) construct a dataset called EventStoryLine for event causality identification. Despite many efforts for this task, most existing methods typically train the models on manually labeled data solely, rarely considering the external structural knowledge. As a result, these methods cannot handle well the cases where there is no explicit causal clue.\nAlthough Liu et al. (2020) leverage the descriptive knowledge to enrich event representations, they directly retrieve the descriptive knowledge from the KB. Therefore, their method cannot handle the cases where there is no knowledge about the event in the KB. In addition, they ignore the relational knowledge between events. By contrast, our method can not only generate the descriptive knowledge when it cannot be retrieved from the KB, but also leverage the relational knowledge. To our knowledge, we are the first to simultaneously make use of the descriptive knowledge and relational knowledge for this task."
    }, {
      "heading" : "3 Methodology",
      "text" : "Following previous works (Ning et al., 2018; Liu et al., 2020), we formulate ECI as a binary clas-\nsification problem. For every pair of events in a sentence, we predict whether a causal relation holds. Figure 2 schematically visualizes our approach, which consists of three major components: (1) Context Encoding (§3.1), which encodes the input sentence and outputs contextualized representations; (2) Descriptive Graph Induction (§3.2), which first obtains the corresponding descriptive knowledge for each event, and then encodes the graph-structured knowledge; (3) Relational Graph Induction (§3.3), which automatically induces a reasoning structure and performs causality reasoning on the induced structure. We will illustrate each component in detail."
    }, {
      "heading" : "3.1 Context Encoding",
      "text" : "Given a sentence with a pair of events (denoted as e1 and e2), the context encoding module aims to extract context features, which takes the sentence as input and outputs the context representations. Our context encoder is based on the Transformer architecture (Vaswani et al., 2017). We adopt the BERT (Devlin et al., 2019) to encode the input sentence,2 which has achieved the state-of-the-art performance for ECI task (Liu et al., 2020; Zuo et al., 2020). After using BERT encoder to compute the contextual representations of the entire sentence, we concatenate representations of [CLS], e1 and e2 as the context representation regarding to the event pair (e1, e2), namely\nF (e1,e2) C = h[CLS] ⊕ he1 ⊕ he2 , (1)\n2Note that the encoder is not our focus in this paper. In fact, other models like convolutional neural networks and long short-term memory networks can also be as encoders.\nwhere ⊕ indicates the concatenation operation. h[CLS] ∈ Rd, he1 ∈ Rd and he2 ∈ Rd are representations of [CLS], e1 and e2, respectively. d is the output hidden size of BERT model."
    }, {
      "heading" : "3.2 Descriptive Graph Induction",
      "text" : ""
    }, {
      "heading" : "3.2.1 Knowledge Obtaining",
      "text" : "Given e1 and e2, we adopt a hybrid method of retrieval and generation to obtain their descriptive knowledge, respectively. The descriptive knowledge forms a sub-graph which is called Descriptive Graph (denoted as Gd). For this paper, we prefer CONCEPTNET (Speer et al., 2017) as the external KB, which contains abundant semantic knowledge of concepts. We take e1 as an example to illustrate the knowledge obtaining procedure:\n(1) If the descriptive knowledge can be retrieved from the KB, we adopt the retrieval method. Our method first grounds e1 to a concept via matching the event mention with the tokens of concepts in CONCEPTNET. We enhance the matching approach with some rules, such as soft matching with lemmatization and filtering of stop words. The grounded concept is called zero-hop concept. Then, our method grows zero-hop concept with one-hop concepts. The zero-hop concept, one-hop concepts and all relations between them form the descriptive graph for e1 (denoted as Gd1).\n(2) If the descriptive knowledge cannot be retrieved from the KB, we adopt the generation method. Our method employs the pre-trained model, COMET (Bosselut et al., 2019), which is originally proposed for the knowledge base completion. Specifically, COMET is obtained by finetuning GPT (Radford et al., 2018) on CONCEPTNET. The input of COMET is the head event and candidate relation, and the output is the tail event. The relation types are the same as the ones used in Bosselut et al. (2019). By leveraging COMET, we can generate the descriptive graph Gd1 for e1.\nIn the same way, we can also construct the descriptive graph Gd2 for e2."
    }, {
      "heading" : "3.2.2 Knowledge Encoding",
      "text" : "Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. In addition, many works show that relational graph convolutional networks (RGCNs) (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively uti-\nlize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019). We thus apply GCNs (Kipf and Welling, 2017) to encode the related descriptive knowledge of e1 and e2.\nFormally, given a descriptive graph Gd (i.e., Gd1 or Gd2) with nd nodes (i.e., concepts), which can be represented with an nd × nd adjacency matrix Ad. If there is a connection between node i and node j, the Adij is set to 1. For the node i at the l-th layer, the convolution computation can be defined as follows:\nu (l) i = ρ( nd∑ j=1 AdijW (l) u u (l−1) j + b (l) u ), (2)\nwhere W (l)u and b (l) u are the weight matrix and bias vector for the l-th layer, respectively. ρ is an activation function (e.g., ReLU). u(0)i ∈ Rd is the initial representation of the i-th node obtained by the pretrained model (i.e., BERT). To consider context information when encoding descriptive knowledge, we use the he1 and he2 obtained in Section 3.1 as the initial representations of events.\nAfter the knowledge encoding, the representations of e1 and e2 in descriptive graphs are denoted as ue1 and ue2 , respectively. We concatenate them as the descriptive knowledge representation:\nF (e1,e2) D = ue1 ⊕ ue2 . (3)"
    }, {
      "heading" : "3.3 Relational Graph Induction",
      "text" : ""
    }, {
      "heading" : "3.3.1 Multi-Hop Path Obtaining",
      "text" : "Given e1 and e2, our model first retrieves the multihop path between the two events from CONCEPTNET. We refer to the multi-hop path as Relational Path. Since shorter connections between two concepts could mean stronger relevance (Lin et al., 2019), our model exploits the shortest path between the two events as the relational path. We represent the CONCEPTNET as a graph, and then use NetworkX toolkit3 to get the shortest path between the two events. When there are multiple shortest paths, we randomly select one path for avoiding information redundancy."
    }, {
      "heading" : "3.3.2 Structure Induction",
      "text" : "To capture potentially useful information and reduce the impact of irrelevant knowledge on the relational path, our model treats the reasoning structure as a latent variable and induces it with the\n3https://networkx.org\ninput of the relational path, which can be shown in Figure 2. We call the induced reasoning structure as Relational Graph (denoted as Gr). The structure induction module is built based on the structured attention (Kim et al., 2017). We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Nan et al., 2020) to learn the graph structure.\nFormally, the nodes of relational graph are the concepts on the relational path. The initialized representation of each node is obtained via the pretrained model (i.e., BERT). The representation of the i-th node is denoted as mi ∈ Rd. We first calculate the pair-wise unnormalized attention score sij between the i-th node and the j-th node:\nsij = (tanh(Wpmi)) TWb(tanh(Wcmj)), (4)\nwhere Wp and Wc are weights matrixes. Wb are the weights for the bilinear transformation. Next, we compute the root score sri which represents the unnormalized probability of the i-th node to be selected as the root node of the structure:\nsri = Wrmi, (5)\nwhere Wr ∈ R1×d is the weight for linear transformation. Suppose the graph Gr has nr nodes, we first assign non-negative weights P ∈ Rnr×nr to the edges of the induced relational graph:\nPij = { 0, if i = j exp(sij), otherwise,\n(6)\nwhere Pij is the weight of the edge between the i-th and the j-th node. Then, following Koo et al. (2007), we define the Laplacian matrix L ∈ Rnr×nr of Gr, and its variant L̂ ∈ Rnr×nr , respectively:\nLij =\n{∑nr k=1Pkj , if i = j\n−Pij , otherwise, (7)\nL̂ij = { exp(sri ), if i = 1 Lij , otherwise.\n(8)\nWe use Arij to denote the marginal probability of the edge between the i-th node and the j-th node, which can be computed as follows:\nArij = (1− δ1,j)Pij [L̂−1]ij − (1− δi,1)Pij [L̂−1]ji,\n(9)\nwhere δ is the Kronecker delta (Koo et al., 2007) and ·−1 denotes matrix inversion. Ar can be regarded as a weighted adjacency matrix of the graph Gr. Finally, Ar is fed into the iterative refinement for event causality reasoning."
    }, {
      "heading" : "3.3.3 Iterative Refinement",
      "text" : "After obtaining the relational graph structure, we perform event causality reasoning on the induced structure. To better capture potential reasoning clues, we adopt the densely connected graph convolutional networks (DCGCNs) (Guo et al., 2019), which allows training a deeper reasoning model. The convolution computation of each layer is:\nv (l) i = ρ( nr∑ j=1 ArijW (l) v g (l) j + b (l) v ), (10)\nwhere g(l)j is the concatenation of the initial node representation and the node representations produced in layers 1, . . . , l − 1, namely g(l)j = mj ⊕ v (1) j ⊕ · · · ⊕ v (l−1) j .\nThe induced structure at once is relatively shallow (Liu et al., 2019; Nan et al., 2020) and may not be optimal for causality reasoning. Therefore, we iteratively refine the induced structure to learn a more informative structure. We stack N blocks (each block is structure induction and DCGCNs reasoning) of this module to induce the structure N times. Intuitively, as the structure gets more refined, the structure is more reasonable.\nAfter the iterative refinement, the representations of e1 and e2 are denoted as ve1 and ve2 , respectively. We concatenate them as the relational knowledge representation:\nF (e1,e2) R = ve1 ⊕ ve2 . (11)"
    }, {
      "heading" : "3.4 Model Prediction and Training",
      "text" : "We concatenate the context representation, descriptive knowledge representation and relational knowledge representation as the final representation:\nFe1,e2 = F (e1,e2) C ⊕ F (e1,e2) D ⊕ F (e1,e2) R . (12)\nTo make the final prediction, we perform a binary classification by taking Fe1,e2 as input:\npe1,e2 = softmax(WsFe1,e2 + bs). (13)\nFor training, we adopt cross entropy as the loss function:\nJ(Θ) = − ∑ s∈D ∑ ei,ej∈Es ei 6=ej yei,ej log(pei,ej ), (14)\nwhere Θ denotes the model parameters. s denotes a sentence in the training set D. Es is the set of events in sentence s. yei,ej is a one-hot vector representing the gold label between ei and ej ."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Evaluation Metrics",
      "text" : "We evaluate our proposed method on two widely used datasets, including EventStoryLine (Caselli and Vossen, 2017) and Causal-TimeBank (Mirza et al., 2014). For EventStoryLine, the dataset contains 258 documents, 5,334 events in total, and 1,770 of 7,805 event pairs are causally related. For Causal-TimeBank, the dataset contains 184 documents, 6,813 events, and 318 of 7,608 event pairs are causally related. We conduct the 5-fold and 10- fold cross-validation on the EventStoryLine dataset and Causal-TimeBank dataset respectively, same as previous methods to ensure fairness. Following previous works (Choubey and Huang, 2017; Gao et al., 2019), we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics."
    }, {
      "heading" : "4.2 Parameter Settings",
      "text" : "In our implementations, our method uses the HuggingFace’s Transformers library4 to implement the uncased BERT base model, which has 12-layers, 768-hidden, and 12-heads. The learning rate is initialized as 2e-5 with a linear decay. We use the Adam algorithm (Kingma and Ba, 2015) to optimize model parameters. The batch size is set to 20. The number of induction blocks (i.e., N ) is set to 2. The dropout of GCN is set to 0.3. Due to the sparseness of positive examples, we adopt a negative sampling strategy for training. The negative sampling rate is 0.6 and 0.7 for the EventStoryLine and Causal-TimeBank, respectively. We utilize CONCEPTNET 5.0 as the external knowledge base."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare the proposed approach LSIN with previous state-of-the-art methods:\nFeature-based methods: (1) Mirza and Tonelli (2014), which proposes a data driven method with causal signals for the task; (2) Mirza (2014), which employs a verb rule based model with data filtering and causal signals enhancement; (3) Choubey and Huang (2017), which proposes a sequence model exploring complex handcrafted features for\n4https://github.com/huggingface/ transformers\nthe task; (4) Gao et al. (2019), which utilizes a logistic regression classifier with the integer linear programming to model causal structure for the task.\nNeural network-based methods: (1) Cheng and Miyao (2017), which proposes a dependency path based bidirectional long short-term memory network (BiLSTM) that models the context between two event mentions for causal relation identification; (2) KMMG (Liu et al., 2020), which proposes a mention masking generalization method and also utilizes the external knowledge; (3) KnowDis (Zuo et al., 2020), which proposes a knowledge enhanced distant data augmentation method to alleviate data lacking problem."
    }, {
      "heading" : "4.4 Overall Results",
      "text" : "Since some baselines are evaluated either on the EventStoryLine dataset or the Causal-TimeBank dataset, the baselines used for the two datasets are different. Table 1 and Table 2 show the results on the EventStoryLine and Causal-TimeBank, respectively. From the tables, we can observe that:\n(1) Our method outperforms all the baselines by a large margin on the two datasets. For example, compared with the state-of-the-art model KnowDis (Zuo et al., 2020), our method LSIN\nachieves 2.8% and 3.1% improvements of F1-score on the EventStoryLine and Causal-TimeBank, respectively. It indicates that our proposed method is very effective for this task.\n(2) Compared with the state-of-the-art model KMMG (Liu et al., 2020), our method achieves 6.0% improvements in terms of Precision score on the EventStoryLine. The reason may be that our method utilizes the relational knowledge between events for causality reasoning, which can improve the confidence of event causality prediction.\n(3) Our method improves upon the BERT model by 8.0% and 11.6% in terms of F1-score on the two datasets, respectively. This suggests that only using the annotated training data is not enough to tackle the task. Moreover, it also indicates that our method is able to effectively leverage the external structural knowledge for ECI task.\n(4) The BERT model achieves comparable performance with complex feature-based methods such as Gao et al. (2019) on the EventStoryLine dataset, which indicates that the BERT is able to extract useful text features for the task."
    }, {
      "heading" : "4.5 Effectiveness of External Structural Knowledge",
      "text" : "We validate the effectiveness of external structural knowledge for this task. Based on the BERT model, we leverage the descriptive knowledge via descriptive graph induction module, and the relational knowledge via relational graph induction module. The results are shown in Table 3. We have two important observations:\n(1) Based on the BERT model, incorporating these two kinds of knowledge can both improve performance. Moreover, simultaneously using these two kinds of knowledge can further improve the performance. It indicates that the external structural knowledge is very effective for this task.\n(2) The performance improvement of using the\nrelational knowledge is more obvious than that of using the descriptive knowledge, achieving 4.0% improvements in terms of F1-score. We guess that the relational knowledge can provide more clues for event causality reasoning."
    }, {
      "heading" : "4.6 Effectiveness of Descriptive Graph Induction",
      "text" : "To verify the effectiveness of descriptive graph induction module, we compare our method with the state-of-the-art model (Liu et al., 2020). Liu et al. (2020) first retrieve the descriptive knowledge, and then transfer the knowledge into a sequence. Finally, they adopt the BERT to encode the knowledge. The results are listed in Table 4. In the table, “DGI-Retrieval”, “DGI-Generation” and “DGI-Hybrid” denote obtaining the descriptive knowledge via retrieval, generation and hybrid method, respectively. Overall, we can observe that:\n(1) The DGI-Hybrid model significantly outperforms Liu et al. (2020), achieving 4.5% improvements of F1-score. Moreover, even if we use the same retrieval method as Liu et al. (2020), our model still achieves better result. It indicates the descriptive graph induction module can better take advantage of the descriptive knowledge.\n(2) Compared with Liu et al. (2020), the DGIHybrid model achieves great improvements in terms of Recall score (i.e., improving 12.6%). The reason is that our method can automatically generate the descriptive knowledge, when the knowledge cannot be retrieved from the KB."
    }, {
      "heading" : "4.7 Effectiveness of Relational Graph Induction",
      "text" : "To validate the effectiveness of the relational graph induction module, we compare our method with other three baselines. The three baselines are illustrated as follows:\n(1) LSTM-based Reasoning, which regards the relational path as a sequence and employs LSTM\nto encode it; (2) Fixed Graph-based Reasoning, which regards the relational path as a graph. Its nodes are concepts on the path and edges only exist between adjacent concepts; (3) Attention-based Reasoning, which uses the self-attention to encode the relational path for modeling the dependencies between arbitrary two concepts.\nThe results are shown in Table 5. From the results, we can observe that:\n(1) Our method LSIN outperforms the three methods by a large margin. For example, compared with LSTM-based reasoning method, our method achieves 4.4% improvements of F1-score. This empirically confirms using induced relational graph structure is more effective than directly using the relational path for causality reasoning.\n(2) Compared with Fixed Graph-based reasoning method, our method achieves 3.6% improvements of F1-score. It indicates that our method is able to effectively capture the potentially useful information and reduce the impact of irrelevant knowledge on the relational path."
    }, {
      "heading" : "4.8 Impact of the Number of Refinements",
      "text" : "We investigate the effect of the refinement on the overall performance. We plot the overall F1-score varying with the number of refinements in Figure 3. From the figure, we can observe that:\n(1) Our method LSIN yields the best performance in the second refinement. Compared with the first induction, the second refinement achieves 1.1% improvements of F1-score on the EventStoryLine dataset. This indicates that the proposed LSIN is able to induce more reasonable reasoning structures by iterative refinement.\n(2) When the number of refinements is too large, the performance on the two datasets stops increasing or even decreases due to over-fitting."
    }, {
      "heading" : "4.9 Case Study",
      "text" : "We conduct case study to further verify the effectiveness of our method. Table 6 shows several cases showing the outputs of BERT and our method LSIN. From the results, we can observe that the BERT model cannot handle the cases where there is no causal clue. By contrast, our method can make correct predictions by leveraging the external structural knowledge. For the second example in Table 6, although the text has no clue indicating the existence of causality between “fights” and “arrested”, there is the relational knowledge between the two events in the KB, namely “fight” HasSubevent−−−−−−−−→ “hurt someone else” HasSubevent−−−−−−−−→ “get arrested”. Our method can make use of the relational knowledge to make a correct prediction. The two examples qualitatively demonstrate our method can effectively leverage the external knowledge for ECI task."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel latent structure induction network (LSIN) to leverage the external structural knowledge for ECI task. To make use of the descriptive knowledge, we devise a descrip-\ntive graph induction module to obtain and encode the graph-structured descriptive knowledge. To utilize the relational knowledge, we propose a relational graph induction module to induce a more reasonable reasoning structure for causality reasoning. Experimental results on two widely used datasets indicate that our approach substantially outperforms previous state-of-the-art methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for their insightful comments and suggestions. This work is supported by the National Key Research and Development Program of China (No. 2020AAA0106400), and the National Natural Science Foundation of China (No. 61806201). This work is also supported by Beijing Academy of Artificial Intelligence (BAAI2019QN0301) and the fund of the joint project with Beijing Baidu Netcom Science Technology Co., Ltd."
    } ],
    "references" : [ {
      "title" : "A sequential model for classifying temporal relations between intra-sentence events",
      "author" : [ "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1796–1802. Asso-",
      "citeRegEx" : "Choubey and Huang.,? 2017",
      "shortCiteRegEx" : "Choubey and Huang.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Minimally supervised event causality identification",
      "author" : [ "Quang Do", "Yee Seng Chan", "Dan Roth." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294–303. Association for Computational Linguis-",
      "citeRegEx" : "Do et al\\.,? 2011",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2011
    }, {
      "title" : "Modeling document-level causal structures for event causal relation identification",
      "author" : [ "Lei Gao", "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Densely connected graph convolutional networks for graph-to-sequence learning",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Zhiyang Teng", "Wei Lu." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:297–312.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Weakly supervised multilingual causality extraction from Wikipedia",
      "author" : [ "Chikara Hashimoto." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Hashimoto.,? 2019",
      "shortCiteRegEx" : "Hashimoto.",
      "year" : 2019
    }, {
      "title" : "Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features",
      "author" : [ "Chikara Hashimoto", "Kentaro Torisawa", "Julien Kloetzer", "Motoki Sano", "István Varga", "Jong-Hoon Oh", "Yutaka Kidawara." ],
      "venue" : "Pro-",
      "citeRegEx" : "Hashimoto et al\\.,? 2014",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2014
    }, {
      "title" : "Inference of fine-grained event causality from blogs and films",
      "author" : [ "Zhichao Hu", "Elahe Rahimtoroghi", "Marilyn Walker." ],
      "venue" : "Proceedings of the Events and Stories in the News Workshop, pages 52–58. Association for Computational Linguistics.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Inferring narrative causality between event pairs in films",
      "author" : [ "Zhichao Hu", "Marilyn Walker." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 342–351. Association for Computational Linguistics.",
      "citeRegEx" : "Hu and Walker.,? 2017",
      "shortCiteRegEx" : "Hu and Walker.",
      "year" : 2017
    }, {
      "title" : "Event causality recognition exploiting multiple annotators’ judgments and background knowledge",
      "author" : [ "Kazuma Kadowaki", "Ryu Iida", "Kentaro Torisawa", "JongHoon Oh", "Julien Kloetzer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Kadowaki et al\\.,? 2019",
      "shortCiteRegEx" : "Kadowaki et al\\.",
      "year" : 2019
    }, {
      "title" : "Structured attention networks",
      "author" : [ "Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush." ],
      "venue" : "5th International Conference on Learning Representations, 2017, Conference Track Proceedings. OpenReview.net.",
      "citeRegEx" : "Kim et al\\.,? 2017",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations, 2017, Conference Track Proceedings. OpenReview.net.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Structured prediction models via the matrix-tree theorem",
      "author" : [ "Terry Koo", "Amir Globerson", "Xavier Carreras", "Michael Collins." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
      "citeRegEx" : "Koo et al\\.,? 2007",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2007
    }, {
      "title" : "Improving event causality recognition with multiple background knowledge sources using multi-column convolutional neu",
      "author" : [ "Canasai Kruengkrai", "Kentaro Torisawa", "Chikara Hashimoto", "Julien Kloetzer", "Jong-Hoon Oh", "Masahiro Tanaka" ],
      "venue" : null,
      "citeRegEx" : "Kruengkrai et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kruengkrai et al\\.",
      "year" : 2017
    }, {
      "title" : "KagNet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge enhanced event causality identification with mention masking generalizations",
      "author" : [ "Jian Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, 2020, pages 3608–3614. ij-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Single document summarization as tree induction",
      "author" : [ "Yang Liu", "Ivan Titov", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Extracting temporal and causal relations between events",
      "author" : [ "Paramita Mirza." ],
      "venue" : "Proceedings of the ACL 2014 Student Research Workshop, pages 10–17. Association for Computational Linguistics.",
      "citeRegEx" : "Mirza.,? 2014",
      "shortCiteRegEx" : "Mirza.",
      "year" : 2014
    }, {
      "title" : "Annotating causality in the TempEval-3 corpus",
      "author" : [ "Paramita Mirza", "Rachele Sprugnoli", "Sara Tonelli", "Manuela Speranza." ],
      "venue" : "Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL), pages",
      "citeRegEx" : "Mirza et al\\.,? 2014",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2014
    }, {
      "title" : "An analysis of causality between events and its relation to temporal information",
      "author" : [ "Paramita Mirza", "Sara Tonelli." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2097–",
      "citeRegEx" : "Mirza and Tonelli.,? 2014",
      "shortCiteRegEx" : "Mirza and Tonelli.",
      "year" : 2014
    }, {
      "title" : "Reasoning with latent structure refinement for document-level relation extraction",
      "author" : [ "Guoshun Nan", "Zhijiang Guo", "Ivan Sekulic", "Wei Lu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546–1557. Asso-",
      "citeRegEx" : "Nan et al\\.,? 2020",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint reasoning for temporal and causal relations",
      "author" : [ "Qiang Ning", "Zhili Feng", "Hao Wu", "Dan Roth." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 2278– 2288. Association for Computational Linguistics.",
      "citeRegEx" : "Ning et al\\.,? 2018",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning causality for news events prediction",
      "author" : [ "Kira Radinsky", "Sagie Davidovich", "Shaul Markovitch." ],
      "venue" : "Proceedings of the 21st World Wide Web Conference, 2012, pages 909–918. ACM.",
      "citeRegEx" : "Radinsky et al\\.,? 2012",
      "shortCiteRegEx" : "Radinsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verbverb associations",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "Proceedings of the SIGdial Meeting on Discourse and Dialogue 2013 Confer-",
      "citeRegEx" : "Riaz and Girju.,? 2013",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2013
    }, {
      "title" : "In-depth exploitation of noun and verb semantics to identify causation in verb-noun pairs",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 161–170. Association",
      "citeRegEx" : "Riaz and Girju.,? 2014",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2014
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European Semantic Web Conference, pages 593–607.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning biological processes with global constraints",
      "author" : [ "Aju Thalappillil Scaria", "Jonathan Berant", "Mengqiu Wang", "Peter Clark", "Justin Lewis", "Brittany Harding", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Meth-",
      "citeRegEx" : "Scaria et al\\.,? 2013",
      "shortCiteRegEx" : "Scaria et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Connecting the dots: A knowledgeable path generator for commonsense question answering",
      "author" : [ "Peifeng Wang", "Nanyun Peng", "Filip Ilievski", "Pedro Szekely", "Xiang Ren." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning cross-lingual entities with multi-aspect information",
      "author" : [ "Hsiu-Wei Yang", "Yanyan Zou", "Peng Shi", "Wei Lu", "Jimmy Lin", "Xu Sun." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph convolution over pruned dependency trees improves relation extraction",
      "author" : [ "Yuhao Zhang", "Peng Qi", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215. Asso-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "KnowDis: Knowledge enhanced data augmentation for event causality detection via distant supervision",
      "author" : [ "Xinyu Zuo", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1544–1550. In-",
      "citeRegEx" : "Zuo et al\\.,? 2020",
      "shortCiteRegEx" : "Zuo et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : ", 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : ", 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "For example, the largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, which is not sufficient to train neural network models (Liu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 177
    }, {
      "referenceID" : 31,
      "context" : "How to effectively encode the graph-structured knowledge is a very challenging problem; (2) The knowledge base is incomplete (Wang et al., 2020), which will inevitably cause the descriptive knowledge of some events cannot be obtained from the KB.",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al.",
      "startOffset" : 98,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al.",
      "startOffset" : 98,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : ", 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al.",
      "startOffset" : 34,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : ", 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al.",
      "startOffset" : 34,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : ", 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : ", 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : ", 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : ", 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task.",
      "startOffset" : 45,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 184
    }, {
      "referenceID" : 34,
      "context" : "network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 184
    }, {
      "referenceID" : 34,
      "context" : "The very recent work (Zuo et al., 2020) propose a data augmentation method to alleviate the data lacking problem for the task.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "Following previous works (Ning et al., 2018; Liu et al., 2020), we formulate ECI as a binary classification problem.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "Following previous works (Ning et al., 2018; Liu et al., 2020), we formulate ECI as a binary classification problem.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 30,
      "context" : "Our context encoder is based on the Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "We adopt the BERT (Devlin et al., 2019) to encode the input sentence,2 which has achieved the state-of-the-art performance for ECI task (Liu et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : ", 2019) to encode the input sentence,2 which has achieved the state-of-the-art performance for ECI task (Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 34,
      "context" : ", 2019) to encode the input sentence,2 which has achieved the state-of-the-art performance for ECI task (Liu et al., 2020; Zuo et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 28,
      "context" : "For this paper, we prefer CONCEPTNET (Speer et al., 2017) as the external KB, which contains abundant semantic knowledge of concepts.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "Specifically, COMET is obtained by finetuning GPT (Radford et al., 2018) on CONCEPTNET.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme.",
      "startOffset" : 76,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme.",
      "startOffset" : 76,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "In addition, many works show that relational graph convolutional networks (RGCNs) (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al.",
      "startOffset" : 82,
      "endOffset" : 110
    }, {
      "referenceID" : 33,
      "context" : ", 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : ", 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019).",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "We thus apply GCNs (Kipf and Welling, 2017) to encode the related descriptive knowledge of e1 and e2.",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "Since shorter connections between two concepts could mean stronger relevance (Lin et al., 2019), our model exploits the shortest path between the two events as the relational path.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "The structure induction module is built based on the structured attention (Kim et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Nan et al., 2020) to learn the graph structure.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Nan et al., 2020) to learn the graph structure.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "Aij = (1− δ1,j)Pij [L̂]ij − (1− δi,1)Pij [L̂]ji, (9) where δ is the Kronecker delta (Koo et al., 2007) and ·−1 denotes matrix inversion.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "To better capture potential reasoning clues, we adopt the densely connected graph convolutional networks (DCGCNs) (Guo et al., 2019), which allows training a deeper reasoning model.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "The induced structure at once is relatively shallow (Liu et al., 2019; Nan et al., 2020) and may not be optimal for causality reasoning.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "The induced structure at once is relatively shallow (Liu et al., 2019; Nan et al., 2020) and may not be optimal for causality reasoning.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "We evaluate our proposed method on two widely used datasets, including EventStoryLine (Caselli and Vossen, 2017) and Causal-TimeBank (Mirza et al., 2014).",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "Following previous works (Choubey and Huang, 2017; Gao et al., 2019), we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Following previous works (Choubey and Huang, 2017; Gao et al., 2019), we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "We use the Adam algorithm (Kingma and Ba, 2015) to optimize model parameters.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Neural network-based methods: (1) Cheng and Miyao (2017), which proposes a dependency path based bidirectional long short-term memory network (BiLSTM) that models the context between two event mentions for causal relation identification; (2) KMMG (Liu et al., 2020), which proposes a mention masking generalization method and also utilizes the external knowledge; (3) KnowDis (Zuo et al.",
      "startOffset" : 247,
      "endOffset" : 265
    }, {
      "referenceID" : 34,
      "context" : ", 2020), which proposes a mention masking generalization method and also utilizes the external knowledge; (3) KnowDis (Zuo et al., 2020), which proposes a knowledge enhanced distant data augmentation method to alleviate data lacking problem.",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 34,
      "context" : "For example, compared with the state-of-the-art model KnowDis (Zuo et al., 2020), our method LSIN",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "(2) Compared with the state-of-the-art model KMMG (Liu et al., 2020), our method achieves 6.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "To verify the effectiveness of descriptive graph induction module, we compare our method with the state-of-the-art model (Liu et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 139
    } ],
    "year" : 2021,
    "abstractText" : "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.",
    "creator" : "LaTeX with hyperref"
  }
}