{
  "name" : "2021.acl-long.173.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Data Augmentation for Text Generation Without Any Augmented Data",
    "authors" : [ "Wei Bi", "Huayang Li", "Jiacheng Huang" ],
    "emails" : [ "victoriabi@tencent.com", "alanili@tencent.com", "eziohuang@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2223–2237\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2223"
    }, {
      "heading" : "1 Introduction",
      "text" : "End-to-end neural models are generally trained in a data-driven paradigm. Many researchers have proposed powerful network structures to fit training data well. It has also become ubiquitous to increase the training data amount to improve model performance. Data augmentation is an effective technique to create additional samples in both vision and text classification tasks (Perez and Wang, 2017; Shorten and Khoshgoftaar, 2019; Wei and Zou, 2019), which perturb samples without changing their labels. For text generation tasks, there can be more types of data perturbation to construct augmented samples, including corrupting the input text (Xie et al., 2017), the output text (Norouzi et al., 2016; Kurata et al., 2016), or both (Zhang et al., 2020). As such, classification tasks can be regarded as special cases of generation tasks in terms of incorporating data augmentation techniques, and this work mainly discusses text generation tasks.\n∗Equal contribution.\nThe focus of previous work on text data augmentation has been to design proper augmentation techniques to create augmented samples. Some augmentation methods have been proposed for general text tasks. For example, different general replacement operations have been explored to edit words in a text sample, ranging from simple look-up tables (Zhang et al., 2015) to pretrained masked language models (Kobayashi, 2018; Wu et al., 2019). Sennrich et al. (2016) propose to augment text sequences by back-translation. For some generation tasks such as dialogue generation, general augmentation methods may not yield stable improvements and it requires to carefully incorporate the task property to design useful augmented samples (Zhang et al., 2020). All these methods need to explicitly construct augmented samples, and the data mapping functions from the original samples to the augmented samples are mostly defined apriori. This motivates us to raise a question, whether we can skip the step to define or choose proper augmented data mapping functions to accomplish effective data augmentation.\nTo answer this question, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions. We start from a conventional data augmentation objective, which is a weighted combination of loss functions associated with the original and augmented samples. We show that the loss parts of the augmented samples can be re-parameterized by variables not dependent on the augmented data mapping functions, if a simple Euclidean loss function between the sentence representations is applied. Based on this observation, we propose to directly define a distribution on the re-parameterized variables. Then we optimize the expectation of the augmented loss parts over this distribution to approximate the original augmented loss parts computed with various augmented data\nmapping functions. We make different assumptions on the variable distributions and find that our proposed objective can be computed and optimized efficiently by simple gradient weighting. If stochastic gradient descent (SGD) is used, our objective is guaranteed with the convergence rate O(1/ √ T ). Our objective can be coupled with popular loss functions on text generation tasks, including the word mover’s distance (Kusner et al., 2015) and the cross-entropy loss.\nOur approach, which utilizes the proposed objective and optimizes it by SGD, has two advantages. First, it provides a unified formulation of various data perturbation types in general text generation models, which sheds a light on understanding the working mechanism of data augmentation. Second, the optimization of our approach is simple and efficient. Without introducing any new sample during training, we can avoid additional calculation efforts on augmented samples, often with the total size much larger than the original data size. Hence, our approach maintains high training efficiency.\nExtensive experiments are conducted to validate the effectiveness of our approach. We mainly use the LSTM-based network structure (Bahdanau et al., 2015; Luong et al., 2015b) and perform experiments on two text generation tasks - neural machine translation and single-turn conversational response generation. Results on five datasets demonstrate that the proposed approach can approximate or even surpass popular data augmentation methods such as masked language model (Devlin et al., 2019) and back-translation (Sennrich et al., 2016)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Data augmentation has shown promising improvements on neural models for different text generation tasks such as language modeling (Xie et al., 2017), machine translation (Sennrich et al., 2016) and dialogue generation (Niu and Bansal, 2019; Cai et al., 2020). Existing text data augmentation methods can be mainly categorized into word-level augmentation and sentence-level augmentation.\nWord-level augmentation methods perturb words within the original sentence. Common operations include word insertion and deletion (Wei and Zou, 2019), synonym replacement (Zhang et al., 2015), and embedding mix-up (Guo et al., 2019). Masked language models can be used by masking some percentages of tokens at random, and predicting the masked words based on its context (Wu et al.,\n2019; Cai et al., 2020). Sentence-level data augmentation is not limited to edit only a few words in the original sentence, but to generate a complete sentence. For example, back-translation is originally proposed to translate monolingual target language data into source language to augment training pairs in machine translation (Sennrich et al., 2016). It is later extended to paraphrase sentences in any text dataset, in which two translation models are applied: one translation model from the source language to target language and another from the target to the source. GANbased and VAE-based models have also achieved impressive results to create entire sentences to augment the training data (Hu et al., 2017; Cheng et al., 2019). For dialogue generation, retrieved sentences can be good supplement of the original corpus (Zhang et al., 2020).\nBoth word-level and sentence-level augmentation methods need to define their augmented data mapping functions (i.e. operations to edit words or models to generate sentences) apriori. Some works train policies to sample a set of word-level operations (Niu and Bansal, 2019), but the operation candidates are still pre-defined. A few works learn to construct augmented samples and optimize the network jointly (Hu et al., 2019; Cai et al., 2020). Different from previous work, our goal is not to propose or learn novel augmented data mapping functions. Instead, we investigate whether the effectiveness of data augmentation can be achieved while we do not bother to use any specific augmented data mapping function.\nBesides data augmentation, data weighting is another useful way to improve model learning. It assigns a weight to each sample to adapt its importance during training. The sample weights are often carefully defined (Freund and Schapire, 1997; Bengio et al., 2009) or learnt by another network (Jiang et al., 2018; Shu et al., 2019). Data augmentation is often combined with data weighting together to weight the original and augmented samples."
    }, {
      "heading" : "3 Background",
      "text" : "We are given original samples D = {(x,y)} with x,y both as text sequences. Without loss of generality, a deep generation model is to learn a mapping function fx,y by a deep neural network that outputs y given x. As mentioned in the introduction, text generation tasks mainly have three types of augmented data:\n• one (or several) perturbed input text x̂ by one (or several) augmented data mapping function φx̂; • one (or several) perturbed output text ŷ by one (or several) augmented data mapping functions φŷ; • one (or several) perturbed paired text (x̂, ŷ) by corresponding augmented data mapping functions. Proper augmented data mapping functions are often supposed to generate perturbed sequences or sequence pairs that are close to the original one. They are assumed to be given apriori in optimizing the generation model for now.\nLet `(fx,y(x),y) denote the loss function to be minimized for each sample. We first use augmented data in the input domain as an example to present the problem formulation and introduce our approach, then later discuss other types of augmented data. Data augmentation methods generally apply an augmented loss per sample with its augmented samples:\n`aug = `(fx,y(x),y) + ∑\nx̂:φx̂∈F wx̂`(fx,y(x̂),y)\n(1) where wx̂ is the importance weight associated with each augmented sample, φx̂ is the augmented data mapping function that constructs x̂, and F is the function space containing all feasible augmented data mapping functions."
    }, {
      "heading" : "4 Our Approach",
      "text" : "In this section, we aim to formulate the problem of data augmentation for general text generation models without any use of augmented data mapping functions. We introduce our approach by assuming that the loss function ` is the most simple Euclidean distance, i.e.\n`(u,v) = ‖u− v‖2 (2)\nwhere u and v are the sentence representations of two sentences, i.e. the target sequence and the predicted sequence. Other conventional loss functions in text generation will be discussed in Section 5.\nWe first rewrite each loss part of an augmented data point in (1) from a polar coordinate system in Sec 4.1. In this way, we can regard the total augmented loss part with multiple augmented data mapping functions as sampling different points in the polar coordinate system. This inspires us that we can skip to define any augmented data mapping function, but only design a joint distribution of the\nperturbation radius and perturbation angle in the polar coordinate system. In Sec 4.2, we show two probability distribution substantiations, and find that our approach can be optimized efficiently by simply re-weighting the gradients. In Sec 4.3, we discuss the extension of our approach for other augmented data mapping function types."
    }, {
      "heading" : "4.1 Proposed Objective",
      "text" : "By treating fx,y(x), fx,y(x̂) and y as three vertices in the Euclidean space, we can form a triangle (illustrated in Fig. 1a) with the three vertices and the loss between them as edges. For a given augmented data mapping function φx̂ and a sample (x,y), we can rewrite `(fx,y(x̂),y) using the polar coordinate system with fx,y(x) as the pole and (fx,y(x),y) as the polar axis:\n`2(fx,y(x̂),y) =\n`2(fx,y(x),y) + ` 2(fx,y(x), fx,y(x̂)) −2`(fx,y(x), fx,y(x̂))`(fx,y(x),y) cos θ (3)\nwhere θ is the radian of fx,y(x̂). We can observe that, the rewritten augmented sample loss part depends on the original sample loss `(fx,y(x),y) as well as the radius r and radian θ of fx,y(x̂). Here r is the data perturbation distance `(fx,y(x), fx,y(x̂)). Therefore, we can map each augmented data mapping function φx̂ ∈ F into (r, θ) ∈ P , where P is a joint distribution of (r, θ) 1. A weighted summation of the augmented loss parts from different augmented data mapping functions can be seen as an empirical estimation of the expectation of the rewritten loss by sampling different (r, θ)’s from their joint distribution P , though the corresponding ground truth P is not observed.\nThis inspires us how to avoid to specifically design or choose several augmented data mapping functions and their weights used in (1). We can directly design the distribution P of (r, θ) and optimize the expectation of the rewritten loss (i.e. the right hand side in (3)) under this distribution. Hence, we propose to optimize the following objective to mimic the effect of data augmentation:\n1It is worth pointing out that even if the three vertices (i.e., fx,y(x̂), y, and fx,y(x) ) lie in high dimensional spaces, we can always use the distribution of (r, θ) cover all possible triangles formed by them. And our derivation will not lose its generalization in high dimensional spaces, since we does not make use of the vertices but only edges of the triangles.\n`our=`(fx,y(x),y)+E(r,θ)∈P [Φ(`(fx,y(x),y))] (4) where Φ(e; r, θ) is a function of an edge e in the loss function space given (r, θ):\nΦ(e; r, θ) = √ e2 + r2 − 2er cos θ. (5)"
    }, {
      "heading" : "4.2 Optimization",
      "text" : "We design specific distributions of (r, θ) used in the proposed objective (4) and their optimization. We assume the two variables are independent:\np(r, θ) = p(r)p(θ). (6)\nIn the following corollary, we first show the result by assuming that both r and θ follow uniform distributions. Recall that proper data mapping functions augment samples close to the original one. An ideal case is thus to perturb samples with their output representations uniformly surrounding that of the original sample. The uniform distribution with a small perturbation radius upper boundR can simulate this ideal case.\nCorollary 1. We are given the perturbation distance upper bound R and assume that\nr ∼ U(0, R), θ ∼ U(0, π). (7)\nE(r,θ)∈P [Φ(`(fx,y(x),y))] is upper bounded by 1 2`(fx,y(x),y) + C1 · `\n2(fx,y(x),y) + C2(R), where C1 is a constant and C2(R) is another constant dependent on R.\nProof is in the Appendix. With the above result, we can optimize the objective in (4) by minimizing the derived upper bound. We calculate its gradient:\n∂`our ∂Θ = 3 2 · ∂`(Θ) ∂Θ + 2C1 · `(Θ) ∂`(Θ) ∂Θ (8)\nwhere Θ contains all neural model parameters. It can be observed that the major difference of the above gradient compared with the original one of the objective in (1) lies in the second part of (8), which weights the original gradient by the loss value. This means that the performance improvement brought by data augmentation under our formulation can be equivalently accomplished by specialized data weighting. Indeed, many data weighting methods (Lin et al., 2017) favors hard examples by reducing the gradient contribution from easy examples and increasing the importance of hard examples (example with large loss value in our approach), which significantly boost the performance. This in turn shows that simple uniform distributions assumed here should be reasonable and effective.\nInstead of uniform distribution, we can assume a uniform distribution on θ but an exponential distribution on r such that a small perturbation distance is preferred with a higher probability. Corollary 2. We are given the expected value of the perturbation distance as R and assume that\nr ∼ Exp( 1 R ), θ ∼ U(0, π). (9)\nE(r,θ)∈P [Φ(`(fx,y(x),y))] is upper bounded by C1(R) · `(fx,y(x),y) + C1(R)2 · `\n2(fx,y(x),y) + C2(R), where C1(R) and C2(R) are constants dependent on R. Proof is in the Appendix. The above corollary shows that even if different distributions are assumed, we can still use gradient weighting to optimize the proposed objective, where C1(R) can be set as a hyper-parameter.\nIf the loss is Lipschitz smooth, of which Euclidean distance is the case, we can prove the convergence of our approach with the convergence rate\nO(1/ √ T ), if SGD is used. The proof is provided in the Appendix, which is extended from results in Reddi et al. (2016).\nTheorem 1. Suppose `our is in the class of finitesum Lipschitz smooth functions, has δ-bounded gradients, and the weight of the loss gradient is clipped to be bounded by [w1, w2]. Let the learning rate of SGD αt = c/ √ T where c =√\n2(`our(Θ0)−`our(Θ∗)) Lσ2w1w2\nwhere L is the Lipschitz constant and Θ∗ is an optimal solution. Then the iterates of SGD of our approach with `our satisfy:\nmin 0≤t≤T−1 E[||∇`our(Θt)||2] ≤√ 2(`our(Θ0)− `our(Θ∗))Lw1\nTw2 σ. (10)"
    }, {
      "heading" : "4.3 Other Types of Augmented Data",
      "text" : "We now discuss how our approach can be applied to other types of augmented data. For augmented data on the output domain, the objective in (1) becomes:\n`aug = `(fx,y(x),y) + ∑ φŷ∈F wŷ`(fx,y(x), ŷ).\n(11) The augmented loss part can be rewritten using the polar coordinate system with y as the pole and (y, fx,y(x)) as the polar axis, illustrated in Fig. 1b:\n`2(fx,y(x), ŷ) = ` 2(y, fx,y(x)) + ` 2(y, ŷ)\n−2`(y, fx,y(x))`(y, ŷ) cos θ. (12)\nSimilarly, the augmented data mapping function φŷ can be re-parameterized into a function of the radius r = `(y, ŷ) (still the perturbation distance) and the radian of ŷ. The objective turns out to be the same as (4).\nFor data perturbation on both the input and output space, we have:\n`aug = `(fx,y(x),y) + ∑\nφx̂,ŷ∈F wx̂,ŷ`(fx,y(x̂), ŷ).\n(13) Illustrated in Fig. 1c, we first make use of the triangle inequality that:\n`(fx,y(x̂), ŷ) ≤ 1\n2 (`(fx,y(x̂),y) + `(y, ŷ))\n+ 1\n2 (`(fx,y(x̂), fx,y(x)) + `(fx,y(x), ŷ)).\n(14)\nUsing (3) and (12), the objective is rewritten as:\n`our = `(fx,y(x),y)\n+E(r,θ)∈P [r + Φ(`(fx,y(x),y))]. (15)\nNote that E(r,θ)∈P [r] is a scalar which is not dependent on any learning parameter. Thus optimizing the above objective is equivalent to optimizing (4).\nFrom the above analysis, we can see that our proposed objective in (4) can be applied to handle all three kinds of augmented data mapping functions in text generation models."
    }, {
      "heading" : "5 Loss Function",
      "text" : "In theory, our approach can be applied to any Lipschitz smooth loss function that holds the equation (3). In this section, we show another valid loss function in our approach – the word mover’s distance (WMD) (Kusner et al., 2015; Zhao et al., 2019), which is previously used in various text generation tasks. Next, we discuss the cross entropy loss, in which the proposed objective is not an upper-bound of the data augmentation objective. However, our approach can still converge with the same convergence rate and experimental results in the next section validate the effectiveness of our approach with the cross-entropy loss."
    }, {
      "heading" : "5.1 Word Mover’s Distance",
      "text" : "WMD, also named the optimal transport distance (Chen et al., 2018a), leverages optimal transport to find an optimal matching of similar words between two sequences, providing a way to measure their semantic similarity:\n`WMD(u,v) = min Ti,j ∑ i,j Ti,jdi,j (16)\ns.t. M∑ j=1 Ti,j = pu,i ∀i\nN∑ i=1 Ti,j = pv,j ∀j\nwhere pu,i/pv,j is the probability distribution of the sentence, i.e. ∑ i pu,i = 1 and ∑ j pv,j = 1. di,j is the cost for mis-predicting ui to vj , where the squared Euclidean distance di,j = ‖ui − vj‖2 is used and ui/vj is the word embedding vector. Note that the Euclidean distance in (2) is a special case of WMD by replacing the 1-gram used in WMD\nto n-gram with n larger than the sentence’s length. WMD is the squared L2 Wasserstein distance. We take its squared root, i.e. `WD = √ `WMD, which holds an upper bound as the right hand side in (3). Also, `WD is Lipschitz smooth.\nTheorem 2. For the L2 Wasserstein distance W2(·, ·) on the Wasserstein space W 2(Rn) and any x, y, z ∈W 2(Rn), we have\nW2(y, z) 2 ≤W2(x, y)2 +W2(z, x)2\n−2 ·W2(x, y) ·W2(z, x) · cos θ. (17)\nHere θ is the angel between the γxy and γzx, γxy is the geodesic (shortest path) connecting x, y in W 2(Rn), and γzx is the geodesic connecting z, x in W 2(Rn). Theorem 3. u and v are given as fixed. Assuming that uΘ is Lipschitz continuous with respect to the parameters Θ. Then `WD(uΘ,v) is Lipschitz continuous with respect to the parameters Θ.\nRoughly speaking, according to Sturm et al. (2006)[Proposition 2.10], the sectional curvature of Wasserstein spaceW 2(Rn) is non-negative. Hence, every geodesic triangle in W 2(Rn) is fatter than the one with same sides length in R2. As a consequence, an inequality like cosine law is satisfied on W 2(Rn), i.e., Theorem 2 holds. A formal proof of the above two theorems is provided in the Appendix. Thus, all our derivations in Section. 4 hold.\nThe exact computation of `WD is expensive during training. In our experiments, we resort to the inexact proximal point method for optimal transport algorithm to compute it (Chen et al., 2018a)."
    }, {
      "heading" : "5.2 Cross-entropy Loss",
      "text" : "Although WMD is effective for various sequence generation tasks, the most conventional loss function adopted in existing generation models is the cross-entropy loss. It measures the word difference at each word yi of the output sequence y:\n`CE(yi,pi) = y T i log(pi) (18)\n`CE(y,p) = |y|∑ i=1 `CE(yi,pi) (19)\nwhere yi is the target one-hot vector with the correct dimension as 1 and 0 elsewhere, and pi is the predicted probability output by a softmax layer. We adopt the maximum likelihood estimation as the training paradigm by assuming truth for preceding words in predicting pi.\nThe cross-entropy loss is also Lipschitz smooth, and thus we can guarantee its convergence from Theorem 1. Unfortunately, it does not satisfy the equation in (3), and thus minimizing our objective in (4) does not necessarily approximate the data augmentation objective in (1). In our experiments, we also try the cross-entropy loss, and results show that our objective is effective to improve the model performance compared with the base model. This is not surprising since our approach is optimized by gradient weighting and thus at least it is a useful data weighting method."
    }, {
      "heading" : "6 Experiments",
      "text" : "The proposed approach provides a new paradigm and understanding of data augmentation for text generation. To evaluate that our approach can mimic the effect of data augmentation, we conduct experiments on two text generation tasks – neural machine translation and conversational response generation. We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text generation tasks: • Masked Language model (MLM): We use a pretrained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence. BERT takes in these masked words to predict these masked positions with new words. We augment one sample from each original training sample. Thus the data size increases to twice of the original one. Note that we only augment the English side of translation datasets. • Back-translation (BT): For neural machine translation, we employ a fixed target-to-source translation model trained on the original dataset. For conversational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an Englishto-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs2. We again augment one sample from each original training sample.\nWe set the same weight w of all augmented loss parts used in `aug as a hyper-parameter, and tune it on the development set of each dataset. Since Euclidean distance is a special case of WMD as dis-\n2Datasets used in this work can be found at https: //nlp.stanford.edu/projects/nmt/,http: //coai.cs.tsinghua.edu.cn/hml/dataset/ #commonsense\ncussed in Sec 5.1, we show results of all methods with the use of the cross-entropy loss and WD. We mainly use the Fairseq (Ott et al., 2019) Seq2seq implementation as our model. Both encoder and decoder are one-layer LSTM. The word embedding dimension is 256. Attention (Luong et al., 2015b) is used with a dropout rate of 0.1. All parameters are randomly initialized based on the uniform distribution [−0.1,+0.1]. We use SGD to optimize our models, and the learning rate is started with 1.0. After 8 epochs, we start to halve the learning rate after each epoch. All experiments are run on a single NVIDIA V100 GPU. Code for our experiments are available once our work is accepted."
    }, {
      "heading" : "6.1 Neural Machine Translation",
      "text" : "We use translation benchmarks IWSLT14 En–De, En–Fr, En–It, and IWSLT15 En–Vi in our experiments. The datasets of IWSLT14 are pre-processed with the script in Fairseq 3. For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set. The IWSLT15 dataset is the same as that used in Luong et al. (2015a), and the validation and test sets are tst2012 and tst2013, respectively.\nTable 1 shows the BLEU scores on their test sets. For both cross-entropy loss and L2 Wasserstein distance, all data augmentation methods (MLM, BT and OURS) perform better than the corresponding base models in most cases. The improvement margins are different across the various datasets. The reason may be that the datasets are in different scales and the alignment difficulty between different languages can also vary. The performance of MLM is not stable from our results, which is largely due to that masked tokens are possible to\n3https://github.com/pytorch/fairseq/ blob/master/examples/translation/ prepare-iwslt14.sh\nbe filled in with different semantic ones and thus the semantics of the sentence changes. Therefore, the augmented data are not aligned indeed, and the translation model learning can be distracted. Note that we also evaluate our method using the Transformer model and get some similar findings. Experimental results of the Transformer model are presented in the appendix.\nCompared to BT and MLM, our approach that mimics the effect of data augmentation without actually constructing augmented samples, shows encouraging results. Note that our proposed objective may not have a theoretical guarantee on the cross-entropy loss. Yet, it still manages to improve the base model except for Fr⇒En, and surpasses MLM on all datasets. With the use of L2 Wasserstein distance, our approach even outperforms BT and achieves the best performance on half test sets. This validates the benefits of not using any specific data augmentation mapping function in data augmentation as in our proposed objective.\nWe provide further analysis on the performance of our approach versus BT. In Fig. 2, we compare testing BLEU scores obtained by models updated with the same number of samples. Since we construct one augmented sample from each original training sample, the total number of samples used in BT is twice as much as that of our approach. We can see that our approach achieves compatible performance with BT, while only requires half of the training data. This shows that our approach, without involving additional calculations on extra samples, can effectively save the computational expense. Fig. 3 shows the sensitivity of performance under different hyper-parameters. For our approach, we vary across different C1(R)’s; for BT, we vary the sample weight w of the augmented samples. We re-scale C1(R) by 10−4 and w by 10−1, in order to visualize them within the same\nrange of x-axis. Both BT and our approach demonstrate their robustness under different settings of their hyper-parameters."
    }, {
      "heading" : "6.2 Conversational Response Generation",
      "text" : "We use the English single-round Reddit conversation dataset (Zhou et al., 2018). Following previous work on data augmentation for dialogue system (Cai et al., 2020; Zhang et al., 2020), we simulate a low data regime so that data augmentation is expected to be more effective. Thus, we select data pairs with the length of both the query and response less than 20, and randomly split them into 200K for training, 2K for validation and 5K for testing. Automatic evaluation for each method is performed on all test data. We report Perplexity, BLEU and BLEU-k (k=1,2) to measure the response coherence; Distinct-k (k=1,2) (Li et al., 2016) to measure the response diversity. We also hire five annotators from a commercial annotation company for manual evaluation on 200 pairs randomly sampled from the test set. Results of all methods are shuffled for annotation fairness. Each annotator rates each response on a 5-point scale (1: not acceptable; 3: acceptable; 5: excellent; 2 and 4: used in unsure case) from two perspectives: Fluency and Relevance.\nResults are summarized in Table 2. On automatic metrics, BT only shows marginal improvements on a few metrics, which can not exhibit its strength as in translation tasks. MLM effectively increases the response diversity (Dist1&2). This is due to nature of the conversation data that conversation pair often remains coherent even if the semantics of the query or response has been slightly\nchanged. Thus, MLM can increase data diversity, which is appreciated in training response generation models. In terms of human evaluation, BT and MLM can barely improve the base model. As for our approach, it achieves the best or second best results on most metrics for both loss functions, demonstrating more robust performance than BT and MLM. This is consistent with our statement in the introduction that we often need to design proper augmented data mapping functions carefully for a target generation task, which requires non-trivial work. As such, it is meaningful to avoid the use of specific data augmentation techniques and find a unified formulation of data augmentation for general generation tasks. From our results, the proposed objective demonstrates its power to achieve the effect of data augmentation across different generation tasks."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We have proposed an objective of formulating data augmentation without any use of any augmented data mapping function. We show its optimization and provide the corresponding convergence rate. Both the L2 Wasserstein distance and the crossentropy loss are discussed with their use in our objective and their corresponding theoretical guarantees. Different from previous data augmentation works that need to add manipulated data into the training process, our gradient based approach provides a potential way to obtain performance improvements, which may come from augmented data, without incurring the computational expense. Experiments on both neural machine translation and conversational response generation validate the effectiveness of our objective compared to existing popular data augmentation methods: masked language models and back-translation.\nWe believe this work provides a new understanding of data augmentation. Our approach can also be useful to a wide range of tasks including text classification tasks, which can be seen as special cases of text generation tasks, and cross-modality generation tasks such as image captioning, in which we can skip the step to use various image augmentation techniques.\nWe would like to point out that some parts of our approach can be improved in the future, which may lead to a better performance and generalization. Firstly, current distributions we choose in the re-parameterized loss are relatively simple. Some\npoints under current continuous distributions may not correspond to valid text sequences in the original text space, due to the discreteness of natural languages. A possible way is that we change to leverage more informative distributions, such as including prior distributions computed from several augmented samples. Secondly, our method is derived under the framework of SGD and it is possible to extend it to the Adam framework (Kingma and Ba, 2014; Chen et al., 2018b; Reddi et al., 2019). We also leave the more general version of our work in the future."
    }, {
      "heading" : "A Proof of Corollary 1",
      "text" : "E(r,θ)∈P [ √ L2 + r2 − 2Lr cos θ]\n= ∫ R r=0 ∫ π θ=0 1 R · 1 π · √ L2 + r2 − 2Lr cos θdrdθ,\n= ∫ R r=0 1 R · 1 π ( ∫ π/2 θ=0 √ L2 + r2 − 2Lr cos θdθ + ∫ π θ=π/2 √ L2 + r2 − 2Lr cos θdθ)dr\n≤ ∫ R r=0 1 R 1 2 ( √ L2 + r2 + L+ r)dr\n= 1\n2 L+\nR 4 + 1 2R ∫ R r=0 √ L2 + r2dr\n≤ 1 2 L+ R 4 + 1 2R ∫ R r=0 1 + L2 + r2 2 dr = 1\n2 L+ L2C1 + C2(R). (20)\nwhere L = `(fx,y(x), y), C1 = 14 , C2(R) = R2 12 + R 4 + 1 4 .\nB Proof of Corollary 2\n∫ ∞ r=0 ∫ π θ=0 1 R exp(− r R ) 1 π ( √ L2 + r2 − 2Lr cos θdrdθ\n≤ ∫ R r=0 R exp(− r R ) 1 2 ( √ L2 + r2 + L+ r)dr\n= ∫ R r=0 R exp(− r R ) 1 2 (L+ r)dr + ∫ R r=0 R exp(− r R ) 1 2 ( √ L2 + r2)dr = R2\n2 (1− e−1)L+ R\n3\n2 (1− 2e−1) + ∫ R r=0 R exp(− r R ) 1 2 ( √ L2 + r2)dr (21)\n≤ R 2 2 (1− e−1)L+ R 3 2 (1− 2e−1) + ∫ R r=0 R exp(− r R ) 1 + L2 + r2 4 dr (22) = LC1(R) + L 2C1(R)\n2 + C2(R)\nwhere C1(R) = (1− e−1)R 2 2 , and C2(R) = R3 2 + 3R2 4 − ( R3 2 + R4 2 )e −1."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "We study the nonconvex finite-sum problems of the form\nmin Θ L(Θ) := 1 n n∑ i=1 `our(Θ, xi, yi), (23)\nwhere both L and `our may be nonconvex. For ease of notation, we use ` to denote `our in the following of the proof. We denote the class of such finite-sum Lipschitz smooth functions by Fn. We optimize functions in Fn with the gradient in Eq. 8 by SGD. For L ∈ Fn, SGD takes an index i ∈ [n] and a sample in the training set, and returns the pair (`i(Θ),∇`i(Θ)). Definition 1. We say L : Rd → R is L-smooth if there is a constant L such that\n||∇`(Θ′)−∇`(Θ)|| ≤ L||Θ′ −Θ||,∀Θ′,Θ ∈ Rd. (24)\nDefinition 2. A point Θ is called -accurate if ||∇`(Θ)||2 ≤ . A stochastic iterative algorithm is said to achieve -accuracy in t iterations if E[||∇`(Θt)||2] ≤ , where the expectation is over the stochasticity of the algorithm.\nDefinition 3. We say ` ∈ Fn has σ-bounded gradients if ||∇`i(θ)|| ≤ σ for all i ∈ [n] and Θ ∈ Rd.\nLet αt denote the learning rate at iteration t, and wit be the gradient weight assigned to sample i by our approach. By SGD, we have\nΘt+1 = Θt − αtwit∇`it(Θt), i ∈ [n]. (25)\nDefinition 4. We say the positive gradient weight w in our approach is bounded if there exist constants w1 and w2 such that w1 ≤ wi ≤ w2 for all i ∈ [n].\nProof of Theorem1. According to the Lipschitz continuity of ∇`, the iterates of our approach satisfy the following bound:\nE[`(Θt+1)] ≤ E[`(θt) + 〈∇`(Θt),Θt+1 −Θt〉+ L 2 ||Θt+1 −Θt||2]. (26)\nAfter substituting (25) into (26), we have:\nE[`(Θt+1)] ≤ E[`(Θt)]− αtwtE[||∇`(Θt)||2] + Lα2tw 2 t\n2 E[||∇`it(Θt)||2]\n≤ E[`(Θt)]− αtwtE[||∇`(Θt)||2] + Lα2tw 2 t\n2 σ2. (27)\nThe first inequality follows from the unbiasedness of the stochastic gradient Eit [∇`it(Θt)] = ∇`(Θt). The second inequality uses the assumption on gradient boundedness in Definition 3. Re-arranging (27) we obtain\nE[||∇`(Θt)||2] ≤ 1 αtwt E[`(Θt)− `(Θt+1)] + Lαtwt 2 σ2. (28)\nSumming (28) from t = 0 to T − 1 and using that αt is a fixed α, we obtain\nmin t E[||∇`(Θt)||2] ≤ 1 T T−1∑ t=0 E[||∇`(Θt)||2]\n≤ 1 T T−1∑ t=0 1 αwt E[`(θt)− `(θt+1)] + 1 T T−1∑ t=0 Lαwt 2 σ2 ≤ 1 Tαw2 ( `(Θ0 − `(ΘT ) ) + Lαw1 2 σ2 ≤ 1 Tαw2 ( `(Θ0 − `(Θ∗) ) + Lαw1 2 σ2 ≤ 1√ T ( 1 cw2 (`(Θ0)− `(Θ∗)) + Lcw1 2 σ2 ) . (29)\nThe first step holds because the minimum is less than the average. The second step is obtained from (28). The third step follows from the assumption on gradient weight boundedness in Definition 4. The fourth step is obtained from the fact that `(Θ∗) ≤ `(ΘT ). The final inequality follows upon using α = c/ √ T .\nBy setting c = √\n2(`(Θ0)−`(Θ∗)) Lσ2w1w2 in the above inequality, we get the desired result.\nD Proof of `WD We begin with some concepts in mathematics. Let (X, | · , · |) be a complete metric space. Definition 5. A rectifiable curve γ(t) : I ⊂ R+ → X connecting two points p, q is called a geodesic if its length is equal to |p, q| and it has unit speed. Here, we say that γ(t) : I → X has unit speed, if for any s, t ∈ I , s < t, we have, the length of the restriction\nγ : [s, t]→ X\nis t − s. A metric space X is called a geodesic space if, for every pair of points p, q ∈ X , there exists some geodesic connecting them.\nDefinition 6. We say that, a geodesic space (X, |· , ·|) has non-negative curvature in the sense of Alexandrov, if it satisfies the following property:\n• for any p ∈ X , and for any unit speed geodesics γ(s) : I → X and σ(t) : J → X with γ(0) = σ(0) := p, the comparison angle\n∠̃γ(s)pσ(t) := arccos\n( t2 + s2 − |γ(s), σ(t)|2\n2 · s · t ) is non-increasing with respect to each of the variables t and s.\nThe angle between γ and σ at p is defined by\nlim s,t→0+ arccos\n( t2 + s2 − |γ(s), σ(t)|2\n2 · s · t\n) ∈ [0, π]."
    }, {
      "heading" : "In other words, every geodesic triangle in X is fatter than the one with sides length in R2 (Figure 4).",
      "text" : "According to Sturm et al. (2006)[Proposition 2.10], the Wasserstein space W 2(Rn) has non-negative curvature in the sense of Alexandrov. Precisely,\nLemma 1. Sturm et al. (2006)[Proposition 2.10] Let n ≥ 1. The Wasserstein space W 2(Rn) equipped with the L2 Wasserstein distance W2(·, ·) has non-negative curvature in the sense of Alexandrov.\nProof of Theorem 2. Let X = W 2(Rn) and |· , ·| be the L2 Wasserstein distance. For any x, y, z ∈ X , we denote by γxy (γzx) the geodesic connecting x and y (resp. z and x). By the above Lemma, X has non-negative curvature in the sense of Alexandrov, hence according to Definition 6, one can define the angle between γxy and γzx at x, denoted by θ, and we have\nθ ≥ ∠̃yxz := arccos ( |x, y|2 + |z, x|2 − |y, z|2\n2 · |x, y| · |z, x|\n) ,\nwhich implies\ncos θ ≤ |x, y| 2 + |z, x|2 − |y, z|2\n2 · |x, y| · |z, x| .\nEquivalently, |y, z|2 ≤ |x, y|2 + |z, x|2 − 2|x, y| · |z, x| · cos θ.\nHence, we complete the proof.\nProof of Theorem 3. We derive from the definition of `WD and the triangle inequality for the L2 Wasserstein distance that for any Θ,Θ′,\n‖`WD(uΘ,v)− `WD(uΘ′ ,v)‖ ≤ `WD(uΘ′ ,uΘ)\n= ` 1/2 WMD(uΘ′ ,uΘ)\n≤ ∑ i,j Ti,jdi,j 1/2\nwhere Ti,j satisfies ∑ j Ti,j = puΘ,i ∀i, ∑ i Ti,j = puΘ′ ,j ∀j.\nTake Ti,j = δij · puΘ,i. According to the assumption that uΘ is Lipschitz continuous with respect to the parameters Θ, we have\ndi,i = ‖uΘ,i − uΘ′,i‖2 ≤ L · ‖Θ′ −Θ‖2\nfor some constant L > 0. Hence, we get that∑ i,j Ti,jdi,j 1/2 ≤ (∑ i Ti,i · L · ‖Θ′ −Θ‖2 )1/2\n= (∑ i Ti,i )1/2 · L1/2 · ‖Θ′ −Θ‖ = L1/2 · ‖Θ′ −Θ‖.\nFinally, we got ‖`WD(uΘ,v)− `WD(uΘ′ ,v)‖ ≤ L1/2 · ‖Θ′ −Θ‖.\nHence, we complete the proof."
    }, {
      "heading" : "E Experimental Results of Transformer",
      "text" : "We also evaluate our method using the Transformer architecture on two translation tasks. To prevent the model from over-fitting, we use a Transformer model with a 2-layer encoder and a 2-layer decoder. Other hyper-parameters are almost the same as in Vaswani et al. (2017), except for the optimizer. In our experiment, we use SGD to train the model, instead of Adam (Vaswani et al., 2017), since our approach is derived under SGD. Results are shown in Table 3, which are consistent with the observations from the LSTM model. We hope that our approach and theoretical analysis can be extended to the Adam framework (Kingma and Ba, 2014; Chen et al., 2018b; Reddi et al., 2019) in the future."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyung Hyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML), pages 41–48.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight",
      "author" : [ "Hengyi Cai", "Hongshen Chen", "Yonghao Song", "Cheng Zhang", "Xiaofang Zhao", "Dawei Yin." ],
      "venue" : "Proceedings of the Annual",
      "citeRegEx" : "Cai et al\\.,? 2020",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving sequence-to-sequence learning via optimal transport",
      "author" : [ "Liqun Chen", "Yizhe Zhang", "Ruiyi Zhang", "Chenyang Tao", "Zhe Gan", "Haichao Zhang", "Bai Li", "Dinghan Shen", "Changyou Chen", "Lawrence Carin." ],
      "venue" : "Proceedings of the International",
      "citeRegEx" : "Chen et al\\.,? 2018a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "On the convergence of a class of adamtype algorithms for non-convex optimization",
      "author" : [ "Xiangyi Chen", "Sijia Liu", "Ruoyu Sun", "Mingyi Hong." ],
      "venue" : "arXiv preprint arXiv:1808.02941.",
      "citeRegEx" : "Chen et al\\.,? 2018b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 4324–4333.",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A decisiontheoretic generalization of on-line learning",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : null,
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Augmenting data with mixup for sentence classification: An empirical study",
      "author" : [ "Hongyu Guo", "Yongyi Mao", "Richong Zhang." ],
      "venue" : "arXiv preprint arXiv:1905.08941.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning data manipulation for augmentation and weighting",
      "author" : [ "Zhiting Hu", "Bowen Tan", "Russ R Salakhutdinov", "Tom M Mitchell", "Eric P Xing." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 15764–15775.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML), pages 1587–1596.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels",
      "author" : [ "Lu Jiang", "Zhengyuan Zhou", "Thomas Leung", "Li-Jia Li", "Li Fei-Fei." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Jiang et al\\.,? 2018",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Labeled data generation with encoder-decoder lstm for semantic slot filling",
      "author" : [ "Gakuto Kurata", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Proceddings of the Conference of the International Speech Communication Association (INTERSPEECH), pages 725–729.",
      "citeRegEx" : "Kurata et al\\.,? 2016",
      "shortCiteRegEx" : "Kurata et al\\.",
      "year" : 2016
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Weinberger." ],
      "venue" : "Proceedings of International Conference on Machine Learning (ICML), pages 957–966.",
      "citeRegEx" : "Kusner et al\\.,? 2015",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Focal loss for dense object detection",
      "author" : [ "Tsung-Yi Lin", "Priya Goyal", "Ross Girshick", "Kaiming He", "Piotr Dollár." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2980–2988.",
      "citeRegEx" : "Lin et al\\.,? 2017",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "2015a. Stanford neural machine translation systems for spoken language domains",
      "author" : [ "Minh-Thang Luong", "Christopher D Manning" ],
      "venue" : "In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),",
      "citeRegEx" : "Luong and Manning,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong and Manning",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015b",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatically learning data augmentation policies for dialogue tasks",
      "author" : [ "Tong Niu", "Mohit Bansal." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Lan-",
      "citeRegEx" : "Niu and Bansal.,? 2019",
      "shortCiteRegEx" : "Niu and Bansal.",
      "year" : 2019
    }, {
      "title" : "Reward augmented maximum likelihood for neural structured prediction",
      "author" : [ "Mohammad Norouzi", "Samy Bengio", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans" ],
      "venue" : "In Advances In Neural Information Processing Systems (NeurIPS),",
      "citeRegEx" : "Norouzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2016
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "The effectiveness of data augmentation in image classification using deep learning",
      "author" : [ "Luis Perez", "Jason Wang." ],
      "venue" : "arXiv preprint arXiv:1712.04621.",
      "citeRegEx" : "Perez and Wang.,? 2017",
      "shortCiteRegEx" : "Perez and Wang.",
      "year" : 2017
    }, {
      "title" : "Stochastic variance reduction for nonconvex optimization",
      "author" : [ "Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex Smola." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML), pages 314–323.",
      "citeRegEx" : "Reddi et al\\.,? 2016",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2016
    }, {
      "title" : "On the convergence of adam and beyond",
      "author" : [ "Sashank J Reddi", "Satyen Kale", "Sanjiv Kumar." ],
      "venue" : "arXiv preprint arXiv:1904.09237.",
      "citeRegEx" : "Reddi et al\\.,? 2019",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 86–96.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey on image data augmentation for deep learning",
      "author" : [ "Connor Shorten", "Taghi M Khoshgoftaar." ],
      "venue" : "Journal of Big Data, 6(1):60.",
      "citeRegEx" : "Shorten and Khoshgoftaar.,? 2019",
      "shortCiteRegEx" : "Shorten and Khoshgoftaar.",
      "year" : 2019
    }, {
      "title" : "Metaweight-net: Learning an explicit mapping for sample weighting",
      "author" : [ "Jun Shu", "Qi Xie", "Lixuan Yi", "Qian Zhao", "Sanping Zhou", "Zongben Xu", "Deyu Meng." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 1919–1930.",
      "citeRegEx" : "Shu et al\\.,? 2019",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "On the geometry of metric measure spaces",
      "author" : [ "Karl-Theodor Sturm" ],
      "venue" : "Acta mathematica, 196(1):65–131.",
      "citeRegEx" : "Sturm,? 2006",
      "shortCiteRegEx" : "Sturm",
      "year" : 2006
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in the Neural Information Processing Systems (NeurIPS), pages 6000–6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Con-",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing: Sys-",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "Proceedings of the International Conference on Computational Science (ICCS), pages 84–95.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Data noising as smoothing in neural network language models",
      "author" : [ "Ziang Xie", "Sida I Wang", "Jiwei Li", "Daniel Lévy", "Aiming Nie", "Dan Jurafsky", "Andrew Y Ng." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "Dialogue distillation: Open-domain dialogue augmentation using unpaired data",
      "author" : [ "Rongsheng Zhang", "Yinhe Zheng", "Jianzhi Shao", "Xiaoxi Mao", "Yadong Xi", "Minlie Huang." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), 28:649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M Meyer", "Steffen Eger." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense knowledge aware conversation generation",
      "author" : [ "Hao Zhou", "Tom Young", "Minlie Huang", "Haizhou Zhao", "Jingfang Xu", "Xiaoyan Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Experimental Results of Transformer We also evaluate our method using the Transformer architecture on two translation tasks. To prevent the model from over-fitting, we use a Transformer model with a 2-layer encoder and a 2-layer decoder. Other hyper-parameters are almost the same as in Vaswani et al",
      "author" : [ "E proof" ],
      "venue" : null,
      "citeRegEx" : "proof.,? \\Q2017\\E",
      "shortCiteRegEx" : "proof.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Data augmentation is an effective technique to create additional samples in both vision and text classification tasks (Perez and Wang, 2017; Shorten and Khoshgoftaar, 2019; Wei and Zou, 2019), which perturb samples without changing their labels.",
      "startOffset" : 118,
      "endOffset" : 191
    }, {
      "referenceID" : 27,
      "context" : "Data augmentation is an effective technique to create additional samples in both vision and text classification tasks (Perez and Wang, 2017; Shorten and Khoshgoftaar, 2019; Wei and Zou, 2019), which perturb samples without changing their labels.",
      "startOffset" : 118,
      "endOffset" : 191
    }, {
      "referenceID" : 31,
      "context" : "Data augmentation is an effective technique to create additional samples in both vision and text classification tasks (Perez and Wang, 2017; Shorten and Khoshgoftaar, 2019; Wei and Zou, 2019), which perturb samples without changing their labels.",
      "startOffset" : 118,
      "endOffset" : 191
    }, {
      "referenceID" : 34,
      "context" : "For text generation tasks, there can be more types of data perturbation to construct augmented samples, including corrupting the input text (Xie et al., 2017), the output text (Norouzi et al.",
      "startOffset" : 140,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : ", 2017), the output text (Norouzi et al., 2016; Kurata et al., 2016), or both (Zhang et al.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : ", 2017), the output text (Norouzi et al., 2016; Kurata et al., 2016), or both (Zhang et al.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "For example, different general replacement operations have been explored to edit words in a text sample, ranging from simple look-up tables (Zhang et al., 2015) to pretrained masked language models (Kobayashi, 2018; Wu et al.",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : ", 2015) to pretrained masked language models (Kobayashi, 2018; Wu et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 79
    }, {
      "referenceID" : 33,
      "context" : ", 2015) to pretrained masked language models (Kobayashi, 2018; Wu et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 79
    }, {
      "referenceID" : 35,
      "context" : "For some generation tasks such as dialogue generation, general augmentation methods may not yield stable improvements and it requires to carefully incorporate the task property to design useful augmented samples (Zhang et al., 2020).",
      "startOffset" : 212,
      "endOffset" : 232
    }, {
      "referenceID" : 15,
      "context" : "Our objective can be coupled with popular loss functions on text generation tasks, including the word mover’s distance (Kusner et al., 2015) and the cross-entropy loss.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "We mainly use the LSTM-based network structure (Bahdanau et al., 2015; Luong et al., 2015b) and perform experiments on two text generation tasks - neural machine translation and single-turn conversational response generation.",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "We mainly use the LSTM-based network structure (Bahdanau et al., 2015; Luong et al., 2015b) and perform experiments on two text generation tasks - neural machine translation and single-turn conversational response generation.",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Results on five datasets demonstrate that the proposed approach can approximate or even surpass popular data augmentation methods such as masked language model (Devlin et al., 2019) and back-translation (Sennrich et al.",
      "startOffset" : 160,
      "endOffset" : 181
    }, {
      "referenceID" : 34,
      "context" : "Data augmentation has shown promising improvements on neural models for different text generation tasks such as language modeling (Xie et al., 2017), machine translation (Sennrich et al.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : ", 2017), machine translation (Sennrich et al., 2016) and dialogue generation (Niu and Bansal, 2019; Cai et al.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 31,
      "context" : "Common operations include word insertion and deletion (Wei and Zou, 2019), synonym replacement (Zhang et al.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 36,
      "context" : "Common operations include word insertion and deletion (Wei and Zou, 2019), synonym replacement (Zhang et al., 2015), and embedding mix-up (Guo et al.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 33,
      "context" : "Masked language models can be used by masking some percentages of tokens at random, and predicting the masked words based on its context (Wu et al., 2019; Cai et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Masked language models can be used by masking some percentages of tokens at random, and predicting the masked words based on its context (Wu et al., 2019; Cai et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 26,
      "context" : "For example, back-translation is originally proposed to translate monolingual target language data into source language to augment training pairs in machine translation (Sennrich et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "GANbased and VAE-based models have also achieved impressive results to create entire sentences to augment the training data (Hu et al., 2017; Cheng et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "GANbased and VAE-based models have also achieved impressive results to create entire sentences to augment the training data (Hu et al., 2017; Cheng et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 35,
      "context" : "For dialogue generation, retrieved sentences can be good supplement of the original corpus (Zhang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "Some works train policies to sample a set of word-level operations (Niu and Bansal, 2019), but the operation candidates are still pre-defined.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "A few works learn to construct augmented samples and optimize the network jointly (Hu et al., 2019; Cai et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "A few works learn to construct augmented samples and optimize the network jointly (Hu et al., 2019; Cai et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "The sample weights are often carefully defined (Freund and Schapire, 1997; Bengio et al., 2009) or learnt by another network (Jiang et al.",
      "startOffset" : 47,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "The sample weights are often carefully defined (Freund and Schapire, 1997; Bengio et al., 2009) or learnt by another network (Jiang et al.",
      "startOffset" : 47,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : ", 2009) or learnt by another network (Jiang et al., 2018; Shu et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", 2009) or learnt by another network (Jiang et al., 2018; Shu et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Indeed, many data weighting methods (Lin et al., 2017) favors hard examples by reducing the gradient contribution from easy examples and increasing the importance of hard examples (example with large loss value in our approach), which significantly boost the performance.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "In this section, we show another valid loss function in our approach – the word mover’s distance (WMD) (Kusner et al., 2015; Zhao et al., 2019), which is previously used in various text generation tasks.",
      "startOffset" : 103,
      "endOffset" : 143
    }, {
      "referenceID" : 37,
      "context" : "In this section, we show another valid loss function in our approach – the word mover’s distance (WMD) (Kusner et al., 2015; Zhao et al., 2019), which is previously used in various text generation tasks.",
      "startOffset" : 103,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "1 Word Mover’s Distance WMD, also named the optimal transport distance (Chen et al., 2018a), leverages optimal transport to find an optimal matching of similar words between two sequences, providing a way to measure their semantic similarity:",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "In our experiments, we resort to the inexact proximal point method for optimal transport algorithm to compute it (Chen et al., 2018a).",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text generation tasks: • Masked Language model (MLM): We use a pretrained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence.",
      "startOffset" : 243,
      "endOffset" : 283
    }, {
      "referenceID" : 22,
      "context" : "We mainly use the Fairseq (Ott et al., 2019) Seq2seq implementation as our model.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "Attention (Luong et al., 2015b) is used with a dropout rate of 0.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 38,
      "context" : "We use the English single-round Reddit conversation dataset (Zhou et al., 2018).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "Following previous work on data augmentation for dialogue system (Cai et al., 2020; Zhang et al., 2020), we simulate a low data regime so that data augmentation is expected to be more effective.",
      "startOffset" : 65,
      "endOffset" : 103
    }, {
      "referenceID" : 35,
      "context" : "Following previous work on data augmentation for dialogue system (Cai et al., 2020; Zhang et al., 2020), we simulate a low data regime so that data augmentation is expected to be more effective.",
      "startOffset" : 65,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "We report Perplexity, BLEU and BLEU-k (k=1,2) to measure the response coherence; Distinct-k (k=1,2) (Li et al., 2016) to measure the response diversity.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "Secondly, our method is derived under the framework of SGD and it is possible to extend it to the Adam framework (Kingma and Ba, 2014; Chen et al., 2018b; Reddi et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "Secondly, our method is derived under the framework of SGD and it is possible to extend it to the Adam framework (Kingma and Ba, 2014; Chen et al., 2018b; Reddi et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 174
    }, {
      "referenceID" : 25,
      "context" : "Secondly, our method is derived under the framework of SGD and it is possible to extend it to the Adam framework (Kingma and Ba, 2014; Chen et al., 2018b; Reddi et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 174
    } ],
    "year" : 2021,
    "abstractText" : "Data augmentation is an effective way to improve the performance of many neural text generation models. However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee. Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods.",
    "creator" : "LaTeX with hyperref"
  }
}