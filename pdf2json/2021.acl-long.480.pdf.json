{
  "name" : "2021.acl-long.480.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation",
    "authors" : [ "Zhiyong Wu", "Lingpeng Kong", "Wei Bi", "Xiang Li", "Ben Kao", "Hong Kong" ],
    "emails" : [ "zywu@cs.hku.hk,", "lpk@cs.hku.hk,", "kao@cs.hku.hk,", "victoriabi@tencent.com,", "xiangli@dase.ecnu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6153–6166\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6153"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality (Specia et al., 2016; Wang et al., 2019). Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT.\nIn particular, Specia et al. (2016); Elliott et al. (2017); Barrault et al. (2018) argue that visual context does not seem to help translation reliably, at\n∗The majority of this work was done while the first author was interning at Tencent AI Lab.\nleast as measured by automatic metrics. Elliott (2018); Grönroos et al. (2018a) provide further evidence by showing that MMT models are, in fact, insensitive to visual input and can translate without significant performance losses even in the presence of features derived from unrelated images. A more recent study (Caglayan et al., 2019), however, shows that under limited textual context (e.g., noun words are masked), models can leverage visual input to generate better translations. But it remains unclear where the gains of MMT methods come from, when the textual context is complete.\nThe main tool utilized in prior discussion is adversarial model comparison — explaining the behavior of complex and black-box MMT models by comparing performance changes when given adversarial input (e.g., random images). Although such an opaque tool is an acceptable beginning to investigate the need for visual context in MMT, they provide rather indirect evidence (Hessel and Lee, 2020). This is because performance differences can often be attributed to factors unrelated to visual input, such as regularization (Kukačka et al., 2017), data bias (Jabri et al., 2016), and some others (Dodge et al., 2019).\nFrom these perspectives, we revisit the need for visual context in MMT by designing two interpretable models. Instead of directly infusing visual features into the model, we design learnable components, which allow the model to voluntarily decide the usefulness of the visual features and reinforce their effects when they are helpful. To our surprise, while our models are shown to be effective on Multi30k (Elliott et al., 2016) and VaTex (Wang et al., 2019) datasets, they learn to ignore the multimodal information. Our further analysis suggests that under sufficient textual context, the improvements come from a regularization effect that is similar to random noise injection (Bishop, 1995) and weight decay (Hanson and Pratt, 1989). The addi-\ntional visual information is treated as noise signals that can be used to enhance model training and lead to a more robust network with lower generalization error (Salamon and Bello, 2017). Repeating the evaluation under limited textual context further substantiates our findings and complements previous analysis (Caglayan et al., 2019).\nOur contributions are twofold. First, we revisit the need for visual context in the popular task of multimodal machine translation and find that: (1) under sufficient textual context, the MMT models’ improvements over text-only counterparts result from the regularization effect (Section 5.2). (2) under limited textual context, MMT models can leverage visual context to help translation (Section 5.3). Our findings highlight the importance of MMT models’ interpretability and the need for a new benchmark to advance the community.\nSecond, for the MMT task, we provide a strong text-only baseline implementation and two models with interpretable components that replicate similar gains as reported in previous works. Different from adversarial model comparison methods, our models are interpretable due to the specifically designed model structure and can serve as standard baselines for future interpretable MMT studies. Our code is available at https://github. com/LividWo/Revisit-MMT."
    }, {
      "heading" : "2 Background",
      "text" : "One can broadly categorize MMT systems into two types: (1) Conventional MMT, where there is gold alignment between the source (target) sentence pair and a relevant image and (2) Retrieval-based MMT, where systems retrieve relevant images from an image corpus as additional clues to assist translation.\nConventional MMT Most MMT systems require datasets consist of images with bilingual annotations for both training and inference. Many early attempts use a pre-trained model (e.g., ResNet (He et al., 2016)) to encode images into feature vectors. This visual representation can be used to initialize the encoder/decoder’s hidden vectors (Elliott et al., 2015; Libovický and Helcl, 2017; Calixto et al., 2016). It can also be appended/prepended to word embeddings as additional input tokens (Huang et al., 2016; Calixto and Liu, 2017). Recent works (Libovický et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder. For\ninstance, Doubly-ATT (Calixto et al., 2017; Helcl et al., 2018; Arslan et al., 2018) insert an extra visual attention sub-layer between the decoder’s source-target attention sub-layer and feed-forward sub-layer. While there are more works on engineering decoders, encoder-based approaches are relatively less explored. To this end, Yao and Wan (2020) and Yin et al. (2020) replace the vanilla Transformer encoder with a multi-modal encoder.\nBesides the exploration on network structure, researchers also propose to leverage the benefits of multi-tasking to improve MMT (Elliott and Kádár, 2017; Zhou et al., 2018). The Imagination architecture (Elliott and Kádár, 2017; Helcl et al., 2018) decomposes multimodal translation into two subtasks: translation task and an auxiliary visual reconstruction task, which encourages the model to learn a visually grounded source sentence representation.\nRetrieval-based MMT The effectiveness of conventional MMT heavily relies on the availability of images with bilingual annotations. This could restrict its wide applicability. To address this issue, Zhang et al. (2020) propose UVR-NMT that integrates a retrieval component into MMT. They use TF-IDF to build a token-to-image lookup table, based on which images sharing similar topics with a source sentence are retrieved as relevant images. This creates image-bilingual-annotation instances for training. Retrieval-based models have been shown to improve performance across a variety of NLP tasks besides MMT, such as question answering (Guu et al., 2020), dialogue (Weston et al., 2018), language modeling (Khandelwal et al., 2019), question generation (Lewis et al., 2020), and translation (Gu et al., 2018)."
    }, {
      "heading" : "3 Method",
      "text" : "In this section we introduce two interpretable MMT models: (1) Gated Fusion for conventional MMT and (2) Dense-Retrieval-augmented MMT (RMMT) for retrieval-based MMT. Our design philosophy is that models should learn, in an interpretable manner, to which degree multimodal information is used. Following this principle, we focus on the component that integrates multimodal information. In particular, we use a gating matrix Λ (Yin et al., 2020; Zhang et al., 2020) to control the amount of visual information to be blended into the textual representation. Such a matrix facilitates interpreting the fusion process: a larger gating value Λij ∈ [0, 1] indicates that the model exploits more\nvisual context in translation, and vice versa."
    }, {
      "heading" : "3.1 Gated Fusion MMT",
      "text" : "Given a source sentence x of length T and an associated image z, we compute the probability of generating target sentence y of length N by:\np(y|x, z) = N∏ i pθ (yi | x, z, y<i) , (1)\nwhere pθ (yi | x, z, y<i) is implemented with a Transformer-based (Vaswani et al., 2017) network. Specifically, we first feed x into a vanilla Transformer encoder to obtain a textual representation Htext ∈ RT×d, which is then fused with visual representation Embed image (z) before fed into the Transformer decoder. For each image z, we use a pre-trained ResNet-50 CNN (He et al., 2016) to extract a 2048-dimensional average-pooled visual representation, which is then projected to the same dimension as Htext:\nEmbed image (z) = Wz ResNetpool (z) . (2)\nWe next generate a gating matrix Λ ∈ [0, 1]T×d to control the fusion of Htext and Embed image (z): Λ = sigmoid ( WΛ Embed image (z) + UΛHtext ) ,\nwhere WΛ and UΛ are model parameters. Note that this gating mechanism has been a building block for many recent MMT systems (Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020). We are, however, the first to focus on its interpretability. Finally, we generate the output vector H by:\nH = Htext + Λ Embed image (z). (3)\nH is then fed into the decoder directly for translation as in vanilla Transformer."
    }, {
      "heading" : "3.2 Retrieval-Augmented MMT (RMMT)",
      "text" : "RMMT consists of two sequential components: (1) an image retriever p(z|x) that takes x as input and returns Top-K most relevant images from an image database; (2) a multi-modal translator p(y|x,Z) = ∏N i pθ (yi | x,Z, y<i) that generates each yi conditioned on the input sentence x, the image set Z returned by the retriever, and the previously generated tokens y<i.\nImage Retriever Based on the TF-IDF model, searching in existing retrieval-based MMT (Zhang et al., 2020) ignores the context information of a given query, which could lead to poor performance. To improve the recall of our image retriever, we compute the similarity between a sentence x and an image z with inner product:\nsim(x, z) = Embed text (x)> Embed image(z),\nwhere Embedtext(x) and Embedimage(z) are ddimensional representations of x and z, respectively. We then retrieve top-K images that are closest to x. For Embedimage(z), we compute it by Eq. 2. For Embedtext(x), we implement it using BERT (Devlin et al., 2019):\nEmbed text (x) = Wtext BERTCLS (x) . (4)\nFollowing standard practices, we use a pre-trained BERT model1 to obtain the “pooled” representation of the sequence (denoted as BERTCLS(x)). Here, Wtext is a projection matrix.\nMultimodal Translator Different from Gated Fusion, p(y|x,Z) now is conditioning on a set of images rather than one single image. For each z in Z , we represent it using Embedimage(z) ∈ Rd as in Equation 2. The image set Z then forms a feature matrix Embedimage(Z) ∈ RK×d, where K = |Z| and each row corresponds to the feature vector of an image. We use a transformation layer fθ(∗) to extract salient features from Embedimage(Z) and obtain a compressed representation Rd of Z . After the transformation, ideally, we can implement p(y|x,Z) using any existing MMT models. For interpretability, we follow the Gated Fusion model to fuse the textual and visual representations with a learnable gating matrix Λ:\nH = Htext + Λfθ( Embed image (Z)). (5)\nHere, fθ(∗) denotes a max-pooling layer with window size K × 1."
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we evaluate our models on the Multi30k and VaTex benchmark."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We perform experiments on the widely-used MMT datasets: Multi30k. We follow a standard split\n1Here we use bert-base-uncased version.\nof 29,000 instances for training, 1,014 for validation and 1,000 for testing (Test2016). We also report results on the 2017 test set (Test2017) with extra 1,000 instances and the MSCOCO test set that includes 461 more challenging out-of-domain instances with ambiguous verbs. We merge the source and target sentences in the officially preprocessed version of Multi30k2 to build a joint vocabulary. We then apply the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 10,000 merging operations to segment words into subwords, which generates a vocabulary of 9,712 (9,544) tokens for En-De (En-Fr). Retriever pre-training. We pre-train the retriever on a subset of the Flickr30k dataset (Plummer et al., 2015) that has overlapping instances with Multi30k removed. We use Multi30k’s validation set to evaluate the retriever. We measure the performance by recall-at-K (R@K), which is defined as the fraction of queries whose closest K images retrieved contain the correct images. The pre-trained retriever achievesR@1 of 22.8% andR@5 of 39.6%."
    }, {
      "heading" : "4.2 Setup",
      "text" : "We experiment with different model sizes (Base, Small, and Tiny, see Appendix A for details). Base is a widely-used model configuration for Transformer in both text-only translation (Vaswani et al., 2017) and MMT (Grönroos et al., 2018b; Ive et al., 2019). However, for small datasets like Multi30k, training such a large model (about 50 million parameters) could cause overfitting. In our preliminary study, we found that even a Small configuration, which is commonly used for low-resourced translation (Zhu et al., 2019), can still overfit on Multi30k. We therefore perform grid search on the En→De validation set in Multi30k and obtain a Tiny configuration that works surprisingly well.\nWe use Adam with β1 = 0.9, β2 = 0.98 for model optimization. We start training with a warmup phase (2,000 steps) where we linearly increase the learning rate from 10−7 to 0.005. Thereafter we decay the learning rate proportional to the number of updates. Each training batch contains at most 4,096 source/target tokens. We set label smoothing weight to 0.1, dropout to 0.3. We follow (Zhang et al., 2020) to early-stop the training if validation loss does not improve for ten epochs. We average the last ten checkpoints for inference as in (Vaswani et al., 2017) and (Wu et al., 2018). We perform\n2https://github.com/multi30k/dataset\nbeam search with beam size set to 5. We report 4-gram BLEU and METEOR scores for all test sets. All models are trained and evaluated on one single machine with two Titan P100 GPUs."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "Our baselines can be categorized into three types:\n• The text-only Transformer; • The conventional MMT models: Doubly-ATT and Imagination; • The retrieval-based MMT models: UVR-NMT.\nDetails of these methods can be found in Section 2. For fairness, all the baselines are implemented by ourselves based on FairSeq (Ott et al., 2019). We use top-5 retrieved images for both UVR-NMT and our RMMT. We also consider two more recent state-of-the-art conventional methods for reference: GMNMT (Yin et al., 2020) and DCCN (Lin et al., 2020), whose results are reported as in their papers.\nNote that most MMT methods are difficult (or even impossible) to interpret. While there exist some interpretable methods (e.g., UVR-NMT) that contain gated fusion layers similar to ours, they perform sophisticated transformations on visual representation before fusion, which lowers the interpretability of the gating matrix. For example, in the gated fusion layer of UVR-NMT, we observe that the visual vector is order-of-magnitude smaller than the textual vector. As a result, interpreting gating weight is meaningless because visual vector has negligible influence on the fused vector."
    }, {
      "heading" : "4.4 Results",
      "text" : "Table 1 shows the BLEU scores of these methods on the Multi30k dataset. From the table, we see that although we can replicate similar BLEU scores of Transformer-Base as reported in (Grönroos et al., 2018b; Ive et al., 2019), these scores (Row 1) are significantly outperformed by Transformer-Small and Transformer-Tiny, which have fewer parameters. This shows that Transformer-Base could overfit the Multi30k dataset. Transformer-Tiny, whose number of parameters is about 20 times smaller than that of Transformer-Base, is more robust and efficient in our test cases. We therefore use it as the base model for all our MMT systems in the following discussion.\nBased on the Transformer-tiny model, both our proposed models (Gated Fusion and RMMT) and baseline MMT models (Doubly-ATT, Imagination and UVR-NMT) significantly outperform the\nstate-of-the-arts (GMNMT and DCCN) on En→De translation. However, the improvement of all these methods (Rows 4-10) over the base TransformerTiny model (Row 3) is very marginal. This shows that visual context might not be as important as we expected for translation, at least on datasets we explored.\nWe further evaluate all the methods on the METEOR scores (see Appendix C). We also run experiments on the VaTex dataset (see Appendix B). Similar results are observed as Table 1. Although various MMT systems have been proposed recently, a well-tuned model that uses text only remain competitive. This motivates us to revisit the importance of visual context for translation in MMT models."
    }, {
      "heading" : "5 Model Analysis",
      "text" : "Taking a closer look at the results given in the previous section, we are surprised by the observation that our models learn to ignore visual context when translating (Sec 5.1). This motivates us to revisit the contribution of visual context in MMT systems (Sec 5.2). Our adversarial evaluation shows that adding model regularization achieves comparable results as incorporating visual context. Finally, we discuss when visual context is needed (Sec 5.3) and how these findings could benefit future research."
    }, {
      "heading" : "5.1 Probe the need for visual context in MMT",
      "text" : "To explore the need for visual context in our models, we focus on the interpretable component: the gated fusion layer (see Equation 3 and 5). Intuitively, a larger gating weight Λij indicates the model learns to depend more on vi-\nsual context to perform better translation. We quantify the degree to which visual context is used by the micro-averaged gating weight Λ =∑M\nm=1 sum(Λ m)/(d× V ). Here M , V are the total number of sentences and words in the corpus, respectively. sum(·) add up all elements in a given matrix, and Λ is a scalar value ranges from 0 to 1. A larger Λ implies more usage of the visual context.\nWe first study models’ behavior after convergence. From Table 2, we observe that Λ is negligibly small, suggesting that both models learn to discard visual context. In other words, visual context may not be as important for translation as previously thought. Since Λ is insensitive to outliers (e.g., large gating weight at few dimensions), we further compute p(Λij > 1e-10): percentage of gating weight entries in Λ that are larger than 1e-10. With no surprise, we find that on all test splits p(Λij > 1e-10) are always zero, which again shows that visual input is not used by the model in inference.\nThe Gated Fusion’s training process also shed\nsome light on how the model accommodates the visual information during training. Figure 1 (a) and (b) shows how Λ changes during training, from the first epoch. We find that, Gated Fusion starts with a relatively high Λ (>0.5), but quickly decreases to ≈ 0.48 after the first epoch. As the training continues, Λ gradually decreases to roughly zero. In the early stages, the model relies heavily on images, possibly because they could provide meaningful features extracted from a pre-trained ResNet-50 CNN, while the textual encoder is randomly initialized. Compared with text-only NMT, utilizing visual features lowers MMT models’ trust in the hidden representations generated from the textual encoders. As the training continues, the textual encoder learns to represent source text better and the importance of visual context gradually decreases. In the end, the textual encoder carries sufficient context for translation and supersedes the contributions from the visual features. Nevertheless, this doesn’t explain the superior performance of the multimodal systems (Table 1). We speculate that visual context is acting as regularization that helps model training in the early stages. We further explore this hypothesis in the next section."
    }, {
      "heading" : "5.2 Revisit need for visual context in MMT",
      "text" : "In the previous section, we hypothesize that the gains of MMT systems come from some regularization effects. To verify our hypothesis, we conduct experiments based on two widely used regularization techniques: random noise injection (Bishop,\n1995) and weight decay (Hanson and Pratt, 1989). The former simulates the effects of assumably uninformative visual representations and the later is a more principled way of regularization that does not get enough attention in the current hyperparameter tuning stage. Inspecting the results, we find that applying these regularization techniques achieves similar gains over the text-only baseline as incorporating multimodal information does.\nFor random noise injection, we keep all hyperparameters unchanged but replace visual features extracted using ResNet with randomly initialized vectors, which are noise drawn from a standard Gaussian distribution. A MMT model equipped with ResNet features is denoted as a ResNet-based model, while the same model with random initialization is denoted as a noise-based model. We run each experiment three times and report the averaged results. Note that values in parentheses indicate the performance gap between the ResNetbased model and its noise-based adversary.\nTable 3 shows BLEU scores on the Multi30k dataset. Each column in the table corresponds to a test set “contest”. From the table, we observe that, among 18 (3 methods × 3 test sets × 2 tasks) contests with the Transformer model (row 1), noise-based models (rows 2-4) achieve better performance 13 times, while ResNet-based models win 14 cases. This shows that noise-based models perform comparably with ResNet-based models. A further comparison between noise-based models and ResNet-based models shows that they are compatible after 18 contests, in which the former wins 8 times and the latter wins 10 times.\nWe observe similar results when repeating above evaluation using METEOR (Tabel 9 ) and on VaTex (Table 7 ). These observations deduce that random noise could function as visual context. In MMT systems, adding random noise or visual context can help reduce overfitting (Bishop et al., 1995) when translating sentences in Multi30k, which are short and repetitive (Caglayan et al., 2019). Moreover, we find that the `2 norm of model weights in ResNet-based Gated Fusion and noise-based Gated Fusion are only 97.7% and 95.2% of that in Transformer on En→De, respectively. This further verifies our speculation that, as random noise injection (An, 1996), visual context can help weight smoothing and improve model generalization.\nFurther, we regularize the models with weight decay. We consider three models: the text-only Trans-\nformer, the representative existing MMT method Doubly-ATT, and our Gated Fusion method. Figure 2 and 3 (in Appendix C) show the BLEU and METEOR scores of these methods on En→De translation as weight decay rate changes, respectively. We see that the best results of the text-only Transformer model with fine-tuned weight decay are comparable or even better than that of the MMT models Doubly-ATT and Gated Fusion that utilize visual context. This again shows that visual context is not as useful as we expected and it essentially plays the role of regularization."
    }, {
      "heading" : "5.3 When is visual context needed in MMT",
      "text" : "Despite the less importance of visual information we showed in previous sections, there also exist works that support its usefulness. For example, Caglayan et al. (2019) experimentally show that, with limited textual context (e.g., masking some input tokens), MMT models will utilize the visual input for translation. This further motivates us to investigate when visual context is needed in MMT models. We conduct experiment with a new masking strategy that does not need any entity linking annotations as in Caglayan et al. (2019). Specifically, we follow Tan and Bansal (2020) to collect a list of visually grounded tokens. A visually grounded token is the one that has more than 30 occurrences in the Multi30k dataset with stop words removed.\nMasking all visually grounded tokens will affect around 45% of tokens in Multi30k.\nTable 4 shows the adversarial study with visually grounded tokens masked. In particular, we select Transformer, Gated Fusion and RMMT as representative methods. From the table, we see that random noise injection (row 5,6) and weight decay (row 2) can only bring marginal improvement over the text-only Transformer model. However, ResNet-based models that utilize visual context significantly improve the translation results. For example, RMMT achieves almost 50% gain over the Transformer on the BLEU score. Moreover, both Gated Fusion and RMMT using ResNet features lead to a larger Λ value than that when textual context is sufficient as shown in Table 2. Those results further suggest that visual context is needed when textual context is insufficient. In addition to token masking, sentences with incorrect, ambiguous and gender-neutral words (Frank et al., 2018) might also need visual context to help translation. Therefore, to fully exert the power of MMT systems, we emphasize the need for a new MMT benchmark, in which visual context is deemed necessary to generate correct translation.\nInterestingly, even with ResNet features, we observe a significant drop in both BLEU and METEOR scores compared with those in Table 1 and 8, similar to that reported in (Chowdhury and Elliott, 2019). The reason could be two-fold. On the one hand, there are many words that can not be visualized. For example, in Table 5 (a), although Gated Fusion can successfully identify the main objects in the image (“little boys pose with a puppy”), it fails to generate the more abstract concept “family picture”. On the other hand, when translating different words, it is difficult to capture correct regions in images. For example, in Table 5 (b), we see that Gated Fusion incorrectly generates the word frauen (women) because it captures the woman at the top-right corner of the image."
    }, {
      "heading" : "5.4 Discussion",
      "text" : "Finally, we discuss how our findings might benefit future MMT research. First, a benchmark that requires more visual information than Multi30k to solve is desired. As shown in Section 5.2, sentences in Multi30k are rather simple and easy-tounderstand. Thus textual context could provide sufficient information for correct translation, making visual modules relatively redundant in these systems. While the MSCOCO test set in Multi30k contains ambiguous verbs and encourages models to use image sources for disambiguation, we still lack a corresponding training set.\nSecond, our methods can serve as a verification tool to investigate whether visual grounding is needed in translation for a new benchmark.\nThird, we find that visual feature selection is also critical for MMT’s performance. While most methods employ the attention mechanism to learn to attend relevant regions in an image, the shortage of annotated data could impair the attention module (see Table 5 (b)). Some recent efforts (Yin\net al., 2020; Lin et al., 2020; Caglayan et al., 2020) address the issue by feeding models with preextracted visual objects instead of the whole image. However, these methods are easily affected by the quality of the extracted objects. Therefore, a more effective end-to-end visual feature selection technique is needed, which can be further integrated into MMT systems to improve performance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we devise two interpretable models that exhibit state-of-the-art performance on the widely adopted MMT datasets — Multi30k and the new video-based dataset — VaTex. Our analysis on the proposed models, as well as on other existing MMT systems, suggests that visual context helps MMT in the similar vein as regularization methods (e.g., weight decay), under sufficient textual context. Those empirical findings, however, should not be understood as us downplaying the importance existing datasets and models; we believe that sophisticated MMT models are necessary\nfor effective grounding of visual context into translation. Our goal, rather, is to (1) provide additional clarity on the remaining shortcomings of current dataset and stress the need for new datasets to move the field forward; (2) emphasise the importance of interpretability in MMT research."
    }, {
      "heading" : "7 Acknowledgement",
      "text" : "Zhiyong Wu is partially supported by a research grant from the HKU-TCL Joint Research Centre for Artificial Intelligence."
    }, {
      "heading" : "A Training Settings",
      "text" : "Table 6 shows the configuration of different model sizes."
    }, {
      "heading" : "B Results on VaTex",
      "text" : "VaTex is a video-based MMT corpus that contains 129,955 English-Chinese sentence pairs for training, 15,000 sentence pairs for validation, and 30,000 sentence pairs for testing. Each pair of sentences is associated with a video clip. Since the testing set is not publicly available, we use half of the validation set for validating and the other half for testing. We apply the byte pair encoding algorithm on the lower-cased English sentences and split Chinese sentences into sequences of characters, resulting in a vocabulary of 17,216 English tokens and 3,384 Chinese tokens. We use the video features provided along with the VaTex dataset, in which each video is represented as Rk∗1024, where k is the number of segments. Since some MMT systems take a “global” visual feature as input, we use 3D-Max-Pooling to extract the pooled representation R1024 for each video.\nThe results are shown in Table 7. We observe that although most MMT systems show improvement over the Transformer baseline, the gains are quite marginal. Indicating that although imagebased MMT models can be directly applied to\nvideo-based MMT, there is still room for improvement due to the challenge of video understanding. We also note that (a) regularize the text-only Transformer with weight decay demonstrates similar gains as injecting video information into the models; (b) replacing video features with random noise replicate comparable performance, which further supports our findings in Section 5.2."
    }, {
      "heading" : "C Results on METEOR",
      "text" : "We also report our results based on METEOR (Banerjee and Lavie, 2005), which consistently demonstrates higher correlation with human judgments than BLEU does in independent evaluations such as in EMNLP WMT 2011 3. From Table 8, we can see that on En-Fr translation, MMT systems demonstrate similar improvements over text-only baselines in both METEOR and BLEU(see Table 1). On En-De translation, however, MMT systems are mostly on-par with Transformer-tiny on METEOR and do not show consistent gains as BLEU. We hypothesis the reason being that En-De sets are created in a imageblind fashion, in which the crowd-sourcing workers produce translations without seeing the images (Frank et al., 2018). Such that source sentence can already provide sufficient context for translation. When creating the En-Fr corpus, the image-blind issue is fixed (Elliott et al., 2017), thus images are perceived as “needed” in the translation for whatever reason. Although BLEU is unable to elicit this difference, evaluation based on METEOR captured it and confirmed previous research. We also compute METEOR scores for our experiments that regularize models with random noise (see Table 9) and weight decay (see Figure 3). The results are consistent with those evaluated using BLEU and further complement our early findings."
    }, {
      "heading" : "D Results on IWSLT’14",
      "text" : "We also evaluate the retrieval-based model RMMT on text-only corpus — IWSLT’14. The IWSLT’14 dataset contains 160k bilingual sentence pairs for En-De translation task. Following the common practice, we lowercase all words, split 7k sentence pairs from the training dataset for validation and concatenate dev2010, dev2012, tst2010, tst2011, tst2012 as the test set. The number of BPE operations is set to 20,000. We use the Small configuration in all our experiments. The dropout and label\n3http://statmt.org/wmt11/papers.html\nsmoothing rate are set to 0.3 and 0.1, respectively. Since there is no images associated with IWSLT, we follow (Zhang et al., 2020) and retrieve top-5 images from Multi30K corpus.\nFrom Table 10, we see that Transformer without weight decay is marginally outperformed by RMMT, but achieves slightly higher BLEU scores when trained with a 0.0001 weight decay. Our discussion in Section 5.2 sheds light on why visual context is helpful on non-grounded lowresourced datasets like IWSLT’14 — for lowresourced dataset like IWSLT’14, injecting visual context help regularize model training and avoid overfitting."
    } ],
    "references" : [ {
      "title" : "The effects of adding noise during backpropagation training on a generalization performance",
      "author" : [ "Guozhong An." ],
      "venue" : "Neural computation, 8(3):643–674.",
      "citeRegEx" : "An.,? 1996",
      "shortCiteRegEx" : "An.",
      "year" : 1996
    }, {
      "title" : "Doubly attentive transformer machine translation",
      "author" : [ "Hasan Sait Arslan", "Mark Fishel", "Gholamreza Anbarjafari." ],
      "venue" : "arXiv preprint arXiv:1807.11605.",
      "citeRegEx" : "Arslan et al\\.,? 2018",
      "shortCiteRegEx" : "Arslan et al\\.",
      "year" : 2018
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Findings of the third shared task on multimodal machine translation",
      "author" : [ "Loı̈c Barrault", "Fethi Bougares", "Lucia Specia", "Chiraag Lala", "Desmond Elliott", "Stella Frank" ],
      "venue" : null,
      "citeRegEx" : "Barrault et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Barrault et al\\.",
      "year" : 2018
    }, {
      "title" : "Training with noise is equivalent to tikhonov regularization",
      "author" : [ "Chris M Bishop." ],
      "venue" : "Neural computation, 7(1):108–116.",
      "citeRegEx" : "Bishop.,? 1995",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1995
    }, {
      "title" : "Neural networks for pattern recognition",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "Oxford university press.",
      "citeRegEx" : "Bishop,? 1995",
      "shortCiteRegEx" : "Bishop",
      "year" : 1995
    }, {
      "title" : "Simultaneous machine translation with visual context",
      "author" : [ "Ozan Caglayan", "Julia Ive", "Veneta Haralampieva", "Pranava Madhyastha", "Loı̈c Barrault", "Lucia Specia" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Caglayan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing the need for visual context in multimodal machine translation",
      "author" : [ "Ozan Caglayan", "Pranava Madhyastha", "Lucia Specia", "Loı̈c Barrault" ],
      "venue" : null,
      "citeRegEx" : "Caglayan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2019
    }, {
      "title" : "Dcu-uva multimodal mt system report",
      "author" : [ "Iacer Calixto", "Desmond Elliott", "Stella Frank." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 634–638.",
      "citeRegEx" : "Calixto et al\\.,? 2016",
      "shortCiteRegEx" : "Calixto et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence-level multilingual multi-modal embedding for natural language",
      "author" : [ "Iacer Calixto", "Qun Liu" ],
      "venue" : null,
      "citeRegEx" : "Calixto and Liu.,? \\Q2017\\E",
      "shortCiteRegEx" : "Calixto and Liu.",
      "year" : 2017
    }, {
      "title" : "2019. Bert: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices",
      "author" : [ "Stella Frank", "Desmond Elliott", "Lucia Specia." ],
      "venue" : "Natural Language Engineering, 24(3):393–413.",
      "citeRegEx" : "Frank et al\\.,? 2018",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2018
    }, {
      "title" : "The MeMAD submission to the WMT18 multimodal",
      "author" : [ "Stig-Arne Grönroos", "Benoit Huet", "Mikko Kurimo", "Jorma Laaksonen", "Bernard Merialdo", "Phu Pham", "Mats Sjöberg", "Umut Sulubacak", "Jörg Tiedemann", "Raphael Troncy", "Raúl Vázquez" ],
      "venue" : null,
      "citeRegEx" : "Grönroos et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grönroos et al\\.",
      "year" : 2018
    }, {
      "title" : "The memad submission to the wmt18 multimodal translation",
      "author" : [ "Stig-Arne Grönroos", "Benoit Huet", "Mikko Kurimo", "Jorma Laaksonen", "Bernard Merialdo", "Phu Pham", "Mats Sjöberg", "Umut Sulubacak", "Jörg Tiedemann", "Raphael Troncy" ],
      "venue" : null,
      "citeRegEx" : "Grönroos et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grönroos et al\\.",
      "year" : 2018
    }, {
      "title" : "Search engine guided neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor OK Li." ],
      "venue" : "AAAI, pages 5133–5140.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "ICML.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Comparing biases for minimal network construction with back-propagation",
      "author" : [ "Stephen José Hanson", "Lorien Y Pratt." ],
      "venue" : "Advances in neural information processing systems, pages 177–185.",
      "citeRegEx" : "Hanson and Pratt.,? 1989",
      "shortCiteRegEx" : "Hanson and Pratt.",
      "year" : 1989
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Cuni system for the wmt18 multimodal translation task",
      "author" : [ "Jindřich Helcl", "Jindřich Libovickỳ", "Dušan Variš." ],
      "venue" : "arXiv preprint arXiv:1811.04697.",
      "citeRegEx" : "Helcl et al\\.,? 2018",
      "shortCiteRegEx" : "Helcl et al\\.",
      "year" : 2018
    }, {
      "title" : "Does my multimodal model learn cross-modal interactions? it’s harder to tell than you might think",
      "author" : [ "Jack Hessel", "Lillian Lee" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Hessel and Lee.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hessel and Lee.",
      "year" : 2020
    }, {
      "title" : "Attention-based multimodal neural machine translation",
      "author" : [ "Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 639–645, Berlin,",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling translations with visual awareness",
      "author" : [ "Julia Ive", "Pranava Madhyastha", "Lucia Specia." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6525– 6538, Florence, Italy. Association for Computa-",
      "citeRegEx" : "Ive et al\\.,? 2019",
      "shortCiteRegEx" : "Ive et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting visual question answering baselines",
      "author" : [ "Allan Jabri", "Armand Joulin", "Laurens Van Der Maaten." ],
      "venue" : "European conference on computer vision, pages 727–739. Springer.",
      "citeRegEx" : "Jabri et al\\.,? 2016",
      "shortCiteRegEx" : "Jabri et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalization through memorization: Nearest neighbor language models",
      "author" : [ "Urvashi Khandelwal", "Omer Levy", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Khandelwal et al\\.,? 2019",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2019
    }, {
      "title" : "Regularization for deep learning: A taxonomy",
      "author" : [ "Jan Kukačka", "Vladimir Golkov", "Daniel Cremers." ],
      "venue" : "arXiv preprint arXiv:1710.10686.",
      "citeRegEx" : "Kukačka et al\\.,? 2017",
      "shortCiteRegEx" : "Kukačka et al\\.",
      "year" : 2017
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention strategies for multi-source sequence-to-sequence learning",
      "author" : [ "Jindřich Libovický", "Jindřich Helcl." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 196–202, Vancou-",
      "citeRegEx" : "Libovický and Helcl.,? 2017",
      "shortCiteRegEx" : "Libovický and Helcl.",
      "year" : 2017
    }, {
      "title" : "Input combination strategies for multi-source transformer decoder",
      "author" : [ "Jindřich Libovický", "Jindřich Helcl", "David Mareček." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 253–260, Belgium, Brussels. Associa-",
      "citeRegEx" : "Libovický et al\\.,? 2018",
      "shortCiteRegEx" : "Libovický et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic context-guided capsule network for multimodal machine translation",
      "author" : [ "Huan Lin", "Fandong Meng", "Jinsong Su", "Yongjing Yin", "Zhengyuan Yang", "Yubin Ge", "Jie Zhou", "Jiebo Luo." ],
      "venue" : "Proceedings of the 28th ACM International Conference",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
      "author" : [ "Bryan A Plummer", "Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Plummer et al\\.,? 2015",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep convolutional neural networks and data augmentation for environmental sound classification",
      "author" : [ "Justin Salamon", "Juan Pablo Bello." ],
      "venue" : "IEEE Signal Processing Letters, 24(3):279–283.",
      "citeRegEx" : "Salamon and Bello.,? 2017",
      "shortCiteRegEx" : "Salamon and Bello.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "A shared task on multimodal machine translation and crosslingual image description",
      "author" : [ "Lucia Specia", "Stella Frank", "Khalil Sima’an", "Desmond Elliott" ],
      "venue" : "In Proceedings of the First Conference on Machine Translation: Volume",
      "citeRegEx" : "Specia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2016
    }, {
      "title" : "Vokenization: Improving language understanding via contextualized, visually-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
      "author" : [ "Xin Wang", "Jiawei Wu", "Junkun Chen", "Lei Li", "YuanFang Wang", "William Yang Wang." ],
      "venue" : "The IEEE International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieve and refine: Improved sequence generation models for dialogue",
      "author" : [ "Jason Weston", "Emily Dinan", "Alexander Miller." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational",
      "citeRegEx" : "Weston et al\\.,? 2018",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2018
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal transformer for multimodal machine translation",
      "author" : [ "Shaowei Yao", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4346– 4350.",
      "citeRegEx" : "Yao and Wan.,? 2020",
      "shortCiteRegEx" : "Yao and Wan.",
      "year" : 2020
    }, {
      "title" : "A novel graph-based multi-modal fusion encoder for neural machine translation",
      "author" : [ "Yongjing Yin", "Fandong Meng", "Jinsong Su", "Chulun Zhou", "Zhengyuan Yang", "Jie Zhou", "Jiebo Luo." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with universal visual representation",
      "author" : [ "Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "A visual attention grounding neural model for multimodal machine translation",
      "author" : [ "Mingyang Zhou", "Runxiang Cheng", "Yong Jae Lee", "Zhou Yu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3643–",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Incorporating bert into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tieyan Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality (Specia et al., 2016; Wang et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 255
    }, {
      "referenceID" : 36,
      "context" : "Multimodal Machine Translation (MMT) aims at designing better translation systems by extending conventional text-only translation systems to take into account multimodal information, especially from visual modality (Specia et al., 2016; Wang et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 255
    }, {
      "referenceID" : 18,
      "context" : "Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT.",
      "startOffset" : 111,
      "endOffset" : 207
    }, {
      "referenceID" : 21,
      "context" : "Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT.",
      "startOffset" : 111,
      "endOffset" : 207
    }, {
      "referenceID" : 28,
      "context" : "Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT.",
      "startOffset" : 111,
      "endOffset" : 207
    }, {
      "referenceID" : 40,
      "context" : "Despite many previous success in MMT that report improvements when models are equipped with visual information (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020), there have been continuing debates on the need for visual context in MMT.",
      "startOffset" : 111,
      "endOffset" : 207
    }, {
      "referenceID" : 7,
      "context" : "A more recent study (Caglayan et al., 2019), however, shows that under limited textual context (e.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "Although such an opaque tool is an acceptable beginning to investigate the need for visual context in MMT, they provide rather indirect evidence (Hessel and Lee, 2020).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 24,
      "context" : "This is because performance differences can often be attributed to factors unrelated to visual input, such as regularization (Kukačka et al., 2017), data bias (Jabri et al.",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : ", 2017), data bias (Jabri et al., 2016), and some others (Dodge et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 36,
      "context" : ", 2016) and VaTex (Wang et al., 2019) datasets, they learn to ignore the multimodal information.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Our further analysis suggests that under sufficient textual context, the improvements come from a regularization effect that is similar to random noise injection (Bishop, 1995) and weight decay (Hanson and Pratt, 1989).",
      "startOffset" : 162,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "Our further analysis suggests that under sufficient textual context, the improvements come from a regularization effect that is similar to random noise injection (Bishop, 1995) and weight decay (Hanson and Pratt, 1989).",
      "startOffset" : 194,
      "endOffset" : 218
    }, {
      "referenceID" : 31,
      "context" : "6154 tional visual information is treated as noise signals that can be used to enhance model training and lead to a more robust network with lower generalization error (Salamon and Bello, 2017).",
      "startOffset" : 168,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "Repeating the evaluation under limited textual context further substantiates our findings and complements previous analysis (Caglayan et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : ", ResNet (He et al., 2016)) to encode images into feature vectors.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : "This visual representation can be used to initialize the encoder/decoder’s hidden vectors (Elliott et al., 2015; Libovický and Helcl, 2017; Calixto et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : "This visual representation can be used to initialize the encoder/decoder’s hidden vectors (Elliott et al., 2015; Libovický and Helcl, 2017; Calixto et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 161
    }, {
      "referenceID" : 20,
      "context" : "It can also be appended/prepended to word embeddings as additional input tokens (Huang et al., 2016; Calixto and Liu, 2017).",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "It can also be appended/prepended to word embeddings as additional input tokens (Huang et al., 2016; Calixto and Liu, 2017).",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "Recent works (Libovický et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder.",
      "startOffset" : 13,
      "endOffset" : 92
    }, {
      "referenceID" : 42,
      "context" : "Recent works (Libovický et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder.",
      "startOffset" : 13,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "Recent works (Libovický et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder.",
      "startOffset" : 13,
      "endOffset" : 92
    }, {
      "referenceID" : 28,
      "context" : "Recent works (Libovický et al., 2018; Zhou et al., 2018; Ive et al., 2019; Lin et al., 2020) employ attention mechanism to generate a visual-aware representation for the decoder.",
      "startOffset" : 13,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "For instance, Doubly-ATT (Calixto et al., 2017; Helcl et al., 2018; Arslan et al., 2018) insert an extra visual attention sub-layer between the decoder’s source-target attention sub-layer and feed-forward sub-layer.",
      "startOffset" : 25,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "For instance, Doubly-ATT (Calixto et al., 2017; Helcl et al., 2018; Arslan et al., 2018) insert an extra visual attention sub-layer between the decoder’s source-target attention sub-layer and feed-forward sub-layer.",
      "startOffset" : 25,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "The Imagination architecture (Elliott and Kádár, 2017; Helcl et al., 2018) decomposes multimodal translation into two subtasks: translation task and an auxiliary visual reconstruction task, which encourages the model to learn a visually grounded source sentence representation.",
      "startOffset" : 29,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "Retrieval-based models have been shown to improve performance across a variety of NLP tasks besides MMT, such as question answering (Guu et al., 2020), dialogue (Weston et al.",
      "startOffset" : 132,
      "endOffset" : 150
    }, {
      "referenceID" : 37,
      "context" : ", 2020), dialogue (Weston et al., 2018), language modeling (Khandelwal et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : ", 2018), language modeling (Khandelwal et al., 2019), question generation (Lewis et al.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : ", 2019), question generation (Lewis et al., 2020), and translation (Gu et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 40,
      "context" : "In particular, we use a gating matrix Λ (Yin et al., 2020; Zhang et al., 2020) to control the amount of visual information to be blended into the textual representation.",
      "startOffset" : 40,
      "endOffset" : 78
    }, {
      "referenceID" : 41,
      "context" : "In particular, we use a gating matrix Λ (Yin et al., 2020; Zhang et al., 2020) to control the amount of visual information to be blended into the textual representation.",
      "startOffset" : 40,
      "endOffset" : 78
    }, {
      "referenceID" : 35,
      "context" : "where pθ (yi | x, z, y<i) is implemented with a Transformer-based (Vaswani et al., 2017) network.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "For each image z, we use a pre-trained ResNet-50 CNN (He et al., 2016) to extract a 2048-dimensional average-pooled visual representation, which is then projected to the same dimension as Htext:",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 41,
      "context" : "Note that this gating mechanism has been a building block for many recent MMT systems (Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 28,
      "context" : "Note that this gating mechanism has been a building block for many recent MMT systems (Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 40,
      "context" : "Note that this gating mechanism has been a building block for many recent MMT systems (Zhang et al., 2020; Lin et al., 2020; Yin et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 142
    }, {
      "referenceID" : 41,
      "context" : "Image Retriever Based on the TF-IDF model, searching in existing retrieval-based MMT (Zhang et al., 2020) ignores the context information of a given query, which could lead to poor performance.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "We then apply the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 10,000 merging operations to segment words into subwords, which generates a vocabulary of 9,712 (9,544) tokens for En-De (En-Fr).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "We pre-train the retriever on a subset of the Flickr30k dataset (Plummer et al., 2015) that has overlapping instances with Multi30k removed.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "Base is a widely-used model configuration for Transformer in both text-only translation (Vaswani et al., 2017) and MMT (Grönroos et al.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 43,
      "context" : "In our preliminary study, we found that even a Small configuration, which is commonly used for low-resourced translation (Zhu et al., 2019), can still overfit on Multi30k.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "We follow (Zhang et al., 2020) to early-stop the training if validation loss does not improve for ten epochs.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "We average the last ten checkpoints for inference as in (Vaswani et al., 2017) and (Wu et al.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "For fairness, all the baselines are implemented by ourselves based on FairSeq (Ott et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 40,
      "context" : "We also consider two more recent state-of-the-art conventional methods for reference: GMNMT (Yin et al., 2020) and DCCN (Lin et al.",
      "startOffset" : 92,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "From the table, we see that although we can replicate similar BLEU scores of Transformer-Base as reported in (Grönroos et al., 2018b; Ive et al., 2019), these scores (Row 1) are significantly outperformed by Transformer-Small and Transformer-Tiny, which have fewer parameters.",
      "startOffset" : 109,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "To verify our hypothesis, we conduct experiments based on two widely used regularization techniques: random noise injection (Bishop, 1995) and weight decay (Hanson and Pratt, 1989).",
      "startOffset" : 124,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : "To verify our hypothesis, we conduct experiments based on two widely used regularization techniques: random noise injection (Bishop, 1995) and weight decay (Hanson and Pratt, 1989).",
      "startOffset" : 156,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : ", 1995) when translating sentences in Multi30k, which are short and repetitive (Caglayan et al., 2019).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "This further verifies our speculation that, as random noise injection (An, 1996), visual context can help weight smoothing and improve model generalization.",
      "startOffset" : 70,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "In addition to token masking, sentences with incorrect, ambiguous and gender-neutral words (Frank et al., 2018) might also need visual context to help translation.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 40,
      "context" : "Some recent efforts (Yin et al., 2020; Lin et al., 2020; Caglayan et al., 2020) address the issue by feeding models with preextracted visual objects instead of the whole image.",
      "startOffset" : 20,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Some recent efforts (Yin et al., 2020; Lin et al., 2020; Caglayan et al., 2020) address the issue by feeding models with preextracted visual objects instead of the whole image.",
      "startOffset" : 20,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Some recent efforts (Yin et al., 2020; Lin et al., 2020; Caglayan et al., 2020) address the issue by feeding models with preextracted visual objects instead of the whole image.",
      "startOffset" : 20,
      "endOffset" : 79
    } ],
    "year" : 2021,
    "abstractText" : "A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional textonly translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodalintegrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models’ interpretability, and discuss how our findings will benefit future research.",
    "creator" : "LaTeX with hyperref"
  }
}