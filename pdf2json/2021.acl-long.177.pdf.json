{
  "name" : "2021.acl-long.177.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus",
    "authors" : [ "Hamdy Mubarak", "Amir Hussein", "Shammur Absar Chowdhury", "Ahmed Ali" ],
    "emails" : [ "info@arabicspeech.org," ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2274–2285\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2274"
    }, {
      "heading" : "1 Introduction",
      "text" : "Research on Automatic Speech Recognition (ASR) has attracted a lot of attention in recent years (Chiu et al., 2018; Watanabe et al., 2018). Such success has brought remarkable improvements in reaching human-level performance (Xiong et al., 2016; Saon et al., 2017; Hussein et al., 2021). This has been achieved by the development of large spoken corpora: supervised (Panayotov et al., 2015; Ardila et al., 2019); semi-supervised (Bell et al., 2015; Ali\n1QASR Qå ¯ in Arabic means “Palace”. The acronym stands for: QCRI Aljazeera Speech Resource.\net al., 2016); and more recently unsupervised (Valk and Alumäe, 2020; Wang et al., 2021) transcription. This work enables to either reduce Word Error Rate (WER) considerably or extract metadata from speech: dialect-identification (Shon et al., 2020); speaker-identification (Shon et al., 2019); and codeswitching (Chowdhury et al., 2020b, 2021).\nNatural Language Processing (NLP), on the other hand values large amount of textual information for designing experiments. NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019). The NLP stack for Modern Standard Arabic (MSA) has reached very high performance in many tasks. With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017).\nOur objective is to release the first Arabic speech and NLP corpus to study spoken MSA and DA. This is to enable empirical evaluation of learning more than the word sequence from the speech. In our view, existing speech and NLP corpora are missing the link between the two different modalities. Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al., 2021; Chowdhury et al., 2021). These challenges are often overlooked when it comes to NLP tasks, since they are not present in typical text data.\nIn this paper, we create and release2 the largest corpus for transcribed Arabic speech. It comprises of 2, 000 hours of speech data with lightly supervised transcriptions. Our contributions are: (i)\n2Data can be obtained from: https://arabicspeech.org/qasr\naligning the transcription with the corresponding audio segments including punctuation for building ASR systems; (ii) providing semi-supervised speaker identification and speaker linking per audio segments; (iii) releasing baseline results for acoustic and linguistic Arabic dialect identification and punctuation restoration; (iv) adding a new layer of annotation in the publicly available MGB-2 testset, for evaluating NER for speech transcription; (v) sharing code-switching data between Arabic and foreign languages for speech and text; and finally, (vi) releasing more than 130M words for Language Model (LM).\nWe believe that providing the research community with access to multi-dialectal speech data along with the corresponding NLP features will foster open research in several areas, such as the analysis of speech and NLP processing jointly. Here, we build models and share the baseline results for all of the aforementioned tasks."
    }, {
      "heading" : "1.1 Related work",
      "text" : "The CallHome task within the NIST benchmark evaluations framework (Pallett, 2003), released one of the first transcribed Arabic dialect dataset. Over years, NIST evaluations provided with more dialectal - mainly in Egyptian and Levantine dialects, as part of language recognition evaluation campaign. Projects such as GALE and TRANSTAC (Olive et al., 2011) program, released more than 251 hours of Arabic data, including the first spoken Iraqi dialect among others. These datasets exposed the research community to the challenges of spoken dialectal Arabic and motivated to design competition to handle dialect identification, dialectal ASR among others (see Ali et al. (2021) for details).\nThe following datasets are released from the Multi-Genre Broadcast MGB challenge: (i) MGB2 (Ali et al., 2016) – this dataset is the first milestone towards designing the first large scale continuous speech recognition for Arabic language. The corpus contains a total of 1, 200 hours of speech with lightly supervised transcriptions and is collected from Aljazeera Arabic news channel span over many years. (ii) MGB-3 (Ali et al., 2017) – focused on only Egyptian Arabic broadcast data comprises of 16 hours. (iii) MGB-5 (Ali et al., 2019) – consists of 13 hours of Moroccan Arabic speech data. In addition, the CommonVoice3 Ara-\n3https://commonvoice.mozilla.org/en/ datasets\nbic dataset, from the CommonVoice project, provides 49 hours of modern standard Arabic (MSA) speech data.4\nUnlike MGB-2, QASR dataset is the largest multi-dialectal corpus with linguistically motivated segmentation. The dataset includes multi-layer information that aids both speech and NLP research community. QASR is the first speech corpora to provide resources for benchmarking NER, punctuation restoration systems. For close comparison between MGB-2 vs QASR, see Table 1."
    }, {
      "heading" : "2 Corpus Creation",
      "text" : ""
    }, {
      "heading" : "2.1 Data Collection",
      "text" : "We obtained Aljazeera Arabic news channel’s archive (henceforth AJ), spanning over 11 years from 2004 until 2015. It contains more than 4, 000 episodes from 19 different programs. These programs cover different domains like politics, society, economy, sports, science, etc. For each episode, we have the following: (i) audio sampled at 16KHz; (ii) manual transcription, the textual transcriptions contained no timing information. The quality of the transcription varied significantly; the most challenging were conversational programs in which overlapping speech and dialectal usage was more frequent; and finally (iii) some metadata.\nFor better evaluation of the QASR corpus, we reused the publicly available MGB-2 (Ali et al., 2016) testset as it has been manually revised, coming from the same channel, thus making this testset ideal to evaluate the QASR corpus. It is worth noting that we ensure that the MGB-2 dev/test sets\n4Reported on June 2021.\nare not included in QASR corpus, so they can be used to report progress on the Arabic ASR challenge. We have also enriched the MGB-2 testset with manually annotated speaker information like country5, gender of the speakers, along with NER information and used it to evaluate our baselines.\nMoreover, we apply topic classification and dialect identification. Our models achieved an overall accuracy of 96% and 88% respectively, which have been measured on internal testsets also created from Aljazeera news articles. More details can be found in ASAD demo paper (Hassan et al., 2021). Table 2 gives a rough estimate about distributions in the updated MGB-2 testset."
    }, {
      "heading" : "2.2 Metadata Information",
      "text" : "Most of the recorded programs have the following metadata: program name, episode title and date, speaker names and topics of the episode. Majority of metadata information appear in the beginning of the file. However, some of them are embedded inside the episode transcription. Figure 1 shows a sample input file from Aljazeera. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President,\n5We use ISO 3166 for country codes. https: //en.wikipedia.org/wiki/List_of_ISO_ 3166_country_codes\nBarack Obama/President of USA, Barck Obama (typo), etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of metadata field names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction followed by manual verification and standardization.\nSample output file from QASR is shown in Figure 2. It contains speaker names as they appear in the current episode and their corresponding standardized forms across all files, which can be useful for tasks such as speaker identification and speaker linking across the entire corpus. For each speaker, we provide gender information and whether the speaker’s name refers to a unique person (e.g. Barack Obama) or not (e.g. One of the protesters, or an audio reporter). Figure 2 has information on the anchor speaker and two guests as they appear in the metadata file, in addition to other speakers that were missed in the original transcription. It is worth noting that we provide gender and country for common Arabic speakers (who have at least 20 segments in the entire corpus). On the other hand, we ignore metadata for foreign speakers because dubbing their speeches can be done by any voice-over. We provide gender information for 2, 000 speakers and this covers 82% of all segments in the whole corpus.\nSpeech and text are aligned (see details in Section 2.3) and split into short segments (see Section\n2.5). For each segment, we provide: words (element), timing information (starttime and endtime) in addition to speaker ID (who), Average Word Duration (AWD) in seconds, Grapheme Match Error Rate (GWER), and Word Match Error Rate (WMER). For details about word and grapheme match, refer to (Bell et al., 2015; Ali et al., 2016). Figure 2 shows information for Segment1 that appears in Figure 1."
    }, {
      "heading" : "2.3 Speech to Text Alignment",
      "text" : "The main concept of this method is to run an Arabic speech recognition system over the entire episode (Khurana and Ali) and use the recognized word sequences and their locations in time for automatic alignment (Braunschweiler et al., 2010).\nFor alignment, Aljazeera and ASR transcriptions are then converted into two long sequences of words. Aligning the sequences was challenging for many reasons; code-switching between MSA and dialects; human transcription was not verbatim, e.g. some spoken words were dropped due to repetition or correction; spelling and grammar mistakes; usage of foreign languages mainly English and French; and many overlapped speeches.\nWe used Smith–Waterman algorithm (Smith et al., 1981), which performs local sequence alignment to determine similar regions between two strings. We modified the algorithm to accept an approximate match between the given transcription and the recognized word sequence. If the Levenshtein distance between two words ≤ half the length (number of characters) in the given transcription, this is considered as an approximate match.\nFigure 3 shows a sample alignment, where each word is assigned to a speaker after parsing Aljazeera text and aligned, if possible, to a word from ASR transcription along with its timing informa-\ntion. Relaxation is applied in case of approximate match. Time information of the missing words (highlighted in red in AJ column) is estimated by interpolation from the matched word before and after. In this example, we consider words éJ. . . , I. . . (because-of, because-of-it) as approximate match."
    }, {
      "heading" : "2.4 Matching ASR Accuracy",
      "text" : "Figure 4 shows the matching accuracy between the ASR and the given transcription at the segment level. We applied two levels of matching to deal with these challenges: exact match (where both transcription and ASR output are identical), and approximate match (where there is a forgiving edit distance between words in the transcription and ASR output). Exact match (100% in the x-axis) would have led to less than 27% of the segments, while approximate match allows to consider more segments."
    }, {
      "heading" : "2.5 Segmentation",
      "text" : "After aligning the given transcription with the ASR words for the whole episode, we want to segment the text into shorter segments. Unlike MGB-2, we considered many factors that we believe lead to better and logical segmentation, namely:\n• Surface: We tried to make segments in the range of [3-10] words. We consider punctuation6 as end of segments if they appear in this range, and we increase the window to 5 words to capture any of them in the neighbouring words. Typically, transcribers insert punctuation marks to indicate end of logical segments (sentences or phrases).\n• Dialog: When a speaker changes in the transcribed text, we consider this as a valid end of segment. By doing this, we assign only one speaker to each segment.\n• Acoustics: If there is a silence duration of at least 150msec between words, we consider this as a signal to potentially end the current segment. We consider the proceeding linguistic rules to confirm the validity of this end.\n• Linguistics: For linguistically motivated segmentation, we want to avoid ending segments in wrong places (e.g. in the middle of Named Entities (NE), Noun Phrases (NP) or Adjective Phrases (AP)). To do so, from the 130M words in the LM data, we extracted the most frequent 10K words that were not followed by any punctuation in 90% of the cases, then we revised them manually7. We call this list ”NOSTOP-LIST”. Examples are: èAm. 'AK. , ø X ñK , ú ¯\n(in, leads-to, towards). Additionally, we used the publicly available Arabic NLP tools (Farasa)8 for NER and Part Of Speech (POS) tagging to label each word in the transcriptions. We put marks to avoid ending segments in the middle of NEs, NPs or APs. These are some examples from MGB-2 that have segmentation errors and words appearing erroneously in different segments: A JË @/ ÈAÓ\n@ (People’s (segi) /hopes\n(segi+1)), éJ k. PA g/ ú «A Ó (external /endeavors)\nand éJ ºK QÓ B@/ èYj JÖÏ @ HAK BñË@ (United States\n/of America). If the surface or acoustics modules suggest end of segment, while contradicting these linguistics rules, this suggestion is ignored.\nDetails of QASR corpus after alignment and segmentation are presented in Table 3."
    }, {
      "heading" : "2.6 Intrasentential Code-Switching",
      "text" : "We discuss here the presence of intrasentential code-switching in QASR. We noticed in addition\n6Common punctuation marks are: Period, Comma, Question mark, Exclamation mark, Semicolon, Colon and Ellipsis.\n7The final list has 2,200 words. 8farasa.qcri.org\nto the intrasentential dialectal code switching (discussed in Section 3.4), the dataset also includes ≈ 6K segments, where alternation between Arabic and English/French languages are seen.\nTo quantify the amount of code-switching present in this data, we calculate both the utterance and corpus level Code-Mixing Index (CMI), motivated by Chowdhury et al. (2020b); Gambäck and Das (2016). Based on the range of utterance-level CMI values, we group our dataset, as shown in Table 4. As for the corpus-level CMI, we observe an average of 30.5 CMI-value, calculated based on the average of utterance-level9 CMI considering the code-switching segments in QASR dataset.\nFurthermore, from utterance-level analysis, we notice that the majority of the code-switched segments falls under 15 < CMI ≤ 30% with an average of 2 alteration points per segment (e.g. Ar → En→ Ar). Even though the code-switching occurs in only 0.4% of the full dataset, we notice that we have very short ≈ 968 segments (ranging CMI value > 30%) with frequent alternating language code, such as: ”ø Y\nJ« duplex @ñk. é J Jm. ' . Building”. In the future, these segments could be used to further explore the effect of such code-switching in the performance of speech and NLP models jointly."
    }, {
      "heading" : "3 Downstream Tasks",
      "text" : ""
    }, {
      "heading" : "3.1 Automatic Speech Recognition",
      "text" : "In this section, we study QASR dataset for the ASR task. We adopt the End-to-End Transformer (E2ET) architecture from Hussein et al. (2021) as our baseline for QASR dataset. We first augment the speech data with the speed perturbation with speed factors of 0.9, 1.0 and 1.1 (Ko et al., 2015). Then, we extract 83-dimensional feature frames consisting of 80-dimensional log Mel-spectrogram and pitch features (Ghahremani et al., 2014) and apply\n9Excluding switches between the utterances.\ncepstral mean and variance normalization. Furthermore, we augment these features using the specaugment approach (Park et al., 2019). We use Espnet (Watanabe et al., 2018) to train the E2E-T model on MGB-2 and QASR datasets. Each model was trained for 30 epochs using 4 NVIDIA Tesla V100 GPUs, each with 16 GB memory, which lasted two weeks. Results of the baseline model on both development and testsets are shown in Table 5.\nIt can be seen that the best E2E-T-MGB-2 achieves slightly better WER with a difference of 0.3% on average. This is expected since adopted E2E-T architecture was carefully tuned on MGB-2 dataset. However, the E2E-T-QASR achieves lower substitution and insertion rates with an absolute difference of 2.7% and 0.5% on average respectively. It can also be noticed that almost half of the E2ET-QASR errors are due to deletions. To investigate these results further, we visualize the distribution of segmentation duration of the MGB-2 train, the QASR train and the testsets as shown in Figure 5. We consider the range within 3 standard deviations of each distribution as the effective segmentation duration that contains 99% of the segments, and the rest 1% of the segments are considered as outliers. From Figure 5, it can be seen that QASR distribution is following the bell curve similar to the testset which was segmented by an expert transcriber. On the other hand, the MGB-2 distribution is right-skewed with segment duration outliers that go beyond 50 seconds. In addition, one can observe that the effective segmentation duration of the testset is 9 seconds, which is larger than QASR effective segmentation duration, which is only 7 seconds. On the other hand, the MGB-2 effective segmentation duration covers a much larger range of over 30 seconds. The difference in the segment duration affects the statistical properties of the data and causes a shift in the data distribution. We think that this is the main reason why the baseline E2ET-QASR achieves worse results than best E2E-TMGB-2. To validate our assumption, we analyze the E2E-T-QASR transcription and found that the\ndeletion errors mainly appeared with segments that are larger than 7 seconds. We illustrate our findings with two transcription examples in Buckwalter (BW) format shown in Figure 6: short segment of 6 seconds, and long segment of 10 seconds. Deletions are highlighted in red, substitutions in yellow, and correct in green. It can be seen from the short example that E2E-T-QASR achieves better results with a potential for code-switching. On the other hand, the long example confirms our assumption about the shift in segments duration distribution between QASR and the testset."
    }, {
      "heading" : "3.2 Automatic Punctuation Restoration",
      "text" : "In this section, we explore QASR for the automatic punctuation restoration task. To prepare the training data, we first segment the utterances from the same speaker with a maximum window of 120 tokens. We then remove utterances with ≤ 6 words and no punctuation in the segment. We pre-process the lexical utterances, removing diacritics, brackets, among others. For the task, we only keep the top 3 punctuation classes (‘,’, ‘?’ and ‘.’) and rest are mapped to class ‘O’ representing no punctuation. The distribution of punctuation in QASR are highly imbalanced (as shown in Table 6), which is expected of a spoken corpus. However, in comparison to the Fisher corpus (Cieri et al., 2004) and other language datasets (see (Li and Lin, 2020)), the distribution is more skewed. This is because in Arabic, punctuation marks are rarely used, e.g., Segment1 in Figure 1, can be logically divided into two segments separated by a full stop.\nWe adapt a simple transformer-biLSTM architecture (Alam et al., 2020) as our baseline model using lexical information. Given an input token sequence (x1, x2..., xm), we extract the subwords (s1, s2..., sn) using wordpiece tokenizer. These subwords are fed into the pre-trained BERT model, which outputs a vector of d dimension for each time step. These d vectors are then passed to a BiLSTM\nlayer, consisting of h hidden units. The choice of using BiLSTM is to make effective use of both past ( −→ h ) and future ( ←− h ) contexts for prediction. The concatenated −→ h + ←− h output at each time step is then fed to a fully-connected layer with four output neurons, which correspond to 3 punctuation marks and the ‘O’ token.\nDuring the training, special tokens identifying start- and end-of the sentence are added to the in-\nput subword sequence.10 For this task, we used AraBERT (Antoun et al., 2020): pre-trained on newspaper articles, containing 3 transformer self attention layers with each hidden layer of 768. These token embeddings are then passed onto a BiLSTM with hidden dimension of 768. The baseline model is trained using Adam optimizer with a learning rate of 1e− 5 and 32 batch size for 10 epochs.\nDespite the fact that Arabic has a skewed distribution in punctuation, the baseline results reported in Table 7 for the 3 punctuation and ‘O’ labels show that the prediction results of the full stop and the question mark are better than the comma. This again reconfirms that in Arabic, the use of comma is highly debatable (Mubarak et al., 2015; Mubarak and Darwish, 2014) and can easily be substituted by the full stop or other punctuation. In the future, we will explore better architectures with information from different modalities, such as acoustics.\n10The maximum length of the subwords is set to 256. In cases, if the sequence exceeds the maximum length, it is then divided into two separate sequences."
    }, {
      "heading" : "3.3 Speaker Verification",
      "text" : "One of the biggest challenges in broadcast domain is its speech diversity. The anchor speaker voice is often clear and planned. However, the spoken style11 of different program guests can present various challenges. Here, we showcase how QASR could be used to evaluate existing speaker models based on the speakers’ role in each episode. In the future, the dataset can also be used to study turntaking and speaker dynamics, given the interaction between speakers in QASR.\nWe adapt one of the widely-known architectures used to model an end-to-end text-independent Speaker Recognition (SR) system. For the study, we use a pre-trained model, with four temporal convolution neural networks followed by a global (statistical) pooling layer and then two fully connected layers. The input to the model is MFCCs features (with 40 coefficient) computed with a 25msec window and 10ms frame-rate from the 16KHz audio. The model is trained on Voxceleb1 (Nagrani et al., 2017) development set (containing 1, 211 speakers and≈ 147K utterances). More details can be found in Shon et al. (2018); Chowdhury et al. (2020a).\nFor speaker verification, we use verified same/different-speaker pairs of speech segments as input. We extract the length normalized embed-\n11The style can vary based on language fluency, speech rate, use of different dialects among other factors.\ndings from the last layer of the SR model and then computed the cosine similarity between pairs.\nFor our evaluation, we constructed these verification pair trials by randomly picking up 40K utterance pairs from: (i) speakers of the same gender; (ii) similar utterance lengths; and (iii) a balanced distribution between positive and negative targets12. For this, we use the most frequent 20 anchor and 20 guest speakers data subset described in Table 8. We then compare the Equal Error Rate (EER) of the model, reported in Table 9, using the designed verification pairs based on a particular job role, or their combination. In addition, we also report the results on VoxCeleb1 official verification testset as a reference.\nFrom the results, we observe that the SR model effectively distinguishes between the positive and negative pairs with ≈ 70% (A) - 72% (G) accuracy. Comparing the EER, we notice that it is harder to differentiate between anchors than guests. This can be due to the fact that anchors are using the same acoustic conditions, and the current models are learning recording conditions (Chowdhury et al., 2020a) as well as speaker information."
    }, {
      "heading" : "3.4 Arabic Dialect Identification",
      "text" : "To understand the dialectal nature of QASR dataset, we analyze the acoustic and lexical representations for 100 segments from each speaker13.\nTo obtain the dialect labels, we run the pretrained dialect identification models for both speech and text modality. We address the dialect identification as multi-stage classification: Firstly, we predict the labels of the segments - MSA vs DA - and, secondly, if the label is DA, we further propagate the labels to detect the country of the selected speaker (i.e fine-grained dialect classification). For country level evaluation, we manually annotate each speaker’s country label (see Table 8).\nFor lexical modality, we use the pre-trained QADI (Abdelali et al., 2020), and for the acoustic modality, we use ADI-514 (Shon et al., 2017; Ali et al., 2019) – as MSA vs DA classifier – along with ADI-1715 (Shon et al., 2020) for fine-grained labels.\n12The official verification pairs are included as a part of QASR.\n13We used the same speaker set as the SV task. 14https://github.com/swshon/dialectID_\ne2e 15https://github.com/swshon/ arabic-dialect-identification\nWe observe that in both the modalities, 50% of the anchors speak MSA in 70% of the time in speech and 90% of the time in text. As for the other 50%, we notice that using the dialect identification modules, we can detect only 20% of the speaker’s nationality correctly. The aforementioned observations are pre-anticipated, as anchors are professionally trained to speak mostly in MSA, making it harder for the model to predict the correct country label. This also explains why the large portion of the data is MSA.\nAs for guest speakers, we notice that the lexical classifier detected that 30% of the speakers use MSA, while 70% of the speakers were detected as DA. As for the acoustic models, we notice that all speakers use dialects more than 70% of the time. Comparing the accuracy of identifying the correct dialects based on annotated country labels, we notice that both the text and acoustic models perform comparatively better in identify the guest speakers’ country – 64% from text and 65% from acoustic. Our hypothesis for such increase in performance is that guest speakers, unlike the anchors, mostly speak using their dialects, making it easier for the model to infer their country.\nWhen comparing the decision from both modalities, we notice that there is an agreement of 67.5% (65% for anchor and 70% for guest speakers) for MSA/DA classification. Most of the classification errors in speech and text dialect identification models are due to confusion between dialects spoken in neighboring countries; e.g. Syria and Lebanon in the Levantine region; Tunisia and Algeria in the North African region."
    }, {
      "heading" : "3.5 Named Entity Recognition (NER)",
      "text" : "NER is essential for a variety of NLP applications such as information extraction and summarization. There are many researches on Arabic NER for news articles, e.g. ANERcorp (Benajiba and Rosso, 2008) and microblogs (Darwish, 2013). However, we are not aware of any studies or datasets for NER in Arabic news transcription, which can be useful for applications like video search. We manually annotate and revised the MGB-2 testset for basic NE types, namely Person (PER), Location (LOC), Organization (ORG) and Others (OTH/MISC) following the guidelines in (Benajiba and Rosso, 2008). The testset (70K words) along with NER annotation is available as part of QASR. From the annotation, we observed NEs are 7% of the corpus and their distribution is as follows: PER= 32%, LOC=\n46%, ORG= 18% and OTH= 5%16. We test the publicly available Arabic Farasa NER on our new testset and compare performance with the standard news testset (ANERcorp). Results are listed in Table 10. As shown, testing NER on transcribed speech has lower F1 by 15% compared to testing on a standard news testset (from 84.3% to 69.8%). We anticipate that characteristics of speech transcription described in Section 2.3 affected NER negatively17. We keep enhancing NER for speech transcription for future work."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we introduce a 2, 000 hours transcribed Arabic speech corpus, QASR. We report results for automatic speech recognition, Arabic dialect identification, speaker verification, and punctuation restoration to showcase the importance and usability of the dataset. QASR is also the first Arabic speech-NLP corpus to study spoken modern standard Arabic and dialectal Arabic. We report for the first time named entity recognition in Arabic news transcription. The 11, 092 unique speakers present in QASR can be used to study turn-taking and speaker dynamics in the broadcast domain. The corpus can also be useful for unsupervised methods to select speaker for text to speech (Gallegos et al., 2020). The QASR is publicly available for the research community."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was made possible with the collaboration between Qatar Computing Research Institute, HBKU and Aljazeera media network. This data is hosted on ArabicSpeech portal18, which is a community based effort that runs for the benefit of Arabic speech science and technologies.\n16ANERcorp contains 150K words. NEs are 11%. Distribution: PER= 39%, LOC= 30%, ORG= 21%, MISC= 10%.\n17Disfluency example: ? àñj J B @ ú æªK I Ë @ Õ æ\nK @ I. J £\n(OK you are isn’t it I mean are you not ashamed?) 18https://arabicspeech.org/\nEthical Concern and Social Impact\nUser Privacy\nQASR dataset only includes programs that have been broadcast by the Aljazeera news media. No additional identity of the guest is revealed in the data, which was made anonymous in the original program. However, in the future, if any concern is raised for a particular content, we will comply to legitimate concerns by removing the affected content from the corpus.\nBiases in QASR\nAny biases found in the dataset are unintentional, and we do not intend to do harm to any group or individual. The bias in our data, for example towards a particular gender is unintentional and is a true representation of the programs. We do address these concerns by collecting examples from both parties before any general suggestion.\nAs for the assigned annotation label, we follow a well-defined schema and available information to perceive a final label. For e.g. gender label – male/female is perceived from the data and might not be a true representative of the speakers’ choice.\nPotential Misuse\nWe request the research community to be aware that our dataset can be used to misuse quotes for the speakers for political or other gain. If such misuse is noticed, human moderation is encouraged in order to ensure this does not occur."
    } ],
    "references" : [ {
      "title" : "Farasa: A fast and furious segmenter for Arabic",
      "author" : [ "Ahmed Abdelali", "Kareem Darwish", "Nadir Durrani", "Hamdy Mubarak." ],
      "venue" : "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Demonstra-",
      "citeRegEx" : "Abdelali et al\\.,? 2016",
      "shortCiteRegEx" : "Abdelali et al\\.",
      "year" : 2016
    }, {
      "title" : "Arabic dialect identification in the wild",
      "author" : [ "Ahmed Abdelali", "Hamdy Mubarak", "Younes Samih", "Sabit Hassan", "Kareem Darwish." ],
      "venue" : "arXiv preprint arXiv:2005.06557.",
      "citeRegEx" : "Abdelali et al\\.,? 2020",
      "shortCiteRegEx" : "Abdelali et al\\.",
      "year" : 2020
    }, {
      "title" : "Nadi 2020: The first nuanced Arabic dialect identification shared task",
      "author" : [ "Muhammad Abdul-Mageed", "Chiyu Zhang", "Houda Bouamor", "Nizar Habash." ],
      "venue" : "arXiv preprint arXiv:2010.11334.",
      "citeRegEx" : "Abdul.Mageed et al\\.,? 2020",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2020
    }, {
      "title" : "Punctuation restoration using transformer models for resource-rich and-poor languages",
      "author" : [ "Tanvirul Alam", "Akib Khan", "Firoj Alam." ],
      "venue" : "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020), pages 132–142.",
      "citeRegEx" : "Alam et al\\.,? 2020",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2020
    }, {
      "title" : "The MGB-2 challenge: Arabic multi-dialect broadcast media recognition",
      "author" : [ "Ahmed Ali", "Peter Bell", "James Glass", "Yacine Messaoui", "Hamdy Mubarak", "Steve Renals", "Yifan Zhang." ],
      "venue" : "2016 IEEE Spoken Language Technology Workshop (SLT), pages 279–",
      "citeRegEx" : "Ali et al\\.,? 2016",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2016
    }, {
      "title" : "Connecting Arabs: bridging the gap in dialectal speech recognition",
      "author" : [ "Ahmed Ali", "Shammur Chowdhury", "Mohamed Afify", "Wassim El-Hajj", "Hazem Hajj", "Mourad Abbas", "Amir Hussein", "Nada Ghneim", "Mohammad Abushariah", "Assal Alqudah." ],
      "venue" : "Commu-",
      "citeRegEx" : "Ali et al\\.,? 2021",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2021
    }, {
      "title" : "The MGB-5 challenge: Recognition and dialect identification of dialectal Arabic speech",
      "author" : [ "Ahmed Ali", "Suwon Shon", "Younes Samih", "Hamdy Mubarak", "Ahmed Abdelali", "James Glass", "Steve Renals", "Khalid Choukri." ],
      "venue" : "IEEE Automatic Speech",
      "citeRegEx" : "Ali et al\\.,? 2019",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2019
    }, {
      "title" : "Speech recognition challenge in the wild: Arabic MGB-3",
      "author" : [ "Ahmed Ali", "Stephan Vogel", "Steve Renals." ],
      "venue" : "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 316– 322. IEEE.",
      "citeRegEx" : "Ali et al\\.,? 2017",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2017
    }, {
      "title" : "Arabert: Transformer-based model for Arabic language understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj." ],
      "venue" : "LREC 2020 Workshop Language Resources and Evaluation Conference 11–16 May 2020, page 9.",
      "citeRegEx" : "Antoun et al\\.,? 2020",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "Common voice: A massivelymultilingual speech corpus",
      "author" : [ "Rosana Ardila", "Megan Branson", "Kelly Davis", "Michael Henretty", "Michael Kohler", "Josh Meyer", "Reuben Morais", "Lindsay Saunders", "Francis M Tyers", "Gregor Weber." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Ardila et al\\.,? 2019",
      "shortCiteRegEx" : "Ardila et al\\.",
      "year" : 2019
    }, {
      "title" : "The MGB challenge: Evaluating multigenre broadcast media recognition",
      "author" : [ "Peter Bell", "Mark JF Gales", "Thomas Hain", "Jonathan Kilgour", "Pierre Lanchantin", "Xunying Liu", "Andrew McParland", "Steve Renals", "Oscar Saz", "Mirjam Wester" ],
      "venue" : null,
      "citeRegEx" : "Bell et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bell et al\\.",
      "year" : 2015
    }, {
      "title" : "Arabic named entity recognition using conditional random fields",
      "author" : [ "Yassine Benajiba", "Paolo Rosso." ],
      "venue" : "Proc. of Workshop on HLT & NLP within the Arabic World, LREC, volume 8, pages 143–153. Citeseer.",
      "citeRegEx" : "Benajiba and Rosso.,? 2008",
      "shortCiteRegEx" : "Benajiba and Rosso.",
      "year" : 2008
    }, {
      "title" : "Lightly supervised recognition for automatic alignment of large coherent speech recordings",
      "author" : [ "Norbert Braunschweiler", "Mark JF Gales", "Sabine Buchholz." ],
      "venue" : "Eleventh Annual Conference of the International Speech Communication Association.",
      "citeRegEx" : "Braunschweiler et al\\.,? 2010",
      "shortCiteRegEx" : "Braunschweiler et al\\.",
      "year" : 2010
    }, {
      "title" : "State-of-the-art speech recognition with sequence-to-sequence models",
      "author" : [ "Chung-Cheng Chiu", "Tara N Sainath", "Yonghui Wu", "Rohit Prabhavalkar", "Patrick Nguyen", "Zhifeng Chen", "Anjuli Kannan", "Ron J Weiss", "Kanishka Rao", "Ekaterina Gonina" ],
      "venue" : null,
      "citeRegEx" : "Chiu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2018
    }, {
      "title" : "What does an end-to-end dialect identification model learn about non-dialectal information? Proc",
      "author" : [ "Shammur A Chowdhury", "Ahmed Ali", "Suwon Shon", "James Glass." ],
      "venue" : "Interspeech 2020, pages 462– 466.",
      "citeRegEx" : "Chowdhury et al\\.,? 2020a",
      "shortCiteRegEx" : "Chowdhury et al\\.",
      "year" : 2020
    }, {
      "title" : "Effects of dialectal code-switching on speech modules: A study using Egyptian Arabic broadcast speech",
      "author" : [ "Shammur A Chowdhury", "Younes Samih", "Mohamed Eldesouki", "Ahmed Ali." ],
      "venue" : "Proc. Interspeech.",
      "citeRegEx" : "Chowdhury et al\\.,? 2020b",
      "shortCiteRegEx" : "Chowdhury et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards one model to rule all: Multilingual strategy for dialectal codeswitching Arabic Asr",
      "author" : [ "Shammur Absar Chowdhury", "Amir Hussein", "Ahmed Abdelali", "Ahmed Ali." ],
      "venue" : "arXiv:2105.14779.",
      "citeRegEx" : "Chowdhury et al\\.,? 2021",
      "shortCiteRegEx" : "Chowdhury et al\\.",
      "year" : 2021
    }, {
      "title" : "Functions of silences towards information flow in spoken conversation",
      "author" : [ "Shammur Absar Chowdhury", "Evgeny Stepanov", "Morena Danieli", "Giuseppe Riccardi." ],
      "venue" : "Proceedings of the Workshop on Speech-Centric Natural Language",
      "citeRegEx" : "Chowdhury et al\\.,? 2017",
      "shortCiteRegEx" : "Chowdhury et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic classification of speech overlaps: Feature representation and algorithms",
      "author" : [ "Shammur Absar Chowdhury", "Evgeny A Stepanov", "Morena Danieli", "Giuseppe Riccardi." ],
      "venue" : "Computer Speech & Language, 55:145–167.",
      "citeRegEx" : "Chowdhury et al\\.,? 2019",
      "shortCiteRegEx" : "Chowdhury et al\\.",
      "year" : 2019
    }, {
      "title" : "The fisher corpus: a resource for the next generations of speech-to-text",
      "author" : [ "Christopher Cieri", "David Miller", "Kevin Walker." ],
      "venue" : "LREC, volume 4, pages 69–71.",
      "citeRegEx" : "Cieri et al\\.,? 2004",
      "shortCiteRegEx" : "Cieri et al\\.",
      "year" : 2004
    }, {
      "title" : "Named entity recognition using cross-lingual resources: Arabic as an example",
      "author" : [ "Kareem Darwish." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1558–1567.",
      "citeRegEx" : "Darwish.,? 2013",
      "shortCiteRegEx" : "Darwish.",
      "year" : 2013
    }, {
      "title" : "Data augmentation for endto-end code-switching speech recognition",
      "author" : [ "Chenpeng Du", "Hao Li", "Yizhou Lu", "Lan Wang", "Yanmin Qian." ],
      "venue" : "2021 IEEE Spoken Language Technology Workshop (SLT), pages 194–200. IEEE.",
      "citeRegEx" : "Du et al\\.,? 2021",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2021
    }, {
      "title" : "An unsupervised method to select a speaker subset from large multispeaker speech synthesis datasets",
      "author" : [ "Pilar Oplustil Gallegos", "Jennifer Williams", "Joanna Rownicka", "Simon King." ],
      "venue" : "Proc. Interspeech 2020, pages 1758–1762.",
      "citeRegEx" : "Gallegos et al\\.,? 2020",
      "shortCiteRegEx" : "Gallegos et al\\.",
      "year" : 2020
    }, {
      "title" : "Comparing the Level of Code-Switching in Corpora",
      "author" : [ "Björn Gambäck", "Amitava Das." ],
      "venue" : "LREC.",
      "citeRegEx" : "Gambäck and Das.,? 2016",
      "shortCiteRegEx" : "Gambäck and Das.",
      "year" : 2016
    }, {
      "title" : "A pitch extraction algorithm tuned for automatic speech recognition",
      "author" : [ "Pegah Ghahremani", "Bagher BabaAli", "Daniel Povey", "Korbinian Riedhammer", "Jan Trmal", "Sanjeev Khudanpur." ],
      "venue" : "IEEE international conference on acoustics, speech and sig-",
      "citeRegEx" : "Ghahremani et al\\.,? 2014",
      "shortCiteRegEx" : "Ghahremani et al\\.",
      "year" : 2014
    }, {
      "title" : "Asad: Arabic social media analytics and understanding",
      "author" : [ "Sabit Hassan", "Hamdy Mubarak", "Ahmed Abdelali", "Kareem Darwish." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System",
      "citeRegEx" : "Hassan et al\\.,? 2021",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2021
    }, {
      "title" : "Arabic speech recognition by end-to-end",
      "author" : [ "Amir Hussein", "Shinji Watanabe", "Ahmed Ali" ],
      "venue" : null,
      "citeRegEx" : "Hussein et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hussein et al\\.",
      "year" : 2021
    }, {
      "title" : "Audio augmentation for speech recognition",
      "author" : [ "Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Sixteenth Annual Conference of the International Speech Communication Association.",
      "citeRegEx" : "Ko et al\\.,? 2015",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2015
    }, {
      "title" : "A 43 language multilingual punctuation prediction neural network model",
      "author" : [ "Xinxing Li", "Edward Lin." ],
      "venue" : "Proc. Interspeech 2020, pages 1067–1071.",
      "citeRegEx" : "Li and Lin.,? 2020",
      "shortCiteRegEx" : "Li and Lin.",
      "year" : 2020
    }, {
      "title" : "Highly effective Arabic diacritization using sequence to sequence modeling",
      "author" : [ "Hamdy Mubarak", "Ahmed Abdelali", "Hassan Sajjad", "Younes Samih", "Kareem Darwish." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Mubarak et al\\.,? 2019",
      "shortCiteRegEx" : "Mubarak et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic correction of Arabic text: A cascaded approach",
      "author" : [ "Hamdy Mubarak", "Kareem Darwish." ],
      "venue" : "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP), pages 132–136.",
      "citeRegEx" : "Mubarak and Darwish.,? 2014",
      "shortCiteRegEx" : "Mubarak and Darwish.",
      "year" : 2014
    }, {
      "title" : "Qcri@ qalb-2015 shared task: Correction of Arabic text for native and non-native speakers’ errors",
      "author" : [ "Hamdy Mubarak", "Kareem Darwish", "Ahmed Abdelali." ],
      "venue" : "Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 150–154.",
      "citeRegEx" : "Mubarak et al\\.,? 2015",
      "shortCiteRegEx" : "Mubarak et al\\.",
      "year" : 2015
    }, {
      "title" : "Voxceleb: a large-scale speaker identification dataset",
      "author" : [ "Arsha Nagrani", "Joon Son Chung", "Andrew Zisserman." ],
      "venue" : "arXiv preprint arXiv:1706.08612.",
      "citeRegEx" : "Nagrani et al\\.,? 2017",
      "shortCiteRegEx" : "Nagrani et al\\.",
      "year" : 2017
    }, {
      "title" : "Handbook of natural language processing and machine translation: DARPA global autonomous language exploitation",
      "author" : [ "Joseph Olive", "Caitlin Christianson", "John McCary." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Olive et al\\.,? 2011",
      "shortCiteRegEx" : "Olive et al\\.",
      "year" : 2011
    }, {
      "title" : "A look at nist’s benchmark asr tests: past, present, and future",
      "author" : [ "David S Pallett." ],
      "venue" : "2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No. 03EX721), pages 483–488. IEEE.",
      "citeRegEx" : "Pallett.,? 2003",
      "shortCiteRegEx" : "Pallett.",
      "year" : 2003
    }, {
      "title" : "Librispeech: an asr corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206–5210.",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Specaugment: A simple data augmentation method for automatic speech recognition",
      "author" : [ "Daniel S Park", "William Chan", "Yu Zhang", "Chung-Cheng Chiu", "Barret Zoph", "Ekin D Cubuk", "Quoc V Le." ],
      "venue" : "Proc. Interspeech, pages 2613–2617.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "A hybrid deep ensemble for speech disfluency classification",
      "author" : [ "Sheena Christabel Pravin", "M Palanivelan." ],
      "venue" : "Circuits, Systems, and Signal Processing, pages 1–28.",
      "citeRegEx" : "Pravin and Palanivelan.,? 2021",
      "shortCiteRegEx" : "Pravin and Palanivelan.",
      "year" : 2021
    }, {
      "title" : "A neural architecture for dialectal Arabic segmentation",
      "author" : [ "Younes Samih", "Mohammed Attia", "Mohamed Eldesouki", "Ahmed Abdelali", "Hamdy Mubarak", "Laura Kallmeyer", "Kareem Darwish." ],
      "venue" : "Proceedings of the Third Arabic Natural Language",
      "citeRegEx" : "Samih et al\\.,? 2017",
      "shortCiteRegEx" : "Samih et al\\.",
      "year" : 2017
    }, {
      "title" : "2017. English conversational telephone speech recognition by humans",
      "author" : [ "George Saon", "Gakuto Kurata", "Tom Sercu", "Kartik Audhkhasi", "Samuel Thomas", "Dimitrios Dimitriadis", "Xiaodong Cui", "Bhuvana Ramabhadran", "Michael Picheny", "Lynn-Li Lim" ],
      "venue" : null,
      "citeRegEx" : "Saon et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Saon et al\\.",
      "year" : 2017
    }, {
      "title" : "MITQCRI Arabic dialect identification system for the 2017 multi-genre broadcast challenge",
      "author" : [ "Suwon Shon", "Ahmed Ali", "James Glass." ],
      "venue" : "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 374–380. IEEE.",
      "citeRegEx" : "Shon et al\\.,? 2017",
      "shortCiteRegEx" : "Shon et al\\.",
      "year" : 2017
    }, {
      "title" : "Adi17: A fine-grained Arabic dialect identification dataset",
      "author" : [ "Suwon Shon", "Ahmed Ali", "Younes Samih", "Hamdy Mubarak", "James Glass." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Shon et al\\.,? 2020",
      "shortCiteRegEx" : "Shon et al\\.",
      "year" : 2020
    }, {
      "title" : "Mce 2018: The 1st multi-target speaker detection and identification challenge evaluation",
      "author" : [ "Suwon Shon", "Najim Dehak", "Douglas Reynolds", "James Glass." ],
      "venue" : "arXiv preprint arXiv:1904.04240.",
      "citeRegEx" : "Shon et al\\.,? 2019",
      "shortCiteRegEx" : "Shon et al\\.",
      "year" : 2019
    }, {
      "title" : "Frame-level speaker embeddings for textindependent speaker recognition and analysis of end-to-end model",
      "author" : [ "Suwon Shon", "Hao Tang", "James Glass." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages",
      "citeRegEx" : "Shon et al\\.,? 2018",
      "shortCiteRegEx" : "Shon et al\\.",
      "year" : 2018
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "Temple F Smith", "Michael S Waterman" ],
      "venue" : "Journal of molecular biology,",
      "citeRegEx" : "Smith and Waterman,? \\Q1981\\E",
      "shortCiteRegEx" : "Smith and Waterman",
      "year" : 1981
    }, {
      "title" : "End-to-end multi-talker overlapping speech recognition",
      "author" : [ "Anshuman Tripathi", "Han Lu", "Hasim Sak." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6129–6133. IEEE.",
      "citeRegEx" : "Tripathi et al\\.,? 2020",
      "shortCiteRegEx" : "Tripathi et al\\.",
      "year" : 2020
    }, {
      "title" : "Voxlingua107: a dataset for spoken language recognition",
      "author" : [ "Jörgen Valk", "Tanel Alumäe." ],
      "venue" : "arXiv preprint arXiv:2011.12998.",
      "citeRegEx" : "Valk and Alumäe.,? 2020",
      "shortCiteRegEx" : "Valk and Alumäe.",
      "year" : 2020
    }, {
      "title" : "Voxpopuli: A large-scale multilingual speech corpus for representation",
      "author" : [ "Changhan Wang", "Morgane Rivière", "Ann Lee", "Anne Wu", "Chaitanya Talnikar", "Daniel Haziza", "Mary Williamson", "Juan Pino", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Espnet: Endto-end speech processing toolkit",
      "author" : [ "Shinji Watanabe", "Takaaki Hori", "Shigeki Karita", "Tomoki Hayashi", "Jiro Nishitoba", "Yuya Unno", "NelsonEnrique Yalta Soplin", "Jahn Heymann", "Matthew Wiesner", "Nanxin Chen" ],
      "venue" : null,
      "citeRegEx" : "Watanabe et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards interactive annotation for hesitation in conversational speech",
      "author" : [ "Jane Wottawa", "Marie Tahon", "Apolline Marin", "Nicolas Audibert." ],
      "venue" : "LREC 2020.",
      "citeRegEx" : "Wottawa et al\\.,? 2020",
      "shortCiteRegEx" : "Wottawa et al\\.",
      "year" : 2020
    }, {
      "title" : "Achieving human parity in conversational speech recognition",
      "author" : [ "Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig." ],
      "venue" : "arXiv preprint arXiv:1610.05256.",
      "citeRegEx" : "Xiong et al\\.,? 2016",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Research on Automatic Speech Recognition (ASR) has attracted a lot of attention in recent years (Chiu et al., 2018; Watanabe et al., 2018).",
      "startOffset" : 96,
      "endOffset" : 138
    }, {
      "referenceID" : 48,
      "context" : "Research on Automatic Speech Recognition (ASR) has attracted a lot of attention in recent years (Chiu et al., 2018; Watanabe et al., 2018).",
      "startOffset" : 96,
      "endOffset" : 138
    }, {
      "referenceID" : 50,
      "context" : "Such success has brought remarkable improvements in reaching human-level performance (Xiong et al., 2016; Saon et al., 2017; Hussein et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 146
    }, {
      "referenceID" : 39,
      "context" : "Such success has brought remarkable improvements in reaching human-level performance (Xiong et al., 2016; Saon et al., 2017; Hussein et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "Such success has brought remarkable improvements in reaching human-level performance (Xiong et al., 2016; Saon et al., 2017; Hussein et al., 2021).",
      "startOffset" : 85,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : "This has been achieved by the development of large spoken corpora: supervised (Panayotov et al., 2015; Ardila et al., 2019); semi-supervised (Bell et al.",
      "startOffset" : 78,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "This has been achieved by the development of large spoken corpora: supervised (Panayotov et al., 2015; Ardila et al., 2019); semi-supervised (Bell et al.",
      "startOffset" : 78,
      "endOffset" : 123
    }, {
      "referenceID" : 46,
      "context" : ", 2016); and more recently unsupervised (Valk and Alumäe, 2020; Wang et al., 2021) transcription.",
      "startOffset" : 40,
      "endOffset" : 82
    }, {
      "referenceID" : 47,
      "context" : ", 2016); and more recently unsupervised (Valk and Alumäe, 2020; Wang et al., 2021) transcription.",
      "startOffset" : 40,
      "endOffset" : 82
    }, {
      "referenceID" : 41,
      "context" : "This work enables to either reduce Word Error Rate (WER) considerably or extract metadata from speech: dialect-identification (Shon et al., 2020); speaker-identification (Shon et al.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 42,
      "context" : ", 2020); speaker-identification (Shon et al., 2019); and codeswitching (Chowdhury et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 219
    }, {
      "referenceID" : 29,
      "context" : "NLP research for Arabic has achieved a milestone in the last few years in morphological disambiguation, Named Entity Recognition (NER) and diacritization (Pasha et al., 2014; Abdelali et al., 2016; Mubarak et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017).",
      "startOffset" : 140,
      "endOffset" : 187
    }, {
      "referenceID" : 38,
      "context" : "With the rise of Dialectal Arabic (DA) content online, more resources and models have been built to study DA textual dialect identification (Abdul-Mageed et al., 2020; Samih et al., 2017).",
      "startOffset" : 140,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : "Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 45,
      "context" : "Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al.",
      "startOffset" : 97,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Speech poses unique challenges such as disfluency (Pravin and Palanivelan, 2021), overlap speech (Tripathi et al., 2020; Chowdhury et al., 2019), hesitation (Wottawa et al.",
      "startOffset" : 97,
      "endOffset" : 144
    }, {
      "referenceID" : 49,
      "context" : ", 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al.",
      "startOffset" : 20,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : ", 2019), hesitation (Wottawa et al., 2020; Chowdhury et al., 2017), and code-switching (Du et al.",
      "startOffset" : 20,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : "The CallHome task within the NIST benchmark evaluations framework (Pallett, 2003), released one of the first transcribed Arabic dialect dataset.",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : "Projects such as GALE and TRANSTAC (Olive et al., 2011) program, released more than 251 hours of Arabic data, including the first spoken Iraqi dialect among others.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "The following datasets are released from the Multi-Genre Broadcast MGB challenge: (i) MGB2 (Ali et al., 2016) – this dataset is the first milestone towards designing the first large scale continuous speech recognition for Arabic language.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "(ii) MGB-3 (Ali et al., 2017) – focused on only Egyptian Arabic broadcast data comprises of 16 hours.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "(iii) MGB-5 (Ali et al., 2019) – consists of 13 hours of Moroccan Arabic speech data.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "For better evaluation of the QASR corpus, we reused the publicly available MGB-2 (Ali et al., 2016) testset as it has been manually revised, coming from the same channel, thus making this testset ideal to evaluate the QASR corpus.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "More details can be found in ASAD demo paper (Hassan et al., 2021).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "For details about word and grapheme match, refer to (Bell et al., 2015; Ali et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "For details about word and grapheme match, refer to (Bell et al., 2015; Ali et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "The main concept of this method is to run an Arabic speech recognition system over the entire episode (Khurana and Ali) and use the recognized word sequences and their locations in time for automatic alignment (Braunschweiler et al., 2010).",
      "startOffset" : 210,
      "endOffset" : 239
    }, {
      "referenceID" : 24,
      "context" : "Then, we extract 83-dimensional feature frames consisting of 80-dimensional log Mel-spectrogram and pitch features (Ghahremani et al., 2014) and apply",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 48,
      "context" : "We use Espnet (Watanabe et al., 2018) to train the E2E-T model on MGB-2 and QASR datasets.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "However, in comparison to the Fisher corpus (Cieri et al., 2004) and other language datasets (see (Li and Lin, 2020)), the distribution is more skewed.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : ", 2004) and other language datasets (see (Li and Lin, 2020)), the distribution is more skewed.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "We adapt a simple transformer-biLSTM architecture (Alam et al., 2020) as our baseline model using lexical information.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "AraBERT (Antoun et al., 2020): pre-trained on newspaper articles, containing 3 transformer self attention layers with each hidden layer of 768.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "This again reconfirms that in Arabic, the use of comma is highly debatable (Mubarak et al., 2015; Mubarak and Darwish, 2014) and can easily be substituted by the full stop or other punctuation.",
      "startOffset" : 75,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "This again reconfirms that in Arabic, the use of comma is highly debatable (Mubarak et al., 2015; Mubarak and Darwish, 2014) and can easily be substituted by the full stop or other punctuation.",
      "startOffset" : 75,
      "endOffset" : 124
    }, {
      "referenceID" : 32,
      "context" : "The model is trained on Voxceleb1 (Nagrani et al., 2017) development set (containing 1, 211 speakers and≈ 147K utterances).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "This can be due to the fact that anchors are using the same acoustic conditions, and the current models are learning recording conditions (Chowdhury et al., 2020a) as well as speaker information.",
      "startOffset" : 138,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "For lexical modality, we use the pre-trained QADI (Abdelali et al., 2020), and for the acoustic modality, we use ADI-514 (Shon et al.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 40,
      "context" : ", 2020), and for the acoustic modality, we use ADI-514 (Shon et al., 2017; Ali et al., 2019) – as MSA vs DA classifier – along with ADI-1715 (Shon et al.",
      "startOffset" : 55,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : ", 2020), and for the acoustic modality, we use ADI-514 (Shon et al., 2017; Ali et al., 2019) – as MSA vs DA classifier – along with ADI-1715 (Shon et al.",
      "startOffset" : 55,
      "endOffset" : 92
    }, {
      "referenceID" : 41,
      "context" : ", 2019) – as MSA vs DA classifier – along with ADI-1715 (Shon et al., 2020) for fine-grained labels.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "ANERcorp (Benajiba and Rosso, 2008) and microblogs (Darwish, 2013).",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "ANERcorp (Benajiba and Rosso, 2008) and microblogs (Darwish, 2013).",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "We manually annotate and revised the MGB-2 testset for basic NE types, namely Person (PER), Location (LOC), Organization (ORG) and Others (OTH/MISC) following the guidelines in (Benajiba and Rosso, 2008).",
      "startOffset" : 177,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "The corpus can also be useful for unsupervised methods to select speaker for text to speech (Gallegos et al., 2020).",
      "startOffset" : 92,
      "endOffset" : 115
    } ],
    "year" : 2021,
    "abstractText" : "We introduce the largest transcribed Arabic speech corpus, QASR1, collected from the broadcast domain. This multi-dialect speech dataset contains 2, 000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acousticsand/or linguisticsbased Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community.",
    "creator" : "LaTeX with hyperref"
  }
}