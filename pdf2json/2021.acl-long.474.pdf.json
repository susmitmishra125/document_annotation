{
  "name" : "2021.acl-long.474.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Focus Attention: Promoting Faithfulness and Diversity in Summarization",
    "authors" : [ "Rahul Aralikatte", "Shashi Narayan", "Joshua Maynez", "Sascha Rothe", "Ryan McDonald" ],
    "emails" : [ "rahul@di.ku.dk", "shashinarayan@google.com", "joshuahm@google.com", "rothe@google.com", "ryanmcd@asapp.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6078"
    }, {
      "heading" : "1 Introduction",
      "text" : "Document summarization — producing the shorter version of a document while preserving salient information (Mani, 2001; Nenkova and McKeown, 2011) — is challenging even for humans. Today, systems can generate summaries with a high level of fluency and coherence. This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017), and large pretrained language models (Devlin et al.,\n∗Work done when authors were interning/working at Google.\n2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019; Dong et al., 2019a; Song et al., 2019; Lewis et al., 2019; Rothe et al., 2020; Raffel et al., 2019; Zhang et al., 2019).\nHowever, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu-\nment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality.\nIn this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves this through a novel technique which augments standard contextual representations with a dynamic source-conditioned vocabulary biasing layer. We present the following experimental findings:\nFAME promotes summaries faithful to the source When evaluated on the BBC extreme summarization task (XSUM; Narayan et al., 2018), experiments with two state-of-the-art summarizers – ROBERTAS2S (Rothe et al., 2020) and PEGASUS (Zhang et al., 2019) – show that both models generate summaries that are more faithful to their input documents when augmented with FAME, in comparison with their vanilla counterparts.1 Faithfulness is measured through a variety of previously proposed metrics. In addition, we leverage the manually annotated document-summary pairs for faithfulness from Maynez et al. (2020) and train a scorer which serves as an efficient proxy for expensive human evaluations. We call this metric BERTFaithful.\nFAME enables diverse summaries FAME, by design, supports Focus Sampling – a technique that is more effective in sampling topically relevant tokens to generate diverse, yet topically consistent and faithful outputs, than other sampling methods (Fan et al., 2018; Holtzman et al., 2020). Figure 1 illustrates how focus sampling generates better summaries than other sampling methods. We demonstrate the effectiveness of our new Focus\n1In the paper we focus on assessing FAME on XSUM. But other summarization and text editing results can be found in Appendix B and C.\nSampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization."
    }, {
      "heading" : "2 Related Work",
      "text" : "Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C).\nTopic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written\nsummary is a good proxy for the input document.\nFaithful Generation Models Cao et al. (2017) force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. Song et al. (2020) propose to jointly generate a sentence and its syntactic dependency parse to induce grammaticality and faithfulness. Tian et al. (2019) learn a confidence score to ensure that the model attends to the source whenever necessary. Wang et al. (2020d) introduce new inputoutput matching and embedding similarity losses to alleviate hallucination issues. Yet, the task of generating text that is consistent with the input remains an open problem (Gabriel et al., 2020).\nDiverse Generation Models There has been a surge of interest in making language models generate more diverse and human-like outputs. Vijayakumar et al. (2018) and Kulikov et al. (2019) diversify beam search, using a task-specific scoring function, or constrain beam hypotheses to be sufficiently different. Others avoid text degeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a).\nFor conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input."
    }, {
      "heading" : "3 Summarization with Focus Attention",
      "text" : "Given an input document X1:n, we aim to generate its summary Y1:m, where n and m are input and output sequence lengths. We address this prob-\nlem using seq2seq architectures with Transformer encoder and decoder, augmented with FAME, as depicted in Figure 2. FAME learns a distribution txi for each input token xi over the vocabulary, measuring similarity of xi (in context) to the tokens in the vocabulary. The vocabulary distributions, txi , for all xi are combined to form a dynamic vocabulary bias that is added to the decoder logits. This mechanism enhances the conditioning on the input source and encourages the decoder to generate tokens that are topically similar to the input.\nTransformer-based seq2seq Model The encoder uses BERT Transformer layers with multiheaded self-attention to encode X to a vector sequence X = x1, . . . ,xn, with xi ∈ Rh, where h is the size of hidden representation. The decoder uses an identical architecture, except that at decoding step t, layer l adds a conditional representation ylt ∈ Rh for the token yt by attending to the output representation Y l−11:t−1 = y l−1 1 , . . . ,y l−1 t−1 generated so far through self-attention and by attending to the input contextual representation X through encoderdecoder attention. The probability of predicting the next token yt from a vocabulary V is:\np(yt|Y1:t−1, X; θ) = softmax(EyLt ), (1)\nwhere, yLt is the representation from the final decoder layer L, E ∈ R|V |×h the embedding matrix and θ the model parameters. Parameters are trained\nby minimizing cross-entropy at each decoding step:\nLMLE(θ) = − 1\nm m∑ i=1 log p(ŷt|Ŷ1:t−1, X; θ),\nwhere, Ŷ1:m is the human-written summary.\nFocus Attention MEchansim (FAME) It is challenging for a decoder to obtain all relevant information from the conditional representation yLt to learn the vocabulary output logits such that predictions yt are consistent with the input. Other modeling factors, specifically the decoder language model, can overwhelm model predictions. FAME (Figure 2) addresses this by introducing a short-circuit from the source to the vocabulary output logits via a source-conditioned bias on vocabulary items.\nWe take the encoder representation X = x1, . . . ,xn and learn a Token-level Vocabulary Distribution txi = gelu(xiW1)W2E ∈ R|V |, for each token xi in the input sequence X . txi measures the contextual similarity of the input token xi to the tokens in the vocabulary; W1 ∈ Rh×h ′ and W2 ∈ Rh ′×h are parameters of newly introduced dense layers, h′ is the intermediate filter size. We define a Source-conditioned Vocabulary Distribution as tX = 1/n ∑n i=1 txi ∈ R|V | as an average of token-level vocabulary distributions for tokens present in the input sequence X , capturing the similarity of X to the tokens in the vocabulary.\nLet aLt ∈ Rn be the encoder-decoder attention distribution over the source tokens for the output token yt and the final decoder layer L. We use aLt to produce a weighted sum of the token-level vocabulary distributions to compute a dynamic vocabulary bias, or Focus Bias ft = ∑n i=1 a L t,itxi ∈ R|V | at decoding step t. We modify the probability of predicting the next token yt from a vocabulary V as:\np(yt|Y1:t−1, X; θ) = softmax(yLt E + ft) (2)\nWe call this Focused Probability Distribution, and it modifies the output logits dynamically to put more focus on those tokens in the vocabulary which are similar to the attended tokens in X . The focus bias introduces a human-inspired control to the model where we do not generate the output in a fully abstractive manner (as in Eq. (1)), but we proactively generate output tokens that are similar to the input tokens (as in Eq. (2)).\nSummary-induced Topic Focused Distribution We aim to guide our focus bias ft to be a better\nrepresentative of the topical content relevant for the task. We achieve this by using the human-written summary Ŷ as a proxy for the topical content of the input and impose the following prior on the source-conditioned vocabulary distribution tX :\nLTopic(θ) = − 1\n|V | |V |∑ i=1 ([vi ∈ Ŷ ] log(σ(tX,i))\n+ [vi /∈ Ŷ ] log(1− σ(tX,i))).(3)\nWe further refine Eq. (3) by replacing Ŷ with Ŷc = Ŷ −F , where F is a set of |F |most frequent tokens in the vocabulary,2 to improve focus on content words. Our final loss function is then\nL = λLMLE + (1− λ)LTopic, (4)\nwhere, λ is an hyper parameter.3\nBy enforcing tX to be a topic distribution for the inputX , we encourage the focus bias ft to promote topically relevant tokens, and subsequently generate topically consistent outputs. Importantly, our focus bias with target-induced topic distribution is task-agnostic and less vulnerable to reference divergence issues (Dhingra et al., 2019; Maynez et al., 2020), and can learn any property embodied in the target relevant for the task. For example, depending on the task, ft can learn to favour input tokens (e.g., for mostly extractive summaries) or new tokens (e.g., for mostly abstractive summaries). This is in sharp contrast to models that introduce task-specific priors, e.g., the pointer-generator network (See et al., 2017) that can copy words from the source text, but does not do well on extreme summarization which is highly abstractive in nature (Narayan et al., 2018).\nFocus Sampling: Promoting Diversity in Faithful Generation We introduce Focus Sampling with FAME to construct a subset Vk ⊆ V by sampling k tokens from the topic distribution tX (Focussample,k). Then, we modify Eq. (2) as\np(yt|Y1:t−1, X; θ) ={ softmax(yLt E + ft)i if vi ∈ Vk ∪ F 0, otherwise. (5)\nFor document summarization, the subset Vk will capture topically salient tokens necessary to generate a summary; F is always added to Vk to ensure\n2which are usually articles or other function words. 3λ is set to 0.5 for all experiments.\nthat the model has access to function words. By tuning the parameters of sampling, we can enforce the model to control the faithfulness or diversity of the outputs.\nFocus sampling has similarities to top-k (Divtop,k; Fan et al., 2018) and nucleus sampling (Divnucleus; Holtzman et al., 2020); in that they all aim to promote diversity. At each decoding step, the top-k sampling diversifies the generation process by sampling a token from the top k tokens in the final output distribution. Similarly, nucleus sampling samples from a dynamic nucleus of tokens containing the vast majority (with a cumulative probability p) of the probability distribution. Both top-k and nucleus sampling shorten the tail of the output distribution at each decoding step, whereas focus sampling constrains the decoder to use a fixed and topically relevant vocabulary Vk. Unlike the other two techniques, Focussample,k can also benefit from standard beam search decoding, leading to superior generation that is not only diverse, but also consistent with the input document."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "In this section we present our experimental setup to assess the ability of our FAME models to generate faithful summaries and to demonstrate that focus sampling is more effective in generating diverse and faithful summaries than other sampling-based decoding methods."
    }, {
      "heading" : "4.1 Extreme Summarization",
      "text" : "We evaluate FAME models on extreme document summarization (XSUM; Narayan et al., 2018). The XSUM summaries, are extreme in that the documents are summarized into single-sentence summaries. These summaries demonstrate a high level of abstractiveness, and generating them automatically requires document-level inference, abstraction, and paraphrasing. Due to their extreme nature, XSUM summaries are ideal to evaluate FAME models’ ability to capture the theme of the document.4 We use on the original cased version consisting of 204,045/11,332/11,334 training/validation/test document-summary pairs. During training, the input documents are truncated to 512 tokens. The\n4We further experiment with long-form story highlight generation (CNN/DM; Hermann et al., 2015) and two text editing tasks: Sentence Fusion (Geva et al., 2019) and Sentence Splitting (Botha et al., 2018). Their results can be found in Appendix B and C. Our FAME models achieve SOTA on both text-editing tasks.\nlength of the summaries are limited to 64."
    }, {
      "heading" : "4.2 Pretrained Models with FAME",
      "text" : "We introduce FAME to two popular seq2seq architectures: RoBERTa initialized seq2seq (ROBERTAS2S, Rothe et al., 2020) and PEGASUS (Zhang et al., 2019). We refer ROBERTAS2S models with FAME as ROBFAME and PEGASUS with FAME with PEGFAME.\nWe experiment with ROBERTAS2S-Large with shared encoder and decoder; it has 24 layers, a hidden size of 1024, filter size of 4096, 16 attention heads, and a vocabulary with 50K sentence pieces (Kudo and Richardson, 2018). ROBERTAS2S has around 455M parameters and ROBFAME has an additional 8M parameters.\nThe best-performing PEGASUS model from Zhang et al. (2019) is not directly comparable with ROBERTAS2S. It does not share the encoder and decoder, it only has 16 layers, a hidden size of 1024, filter size of 4096, 16 attention heads, with a total of 568M parameters, and it also uses a much larger vocabulary with 91K sentence pieces. Hence, we trained our own PEGASUS model. We use the same architecture as ROBERTAS2S and pretrain it on a mixture of C4 (Raffel et al., 2019) and HugeNews (Zhang et al., 2019) datasets with the original objective of generating salient GAP-sentences.\nOur experiments focus on this newly trained PEGASUS model which has same number of parameters and vocabulary as ROBERTAS2S. But in contrast to ROBERTAS2S, the encoder-decoder attention in PEGASUS is pretrained. This allows us to analyse how focus attention affects pretrained (PEGASUS) vs randomly-initialized (ROBERTAS2S) encoder-decoder attentions.5"
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "Lexical Overlap We report ROUGE F1 scores (Lin and Hovy, 2003) against reference summaries; in particular, we report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L for fluency.6\nSemantic Similarity We report BERTScore (Zhang et al., 2020) which computes the contextual similarity between a candidate and its reference summary.\n5See Appendix A for implementation details and hyperparameter settings.\n6We lowercased candidate and reference summaries and used pyrouge with parameters “-a -c 95 -m -n 4 -w 1.2.”\nFaithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020). Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020). Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kryściński et al., 2019; Sellam et al., 2020; Rei et al., 2020).\nWe evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the MultiNLI dataset (Williams et al., 2018). For Feqa, we use a fine-tuned BART (Lewis et al., 2019) language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD (Rajpurkar et al., 2018) to answer the generated questions with input document as context.7\nIn addition to ent. and Feqa, we train a scorer leveraging manually annotated document-summary pairs for faithfulness, as a surrogate for human evaluation and call this metric BERTFaithful.8 In particular, we finetune a BERT-Base classi-\n7We used the Feqa code available here: https:// github.com/esdurmus/feqa/.\n8A very similar scorer was used in the GEM benchmark (Gehrmann et al., 2021) to identify and extract the subset with faithful reference summaries from the XSum dataset (Narayan et al., 2018).\nfier on 500 manually annotated document and gold summary pairs for the XSum dataset from Maynez et al. (2020) to predict whether a summary is faithful to the input document or not.9 We report the percentage of summaries that were faithful ( 1N ∑ i 1[pi(faithful) > 0.5]) and the model’s confidence to generate faithful summaries ( 1N ∑ i pi(faithful)); N is the total number of examples in the test set.\nDiversity We report the number of times (out of n), a model is able to generate a completely new summary (Unique), and Distinct-N (Li et al., 2016a), measuring the lexical diversity in the generated summaries. Distinct-N is estimated as the number of distinct n-grams of order n divided by the total number of n-grams of the same order, in all generated summaries.\nFinally, we also report the average length of summaries (Len.), repetition errors (Rep., estimated as the percentage of summaries with at least one repetition of rare or content words), and ROUGE-1 precision against the input document (R1, P%), to better understand their quality."
    }, {
      "heading" : "5 Results",
      "text" : "FAME Summaries are More Fluent, Informative and Faithful. Table 1 presents results comparing our FAME models, ROBFAME and PEGFAME, against their counterparts ROBERTAS2S\n9Out of 500, 90% of the document-summary pairs were used for training and the rest 50 document-summary pairs were used for validation. We used the validation set to estimate Spearman’s correlation coefficients of different metrics with the human assessment for faithfulness. We found that both entailment scores (ent.) and BERTFaithful are moderately correlated with faithfulness with correlation coefficients of 0.4387 and 0.3889, respectively. As such, we believe that BERTFaithful works as an efficient proxy for expensive human evaluation for faithfulness for XSum summaries. More work is needed to understand if BERTFaithful generalizes to other datasets.\nand PEGASUS, respectively. Both FAME models clearly outperform their vanilla counterparts in terms of generating summaries that are more fluent (see RL and Rep.), more informative (see R1, R2 and BERTSc.) and more faithful (see ent., Feqa and BERTFaithful). Among all four models, PEGFAME summaries are most fluent, informative and faithful.\nWe further did pairwise comparisons for all measures in Table 1 and found that all differences are statistically significant except for BERTScore and faithfulness measures between PEGASUS and PEGFAME.10 These assessments demonstrate that FAME models aid both ROBERTAS2S and PEGASUS in generating fluent, faithful and relevant summaries, but are more effective in ROBERTAS2S than in PEGASUS for extreme summarization.\nGenerating Diverse and Faithful Summaries with Focus Sampling. Table 2 presents results assessing focus sampling (Focussample,k), top-k sampling (Divtop,k) and nucleus sampling (Divnucleus), for their abilities to generate diverse and faithful summaries. For Focussample,k, we choose k = 10, 000. We follow Holtzman et al. (2020) and choose k = 640 and the nucleus probability p = 0.95, for Divtop,k and Divnucleus, respectively. For Focussample,k, we decode with a beam size of 4. We also report Focussample,k with Divtop,k and Divnucleus to assess if they can benefit one-another. In each setting we sample 10 sum-\n10All significance tests in this work are pairwise comparisons (one-way ANOVA with posthoc Tukey HSD tests; p < 0.01).\nmaries for each input document. For all metrics, we report the average over all 10 samples.11\nBoth Divtop,k and Divnucleus almost always generate a new summary. In comparison Focussample,k generates 1.61 and 2.77 unique summaries using ROBFAME and PEGFAME models, respectively. Divnucleus tends to generate the most distinct unigrams, bigrams, and trigrams. Interestingly, Focussample,k summaries have a more diverse collection of unigrams than in Divtop,k summaries (3.5% vs 2.3% for ROBFAME and 2.4% vs 1.9% for PEGFAME).\nThe high diversity in Divtop,k and Divnucleus comes at the cost of faithfulness; summaries generated with these sampling techniques have poor entailment scores. Focussample,k, on the other hand, generates summaries which entail documents the most. It also has the highest ROUGE scores across the board. Some of the generated examples can be seen in Figure 1. More predictions from other models can be found in Appendix E. Augmenting Divtop,k and Divnucleus with Focussample,k is not desirable because, though it increases diversity in terms of uniqueness and Distinct-3 scores, faithfulness suffers again.\nComparing results in Table 2 to the results in Table 1, it is clear that diversity comes at the cost of quality (e.g., RL/ent. scores for ROBFAME and ROBFAME-Focussample,k are 34.81/41.3 and 31.0/34.3, respectively). However, Focussample,k is superior to both Divtop,k and Divnucleus in gen-\n11Feqa and BERTFaithful scores are dropped due to time constraints.\nerating better quality summaries.\nFocus Attention and Sampling Work Differently in ROBFAME and PEGFAME. Since both encoder-decoder and focus attention parameters of ROBFAME are randomly initialized, they learn to compliment each other and learn a peaky topic distribution. On the other hand, since PEGFAME’s encoder-decoder attention is pre-trained, there is a push-pull effect between it and focus attention. This results in a smoother topic distribution, as seen in Figure 3.12\nAlthough we see that both models’ token sets capture the target intent well, the peaky distribu-\n12This difference in topic distributions is consistent across the whole test set. We compute the peakiness score of a topic distribution as the slope of the line connecting logits of the top-1st token to the top-100th token. The average peakiness scores across the XSUM testset for ROBFAME and PEGFAME are 1.25 (51◦) and 0.45 (24.3◦), respectively.\ntion of ROBFAME enables more accurate predictions than that of PEGFAME, in a controlled generation setting. A comparison is presented in Figure 4 where we show how ROUGE-1 scores vary when we use only top-k tokens from tX for generation.13 We observe that ROBFAME consistently outperforms PEGFAME with the lower values of k ∈ {50, 100, 200, 500, 1000}.\nFurther, we observe that ROBFAME generates fewer unique summaries (1.61 vs 2.77) but has higher Distinct-N scores (3.5/22.4/43.9 vs 2.4/16.5/34.2) than PEGFAME, with Focussample,k in Table 2. This can be again be attributed to how FAME works differently in ROBFAME and PEGFAME. When Vk is sampled from ROBFAME’s peaky distribution, the beam search decoding often tends to generate similar summaries (leading to a lower Uniqueness score) as the sampled Vks do not diverge by much from each other. But when it does diverge, the decoder tends to generate completely new summaries (leading to higher DistinctN scores).\nCurrently, we set k = 10, 000 for our focus sampling experiments following our observations in Figure 4. Future work will focus on how to better leverage trade-off between diversity and faithfulness by controlling the peakiness of the topic distribution tX .\nAblations and SOTA Comparisons We emphasize that FAME or focus sampling does not aim to improve on state-of-the-results in terms of ROUGE, but to generate more faithful or diverse summaries\n13Additional results and model predictions for these experiments can be found in Appendix D.\nwhile maintaining their quality. For completeness, we compare our ROBFAME and PEGFAME models to their ablations and other state-of-the-art models on XSUM in Table 3.\nWe report ROUGE scores for FAME in the ideal scenario (ORACLE) where it focuses on all the correct tokens in the input, i.e., the topic distribution tX is identical to the distribution observed in the reference summary. These models generate summaries with very high ROUGE scores when the model is given the correct tokens to focus on. The gap between the ORACLE and FAME scores suggests that there is still a lot of work to be done in this space. Focus attention without any topical supervision (models w/o Eq. (3)) is not significantly better than the baselines. But ROBFAME and PEGFAME (trained with joint supervision in Eq. (4)) significantly outperform ROBERTAS2S and PEGASUS, respectively.\nOur best model PEGFAME performs better than PtGen (See et al., 2017), ConvS2S (Narayan et al., 2018), MMN (Kim et al., 2019), MASS (Song et al., 2019) and BART (Lewis et al., 2019), but worse when the original PEGASUS (Zhang et al., 2019). This can be expected as the number of parameters in PEGFAME is far less than that in the original PEGASUS."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced FAME, a new attention mechanism which dynamically biases the decoder to proactively generate tokens that are topically similar to the input. FAME enhances the faithfulness of existing state-of-the-art abstract summarization models while improving their overall ROUGE scores. Finally, our newly introduced focus sampling technique is a better alternative to top-k or nucleus sampling to generate diverse set of faithful summaries."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Sebastian Gehrmann, Slav Petrov, the reviewers, and the action editor for their invaluable feedback.\nEthical Considerations\nThe nature of text generation leads to multiple ethical considerations when applied to applications. The main failure mode is that the model can learn to mimic target properties in the training data that are not desirable.\nFaithfulness and Factuality Since models create new text, there is the danger that they may neither be faithful to the source material nor factual. This can be exacerbated when the data itself has highly abstractive targets, which require the model to generate words not seen in the source material during training. This often leads the model to generate content inconsistent with the source material (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).\nTrustworthy Data If the data itself is not trustworthy (comes from suspect or malicious sources) the model itself will naturally become untrustworthy as it will ultimately learn the language and topics of the training data. For instance, if the training data is about Obama birther conspiracies, and the model is asked to generate information about the early life of Obama, there is a risk that such false claims will be predicted by the model.\nBias in Data Similarly, biases in the data around gender, race, etc., risk being propagated in the model predictions, which is common for most NLP tasks. This is especially true when the models are trained from non-contemporary data that do not represent current norms and practices (Blodgett et al., 2020).\nThe above considerations are non-malicious, in that the model is merely learning to behave as its underlying source material. If users of such models are not aware of these issues and do not account for them, e.g., with better data selection, evaluation, etc., then the generated text can be damaging.\nGeneration models can also be misused in malicious ways. These include generating fake news, spam, and other text meant to mislead large parts of the general population."
    }, {
      "heading" : "B Abstractive Summarization Results on CNN/DailyMail",
      "text" : "The CNN/DM dataset (Hermann et al., 2015) consists of 287,227/13,368/11,490 train-\ning/validation/test document-summary pairs. The CNN/DM summaries are in the form of bullet-point story highlights and exhibit a high degree of extraction, requiring the models to learn to copy from the source documents. The XSUM summaries, on the other hand, are extreme, in that the documents are summarized into single-sentence summaries with a high level of abstractiveness. For comparison, the XSUM summaries show a much larger percentages of novel constructions than found in CNN/DM summaries (35.8/83.5/95.5/98.5 vs 16.8/54.3/72.4/80.4 novel 1/2/3/4-grams). We use the original cased version. During training, the input documents are truncated to 512 tokens and the length of the summaries are limited to 128 tokens.\nTable 4 and 5 present complete results for CNN/DM dataset. We see similar kind of improvements as observed in Table 1, except for ROUGE-2 for ROBFAME which is 0.23 points worse than the ROBERTAS2S baseline. Our best model PEGFAME performs better than both copy mechanism models: LSTM-based PtGen (See et al., 2017) and Transformer-based SAGCopy (Xu et al., 2020). PEGFAME performs worse when compared with T5 (Raffel et al., 2019), the original PEGASUS (Zhang et al., 2019) and ProphetNet (Qi et al., 2020). This can be expected as the number of parameters in PEGFAME is almost half of T5 or ProphetNet, and is 100M less than that in the original PEGASUS.\nROBFAME performs worse than ROBERTAS2S on both ent. and Feqa measures for CNN/DM, similar to ROUGE-2 in Table 4. We hypothesize that this is due to the extractive nature of the CNN/DM dataset and the fact that it is not able to copy to-\nkens from the input to the necessary extent as the encoder-decoder attention is not pre-trained. Moreover, Feqa scores for ROBERTAS2S and ROBFAME may not be fully comparable due to variation in their summary lengths and the number of Feqa questions generated; the ROBFAME summaries, on average, are 3 words longer and generate 1.2 more questions than that of ROBERTAS2S. Nevertheless, we don’t see this kind of drop in ¬cont. scores (i.e., summary not contradicting, either entailed by or neutral to the document) and BERTScores."
    }, {
      "heading" : "C Text Editing Results",
      "text" : "We also train the FAME models on two text editing tasks: (i) for sentence fusion – the problem of combining multiple sentences into a single coherent sentence – we used the “balanced Wikipedia” portion of the DiscoFuse dataset (Geva et al., 2019), and (ii) for split-and-rephrase – the reverse task of sentence fusion – we used the WikiSplit dataset (Botha et al., 2018), which consists of 1M examples of sentence splits extracted from the Wikipedia edit history. As the name suggests, both text editing tasks require a low degree of abstraction.\nFor both the tasks, we train the models for 300k steps with a global batch size of 256. The input and output are padded to a length of 128, which covers 100% of the training, evaluation and test data. The vocabulary for functional tokens F is constructed by taking the top 100 and 500 sentence pieces for DiscoFuse and WikiSplit respectively.\nWe report corpus-level BLEU14, the exact match accuracy, and SARI scores (Xu et al., 2016)15. The results can be seen in Table 6. The vanilla PEGASUS model already beats the current state-of-the-art on both DiscoFuse and WikiSplit. The PEGFAME\n14We use NLTK v3.2.2 with case sensitive scoring to estimate BLEU scores.\n15SARI is a lexical similarity metric which compares the model’s output to multiple references and the input in order to assess the model’s ability to add, delete, and keep an n-gram. It’s implementation is available at: https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ utils/sari_hook.py.\nmodel performs better, albeit by a small margin, on all metrics on DiscoFuse. On WikiSplit, it has a higher exact match accuracy while maintaining the SARI score and performs 0.1 BLEU worse than PEGASUS."
    }, {
      "heading" : "D Controlled Generation with focus",
      "text" : "attention using Top-k tokens\nTable 7 presents results from our controlled summary generation experiments with top-k tokens from tX using focus attention (Focustop,k) on the XSUM test set. In Figures 3 and 4, we describe how ROBFAME consistently outperforms PEGFAME at lower values of k ∈ {50, 100, 200, 500, 1000} due to their peaky and smooth tX , respectively. While Figure 4 only plots ROUGE-1 F1 scores, Table 7 additionally reports ROUGE-2, ROUGE-L, entailment, Feqa, and BERTScores. Figure 6 presents predictions from models using Focustop,k for the article presented in Figures 1 and 5.\nE Diverse Summarization with Divtop,k, Divnucleus and Focussample,k\nFigures 7 show the diverse summaries generated using Focussample,k for the article shown in Figure 5. The predictions from Divtop,k and Divnucleus are omitted due to the prescribed limit on the number of pages allowed for the Appendix. Please find them on the arXiv version at https://arxiv.org/ abs/2105.11921."
    } ],
    "references" : [ {
      "title" : "Topic augmented generator for abstractive summarization",
      "author" : [ "Melissa Ailem", "Bowen Zhang", "Fei Sha." ],
      "venue" : "CoRR, abs/1908.07026.",
      "citeRegEx" : "Ailem et al\\.,? 2019",
      "shortCiteRegEx" : "Ailem et al\\.",
      "year" : 2019
    }, {
      "title" : "Guiding extractive summarization with question-answering rewards",
      "author" : [ "Kristjan Arumae", "Fei Liu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Arumae and Liu.,? 2019",
      "shortCiteRegEx" : "Arumae and Liu.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Using lexical chains for text summarization",
      "author" : [ "Regina Barzilay", "Michael Elhadad." ],
      "venue" : "Intelligent Scalable Text Summarization.",
      "citeRegEx" : "Barzilay and Elhadad.,? 1997",
      "shortCiteRegEx" : "Barzilay and Elhadad.",
      "year" : 1997
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "The Journal of Machine Learning Research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Language (technology) is power: A critical survey of “bias” in NLP",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to split and rephrase from Wikipedia edit history",
      "author" : [ "Jan A. Botha", "Manaal Faruqui", "John Alex", "Jason Baldridge", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Botha et al\\.,? 2018",
      "shortCiteRegEx" : "Botha et al\\.",
      "year" : 2018
    }, {
      "title" : "DivGAN: Towards diverse paraphrase generation via diversified generative adversarial network",
      "author" : [ "Yue Cao", "Xiaojun Wan." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2411–2421, Online. Association for Computa-",
      "citeRegEx" : "Cao and Wan.,? 2020",
      "shortCiteRegEx" : "Cao and Wan.",
      "year" : 2020
    }, {
      "title" : "Faithful to the original: Fact aware neural abstractive summarization",
      "author" : [ "Ziqiang Cao", "Furu Wei", "Wenjie Li", "Sujian Li" ],
      "venue" : null,
      "citeRegEx" : "Cao et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2017
    }, {
      "title" : "Mixture content selection for diverse sequence generation",
      "author" : [ "Jaemin Cho", "Minjoon Seo", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Cho et al\\.,? 2019",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2019
    }, {
      "title" : "Fˆ2-softmax: Diversifying neural text generation via frequency factorized softmax",
      "author" : [ "Byung-Ju Choi", "Jimin Hong", "David Park", "Sang Wan Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards diverse and natural image descriptions via a conditional GAN",
      "author" : [ "Bo Dai", "Dahua Lin", "Raquel Urtasun", "Sanja Fidler." ],
      "venue" : "CoRR, abs/1703.06029.",
      "citeRegEx" : "Dai et al\\.,? 2017",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Handling divergent reference texts when evaluating table-to-text generation",
      "author" : [ "Bhuwan Dhingra", "Manaal Faruqui", "Ankur Parikh", "Ming-Wei Chang", "Dipanjan Das", "William Cohen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Dhingra et al\\.,? 2019",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2019
    }, {
      "title" : "TopicRNN: A recurrent neural network with long-range semantic dependency",
      "author" : [ "Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations, Toulon, France.",
      "citeRegEx" : "Dieng et al\\.,? 2017",
      "shortCiteRegEx" : "Dieng et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to paraphrase for question answering",
      "author" : [ "Li Dong", "Jonathan Mallinson", "Siva Reddy", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 875–886, Copenhagen, Denmark. Associ-",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "H. Wallach, H. Larochelle,",
      "citeRegEx" : "Dong et al\\.,? 2019a",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "author" : [ "Yue Dong", "Zichao Li", "Mehdi Rezagholizadeh", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Dong et al\\.,? 2019b",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "author" : [ "Esin Durmus", "He He", "Mona Diab." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–",
      "citeRegEx" : "Durmus et al\\.,? 2020",
      "shortCiteRegEx" : "Durmus et al\\.",
      "year" : 2020
    }, {
      "title" : "Augmenting neural response generation with context-aware topical attention",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of the First Workshop on NLP for Conversational AI, pages 18–31, Florence, Italy. Associ-",
      "citeRegEx" : "Dziri et al\\.,? 2019",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
      "author" : [ "Tobias Falke", "Leonardo F.R. Ribeiro", "Prasetya Ajie Utama", "Ido Dagan", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 57th Annual",
      "citeRegEx" : "Falke et al\\.,? 2019",
      "shortCiteRegEx" : "Falke et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "WordNet: an electronic lexical database",
      "author" : [ "Christiane Fellbaum", "editor" ],
      "venue" : null,
      "citeRegEx" : "Fellbaum and editor.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fellbaum and editor.",
      "year" : 1998
    }, {
      "title" : "BLEU might be guilty but references are not innocent",
      "author" : [ "Markus Freitag", "David Grangier", "Isaac Caswell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 61–71, Online. Association for",
      "citeRegEx" : "Freitag et al\\.,? 2020",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2020
    }, {
      "title" : "Go figure! a meta evaluation of factuality in summarization",
      "author" : [ "Saadia Gabriel", "Asli Celikyilmaz", "Rahul Jha", "Yejin Choi", "Jianfeng Gao" ],
      "venue" : null,
      "citeRegEx" : "Gabriel et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gabriel et al\\.",
      "year" : 2020
    }, {
      "title" : "The GEM benchmark: Natural language generation, its evaluation and metrics. CoRR, abs/2102.01672",
      "author" : [ "ramani", "Wei Xu", "Diyi Yang", "Akhila Yerukola", "Jiawei Zhou" ],
      "venue" : null,
      "citeRegEx" : "ramani et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "ramani et al\\.",
      "year" : 2021
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium. Association",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "DiscoFuse: A large-scale dataset for discourse-based sentence fusion",
      "author" : [ "Mor Geva", "Eric Malmi", "Idan Szpektor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual LSTM (CLSTM) models for large scale NLP tasks",
      "author" : [ "Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck." ],
      "venue" : "CoRR, abs/1602.06291.",
      "citeRegEx" : "Ghosh et al\\.,? 2016",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Unifying human and statistical evaluation for natural language generation",
      "author" : [ "Tatsunori Hashimoto", "Hugh Zhang", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Hashimoto et al\\.,? 2019",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "TILM: Neural language models with evolving topical influence",
      "author" : [ "Shubhra Kanti Karmaker Santu", "Kalyan Veeramachaneni", "Chengxiang Zhai." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages",
      "citeRegEx" : "Santu et al\\.,? 2019",
      "shortCiteRegEx" : "Santu et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive summarization of Reddit posts with multi-level memory networks",
      "author" : [ "Byeongchang Kim", "Hyunwoo Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1910.12840.",
      "citeRegEx" : "Kryscinski et al\\.,? 2019",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryściński", "Bryan McCann", "Caiming Xiong", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Kryściński et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kryściński et al\\.",
      "year" : 2019
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Importance of search and evaluation strategies in neural dialogue modeling",
      "author" : [ "Ilia Kulikov", "Alexander Miller", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 76–87, Tokyo,",
      "citeRegEx" : "Kulikov et al\\.,? 2019",
      "shortCiteRegEx" : "Kulikov et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple, fast diverse decoding algorithm for neural generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "CoRR, abs/1611.08562.",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "The automated acquisition of topic signatures for text summarization",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "COLING 2000 Volume 1: The 18th International Conference on Computational Linguistics.",
      "citeRegEx" : "Lin and Hovy.,? 2000",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2000
    }, {
      "title" : "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "author" : [ "Chin Yew Lin", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Felix: Flexible text editing through tagging and insertion",
      "author" : [ "Jonathan Mallinson", "Aliaksei Severyn", "Eric Malmi", "Guillermo Garrido" ],
      "venue" : null,
      "citeRegEx" : "Mallinson et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mallinson et al\\.",
      "year" : 2020
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic summarization, volume 3",
      "author" : [ "Inderjeet Mani." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Mani.,? 2001",
      "shortCiteRegEx" : "Mani.",
      "year" : 2001
    }, {
      "title" : "Sparse text generation",
      "author" : [ "Pedro Henrique Martins", "Zita Marinho", "André F.T. Martins." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4252–4273, Online. Association for Computational",
      "citeRegEx" : "Martins et al\\.,? 2020",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2020
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On-",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Geoffrey Zweig." ],
      "venue" : "Proceedings of the Spoken Language Technology Workshop, pages 234–239. IEEE.",
      "citeRegEx" : "Mikolov and Zweig.,? 2012",
      "shortCiteRegEx" : "Mikolov and Zweig.",
      "year" : 2012
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Paraphrase generation from latent-variable PCFGs for semantic parsing",
      "author" : [ "Shashi Narayan", "Siva Reddy", "Shay B. Cohen." ],
      "venue" : "Proceedings of the 9th International Natural Language Generation conference, pages 153–162, Edinburgh, UK. Association",
      "citeRegEx" : "Narayan et al\\.,? 2016",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic summarization",
      "author" : [ "Ani Nenkova", "Kathleen McKeown." ],
      "venue" : "Foundations and Trends in Information Retrieval, 5(2–3):103–233.",
      "citeRegEx" : "Nenkova and McKeown.,? 2011",
      "shortCiteRegEx" : "Nenkova and McKeown.",
      "year" : 2011
    }, {
      "title" : "Multireward reinforced summarization with saliency and entailment",
      "author" : [ "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Pasunuru and Bansal.,? 2018",
      "shortCiteRegEx" : "Pasunuru and Bansal.",
      "year" : 2018
    }, {
      "title" : "ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training",
      "author" : [ "Weizhen Qi", "Yu Yan", "Yeyun Gong", "Dayiheng Liu", "Nan Duan", "Jiusheng Chen", "Ruofei Zhang", "Ming Zhou." ],
      "venue" : "Findings of the Association for Computational Linguistics:",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "Technical report, OpenAI.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1901.07291.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 784–789, Melbourne, Australia. As-",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Associa-",
      "citeRegEx" : "Rei et al\\.,? 2020",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation tasks",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:264–280.",
      "citeRegEx" : "Rothe et al\\.,? 2020",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "this is a problem, don’t you agree?” framing and bias in human evaluation for natural language generation",
      "author" : [ "Stephanie Schoch", "Diyi Yang", "Yangfeng Ji." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 10–16, Online (Dublin,",
      "citeRegEx" : "Schoch et al\\.,? 2020",
      "shortCiteRegEx" : "Schoch et al\\.",
      "year" : 2020
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073–1083, Vancouver, Canada.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint parsing and generation for abstractive summarization",
      "author" : [ "Kaiqiang Song", "Logan Lebanoff", "Q. Guo", "Xipeng Qiu", "X. Xue", "Chen Li", "Dong Yu", "Fei Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "MASS: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 5926–5936.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "On the importance of diversity in question generation for QA",
      "author" : [ "Md Arafat Sultan", "Shubham Chandel", "Ramón Fernandez Astudillo", "Vittorio Castelli." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Sultan et al\\.,? 2020",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2020
    }, {
      "title" : "Sticking to the facts: Confident decoding for faithful data-to-text generation",
      "author" : [ "Ran Tian", "Shashi Narayan", "Thibault Sellam", "Ankur P. Parikh" ],
      "venue" : null,
      "citeRegEx" : "Tian et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diverse beam search for improved description of complex scenes",
      "author" : [ "Ashwin K. Vijayakumar", "Michael Cogswell", "Ramprasaath R. Selvaraju", "Qing Sun", "Stefan Lee", "David J. Crandall", "Dhruv Batra." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Confer-",
      "citeRegEx" : "Vijayakumar et al\\.,? 2018",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Asking and answering questions to evaluate the factual consistency of summaries",
      "author" : [ "Alex Wang", "Kyunghyun Cho", "Mike Lewis." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020, Online.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Diversify question generation with continuous content selectors and question type modeling",
      "author" : [ "Zhen Wang", "Siwei Rao", "Jie Zhang", "Zhen Qin", "Guangjian Tian", "Jun Wang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Friendly topic assistant for transformer based abstractive summarization",
      "author" : [ "Zhengjue Wang", "Zhibin Duan", "Hao Zhang", "Chaojie Wang", "Long Tian", "Bo Chen", "Mingyuan Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Wang et al\\.,? 2020c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards faithful neural table-to-text generation with content-matching",
      "author" : [ "Zhenyi Wang", "Xiaoyang Wang", "Bang An", "Dong Yu", "Changyou Chen" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "CoRR, abs/1908.04319.",
      "citeRegEx" : "Welleck et al\\.,? 2019a",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "Dialogue natural language inference",
      "author" : [ "Sean Welleck", "Jason Weston", "Arthur Szlam", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731–3741, Florence, Italy.",
      "citeRegEx" : "Welleck et al\\.,? 2019b",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Topic aware neural response generation",
      "author" : [ "Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page 3351–3357. AAAI Press.",
      "citeRegEx" : "Xing et al\\.,? 2017",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2017
    }, {
      "title" : "Diversity-promoting GAN: A crossentropy based generative adversarial network for diversified text generation",
      "author" : [ "Jingjing Xu", "Xuancheng Ren", "Junyang Lin", "Xu Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-attention guided copy mechanism for abstractive summarization",
      "author" : [ "Song Xu", "Haoran Li", "Peng Yuan", "Youzheng Wu", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:401–415.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "CoRR, abs/1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "BERTScore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "Proceedings of the 8th International Conference on Learning Representations, Virtual Conference, Formerly",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Beta-negative binomial process and poisson factor analysis",
      "author" : [ "Mingyuan Zhou", "Lauren Hannah", "David Dunson", "Lawrence Carin." ],
      "venue" : "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, volume 22 of Proceed-",
      "citeRegEx" : "Zhou et al\\.,? 2012",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    }, {
      "title" : "Selective encoding for abstractive sentence summarization",
      "author" : [ "Qingyu Zhou", "Nan Yang", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1095–1104,",
      "citeRegEx" : "Zhou et al\\.,? 2017",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "2018), which consists of 1M examples of sentence splits extracted from the Wikipedia edit history. As the name suggests, both text editing tasks require a low degree of abstraction",
      "author" : [ "Botha" ],
      "venue" : null,
      "citeRegEx" : "Botha,? \\Q2018\\E",
      "shortCiteRegEx" : "Botha",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 49,
      "context" : "Document summarization — producing the shorter version of a document while preserving salient information (Mani, 2001; Nenkova and McKeown, 2011) — is challenging even for humans.",
      "startOffset" : 106,
      "endOffset" : 145
    }, {
      "referenceID" : 55,
      "context" : "Document summarization — producing the shorter version of a document while preserving salient information (Mani, 2001; Nenkova and McKeown, 2011) — is challenging even for humans.",
      "startOffset" : 106,
      "endOffset" : 145
    }, {
      "referenceID" : 32,
      "context" : "This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al.",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al.",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al.",
      "startOffset" : 118,
      "endOffset" : 192
    }, {
      "referenceID" : 70,
      "context" : ", 2016), fully attention-based Transformer architectures (Vaswani et al., 2017), and large pretrained language models (Devlin et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 51,
      "context" : "For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 24,
      "context" : "For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 253
    }, {
      "referenceID" : 40,
      "context" : "this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 253
    }, {
      "referenceID" : 23,
      "context" : "this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 253
    }, {
      "referenceID" : 10,
      "context" : "this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 253
    }, {
      "referenceID" : 30,
      "context" : "much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al.",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : ", 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality.",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : ", 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality.",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 53,
      "context" : "FAME promotes summaries faithful to the source When evaluated on the BBC extreme summarization task (XSUM; Narayan et al., 2018), experiments with two state-of-the-art summarizers",
      "startOffset" : 100,
      "endOffset" : 128
    }, {
      "referenceID" : 62,
      "context" : "– ROBERTAS2S (Rothe et al., 2020) and PEGASUS (Zhang et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 84,
      "context" : ", 2020) and PEGASUS (Zhang et al., 2019) – show that both models generate summaries that are more faithful to their input documents when augmented with FAME, in comparison with their vanilla counterparts.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "FAME enables diverse summaries FAME, by design, supports Focus Sampling – a technique that is more effective in sampling topically relevant tokens to generate diverse, yet topically consistent and faithful outputs, than other sampling methods (Fan et al., 2018; Holtzman et al., 2020).",
      "startOffset" : 243,
      "endOffset" : 284
    }, {
      "referenceID" : 33,
      "context" : "FAME enables diverse summaries FAME, by design, supports Focus Sampling – a technique that is more effective in sampling topically relevant tokens to generate diverse, yet topically consistent and faithful outputs, than other sampling methods (Fan et al., 2018; Holtzman et al., 2020).",
      "startOffset" : 243,
      "endOffset" : 284
    }, {
      "referenceID" : 64,
      "context" : "Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing.",
      "startOffset" : 31,
      "endOffset" : 66
    }, {
      "referenceID" : 81,
      "context" : "Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing.",
      "startOffset" : 31,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "(Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 87,
      "context" : ", 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 79,
      "context" : ", 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : ", 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 53,
      "context" : ", 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c).",
      "startOffset" : 50,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : ", 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c).",
      "startOffset" : 50,
      "endOffset" : 112
    }, {
      "referenceID" : 74,
      "context" : ", 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c).",
      "startOffset" : 50,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "(2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 86,
      "context" : "(2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Others avoid text degeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens",
      "startOffset" : 161,
      "endOffset" : 195
    }, {
      "referenceID" : 33,
      "context" : "with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 50,
      "context" : "Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 76,
      "context" : ", 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 54,
      "context" : "For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al.",
      "startOffset" : 83,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al.",
      "startOffset" : 83,
      "endOffset" : 165
    }, {
      "referenceID" : 68,
      "context" : "For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al.",
      "startOffset" : 83,
      "endOffset" : 165
    }, {
      "referenceID" : 73,
      "context" : "For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al.",
      "startOffset" : 83,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "Importantly, our focus bias with target-induced topic distribution is task-agnostic and less vulnerable to reference divergence issues (Dhingra et al., 2019; Maynez et al., 2020), and can learn any property embodied",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 51,
      "context" : "Importantly, our focus bias with target-induced topic distribution is task-agnostic and less vulnerable to reference divergence issues (Dhingra et al., 2019; Maynez et al., 2020), and can learn any property embodied",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 64,
      "context" : ", the pointer-generator network (See et al., 2017) that can copy words from the source text, but does not do well on extreme summarization which is highly abstractive in nature (Narayan et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 53,
      "context" : ", 2017) that can copy words from the source text, but does not do well on extreme summarization which is highly abstractive in nature (Narayan et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "Focus sampling has similarities to top-k (Divtop,k; Fan et al., 2018) and nucleus sampling (Divnucleus; Holtzman et al.",
      "startOffset" : 41,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : ", 2018) and nucleus sampling (Divnucleus; Holtzman et al., 2020); in that they all aim to promote diversity.",
      "startOffset" : 29,
      "endOffset" : 64
    }, {
      "referenceID" : 31,
      "context" : "We further experiment with long-form story highlight generation (CNN/DM; Hermann et al., 2015) and two text editing tasks: Sentence Fusion (Geva et al.",
      "startOffset" : 64,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : ", 2015) and two text editing tasks: Sentence Fusion (Geva et al., 2019) and Sentence Splitting (Botha et al.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "heads, and a vocabulary with 50K sentence pieces (Kudo and Richardson, 2018).",
      "startOffset" : 49,
      "endOffset" : 76
    }, {
      "referenceID" : 59,
      "context" : "on a mixture of C4 (Raffel et al., 2019) and HugeNews (Zhang et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 84,
      "context" : ", 2019) and HugeNews (Zhang et al., 2019) datasets with the original objective of generating salient GAP-sentences.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 45,
      "context" : "Lexical Overlap We report ROUGE F1 scores (Lin and Hovy, 2003) against reference summaries; in particular, we report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L for fluency.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 85,
      "context" : "Semantic Similarity We report BERTScore (Zhang et al., 2020) which computes the contextual similarity between a candidate and its reference",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 51,
      "context" : "Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 63,
      "context" : "But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kryściński et al., 2019; Sellam et al., 2020; Rei et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kryściński et al., 2019; Sellam et al., 2020; Rei et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 141
    }, {
      "referenceID" : 65,
      "context" : "of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kryściński et al., 2019; Sellam et al., 2020; Rei et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 141
    }, {
      "referenceID" : 61,
      "context" : "of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kryściński et al., 2019; Sellam et al., 2020; Rei et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 141
    }, {
      "referenceID" : 56,
      "context" : "metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al.",
      "startOffset" : 35,
      "endOffset" : 130
    }, {
      "referenceID" : 77,
      "context" : "metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al.",
      "startOffset" : 35,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al.",
      "startOffset" : 35,
      "endOffset" : 130
    }, {
      "referenceID" : 36,
      "context" : "metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al.",
      "startOffset" : 35,
      "endOffset" : 130
    }, {
      "referenceID" : 51,
      "context" : ") its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 41,
      "context" : "For Feqa, we use a fine-tuned BART (Lewis et al., 2019) language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD (Rajpurkar et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 60,
      "context" : ", 2019) language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD (Rajpurkar et al., 2018) to answer the generated questions with input document as context.",
      "startOffset" : 130,
      "endOffset" : 154
    }, {
      "referenceID" : 53,
      "context" : ", 2021) to identify and extract the subset with faithful reference summaries from the XSum dataset (Narayan et al., 2018).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : "new summary (Unique), and Distinct-N (Li et al., 2016a), measuring the lexical diversity in the generated summaries.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 64,
      "context" : "Our best model PEGFAME performs better than PtGen (See et al., 2017), ConvS2S (Narayan et al.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 41,
      "context" : ", 2019) and BART (Lewis et al., 2019), but worse when the original PEGASUS (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 84,
      "context" : ", 2019), but worse when the original PEGASUS (Zhang et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 37,
      "context" : "This often leads the model to generate content inconsistent with the source material (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 153
    }, {
      "referenceID" : 51,
      "context" : "This often leads the model to generate content inconsistent with the source material (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 153
    }, {
      "referenceID" : 24,
      "context" : "This often leads the model to generate content inconsistent with the source material (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 153
    } ],
    "year" : 2021,
    "abstractText" : "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus samplingbased decoding methods.",
    "creator" : "LaTeX with hyperref"
  }
}