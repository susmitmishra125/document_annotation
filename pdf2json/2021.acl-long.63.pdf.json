{
  "name" : "2021.acl-long.63.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Discontinuous Named Entity Recognition as Maximal Clique Discovery",
    "authors" : [ "Yucheng Wang", "Bowen Yu", "Hongsong Zhu", "Tingwen Liu", "Nan Yu", "Limin Sun" ],
    "emails" : [ "wangyucheng@iie.ac.cn", "yubowen@iie.ac.cn", "zhuhongsong@iie.ac.cn", "liutingwen@iie.ac.cn", "yunan@iie.ac.cn", "sunlimin@iie.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 764–774\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n764"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is the task of detecting mentions of real-world entities from text and classifying them into predefined types. NER benefits many natural language processing applications (e.g., information retrieval (Berger and Lafferty, 2017), relation extraction (Yu et al., 2019), and question answering (Khalid et al., 2008)).\nNER methods have been extensively investigated and researchers have proposed effective ones. Most prior approaches (Huang et al., 2015; Chiu and\n∗ The two authors contribute equally. † Corresponding author.\n1The source code is available at https://github. com/131250208/InfExtraction\nNichols, 2016; Gridach, 2017; Zhang and Yang, 2018; Gui et al., 2019; Xue et al., 2020) cast this task as a sequence labeling problem where each token is assigned a label that represents its entity type. Their underlying assumption is that an entity mention should be a short span of text (Muis and Lu, 2016), and should not overlap with each other. While such assumption is valid for most cases, it does not always hold, especially in clinical corpus (Pradhan et al., 2015). For example, Figure 1 shows two discontiguous entity mentions with overlapping segments. Thus, there is a need to move beyond continuous entities and devise methods to extract discontinuous ones.\nTowards this goal, current state-of-the-art (SOTA) models can be categorized into two classes: combination-based and transition-based. Combination-based models first detect all the overlapping spans and then learn to combine these segments with a separate classifier (Wang and Lu, 2019); Transition-based models incrementally label the discontinuous spans through a sequence of shift-reduce actions (Dai et al., 2020b). Although these methods have achieved reasonable performance, they continue to have difficulty with the same problem: exposure bias (Zhang et al., 2019). Specifically, combination-based methods use the gold segments to guide the classifier during the training process while at inference the input segments are given by a trained model, leading to a gap between training and inference (Wang and Lu, 2019). For transition-based models, at training time, the current action relies on the golden previ-\nous actions, while in the testing phase, the entire action sequence is generated by the model. As a result, a skewed prediction will further deviate the predictions of the follow-up actions. Such accumulated discrepancy may hurt the performance.\nIn order to overcome the limitation of such prior works, we propose Mac, a Maximal clique discovery based discontinuous NER model. The core insight behind Mac is that all (potentially discontinuous) entity mentions in the sentence can naturally form a segment graph by interpreting their contained continuous segments as nodes, and connecting segments of the same entity to each other as edges. Then the discontinuous NER task is equivalent to finding the maximal cliques from the graph, which is a well-studied problem in graph theory. So, the question that remains is how to construct such a segment graph. We decompose it into two uncoupled subtasks, segment extraction (SE) and edge prediction (EP) in Mac. Typically, given an ntoken sentence, two n×n tag tables are formed for SE and EP respectively where each entry captures the interaction between two individual tokens. SE is then regarded as a labeling problem where tags are assigned to distinguish the boundary tokens of each segment, which have benefits in identifying overlapping segments. EP is converted as the problem of aligning the boundary tokens of segments contained in the same entity. Overall, the tag tables of SE and EP are generated independently, and will be consumed together by a maximum clique searching algorithm to recover desired entities from them, thus immune from the exposure bias problem.\nWe conducted experiments on three standard discontinuous NER benchmarks. Experiments show that Mac can effectively recognize discontinuous entity mentions without sacrificing the accuracy on continuous mentions. This leads to a new state-ofthe-art (SOTA) on this task, with substantial gains of up to 3.5% absolute percentage points over previous best reported result. Lastly, we show that in the runtime experiments on GPU environments, Mac is about five times faster than the SOTA model."
    }, {
      "heading" : "2 Related Work",
      "text" : "Discontinuous NER requires to identify all entity mentions that have discontinuous structures. To achieve this end, several researchers introduced new position indicators into the traditional BIO tagging scheme so that the sequential labeling models can be employed (Tang et al., 2013; Metke-\nJimenez and Karimi, 2016; Dai et al., 2017; Tang et al., 2018). However, this model suffers from the label ambiguity problem due to the limited flexibility of the extended tag set. As the improvement, Muis and Lu (2016) used hyper-graphs to represent entity spans and their combinations, but did not completely resolve the ambiguity issue (Dai et al., 2020b). Wang and Lu (2019) presented a pipeline framework which first detects all the candidate spans of entities and then merges them into entities. By decomposing the task into two interdependency steps, this approach does not have the ambiguity issue, but meanwhile being susceptible to exposure bias. Recently, Dai et al. (2020b) constructed a transition action sequence for recognizing discontinuous and overlapping structure. At training time, it predicts with the ground truth previous actions as condition while at inference it has to select the current action based on the results of previous steps, leading to exposure bias. In this paper, for the first time we propose a one-stage method to address discontinuous NER while without suffering from the ambiguity issue, realizing the consistency of training and inference.\nJoint extraction aims to detect entity pairs along with their relations using a single model (Yu et al., 2020). Discontinuous NER is related to joint extraction where the discontiguous entities can be viewed as relation links between segments (Wang and Lu, 2019). Our model is motivated by TPLinker (Wang et al., 2020), which formulates joint extraction as a token pair linking problem by aligning the boundary tokens of entity pairs. The main differences between our model and TPLinker are two-fold: (1) We propose a tailor-designed tagging scheme for recognizing discontinuous segments; (2) The maximal clique discovery algorithm is introduced into our model to accurately merge the discontinuous segments.\nMaximal clique discovery is to find a clique of maximum size in a given graph (Dutta and Lauri, 2019). Here, a clique is a subset of the vertices all of which are pairwise adjacent. Maximal clique discovery finds extensive application across diverse domains (Stix, 2004; Boginski et al., 2005; Imbiriba et al., 2017). In this paper, we reformulate discontinuous NER as the task of maximal clique discovery by constructing a segment graph and leveraging the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) to find all the maximum cliques as the entities."
    }, {
      "heading" : "3 Methodology",
      "text" : "In graph theory, a clique is a vertex subset of an undirected graph where every two vertices in the clique are adjacent, while a maximal clique is the one that cannot be extended by including one more adjacent vertex. That means each vertex in the maximal clique has close relations with each other, and no other vertex can be added, which is similar to the relations between segments in a discontinuous entity. Based on this insight, we claim that discontinuous NER can be equivalently interpreted as discovering maximal cliques from a segment graph, where nodes represent segments that either form entities on their own or present as parts of a discontinuous entity, and edges connect segments that belong to the same entity mention.\nConsidering the maximum clique searching process is usually non-parametric (Bron and Kerbosch, 1973), discontinuous NER is actually decomposed into two subtasks: segment extraction and edge prediction, to respectively create the nodes and edges of the segment graph. Their prediction results can be generated independently with our proposed grid tagging scheme, and will be consumed together to construct a segment graph, so that the maximal clique discovery algorithm can be applied to recover desired entities. The overall extraction process is depicted in Figure 2. Next, we will first introduce our grid tagging scheme and its decoding workflow. Then we will detail the Mac, a Maximal clique discovery based discontinuous NER model based on this tagging scheme."
    }, {
      "heading" : "3.1 Grid Tagging Scheme",
      "text" : "Inspired by Wang et al. (2020), we implement single-stage segment extraction and edge prediction based on a novel grid tagging scheme. Given an n-token sentence, our scheme constructs an\nn× n tag table by enumerating all possible token pairs and giving each token pair the tag(s) based on their relation(s). Note that one token pair may have multiple tags according to the pre-defined tag set."
    }, {
      "heading" : "3.1.1 Segment Extraction",
      "text" : "As demonstrated in Figure 1, entity mentions could overlap with each other. To make our model capable of extracting such overlapping segments, we construct a two-dimensional tag table. Figure 3 provides an example. A pair of tokens (ti, tj) will be assigned with a set of labels if a segment from ti to tj belongs to the corresponding categories. Considering j ≥ i, we discard the lower triangle region of the tag table, so n\n2+n 2 grids are actually gener-\nated for an n-token sentence. In practice, the BIS tagging scheme is adopted to represent if a segment is a continuous entity mention (X-S) or locates at the beginning (X-B) or inside (X-I) of a discontinuous entity of type X. For example, (upper, body) is assigned with the tag POB-S since “upper body” is a continuous entity of type Part of Body (POB). And the tag of (Sever, joint) is ADE-B as “Sever joint” is a beginning segment of the discontinuous mention “Sever joint pain” of type Adverse Drug Event (ADE). Meanwhile, “joint” is also recognized as an entity since there is a POB-S tag in the place of (joint, joint), thus the overlapping segment extraction problem is solved."
    }, {
      "heading" : "3.1.2 Edge Prediction",
      "text" : "Edge prediction is to construct the links between segments of the same entity mention by aligning their boundary tokens. The tagging scheme is defined as follows: (1) head to head (X-H2H) indicates it locates in a place (ti, tj) where ti and tj are respectively the beginning tokens of two segments which constitute the same entity of type X; (2) tail to tail (X-T2T) is similar to X-H2H, but focusing on the ending token. As shown in Figure 4, “Sever” has the ADE-H2H and ADE-T2T relations\nto “shoulder” and “pain”, because the type of the discontinuous entity mention “Sever shoulder pain” is Adverse Drug Event . The same logic goes for other tags in the matrix."
    }, {
      "heading" : "3.2 Decoding Workflow",
      "text" : "Formally, the decoding procedure is summarized in Algorithm 1. The segment tagging table S and edge tagging table E of a sentence T serve as the inputs. Firstly, we extract all the typed segments through decoding S. Then we construct a segment graph G, in which segments that belong to the same entity (decoded from E) have edges with each other. Figure 2 gives an example. Correspondingly, we can yield a continuous entity mention from the singlevertex clique directly, and concatenate segments in each multiple-vertex clique following their original sequential order in T to recover discontinuous entity mentions. We choose the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) for finding the maximal cliques in G, which takes O(3 m 3 ) time, where m is the number of nodes."
    }, {
      "heading" : "3.3 Model Structure",
      "text" : "With the grid tagging scheme, we propose an endto-end neural architecture named Mac. Figure 5 reveals the overview structure."
    }, {
      "heading" : "3.3.1 Token Representation",
      "text" : "Given an n-token sentence [t1, · · · , tn], we first map each token ti into a low-dimensional contextual vector hi with a basic encoder. Then we gen-\nAlgorithm 1 Decoding Procedure Input: The segment tagging results S and edge tagging re-\nsults E of sentence T. S(ti, tj) and E(ti, tj) respectively denote the tag set of token pair (ti, tj) in two schemes. Output: R = {(ek, tk)}mk=1, ek, tk are respectively the text and the type of the k-th entity.\n1: Initialize the edge set A and entity set R with ∅ 2: Obtain the segment set N by decoding S. 3: for segment s ∈ N do 4: for segment g ∈ N do 5: Define type← the entity type of s or g 6: if type-H2H ∈ E(s.start, g.start) & type-T2T ∈\nE(s.end, g.end) then 7: Add (s, g) to A 8: end if 9: end for\n10: end for 11: Construct the segment graph G based on N and A 12: Find the maximal cliques C in G with the B-K algorithm 13: for clique c ∈ C do 14: Define t← the entity type of a random segment in c 15: Concat the segments of c with their order in T as e 16: Add (e, t) to R 17: end for 18: return R\nAlgorithm 2 B-K Backtracking Algorithm Input: The graph G Output: the set of all maximal cliques: C. 1: Initialize C and two vertex sets R, X with ∅ 2: Define P← the node set of G 3: function BRONKER(R, P, X) 4: if P = ∅ & X= ∅ then 5: Add R to C 6: end if 7: for v ∈ P do 8: Define N(v)← the neighbor set of v 9: BRONKER( R ∪ N(v), P ∩ N(v), X ∩ N(v)) 10: P← P \\ v 11: X← X ∪ v 12: end for 13: end function 14: BRONKER(R, P, X) // call the BronKer function 15: return C\nerate two representations, hsi and h e i , as the taskspecific features for the segment extractor and the edge predictor, respectively:\nhsi = W s h · hi + bsh, (1) hei = W e h · hi + beh, (2)\nwhere W∗h is a parameter matrix and b ∗ h is a bias vector to be learned during training."
    }, {
      "heading" : "3.3.2 Segment Extractor",
      "text" : "The probability that a pair of tokens are the boundary tokens of a segment can be represented as:\nP (ti, tj) = P (e = tj |b = ti)P (b = ti), (3)\nwhere b and e denotes the beginning token and ending token. In our tagging scheme (Figure 3), we\nhave a fixed beginning token ti at the i-th row, and take the given beginning token as the condition to label the corresponding ending token, so P (b = ti) in the i-th row is always 1. Hence, all we need to do is to calculate P (e = tj |b = ti).\nInspired by Su (2019) and Yu et al. (2021), we levderage the Conditional Layer Normalization (CLN) mechanism to model the conditional probability. That is, a conditional vector is introduced as extra contextual information to generate the gain parameter γ and bias λ of the well known layer normalization mechanism (Ba et al., 2016) as follows:\nCLN(c,x) = γc ( x− µ σ ) + λc, (4)\nµ = 1\nd d∑ i=1 xi, σ = √√√√1 d d∑ i=1 (xi − µ)2, (5)\nγc = Wαc+ bα, λc = Wβc+ bβ. (6)\nwhere c and x are the conditional vector and input vector respectively. xi denotes the i-th element of x, µ and σ are the mean and standard deviation taken across the elements of x, respectively. x is firstly normalized by fixing the mean and variance and then scaled and shifted by γc and λc respectively. Based on the CLN mechanism, the representation of token pair (ti, tj) being a segment boundary can be defined as:\nhsbi,j = CLN(h s i ,h s j). (7)\nIn this way, For different ti, different LN parameters are generated, which results in effectively adapting hj to be more ti-specific.\nFurthermore, besides the features of boundary tokens, we also consider inner tokens and segment length to learn a better segment representation. Specifically, we deploy a LSTM network (Hochreiter and Schmidhuber, 1997) to compute the hidden states of inner tokens, and use a looking-up table to embed the segment length. Since the ending token is always behind the beginning one, in each row ri, only the tokens behind ti will be fed into the LSTM. We take the hidden state outputted at each time step tj as the inner token representation of the segment si:j . Then the representation of a segment from ti to tj can be defined as follows:\nhini:j = LSTM(h s i , ...,h s j), j ≥ i, (8) eleni:j = Emb(j − i), j ≥ i, (9) hsi:j = h sb i,j + h in i:j + e len i:j . (10)"
    }, {
      "heading" : "3.3.3 Edge Predictor",
      "text" : "Edge prediction is similar with segment extraction since they all need to learn the representation of each token pair. The key differences are summarized in the following two aspects: (1) the distance between segments is usually not informative, so the length embedding eleni:j is valueless in edge prediction; (2) encoding the tokens between segments may carry noisy semantics for correlation tagging and aggravate the burden of training, so no hini:j is required. Under such considerations, we represent each token pair for edge prediction as:\nhei,j = CLN(h e i ,h e j). (11)"
    }, {
      "heading" : "3.4 Training and Inference",
      "text" : "In practical, our grid tagging scheme aims to tag most relevant labels for each token pair, so it can be seen as a multi-label classification problem. Once having the comprehensive token pair representations (hsi:j and h e i:j), we can build the multi-label classifier via a fully connected network. Mathematically, the predicted probability of each tag for (ti, tj) can be estimated via:\npIi,j = sigmoid(W I · hIi,j + bI), (12)\nwhere I ∈ {s, e} is the symbol of subtask indicator, denoting segment extraction and edge prediction respectively, and each dimension of pIi,j denotes the probability of a tag between ti and tj . The sigmoid function is used to transfer the projected value into a probability, in this case, the cross-entropy loss can be used as the loss function which has been proved suitable for multi-label classification task:\nLI = − n∑ i=1 n∑ j=sI KI∑ k=1 (yIi,j [k]log(p I i,j [k]) (13)\n+ (1− yIi,j [k])log(1− pIi,j [k])),\nwhere KI is the number of pre-defined tags in I, pIi,j [k] ∈ [0, 1] is the predicted probability of (ti, tj) along the k-th tag, and yIi,j [k] ∈ {0, 1} is the corresponding ground truth. sI equals to 1 if I = e or i if I = s. Then, the losses from segment extraction and edge prediction are aggregated to form the training objective J (θ):\nJ (θ) = Ls + Le. (14) At inference, the probability vector pIi,j needs thresholding to be converted to tags. We enumerate several values in the range (0, 1) and pick the one that maximizes the evaluation metrics on the validation (dev) set as the threshold.\nCADEC ShARe 13 ShARe 14 train dev test train dev test train dev test\nS 5,340 1,097 1,160 8,508 1,250 9,009 17,407 1,361 15,850 M 4,430 898 990 5,146 669 5,333 10,354 771 7,922 D 491 94 94 581 71 436 1,004 80 566 P 11.1 10.5 9.5 11.3 10.6 8.2 9.7 10.4 7.1\nTable 1: Statistics of datasets. S, M, and D respectively represent the number of sentences, total mentions, and discontinuous mentions. P denotes the percentage of discontinuous mentions in total mentions."
    }, {
      "heading" : "4 Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Following previous work (Dai et al., 2020b), we conduct experiments on three benchmark datasets from the biomedical domain: (1) CADEC (Karimi et al., 2015) is sourced from AskaPatient: an online forum where patients can discuss their experiences with medications. We use the dataset pre-processed by Dai et al.(2020b) which selected Adverse Drug Event (ADE) annotations from the original dataset because only the ADEs involve discontinuous annotations. (2) ShARe 13 (Pradhan et al., 2013) and (3) ShARe 14 (Mowery et al., 2014) focus on the identification of disorder mentions in clinical notes, including discharge summaries, electrocardiogram, echocardiogram, and radiology reports. Around 10% of mentions in these three data sets are discontinuous. The descriptive statistics of the datasets are reported in Table 1."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We implement our model upon the in-field BERT base model: Yelp Bert (Dai et al., 2020a) for CADEC, and Clinical BERT (Alsentzer et al., 2019) for ShARe 13 and 14. The network parameters are optimized by Adam (Kingma and Ba, 2014) with a learning rate of 1e-5. The batch size is fixed to 12. The threshold for converting probability to tag is set as 0.5. All the hyper-parameters are tuned on the dev set. We run our experiments on a NVIDIA Tesla V100 GPU for at most 300 epochs, and choose the model with the best performance on the dev set to output results on the test set. we report the test score of the run with the median dev score among 5 randomly initialized runs."
    }, {
      "heading" : "4.3 Comparison Models",
      "text" : "For comparison, we employ the following models as baselines: (1) BIOE (Metke-Jimenez and\nKarimi, 2016) expands the BIO tagging scheme with additional tags to represent discontinuous entity; (2) Graph (Muis and Lu, 2016) uses hypergraphs to organize entity spans and their combinations; (3) Comb (Wang and Lu, 2019) first detects entity spans, then deploys a classifier to merge them. For fair comparison, we re-implement Comb based on the in-fild BERT backbone called CombB ; (4) TransE (Dai et al., 2020b) is the current best discontinuous NER method, which generates a sequence of actions with the aid of buffer and stack structure to detect entity; Note that the original TransE model is based on ELMo. For fair comparison with our model, we also implement the in-field BERT-based Trans models, namely TransB ."
    }, {
      "heading" : "4.4 Main results",
      "text" : "Table 2 reports the results of our models against other baseline methods. We have the following observations. (1) Our method, Mac, significantly outperforms all other methods and achieves the SOTA F1 score on all three datasets. (2) BERT-based Trans model achieves poorer results than its ELMobased counterpart, which is in line with the claim in the original paper. (3) Over the SOTA method TransE , Mac achieves substantial improvements of 2.6% in F1 score on three datasets averagely. Moreover, the Wilcoxon’s test shows that a significant difference (p < 0.05) exists between our model and TransE . We consider that it is because TransE is inherently a multi-stage method as it introduces several dependent actions, thus suffering from the exposure bias problem. While for our Mac method, it elegantly decomposes the discontinuous NER task into two independent subtasks and learns them together with a joint model, realizing the consistency of training and inference. (4) CombB can be approximately seen as the pipeline version of our method, their performance gap again confirms the effectiveness of our one-stage learning framework.\nAs shown in Table 1, only around 10% mentions are discontinuous in all three datasets, which is far less than the continuous entity mentions. To evaluate the effectiveness of our proposed model on recognizing discontinuous mentions, following Muis and Lu (2016), we report the results on sentences that include at least one discontinuous mention. We also report the evaluation results when only discontinuous mentions are considered. The scores in these two settings are separated by a slash in Table 3. Comparing Table 2 and 3, we can see that the BIOE model performs better than the Graph when testing on the full dataset but far worse on discontinuous mentions. Consistently, our model again defeat the baseline models in terms of F1 score. Even though some models outperform Mac on precision or recall, they greatly sacrifice another score, which results in lower F1 score than Mac."
    }, {
      "heading" : "4.5 Model Ablation Study",
      "text" : "To verify the effectiveness of each component, we ablate one component at a time to understand its impact on the performance. Concretely, we investigated the tagging scheme of segments, the segment length embedding, the CLN mechanism (by replacing it with the vector concatenation), and the segment inner token representation.\nFrom these ablations shown in Table 4, we find\nthat: (1) When we take B, I and S tags in segment extraction as one class, the score slightly drops by 0.5%, which indicates the segments in different positions of entities may have different semantic features, so distinguishing them can reduce the confusion in the process of model recognition; (2) When we remove the segment length embedding (Formula 9), the overall F1 score drops by 0.6%, showing that it is necessary to let segment extractor aware of the token pair distance information to filter out impossible segments by implicit distance constraint; (3) Compared with concatenating, it is a better choice to use CLN (Formula 7 and 11) to fuse the features of two tokens, which brings 1.9% improvement; (4) Removing segment inner features (Formula 8) results in a remarkable drop on the overall F1 score while little drop on the scores of discontinuous mentions, which suggests that the information of inner tokens is essential to recognize continuous entity mentions. Overall, we can conclude that the improvement of grid encoder brings significant performance gains."
    }, {
      "heading" : "4.6 Performance Analysis",
      "text" : ""
    }, {
      "heading" : "4.6.1 Impact of Overlapping Structure",
      "text" : "As discussed in the introduction, overlap is very common in discontinuous entity mentions. To evaluate the capability of our model on extraction overlapping structures, as suggested in (Dai et al., 2020b), we divide the test set into four categories: (1) no overlap; (2) left overlap; (3) right overlap; and (4) multiple overlap. Figure 6 gives examples for each overlapping pattern. As illustrated in Figure 7, Mac outperforms TransE on all the overlapping patterns. TransE gets zero scores on some patterns. It might result from insufficient training since these overlapping patterns have relatively fewer samples in the training sets (see Table 5), while the sequential action structure of transition-based model is a bit data hungry. By contrast, Mac is more resilient to overlapping patterns, we attribute the performance gains to two design choices: (1) the grid tagging scheme has strong power in accurately identifying overlapping segments and assembling them into a segment graph; (2) Based on the graph, the maximal clique discovery algorithm can effectively recover all the candidate overlapping entity mentions."
    }, {
      "heading" : "4.6.2 Impact of Interval and Span Length",
      "text" : "Intervals between segments usually make the total length of a discontinuous mention longer than continuous one. Considering the involved segments, the whole span is even longer. That is, different words of a discontinuous mention may be distant to each other, which makes discontinuous NER harder than the conventional NER task. To further evaluate the robustness of Mac in different settings, we analyse the results of test sets on different interval and span lengths. The interval length refers to the\nnumber of words between discontinuous segments. The span length refers to the number of words of the whole span. For example, for the entity mention “Sever shoulder pain” in “Sever joint, shoulder and upper body pain.”, the interval length is 5, and the span length is 8. Such phenomenon requires models to have the ability of capturing the semantic dependency between distant segments.\nFor the convenience of analysis, we report all datasets’ distribution on interval and span length in Table 6 and 7, respectively. And Figure 8 shows the F1 scores of TransE and Mac on different interval and span lengths. As we can see, Mac outperforms TransE in most setting. Even though Mac is defeated in some cases, the sample number in those cases is too small to disprove the superiority of\nMac. For example, on CADEC, TransE outperforms Mac when span length is 8, but the sample number in the test set is only 10.\nWe figure out an interesting phenomenon: Both Mac and TransE show poor performance when interval length is 1 and span length is 3, even though the corresponding training samples are sufficient enough (see length = 1 in Table 6 and length = 3 in Table 72). This might result from two folds: (1) Even though the training samples are sufficient, their features and context are different from the ones in the test set; (2) discontinuous mentions with interval length equal to 1 are harder cases than the others, since only one word to separate the segments makes these discontinuous mentions very similar to the continuous ones, which confuse the model to treat them as a continuous mention. We leave this problem to our future work."
    }, {
      "heading" : "4.6.3 Analysis on Running Speed",
      "text" : "Table 8 shows the comparison of computational efficiency between the SOTA model TransE , TransB , and our proposed Mac. All of these models are im-\n2For discontinuous mentions, when span length is 3, the interval length can only be 1.\nplemented by Pytorch and ran on a single Tesla V100 GPU environment. As we can see, the prediction speed of Mac is around 5 times faster than TransE . Since the transition-based model employs a stack to store partially processed spans and a buffer to store unprocessed tokens (Dai et al., 2020b), it is difficult to utilize GPU parallel computing to speed up the extraction process. In the official implementation, TransE is restricted to processes one token at a time, which means it is seriously inefficient and difficult to deploy in real development environment. By contrast, Mac is capable of handling data in batch mode because it is a single-stage sequence labeling model in essence."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we reformulate discontinuous NER as the task of discovering maximal cliques in a segment graph, and propose a novel Mac architecture. It decomposes the construction of segment graph as two independent 2-D grid tagging problems, and solves them jointly in one stage, addressing the exposure bias issue in previous studies. Extensive experiments on three benchmark datasets show that Mac beats the previous SOTA method by as much as 3.5 pts in F1, while being 5 times faster. Further analysis demonstrates the ability of our model in recognizing discontinuous and overlapping entity mentions. In the future, we would like to explore similar formulation in other information extraction tasks, such as event extraction and nested NER."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their insightful suggestions. This work is supported by the National Key Research and Development Program of China (Grant No.2017YFB0802804), the Guangdong Province Key Area Research and Development Program of China (Grant No.2019B010137004), the Youth Innovation Promotion Association of Chinese Academy of Sciences (Grant No.2021153), and the Key Program of National Natural Science Foundation of China (Grant No.U1766215)."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Information retrieval as statistical translation",
      "author" : [ "Adam Berger", "John Lafferty." ],
      "venue" : "Proceedings of ACM SIGIR.",
      "citeRegEx" : "Berger and Lafferty.,? 2017",
      "shortCiteRegEx" : "Berger and Lafferty.",
      "year" : 2017
    }, {
      "title" : "Statistical analysis of financial networks",
      "author" : [ "Vladimir Boginski", "Sergiy Butenko", "Panos M Pardalos." ],
      "venue" : "Computational statistics & data analysis.",
      "citeRegEx" : "Boginski et al\\.,? 2005",
      "shortCiteRegEx" : "Boginski et al\\.",
      "year" : 2005
    }, {
      "title" : "Algorithm 457: finding all cliques of an undirected graph",
      "author" : [ "Coen Bron", "Joep Kerbosch." ],
      "venue" : "Communications of the ACM.",
      "citeRegEx" : "Bron and Kerbosch.,? 1973",
      "shortCiteRegEx" : "Bron and Kerbosch.",
      "year" : 1973
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Cost-effective selection of pretraining data: A case study of pretraining bert on social media",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Ben Hachey", "Cecile Paris." ],
      "venue" : "Proceedings of EMNLP: Findings.",
      "citeRegEx" : "Dai et al\\.,? 2020a",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "An effective transition-based model for discontinuous ner",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Ben Hachey", "Cecile Paris." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Dai et al\\.,? 2020b",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "Medication and adverse event extraction from noisy text",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Cecile Paris." ],
      "venue" : "Proceedings of the Australasian Language Technology Association Workshop.",
      "citeRegEx" : "Dai et al\\.,? 2017",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2017
    }, {
      "title" : "Finding a maximum clique in dense graphs via χ2 statistics",
      "author" : [ "Sourav Dutta", "Juho Lauri." ],
      "venue" : "Proceedings of CIKM.",
      "citeRegEx" : "Dutta and Lauri.,? 2019",
      "shortCiteRegEx" : "Dutta and Lauri.",
      "year" : 2019
    }, {
      "title" : "Character-level neural network for biomedical named entity recognition",
      "author" : [ "Mourad Gridach." ],
      "venue" : "Journal of biomedical informatics.",
      "citeRegEx" : "Gridach.,? 2017",
      "shortCiteRegEx" : "Gridach.",
      "year" : 2017
    }, {
      "title" : "Cnn-based chinese ner with lexicon rethinking",
      "author" : [ "Tao Gui", "Ruotian Ma", "Qi Zhang", "Lujun Zhao", "Yu-Gang Jiang", "Xuanjing Huang." ],
      "venue" : "Proceedings of IJCAI.",
      "citeRegEx" : "Gui et al\\.,? 2019",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Band selection for nonlinear unmixing of hyperspectral images as a maximal clique problem",
      "author" : [ "Tales Imbiriba", "José Carlos Moreira Bermudez", "Cedric Richard." ],
      "venue" : "IEEE Transactions on Image Processing.",
      "citeRegEx" : "Imbiriba et al\\.,? 2017",
      "shortCiteRegEx" : "Imbiriba et al\\.",
      "year" : 2017
    }, {
      "title" : "Cadec: A corpus of adverse drug event annotations",
      "author" : [ "Sarvnaz Karimi", "Alejandro Metke-Jimenez", "Madonna Kemp", "Chen Wang." ],
      "venue" : "Journal of biomedical informatics.",
      "citeRegEx" : "Karimi et al\\.,? 2015",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2015
    }, {
      "title" : "The impact of named entity normalization on information retrieval for question answering",
      "author" : [ "Mahboob Alam Khalid", "Valentin Jijkoun", "Maarten De Rijke." ],
      "venue" : "Proceedings of ECIR.",
      "citeRegEx" : "Khalid et al\\.,? 2008",
      "shortCiteRegEx" : "Khalid et al\\.",
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Concept identification and normalisation for adverse drug event discovery in medical forums",
      "author" : [ "Alejandro Metke-Jimenez", "Sarvnaz Karimi." ],
      "venue" : "Proceedings of ISWC.",
      "citeRegEx" : "Metke.Jimenez and Karimi.,? 2016",
      "shortCiteRegEx" : "Metke.Jimenez and Karimi.",
      "year" : 2016
    }, {
      "title" : "Task 2: Share/clef ehealth evaluation lab",
      "author" : [ "Danielle L Mowery", "Sumithra Velupillai", "Brett R South", "Lee Christensen", "David Martinez", "Liadh Kelly", "Lorraine Goeuriot", "Noemie Elhadad", "Sameer Pradhan", "Guergana Savova" ],
      "venue" : null,
      "citeRegEx" : "Mowery et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mowery et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to recognize discontiguous entities",
      "author" : [ "Aldrian Obaja Muis", "Wei Lu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Muis and Lu.,? 2016",
      "shortCiteRegEx" : "Muis and Lu.",
      "year" : 2016
    }, {
      "title" : "Evaluating the state of the art in disorder recognition and normalization of the clini",
      "author" : [ "Sameer Pradhan", "Noémie Elhadad", "Brett R South", "David Martinez", "Lee Christensen", "Amy Vogel", "Hanna Suominen", "Wendy W Chapman", "Guergana Savova" ],
      "venue" : null,
      "citeRegEx" : "Pradhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Task 1: Share/clef ehealth evaluation lab 2013",
      "author" : [ "Sameer Pradhan", "Noemie Elhadad", "Brett R South", "David Martinez", "Lee M Christensen", "Amy Vogel", "Hanna Suominen", "Wendy W Chapman", "Guergana K Savova." ],
      "venue" : "Proceedings of CLEF.",
      "citeRegEx" : "Pradhan et al\\.,? 2013",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "Finding all maximal cliques in dynamic graphs",
      "author" : [ "Volker Stix." ],
      "venue" : "Computational Optimization and applications.",
      "citeRegEx" : "Stix.,? 2004",
      "shortCiteRegEx" : "Stix.",
      "year" : 2004
    }, {
      "title" : "Conditional text generation based on conditional layer normalization",
      "author" : [ "Jianlin Su" ],
      "venue" : null,
      "citeRegEx" : "Su.,? \\Q2019\\E",
      "shortCiteRegEx" : "Su.",
      "year" : 2019
    }, {
      "title" : "Recognizing clinical entities in hospital discharge summaries using structural support vector machines with word representation features",
      "author" : [ "Buzhou Tang", "Hongxin Cao", "Yonghui Wu", "Min Jiang", "Hua Xu." ],
      "venue" : "BMC medical informatics and decision",
      "citeRegEx" : "Tang et al\\.,? 2013",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "Recognizing continuous and discontinuous adverse drug reaction mentions from social media using lstm-crf",
      "author" : [ "Buzhou Tang", "Jianglu Hu", "Xiaolong Wang", "Qingcai Chen." ],
      "venue" : "Wireless Communications and Mobile Computing.",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "Combining spans into entities: A neural two-stage approach for recognizing discontiguous entities",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Wang and Lu.,? 2019",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2019
    }, {
      "title" : "Tplinker: Single-stage joint extraction of entities and relations through token pair linking",
      "author" : [ "Yucheng Wang", "Bowen Yu", "Yueyang Zhang", "Tingwen Liu", "Hongsong Zhu", "Limin Sun." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Coarse-to-fine pretraining for named entity recognition",
      "author" : [ "Mengge Xue", "Bowen Yu", "Zhenyu Zhang", "Tingwen Liu", "Yue Zhang", "Bin Wang." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond word attention: Using segment attention in neural relation extraction",
      "author" : [ "Bowen Yu", "Zhenyu Zhang", "Tingwen Liu", "Bin Wang", "Sujian Li", "Quangang Li." ],
      "venue" : "Proceedings of IJCAI.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-open information extraction",
      "author" : [ "Bowen Yu", "Zhenyu Zhang", "Jiawei Sheng", "Tingwen Liu", "Yubin Wang", "Yucheng Wang", "Bin Wang." ],
      "venue" : "Proceedings of the Web Conference.",
      "citeRegEx" : "Yu et al\\.,? 2021",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2021
    }, {
      "title" : "Joint extraction of entities and relations based on a novel decomposition strategy",
      "author" : [ "Bowen Yu", "Zhenyu Zhang", "Jianlin Su." ],
      "venue" : "Proceedings of ECAI.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese ner using lattice lstm",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", information retrieval (Berger and Lafferty, 2017), relation extraction (Yu et al.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : ", information retrieval (Berger and Lafferty, 2017), relation extraction (Yu et al., 2019), and question answering (Khalid et al.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "Their underlying assumption is that an entity mention should be a short span of text (Muis and Lu, 2016), and should not overlap with each other.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "While such assumption is valid for most cases, it does not always hold, especially in clinical corpus (Pradhan et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "Combination-based models first detect all the overlapping spans and then learn to combine these segments with a separate classifier (Wang and Lu, 2019); Transition-based models incrementally label the discontinuous spans through a sequence of shift-reduce actions (Dai et al.",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 6,
      "context" : "Combination-based models first detect all the overlapping spans and then learn to combine these segments with a separate classifier (Wang and Lu, 2019); Transition-based models incrementally label the discontinuous spans through a sequence of shift-reduce actions (Dai et al., 2020b).",
      "startOffset" : 264,
      "endOffset" : 283
    }, {
      "referenceID" : 32,
      "context" : "Although these methods have achieved reasonable performance, they continue to have difficulty with the same problem: exposure bias (Zhang et al., 2019).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 26,
      "context" : "Specifically, combination-based methods use the gold segments to guide the classifier during the training process while at inference the input segments are given by a trained model, leading to a gap between training and inference (Wang and Lu, 2019).",
      "startOffset" : 230,
      "endOffset" : 249
    }, {
      "referenceID" : 24,
      "context" : "To achieve this end, several researchers introduced new position indicators into the traditional BIO tagging scheme so that the sequential labeling models can be employed (Tang et al., 2013; MetkeJimenez and Karimi, 2016; Dai et al., 2017; Tang et al., 2018).",
      "startOffset" : 171,
      "endOffset" : 258
    }, {
      "referenceID" : 7,
      "context" : "To achieve this end, several researchers introduced new position indicators into the traditional BIO tagging scheme so that the sequential labeling models can be employed (Tang et al., 2013; MetkeJimenez and Karimi, 2016; Dai et al., 2017; Tang et al., 2018).",
      "startOffset" : 171,
      "endOffset" : 258
    }, {
      "referenceID" : 25,
      "context" : "To achieve this end, several researchers introduced new position indicators into the traditional BIO tagging scheme so that the sequential labeling models can be employed (Tang et al., 2013; MetkeJimenez and Karimi, 2016; Dai et al., 2017; Tang et al., 2018).",
      "startOffset" : 171,
      "endOffset" : 258
    }, {
      "referenceID" : 6,
      "context" : "As the improvement, Muis and Lu (2016) used hyper-graphs to represent entity spans and their combinations, but did not completely resolve the ambiguity issue (Dai et al., 2020b).",
      "startOffset" : 158,
      "endOffset" : 177
    }, {
      "referenceID" : 31,
      "context" : "Joint extraction aims to detect entity pairs along with their relations using a single model (Yu et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "Discontinuous NER is related to joint extraction where the discontiguous entities can be viewed as relation links between segments (Wang and Lu, 2019).",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 27,
      "context" : "Our model is motivated by TPLinker (Wang et al., 2020), which formulates joint extraction as a token pair linking problem by aligning the boundary tokens of entity pairs.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "Maximal clique discovery is to find a clique of maximum size in a given graph (Dutta and Lauri, 2019).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "Maximal clique discovery finds extensive application across diverse domains (Stix, 2004; Boginski et al., 2005; Imbiriba et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "Maximal clique discovery finds extensive application across diverse domains (Stix, 2004; Boginski et al., 2005; Imbiriba et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "Maximal clique discovery finds extensive application across diverse domains (Stix, 2004; Boginski et al., 2005; Imbiriba et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we reformulate discontinuous NER as the task of maximal clique discovery by constructing a segment graph and leveraging the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) to find all the maximum cliques as the entities.",
      "startOffset" : 174,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "Considering the maximum clique searching process is usually non-parametric (Bron and Kerbosch, 1973), discontinuous NER is actually decomposed into two subtasks: segment extraction and edge prediction, to respectively create the nodes and edges of the segment graph.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "We choose the classic B-K backtracking algorithm (Bron and Kerbosch, 1973) for finding the maximal cliques in G, which takes",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "That is, a conditional vector is introduced as extra contextual information to generate the gain parameter γ and bias λ of the well known layer normalization mechanism (Ba et al., 2016) as follows:",
      "startOffset" : 168,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : "Specifically, we deploy a LSTM network (Hochreiter and Schmidhuber, 1997) to compute the hidden states of inner tokens, and use a looking-up table to embed the segment length.",
      "startOffset" : 39,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "Following previous work (Dai et al., 2020b), we conduct experiments on three benchmark datasets from the biomedical domain: (1) CADEC (Karimi et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : ", 2020b), we conduct experiments on three benchmark datasets from the biomedical domain: (1) CADEC (Karimi et al., 2015) is sourced from AskaPatient: an online forum where patients can discuss their experiences with medications.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "(2) ShARe 13 (Pradhan et al., 2013) and (3) ShARe 14 (Mowery et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : ", 2013) and (3) ShARe 14 (Mowery et al., 2014) focus on the identification of disorder mentions in clinical notes, including discharge summaries, electrocardiogram, echocardiogram, and radiology reports.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "We implement our model upon the in-field BERT base model: Yelp Bert (Dai et al., 2020a) for CADEC, and Clinical BERT (Alsentzer et al.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "The network parameters are optimized by Adam (Kingma and Ba, 2014) with a learning rate of 1e-5.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "Karimi, 2016) expands the BIO tagging scheme with additional tags to represent discontinuous entity; (2) Graph (Muis and Lu, 2016) uses hypergraphs to organize entity spans and their combinations; (3) Comb (Wang and Lu, 2019) first detects entity spans, then deploys a classifier to merge them.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : "Karimi, 2016) expands the BIO tagging scheme with additional tags to represent discontinuous entity; (2) Graph (Muis and Lu, 2016) uses hypergraphs to organize entity spans and their combinations; (3) Comb (Wang and Lu, 2019) first detects entity spans, then deploys a classifier to merge them.",
      "startOffset" : 206,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "For fair comparison, we re-implement Comb based on the in-fild BERT backbone called CombB ; (4) TransE (Dai et al., 2020b) is the current best discontinuous NER method, which generates a sequence of actions with the aid of buffer and stack structure to detect entity; Note that the original TransE model is based on ELMo.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "To evaluate the capability of our model on extraction overlapping structures, as suggested in (Dai et al., 2020b), we divide the test set into four categories: (1) no overlap; (2) left overlap; (3) right overlap; and (4) multiple overlap.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "Since the transition-based model employs a stack to store partially processed spans and a buffer to store unprocessed tokens (Dai et al., 2020b), it is difficult to utilize GPU parallel computing to speed up the extraction process.",
      "startOffset" : 125,
      "endOffset" : 144
    } ],
    "year" : 2021,
    "abstractText" : "Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-ofthe-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.1",
    "creator" : "LaTeX with hyperref"
  }
}