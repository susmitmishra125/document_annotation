{
  "name" : "2021.acl-long.156.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hierarchical Context-aware Network for Dense Video Event Captioning",
    "authors" : [ "Lei Ji", "Xianglin Guo", "Haoyang Huang", "Xilin" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2004–2013\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2004"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the increase of video data uploaded online every day, the acquisition of knowledge from videos especially for Howto tasks is indispensable for people’s daily life and work. However, watching a whole long video is time-consuming. Existing technologies focus on two main research directions to compact video information: video summarization to trim long videos to short ones and (dense) video captioning to generate a textual description of the key events in the video. Typically for long untrimmed videos, dense video event captioning generates fine-grained captions for all events to facilitate users quickly skimming the video content and enables various applications e.g. video chaptering and search inside a video.\n∗Equal contribution\nDense video event captioning (Krishna et al., 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) aims to generate a sequence of captions for all events regarding to uni-modality (video) or multi-modality (video + speech) inputs. Figure 1 presents a showcase, which demonstrates the challenges of this task from both vision and speech text perspective. For vision understanding, the fine-grained objects are hard to recognize due to ambiguity, occlusion, or state change. In this case, the object ”dough” is occluded in event 1 and is hard to recognize from the video. However, it can be recognized from the previous neighbor video frame with a clear appearance. From speech text perspective, although the speech text offers semantic concepts (Shi et al., 2019; Iashin and Rahtu, 2020b), it brings another challenge of co-reference and ellipsis in speech text due to the informal utterance of oral speeches. In the case of Figure 1, the entity ”dough” in event 3 is an ellipsis in the text. Nonetheless, it is capable of generating consistent objects ”dough” in event 3 with the contextual information from other events such as event 1 in this example. To sum up, both\nlocal neighbor-clip and global inter-event contexts are important for event-level captioning to generate coherent and less duplication descriptions between events.\nPrevious endeavors widely used recurrent neural network(Krishna et al., 2017) which suffers from capturing long dependency, while recently attention-based model(Zhou et al., 2018b; Sun et al., 2019b,a) is becoming the new paradigm for dense video event captioning and effective for multi-modal video captioning (Shi et al., 2019; Iashin and Rahtu, 2020b). However, existing attention-based models generate the captioning only relying on the video clip inside each event, and ignore video-level local and global context. Motivated by this, we mainly investigate how to effectively and jointly leverage both local and global context for video captioning.\nIn this paper, we propose a novel hierarchical context-aware model for dense video event captioning (HCN) to capture both the local and global context simultaneously. In detail, we first exploit a local context encoder to embed the visual and linguistic features of the source and surrounding clips, then design a global context encoder to capture relevant features from other events. Specifically, we apply different mechanisms: a flat attention module between the source and local context; a cross attention module for the source to select the global context. With regards to the neighbor frames (temporally close) usually alike, e.g. with the same objects, the flat attention is a full interaction to generate accurate and coherent captions. Contemporaneously, the cross attention on global context can selectively attend to the relevant events and capture prior temporal dependency between events to generate coherent and less duplicate captions. The experimental results demonstrate the effectiveness of our model. Our contributions can be summarized as:\n1) We propose a hierarchical context-aware model for dense video event captioning to capture video-level context.\n2) We carefully design different mechanisms to capture both local and global context: a flat attention model with full interaction between neighbor frames and a cross attention model to selectively capture inter-event features.\n3) Experimental results on both Youcook2 and Activitynet Captions dataset demonstrate the effectiveness of our models and outperforms the context-\nagnostic model to a large extent."
    }, {
      "heading" : "2 Preliminary",
      "text" : "The dense video event captioning task is to produce a sequence of events and generate a descriptive sentence for each event given a long untrimmed video. In this work, we focus only on the task to generate captions and directly apply the ground-truth event proposals similar to (Hessel et al., 2019; Iashin and Rahtu, 2020b). The paradigm for video captioning is an encoder-decoder network, which inputs video features and outputs descriptions for each event. In this section, we describe the task formulation including the context-agnostic model as well as the context-aware model in one framework."
    }, {
      "heading" : "2.1 Overview",
      "text" : "Problem Definition We define a sequence of event segment proposals as e = { ei|i ∈ [1,m] } , representing the video withm proposals, ei is the feature of the i-th event including both video and text feature, ei = {vi, ti}, where vi is video feature and ti is transcript text feature (if available) of the i-th event. We take all the video frames and transcript tokens of the event between the start and end time. The number of video frames is likely to be different from the number of text tokens depending on the actual video clip. Given all events e, the goal is to predict the target descriptive sentences Y = { yi|i ∈ [1,m] } . Each yi is a sequence of descriptive words corresponding to each event ei. The probability of the expected sentences Y.\nP (Y |e) = − m∏ i=1 P (yi | ei) (1)\nwhich is to predict yi conditioned on the event ei. The context-aware model considers local context v 6=i (the neighboring video clip) and global context e6=i (the clips of past and future events) respectively. The context-aware probability can be approximated as\nP (Y |e) = − m∏ i=1 P (yi | ei, v 6=i, e 6=i) (2)"
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Context-agnostic model",
      "text" : "The context-agnostic model of captioning is to generate a descriptive sentence given the shorttrimmed video clip of each event. The paradigm\nfor multi-modal video captioning is an encoderdecoder network as in (Hessel et al., 2019). First, we pre-process each event and extract features separately. For the event ei, we extract both video feature vi and transcript feature ti if available. Next, both the video features and transcript features are concatenated together as the input to the transformer encoder. This encoder implements selfattention of each modality and cross attention between both modalities in one unified transformer. Finally, a transformer decoder generates the text tokens of the description with the enhanced features."
    }, {
      "heading" : "3.2 Context-aware model",
      "text" : "We propose a context-aware video event captioning model with a hierarchical context-aware network (HCN) and the architecture is a general framework for either uni-modal or multi-modal inputs as explained in Figure 2."
    }, {
      "heading" : "3.2.1 Multi-modal Feature Representation",
      "text" : "For visual features, we adopt a pre-trained 3D feature extractor to extract k features as vi = { vj |j ∈\n[1, k] }\nof the i-th event. We further add a projection layer to map the raw feature to the input dimension through an embedding layer f(vi) =\n{e|e = Embedding(vi)}. For transcript text, we tokenize the text into words and represent each word with 1-hot representation. The tokens within each event are represented as ti = {tj |j ∈ [1, l]}, where l is the length of the tokens corresponding to the number of the transcript text in the speech of the event. Moreover, we embed each token to continuous representation by an embedding layer f(ti) = {e|e = Embedding(ti)}. Similar to the work in (Hessel et al., 2019), we build the vocabulary using all tokens in the captioning sentence.\nThe input for each event comprises of three types of embedding: 1) visual feature f(vi) (and speech text feature f(ti) if available); 2) position embedding p(vi) and p(ti) as introduced in the transformer model(Vaswani et al., 2017); 3) type embedding s(vi) and s(ti) representing whether the current embedding is from context or source.\nE(vi) = [f(vi) + p(vi) + s(vi)] (3) E(ti) = [f(ti) + p(ti) + s(ti)] (4)\nwhere + is the add operator, E(vi) and E(ti) are the embeddings of video and text respectively. For multi-modal input, both visual and text features are concatenated for further processing.\nWe extract two types of contextual information: event-agnostic local context and event-aware global context. Event-agnostic context takes frames temporally close to the video event. Video is a continuous signal and neighboring video frames are likely to be semantically related to each other e.g. same objects. This is especially helpful for recognizing objects with state change or occluded in the current event. Moreover, objects are likely to be explicitly mentioned in the contextual transcript which can be used to deal with object co-reference and ellipsis typically for instructional videos. Event-aware context utilizes the video frames of both previous and future events, which attempts to model the relation between events. The global context provides overall features and prior knowledge of temporal dependency. Specifically for a particular domain like a recipe, the event “mix the flour and water” is often followed by “knead the dough”. This prior knowledge of event dependency learned from a global context is effective for understanding long videos."
    }, {
      "heading" : "3.2.2 Hierarchical Context-aware Network",
      "text" : "The overall pipeline includes 4 modules: 1) the hierarchical model starts with a local context module (LCM) to encode the local context features,\nthe neighbor video clip temporally close to the event. Specifically, the LCM adopts a flat attention model similar to (Ma et al., 2020) to enhance the source video feature by local context. Besides, given multi-modal inputs, LCM is a general model to fuse both the visual features f(vi) and the text features f(ti) inside the event with one unified transformer as in (Hessel et al., 2019); 2) we further employ a global context module (GCM) to make the source event to interact with other event features flexibly. The GCM is a cross attention model, which contains one source encoder SEncoder and one cross encoder CEncoder. SEncoder is a self-attention module for encoding event features, and CEncoder is a cross attention module for interaction between source and context events; 3) the hierarchical context-aware model further combines both the neighbor-clip (around the event) or inter-event (other events) context from both previous and future using gating mechanism; 4) finally, an auto-regressive decoder is used to generate the sentence with a masked transformer model.\nLocal Context Module We first introduce the local context module to encode multi-modal source video features together with the event-agnostic context features (surrounding frames). The flat transformer in (Ma et al., 2020) is effective for encoding contextual information with full interaction between source and context features. In addition, when the speech text is available for multi-modal video captioning, this flat encoder can also perform the fusion of visual and text modalities, which is similar to (Hessel et al., 2019). To sum up, we employ one unified flat encoder to accomplish two actions: source-context interaction and multi-modal fusion as explained in Figure 3a.\nE(ei) = [E(vi);E(ti)] (5) H(mi) = FFN(MultiHead([E(vi±kl);E(ei)])) (6) H(eli) = H(mi)[i1 : in] (7)\nwhere [;] is concatenation operation, FFN means the feed-forward network and MultiHead is the multi-head attention network in transformer(Vaswani et al., 2017). We apply residual connection for all components. We only perform equation 5 for multi-modal video event captioning, and E(ei) is the concatenation of the visual embedding and text embedding for the event i. We then feed the embedding E(ei) together with the embedding of neighbor frames E(vi±kl) into the\ntransformer blocks and get context-aware encoding H(mi), and kl is the local context length. Finally, we only select the output of source encoding instead of using all embedding for further processing. Intuitively, the source is more important than the context. In equation 7, H(eli) is the hidden state of the source input, which requires the model to focus on the current source event, i1 is the start of the event i and in is the end of the event i. LCM outputs the enhanced event representation by local context and multi-modal inputs.\nGlobal Context Module We then illustrate the global context module to encode the output of LCM together with event-aware context (previous or future events). GCM is a cross attention model, which selectively attends to previous or future events to enhance the source video representation. Different from LCM, which applies a unified transformer to encode a short context, GCM exploits a cross attention model similar to (Maruf et al., 2019) to encode long global context efficiently. The unified transformer model is hard to deal with long input sequences due to complexity. The cross attention model facilitates the source to interact with each context event and can easily be scaled out for long videos. Figure 3b illustrates the GCM model structure.\nWe exploit the GCM for each contextual event and then combine all the encoding through a context gating mechanism similar to (Maruf et al., 2019). First, the self-attention module encodes each source or context event separately. Then, the\ncross attention module empowers the source to attend to context.\nH(êi) = FFN(MultiHead([H(eli)])) (8) H(ej) = FFN(MultiHead([E(ej)])) (9) H(ecj) = FFN(MultiHead([H(êi), H(ej)])) (10)\nwhere H(êi) is the encoding of source event i, H(ej) is encoding of the j-th context event, and H(ecj) is the source attended to the j-th event.\nNext, we adopt a gated recurrent unit (GRU) (Cho et al., 2014) to selectively update the source feature with context enhanced feature which is shown to be effective in our ablation study.\nzj = σ(wzH(êi) + uzH(e c j) + bz) (11) rj = σ(wrH(êi) + urH(e c j) + br) (12) ĥj = φ(whH(êi) + uh(rj H(ecj) + bh) (13) hj = (1− zj) H(ecj) + zj ĥj (14)\nwhere σ is a logistic sigmoid operation, φ is the activation function tanh, w and u are learnable weight matrices, and hj is the encoded representation after the source event i attended to the context event j.\nContext Gating We adopt the gate in (Tu et al., 2018) to regulate the source H(eli) and context information hj . Then we get the context-enhanced source embedding for further decoding.\nγ = σ(wjhp + wkhf ) (15) hc = γhp + (1− γ)hf (16) λ = σ(wchc + wsH(e l i)) (17) H = λhc + (1− λ)H(eli) (18)\nwhere hc is the integration of all previous context hp and future context hf . The wj , wk, wc and ws are learnable parameter matrices, and H is the final representation."
    }, {
      "heading" : "3.2.3 Decoding and Loss",
      "text" : "The decoder is an auto-regressive transformer model to generate tokens one by one. We adopt the cross-entropy loss to minimize the negative loglikelihood over ground-truth words and apply the label smoothing strategy.\nL = − m∑ i=1 logP (yi | ei, v 6=i, e 6=i) (19)"
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and evaluation metrics",
      "text" : "We run our experiments on both Youcook2 dataset (Zhou et al., 2018a) and ActivityNet Caption dataset (Krishna et al., 2017). YouCook2 is the task-oriented instructional video dataset for video procedural captioning on the recipe domain. We follow the data partition in VideoBERT (Sun et al., 2019b) which uses 457 videos in the YouCook2 validation set as the testing set and the rest for development. In all, we use 1,278 videos for training and validation. We extract the visual feature by S3D model pre-trained on Howto100M(Miech et al., 2019) dataset through MIL-NCE(Miech et al., 2020) model. This visual representation is a better representation of Howto videos. The ASR transcript is automatically extracted from the off-theshelf recognition tool1.\nDifferent from the Youcook2 dataset, Activitynet captions are open-domain videos with overlapping proposals, while Youcook2 has non-overlapping event proposals. We apply the same data partition in (Iashin and Rahtu, 2020b) with the ground truth labels. We directly download the copy of the dataset in (Iashin and Rahtu, 2020b) which contains 9,167 (out of 10,009) training and 4,483 (out of 4,917) validation videos. The dataset only contains partially available videos (91%) due to no longer available Youtube links. To make a fair comparison, we only list the experimental results on the same dataset. This open-source code and data portal contains the speech content extracted from the closed captions (CC) from the YouTube ASR system.\nWe employ the metrics BLEU3, BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L(Lin and Och, 2004) and CIDEr(Vedantam et al., 2015) to evaluate the performance. We follow the work in (Iashin and Rahtu, 2020a) on ActivityNet caption dataset which reported BLEU3, BLEU4 and METEOR. We directly apply the open-source tool 2 to evaluate our results as in (Krishna et al., 2017).\n1https://azure.microsoft.com/ en-us/services/cognitive-services/ speech-to-text/\n2https://github.com/ ranjaykrishna/densevid_eval/tree/ 9d4045aced3d827834a5d2da3c9f0692e3f33c1c"
    }, {
      "heading" : "4.2 Implementation details",
      "text" : "We develop our model based on the open-source code 3 of MDVC(Iashin and Rahtu, 2020b), and will release our code later. The embedding size of video, hidden size of the multi-head, and feedforward layer are 1024, 512, and 128 respectively. The number of the head is 8 and the dropout rate is 0.4. We set the local context length kl as 10, that is, the 10 previous and 10 future frames as a local event-agnostic context, and one previous event and one next event as a global event-aware context for a trade-off between performance and efficiency. We adopt the Adam optimizer (Kingma and Ba, 2015) with learning rate of 1e-4, and set two momentum parameters β1= 0.9 and β2= 0.98. For label smoothing, and the smoothing rate is 0.4. We set the batch size to 128. For model complexity, the HCN model introduces only 3% more parameters to the base model. All models are trained on 1 Tesla P100 GPUs for 4 hours for Youcook2 and 30 hours for Activitynet Captions.\nVideo features We sampled frames at 16 fps and took the feature activations before the final\n3https://github.com/v-iashin/MDVC\nlinear classifier of the S3D backbone and applied 3D average pooling to obtain a 1024-dimension feature vector. We got 1 feature per second and set k to 80."
    }, {
      "heading" : "4.3 Compare with State-of-the-art results",
      "text" : "We demonstrate the results of our context-aware model on the Youcook2 dataset in Table 3. There are several existing baseline models: (1) Bi-LSTM with Temporal Attention (Bi-LSTM + TempoAttn) (Shou et al., 2016), which adopts Bi-LSTM language encoder; (2) End-to-End Masked Transformer (EMT) (Zhou et al., 2018b), an transformer based model; (3,4) VideoBERT (Sun et al., 2019b) and Contrastive Bidirectional Transformer (CBT) (Sun et al., 2019a), the pre-training based methods; (5) AT+Video (Hessel et al., 2019), the multimodal transformer method. Besides the work (Shou et al., 2016) using a recurrent network, other baseline methods adopted the transformer model. Our context-aware model achieves the best results for uni-modal video event captioning and outperforms the context-agnostic base model by a large margin. Furthermore, our HCN model with multimodal inputs can achieve comparable results with state-of-the-art results.\nWe list experimental results on a partial dataset of ActivityNet Captions as (Iashin and Rahtu, 2020b) and ignore others on the full dataset as (Krishna et al., 2017) to make a fair comparison. Table 2 presents the results of baseline methods and HCN. There are several baseline methods: (1) WLT (Rahman et al., 2019), a weakly supervised method with multi-modal input; (2) multi-modal video event captioning (MDVC) (Iashin and Rahtu, 2020b), a transformer-based model with multi-modal inputs; (3) BMT (Iashin and Rahtu, 2020a), a better use of\nvisual-audio information. Among these methods, WLT encoded the context using a recurrent network, while others are transformer models. HCN outperforms the base context-agnostic methods to a large extent and achieves state-of-the-art results.\nFrom both experimental results, we can see that our methods with context-aware information can improve the base context-agnostic model by a large margin for both unimodal or multi-modal input."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "We introduce the ablation study of the HCN model on the Youcook2 dataset. In our experiment, we use uni-modal input and illustrate the ablation results in Table 3. We remove one component at a time from the full HCN model to compare the performance. Type embedding: we remove the type embedding which is used to distinguish whether the input is source or context event. From the results, we can observe the performance drop by removing the type embedding. Past/Future context: we investigate the model with the only past context or future context and found that both past and future contexts are effective and complementary with each other. The model with the context in both directions achieves the best result. Cross attention gate: The GRU gate in the cross attention model is more effective than the simple combination, which shows that the GRU gate is better for modeling a sequential context. Local/global context: From the results in Table 3, we can see that the global context is more effective than the local context. The HCN model with both contexts outperforms all the models. Context length. 1) With regards to the local context, the results of 10 or 20 context frames are similar with CIDEr as 141.1 and 141.3 correspondingly, while the performance with 40 frames drops with CIDEr as 138. 2) For the global context, we have increased the number of previous and next events as the global context, but there is no further\nimprovement. We found that irrelevant events even bring noise or duplicated information to learn."
    }, {
      "heading" : "4.5 Qualitative Analysis",
      "text" : "We analyzed several cases and found two interesting videos shown in Figure 4 and 5. We depict the visual thumbnail, ground-truth caption, predicted results of our baseline and HCN methods.\nFrom the case in Figure 4, we can see that the baseline context-agnostic model generates the caption of each event solely leading to inconsistent captions. The baseline model predicts the ambiguous object as ”chicken” for event 1 with prior bias, but output the object as ”pork” for event 2. Our HCN model can tackle this issue and is prone to predict captions with a consistent object in the procedure. Besides, as shown in event 1, the entity ”pork” can also be learned from previous frames. The context-aware model is effective in resolving entity ambiguity and generating coherent captions.\nThe case in Figure 5 presents another challenge. Since the visual cue of the three events is very similar, the base context-agnostic model inevitably\npredicts the same caption as ”knead the dough”. The HCN model can learn the prior dependency between events, and hinder generating redundant sentences for similar events in the video. Therefore, the HCN model can generate the correct sentence for event 3. However, although the model tries to predict different captions for event 1, it is still hard to recognize the fine-grained entity ”salt” from the video, and all models predict the object by mistake. Fine-grained entity recognition from a video is still a challenging problem.\nTo sum up, from these cases we can see that, 1) the neighboring context can provide extra information to make an accurate and coherent prediction. 2) the HCN model can capture the temporal dependency between events as prior knowledge, and generate consistent and less duplicate captions between events. 3) fine-grained object recognition from a video is still a challenging problem. Visual coreference resolution (Kottur et al., 2018) can be the future work to tackle this problem."
    }, {
      "heading" : "5 Related Work",
      "text" : "Video Captioning The tasks mainly contain three types of captioning: single-sentence captioning (Xu et al., 2016; Wang et al., 2018b; Zhang et al., 2018), paragraph-level captioning (Yu et al., 2016; Lei et al., 2020; Ging et al., 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b). The difference between these tasks is whether to generate one or multiple sentences for the whole video or each separate event of the video. In this paper, we focus on the more challenging dense event-level video captioning task to generate descriptions for each event. Previous works (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a) mainly exploited recurrent neural models such as long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or recurrent unit (GRU) (Cho et al., 2014) to encode context. However, the recurrent model suffers from modeling long dependency effectively. Zhou et al. (Zhang et al., 2018; Sun et al., 2019b,a) introduced a self-attention model (Vaswani et al., 2017) which generates the caption based on the clip of each event solely. Compared with these works, we are the first to implement a novel video-level hierarchical context-aware network for dense video event captioning.\nMulti-modal Video Captioning Video natu-\nrally has multi-modal inputs including visual, speech text, and audio. Previous works explore visual RGB, motion, optical flow features, audio features (Hori et al., 2017; Wang et al., 2018b; Rahman et al., 2019) as well as speech text features (Shi et al., 2019; Hessel et al., 2019; Iashin and Rahtu, 2020b) for captioning. According to the work in (Shi et al., 2019; Hessel et al., 2019; Iashin and Rahtu, 2020b), although the speech text is noisy and informal, it can still capture better semantic features and improve performance especially for instructional videos. Later on, Lashin et al. (Iashin and Rahtu, 2020b) proposed to embed all visual, audio, and speech text for dense video event captioning. However, context-aware models are rarely investigated in multi-modal video event captioning. Therefore, we propose a novel attention model for effectively encoding the local and global context to tackle ambiguous object recognition and transcript co-reference through jointly modeling multi-modal inputs.\nContext-aware Language Generation Our work is inspired by context-aware language generation e.g. document-level neural machine translation (NMT) (Miculicich et al., 2018; Maruf et al., 2019; Ma et al., 2020). Miculicich et al. (Miculicich et al., 2018) adopted a hierarchical context-aware network in a structured and dynamic manner. Marcuf et al. (Maruf et al., 2019) and Ma (Ma et al., 2020) further explored a scalable and effective attention mechanism. For the local neighbor-clip and global inter-event context, we further design a hierarchical context-aware network with a hybrid mechanism of multi-modal video captioning to dynamically leverage various video-level information through a gating scalar."
    }, {
      "heading" : "6 Conclusion and Discussion",
      "text" : "Dense video event captioning is a typical video understanding task to learn procedural events in a long untrimmed video. It is essential to model holistic video information for event understanding. In this paper, we propose a novel hierarchical context-aware network to encode both the local and global context of long videos. Our HCN model is effective in modeling context and outperforms the context-agnostic model by a large margin.\nIn future work, we tend to extend our hierarchical network to further investigate how to effectively attend to the long context to filter ambiguous and irrelevant information."
    } ],
    "references" : [ {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Activitynet 2019 task 3: Exploring contexts for dense captioning events in videos",
      "author" : [ "Shizhe Chen", "Yuqing Song", "Yida Zhao", "Qin Jin", "Zhaoyang Zeng", "Bei Liu", "Jianlong Fu", "Alexander Hauptmann." ],
      "venue" : "arXiv preprint arXiv:1907.05092.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Coot: Cooperative hierarchical transformer for video-text representation learning",
      "author" : [ "Simon Ging", "Mohammadreza Zolfaghari", "Hamed Pirsiavash", "Thomas Brox." ],
      "venue" : "arXiv preprint arXiv:2011.00597.",
      "citeRegEx" : "Ging et al\\.,? 2020",
      "shortCiteRegEx" : "Ging et al\\.",
      "year" : 2020
    }, {
      "title" : "A case study on combining asr and visual features for generating instructional video captions",
      "author" : [ "Jack Hessel", "Bo Pang", "Zhenhai Zhu", "Radu Soricut." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL).",
      "citeRegEx" : "Hessel et al\\.,? 2019",
      "shortCiteRegEx" : "Hessel et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Attention-based multimodal fusion for video description",
      "author" : [ "Chiori Hori", "Takaaki Hori", "Teng-Yok Lee", "Ziming Zhang", "Bret Harsham", "John R Hershey", "Tim K Marks", "Kazuhiko Sumi." ],
      "venue" : "Proceedings of the IEEE international conference on com-",
      "citeRegEx" : "Hori et al\\.,? 2017",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2017
    }, {
      "title" : "A better use of audio-visual cues: Dense video captioning with bi-modal transformer",
      "author" : [ "Vladimir Iashin", "Esa Rahtu." ],
      "venue" : "arXiv preprint arXiv:2005.08271.",
      "citeRegEx" : "Iashin and Rahtu.,? 2020a",
      "shortCiteRegEx" : "Iashin and Rahtu.",
      "year" : 2020
    }, {
      "title" : "Multi-modal dense video captioning",
      "author" : [ "Vladimir Iashin", "Esa Rahtu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 958–959.",
      "citeRegEx" : "Iashin and Rahtu.,? 2020b",
      "shortCiteRegEx" : "Iashin and Rahtu.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Visual coreference resolution in visual dialog using neural module networks",
      "author" : [ "Satwik Kottur", "José MF Moura", "Devi Parikh", "Dhruv Batra", "Marcus Rohrbach." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 153–169.",
      "citeRegEx" : "Kottur et al\\.,? 2018",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2018
    }, {
      "title" : "Mart: Memoryaugmented recurrent transformer for coherent video paragraph captioning",
      "author" : [ "Jie Lei", "Liwei Wang", "Yelong Shen", "Dong Yu", "Tamara Berg", "Mohit Bansal." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly localizing and describing events for dense video captioning",
      "author" : [ "Yehao Li", "Ting Yao", "Yingwei Pan", "Hongyang Chao", "Tao Mei." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7492–7500.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "Chin-Yew Lin", "Franz Josef Och." ],
      "venue" : "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Lin and Och.,? 2004",
      "shortCiteRegEx" : "Lin and Och.",
      "year" : 2004
    }, {
      "title" : "A simple and effective unified encoder for documentlevel machine translation",
      "author" : [ "Shuming Ma", "Dongdong Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3505–3511.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Selective attention for contextaware neural machine translation",
      "author" : [ "Sameen Maruf", "André FT Martins", "Gholamreza Haffari." ],
      "venue" : "arXiv preprint arXiv:1903.08788.",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "arXiv preprint arXiv:1809.01576.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-End Learning of Visual Representations from Uncurated Instructional Videos",
      "author" : [ "Antoine Miech", "Jean-Baptiste Alayrac", "Lucas Smaira", "Ivan Laptev", "Josef Sivic", "Andrew Zisserman." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Miech et al\\.,? 2020",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2020
    }, {
      "title" : "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
      "author" : [ "Antoine Miech", "Dimitri Zhukov", "Jean-Baptiste Alayrac", "Makarand Tapaswi", "Ivan Laptev", "Josef Sivic." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Miech et al\\.,? 2019",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2019
    }, {
      "title" : "Streamlined dense video captioning",
      "author" : [ "Jonghwan Mun", "Linjie Yang", "Zhou Ren", "Ning Xu", "Bohyung Han." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6588–6597.",
      "citeRegEx" : "Mun et al\\.,? 2019",
      "shortCiteRegEx" : "Mun et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Watch, listen and tell: Multi-modal weakly supervised dense event captioning",
      "author" : [ "Tanzila Rahman", "Bicheng Xu", "Leonid Sigal." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 8908–8917.",
      "citeRegEx" : "Rahman et al\\.,? 2019",
      "shortCiteRegEx" : "Rahman et al\\.",
      "year" : 2019
    }, {
      "title" : "Dense procedure captioning in narrated instructional videos",
      "author" : [ "Botian Shi", "Lei Ji", "Yaobo Liang", "Nan Duan", "Peng Chen", "Zhendong Niu", "Ming Zhou." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Shi et al\\.,? 2019",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2019
    }, {
      "title" : "Temporal action localization in untrimmed videos via multi-stage cnns",
      "author" : [ "Zheng Shou", "Dongang Wang", "Shih-Fu Chang." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1049–1058.",
      "citeRegEx" : "Shou et al\\.,? 2016",
      "shortCiteRegEx" : "Shou et al\\.",
      "year" : 2016
    }, {
      "title" : "Contrastive bidirectional transformer for temporal representation learning",
      "author" : [ "Chen Sun", "Fabien Baradel", "Kevin Murphy", "Cordelia Schmid." ],
      "venue" : "arXiv preprint arXiv:1906.05743.",
      "citeRegEx" : "Sun et al\\.,? 2019a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Videobert: A joint model for video and language representation learning",
      "author" : [ "Chen Sun", "Austin Myers", "Carl Vondrick", "Kevin Murphy", "Cordelia Schmid." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision.",
      "citeRegEx" : "Sun et al\\.,? 2019b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Bidirectional attentive fusion with context gating for dense video captioning",
      "author" : [ "Jingwen Wang", "Wenhao Jiang", "Lin Ma", "Wei Liu", "Yong Xu." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7190–7198.",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Watch, listen, and describe: Globally and locally aligned cross-modal attentions for video captioning",
      "author" : [ "Xin Wang", "Yuan-Fang Wang", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1804.05448.",
      "citeRegEx" : "Wang et al\\.,? 2018b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Msrvtt: A large video description dataset for bridging video and language",
      "author" : [ "Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288–5296.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Video paragraph captioning using hierarchical recurrent neural networks",
      "author" : [ "Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4584–4593.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Crossmodal and hierarchical modeling of video and text",
      "author" : [ "Bowen Zhang", "Hexiang Hu", "Fei Sha." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 374–390.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards automatic learning of procedures from web instructional videos",
      "author" : [ "Luowei Zhou", "Chenliang Xu", "Jason J Corso." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhou et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end dense video captioning with masked transformer",
      "author" : [ "Luowei Zhou", "Yingbo Zhou", "Jason J Corso", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739–8748.",
      "citeRegEx" : "Zhou et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "(Iashin and Rahtu, 2020b) aims to generate a sequence of captions for all events regarding to uni-modality (video) or multi-modality (video + speech) inputs.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "From speech text perspective, although the speech text offers semantic concepts (Shi et al., 2019; Iashin and Rahtu, 2020b), it brings another challenge of co-reference and ellipsis in speech text due to the informal utterance of oral speeches.",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "From speech text perspective, although the speech text offers semantic concepts (Shi et al., 2019; Iashin and Rahtu, 2020b), it brings another challenge of co-reference and ellipsis in speech text due to the informal utterance of oral speeches.",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : ", 2019b,a) is becoming the new paradigm for dense video event captioning and effective for multi-modal video captioning (Shi et al., 2019; Iashin and Rahtu, 2020b).",
      "startOffset" : 120,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : ", 2019b,a) is becoming the new paradigm for dense video event captioning and effective for multi-modal video captioning (Shi et al., 2019; Iashin and Rahtu, 2020b).",
      "startOffset" : 120,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Similar to the work in (Hessel et al., 2019), we build the vocabulary using all tokens in the captioning sentence.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "of embedding: 1) visual feature f(vi) (and speech text feature f(ti) if available); 2) position embedding p(vi) and p(ti) as introduced in the transformer model(Vaswani et al., 2017); 3) type embedding s(vi) and s(ti) representing whether the current embedding is from context or source.",
      "startOffset" : 160,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Specifically, the LCM adopts a flat attention model similar to (Ma et al., 2020) to enhance the source video feature by local context.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Besides, given multi-modal inputs, LCM is a general model to fuse both the visual features f(vi) and the text features f(ti) inside the event with one unified transformer as in (Hessel et al., 2019); 2) we further employ a global context module (GCM) to make the source event to interact with other event features flexibly.",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "The flat transformer in (Ma et al., 2020) is effective for encoding contextual information with full interaction be-",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "In addition, when the speech text is available for multi-modal video captioning, this flat encoder can also perform the fusion of visual and text modalities, which is similar to (Hessel et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 27,
      "context" : "where [;] is concatenation operation, FFN means the feed-forward network and MultiHead is the multi-head attention network in transformer(Vaswani et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "Different from LCM, which applies a unified transformer to encode a short context, GCM exploits a cross attention model similar to (Maruf et al., 2019) to encode long global context efficiently.",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 15,
      "context" : "We exploit the GCM for each contextual event and then combine all the encoding through a context gating mechanism similar to (Maruf et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Next, we adopt a gated recurrent unit (GRU) (Cho et al., 2014) to selectively update the source feature with context enhanced feature which is shown to be effective in our ablation study.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "Context Gating We adopt the gate in (Tu et al., 2018) to regulate the source H(ei) and context information hj .",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "We run our experiments on both Youcook2 dataset (Zhou et al., 2018a) and ActivityNet Caption dataset (Krishna et al.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : "follow the data partition in VideoBERT (Sun et al., 2019b) which uses 457 videos in the YouCook2 validation set as the testing set and the rest for development.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "by S3D model pre-trained on Howto100M(Miech et al., 2019) dataset through MIL-NCE(Miech et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : ", 2019) dataset through MIL-NCE(Miech et al., 2020) model.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "We apply the same data partition in (Iashin and Rahtu, 2020b) with the ground truth labels.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "We directly download the copy of the dataset in (Iashin and Rahtu, 2020b) which",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "We employ the metrics BLEU3, BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L(Lin and Och, 2004) and CIDEr(Vedantam et al.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L(Lin and Och, 2004) and CIDEr(Vedantam et al.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L(Lin and Och, 2004) and CIDEr(Vedantam et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 28,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L(Lin and Och, 2004) and CIDEr(Vedantam et al., 2015) to evaluate the performance.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "We follow the work in (Iashin and Rahtu, 2020a) on ActivityNet caption dataset which reported BLEU3, BLEU4 and METEOR.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "2009 Methods V/T B-3 B-4 M R-L CIDEr Bi-LSTM + TempoAttn (Shou et al., 2016) V - 0.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "50 49 VideoBERT (+S3D feature)(Sun et al., 2019b) V 7.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Transformer(w/o context) is the base method similar to (Hessel et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "We develop our model based on the open-source code 3 of MDVC(Iashin and Rahtu, 2020b), and will release our code later.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "We adopt the Adam optimizer (Kingma and Ba, 2015) with learning rate of 1e-4, and set two momentum parameters β1= 0.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "are several existing baseline models: (1) Bi-LSTM with Temporal Attention (Bi-LSTM + TempoAttn) (Shou et al., 2016), which adopts Bi-LSTM language encoder; (2) End-to-End Masked Transformer (EMT) (Zhou et al.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : ", 2016), which adopts Bi-LSTM language encoder; (2) End-to-End Masked Transformer (EMT) (Zhou et al., 2018b), an transformer",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "based model; (3,4) VideoBERT (Sun et al., 2019b) and Contrastive Bidirectional Transformer (CBT) (Sun et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : ", 2019b) and Contrastive Bidirectional Transformer (CBT) (Sun et al., 2019a), the pre-training based methods; (5) AT+Video (Hessel et al.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : ", 2019a), the pre-training based methods; (5) AT+Video (Hessel et al., 2019), the multimodal transformer method.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "(Shou et al., 2016) using a recurrent network, other baseline methods adopted the transformer model.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "We list experimental results on a partial dataset of ActivityNet Captions as (Iashin and Rahtu, 2020b) and ignore others on the full dataset as (Krishna et al.",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "There are several baseline methods: (1) WLT (Rahman et al., 2019), a weakly supervised method with multi-modal input; (2) multi-modal video event captioning (MDVC) (Iashin and Rahtu, 2020b), a transformer-based model with multi-modal inputs; (3) BMT (Iashin and Rahtu, 2020a), a better use of",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : ", 2019), a weakly supervised method with multi-modal input; (2) multi-modal video event captioning (MDVC) (Iashin and Rahtu, 2020b), a transformer-based model with multi-modal inputs; (3) BMT (Iashin and Rahtu, 2020a), a better use of",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : ", 2019), a weakly supervised method with multi-modal input; (2) multi-modal video event captioning (MDVC) (Iashin and Rahtu, 2020b), a transformer-based model with multi-modal inputs; (3) BMT (Iashin and Rahtu, 2020a), a better use of",
      "startOffset" : 192,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "coreference resolution (Kottur et al., 2018) can be the future work to tackle this problem.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "2018), paragraph-level captioning (Yu et al., 2016; Lei et al., 2020; Ging et al., 2020) and event-level captioning (Krishna et al.",
      "startOffset" : 34,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "2018), paragraph-level captioning (Yu et al., 2016; Lei et al., 2020; Ging et al., 2020) and event-level captioning (Krishna et al.",
      "startOffset" : 34,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "2018), paragraph-level captioning (Yu et al., 2016; Lei et al., 2020; Ging et al., 2020) and event-level captioning (Krishna et al.",
      "startOffset" : 34,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b).",
      "startOffset" : 35,
      "endOffset" : 151
    }, {
      "referenceID" : 29,
      "context" : ", 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b).",
      "startOffset" : 35,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : ", 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b).",
      "startOffset" : 35,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : ", 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b).",
      "startOffset" : 35,
      "endOffset" : 151
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and event-level captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a; Mun et al., 2019; Chen et al., 2019; Zhou et al., 2018b).",
      "startOffset" : 35,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "Previous works (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a) mainly exploited recurrent neural models such as long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or recurrent unit (GRU) (Cho et al.",
      "startOffset" : 15,
      "endOffset" : 74
    }, {
      "referenceID" : 29,
      "context" : "Previous works (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018a) mainly exploited recurrent neural models such as long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or recurrent unit (GRU) (Cho et al.",
      "startOffset" : 15,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : ", 2018a) mainly exploited recurrent neural models such as long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or recurrent unit (GRU) (Cho et al.",
      "startOffset" : 96,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : ", 2018a) mainly exploited recurrent neural models such as long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or recurrent unit (GRU) (Cho et al., 2014) to encode context.",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 27,
      "context" : ", 2019b,a) introduced a self-attention model (Vaswani et al., 2017) which generates the caption based on the clip of each event solely.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Previous works explore visual RGB, motion, optical flow features, audio features (Hori et al., 2017; Wang et al., 2018b; Rahman et al., 2019) as well as speech text features (Shi et al.",
      "startOffset" : 81,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "Previous works explore visual RGB, motion, optical flow features, audio features (Hori et al., 2017; Wang et al., 2018b; Rahman et al., 2019) as well as speech text features (Shi et al.",
      "startOffset" : 81,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Previous works explore visual RGB, motion, optical flow features, audio features (Hori et al., 2017; Wang et al., 2018b; Rahman et al., 2019) as well as speech text features (Shi et al.",
      "startOffset" : 81,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "According to the work in (Shi et al., 2019; Hessel et al., 2019; Iashin and Rahtu, 2020b), although the speech text is noisy and informal, it can still capture better semantic features and improve performance especially for instructional videos.",
      "startOffset" : 25,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "According to the work in (Shi et al., 2019; Hessel et al., 2019; Iashin and Rahtu, 2020b), although the speech text is noisy and informal, it can still capture better semantic features and improve performance especially for instructional videos.",
      "startOffset" : 25,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "According to the work in (Shi et al., 2019; Hessel et al., 2019; Iashin and Rahtu, 2020b), although the speech text is noisy and informal, it can still capture better semantic features and improve performance especially for instructional videos.",
      "startOffset" : 25,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "(Miculicich et al., 2018) adopted a hierarchical context-aware network in a structured and dynamic manner.",
      "startOffset" : 0,
      "endOffset" : 25
    } ],
    "year" : 2021,
    "abstractText" : "Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events. In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture context from various aspects. In detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions. The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events. According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets, the videolevel HCN model outperforms the event-level context-agnostic model by a large margin. The code is available at https://github.com/ KirkGuo/HCN.",
    "creator" : "LaTeX with hyperref"
  }
}