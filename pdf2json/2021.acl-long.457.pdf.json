{
  "name" : "2021.acl-long.457.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining",
    "authors" : [ "Taolin Zhang", "Zerui Cai", "Chengyu Wang", "Minghui Qiu", "Bite Yang", "Xiaofeng He" ],
    "emails" : [ "zhangtl0519@gmail.com,", "flow@126.com,", "yangbt@dxy.cn", "minghui.qmh}@alibaba-inc.com,", "hexf@cs.ecnu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5882–5893\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5882"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained Language Models (PLMs) learn effective context representations with self-supervised tasks, spotlighting in various NLP tasks (Wang et al., 2019a; Nan et al., 2020; Liu et al., 2020a). In addition, Knowledge-Enhanced PLMs (KEPLMs) (Zhang et al., 2019; Liu et al., 2020b; Wang et al., 2019b) further benefit language understanding by\n∗Corresponding author. 1The code and pre-trained models will be available at\nhttps://github.com/MatNLP/SMedBERT.\ngrounding these PLMs with high-quality, humancurated knowledge facts, which are difficult to learn from raw texts.\nIn the literatures, a majority of KEPLMs (Zhang et al., 2020a; Hayashi et al., 2020; Sun et al., 2020) inject information of entities corresponding to mention-spans from Knowledge Graphs (KGs) into contextual representations. However, those KEPLMs only utilize linked-entity in the KGs as auxiliary information, which pay little attention to the neighboring structured semantics information of the entity linked with text mentions. In the medical context, there exist complicated domain knowledge such as relations and medical facts among medical terms (Rotmensch et al., 2017; Li et al., 2020), which are difficult to model using previous approaches. To address this issue, we consider leveraging structured semantics knowledge in medical KGs from the two aspects. (1) Rich semantic information from neighboring structures of linked-entities, such as entity types and relations, are highly useful for medical text understanding. As in Figure 1, “新型冠状病毒” (novel coronavirus) can be the cause of many diseases, such as “肺炎” (pneumonia) and “呼吸综合征”\n(respiratory syndrome). 2 (2) Additionally, we leverage neighbors of linked-entity as global “contexts” to complement plain-text contexts used in (Mikolov et al., 2013a; Pennington et al., 2014). The structure knowledge contained in neighbouring entities can act as the “knowledge bridge” between mention-spans, facilitating the interaction of different mention representations. Hence, PLMs can learn better representations for rare medical terms.\nIn this paper, we introduce SMedBERT, a KEPLM pre-trained over large-scale medical corpora and medical KGs. To the best of our knowledge, SMedBERT is the first PLM with structured semantics knowledge injected in the medical domain. Specifically, the contributions of SMedBERT mainly include two modules: Mention-neighbor Hybrid Attention: We fuse the embeddings of the node and type of linkedentity neighbors into contextual target mention representations. The type-level and node-level attentions help to learn the importance of entity types and the neighbors of linked-entity, respectively, in order to reduce the knowledge noise injected into the model. The type-level attention transforms the homogeneous node-level attention into a heterogeneous learning process of neighboring entities. Mention-neighbor Context Modeling: We propose two novel self-supervised learning tasks for promoting interaction between mention-span and corresponding global context, namely masked neighbor modeling and masked mention modeling. The former enriches the representations of “context” neighboring entities based on the well trained “target word” mention-span, while the latter focuses on gathering those information back from neighboring entities to the masked target like low-frequency mention-span which is poorly represented (Turian et al., 2010).\nIn the experiments, we compare SMedBERT against various strong baselines, including mainstream KEPLMs pre-trained over our medical resources. The underlying medical NLP tasks include: named entity recognition, relation extraction, question answering, question matching and natural language inference. The results show that SMedBERT consistently outperforms all the baselines on these tasks.\n2Although we focus on Chinese medical PLMs here. The proposed method can be easily adapted to other languages, which is beyond the scope of this work."
    }, {
      "heading" : "2 Related Work",
      "text" : "PLMs in the Open Domain. PLMs have gained much attention recently, proving successful for boosting the performance of various NLP tasks (Qiu et al., 2020). Early works on PLMs focus on feature-based approaches to transform words into distributed representations (Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018). BERT (Devlin et al., 2019) (as well as its robustly optimized version RoBERTa (Liu et al., 2019b)) employs bidirectional transformer encoders (Vaswani et al., 2017) and self-supervised tasks to generate context-aware token representations. Further improvement of performances mostly based on the following three types of techniques, including self-supervised tasks (Joshi et al., 2020), transformer encoder architectures (Yang et al., 2019) and multi-task learning (Liu et al., 2019a). Knowledge-Enhanced PLMs. As existing BERTlike models only learn knowledge from plain corpora, various works have investigated how to incorporate knowledge facts to enhance the language understanding abilities of PLMs. KEPLMs are mainly divided into the following three types. (1) Knowledge-enhanced by Entity Embedding: ERNIE-THU (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) inject linked-entity as heterogeneous features learned by KG embedding algorithms such as TransE (Bordes et al., 2013). (2) Knowledge-enhanced by Entity Description: EBERT (Zhang et al., 2020a) and KEPLER (Wang et al., 2019b) add extra description text of entities to enhance semantic representation. (3) Knowledgeenhanced by Triplet Sentence: K-BERT (Liu et al., 2020b) and CoLAKE (Sun et al., 2020) convert triplets into sentences and insert them into the training corpora without pre-trained embedding. Previous studies on KG embedding (Nguyen et al., 2016; Schlichtkrull et al., 2018) have shown that utilizing the surrounding facts of entity can obtain more informative embedding, which is the focus of our work. PLMs in the Medical Domain. PLMs in the medical domain can be generally divided into three categories. (1) BioBERT (Lee et al., 2020), BlueBERT (Peng et al., 2019), SCIBERT (Beltagy et al., 2019) and ClinicalBert (Huang et al., 2019) apply continual learning on medical domain texts, such as PubMed abstracts, PMC full-text articles and MIMIC-III clinical notes. (2) PubMedBERT\n(Gu et al., 2020) learns weights from scratch using PubMed data to obtain an in-domain vocabulary, alleviating the out-of-vocabulary (OOV) problem. This training paradigm needs the support of largescale domain data and resources. (3) Some other PLMs use domain self-supervised tasks for pretraining. For example, MC-BERT (Zhang et al., 2020b) masks Chinese medical entities and phrases to learn complex structures and concepts. DiseaseBERT (He et al., 2020) leverages the medical terms and its category as the labels to pre-train the model. In this paper, we utilize both domain corpora and neighboring entity triplets of mentions to enhance the learning of medical language representations."
    }, {
      "heading" : "3 The SMedBERT Model",
      "text" : ""
    }, {
      "heading" : "3.1 Notations and Model Overview",
      "text" : "In the PLM, we denote the hidden feature of each token {w1, ..., wN} as {h1, h2, ..., hN} where N is the maximum input sequence length and the total number of pre-training samples as M . Let E be the set of mention-span em in the training corpora. Furthermore, the medical KG consists of the entities set E and the relations set R. The triplet set is S = {(h, r, t) | h ∈ E , r ∈ R, t ∈ E}, where h is the head entity with relation r to the tail entity t. The embeddings of entities and relations trained on KG by TransR (Lin et al., 2015) are represented as Γent and Γrel, respectively. The neighboring entity set recalled from KG by em is denoted as Nem = {e1m, e2m, ..., eKm} where K is the threshold of our PEPR algorithm. We denote the number of\nentities in the KG as Z. The dimensions of the hidden representation in PLM and the KG embeddings are d1 and d2, respectively.\nThe main architecture of the our model is shown in Figure 2. SMedBERT mainly includes three components: (1) Top-K entity sorting determine which K neighbour entities to use for each mention. (2) Mention-neighbor hybrid attention aims to infuse the structured semantics knowledge into encoder layers, which includes type attention, node attention and gated position infusion module. (3) Mention-neighbor context modeling includes masked neighbor modeling and masked mention modeling aims to promote mentions to leverage and interact with neighbour entities."
    }, {
      "heading" : "3.2 Top-K Entity Sorting",
      "text" : "Previous research shows that simple neighboring entity expansion may induce knowledge noises during PLM training (Wang et al., 2019a). In order to recall the most important neighboring entity set from the KG for each mention, we extend the Personalized PageRank (PPR) (Page et al., 1999) algorithm to filter out trivial entities. 3 Recall that the iterative process in PPR is Vi = (1−α)A·Vi−1+αP where A is the normalized adjacency matrix, α is the damping factor, P is uniformly distributed jump probability vector, and V is the iterative score vector for each entity.\nPEPR specifically focuses on learning the weight for the target mention span in each iteration. It\n3We name our algorithm to be Personalized Entity PageRank, abbreviated as PEPR.\nassigns the span em a higher jump probability 1 in P with the remaining as 1Z . It also uses the entity frequency to initialize the score vector V :\nVem = { tem T em ∈ E 1 M em /∈ E\n(1)\nwhere T is the sum of frequencies of all entities. tem is the frequency of em in the corpora. After sorting, we select the top-K entity set Nem ."
    }, {
      "heading" : "3.3 Mention-neighbor Hybrid Attention",
      "text" : "Besides the embeddings of neighboring entities, SMedBERT integrates the type information of medical entities to further enhance semantic representations of mention-span."
    }, {
      "heading" : "3.3.1 Neighboring Entity Type Attention",
      "text" : "Different types of neighboring entities may have different impacts. Given a specific mention-span em, we compute the neighboring entity type attention. Concretely, we calculate hidden representation of each entity type τ as hτ = ∑ eim∈Eτm heim . Eτm are neighboring entities of em with the same type τ and heim = Γent ( eim ) ∈ Rd2 .\nh′em = LN (σ (fsp (hi, . . . , hj)Wbe)) (2)\nwhere fsp is the self-attentive pooling (Lin et al., 2017) to generate the mention-span representation hem ∈ Rd1 and the (hi, hi+1, . . . , hj) is the hidden representation of tokens (wi, wi+1, . . . , wj) in mention-span em trained by PLMs. h′em ∈ R d2 is obtained by σ(·) non-linear activation function GELU (Hendrycks and Gimpel, 2016) and the learnable projection matrix Wbe ∈ Rd1×d2 . LN is the LayerNorm function (Ba et al., 2016). Then, we calculate the each type attention weight using the type representation hτ ∈ Rd2 and the transformed mention-span representation h′em :\nα′τ = tanh ( h′emWt + hτWt′ ) Wa (3)\nwhere Wt ∈ Rd2×d2 , Wt′ ∈ Rd2×d2 and Wa ∈ Rd2×1. Finally, the neighboring entity type attention weights ατ are obtained by normalizing the attention score α′τ among all entity types T ."
    }, {
      "heading" : "3.3.2 Neighboring Entity Node Attention",
      "text" : "Apart from entity type information, different neighboring entities also have different influences. Specifically, we devise the neighboring entity node\nattention to capture the different semantic influences from neighboring entities to the target mention span and reduce the effect of noises. We calculate the entity node attention using the mentionspan representation h′em and neighboring entities representation heim with entity type τ as:\nβ′emeim =\n( h′emWq ) ( heimWk )T √ d2 ατ (4)\nβemeim = exp\n( β′ emeim ) ∑\neim∈Nem exp ( β′ emeim ) (5) where Wq ∈ Rd2×d2 and Wk ∈ Rd2×d2 are the attention weight matrices.\nThe representations of all neighboring entities in Nem are aggregated to h̄′em ∈ R d2 :\nĥ′em = ∑\neim∈Nem\nβemeim ( heimWv + bv ) (6)\nh̄′em = LN ( ĥ′em + ( σ ( ĥ′emWl1 + bl1 ) Wl2 )) (7)\nwhere Wv ∈ Rd2×d2 , Wl1 ∈ Rd2×4d2 , Wl2 ∈ R4d2×d2 . bv ∈ Rd2 and bl1 ∈ R4d2 are the bias vectors. h̄′em is the mention-neighbor representation from hybrid attention module."
    }, {
      "heading" : "3.3.3 Gated Position Infusion",
      "text" : "Knowledge-injected representations may divert the texts from its original meanings. We further reduce knowledge noises via gated position infusion:\nh′emf = σ ([ h̄′em ‖ h ′ em ] Wmf + bmf ) (8)\nh̃′emf = LN (h ′ emf Wbp + bbp) (9)\nwhere Wmf ∈ R2d2×2d2 , Wbp ∈ R2d2×d1 , bmf ∈ R2d2 , bbp ∈ Rd1 . h′emf ∈ R\n2d2 is the span-level infusion representation. “‖” means concatenation operation. h̃′emf ∈ R\nd1 is the final knowledgeinjected representation for mention em. We generate the output token representation hif by 4:\ngi = tanh (([ hi ‖ h̃′emf ]) Wug + bug ) (10)\nhif = σ (([ hi ‖ gi ∗ h̃′emf ]) Wex + bex ) + hi\n(11)\nwhere Wug, Wex ∈ R2d1×d1 . bug, bex ∈ Rd1 . “∗” means element-wise multiplication.\n4We find that restricting the knowledge infusion position to tokens is helpful to improve performance."
    }, {
      "heading" : "3.4 Mention-neighbor Context Modeling",
      "text" : "To fully exploit the structured semantics knowledge in KG, we further introduce two novel selfsupervised pre-training tasks, namely Masked Neighbor Modeling (MNeM) and Masked Mention Modeling (MMeM)."
    }, {
      "heading" : "3.4.1 Masked Neighbor Modeling",
      "text" : "Formally, let r be the relation between the mentionspan em and a neighboring entity eim:\nhmf = LN (σ (fsp (hif , . . . , hjf )Wsa)) (12)\nwhere hmf is the mention-span hidden features based on the tokens hidden representation( hif , h(i+1)f , . . . , hjf ) . hr = Γrel (r) ∈ Rd2 is the relation r representation and Wsa ∈ Rd1×d2 is a learnable projection matrix. The goal of MNeM is leveraging the structured semantics in surrounding entities while reserving the knowledge of relations between entities. Considering the object functions of skip-gram with negative sampling (SGNS) (Mikolov et al., 2013a) and score function of TransR (Lin et al., 2015):\nLS = log fs(w, c) + k · Ecn∼PD [log fs(w,−cn)] (13)\nftr(h, r, t) =‖ hMr + r − tMr ‖ (14)\nwhere thew inLS is the target word of context c. fs is the compatibility function measuring how well the target word is fitted into the context. Inspired by SGNS, following the general energy-based framework (LeCun et al., 2006), we treat mention-spans in corpora as “target words”, and neighbors of corresponding entities in KG as “contexts” to provide additional global contexts. We employ the Sampled-Softmax (Jean et al., 2015) as the criterion LMNeM for the mention-span em:∑ Nem log exp(fs(θ)) exp(fs(θ)) +K · Een∼Q(en)[exp(fs(θ′))]\n(15)\nwhere θ denotes the triplet (em, r, eim), e i m ∈ Nem . θ′ is the negative triplets (em, r, en), and en is negative entity sampled with Q(eim) detailed in Appendix B. To keep the knowledge of relations between entities, we define the compatibility function as:\nfs ( em, r, e i m ) =\nhmfMr + hr ||hmfMr + hr|| · (heimMr)\nT\n||heimMr|| µ\n(16)\nwhere µ is a scale factor. Assuming the norms of both hmfMr + hr and heimMr are 1,we have:\nfs ( em, r, e i m ) = µ ⇐⇒ ftr(hmf , hr, heim) = 0\n(17) which indicates the proposed fs is equivalence with ftr. Because | henMr | needs to be calculated for each en, the computation of the score function fs is costly. Hence, we transform part of the formula fs as follows:\n(hmfMr + hr) · (henMr) T =[ hmf 1 ] [ Mr\nhr ] [ Mr hr ]T [ hen 0 ]T = [ hmf 1 ] MPr [ hen 0\n]T (18)\nIn this way, we eliminate computation of transforming each hen . Finally, to compensate the offset introduced by the negative sampling function Q(eim) (Jean et al., 2015), we complement fs(em, r, eim) as:[\nhmf 1 ] MPr\n‖ [ hmf 1 ] MPr ‖\n· [ heim 0 ] ‖ heim ‖ µ−µ logQ(eim)\n(19)"
    }, {
      "heading" : "3.4.2 Masked Mention Modeling",
      "text" : "In contrast to MNeM, MMeM transfers the semantic information in neighboring entities back to the masked mention em.\nYm = LN (σ (fsp (hip, . . . , hjp)Wsa)) (20)\nwhere Ym is the ground-truth representation of em and hip = Γp(wi) ∈ Rd2 . Γp is the pre-trained embedding of BERT in our medical corpora. The mention-span representation obtained by our model is hmf . For a sample s, the loss of MMeM LMMeM is calculated via Mean-Squared Error:\nLMMeM = Ms∑ mi ‖ hmif − Ymi ‖ 2 (21)\nwhereMs is the set of mentions of sample s."
    }, {
      "heading" : "3.5 Training Objective",
      "text" : "In SMedBERT, the training objectives mainly consist of three parts, including the self-supervised loss proposed in previous works and the mentionneighbor context modeling loss proposed in our work. Our model can be applied to medical text pre-training directly in different languages as long\nas high-quality medical KGs can be obtained. The total loss is as follows:\nLtotal = LEX + λ1LMNeM + λ2LMMeM (22)\nwhere LEX is the sum of sentence-order prediction (SOP) (Lan et al., 2020) and masked language modeling. λ1 and λ2 are the hyperparameters."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data Source",
      "text" : "Pre-training Data. The pre-training corpora after pre-processing contains 5,937,695 text segments with 3,028,224,412 tokens (4.9 GB). The KGs embedding trained by TransR (Lin et al., 2015) on two trusted data sources, including the SymptomIn-Chinese from OpenKG5 and DXY-KG 6 containing 139,572 and 152,508 entities, respectively. The number of triplets in the two KGs are 1,007,818 and 3,764,711. The pre-training corpora and the KGs are further described in Appendix A.1. Task Data. We use four large-scale datasets in ChineseBLUE (Zhang et al., 2020b) to evaluate our model, which are benchmark of Chinese medical NLP tasks. Additionally, we test models on four datasets from real application scenarios provided by DXY company 7 and CHIP 8, i.e., Named Entity Recognition (DXY-NER), Relation Extraction (DXY-RE, CHIP-RE) and Question Answer (WebMedQA (He et al., 2019)). For other information of the downstream datasets, we refer readers to Appendix A.2."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "In this work, we compare SMedBERT with general PLMs, domain-specific PLMs and KEPLMs with knowledge embedding injected, pre-trained on our Chinese medical corpora: General PLMs: We use three Chinese BERT-style models, namely BERT-base (Devlin et al., 2019), BERT-wwm (Cui et al., 2019) and RoBERTa (Liu et al., 2019b). All the weights are initialized from (Cui et al., 2020). Domain-specific PLMs: As very few PLMs in the Chinese medical domain are available, we consider the following models. MC-BERT (Zhang et al.,\n5http://www.openkg.cn/dataset/ symptom-in-chinese\n6https://portal.dxy.cn/ 7https://auth.dxy.cn/accounts/login 8http://www.cips-chip.org.cn:8088/home\n2020b) is pre-trained over a Chinese medical corpora via masking different granularity tokens. We also pre-train BERT using our corpora, denoted as BioBERT-zh. KEPLMs: We employ two SOTA KEPLMs continually pre-trained on our medical corpora as our baseline models, including ERNIE-THU (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). For a fair comparison, KEPLMs use other additional resources rather than the KG embedding are excluded (See Section 2), and all the baseline KEPLMs are injected by the same KG embedding.\nThe detailed parameter settings and training procedure are in Appendix B."
    }, {
      "heading" : "4.3 Intrinsic Evaluation",
      "text" : "To evaluate the semantic representation ability of SMedBERT, we design an unsupervised semantic similarity task. Specifically, we extract all entities pairs with equivalence relations in KGs as positive pairs. For each positive pair, we use one of the entity as query entity while the other as positive candidate, which is used to sample other entities as negative candidates. We denote this dataset as D1. Besides, the entities in the same positive pair often have many neighbours in common. We select positive pairs with large proportions of common neighbours as D2. Additionally, to verify the ability of SMedBERT of enhancing the low-frequency mention representation, we extract all positive pairs that with at least one low-frequency mention as D3. There are totally 359,358, 272,320 and 41,583 samples for D1, D2, D3 respectively. We describe the\ndetails of collecting data and embedding words in Appendix C. In this experiments, we compare SMedBERT with three types of models: classical word embedding methods (SGNS (Mikolov et al., 2013a), GLOVE (Pennington et al., 2014)), PLMs and KEPLMs. We compute the similarity between the representation of query entities and all the other entities, retrieving the most similar one. The evaluation metric is top-1 accuracy (Acc@1).\nExperiment results are shown in Table 1. From the results, we observe that: (1) SMedBERT greatly outperforms all baselines especially on the dataset D2 (+1.36%), where most positive pairs have many shared neighbours, demonstrating that ability of SMedBERT to utilize semantic information from the global context. (2) In dataset D3, SMedBERT improve the performance significantly (+1.01%), indicating our model is effective to enhance the representation of low-frequency mentions."
    }, {
      "heading" : "4.4 Results of Downstream Tasks",
      "text" : "We first evaluate our model in NER and RE tasks that are closely related to entities in the input texts.\nTable 2 shows the performances on medical NER and RE tasks. In NER and RE tasks, we can observe from the results: (1) Compared with PLMs trained in open-domain corpora, KEPLMs with medical corpora and knowledge facts achieve better results. (2) The performance of SMedBERT is greatly improved compared with the strongest baseline in two NER datasets (+0.88%, +2.07%), and (+0.68%, +0.92%) on RE tasks. We also evaluate SMedBERT on QA, QM and NLI tasks and the performance is shown in Table 3. We can observe that SMedBERT improve the performance consistently on these datasets (+0.90% on QA, +0.89% on QM and +0.63% on NLI). In general, it can be seen from Table 2 and Table 3 that injecting the domain knowledge especially the structured semantics knowledge can improve the result greatly."
    }, {
      "heading" : "4.5 Influence of Entity Hit Ratio",
      "text" : "In this experiment, we explore the model performance in NER and RE tasks with different entity hit ratios, which control the proportions of knowledgeenhanced mention-spans in the samples. The aver-\nage number of mention-spans in samples is about 40. Figure 3 illustrates the performance of SMedBERT and ERNIE-med (Zhang et al., 2019). From the result, we can observe that: (1) The performance improves significantly at the beginning and then keeps stable as the hit ratio increases, proving the heterogeneous knowledge is beneficial to improve the ability of language understanding and indicating too much knowledge facts are unhelpful to further improve model performance due to the knowledge noise (Liu et al., 2020b). (2) Compared with previous approaches, our SMedBERT model improves performance greatly and more stable."
    }, {
      "heading" : "4.6 Influence of Neighboring Entity Number",
      "text" : "We further evaluate the model performance under different K over the test set of DXY-NER and DXY-RE. Figure 4 shows the the model result with K = {5, 10, 20, 30}. In our settings, the SMedBERT can achieve the best performance in different tasks around K = 10. The results of SMedBERT show that the model performance increasing first and then decreasing with the increasing of K. This phenomenon also indicates the knowledge noise problem that injecting too much knowledge of neighboring entities may hurt the performance."
    }, {
      "heading" : "4.7 Ablation Study",
      "text" : "In Table 4, we choose three important model components for our ablation study and report the test\nset performance on four datasets of NER and RE tasks that are closely related to entities. Specifically, the three model components are neighboring entity type attention, the whole hybrid attention module, and mention-neighbor context modeling respectively, which includes two masked language model loss LMNeM and LMMeM.\nFrom the result, we can observe that: (1) Without any of the three mechanisms, our model performance can also perform competitively with the strong baseline ERNIE-med (Zhang et al., 2019). (2) Note that after removing the hybrid attention module, the performance of our model has the greatest decline, which indicates that injecting rich heterogeneous knowledge of neighboring entities is effective."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we address medical text mining tasks with the structured semantics KEPLM proposed named SMedBERT. Accordingly, we inject entity type semantic information of neighboring entities into node attention mechanism via heterogeneous feature learning process. Moreover, we treat the neighboring entity structures as additional global contexts to predict the masked candidate entities based on mention-spans and vice versa. The experimental results show the significant improvement of our model on various medical NLP tasks and the intrinsic evaluation. There are two research directions that can be further explored: (1) Injecting deeper knowledge by using “farther neighboring” entities as contexts; (2) Further enhancing Chinese medical long-tail entity semantic representation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank anonymous reviewers for their valuable comments. This work is supported by\nthe National Key Research and Development Program of China under Grant No. 2016YFB1000904, and Alibaba Group through Alibaba Research Intern Program."
    }, {
      "heading" : "A Data Source",
      "text" : "A.1 Pre-training Data\nA.1.1 Training Corpora The pre-training corpora is crawled from DXY BBS (Bulletin Board System) 9, which is a very popular Chinese social network for doctors, medical institutions, life scientists, and medical practitioners. The BBS has more than 30 channels, which contains 18 forums and 130 fine-grained groups, covering most of the medical domains. For our pre-training purpose, we crawl texts from channels about clinical medicine, pharmacology, public health and consulting. For text pre-processing, we mainly follow the methods of (Xu et al., 2020). Additionally, (1) we remove all URLs, HTML tags, e-mail addresses, and all tokens except characters, digits, and punctuation (2) all documents shorter than 256 are discard, while documents longer than 512 are cut into shorter text segments.\nA.1.2 Knowledge Graph The DXY knowledge graph is construed by extracting structured text from DXY website10, which includes information of diseases, drugs and hospitals edited by certified medical experts, thus the quality of the KG is guaranteed. The KG is mainly disease-centered, including totally 3,764,711 triples, 152.508 unique entities, and 44 types of relations. The details of Symptom-InChinese from OpenKG is available 11. We finally get 26 types of entities, 274,163 unique entities, 56 types of relations, and 4,390,726 triples after the fusion of the two KGs.\nA.2 Task Data\nWe choose the four large-scale datasets in ChineseBlue tasks (Zhang et al., 2020b) while others are ignored due to the limitation of datasets size, which are cMedQANER, cMedQQ, cMedQNLI and cMedQA. WebMedQA (He et al., 2019) is a real-world Chinese medical question answering dataset and CHIP-RE dataset are collected from online health consultancy websites. Note that since both the WebMedQA and cMedQA datasets are very large while we have many baselines to be compared, we randomly sample the official training set, development set and test set respectively\n9https://www.dxy.cn/bbs/newweb/pc/home 10https://portal.dxy.cn/ 11http://openkg.cn/dataset/\nsymptom-in-chinese\nto form their corresponding smaller version for experiments. DXY-NER and DXY-RE are datasets from real medical application scenarios provided by a prestigious Chinese medical company. The DXY-NER contains 22 unique entity types and 56 relation types in the DXY-RE. These two datasets are collected from the medical forum of DXY and books in the medical domain. Annotators are selected from junior and senior students with clinical medical background. In the process of quality control, the two datasets are annotated twice by different groups of annotators. An expert with medical background performs quality check manually again when annotated results are inconsistent, whereas perform sampling quality check when results are consistent. Table 5 shows the datasets size of our experiments."
    }, {
      "heading" : "B Model Settings and Training Details",
      "text" : "Hyper-parameters. d1=768, d2=200, K=10, µ =10, λ1=2, λ2=4.\nModel Details. We align the all mention-spans to the entity in KG by exact match for comparison purpose with ENIRE-THU (Zhang et al., 2019). The negative sampling function is defined as Q(eim) = t eim C eim , where Ceim is the sum of frequency of all mentions with the same type of eim. The Mention-neighbor Hybrid Attention module is inserted after the tenth transformer encoder layer to compare with KnowBERT (Peters et al., 2019), while we perform the Mention-neighbor Context Modeling based on the output of BERT encoder. We use all the base-version PLMs in the experiments. The size of SMedBERT is 474MB while 393MB of that are components of BERT, and the added 81MB is mostly of the KG embedding. Results are presented in average with 5 random runs with different random seeds and the same hyperparameters.\nTraining Procedure. We strictly follow the originally pre-training process and parameter setting of other KEPLMs. We only adapt their publicly available code from English to Chinese and use the knowledge embedding trained on our medical KG. To have a fair comparison, the pre-training processing of SMedBERT is mostly set based on ENIRE-THU (Zhang et al., 2019) without layerspecial learning rates in KnowBERT (Peters et al., 2019). We only pre-train SMedBERT on the collected medical data for 1 epoch. In pre-training\nprocess, the learning rate is set to 5e−5 and batch size is 512 with the max sequence length is 512. For fine-tuning, we find the following ranges of possible values work well, i.e., batch size is {8,16}, learning rate (AdamW) is {2e−5, 4e−5, 6e−5} and the number of epochs is {2,3,4}. Pre-training SMedBERT takes about 36 hours per epoch on 2 NVIDIA GeForce RTX 3090 GPUs."
    }, {
      "heading" : "C Data and Embedding of Unsupervised Semantic Similarity",
      "text" : "Since the KGs used in this paper is a directed graph, we first transform the directed ”等价关系” (equivalence relations) pairs to undirected pairs and discard the duplicated pairs. For each positive pairs, we use head and tail as query respectively and sample the negative candidates based on the other. Specifically, we randomly select 19 negative entities with the same type and has a JaroWinkle similarity (Winkler, 1990) bigger 0.6 with the ground-truth entity. We select from all samples in Dataset-1 with positive pairs that the neighbours sets of head and tail entity have Jaccard Index (Jaccard, 1912) no less than 0.75 and at least 3 common element to construct the Dataset-2. For Dataset-3, we count the frequency of all entity mentions in pretraining corpora, and treat mentions with frequency no more than 200 as low-frequency mentions.\nClassic Word Representation Embedding: We train the character-level and word-level embedding using SGNS (Mikolov et al., 2013a) and GLOVE (Pennington et al., 2014) model respectively on our medical corpora with open-source toolkits12. We average the character embedding for all tokens in the mention to get the character-level representation. However, since some mentions are very rare in the corpora for word-level representation, we use the character-level representation as their word-level representation.\nBERT-like Representation Embedding: We extract the token hidden features of the last layer and average the representations of the input tokens except [CLS] and [SEP] tag, to get a vector for each entity.\nSimilarity Measure: We try using the inverse of L2-distance and cosine similarity as measurement, and we find that cosine similarity always perform better. Hence, we report all experiment results under the cosine similarity metric.\n12SGNS: https://github.com/JuGyang/ word2vec-SGNS. Glove: https://github.com/stanfordnlp/ GloVe"
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Lei Jimmy Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton." ],
      "venue" : "CoRR, abs/1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "EMNLP, pages 3613–3618.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcı́aDurán", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "ICML, pages 160–167.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Revisiting pretrained models for chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "EMNLP, pages 657–668.",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training with whole word masking for chinese BERT",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu." ],
      "venue" : "CoRR, abs/1906.08101.",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain-specific language model pretraining for biomedical natural language processing",
      "author" : [ "Yu Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "CoRR,",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent relation language models",
      "author" : [ "Hiroaki Hayashi", "Zecong Hu", "Chenyan Xiong", "Graham Neubig." ],
      "venue" : "AAAI, pages 7911–7918.",
      "citeRegEx" : "Hayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Hayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Applying deep matching networks to chinese medical question answering: a study and a dataset",
      "author" : [ "Junqing He", "Mingming Fu", "Manshu Tu." ],
      "venue" : "BMC Medical Informatics Decis. Mak., 19-S(2):91–100.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Infusing disease knowledge into BERT for health question answering, medical inference and disease name recognition",
      "author" : [ "Yun He", "Ziwei Zhu", "Yin Zhang", "Qin Chen", "James Caverlee." ],
      "venue" : "EMNLP, pages 4604–4614.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Gaussian error linear units (gelus)",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "arXiv:1606.08415.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2016",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
      "author" : [ "Kexin Huang", "Jaan Altosaar", "Rajesh Ranganath." ],
      "venue" : "CoRR, abs/1904.05342.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "The distribution of the flora in the alpine zone",
      "author" : [ "Paul Jaccard." ],
      "venue" : "New Phydvtologist, 11(2):37–50.",
      "citeRegEx" : "Jaccard.,? 1912",
      "shortCiteRegEx" : "Jaccard.",
      "year" : 1912
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "ACL, pages 1–10.",
      "citeRegEx" : "Jean et al\\.,? 2015",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "A tutorial on energy-based learning",
      "author" : [ "Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang." ],
      "venue" : "Predicting structured data, 1(0).",
      "citeRegEx" : "LeCun et al\\.,? 2006",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2006
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinform., 36(4):1234–",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Real-world data medical knowledge graph: construction and applications",
      "author" : [ "Linfeng Li", "Peng Wang", "Jun Yan", "Yao Wang", "Simin Li", "Jinpeng Jiang", "Zhe Sun", "Buzhou Tang", "TsungHui Chang", "Shenghui Wang", "Yuting Liu." ],
      "venue" : "Artif. Intell. Medicine,",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning entity and relation embeddings for knowledge graph completion",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu." ],
      "venue" : "AAAI, pages 2181–2187.",
      "citeRegEx" : "Lin et al\\.,? 2015",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "A structured self-attentive sentence embedding",
      "author" : [ "Zhouhan Lin", "Minwei Feng", "Cı́cero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio" ],
      "venue" : "In ICLR",
      "citeRegEx" : "Lin et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "Rikinet: Reading wikipedia pages for natural question answering",
      "author" : [ "Dayiheng Liu", "Yeyun Gong", "Jie Fu", "Yu Yan", "Jiusheng Chen", "Daxin Jiang", "Jiancheng Lv", "Nan Duan." ],
      "venue" : "ACL, pages 6762–6771.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "K-BERT: enabling language representation with knowledge graph",
      "author" : [ "Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Qi Ju", "Haotang Deng", "Ping Wang." ],
      "venue" : "AAAI, pages 2901–2908.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "ACL, pages 4487–4496.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomás Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "NIPS, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasoning with latent structure refinement for document-level relation extraction",
      "author" : [ "Guoshun Nan", "Zhijiang Guo", "Ivan Sekulic", "Wei Lu." ],
      "venue" : "ACL, pages 1546–1557.",
      "citeRegEx" : "Nan et al\\.,? 2020",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2020
    }, {
      "title" : "Neighborhood mixture model for knowledge base completion",
      "author" : [ "Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson." ],
      "venue" : "CoNLL, pages 40–",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "The pagerank citation ranking: Bringing order to the web",
      "author" : [ "Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd." ],
      "venue" : "Technical Report 1999-66, Stanford InfoLab.",
      "citeRegEx" : "Page et al\\.,? 1999",
      "shortCiteRegEx" : "Page et al\\.",
      "year" : 1999
    }, {
      "title" : "Transfer learning in biomedical natural language processing: An evaluation of BERT and elmo on ten benchmarking datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "BioNLP, pages 58–65.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge enhanced contextual word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Robert L. Logan IV", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith." ],
      "venue" : "EMNLP, pages 43–54.",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "CoRR, abs/2003.08271.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning a health knowledge graph from electronic medical records",
      "author" : [ "Maya Rotmensch", "Yoni Halpern", "Abdulhakim Tlimat", "Steven Horng", "David Sontag." ],
      "venue" : "Scientific reports, 7(1):1–11.",
      "citeRegEx" : "Rotmensch et al\\.,? 2017",
      "shortCiteRegEx" : "Rotmensch et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Sejr Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "ESWC, pages 593–607.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Colake: Contextualized language and knowledge embedding",
      "author" : [ "Tianxiang Sun", "Yunfan Shao", "Xipeng Qiu", "Qipeng Guo", "Yaru Hu", "Xuanjing Huang", "Zheng Zhang." ],
      "venue" : "COLING, pages 3660–3670.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio." ],
      "venue" : "ACL, pages 384–394.",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving natural language inference using external",
      "author" : [ "Xiaoyan Wang", "Pavan Kapanipathi", "Ryan Musa", "Mo Yu", "Kartik Talamadupula", "Ibrahim Abdelaziz", "Maria Chang", "Achille Fokoue", "Bassem Makni", "Nicholas Mattei", "Michael Witbrock" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
      "author" : [ "Xiaozhi Wang", "Tianyu Gao", "Zhaocheng Zhu", "Zhiyuan Liu", "Juanzi Li", "Jian Tang." ],
      "venue" : "CoRR, abs/1911.06136.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage",
      "author" : [ "William E Winkler" ],
      "venue" : null,
      "citeRegEx" : "Winkler.,? \\Q1990\\E",
      "shortCiteRegEx" : "Winkler.",
      "year" : 1990
    }, {
      "title" : "Cluecorpus2020: A large-scale chinese corpus for pre-training language model",
      "author" : [ "Liang Xu", "Xuanwei Zhang", "Qianqian Dong." ],
      "venue" : "CoRR, abs/2003.01355.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "NIPS, pages 5754– 5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "E-BERT: A phrase and product knowledge enhanced language model for ecommerce",
      "author" : [ "Denghui Zhang", "Zixuan Yuan", "Yanchi Liu", "Zuohui Fu", "Fuzhen Zhuang", "Pengyang Wang", "Haifeng Chen", "Hui Xiong." ],
      "venue" : "CoRR, abs/2009.02835.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Conceptualized representation learning for chinese biomedical text mining",
      "author" : [ "Ningyu Zhang", "Qianghuai Jia", "Kangping Yin", "Liang Dong", "Feng Gao", "Nengwei Hua." ],
      "venue" : "CoRR, abs/2008.10813.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Chinese medical question answer matching using end-to-end characterlevel multi-scale cnns",
      "author" : [ "Sheng Zhang", "Xin Zhang", "Hui Wang", "Jiajun Cheng", "Pei Li", "Zhaoyun Ding." ],
      "venue" : "Applied Sciences, 7(8):767.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "ERNIE: enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "ACL, pages 1441–1451.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Pre-trained Language Models (PLMs) learn effective context representations with self-supervised tasks, spotlighting in various NLP tasks (Wang et al., 2019a; Nan et al., 2020; Liu et al., 2020a).",
      "startOffset" : 137,
      "endOffset" : 194
    }, {
      "referenceID" : 22,
      "context" : "Pre-trained Language Models (PLMs) learn effective context representations with self-supervised tasks, spotlighting in various NLP tasks (Wang et al., 2019a; Nan et al., 2020; Liu et al., 2020a).",
      "startOffset" : 137,
      "endOffset" : 194
    }, {
      "referenceID" : 49,
      "context" : "In addition, Knowledge-Enhanced PLMs (KEPLMs) (Zhang et al., 2019; Liu et al., 2020b; Wang et al., 2019b) further benefit language understanding by",
      "startOffset" : 46,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "In addition, Knowledge-Enhanced PLMs (KEPLMs) (Zhang et al., 2019; Liu et al., 2020b; Wang et al., 2019b) further benefit language understanding by",
      "startOffset" : 46,
      "endOffset" : 105
    }, {
      "referenceID" : 42,
      "context" : "In addition, Knowledge-Enhanced PLMs (KEPLMs) (Zhang et al., 2019; Liu et al., 2020b; Wang et al., 2019b) further benefit language understanding by",
      "startOffset" : 46,
      "endOffset" : 105
    }, {
      "referenceID" : 46,
      "context" : "In the literatures, a majority of KEPLMs (Zhang et al., 2020a; Hayashi et al., 2020; Sun et al., 2020) inject information of entities corresponding to mention-spans from Knowledge Graphs (KGs) into contextual representations.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "In the literatures, a majority of KEPLMs (Zhang et al., 2020a; Hayashi et al., 2020; Sun et al., 2020) inject information of entities corresponding to mention-spans from Knowledge Graphs (KGs) into contextual representations.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 38,
      "context" : "In the literatures, a majority of KEPLMs (Zhang et al., 2020a; Hayashi et al., 2020; Sun et al., 2020) inject information of entities corresponding to mention-spans from Knowledge Graphs (KGs) into contextual representations.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 36,
      "context" : "In the medical context, there exist complicated domain knowledge such as relations and medical facts among medical terms (Rotmensch et al., 2017; Li et al., 2020), which are difficult to model using previous approaches.",
      "startOffset" : 121,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : "In the medical context, there exist complicated domain knowledge such as relations and medical facts among medical terms (Rotmensch et al., 2017; Li et al., 2020), which are difficult to model using previous approaches.",
      "startOffset" : 121,
      "endOffset" : 162
    }, {
      "referenceID" : 26,
      "context" : "2 (2) Additionally, we leverage neighbors of linked-entity as global “contexts” to complement plain-text contexts used in (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 170
    }, {
      "referenceID" : 32,
      "context" : "2 (2) Additionally, we leverage neighbors of linked-entity as global “contexts” to complement plain-text contexts used in (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 170
    }, {
      "referenceID" : 39,
      "context" : "The former enriches the representations of “context” neighboring entities based on the well trained “target word” mention-span, while the latter focuses on gathering those information back from neighboring entities to the masked target like low-frequency mention-span which is poorly represented (Turian et al., 2010).",
      "startOffset" : 296,
      "endOffset" : 317
    }, {
      "referenceID" : 35,
      "context" : "PLMs have gained much attention recently, proving successful for boosting the performance of various NLP tasks (Qiu et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Early works on PLMs focus on feature-based approaches to transform words into distributed representations (Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 203
    }, {
      "referenceID" : 27,
      "context" : "Early works on PLMs focus on feature-based approaches to transform words into distributed representations (Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 203
    }, {
      "referenceID" : 32,
      "context" : "Early works on PLMs focus on feature-based approaches to transform words into distributed representations (Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 203
    }, {
      "referenceID" : 34,
      "context" : "Early works on PLMs focus on feature-based approaches to transform words into distributed representations (Collobert and Weston, 2008; Mikolov et al., 2013b; Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "BERT (Devlin et al., 2019) (as well as its robustly optimized version RoBERTa (Liu et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 25,
      "context" : ", 2019) (as well as its robustly optimized version RoBERTa (Liu et al., 2019b)) employs bidirectional transformer encoders (Vaswani et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 40,
      "context" : ", 2019b)) employs bidirectional transformer encoders (Vaswani et al., 2017) and self-supervised tasks to generate context-aware token representations.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Further improvement of performances mostly based on the following three types of techniques, including self-supervised tasks (Joshi et al., 2020), transformer encoder architectures (Yang et al.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 45,
      "context" : ", 2020), transformer encoder architectures (Yang et al., 2019) and multi-task learning (Liu et al.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 49,
      "context" : "(1) Knowledge-enhanced by Entity Embedding: ERNIE-THU (Zhang et al., 2019) and KnowBERT (Peters et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : ", 2019) and KnowBERT (Peters et al., 2019) inject linked-entity as heterogeneous features learned by KG embedding algorithms such as TransE (Bordes et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : ", 2019) inject linked-entity as heterogeneous features learned by KG embedding algorithms such as TransE (Bordes et al., 2013).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 46,
      "context" : "(2) Knowledge-enhanced by Entity Description: EBERT (Zhang et al., 2020a) and KEPLER (Wang et al.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 42,
      "context" : ", 2020a) and KEPLER (Wang et al., 2019b) add extra description text of entities to enhance semantic representation.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 23,
      "context" : "(3) Knowledgeenhanced by Triplet Sentence: K-BERT (Liu et al., 2020b) and CoLAKE (Sun et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : ", 2020b) and CoLAKE (Sun et al., 2020) convert triplets into sentences and insert them into the training corpora without pre-trained embedding.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : "Previous studies on KG embedding (Nguyen et al., 2016; Schlichtkrull et al., 2018) have shown that utilizing the surrounding facts of entity can obtain more informative embedding, which is the focus of our work.",
      "startOffset" : 33,
      "endOffset" : 82
    }, {
      "referenceID" : 37,
      "context" : "Previous studies on KG embedding (Nguyen et al., 2016; Schlichtkrull et al., 2018) have shown that utilizing the surrounding facts of entity can obtain more informative embedding, which is the focus of our work.",
      "startOffset" : 33,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "(1) BioBERT (Lee et al., 2020), BlueBERT (Peng et al.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 31,
      "context" : ", 2020), BlueBERT (Peng et al., 2019), SCIBERT (Beltagy et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : ", 2019), SCIBERT (Beltagy et al., 2019) and ClinicalBert (Huang et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : ", 2019) and ClinicalBert (Huang et al., 2019) apply continual learning on medical domain texts, such as PubMed abstracts, PMC full-text articles and MIMIC-III clinical notes.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "(Gu et al., 2020) learns weights from scratch using PubMed data to obtain an in-domain vocabulary, alleviating the out-of-vocabulary (OOV) problem.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 47,
      "context" : "For example, MC-BERT (Zhang et al., 2020b) masks Chinese medical entities and phrases to learn complex structures and concepts.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "DiseaseBERT (He et al., 2020) leverages the medical terms and its category as the labels to pre-train the model.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "The embeddings of entities and relations trained on KG by TransR (Lin et al., 2015) are represented as Γent and Γrel, respectively.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 30,
      "context" : "In order to recall the most important neighboring entity set from the KG for each mention, we extend the Personalized PageRank (PPR) (Page et al., 1999) algorithm to filter out trivial entities.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "where fsp is the self-attentive pooling (Lin et al., 2017) to generate the mention-span representation hem ∈ Rd1 and the (hi, hi+1, .",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "hem ∈ R d2 is obtained by σ(·) non-linear activation function GELU (Hendrycks and Gimpel, 2016) and the learnable projection matrix Wbe ∈ Rd1×d2 .",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "Considering the object functions of skip-gram with negative sampling (SGNS) (Mikolov et al., 2013a) and score function of TransR (Lin et al.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : ", 2013a) and score function of TransR (Lin et al., 2015):",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Inspired by SGNS, following the general energy-based framework (LeCun et al., 2006), we treat mention-spans in corpora as “target words”, and neighbors of corresponding entities in KG as “contexts” to provide additional global contexts.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "We employ the Sampled-Softmax (Jean et al., 2015) as the criterion LMNeM for the mention-span em: ∑",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Finally, to compensate the offset introduced by the negative sampling function Q(em) (Jean et al., 2015), we complement fs(em, r, em) as:[ hmf 1 ] MPr ‖ [ hmf 1 ] MPr ‖ · [ heim 0 ]",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "where LEX is the sum of sentence-order prediction (SOP) (Lan et al., 2020) and masked language modeling.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "The KGs embedding trained by TransR (Lin et al., 2015) on two trusted data sources, including the SymptomIn-Chinese from OpenKG5 and DXY-KG 6 contain-",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 47,
      "context" : "We use four large-scale datasets in ChineseBLUE (Zhang et al., 2020b) to evaluate our model, which are benchmark of Chinese medical NLP tasks.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : ", Named Entity Recognition (DXY-NER), Relation Extraction (DXY-RE, CHIP-RE) and Question Answer (WebMedQA (He et al., 2019)).",
      "startOffset" : 106,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "General PLMs: We use three Chinese BERT-style models, namely BERT-base (Devlin et al., 2019), BERT-wwm (Cui et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : ", 2019), BERT-wwm (Cui et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "All the weights are initialized from (Cui et al., 2020).",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 49,
      "context" : "KEPLMs: We employ two SOTA KEPLMs continually pre-trained on our medical corpora as our baseline models, including ERNIE-THU (Zhang et al., 2019) and KnowBERT (Peters et al.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 26,
      "context" : "In this experiments, we compare SMedBERT with three types of models: classical word embedding methods (SGNS (Mikolov et al., 2013a), GLOVE (Pennington et al.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 32,
      "context" : ", 2013a), GLOVE (Pennington et al., 2014)), PLMs and KEPLMs.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 49,
      "context" : "Figure 3 illustrates the performance of SMedBERT and ERNIE-med (Zhang et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "From the result, we can observe that: (1) The performance improves significantly at the beginning and then keeps stable as the hit ratio increases, proving the heterogeneous knowledge is beneficial to improve the ability of language understanding and indicating too much knowledge facts are unhelpful to further improve model performance due to the knowledge noise (Liu et al., 2020b).",
      "startOffset" : 365,
      "endOffset" : 384
    }, {
      "referenceID" : 49,
      "context" : "From the result, we can observe that: (1) Without any of the three mechanisms, our model performance can also perform competitively with the strong baseline ERNIE-med (Zhang et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 187
    } ],
    "year" : 2021,
    "abstractText" : "Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantics knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneousentity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference.1",
    "creator" : "LaTeX with hyperref"
  }
}