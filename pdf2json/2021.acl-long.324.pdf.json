{
  "name" : "2021.acl-long.324.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "End-to-End AMR Coreference Resolution",
    "authors" : [ "Qiankun Fu", "Linfeng Song", "Wenyu Du", "Yue Zhang" ],
    "emails" : [ "fqiankun@gmail.com", "lfsong@tencent.com", "zhangyue}@westlake.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4204–4214\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4204\nEnd-to-End AMR Coreference Resolution\nQiankun Fu1,2,3, Linfeng Song4, Wenyu Du2,3, Yue Zhang2,3 1. Zhejiang University\n2. School of Engineering, Westlake University 3. Institute of Advanced Technology, Westlake Institute for Advanced Study\n4. Tencent AI Lab, Bellevue, WA, USA fqiankun@gmail.com and lfsong@tencent.com {duwenyu, zhangyue}@westlake.edu.cn\nAbstract\nAlthough parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on many sentence-level tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization."
    }, {
      "heading" : "1 Introduction",
      "text" : "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020).\nExisting work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with\nSentence1: Bill left for Paris. Sentence2: He arrived at noon.\ncross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components referring to the same entity. Figure 1 shows the AMR graphs of two consecutive sentences in a document. An AMR coreference resolution model need to identify two coreference cases: “he” refers to “Bill” in the first graph, and “arrive-01” omits an argument “:arg3” that refers to “Paris”.\nRelatively little research has been done on AMR coreference resolution. Initial attempts (Liu et al., 2015) merge the nodes that have the same surface string. To minimize noise, only named entities and date entities are considered, and they do not consider merging non-identical nodes (e.g., “Bill” and “he” in Figure 1) that are also frequent in reallife situation. Subsequent work considers more\nco-reference cases by either manually annotating AMR coreference information (O’Gorman et al., 2018) or taking a pipeline system (Anikina et al., 2020) consisting of a textual coreference resolution model (Lee et al., 2018) and an AMR-to-text aligner (Flanigan et al., 2014). Yet there is little research on automatically resolving coreference ambiguities directly on AMR, making use of AMR graph-structural features.\nIn this work, we formulate AMR coreference resolution as a missing-link prediction problem over AMR graphs, where the input consists of multiple sentence-level AMRs, and the goal is to recover the missing coreference links connecting the AMR nodes that represent to the same entity. There are two types of links. The first type corresponds to the standard situation, where the edge connects two entity nodes (e.g., “Bill” and “he” in Figure 1) that refer to the same entity. The second type is the implicit role coreference, where one node (e.g., “Paris” in Figure 1) is a dropped argument (“:arg3”) of other predicate node (“arrive-01”).\nWe propose an AMR coreference resolution model by extending an end-to-end text-based coreference resolution model (Lee et al., 2017). In particular, we use a graph neural network to represent input AMRs for inducing expressive features. To enable cross-sentence information exchange, we make connections between sentence-level AMRs by linking their root nodes. Besides, we introduce a concept identification module to distinguish functional graph nodes (non-concept nodes, e.g., “person” in Figure 1), entity nodes (e.g., “Bill”), verbal nodes with implicit role (e.g., “arrive-01”) and other regular nodes (e.g., “leave-11”) to help improve the performance. The final antecedent prediction is conducted between the selected nodes and all their possible antecedent candidates, following previous work on textual coreference resolution (Lee et al., 2017).\nExperiments on the MS-AMR benchmark1 (O’Gorman et al., 2018) show that our model outperforms competitive baselines by a large margin. To verify the effectiveness and generalization of our proposed model, we annotate an out-of-domain test set over the gold AMR Little Prince 3.0 data following the guidelines of O’Gorman et al. (2018), and the corresponding results show that our model is consistently more robust than the baselines in domain-transfer scenarios. Finally, results on docu-\n1It consists gold coreference links on gold AMRs.\nment abstractive summarization show that our document AMRs lead to much better summary quality compared to the document AMRs by Liu et al. (2015). This further verifies the practical value of our approach. Our code and data is available at https://github.com/Sean-Blank/AMRcoref"
    }, {
      "heading" : "2 Model",
      "text" : "Formally, an input instance of AMR coreference resolution consists of multiple sentence-level AMRs G1, G2, ..., Gn, where each Gi can be written as Gi = 〈Vi, Ei〉 with Vi and Ei representing the corresponding nodes and edges for Gi. We consider a document-level AMR graph Ĝ = [G1, G2, ..., Gn; ê1, ê2, ..., êm], where each êi is a coreference link connecting two nodes from different sentence-level AMRs. The task of AMR coreference resolution aims to recover ê1, ..., êm, which are missing from the inputs. Figure 2 shows the architecture of our model, which consists of a graph encoder (§ 2.1), a concept identifier (§ 2.2), and an antecedent prediction module (§ 2.3)."
    }, {
      "heading" : "2.1 Representing Input AMRs using GRN",
      "text" : "Given sentence-level AMRs G1, ..., Gn as the input, randomly initialized word embeddings are adopted to represent each node vk as a dense vector ek. To alleviate data sparsity and to obtain better node representation, character embeddings echark are computed by using a character-level CNN. We concatenate both ek and echark embeddings for each concept before using a linear projection to form the initial representation:\nxk = W node([ek; e char k ]) + b node, (1)\nwhere W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017;\nBastings et al. 2017) and Graph Attention Network (GAT, Veličković et al. 2018), GRN has been shown to give competitive results.\nMessage passing In the message passing framework, a node vk receives information from its directly connected neighbor nodes at each layer l. We use a hidden state vector hlk to represent each node, and the initial state h0k is defined as a vector of zeros.\nIn the first step at each message passing layer, the concept representation of each neighbor of vk is combined with the corresponding edge representation to make a message xk,j . This is because edges contain semantic information that are important for learning global representation and subsequent reasoning. Formally, a neighbor vj of node vk can be represented as\nxk,j = W node([ej ; e char j ; e label k,j ]) + b node, (2)\nwhere elabelk,j denotes the label embedding of the edge from node vk and to vj .\nNext, representations of neighboring nodes from the incoming and outgoing directions are aggregated:\nxink = ∑\ni∈Nin(k)\nxli,k\nxoutk = ∑\nj∈Nout(k)\nxlk,j\nxlk = [x in k ,x out k ],\n(3)\nwhere Nin(k) and Nout(k) denote the set of incoming and outgoing neighbors of vk, respectively.\nSimilarly, the hidden states from incoming and outgoing neighbors are also summed up:\nmink = ∑\ni∈Nin(k)\nhl−1i\nmoutk = ∑\nj∈Nout(k)\nhl−1j\nmlk = [m in k ,m out k ],\n(4)\nwhere hl−1j denotes the hidden state vector for node vj at the previous (l−1) layer. Finally, the message passing from layer l − 1 to l is conducted following the gated operations of LSTM (Hochreiter and Schmidhuber, 1997):\nilk = σ(W m i m l k +W x i x l k + bi) olk = σ(W m o m l k +W x o x l k + bo) f lk = σ(W m f m l k +W x f x l k + bf ) ulk = σ(W m u m l k +W x ux l k + bu) clk = f l k cl−1k + i l k ulk hlk = o l k tanh(clk),\n(5)\nwhere ilk, o l k and f l k are a set of input, output and forget gates to control information flow from different sources, ulk represents the input messages, c l k is the cell vector to record memory, and c0k is also initialized as a vector of zeros. Wmz , W x z and bz (z ∈ {i, o, f, u}) are model parameters. We adopt L GRN layers in total, where L is determined by a development experiment. The output hLk at layer L is adopted as the representation of each node vk for subsequent procedures."
    }, {
      "heading" : "2.2 Concept Identification",
      "text" : "Concept identification aims to distinguish the AMR nodes in regard to its concept type. We consider 6 concept types T = {func, ent, ver0, ver1, ver2, reg}, which denotes the functional nodes, entity concepts, verbal concepts verx with implicit arguments (i.e., “:argx” x ∈ {0, 1, 2}2) and other regular nodes (e.g., “leave-11”), respectively. This module is comparable to the mention detection procedure in textual coreference resolution (Lee et al., 2017).\nFormally, a concept representation hLk from the top GRN layer is concatenated with a learnable type embedding etypek (t) of type t for each concept vk, and the corresponding type score sktype(t) is computed using a feed-forward network:\nsktype(t) = FFNNtype(W type[hLk ; e type k (t)]), (6)\nwhere W type is a mapping matrix. etypek (t) represents a concept-type embedding and is randomly initialized. A probability distribution P (t|vk) over all concept types T for each concept vk is calculated as follows using a softmax layer:\nP (t|vk) = es k type(t)∑\nt′∈T e sktype(t\n′ ) . (7)\nFinally, we predicate the type t∗k for each concept\nt∗k = argmaxt∈T s k type(t), (8)\nand use it to filter the input nodes. In particular, functional concepts are dropped directly and the other concepts (i.e., ent, ver0, ver1, ver2, reg) are selected as candidate nodes for antecedent prediction."
    }, {
      "heading" : "2.3 Antecedent Prediction",
      "text" : "Given a selected node vk by the concept identifier, the goal is to predict its antecedent yk from all possible candidate nodes Yk = { , yπ, ..., yk−1}, where a dummy antecedent is adopted for the nodes that are not coreferent with any previous concepts. π = min(1, k − ψ), where ψ represents the maximum antecedents considered as candidates. As mentioned by previous work on textual coreference resolution (Lee et al., 2017), considering too many candidates can hurt the final performance. We conduct development experiments to decide the best ψ. The finally predicted coreference links implicitly determine the coreference clusters.\n2We do not model other :argx to avoid long tail issue.\nType information in § 2.2 can help to guide the antecedent prediction and ensure global type consistency. We combine the node hidden vector and its type representation as the final concept state:\nhmk = [h L k ; e type k (t ∗)], (9)\nwhere etypek (t ∗) denotes the learned embedding of the concept type of node vk. Similar with Lee et al. (2017), the goal of the antecedent prediction module is to learn a distribution Q(yk) over the antecedents for each node vk:\nQ(yk) = es(k,yk)∑\ny′∈Y(k) e s(k,y′)\n(10)\nwhere s(k, a) computes a coreference link score for each concept pair (vk, va):\ns(k, a) = sm(k) + sm(a) + san(k, a). (11)\nHere a < k, and sm(k) means whether concept vk is a mention involved in a coreference link. It is calculated by using a feed-forward network:\nsm(k) = FFNNm(hmk ). (12)\nsan(k, a) indicates whether mention va is an antecedent of vk and measures the semantic similarity between vk and va, computed with rich features using a feed-forward network:\nsan(k, a) = FFNNan([hmk ,h m a ,h m k ◦hma , φ(k, a)]) (13) where ◦ denotes element-wise multiplication of each mention pair (vk, va), and a feature vector φ(k, a) represents the normalized distance between two mentions and the speaker information if available. Following Lee et al. (2017), we also normalize the distance values by grouping them into the following buckets [1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. All features (speaker, distance, concept type) are randomly initialized 32-dimensional embeddings jointly learned with the model."
    }, {
      "heading" : "2.4 Training",
      "text" : "Our objective function takes two parts: Ltype(θ) (i.e., the concept-type identification loss), and Lantecedent (i.e., the antecedent prediction loss)\nL(θ) = Ltype(θ) + λLantecedent(θ), (14)\nwhere λ is the weight coefficient (we empirically set λ = 0.1 in this paper).\nConcept Identification Loss. Ltype measures whether our model can accurately identify meaningful concepts and learn the correct type representations. Specifically, given the concept set V = {v1, ...vN}, the concept identifier is trained to minimize an average cross-entropy loss:\nLtype(θ) = − 1\nN N∑ k=1 logP (t∗k|vk), (15)\nwhere θ are the set of model parameters, P (t∗k|vk) denotes the output probability of predicted type t∗k for each node vk as in Eq. 7. Antecedent Prediction Loss. Given a training AMR document with gold coreference clusters GOLD(k)|Nk=1 and antecedent candidates Yk = { , yπ, ..., yk−1} for mention vk, Lantecedent measures whether mentions are linked to their correct antecedent. Since the antecedents are latent, the antecedent loss is a marginal log-likelihood of all correct antecedents implied by gold clustering:\nLantecedent(θ) = N∏ k=1 ∑ y∈Yk∩GOLD(k) logQ(y)\n(16)\nwhere GOLD(k) = if mention vk does not belong to any gold cluster. Q(y) is calculated using Eq. 10."
    }, {
      "heading" : "3 Experiments",
      "text" : "We conduct experiments on the MS-AMR dataset3 (O’Gorman et al., 2018), which is annotated over a previous gold AMR corpus (LDC2017T10). It has 293 annotated documents in total with an average of 27.4 AMRs per document, covering roughly 10% of the total AMR corpus. We split a dev data with the same size as the test set from the training set.\nFollowing the annotation guidelines of MSAMR, we manually annotate the AMR coreference\n3The MS-AMR dataset considers 3 types of coreference links: regular, implicit and part-whole. We ignore the last type, which has been challenging and ignored since textual coreference resolution.\nresolution information over the development and test data of the Little Prince (LP) AMR corpus4 and use it as an out-of-domain test set. For this dataset, we consider each chapter as a document. The data statistics are shown in Table 1."
    }, {
      "heading" : "3.1 Setup",
      "text" : "Evaluation Metrics We use the standard evaluation metrics for coreference resolution evaluation, computed using the official CoNLL-2012 evaluation toolkit. Three measures include: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores.\nBaselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines:\n• Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities.\n• Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al., 2014). The former generates coreference from text, and the later projects this information from text to AMRs.\nModels We study two versions of our model with or without BERT features.\n• AMRcoref-base: it corresponds to our model described in § 2 only with word embeddings.\n• AMRcoref-bert: it denotes our model in § 2 except that the word embeddings (ek in Eq. 1) are concatenated with BERT outputs. Specifically, we use a cased BERT-base model with fixed parameters to encode a sentence, taking an AMRto-text aligner (Flanigan et al., 2014) to project BERT outputs to the corresponding AMR nodes.\nHyperparameters We set the dimension of concept embeddings to 256. Characters in the character CNN (§ 2.1) are represented as learned embeddings with 32 units and the convolution window sizes include 2, 3, and 4 characters, each consisting of 100 filters. We use Adam (Kingma and Ba, 2015) with a learning rate of 0.005 for optimization.\n4https://amr.isi.edu/download/amr-bank-struct-v3.0.txt."
    }, {
      "heading" : "3.2 Development Experiments",
      "text" : "We first conduct development experiment to choose the values for the crucial hyperparameters. GRN Encoder Layers The number of recurrent layers L in GRN defines the amount of message interactions. Large message passing layers may lead to over-smoothing problems, while small layers may result in weak graph representation (Qin et al., 2020; Zhang et al., 2018). Figure 3 shows development experiments of the AMRcoref-base model in this aspect. We observe large improvements when increasing the layers from 1 to 3, but further increase from 3 to 7 does not lead to further improvements. Therefore, we choose 3 layers for our final model. Antecedent Candidates How many antecedents are considered as candidates (denoted as ψ in Section 2.3) for making each coreference decision is another important hyperparameter in a coreference resolution model (Lee et al., 2017). Intuitively, allowing more antecedents gives a higher upper bound, but that also introduces a larger search space. Table 3 shows the statistics of the distance between each mention and its gold antecedent and the devset performance of AMRcoref-base model that uses this distance as the search space. The performance of AMRcoref-base improves when increasing the search space, and the best performance was observed when 250 antecedents are considered as the search space. We choose ψ =250 in subsequent experiments."
    }, {
      "heading" : "3.3 Main Results",
      "text" : "Table 2 shows the final in-domain results on the MS-AMR test set and out-domain results on the annotated Little Prince (LP) data, where we compare our model (AMRcoref-base and AMRcoref-bert) with the Rule-based and Pipeline baselines.\nIn-domain Results The Rule-based method performs the worst, because it only links the identical entity and suffers from low recall. The Pipeline model performs better than the Rule-based model due to better coverage, but it can suffer from error propagation in both textual coreference and inaccurate AMR aligner. In addition, it does not make use of AMR structure features, which is less sparse compared to text cues. Our proposed AMRcorefbase model outperforms the two baselines by a huge margin, gaining at least 9.3% and 13.2% average F1 scores, respectively. This verifies the effectiveness of the end-to-end framework.\nOut-domain Results On the cross-domain LP data, our model largely outperforms both Rulebased method and the Pipeline model. Compared with the in-domain setting, there is minor drop on the out-of-domain dataset (4.1% and 2.3% F1 score for AMRcoref-base and AMRcoref-bert respectively). Neither the performances of Rulebased nor Pipeline change much on this dataset, which is because these systems are not trained on a certain domain. This also reflects the quality of our LP annotations, because of the consistent performance changes of both AMRcoref-base and AMRcoref-bert when switching from MS-AMR to LP."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "We analyze the effects of mention type, textual embedding and various extra features in this section. Concept Identification As shown in the first group of Table 4, we conduct an ablation study on the concept identification module, which has been shown crucial on the textual coreference resolution (Lee et al., 2017). Removing the concept identifier from the AMRcoref-base model results in a large performance degradation of up to 19.9%, indicating that concept type information of the AMR node can positively guide the prediction of coreference links. On the other hand, when the concept identifier outputs are replaced with gold mentions, the results can be further improved by 19.1%. This indicates that better performances can be expected if concept identification can be further improved.\nInjecting BERT knowledge As shown in the second group of Table 4, we study the influence of rich features from BERT in our model, which has been proven effective on text-based coreference resolution. Two alternatives of using BERT are studied, concatenate (i.e. AMRcoref-bert) denotes concatenating the AMR node embeddings with the corresponding textual BERT embedding, and graph means that we construct an AMR-token graph that connects AMR nodes and the corresponding tokens. We find that the AMRcoref-base model can be improved by a similar margin using both approaches. This is consistent with existing observations from other structured prediction tasks, such as constituent parsing (Kitaev et al., 2019) and dependency parsing (Li et al., 2019). Due to the limited scale of our training data, we expect the gain to be less with more training data.\nFeatures Ablation As shown by the last group in Table 4, we investigate the impacts of each component in our proposed model on the development set of MS-AMR. We have the following observations. First, consistent with findings of Lee et al. (2017),\nthe distance between a pair of AMR concepts is an important feature. The final model performance drops by 2.1% when removing the distance feature (Eq. 13). Second, the speaker indicator features (Eq. 13) contribute to our model by a 1.9% improvement. Intuitively, speaker information is helpful for pronoun coreference resolution in dialogues. For example, “my package” in one sentence may represent identical entity with “your package” in the next utterance. Third, the character CNN provides morphological information and a way to back off for out-of-vocabulary tokens. For AMR node representations, we see a modest contribution of 1.2% F1 score. Finally, we exploit the necessity of cross-sentence AMR connections. Compared to encoding each AMR graph individually, global information exchange across sentences can help to achieve a significant performance improvement.\nData Hunger Similar to other results, it is important to study how much data is necessary to obtain a strong performance (at least be better than the baseline). Figure 4 shows the performances when training the AMRcoref-base model on different portions of data. As the number of training samples increases, the performance of our model continuously improves. This shows that our model has room for further improvement with more training data. Moreover, our model even outperforms the Pipeline baseline when trained on only 20% data. This confirms the robustness of our end-toend framework.\nEffect of Document Length Figure 5 shows the performance on different MS-AMR document lengths (i.e., the number of AMR graphs in the document). We can see that both our model and the Pipeline model show performance decrease\nwhen increasing input document length. This is likely because a longer document usually involves more complex coreference situations and brings more challenge for the encoder. Insufficient information interaction for distant nodes further leads to weaker inference performance. As expected, the Rule-based approach (Liu et al., 2015) is not significantly affected, but its result is still pretty low. When the document contains more than 30 sentences, the AMRcoref-base model slightly under-performs both the Rule-based method and the Pipeline baseline. One reason is that only a few training instances have a long document length, so we expect that the performance of our model can be further improved given more long documents."
    }, {
      "heading" : "3.5 Application on Summarization",
      "text" : "Table 5 compares the summarization performances using the document-level AMRs generated by various methods on the LDC2015E86 benchmark (Knight et al., 2014). Following Liu et al. (2015), Rouge scores (R-1/2/L Lin 2004) are used as the metrics. To consume each document AMR and the corresponding text, we take a popular dual-tosequence model (D2S, Song et al. 2019b), which extends the standard sequence-to-sequence framework with an additional graph encoder and a dual attention mechanism for extracting both text and graph contexts during decoding.\nFor previous work, summarization using AMR was first explored by Liu et al. (2015). They first use a rule-based method to build document AMRs and then take a statistic model to generate summaries. Dohare et al. (2017) improves this approach by selecting important sentences before building a document AMR. The D2S-Rule-based can be considered as a fair comparison with Liu et al. (2015) on the same summerization platform.\nThe overall performance of the D2S models outperform the previous approaches, indicating that our experiments are conducted on a stronger baseline. Though Pipeline is better than Rule-based on AMR coreference resolution, D2S-Pipeline is comparable with D2S-Rule-based on the downstream summerization task. This shows that the error propagation issue of Pipeline can introduce further negative effects to a downstream application. On the other hand, both D2S-AMRcoref-base and D2SAMRcoref-bert show much better results than the baselines across all Rouge metrics. This demonstrates that the improvements made by our end-toend model is solid and can transfer to a downstream application. D2S-AMRcoref-bert achieves the best performance, which is consistent with the above experiments."
    }, {
      "heading" : "4 Related Work",
      "text" : "Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence. On the other hand, previous work on multi-sentence AMR primarily focuses on data annotation. Song et al. (2019a) annotate dropped pronouns over Chinese AMR but only deals with implicit roles in specific constructions. Gerber and Chai (2012) provide implicit role annotations, but the resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering coreference, implicit role coreference and bridging relations. We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task.\nCoreference Resolution Coreference resolution is a fundamental problem in natural language processing. Neural network models have shown promising results over the years. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019)\ntackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution.\nAMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose a classic GRN model following Song et al. (2018) to represent our document-level AMR graph and leave the exploiting on a more efficient GNN structure for future work."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network. Compared with previous rulebased and pipeline methods, our model better captures multi-sentence semantic information. Results on MS-AMR (in-domain) and LP (out-of-domain) datasets show the superiority and robustness of our model. In addition, experiments on the downstream text summarization task further demonstrate the effectiveness of the document-level AMRs produced by our model.\nIn future work, we plan to resolve both the crossAMR coreference links and the sentence-level ones together with our model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Linfeng Song is the corresponding author. We would like to thank the anonymous reviewers for their insightful comments. We gratefully acknowledge funding from the National Natural Science Foundation of China (NSFC No.61976180). It also receives supported by Tencent AI Lab Rhino-Bird Focused Research Program."
    } ],
    "references" : [ {
      "title" : "Predicting coreference in Abstract Meaning Representations",
      "author" : [ "Tatiana Anikina", "Alexander Koller", "Michael Roth." ],
      "venue" : "Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference, pages 33–38, Barcelona,",
      "citeRegEx" : "Anikina et al\\.,? 2020",
      "shortCiteRegEx" : "Anikina et al\\.",
      "year" : 2020
    }, {
      "title" : "Algorithms for scoring coreference chains",
      "author" : [ "Amit Bagga", "Breck Baldwin." ],
      "venue" : "The first international conference on language resources and evaluation workshop on linguistics coreference, volume 1, pages 563–566.",
      "citeRegEx" : "Bagga and Baldwin.,? 1998",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Abstract Meaning Representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguis-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Graph convolutional encoders for syntax-aware neural machine translation",
      "author" : [ "Jasmijn Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Sima’an" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Bastings et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Bastings et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph-to-sequence learning using gated graph neural networks",
      "author" : [ "Daniel Beck", "Gholamreza Haffari", "Trevor Cohn." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Beck et al\\.,? 2018",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogue-AMR: Abstract Meaning Representation for dialogue",
      "author" : [ "Claire Bonial", "Lucia Donatelli", "Mitchell Abrams", "Stephanie M. Lukin", "Stephen Tratz", "Matthew Marge", "Ron Artstein", "David Traum", "Clare Voss." ],
      "venue" : "Proceedings of the 12th Lan-",
      "citeRegEx" : "Bonial et al\\.,? 2020",
      "shortCiteRegEx" : "Bonial et al\\.",
      "year" : 2020
    }, {
      "title" : "AMR parsing via graph-sequence iterative inference",
      "author" : [ "Deng Cai", "Wai Lam." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1290–1301, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Cai and Lam.,? 2020a",
      "shortCiteRegEx" : "Cai and Lam.",
      "year" : 2020
    }, {
      "title" : "Graph transformer for graph-to-sequence learning",
      "author" : [ "Deng Cai", "Wai Lam." ],
      "venue" : "AAAI, pages 7464– 7471.",
      "citeRegEx" : "Cai and Lam.,? 2020b",
      "shortCiteRegEx" : "Cai and Lam.",
      "year" : 2020
    }, {
      "title" : "Structural neural encoders for AMR-to-text generation",
      "author" : [ "Marco Damonte", "Shay B. Cohen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-",
      "citeRegEx" : "Damonte and Cohen.,? 2019",
      "shortCiteRegEx" : "Damonte and Cohen.",
      "year" : 2019
    }, {
      "title" : "Text summarization using abstract meaning representation",
      "author" : [ "Shibhansh Dohare", "Harish Karnick", "Vivek Gupta." ],
      "venue" : "arXiv preprint arXiv:1706.01678.",
      "citeRegEx" : "Dohare et al\\.,? 2017",
      "shortCiteRegEx" : "Dohare et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling source syntax and semantics for neural amr parsing",
      "author" : [ "DongLai Ge", "Junhui Li", "Muhua Zhu", "Shoushan Li." ],
      "venue" : "Proceedings of the TwentyEighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4975–4981. Interna-",
      "citeRegEx" : "Ge et al\\.,? 2019",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic role labeling of implicit arguments for nominal predicates",
      "author" : [ "Matthew Gerber", "Joyce Y. Chai." ],
      "venue" : "Computational Linguistics, 38(4):755–798.",
      "citeRegEx" : "Gerber and Chai.,? 2012",
      "shortCiteRegEx" : "Gerber and Chai.",
      "year" : 2012
    }, {
      "title" : "Guided neural language generation for abstractive summarization using Abstract Meaning Representation",
      "author" : [ "Hardy Hardy", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 768–",
      "citeRegEx" : "Hardy and Vlachos.,? 2018",
      "shortCiteRegEx" : "Hardy and Vlachos.",
      "year" : 2018
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "EXPR at SemEval-2018 task 9: A combined approach for hypernym discovery",
      "author" : [ "Ahmad Issa Alaa Aldine", "Mounira Harzallah", "Giuseppe Berio", "Nicolas Béchet", "Ahmad Faour." ],
      "venue" : "Proceedings of The 12th International Workshop on Semantic Eval-",
      "citeRegEx" : "Aldine et al\\.,? 2018",
      "shortCiteRegEx" : "Aldine et al\\.",
      "year" : 2018
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Coreference resolution with entity equalization",
      "author" : [ "Ben Kantor", "Amir Globerson." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 673–677, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Kantor and Globerson.,? 2019",
      "shortCiteRegEx" : "Kantor and Globerson.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Multilingual constituency parsing with self-attention and pre-training",
      "author" : [ "Nikita Kitaev", "Steven Cao", "Dan Klein." ],
      "venue" : "Proceedings of the 57th Annual",
      "citeRegEx" : "Kitaev et al\\.,? 2019",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2019
    }, {
      "title" : "Deft phase 2 amr annotation r1 ldc2015e86",
      "author" : [ "Kevin Knight", "Laura Baranescu", "Claire Bonial", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Daniel Marcu", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "philadelphia: Linguistic data consor-",
      "citeRegEx" : "Knight et al\\.,? 2014",
      "shortCiteRegEx" : "Knight et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197, Copenhagen, Denmark. Association",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Higher-order coreference resolution with coarse-tofine inference",
      "author" : [ "Kenton Lee", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving event detection with Abstract Meaning Representation",
      "author" : [ "Xiang Li", "Thien Huu Nguyen", "Kai Cao", "Ralph Grishman." ],
      "venue" : "Proceedings of the First Workshop on Computing News Storylines, pages 11–15, Beijing, China. Association for Com-",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Self-attentive biaffine dependency parsing",
      "author" : [ "Ying Li", "Zhenghua Li", "Min Zhang", "Rui Wang", "Sheng Li", "Luo Si." ],
      "venue" : "IJCAI, pages 5067–5073.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstract Meaning Representation for multi-document summarization",
      "author" : [ "Kexin Liao", "Logan Lebanoff", "Fei Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1178–1190, Santa Fe, New Mexico, USA. As-",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Toward abstractive summarization using semantic representations",
      "author" : [ "Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "On coreference resolution performance metrics",
      "author" : [ "Xiaoqiang Luo." ],
      "venue" : "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 25–32, Vancouver, British Columbia, Canada.",
      "citeRegEx" : "Luo.,? 2005",
      "shortCiteRegEx" : "Luo.",
      "year" : 2005
    }, {
      "title" : "AMR parsing as graph prediction with latent alignment",
      "author" : [ "Chunchuan Lyu", "Ivan Titov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 397–407, Melbourne, Australia. Asso-",
      "citeRegEx" : "Lyu and Titov.,? 2018",
      "shortCiteRegEx" : "Lyu and Titov.",
      "year" : 2018
    }, {
      "title" : "Rewarding Smatch: Transition-based AMR parsing with reinforcement learning",
      "author" : [ "Tahira Naseem", "Abhishek Shah", "Hui Wan", "Radu Florian", "Salim Roukos", "Miguel Ballesteros." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Naseem et al\\.,? 2019",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2019
    }, {
      "title" : "2018. AMR beyond the sentence: the multi-sentence AMR corpus",
      "author" : [ "Tim O’Gorman", "Michael Regan", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Martha Palmer" ],
      "venue" : "In Proceedings of the 27th International Conference on Computational Linguistics,",
      "citeRegEx" : "O.Gorman et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "O.Gorman et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised entity linking with abstract meaning representation",
      "author" : [ "Xiaoman Pan", "Taylor Cassidy", "Ulf Hermjakob", "Heng Ji", "Kevin Knight." ],
      "venue" : "Proceedings of the 2015 conference of the north american chapter of the association for computational",
      "citeRegEx" : "Pan et al\\.,? 2015",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2015
    }, {
      "title" : "AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling",
      "author" : [ "Libo Qin", "Xiao Xu", "Wanxiang Che", "Ting Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1807–1816, Online.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Biomedical event extraction using Abstract Meaning Representation",
      "author" : [ "Sudha Rao", "Daniel Marcu", "Kevin Knight", "Hal Daumé III." ],
      "venue" : "BioNLP 2017, pages 126–135, Vancouver, Canada,. Association for Computational Linguistics.",
      "citeRegEx" : "Rao et al\\.,? 2017",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2017
    }, {
      "title" : "The graph neural network model",
      "author" : [ "F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini." ],
      "venue" : "IEEE Transactions on Neural Networks, 20(1):61–80.",
      "citeRegEx" : "Scarselli et al\\.,? 2009",
      "shortCiteRegEx" : "Scarselli et al\\.",
      "year" : 2009
    }, {
      "title" : "An easier and efficient framework to annotate semantic roles: Evidence from the chinese amr corpus",
      "author" : [ "Li Song", "Yuan Wen", "Sijia Ge", "Bin Li", "Weiguang Qu." ],
      "venue" : "Workshop on Chinese Lexical Semantics, pages 474–485. Springer.",
      "citeRegEx" : "Song et al\\.,? 2019a",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic neural machine translation using AMR",
      "author" : [ "Linfeng Song", "Daniel Gildea", "Yue Zhang", "Zhiguo Wang", "Jinsong Su." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:19–31.",
      "citeRegEx" : "Song et al\\.,? 2019b",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "A graph-to-sequence model for AMRto-text generation",
      "author" : [ "Linfeng Song", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "The role of reentrancies in Abstract Meaning Representation parsing",
      "author" : [ "Ida Szubert", "Marco Damonte", "Shay B. Cohen", "Mark Steedman." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2198–2207, Online. As-",
      "citeRegEx" : "Szubert et al\\.,? 2020",
      "shortCiteRegEx" : "Szubert et al\\.",
      "year" : 2020
    }, {
      "title" : "Dealing with co-reference in neural semantic parsing",
      "author" : [ "Rik Van Noord", "Johan Bos." ],
      "venue" : "Proceedings of the 2nd Workshop on Semantic Deep Learning (SemDeep-2), pages 41–49.",
      "citeRegEx" : "Noord and Bos.,? 2017",
      "shortCiteRegEx" : "Noord and Bos.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "A modeltheoretic coreference scoring scheme",
      "author" : [ "Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman." ],
      "venue" : "Sixth Message Understanding Conference (MUC-6): Proceedings of a Conference Held in Columbia, Maryland,",
      "citeRegEx" : "Vilain et al\\.,? 1995",
      "shortCiteRegEx" : "Vilain et al\\.",
      "year" : 1995
    }, {
      "title" : "AMR-to-text generation with graph transformer",
      "author" : [ "Tianming Wang", "Xiaojun Wan", "Hanqi Jin." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:19–33.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "AMR parsing as sequence-tograph transduction",
      "author" : [ "Sheng Zhang", "Xutai Ma", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 80–94, Florence, Italy. Associa-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Lightweight, dynamic graph convolutional networks for AMR-to-text generation",
      "author" : [ "Yan Zhang", "Zhijiang Guo", "Zhiyang Teng", "Wei Lu", "Shay B. Cohen", "Zuozhu Liu", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentencestate LSTM for text representation",
      "author" : [ "Yue Zhang", "Qi Liu", "Linfeng Song." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 317–327, Melbourne, Australia. Association",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "AMR parsing with latent structural information",
      "author" : [ "Qiji Zhou", "Yue Zhang", "Donghong Ji", "Hao Tang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4306–4319, Online. Association for Computa-",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 34,
      "context" : "Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al.",
      "startOffset" : 170,
      "endOffset" : 188
    }, {
      "referenceID" : 27,
      "context" : ", 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al.",
      "startOffset" : 28,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : ", 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al.",
      "startOffset" : 28,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : ", 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al.",
      "startOffset" : 28,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : ", 2018), event detection (Li et al., 2015), machine translation (Song et al.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 37,
      "context" : ", 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : ", 2019b) and dialogue understanding (Bonial et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 29,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 30,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 44,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 47,
      "context" : "Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 32,
      "context" : "Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is",
      "startOffset" : 54,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is",
      "startOffset" : 54,
      "endOffset" : 141
    }, {
      "referenceID" : 37,
      "context" : "Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is",
      "startOffset" : 54,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "Initial attempts (Liu et al., 2015) merge the nodes that have the same surface string.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 31,
      "context" : "4205 co-reference cases by either manually annotating AMR coreference information (O’Gorman et al., 2018) or taking a pipeline system (Anikina et al.",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : ", 2018) or taking a pipeline system (Anikina et al., 2020) consisting of a textual coreference resolution model (Lee et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : ", 2020) consisting of a textual coreference resolution model (Lee et al., 2018) and an AMR-to-text aligner (Flanigan et al.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "The final antecedent prediction is conducted between the selected nodes and all their possible antecedent candidates, following previous work on textual coreference resolution (Lee et al., 2017).",
      "startOffset" : 176,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "Experiments on the MS-AMR benchmark1 (O’Gorman et al., 2018) show that our model outperforms competitive baselines by a large margin.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 46,
      "context" : "We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations.",
      "startOffset" : 38,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations.",
      "startOffset" : 38,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009).",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "Finally, the message passing from layer l − 1 to l is conducted following the gated operations of LSTM (Hochreiter and Schmidhuber, 1997):",
      "startOffset" : 103,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "This module is comparable to the mention detection procedure in textual coreference resolution (Lee et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 21,
      "context" : "As mentioned by previous work on textual coreference resolution (Lee et al., 2017), considering too many candidates can hurt the final performance.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "We conduct experiments on the MS-AMR dataset3 (O’Gorman et al., 2018), which is annotated over a previous gold AMR corpus (LDC2017T10).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 42,
      "context" : "Three measures include: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005).",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005).",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005).",
      "startOffset" : 49,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 27,
      "context" : "• Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "• Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : ", 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : ", 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "We use Adam (Kingma and Ba, 2015) with a learning rate of 0.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 33,
      "context" : "Large message passing layers may lead to over-smoothing problems, while small layers may result in weak graph representation (Qin et al., 2020; Zhang et al., 2018).",
      "startOffset" : 125,
      "endOffset" : 163
    }, {
      "referenceID" : 46,
      "context" : "Large message passing layers may lead to over-smoothing problems, while small layers may result in weak graph representation (Qin et al., 2020; Zhang et al., 2018).",
      "startOffset" : 125,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "3) for making each coreference decision is another important hyperparameter in a coreference resolution model (Lee et al., 2017).",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : "Concept Identification As shown in the first group of Table 4, we conduct an ablation study on the concept identification module, which has been shown crucial on the textual coreference resolution (Lee et al., 2017).",
      "startOffset" : 197,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "This is consistent with existing observations from other structured prediction tasks, such as constituent parsing (Kitaev et al., 2019) and dependency parsing (Li et al.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 27,
      "context" : "As expected, the Rule-based approach (Liu et al., 2015) is not significantly affected, but its result is still pretty low.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "Table 5 compares the summarization performances using the document-level AMRs generated by various methods on the LDC2015E86 benchmark (Knight et al., 2014).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 39,
      "context" : "Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence.",
      "startOffset" : 47,
      "endOffset" : 94
    }, {
      "referenceID" : 38,
      "context" : "AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al.",
      "startOffset" : 86,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al.",
      "startOffset" : 86,
      "endOffset" : 124
    }, {
      "referenceID" : 47,
      "context" : ", 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 45,
      "context" : ", 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : ", 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : ", 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 43,
      "context" : ", 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced.",
      "startOffset" : 17,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on many sentence-level tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.",
    "creator" : "LaTeX with hyperref"
  }
}