{
  "name" : "2021.acl-long.398.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Pre-training Universal Language Representation",
    "authors" : [ "Yian Li", "Hai Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5122–5133\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5122"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we propose universal language representation (ULR) that uniformly embeds linguistic units in different hierarchies in the same vector space. A universal language representation model encodes linguistic units such as words, phrases or sentences into fixed-sized vectors and handles multiple layers of linguistic objects in a unified way. ULR learning may offer a great convenience when confronted with sequences of different lengths, especially in tasks such as Natural Language Understanding (NLU) and Question Answering (QA),\n∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab.\nhence it is of great importance in both scientific research and industrial applications.\nAs is well known, embedding representation for a certain linguistic unit (i.e., word) enables linguistics-meaningful arithmetic calculation among different vectors, also known as word analogy (Mikolov et al., 2013). For example:\nKing−Man = Queen−Woman\nIn fact, manipulating embeddings in the vector space reveals syntactic and semantic relations between the original symbol sequences and this feature is indeed useful in true applications. For example, “London is the capital of England” can be formulized as:\nEngland + capital ≈ London\nThen given two documents one of which contains “England” and “capital”, the other contains “London”, we consider them relevant. While a ULR model may generalize such good analogy features onto free text with all language levels involved together. For example, Eat an onion : Vegetable :: Eat a pear : Fruit.\nULR has practical values in dialogue systems, by which human-computer communication will go far beyond executing instructions. One of the main challenges of dialogue systems is Dialogue State Tracking (DST). It can be formulated as a semantic parsing task (Cheng et al., 2020), namely, converting natural language utterances with any length into unified representations. Thus this is essentially a problem that can be conveniently solved by mapping sequences with similar semantic meanings into similar representations in the same vector space according to a ULR model.\nAnother use of ULR is in the Frequently Asked Questions (FAQ) retrieval task, where the goal is to answer a user’s question by retrieving question\nparaphrases that already have an answer from the database. Such task can be accurately done by only manipulating vectors such as calculating and ranking vector distance (i.e., cosine similarity). The core is to embed sequences of different lengths in the same vector space. Then a ULR model retrieves the correct question-answer pair for the user query according to vector distance.\nIn this paper, we propose a universal language representation learning method that generates fixedsized vectors for sequences of different lengths based on pre-trained language models (Devlin et al., 2019; Lan et al., 2019; Clark et al., 2020). We first introduce an efficient approach to extract and prune meaningful n-grams from unlabeled corpus. Then we present a new pre-training objective, Minimizing Symbol-vector Algorithmic Difference (MiSAD), that explicitly applies a penalty over different levels of linguistic units if their representations tend not to be in the same vector space.\nTo investigate our model’s ability of capturing different levels of language information, we introduce an original universal analogy task derived from Google’s word analogy dataset, where our model significantly improves the performance of previous pre-trained language models. Evaluation on a wide range of downstream tasks also demonstrates the effectiveness of our ULR model. Overall, our ULR-BERT reaches the highest average accuracy on the universal analogy dataset and obtains 1.1% gain over Google BERT on the GLUE benchmark. Extensive experimental results on a question answering task verifies that our model can be easily applied to real-world applications in an extremely convenient way."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token. Although such pre-trained language models (PrLMs) more or less are capable of offering universal language representation through their general-purpose training objectives, all the PrLMs devote into the contex-\ntualized representations from a generic text background and pay little attention on our concerned universal language presentation.\nAs a typical PrLM, BERT is trained on a large amount of unlabeled data including two training targets: Masked Language Model (MLM), and Next Sentence Prediction (NSP). ALBERT (Lan et al., 2019) is trained with Sentence-Order Prediction (SOP) as a replacement of NSP. StructBERT (Wang et al., 2020) combines NSP and SOP to learn inter-sentence structural information. Nevertheless, RoBERTa (Liu et al., 2019) and SpanBERT (Joshi et al., 2020) show that single-sequence training is better than the sentence-pair scenario. Besides, BERT-wwm (Cui et al., 2019), StructBERT (Joshi et al., 2020), SpanBERT (Wang et al., 2020) perform MLM on higher linguistic levels, augmenting the MLM objective by masking whole words, trigrams or spans, respectively. ELECTRA (Clark et al., 2020) further improves pre-training through a generator and discriminator architecture. The aforementioned models may seemingly handle different sized input sequences, but all of them focus on sentence-level specific representation still for each word, which may cause unsatisfactory performance in real-world situations.\nThere are a series of downstream NLP tasks especially on question answering which may be conveniently and effectively solved through ULR like solution. Actually, though in different forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019)."
    }, {
      "heading" : "3 Model",
      "text" : "As pre-trained contextualized language models show their powerfulness in generic language representation for various downstream NLP tasks, we present a BERT-style ULR model that is especially designed to effectively learn universal, fixed-sized representations for input sequences of any granularity, i.e., words, phrases, and sentences. Our proposed pre-training method is furthermore strengthened in three-fold. First, we extract a large number of meaningful n-grams from monolingual corpus based on point-wise mutual information to leverage the multi-granular structural information. Second, inspired by word and phrase representation\nand their compositionality, we introduce a novel pre-training objective that directly models the input sequences and the extracted n-grams through manipulating their representations. Finally, we implement a normalized score for each n-gram to guide their sampling for training.\n3.1 n-gram Extracting\nGiven a symbol sentence, Joshi et al. (2020) utilize span-level information by randomly masking and predicting contiguous segments. Different from such random sampling strategy, our method is based on point-wise mutual information (PMI) (Church and Hanks, 1989) that makes efficient use of statistics and automatically extracts meaningful n-grams from unlabeled corpus.\nMutual information (MI) describes the association between two tokens by comparing the probability of observing them together with the probabilities of observing them independently. Higher mutual information indicates stronger association between the tokens. To be specific, an n-gram is denoted as w = (x1, . . . , x|w|), where |w| is the number of tokens in w and |w| > 1. Therefore, we present an extended PMI formula as follows:\nPMI(w) = 1\n|w| logP (w)− |w|∑ k=1 logP (xk)  where the probabilities are estimated by counting the number of observations of each token and ngram in the corpus, and normalizing by the corpus size. 1|w| is an additional normalization factor which avoids extremely low scores for long n-grams.\nWe first collect all n-grams with lengths up to N using the SRILM toolkit1 (Stolcke, 2002), and compute PMI scores for all the n-grams based on their occurrences. Then, only n-grams with PMI scores higher than the chosen threshold are selected and input sequences are marked with the corresponding n-grams."
    }, {
      "heading" : "3.2 Training Objective",
      "text" : "While the MLM training objective as in BERT (Devlin et al., 2019) and its extensions (Cui et al., 2019; Joshi et al., 2020; Wang et al., 2020) are widely used for pre-trained contextualized language modeling, they do not focus on our concerned ULR,\n1http://www.speech.sri.com/projects/srilm/download.html\nwhich demands an arithmetic corresponding relationship between the symbol and its represented vector. In order to directly model such demand, we propose a novel training target – Minimizing Symbol-vector Algorithmic Difference (MiSAD) – that leverages the vector space regularity of different granular linguistic units. For example, the following symbol sequence equation\n“London is” + “the capital of England”\n=“London is the capital of England” (1)\nindicates a vector algorithmic equation according to our ULR goal,\nvector(“London is”) + vector(“the capital of\nEngland”)\n=vector(“London is the capital of England”) (2)\nThus, if the symbol equation (1) cannot imply the respective vector equation (2), we may set a training objective to let the ULR model forcedly learn such relationship.\nFormally, we denote the input sequence by S = {x1, . . . , xm}, where m is the number of tokens in S. After n-gram extracting and pruning by means of PMI, each sequence is marked with several n-grams. During pre-training, only one of them is selected by the n-gram scoring function, which will be introduced in detail in Section 3.3, and the input sequence is represented as S = {x1, . . . , xi−1, w, xj+1, . . . , xm}, where the n-gram w = {xi, . . . , xj} (1 ≤ i < j ≤ m) is a sub-sequence of S. Then we convert S into two independent parts – the n-gram w and the rest of the tokens R = {x1, . . . , xi−1, xj+1, . . . , xm} – which are fed into the model separately along with the original complete sequence.\nThe Transformer encoder generates a contextualized representation for each token in the sequence. To derive fixed-sized vectors for sequences of different lengths, we use the pooled output of the [CLS] token as sequence embeddings. The model is trained to minimize the following Mean Square Error (MSE) loss:\nLMiSAD = MSE(Ew + ER, ES)\nwhere Ew, ER and ES are representations of w, R and S, respectively, and are all normalized to unit lengths. To enhance the robustness of the model, we jointly train MiSAD and the MLM objective\nLMLM as in BERT with equal weights. Since the input sentence S is split into w+R, we must avoid masking out the n-gram w in the original sentence in order not to affect the semantics after vector space combination. However, tokens in n-grams other than w have equal weights of being replaced with [MASK] as other tokens. The final loss function is as follows:\nL = LMiSAD + LMLM\n3.3 n-gram Sampling\nFor a given sequence, the importance of different n-grams and the degree to which the model understands their semantics are different. Instead of sampling n-grams at random, we let the model decide which n-gram to choose based on the knowledge learned in the pre-training stage. Following Tamborrino et al. (2020), we employ a normalized score for each n-gram in the input sequence using the masked language modeling head.\nWe mask one n-gram at a time and the model outputs probabilities of the masked tokens given their surrounding context. The score of an n-gram w is calculated as the average probabilities of all tokens in it.\nscorew = 1\n|w| |w|∑ k=1 P (xk|S\\w)\nwhere |w| is the length of w and S\\w is the notation of an input sequence S with all tokens within w replaced by the special token [MASK]. Finally, we choose the n-gram with the lowest score for our training target."
    }, {
      "heading" : "4 Implementation of ULR Pre-training",
      "text" : "This section introduces our ULR pre-training details.\nAs for the pre-training corpus, we download the English Wikipedia Corpus2 and pre-process with process wiki.py3, which extracts text from xml files. When processing paragraphs from Wikipedia, we find that a large number of entities are annotated with special marks, which may be useful for our task. Therefore, we identify all the entities and treat them as high-quality n-grams. Then, we remove punctuation marks and characters\n2https://dumps.wikimedia.org/enwiki/latest 3https://github.com/panyang/Wikipedia Word2vec/blob/\nmaster/v1/process wiki.py\nin other languages based on regular expressions, and finally get a corpus of 2,266M words.\nAs for n-gram pruning, PMI scores of all ngrams with a maximum length of N = 6 are calculated for each document. We manually evaluate the extracted n-grams and find more than 50% of the top 2000 n-grams contain 2 ∼ 3 words, and only less than 3% n-grams are longer than 4. Although a larger n-gram vocabulary can cover longer ngrams, it will cause too many meaningless n-grams at the same time. Therefore, we empirically retain the top 3000 n-grams for each document. Finally, we randomly sample 10M sentences from the entire corpus to reduce training time.\nDuring pre-training, BERT packs sentence pairs into a single sequence and use the special [CLS] token as sentence-pair representation. However, our MiSAD training objective requires singlesentence inputs. Thus in our experiments, each input is an n-ngram or a single sequence with a maximum length of 128. Special tokens [CLS] and [SEP] are added at the front and end of each input, respectively. Instead of training from scratch, we initialize our model with the officially released checkpoints of BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019) and ELECTRA (Clark et al., 2020). We use Adam optimizer (Kingma and Ba, 2017) with initial learning rate of 5e-5 and linear warmup over the first 10% of the training steps. Batch size is 64 and dropout rate is 0.1. Each model is trained for one epoch over 10M training examples on four Nvidia Tesla P40 GPUs."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Tasks",
      "text" : "We construct a universal analogy dataset in terms of words, phrases and sentences and experiment with multiple representation models to examine their ability of representing different levels of linguistic units through a task-independent evaluation4. Furthermore, we conduct experiments on a wide range of downstream tasks from the GLUE benchmark and a question answering task."
    }, {
      "heading" : "5.1.1 Universal Analogy",
      "text" : "Our universal analogy dataset is based on Google’s word analogy dataset and contains three levels of tasks: words, phrases and sentences.\n4Code and dataset are available at: https://github.com/ Liyianan/ULR.\nWord-level Recall that in a word analogy task (Mikolov et al., 2013), two pairs of words that share the same type of relationship, denoted as A : B :: C : D, are involved. The goal is to retrieve the last word from the vocabulary given the first three words. To facilitate comparison between models with different vocabularies, we construct a closedvocabulary analogy task based on Google’s word analogy dataset through negative sampling. Concretely, for each original question, we use GloVe to rank every word in the vocabulary and the top 5 results are considered to be candidate words. If GloVe fails to retrieve the correct answer, we manually add it to make sure it is included in the candidates. During evaluation, the model is expected to select the correct answer from 5 candidate words. Table 1 shows examples from our word anlogy dataset. Phrase-/Sentence-level To derive higher level analogy datasets, we put word pairs from the wordlevel dataset into contexts so that the resulting phrase and sentence pairs also have linear relationships. Phrase and sentence templates are extrated from the English Wikipedia Corpus. Both phrase and sentence datasets have four types of semantic analogy and three kinds of syntactic analogy. Please refer to Appendix A for details about our approach of constructing the universal analogy dataset."
    }, {
      "heading" : "5.1.2 GLUE",
      "text" : "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of tasks that are widely used to evaluate the performance of a model in language understanding. We divide NLU tasks from the GLUE benchmark into three main categories. Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli\net al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them.\nIn the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sentence pair tasks, the hidden state of the first token [CLS] is used for softmax classification. We use the same sets of hyperparameters for all the evaluated models. Experiments are ran with batch sizes in {8, 16, 32, 64} and learning rate of 3e-5 for 3 epochs."
    }, {
      "heading" : "5.1.3 GEOGRANNO",
      "text" : "GEOGRANNO (Herzig and Berant, 2019) contains natural language paraphrases paired with logical forms. The dataset is manually annotated: For each natural language utterance, a correct canonical utterance paraphrase is selected. The train/dev sets have 487 and 59 paraphrase pairs, respectively. In our experiments, we focus on question paraphrase retrieval, whose task is to retrieve the correct paraphrase from all 158 different sentences when given a question. Most of the queries have only one correct answer while some have two or more matches. Evaluation metrics are Top-1/5/10 accuracy.\nFor GEOGRANNO and the universal analogy task, we apply three pooling strategies on top of the PrLM: Using the vector of the [CLS] token, mean-pooling of all token embeddings and maxpooling over time of all embeddings. The default setting is mean-pooling."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "On the universal analogy task, we adopt three types of baselines including bag-of-words (BoW) model from pre-trained word embeddings: GloVe (Pennington et al., 2014), sentence embedding models: InferSent (Conneau et al., 2017), GenSen (Subra-\nmanian et al., 2018), USE (Cer et al., 2018) and LASER (Artetxe and Schwenk, 2019), and pretrained contextualized language models: BERT, ALBERT and ELECTRA.\nOn GLUE and GEOGRANNO, we especially evaluate our model and two baseline models:\nBERT The officially released pre-trained BERT models (Devlin et al., 2019).\nMLM-BERT BERT models trained with the same additional steps with our model on Wikipedia using only the MLM objective.\nULR-BERT Our universal language representation model trained on Wikipedia with MLM and MiSAD."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Universal Analogy",
      "text" : "Results on our universal analogy dataset are reported in Table 2. Generally, semantic analogies are more challenging than the syntactic ones and higher-level relationships between sequences are more difficult to capture, which is observed in almost all the evaluated models. On the word analogy task, GloVe achieves the highest accuracy (80.3%) while its performance drops sharply on higher-level tasks. All well trained PrLMs like BERT, ALBERT\n5https://gluebenchmark.com\nand ELECTRA hardly exhibit arithmetic characteristics and increasing the model size usually leads to a decrease in accuracy.\nHowever, training models with our properly designed MiSAD objective greatly improves the performance. Especially, ULR-BERT obtains 15% ∼ 25% absolute gains on word-level analogy, such results are so strong to be comparable to GloVe, which especially focuses on the linear word analogy feature from its training scheme. Meanwhile GloVe performs far worse than our model on higher-level analogies. Overall, ULR-BERT achieves the highest average accuracy (45.8%), an absolute gain of 8.1% over BERT, indicating that it has indeed more effectively learned universal language representations across different linguistic units. It demonstrates that our pre-training method is effective and can be adapted to different PrLMs."
    }, {
      "heading" : "6.2 GLUE",
      "text" : "Table 3 shows the performance on the GLUE benchmark. Our model improves the BERTBASE and BERTLARGE by 1.1% and 0.7% on average, respectively. Since our model is established on the released checkpoints of Google BERT, we make additional comparison with MLM-BERT that is trained under the same procedure as our model except for the pre-training objective. While the model trained with more MLM updates may improve the performance on some tasks, it underper-\nforms BERT on datasets such as MRPC, RTE and SST-2. Our model exceeds MLM-BERTBASE and MLM-BERTLARGE by 0.9% and 0.7% on average respectively. The main gains from the base model are in CoLA (+4.6%) and RTE (+1.4%), which are entirely contributed by our MiSAD training objective. Overall, our model improves the performance of its baseline on every dataset in the GLUE benchmark, demonstrating its effectiveness in real applications of natural language understanding."
    }, {
      "heading" : "6.3 GEOGRANNO",
      "text" : "Table 4 shows the performance on GEOGRANNO. As we can see, 4 out of 6 evaluated pre-trained language models significantly outperform BM25 for Top-1 accuracy, indicating the superiority of contextualized embedding-based models over the statistical method. Among all the evaluated models, ULR-BERT yields the highest accuracies (39.7%/68.8%/77.3%). To be specific, our ULR models exceeds BERTBASE and BERTLARGE by\n10.1% and 19.2% and obtains 2.7% and 10.6% improvements compared with MLM-BERTBASE and MLM-BERTLARGE in terms of Top-1 accuracy, respectively, which are consistent with the results on the GLUE benchmark. Since n-grams and sentences of different lengths are involved in the pre-training of our model, it is especially better at understanding the semantics of input sequences and mapping queries to their paraphrases according to the learned sense of semantic equality."
    }, {
      "heading" : "7 Ablation Study",
      "text" : "In this section, we explore to what extent does our model benefit from the MiSAD objective and sampling strategy, and further confirm that our pretraining procedure improves the model’s ability of encoding variable-length sequences."
    }, {
      "heading" : "7.1 Effect of Training Objectives",
      "text" : "To make a fair comparison, we train BERT with the same additional updates using different combinations of training tasks:\nNSP-BERT is trained with MLM and NSP, whose goal is to distinguish whether two input sentences are consecutive. For each sentence, we choose its following sentence 50% of the time and randomly sample a sentence 50% of the time.\nSOP-BERT is trained with MLM and SOP, a substitute of the NSP task that aims at better modeling the coherence between sentences. Consistent with Lan et al. (2019), we sample two consecutive sentences in the same document as a positive\nsample, and reverse their order 50% of the time to create a negative sample.\nFor both baselines and ULR, we use the same set of parameters for 5 runs, and average scores on the GLUE test set are reported in Table 5. Although we expect NSP and SOP to help the model better understand the relationship between sentences and benefit tasks like natural language inference, they hardly improve the performance on GLUE according to our strict implementation. Specifically, NSP-BERT outperforms MLM-BERT on datasets such as CoLA, QNLI and QQP while less satisfactory on other tasks. SOP-BERT is on a par with MLM-BERT on three NLI tasks but it sharply decreases the score on other datasets. In general, single-sentence training with only the MLM objective accounts for better performance as described by Liu et al. (2019); Joshi et al. (2020). Besides, our training strategy which combines MLM and MiSAD yields the most considerable gains compared with other training objectives.\nTable 6 shows standard deviation, mean and maximum performance on CoLA/RTE/MRPC dev set when fine-tuning BERT and ULR-BERT over 5 random seeds, which clearly shows that our model is generally more stable and yields better results compared with BERT."
    }, {
      "heading" : "7.2 Effect of Sampling Strategies",
      "text" : "We compare our PMI-based n-gram sampling scheme with two alternatives. Specifically, we train the following two baseline models under the same model settings except for the sampling strategy. Random Spans We replace our n-gram module\nwith the masking strategy as proposed by Joshi et al. (2020), where the sampling probability of span length l is based on a geometric distribution l ∼ Geo(p). The parameter p is set to 0.2 and maximum span length lmax = 6. Named Entities We only retain named entities that are annotated in the Wikipedia Corpus.\nTable 7 shows the effect of different sampling schemes on the GLUE dev set. As we can see, our PMI-based n-gram sampling is preferable to other strategies on 6 out of 8 tasks. CoLA and RTE are more sensible to sampling strategies than other tasks. On average, using named entities and meaningful n-grams is better than randomly sampled spans. We attribute the source to the reason is that random span sampling ignores important semantic and syntactic structure of a sequence, resulting in a large number of meaningless segments. Compared with using only named entities, our PMI-based approach automatically discovers structures within any sequence and is not limited to any granularity, which is critical to pre-training universal language representation."
    }, {
      "heading" : "7.3 Application to Different Models",
      "text" : "Experiments on the universal analogy task reveal that our proposed training scheme can be adapted to various pre-trained langauge models. In this subsection, we compare our model with BERT, ALBERT and ELECTRA on GEOGRANNO and the GLUE benchmark.\nTable 8 shows the results on GEOGRANNO and the GLUE dev set, where our approach can enhance the performance of all three pre-trained mod-\nels. Among all the evaluated models, ULR-BERT achieves the largest gains on GLUE while ULRELECTRA obtains the most significant improvement on GEOGRANNO. It further verifies the effectiveness and universality of our model."
    }, {
      "heading" : "7.4 Effect of Sequence Length",
      "text" : "In previous experiments on GEOGRANNO, our model has shown considerable improvement over all three evaluated PrLMs. The task involves text matching between linguistic units at different levels where queries are sentences and labels are often phrases. Thus the performance on such task highly depends on the model’s ability to uniformly deal with linguistic units of different granularities.\nIn the following, we explore deeper details and interpretability of how our proposed objective act at different levels of linguistic units. Specifically,\nwe intuitively show the consistency of the representations learned by ULR-BERT by grouping the dataset according to query length |q| and the absolute difference between query length and Question length abs(|q| − |Q|), respectively.\nResults are shown in Table 9, which clearly shows that as the length of the query increases, the performance of BERT drops sharply. Similarly, BERT is more sensible to the difference between query length and Question length. In contrast, ULR-BERT is more stable when dealing with sequences of different lengths and is superior to BERT in terms of representation consistency, which we speculate is due to the interaction between different levels of linguistic units in the pretraining procedure."
    }, {
      "heading" : "8 Conclusion",
      "text" : "This work formally introduces universal language representation learning to enable unified vector operations among different language hierarchies. For such a purpose, we propose three highlighted ULR learning enhancement, including the newly designed training objective, Minimizing Symbolvector Algorithmic Difference (MiSAD). In detailed model implementation, we extend BERT’s pre-training objective to a more general level, which leverages information from sequences of different lengths in a comprehensive way. In addition, we provide a universal analogy dataset as a task-independent evaluation benchmark. Overall experimental results show that our proposed ULR model is generally effective in a broad range of NLP tasks including natural language question answering and so on."
    }, {
      "heading" : "A Universal Analogy",
      "text" : "As a new task, universal representation has to be evaluated in a multiple-granular analogy dataset. In this section, we introduce the procedure of constructing different levels of analogy datasets based on Google’s word analogy dataset.\nA.1 Word-level analogy\nThe goal of a word analogy task is to solve questions like “A is to B as C is to ?”, which is to retrieve the last word from the vocabulary given the first three words. The objective can be formulated as maximizing the cosine similarity between the target word embedding and the linear combination of the given vectors:\nd∗ = argmax d∗ cosine(c+ b− a, d) cosine(u, v) = u · v ‖u‖‖v‖\nwhere a, b, c, d represent embeddings of the corresponding words and are all normalized to unit lengths.\nWe construct a closed-vocabulary analogy task based on Google’s word analogy dataset through negative sampling. During evaluation, the model is expected to select the correct answer from 5 candidate words.\nA.2 Phrase/Sentence-level analogy\nTo investigate the arithmetic properties of vectors for higher levels of linguistic units, we present phrase and sentence analogy tasks based on the proposed word analogy dataset. Statistics are shown in Table 10.\nA.2.1 Semantic Semantic analogies can be divided into four subsets: “capital-common”, “capital-world”, “citystate” and “male-female”. The first two sets can be merged into a larger dataset: “capital-country”, which contains pairs of countries and their capital cities; the third involves states and their cities; the last one contains pairs with gender relations. Considering GloVe’s poor performance on wordlevel “country-currency” questions (<32%), we discard this subset in phrase and sentence-level analogies. Then we put words into contexts so that the resulting phrases and sentences also have linear relationships. For example, based on relationship\nAthens : Greece :: Baghdad : Iraq,\nwe select phrases and sentences that contain the word “Athens” from the English Wikipedia Corpus. We manually modify some words to ensure text coherence: “He was hired as being professor of physics by the university of Athens.” and create examples:\nhired by ... Athens : hired by ... Greece :: hired by ... Baghdad : hired by ... Iraq.\nHowever, we found that such a question is identical to word-level analogy for BOW methods like averaging GloVe vectors, because they treat embeddings independently despite the content and word order. To avoid lexical overlapping between sequences, we replace certain words and phrases with their synonyms and paraphrases, e.g.,\nhired by ... Athens : employed by ... Greece :: employed by ... Baghdad : hired by ... Iraq.\nA.2.2 Syntactic We consider three typical syntactic analogies: Tense, Comparative and Negation, corresponding to three subsets: “present-participle”, “positivecomparative”, “positive-negative”, where the model needs to distinguish the correct answer from “past tense”, “superlative” and “positive”, respectively. For example, given phrases\nPigs are bright : Pigs are brighter than goats :: The train is slow,\nthe model need to give higher similarity score to the sentence that contains “slower” than the one that contains “slowest”. Similarly, we add synonyms and synonymous phrases for each question to evaluate the model ability of learning context-aware embeddings rather than interpreting each word in the question independently. For instance, “pleasant” ≈ “not unpleasant” and “unpleasant” ≈ “not pleasant”."
    } ],
    "references" : [ {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Peter Clark", "Dagan Ido", "Giampiccolo Danilo." ],
      "venue" : "Proceedings of TAC.",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "Large scale question paraphrase retrieval with smoothed deep metric learning",
      "author" : [ "Daniele Bonadiman", "Anjishnu Kumar", "Arpit Mittal." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text (WNUT 2019), pages 68–75.",
      "citeRegEx" : "Bonadiman et al\\.,? 2019",
      "shortCiteRegEx" : "Bonadiman et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised dual paraphrasing for two-stage semantic parsing",
      "author" : [ "Ruisheng Cao", "Su Zhu", "Chenyu Yang", "Chen Liu", "Rao Ma", "Yanbin Zhao", "Lu Chen", "Kai Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evalu-",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal sentence encoder for English",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "Conversational semantic parsing for dialog state tracking",
      "author" : [ "Johannsen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8107–8117.",
      "citeRegEx" : "Johannsen.,? 2020",
      "shortCiteRegEx" : "Johannsen.",
      "year" : 2020
    }, {
      "title" : "Word association norms, mutual information, and lexicography",
      "author" : [ "Kenneth Ward Church", "Patrick Hanks." ],
      "venue" : "27th Annual Meeting of the Association for Computational Linguistics, pages 76–83.",
      "citeRegEx" : "Church and Hanks.,? 1989",
      "shortCiteRegEx" : "Church and Hanks.",
      "year" : 1989
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Pre-training with whole word masking for Chinese BERT",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu" ],
      "venue" : null,
      "citeRegEx" : "Cui et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "Optimized transformer models for FAQ answering",
      "author" : [ "Sonam Damani", "Kedhar Nath Narahari", "Ankush Chatterjee", "Manish Gupta", "Puneet Agrawal." ],
      "venue" : "PAKDD 2020, Lecture Notes in Computer Science.",
      "citeRegEx" : "Damani et al\\.,? 2020",
      "shortCiteRegEx" : "Damani et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Don’t paraphrase, detect! rapid and effective data collection for semantic parsing",
      "author" : [ "Jonathan Herzig", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Herzig and Berant.,? 2019",
      "shortCiteRegEx" : "Herzig and Berant.",
      "year" : 2019
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2017
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut" ],
      "venue" : null,
      "citeRegEx" : "Lan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "The winograd schema challenge",
      "author" : [ "Hector J. Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "KR’12, page 552–561.",
      "citeRegEx" : "Levesque et al\\.,? 2012",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2012
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Comparison of the predicted and observed secondary structure of t4 phage lysozyme",
      "author" : [ "B.W. Matthews." ],
      "venue" : "Biochimica et Biophysica Acta (BBA) Protein Structure, 405(2):442 – 451.",
      "citeRegEx" : "Matthews.,? 1975",
      "shortCiteRegEx" : "Matthews.",
      "year" : 1975
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "URL https://s3us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/language understand-",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "FAQ retrieval using queryquestion similarity and BERT-based query-answer relevance",
      "author" : [ "Wataru Sakata", "Tomohide Shibata", "Ribeka Tanaka", "Sadao Kurohashi." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-",
      "citeRegEx" : "Sakata et al\\.,? 2019",
      "shortCiteRegEx" : "Sakata et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "SRILM - an extensible language modeling toolkit",
      "author" : [ "Andreas Stolcke." ],
      "venue" : "Seventh international conference on spoken language processing.",
      "citeRegEx" : "Stolcke.,? 2002",
      "shortCiteRegEx" : "Stolcke.",
      "year" : 2002
    }, {
      "title" : "Learning general purpose distributed sentence representations via large scale multi-task learning",
      "author" : [ "Sandeep Subramanian", "Adam Trischler", "Yoshua Bengio", "Christopher J Pal." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Subramanian et al\\.,? 2018",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretraining is (almost) all you need: An application to commonsense reasoning",
      "author" : [ "Alexandre Tamborrino", "Nicola Pellicanò", "Baptiste Pannier", "Pascal Voitot", "Louise Naudin." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Tamborrino et al\\.,? 2020",
      "shortCiteRegEx" : "Tamborrino et al\\.",
      "year" : 2020
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "StructBERT: Incorporating language structures into pretraining for deep language understanding",
      "author" : [ "Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : ", word) enables linguistics-meaningful arithmetic calculation among different vectors, also known as word analogy (Mikolov et al., 2013).",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we propose a universal language representation learning method that generates fixedsized vectors for sequences of different lengths based on pre-trained language models (Devlin et al., 2019; Lan et al., 2019; Clark et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 243
    }, {
      "referenceID" : 17,
      "context" : "In this paper, we propose a universal language representation learning method that generates fixedsized vectors for sequences of different lengths based on pre-trained language models (Devlin et al., 2019; Lan et al., 2019; Clark et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 243
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we propose a universal language representation learning method that generates fixedsized vectors for sequences of different lengths based on pre-trained language models (Devlin et al., 2019; Lan et al., 2019; Clark et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 243
    }, {
      "referenceID" : 21,
      "context" : "Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : ", 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : ", 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al.",
      "startOffset" : 15,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : ", 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : ", 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : ", 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : ", 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 35,
      "context" : ", 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "ALBERT (Lan et al., 2019) is trained with Sentence-Order Prediction (SOP) as a replacement of NSP.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 32,
      "context" : "StructBERT (Wang et al., 2020) combines NSP and SOP to learn inter-sentence structural information.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "Nevertheless, RoBERTa (Liu et al., 2019) and SpanBERT (Joshi et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and SpanBERT (Joshi et al., 2020) show that single-sequence training is better than the sentence-pair scenario.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "Besides, BERT-wwm (Cui et al., 2019), StructBERT (Joshi et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : ", 2019), StructBERT (Joshi et al., 2020), SpanBERT (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 32,
      "context" : ", 2020), SpanBERT (Wang et al., 2020) perform MLM on higher linguistic levels, augmenting the MLM objective by masking whole words, trigrams or spans, respectively.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "terance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : ", 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : ", 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019).",
      "startOffset" : 48,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : ", 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019).",
      "startOffset" : 48,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "Different from such random sampling strategy, our method is based on point-wise mutual information (PMI) (Church and Hanks, 1989) that makes efficient use of statistics and automatically extracts meaningful n-grams from unlabeled corpus.",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "We first collect all n-grams with lengths up to N using the SRILM toolkit1 (Stolcke, 2002), and compute PMI scores for all the n-grams based on their occurrences.",
      "startOffset" : 75,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "While the MLM training objective as in BERT (Devlin et al., 2019) and its extensions (Cui et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : ", 2019) and its extensions (Cui et al., 2019; Joshi et al., 2020; Wang et al., 2020) are widely used for pre-trained contextualized language modeling, they do not focus on our concerned ULR,",
      "startOffset" : 27,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and its extensions (Cui et al., 2019; Joshi et al., 2020; Wang et al., 2020) are widely used for pre-trained contextualized language modeling, they do not focus on our concerned ULR,",
      "startOffset" : 27,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : ", 2019) and its extensions (Cui et al., 2019; Joshi et al., 2020; Wang et al., 2020) are widely used for pre-trained contextualized language modeling, they do not focus on our concerned ULR,",
      "startOffset" : 27,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "Instead of training from scratch, we initialize our model with the officially released checkpoints of BERT (Devlin et al., 2019), ALBERT (Lan et al.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : ", 2019), ALBERT (Lan et al., 2019) and ELECTRA (Clark et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "We use Adam optimizer (Kingma and Ba, 2017) with initial learning rate of 5e-5 and linear warmup over the first 10% of the training steps.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "Word-level Recall that in a word analogy task (Mikolov et al., 2013), two pairs of words that share the same type of relationship, denoted as A : B :: C : D, are involved.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of tasks that are widely used to evaluate the performance of a model in language understanding.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 33,
      "context" : ", 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 34,
      "context" : "Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : ", 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : ", 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al.",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : ", 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "GEOGRANNO (Herzig and Berant, 2019) contains natural language paraphrases paired with logical forms.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "On the universal analogy task, we adopt three types of baselines including bag-of-words (BoW) model from pre-trained word embeddings: GloVe (Pennington et al., 2014), sentence embedding models: InferSent (Conneau et al.",
      "startOffset" : 140,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : ", 2014), sentence embedding models: InferSent (Conneau et al., 2017), GenSen (Subra-",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : ", 2018), USE (Cer et al., 2018) and LASER (Artetxe and Schwenk, 2019), and pretrained contextualized language models: BERT, ALBERT and ELECTRA.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : ", 2018) and LASER (Artetxe and Schwenk, 2019), and pretrained contextualized language models: BERT, ALBERT and ELECTRA.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "BERT The officially released pre-trained BERT models (Devlin et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "“mc” and “pc” are Matthews correlation coefficient (Matthews, 1975) and Pearson correlation coefficient, respectively.",
      "startOffset" : 51,
      "endOffset" : 67
    } ],
    "year" : 2021,
    "abstractText" : "Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pretraining scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.",
    "creator" : "LaTeX with hyperref"
  }
}