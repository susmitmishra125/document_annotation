{
  "name" : "2021.acl-long.47.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks",
    "authors" : [ "Rabeeh Karimi Mahabadi", "Sebastian Ruder", "Mostafa Dehghani" ],
    "emails" : [ "rabeeh.karimimahabadi@epfl.ch", "ruder@google.com", "dehghani@google.com", "james.henderson@idiap.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 565–576\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n565"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transfer learning from pretrained large-scale language models yields state-of-the-art results in a variety of tasks (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b). As a highly expressive and abstract framework, Raffel et al. (2020) explored the landscape of transfer learning by converting text-based natural language processing (NLP) problems into a sequence-to-sequence format to train a unified model on several tasks simultaneously. Multi-task learning with pretrained language models (Ruder, 2017) is appealing for multiple reasons: 1) Training individual models per task results in higher computational costs, which hinders deployment and maintenance. These costs are substantially reduced by training a single\n∗Work done while the author was at Google.\nmodel. 2) Fine-tuning the model across multiple tasks allows sharing information between the different tasks and positive transfer to other related tasks. Specifically, when target datasets have limited training data, multi-task learning improves the performance compared to individually trained models (Liu et al., 2019a; Ratner et al., 2018). However, multi-task fine-tuning can result in models underperforming on high-resource tasks due to constrained capacity (Arivazhagan et al., 2019; McCann et al., 2018). An additional issue with multi-task fine-tuning is the potential for task interference or negative transfer,\nwhere achieving good performance on one task can hinder performance on another (Wang et al., 2019c).\nAs an alternative to fine-tuning (Howard and Ruder, 2018), adapter layers (Houlsby et al., 2019) insert a small number of additional parameters per task into the model. During fine-tuning, only the adapter modules, layer normalizations, and parameters of the final classification layer are updated, while the original pretrained model parameters remain frozen. Such task-specific adapters eliminate negative task interference by encapsulating task-specific information (Pfeiffer et al., 2020). However, so far there has not been an effective and parameter-efficient way to share information across multiple adapters to enable positive transfer to low-resource and related tasks.\nTo address this problem and to enable sharing information across tasks while reaping the benefits of adapter layers, as depicted in Figure 1, we propose HYPERFORMER++, which employs a compact hypernetwork (Ha et al., 2017; Oswald et al., 2020) shared across tasks and layers. The hypernetwork learns to generate task and layer-specific adapter parameters, conditioned on task and layer id embeddings. The hypernetwork is jointly learned between all tasks and is thus able to share information across them, while negative interference is minimized by generating separate adapter layers for each task. For each new task, our model only requires learning an additional task embedding, reducing the number of trained parameters.\nWe use the encoder-decoder T5 model (Raffel et al., 2020) as the underlying model for our experiments and evaluate on the standard GLUE benchmark (Wang et al., 2019b). We achieve strong gains over both the T5BASE model as well as adapters (Houlsby et al., 2019). To our knowledge, this is the first time that adapters have been successfully integrated into a stateof-the-art encoder-decoder model beyond machine translation (Bapna and Firat, 2019), demonstrating that our method effectively balances sharing information across tasks while minimizing negative transfer.\nIn summary, we make the following contributions: (1) We propose a parameter-efficient method for multitask fine-tuning based on hypernetworks and adapter layers. (2) We demonstrate that our method scales more efficiently than prior work. (3) We provide empirical results on GLUE demonstrating the effectiveness of the proposed method on multi-task learning. (4) We perform extensive few-shot domain transfer experiments, which reveal that the captured shared knowledge can positively transfer to unseen in-domain tasks. We release our code to facilitate future work."
    }, {
      "heading" : "2 HYPERFORMER",
      "text" : "In this section, we present our HYPERFORMER model, which integrates hypernetwork-based adapter layers into a multi-task transformer model. In §2.4, we introduce a parameter-efficient variant of this model, called HYPERFORMER++.\nProblem formulation: We consider a general multi-task learning problem, where we are given the data from a set of tasks {Dτ}Tτ=1, where T is the total number of tasks andDτ={(xiτ ,yiτ)}Nτi=1 shows the training data for τ-th task with Nτ samples. We assume we are also given a large-scale pretrained language model fθ(.) parameterized by θ that computes the output for inputxiτ . Standard multi-task fine-tuning minimizes the following loss on the training set: L(θ,{Dτ}Tτ=1)= T∑ τ=1 ∑ (xiτ ,y i τ )∈Dτ wτ l ( fθ(x i τ ),y i τ ) , (1)\nwhere l is typically the cross-entropy loss, and wτ shows the sampling weight for τ-th task. Our goal is to finetune the pretrained model in a multi-task learning setup efficiently, while allowing sharing information across tasks and at the same time, enabling the model to adapt to each individual task.\nThe key idea of our approach, depicted in Figure 1, is to learn a parametric task embedding {Iτ}Tτ=1 for each task, and then feed these task embeddings to hypernetworks parameterized by ν that generate the task-specific adapter layers (Houlsby et al., 2019). We insert adapter modules within the layers of a pretrained model, making the final model of Xν(xiτ , θ, Iτ ) parameterized by ν that computes the output for input xiτ . During training, we only train hypernetwork parameters ν, task embeddings {Iτ}Tτ=1, and layer normalizations in fθ(.), while the rest of the pretrained model parameters θ are fixed:\nL(ν,{Iτ}Ti=1,{Dτ}Tτ=1)= T∑ τ=1 ∑ (xiτ ,y i τ )∈Dτ wτ l ( Xν(xiτ ,θ,Iτ ),yiτ ) , (2)\nThe hypernetworks capture the shared information across tasks in a multi-task learning model enabling positive transfer between related domains and transferable tasks, while adapters are reducing negative interference, encapsulating task-specific information.\nBase model: All of our models are built on top of the state-of-the-art T5 transformer model (Raffel\net al., 2020). This model frames text-based language tasks as sequence-to-sequence problems. T5 consists of an encoder-decoder Transformer (Vaswani et al., 2017) with minor modifications (Raffel et al., 2020). The model is trained simultaneously on multiple tasks, obtaining state-of-the-art performance across a diverse set of tasks. We use the T5 framework as it enables training a universal model that interfaces with many language tasks. Our model has three main components: 1) task conditional adapter layers; 2) task conditional layer normalizations; and 3) hypernetworks that generate task-specific parameters. We next describe these components."
    }, {
      "heading" : "2.1 Task Conditional Adapter Layers",
      "text" : "Prior work has shown that fine-tuning all parameters of the model can result in a sub-optimal solution, particularly for resource-limited datasets (Peters et al., 2019). As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1. Adapters introduce no change to the structure or parameters of the original model.\nIn this work, we propose conditional adapter modules, in which we generate the adapters weights based on input task embeddings using shared hypernetworks (Ha et al., 2017), which capture information across tasks that can be used to positively transfer to other relevant tasks.\nEach layer of a transformer model consists of an attention block and a feed-forward block, each followed by a skip connection. Following Houlsby et al. (2019), as depicted in Figure 1, we introduce a conditional adapter layer after each block before the skip connection. The conditional adapter layer Alτ for layer l consists of a down-projection,Dlτ ∈Rh×d, GeLU non-linearity (Hendrycks and Gimpel, 2016), and up-projection U lτ ∈Rd×h, where h is the input dimension, and d is the bottleneck dimension for the adapter layer, mathematically defined as:\nAlτ(x)=LN l τ ( U lτ (GeLU(D l τ (x))) ) +x, (3)\nwhere x is the input hidden state and LN lτ is the conditional layer norm defined in the next section. We generate adapter weights (U lτ , D l τ ) through a hypernetwork described in §2.3."
    }, {
      "heading" : "2.2 Task Conditional Layer Normalization",
      "text" : "Conventional layer normalization (Ba et al., 2016) is defined as:\nLN lτ(x i τ )=γ l τ xiτ−µτ στ +βlτ , (4)\nwhere is the element-wise multiplication between two vectors, and γlτ and β l τ are learnable parameters with the same dimension as xiτ . Values of µτ and στ show the mean and standard deviation of training data for the τ-th task.\nTo allow the layer normalization inside adapters to adapt to each task, inspired by Perez et al. (2018); De Vries et al. (2017), we generate γlτ , β l τ via a hypernetwork as a function of task embeddings (§2.3)."
    }, {
      "heading" : "2.3 Task Conditioned Hypernetworks",
      "text" : "In order to have a model that can share information while being able to adapt to each individual task, we generate the parameters of task conditional adapter layers and layer normalization using hypernetworks. A hypernetwork is a network that generates the weights of another network (Ha et al., 2017).\nThe hypernetworks capture the shared information, while the generated task conditional adapters and layer normalization allow the model to adapt to each individual task to reduce negative task interference.\nLearned task embedding: We first compute a task embedding Iτ ∈Rt for each individual task using a task projector network hI(.), which is a multi-layer perceptron consisting of two feed-forward layers and a ReLU non-linearity:\nIτ=hI(zτ ), (5)\nwhere zτ ∈Rt ′\ncan be a learnable parameter or any pretrained task features (Vu et al., 2020), and the task projector network hI(.) learns a suitable compressed task embedding from input task features. In this work, we consider a parametric zτ to allow end-to-end training which is convenient in practice.1\nRemoving task prefixes: The T5 model prepends task-specific prefixes to the input sequence for conditioning. For instance, when training on CoLA (Warstadt et al., 2019), cola sentence: is prepended to each sample. Instead, we remove task prefixes and use task embeddings for conditioning.\nTask conditioned hypernetworks: We consider simple linear layers as hypernetworks that are functions of input task embeddings Iτ . We introduce these hypernetworks in each layer of the transformer. We define hypernetwork hlA(.) that generates task conditional adapter weights (U lτ ,D l τ ):\n1We ran some pilot experiments with pretrained task embeddings (Vu et al., 2020), but did not observe extra benefits.\n(U lτ ,D l τ ):=h l A(Iτ )=\n( WU l ,WD l ) Iτ , (6)\nwhere WU l ∈ R(d×h)×t and WDl ∈ R(h×d)×t are the respective hypernetwork parameters. We additionally define the hypernetwork hlLN(.) that computes the layer normalization parameters:\n(γlτ ,β l τ ):=h l LN(Iτ )=\n( Wγ l ,Wβ l ) Iτ , (7)\nwhereWγ l∈Rh×t andWβl∈Rh×t.\n2.4 HYPERFORMER++ A downside of introducing a separate hypernetwork in each layer of the Transformer is that it increases the overall number of parameters. We, therefore, propose to share hypernetworks across transformer layers. By having a shared hypernetwork that is reusable, this strategy results in a substantial reduction in the number of parameters. However, reapplying the same hypernetwork across all the layers introduces weight sharing across target parameters, which may not be desirable. To allow for a flexible parameterization of task conditional adapters/layer normalization, for a transformer of L layers, we introduce a set of layer id embeddings I = {li}Li=1, and adapter position embeddings P={pj}2j=1, which specify the position of adapter layers in each transformer block (after the attention layer or feed-forward layer), which are used as additional inputs to the hypernetworks. For simplicity, we consider li∈Rt, pj∈Rt, and zτ ∈Rt. We feed a concatenation of (zτ ,li,pj) to a similar task projector network h′I as in Eq. (5):\nIτ=h ′ I(zτ ,li,pj), (8)\nwhich is then followed by a shared layer normalization to compute final task embeddings Iτ ∈Rt to the hypernetwork. This way, the hypernetwork is able to produce distinct weights for each task, adapter position, and layer of a transformer. Furthermore, layer id and adapter position embeddings are parameters that are learned via back-propagation, allowing us to train the whole model end-to-end conveniently."
    }, {
      "heading" : "3 Experiments",
      "text" : "Datasets: Following Raffel et al. (2020), we evaluate the performance of the models on the GLUE benchmark (Wang et al., 2019b). This benchmark covers multiple tasks of paraphrase detection (MRPC, QQP), sentiment classification (SST-2), natural language inference (MNLI, RTE, QNLI), and linguistic acceptability (CoLA).2 The original test\n2Following Raffel et al. (2020); Devlin et al. (2019), as a common practice, due to the adversarial nature of WNLI with respect to the training set, we do not experiment with WNLI.\nsets are not publicly available, and following Zhang et al. (2021), for datasets fewer than 10K samples (RTE, MRPC, STS-B, CoLA), we divide the original validation set in half, using one half for validation and the other for the test. For the other larger datasets, we split 1k samples from the training set as our validation data and test on the original validation set.\nExperimental details: We use the HuggingFace implementation (Wolf et al., 2020a) of the T5 model (Raffel et al., 2020). We fine-tune all models with a constant learning rate of 0.0003 and following Raffel et al. (2020), we use 218=262144 steps in all experiments. We save a checkpoint every 1000 steps for all models (see also §A). Raffel et al. (2020) report the results based on the best checkpoint for each task independently. In contrast, we focus on the more realistic setting where we report the results on a single checkpoint with the highest average validation performance across all tasks. The hyperparameters are selected in the same manner. In contrast to prior work (Houlsby et al., 2019), we do not learn a separate output layer for each task but instead share a frozen output layer for all the tasks, which makes our setting more parameter-efficient than prior work and is an advantage of multi-task learning with encoder-decoder models.3\nBaselines: We compare to the strong adapter baseline (Houlsby et al., 2019). Following Houlsby et al. (2019), we add adapters modules for each task after the two feed-forward modules in each transformer block of the T5 model. As suggested in Houlsby et al. (2019), we train the layer normalization parameters inside the T5 model, per task. We refer to this method as Adapters. We additionally propose a variant of this model, in which we share all layer normalization parameters (T5 and adapters) across all tasks. We refer to this model as Adapters†. We compare our models to the state-of-the-art T5 model, in which we fine-tune all parameters of the model on all tasks. We refer to this method as T5SMALL/T5BASE in experiments.\nSampling tasks: During training, we sample tasks with conventional temperature-based sampling with temperature T =10 for all methods. We sample different tasks proportional to p1/Tτ where pτ= Nτ∑T\ni=1Nτ\nand Nτ is the number of training samples for the τth task. We did not experiment with more complex sampling strategies (Raffel et al., 2020) or tuning of T .\n3According to our initial experiments, fine-tuning the final output layer did not improve performance for adapter-based methods."
    }, {
      "heading" : "3.1 Results on the GLUE Benchmark",
      "text" : "Table 1 shows the results on GLUE for single-task and multi-task training. We experiment with reduction factors of r = {8,16,32} for all adapter-based methods, where r = hd . We report the results both with T5SMALL (6 layers and 60M parameters) and T5BASE models (12 layers and 222M parameters).\nOverall, our proposed HYPERFORMER++ obtains strong gains over Adapters (82.51 versus 79.53 for T5SMALL and 86.48 versus 84.88 for T5BASE) while being more parameter-efficient.\nOur variant of Adapters†, which shares layer norms across tasks, outperforms prior work (Houlsby et al., 2019), which does not share such information (80.85 versus 79.53 for T5SMALL and 85.83 versus 84.88 for T5BASE). This demonstrates that in encoder-decoder models such as T5 more sharing of information across tasks is beneficial.\nOur proposed HYPERFORMER obtains consistent improvement over our proposed Adapters† method. We attribute this improvement to the ability to learn the shared information across tasks through our hypernetworks. Interestingly, HYPERFORMER++ obtains similar performance as HYPERFORMER while being more than an order of magnitude more parameterefficient. Adapter modules thus seem to be similar enough so that much of their information can be modeled by a single, appropriately conditioned network.\nCompared to single-task fine-tuning of all param-\neters, our methods on average improve the results by 0.45 for T5SMALL and 1.81 for T5BASE with substantial improvement on low-resource datasets like CoLA (63.73 versus 54.85) and RTE (75.36 versus 67.39) due to shared hypernetworks that capture the shared information and enable positive transfer effects.\nWe also report the total number of parameters and trainable parameters for all methods in Table 1. For adapter-based methods, the number of parameters varies based on the adapter size (we report all numbers with r=32). The multiple in terms of the number of parameters of HYPERFORMER++BASE with regard to T5BASE is 1.02×with only 0.29% trainable parameters per task. Note that by keeping the output layer frozen for AdaptersSMALL and AdaptersBASE, they require 5.51× and 2.53× fewer parameters respectively compared to a direct application of prior work (Houlsby et al., 2019). Despite using more efficient baselines, compared to AdaptersBASE, HYPERFORMER++BASE requires 3× fewer trainable parameters."
    }, {
      "heading" : "3.2 Few-shot Domain Transfer",
      "text" : "Finally, we assess how well a trained HYPERFORMER can generalize to new tasks. We evaluate performance on 5 tasks and 7 datasets. In particular, we consider 1) the natural language inference (NLI) datasets SciTail (Khot et al., 2018), and CB (De Marneffe et al., 2019) from SuperGLUE (Wang et al., 2019a) 2) the question answering (QA) dataset BoolQ (Clark\net al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002).\nFor CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets.\nWe consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the adapter and the task embedding respectively trained on the most similar GLUE task for initialization, i.e. MNLI for NLI, QNLI for QA, SST-2 for sentiment analysis, and QQP for paraphrase detection. Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI. We show the results of full fine-tuning of all model’s parameters, Adapters†, and HYPERFORMER++4 in Table 2. Our method significantly surpasses the baselines on the majority of settings."
    }, {
      "heading" : "3.3 Low-resource Fine-tuning",
      "text" : "Given that our model HYPERFORMER++BASE has substantially fewer trainable parameters than T5BASE, we investigate whether it generalizes better in a low-resource setting. We subsample each individual task in GLUE for varying training sizes. We train the models for 15,000 steps, which we found to be\n4We finetune hypernetworks and task embeddings parameters. We also tried only fine-tuning the task embedding but found that this achieves lower performance in the few-shot setting and comparable performance with more samples.\nsufficient to allow them to converge. Figure 2 shows the results. HYPERFORMER++BASE substantially improves results with limited training data, indicating more effective fine-tuning in this regime."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Parameter Efficiency",
      "text" : "In this section, we compare the number of parameters of HYPERFORMER++ with Adapters.\nAdapters parameters: The standard setting (Houlsby et al., 2019) employs two adapters per layer for each task. Each adapter layer has 2hd parameters for projection matrices (U lτ andD l τ ) and 2h parameters for the layer normalization. The total number of parameters for Adapters for L Transformer layers in both an encoder and a decoder across T tasks is, therefore, 4TL(2hd+2h), which scales linearly with the number of tasks times the number of layers.\nHYPERFORMER++ parameters: Our approach learns a task feature embedding per task, consisting of Tt parameters. We additionally employ layer id and adapter position embeddings in the encoder and decoder, which require 2(2+L)t parameters, with a fixed embedding size of t for all these feature embeddings. We consider a separate task projector networks h′I for encoder and decoder, which is in both cases a two-layer MLP, consisting of a total of 2(3te+et) parameters, where e=128 is the hidden dimension for the task-projector network. Our hypernetwork for adapters in encoder/decoder consists of 2(2thd) parameters and our layer normalization hypernetwork consists of 2(2th) parameters. In total, this results in t(T+4+2L)︸ ︷︷ ︸\nTask features + 8te+2t(2hd+2h)︸ ︷︷ ︸ Hypernetworks parameters.\nThe total number of parameters for hypernetworks remains constant, while the task feature parameters scale with the number of tasks or layers times t, where t=64 in our experiments.\nIn settings with a large number of layers and a large number of tasks, since t 2hd+2h and T+L TL, our method is much more parameter-efficient compared to Adapters. In the current setting, the term hd is the largest term, and the factor 2TL for Adapters is larger than the factor t for HYPERFORMER++."
    }, {
      "heading" : "4.2 Do Extra Parameters Make a Difference?",
      "text" : "While our HYPERFORMER++ is more parameterefficient than the baselines, the number of parameters of HYPERFORMER per task is higher compared to Adapters†. To confirm that the improvements of\nHYPERFORMER are due to its capability of sharing information across tasks and not the number of parameters, as an ablation, we run the Adapters† with r = {2,4} and choose the model performing the best on the validation set. This allows Adapters† to have a higher number of parameters compared to HYPERFORMER. We report the results in Table 3 and compare them with results of HYPERFORMER in Table 1. The results demonstrate that even with an increased number of parameters, Adapters† is not able to reach the performance of HYPERFORMER, and HYPERFORMER performs substantially better."
    }, {
      "heading" : "4.3 Impact of the Framework Components",
      "text" : "We investigate the impact of the components of our framework including: (1) task conditional adapter blocks; (2) task conditional layer normalization; (3) task projection network; (4) fine-tuning of layer normalizations in the T5 model; (5) task conditional layer normalization in adapter modules and fine-tuning of layer normalizations inside the T5 model. We consider our small model of Table 1 and train different variants of it. Table 4 shows the results on GLUE, demonstrating that each component of the model contributes positively to its final performance."
    }, {
      "heading" : "4.4 Visualization of Task Embeddings",
      "text" : "To analyze what HYPERFORMER++BASE has learned about the relations between different tasks, we visualize the learned task embeddings for the models trained\nwith the largest number of samples in Table 1 and 2. Figure 3 illustrates the 2D vector projections of task embeddings using PCA (Wold et al., 1987). Interestingly, the observed groupings correspond to similar tasks. This shows that learned task embeddings by HYPERFORMER++BASE are meaningful. For CB, an NLI dataset despite being initialized from MNLI, after few-shot training the task embedding is closest to RTE, another NLI dataset. This is plausible as premises and hypotheses in both the discourse-based CB and the news and Wikipedia-based RTE are more complex compared to MNLI. The sentence similarity dataset STS-B is grouped close to the MRPC paraphrase dataset. CoLA, which focuses on linguistic acceptability is very different from other tasks and is not grouped with any of the observed task embeddings. In addition, the task embeddings for 1) all the sentiment analysis datasets namely SST-2, Yelp polarity, and IMDB; 2) the two large-scale NLI datasets namely MNLI and SciTail; 3) question answering datasets, i.e. BoolQ and QNLI; and 4) paraphrase datasets namely QQP and PAWS are each grouped together."
    }, {
      "heading" : "5 Related Work",
      "text" : "Multi-task learning: Multi-task learning, i.e., learning a unified model to perform well on multiple different tasks, is a challenging problem in NLP. It requires addressing multiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019). Liu et al. (2019a) proposed Multi-Task Deep Neural Network (MTDNN) for learning from multiple NLU tasks. Although MTDNN obtains impressive results on GLUE, it applies multi-task learning as a form of pretraining followed by task-specific fine-tuning. Concurrently\nwith us, Tay et al. (2021) propose a multi-task learning method by training task-conditioned hyper networks; however, their method is 43x less parameter efficient compared to ours. In another line of research, Clark et al. (2019b) proposed to learn multi-task models with knowledge distillation. Houlsby et al. (2019) trained adapters for each task separately, keeping the model fixed. Stickland and Murray (2019) share the model parameters across tasks and introduce task-specific adapter parameters, which is more parameter-inefficient than our method.\nHypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights of the target model. Our method is substantially more efficient as we do not generate all the weights of the target model but a very small number of parameters for adapter modules to allow the model to adapt to each individual task efficiently. Similarly, Jin et al. (2020) generate the full model from task-specific descriptions in different domains whereas we efficiently generate only small adapter modules for each task.\nPrior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020). Meta-learning approaches are notoriously slow to train. In addition, generating softmax parameters requires a substantially higher number of parameters, leaves the method unable to adapt the lower layers of the model, and restricts their application to classification tasks.\nIn contemporaneous work, Üstün et al. (2020) proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings. Their study is limited to multilingual dependency parsing, while our work studies multi-task learning and applies to several tasks thanks to the general sequence-to-sequence nature of our model. Moreover, their number of trainable parameters is 2.88× larger than their base model since they employ a contextual parameter generator in each layer. In contrast, we use a single compact hypernetwork allowing us to efficiently condition on multiple tasks and layers of a transformer model."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a parameter-efficient method for multi-task fine-tuning. Our approach is to train shared hypernetworks to generate task-specific adapters conditioned on the task, layer id, and adapter position embeddings. The shared hypernetworks capture the knowledge across tasks and enable positive transfer to low-resource and related tasks, while task-specific layers allow the model to adapt to each individual task. Extensive experiments show that our method obtains strong improvement over multi-task learning on the GLUE benchmark, and substantially improves the in-domain task generalization."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to Dani Yogatama, Neil Houlsby, and Colin Raffel for feedback on a draft of this paper. We would like to also thank Adam Paszke, Jamie Kiros, and George Dahl for useful comments and discussions."
    }, {
      "heading" : "A Experimental Details",
      "text" : "Computing infrastructure: We run the experiments in Table 1 on 4 GPUs, and the rest of the experiments on 1 GPU on a heterogeneous cluster with Tesla V100, Tesla A100, Tesla P4, and GTX1080ti GPUs.\nHyperparameters: We use a batch size of 64 for T5SMALL and 32 for T5BASE to fit the GPU memory. We set the dimension of the task feature embedding (zτ ) to t′=512, and the dimension of the task embedding (Iτ ) to t=64. For low-resource fine-tuning in §3.3, we use reduction factors of {16,32,64}.\nData pre-processing: We download all datasets from the HuggingFace Datasets library (Wolf et al., 2020b). Following Raffel et al. (2020), we cast all datasets into a sequence-to-sequence format, and recast STS-B as a 21-class classification task by rounding its target scores to their nearest increment of 0.2.\nPerformance evaluation: Table 5 and 6 present the efficiency evaluation in terms of memory, and time for all the methods measured on the GLUE benchmark. We report the time for 1000 training steps.\nOur approach has several attractive properties. Our HYPERFORMER++BASE approach offers a much better memory usage with low-overhead, while HYPERFORMERBASE and T5BASE cause substantial memory overhead. In dealing with large-scale transformer models like T5, efficient memory usage is of paramount importance. Second, in terms of training time, our method is much faster than Adapters†BASE. Relative to T5BASE, HYPERFORMER++BASE increases the training time by 30.49%, while Adapters†BASE causes the substantial training time overhead of 84.93%.\nImpact of adapter’s bottleneck size on the performance Similar to (Houlsby et al., 2019), adapter’s reduction factor needs to be set per dataset. Table 7 shows the validation performance of HYPERFORMER++ on the GLUE tasks for different adapters’ reduction factors. While the pattern may not be always consistent, generally, smaller datasets seem to benefit more from smaller bottleneck size, i.e., less parameters for adapters, while the opposite is the case for larger datasets, which require more modeling capacity."
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation in the wild: Findings and challenges",
      "author" : [ "Naveen Arivazhagan", "Ankur Bapna", "Orhan Firat", "Dmitry Lepikhin", "Melvin Johnson", "Maxim Krikun", "Mia Xu Chen", "Yuan Cao", "George Foster", "Colin Cherry" ],
      "venue" : null,
      "citeRegEx" : "Arivazhagan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Paws: Paraphrase adversaries from word scrambling",
      "author" : [ "Jason Baldridge", "Luheng He", "Yuan Zhang." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Baldridge et al\\.,? 2019",
      "shortCiteRegEx" : "Baldridge et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks",
      "author" : [ "Trapit Bansal", "Rishikesh Jha", "Andrew McCallum." ],
      "venue" : "COLING.",
      "citeRegEx" : "Bansal et al\\.,? 2020",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "author" : [ "Christopher Clark", "Kenton Lee", "Ming-Wei Chang", "Tom Kwiatkowski", "Michael Collins", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Clark et al\\.,? 2019a",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Bam! born-again multi-task networks for natural language understanding",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Urvashi Khandelwal", "Christopher D Manning", "Quoc Le." ],
      "venue" : "ACL.",
      "citeRegEx" : "Clark et al\\.,? 2019b",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Senteval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "LREC.",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "The commitmentbank: Investigating projection in naturally occurring discourse",
      "author" : [ "Marie-Catherine De Marneffe", "Mandy Simons", "Judith Tonhauser." ],
      "venue" : "proceedings of Sinn und Bedeutung.",
      "citeRegEx" : "Marneffe et al\\.,? 2019",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2019
    }, {
      "title" : "Modulating early visual processing by language",
      "author" : [ "Harm De Vries", "Florian Strub", "Jérémie Mary", "Hugo Larochelle", "Olivier Pietquin", "Aaron C Courville." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Vries et al\\.,? 2017",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Hypernetworks",
      "author" : [ "David Ha", "Andrew Dai", "Quoc V. Le." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Ha et al\\.,? 2017",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2017
    }, {
      "title" : "Gaussian error linear units (gelus)",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:1606.08415.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2016",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "ICML.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal Language Model Fine-tuning for Text Classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "ACL.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Language to network: Conditional parameter adaptation with natural language descriptions",
      "author" : [ "Tian Jin", "Zhun Liu", "Shengjia Yan", "Alexandre Eichenberger", "Louis-Philippe Morency." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Scitail: A textual entailment dataset from science question answering",
      "author" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Khot et al\\.,? 2018",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "COLING.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "ACL.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "ACL.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Continual learning with hypernetworks",
      "author" : [ "Johannes Von Oswald", "Christian Henning", "João Sacramento", "Benjamin F Grewe." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Oswald et al\\.,? 2020",
      "shortCiteRegEx" : "Oswald et al\\.",
      "year" : 2020
    }, {
      "title" : "Film: Visual reasoning with a general conditioning layer",
      "author" : [ "Ethan Perez", "Florian Strub", "Harm De Vries", "Vincent Dumoulin", "Aaron Courville." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Perez et al\\.,? 2018",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2018
    }, {
      "title" : "To tune or not to tune? adapting pretrained representations to diverse tasks",
      "author" : [ "Matthew E Peters", "Sebastian Ruder", "Noah A Smith." ],
      "venue" : "RepL4NLP.",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "AdapterHub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "EMNLP: System Demonstrations.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextual parameter generation for universal neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Mrinmaya Sachan", "Graham Neubig", "Tom Mitchell." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Platanios et al\\.,? 2018",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2018
    }, {
      "title" : "Parameter space factorization for zero-shot learning across tasks and languages",
      "author" : [ "Edoardo M Ponti", "Ivan Vulić", "Ryan Cotterell", "Marinela Parovic", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "arXiv preprint arXiv:2001.11453.",
      "citeRegEx" : "Ponti et al\\.,? 2020",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "JMLR.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Snorkel metal: Weak supervision for multi-task learning",
      "author" : [ "Alex Ratner", "Braden Hancock", "Jared Dunnmon", "Roger Goldman", "Christopher Ré." ],
      "venue" : "DEEM workshop.",
      "citeRegEx" : "Ratner et al\\.,? 2018",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient parametrization of multidomain deep neural networks",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Hakan Bilen", "Andrea Vedaldi." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Rebuffi et al\\.,? 2018",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2018
    }, {
      "title" : "An Overview of Multi-Task Learning in Deep Neural Networks",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "arXiv preprint arXiv:1706.05098.",
      "citeRegEx" : "Ruder.,? 2017",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2017
    }, {
      "title" : "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning",
      "author" : [ "Asa Cooper Stickland", "Iain Murray." ],
      "venue" : "ICML.",
      "citeRegEx" : "Stickland and Murray.,? 2019",
      "shortCiteRegEx" : "Stickland and Murray.",
      "year" : 2019
    }, {
      "title" : "Hypergrid transformers: Towards a single model for multiple tasks",
      "author" : [ "Yi Tay", "Zhe Zhao", "Dara Bahri", "Don Metzler", "DaCheng Juan." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Udapter: Language adaptation for truly universal dependency parsing",
      "author" : [ "Ahmet Üstün", "Arianna Bisazza", "Gosse Bouma", "Gertjan van Noord." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Üstün et al\\.,? 2020",
      "shortCiteRegEx" : "Üstün et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring and predicting transferability across NLP tasks",
      "author" : [ "Tu Vu", "Tong Wang", "Tsendsuren Munkhdalai", "Alessandro Sordoni", "Adam Trischler", "Andrew Mattarella-Micke", "Subhransu Maji", "Mohit Iyyer." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Vu et al\\.,? 2020",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2020
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Characterizing and avoiding negative transfer",
      "author" : [ "Zirui Wang", "Zihang Dai", "Barnabás Póczos", "Jaime Carbonell." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Wang et al\\.,? 2019c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "TACL.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "Principal component analysis",
      "author" : [ "Svante Wold", "Kim Esbensen", "Paul Geladi." ],
      "venue" : "Chemometrics and intelligent laboratory systems.",
      "citeRegEx" : "Wold et al\\.,? 1987",
      "shortCiteRegEx" : "Wold et al\\.",
      "year" : 1987
    }, {
      "title" : "Transformers: Stateof-the-art natural language processing",
      "author" : [ "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "EMNLP: System Demonstrations.",
      "citeRegEx" : "Gugger et al\\.,? 2020a",
      "shortCiteRegEx" : "Gugger et al\\.",
      "year" : 2020
    }, {
      "title" : "Datasets",
      "author" : [ "McMillan-Major", "Simon Brandeis", "Sylvain Gugger", "François Lagunas", "Lysandre Debut", "Morgan Funtowicz", "Anthony Moi", "Sasha Rush", "Philipp Schmidd", "Pierric Cistac", "Victor Muštar", "Jeff Boudier", "Anna Tordjmann." ],
      "venue" : "GitHub. Note:",
      "citeRegEx" : "McMillan.Major et al\\.,? 2020b",
      "shortCiteRegEx" : "McMillan.Major et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal natural language processing with limited annotations: Try few-shot textual entailment as a start",
      "author" : [ "Wenpeng Yin", "Nazneen Fatema Rajani", "Dragomir Radev", "Richard Socher", "Caiming Xiong." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting few-sample bert fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann Lecun." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Transfer learning from pretrained large-scale language models yields state-of-the-art results in a variety of tasks (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b).",
      "startOffset" : 116,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "Transfer learning from pretrained large-scale language models yields state-of-the-art results in a variety of tasks (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b).",
      "startOffset" : 116,
      "endOffset" : 178
    }, {
      "referenceID" : 19,
      "context" : "Transfer learning from pretrained large-scale language models yields state-of-the-art results in a variety of tasks (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b).",
      "startOffset" : 116,
      "endOffset" : 178
    }, {
      "referenceID" : 33,
      "context" : "Multi-task learning with pretrained language models (Ruder, 2017) is appealing for multiple reasons: 1) Training individual models per task results in higher computational costs, which hinders deployment and maintenance.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "Specifically, when target datasets have limited training data, multi-task learning improves the performance compared to individually trained models (Liu et al., 2019a; Ratner et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "Specifically, when target datasets have limited training data, multi-task learning improves the performance compared to individually trained models (Liu et al., 2019a; Ratner et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 0,
      "context" : "However, multi-task fine-tuning can result in models underperforming on high-resource tasks due to constrained capacity (Arivazhagan et al., 2019; McCann et al., 2018).",
      "startOffset" : 120,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "However, multi-task fine-tuning can result in models underperforming on high-resource tasks due to constrained capacity (Arivazhagan et al., 2019; McCann et al., 2018).",
      "startOffset" : 120,
      "endOffset" : 167
    }, {
      "referenceID" : 41,
      "context" : "566 where achieving good performance on one task can hinder performance on another (Wang et al., 2019c).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "2018), adapter layers (Houlsby et al., 2019) insert a small number of additional parameters per task into the model.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "Such task-specific adapters eliminate negative task interference by encapsulating task-specific information (Pfeiffer et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "To address this problem and to enable sharing information across tasks while reaping the benefits of adapter layers, as depicted in Figure 1, we propose HYPERFORMER++, which employs a compact hypernetwork (Ha et al., 2017; Oswald et al., 2020) shared across tasks and layers.",
      "startOffset" : 205,
      "endOffset" : 243
    }, {
      "referenceID" : 22,
      "context" : "To address this problem and to enable sharing information across tasks while reaping the benefits of adapter layers, as depicted in Figure 1, we propose HYPERFORMER++, which employs a compact hypernetwork (Ha et al., 2017; Oswald et al., 2020) shared across tasks and layers.",
      "startOffset" : 205,
      "endOffset" : 243
    }, {
      "referenceID" : 30,
      "context" : "We use the encoder-decoder T5 model (Raffel et al., 2020) as the underlying model for our experiments and evaluate on the standard GLUE benchmark (Wang et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 40,
      "context" : ", 2020) as the underlying model for our experiments and evaluate on the standard GLUE benchmark (Wang et al., 2019b).",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "We achieve strong gains over both the T5BASE model as well as adapters (Houlsby et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "To our knowledge, this is the first time that adapters have been successfully integrated into a stateof-the-art encoder-decoder model beyond machine translation (Bapna and Firat, 2019), demonstrating that our method effectively balances sharing information across tasks while minimizing negative transfer.",
      "startOffset" : 161,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "to hypernetworks parameterized by ν that generate the task-specific adapter layers (Houlsby et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 37,
      "context" : "T5 consists of an encoder-decoder Transformer (Vaswani et al., 2017) with minor modifications (Raffel et al.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "Prior work has shown that fine-tuning all parameters of the model can result in a sub-optimal solution, particularly for resource-limited datasets (Peters et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1.",
      "startOffset" : 72,
      "endOffset" : 144
    }, {
      "referenceID" : 32,
      "context" : "As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1.",
      "startOffset" : 72,
      "endOffset" : 144
    }, {
      "referenceID" : 34,
      "context" : "As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1.",
      "startOffset" : 72,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "hypernetworks (Ha et al., 2017), which capture information across tasks that can be used to positively transfer to other relevant tasks.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "The conditional adapter layer Aτ for layer l consists of a down-projection,Dl τ ∈Rh×d, GeLU non-linearity (Hendrycks and Gimpel, 2016), and up-projection U l τ ∈Rd×h, where h is the input dimension, and d is the bottleneck dimension for the adapter layer, mathematically defined as:",
      "startOffset" : 106,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Conventional layer normalization (Ba et al., 2016) is defined as: LN τ(x i τ )=γ l τ xτ−μτ στ +β τ , (4)",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "A hypernetwork is a network that generates the weights of another network (Ha et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 38,
      "context" : "where zτ ∈Rt ′ can be a learnable parameter or any pretrained task features (Vu et al., 2020), and the task projector network hI(.",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 42,
      "context" : "For instance, when training on CoLA (Warstadt et al., 2019), cola sentence: is prepended to each sample.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "We ran some pilot experiments with pretrained task embeddings (Vu et al., 2020), but did not observe extra benefits.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 40,
      "context" : "(2020), we evaluate the performance of the models on the GLUE benchmark (Wang et al., 2019b).",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "In contrast to prior work (Houlsby et al., 2019), we do not learn a separate output layer for each task but instead share a frozen output layer for all the tasks, which makes our setting more parameter-efficient than prior work and is an advantage of multi-task learning with encoder-decoder models.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "Baselines: We compare to the strong adapter baseline (Houlsby et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "We did not experiment with more complex sampling strategies (Raffel et al., 2020) or tuning of T .",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Our variant of Adapters†, which shares layer norms across tasks, outperforms prior work (Houlsby et al., 2019), which does not share such information (80.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "53× fewer parameters respectively compared to a direct application of prior work (Houlsby et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "In particular, we consider 1) the natural language inference (NLI) datasets SciTail (Khot et al., 2018), and CB (De Marneffe et al.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 39,
      "context" : ", 2019) from SuperGLUE (Wang et al., 2019a) 2) the question answering (QA) dataset BoolQ (Clark",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : ", 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 48,
      "context" : ", 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : ", 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002).",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : ", 2019); 5) the question classification dataset TREC (Li and Roth, 2002).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI.",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 46,
      "context" : "Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI.",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI.",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "Adapters parameters: The standard setting (Houlsby et al., 2019) employs two adapters per layer for each task.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 43,
      "context" : "Figure 3 illustrates the 2D vector projections of task embeddings using PCA (Wold et al., 1987).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "It requires addressing multiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019).",
      "startOffset" : 212,
      "endOffset" : 238
    }, {
      "referenceID" : 11,
      "context" : "Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 28,
      "context" : "Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : "(2020) proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings.",
      "startOffset" : 119,
      "endOffset" : 143
    } ],
    "year" : 2021,
    "abstractText" : "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/ rabeehk/hyperformer.",
    "creator" : "LaTeX with hyperref"
  }
}