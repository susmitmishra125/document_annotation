{
  "name" : "2021.acl-long.232.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering",
    "authors" : [ "Reinald Adrian Pugoy", "Hung-Yu Kao" ],
    "emails" : [ "rdpugoy@up.edu.ph,", "hykao@mail.ncku.edu.tw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2981–2990\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2981\nWe pioneer the first extractive summarizationbased collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user/item explainability. Our experiments illustrate that ESCOFILT’s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types."
    }, {
      "heading" : "1 Introduction",
      "text" : "Collaborative filtering (CF) approaches are the most dominant and outstanding models in recommender systems literature. CF mainly focuses on learning accurate representations of users and items, denoting user preferences and item characteristics, respectively (Chen et al., 2018; Tay et al., 2018). The earliest CF models learned such representations based on user-given numeric ratings, but employing them is an oversimplification of user preferences and item characteristics (Koren et al., 2009; Musto et al., 2017). In this regard, review texts have been utilized to alleviate this issue.\nThe primary benefit of using reviews as the source of features is that they can cover the inherently multi-faceted nature of user opinions. Users can explain their rationales for the ratings they give to items. Thus, reviews contain a large quantity of rich latent information that cannot be otherwise acquired solely from ratings (Chen et al., 2018).\nStill, a typical limitation exists for most reviewbased recommender systems recently; the intrin-\nsic black-box nature of neural networks (NN) makes the explainability behind predictions obscure (Ribeiro et al., 2016; Wang et al., 2018b). The intricate architecture of hidden layers has opaqued the decision-making processes of neural models (Peake and Wang, 2018). Providing explanations is essential as they could help persuade users to develop further trust in a recommender system and make eventual purchasing decisions (Peake and Wang, 2018; Ribeiro et al., 2016; Zhang et al., 2014).\nIn light of this, current research efforts have attempted to improve the explainability aspect of recommender systems. Common types of explanations include review-level and word-level. In a review-level explanation, the attention mechanism is applied to measure every review’s contribution to the item (or user) embedding (Chen et al., 2018; Feng and Zeng, 2019). High-scoring reviews are then selected to serve as explanations. On the other hand, in a word-level or token-level explanation, informative words in a local window or textual block are selected together (Liu et al., 2019a; Pugoy and Kao, 2020; Seo et al., 2017). Similar to the first mechanism, top words are chosen due to their high attention weights.\nEvidently, review-level and word-level explanations are side-effects of applying the attention mechanism to reviews and words. These have been integral and beneficial in formulating better user and item representations. However, we contend that both types of explanations may not completely resemble real-life explanations. In logic, an explanation is a set of intelligible statements usually constructed to describe and clarify the causes, context, and consequences of objects, events, or phenomena under examination (Drake, 2018). Based on our example in Table 1, the review-level explanation is exactly the same as the second item review, assuming that it has the higher attention weight. Due to this, it also inadvertently disregards other possibly useful sentences from other reviews with lower attention scores. Furthermore, even though the word-level explanation contains informative words, it may not be practical in an actual recommendation scenario since it typically appears as fragments. Word-level explanations may not be intelligible enough due to humans’ natural bias toward sentences, which are defined to express complete thoughts (Andersen, 2014).\nTherefore, in this paper, we propose the first\nextractive summarization-based collaborative filtering model, ESCOFILT. For every item and user, our novel model generates extractive summaries that bear more resemblance to real-life explanations, as seen in Table 1’s last row. Unlike a review-level explanation, a summary-level explanation (which we also call extractive summary, representative summary, and representation-explanation in different sections of this paper) is composed of informative statements gathered from different reviews. As opposed to a word-level explanation, an ESCOFILT-produced explanation is more comprehensible as it can convey complete thoughts. It should be noted that our model performs extractive summarization in an unsupervised manner since expecting ground-truth summaries for all items and users in a large dataset is unrealistic. The strength of ESCOFILT lies in the fact that it uniquely unifies representation and explanation. In other words, an extractive summary both represents and explains a particular item (or user). We argue that our approach enhances both rating prediction accuracy and user/item explainability, which are later validated by our experiments and explainability study."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "These are the main contributions of our paper:\n• To the best of our knowledge, we pioneer the first extractive summarization-based CF framework.\n• Our proposed model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron (MLP) to respectively learn sentence embeddings, extractive representation-explanations, and user-item interactions.\n• To the extent of our knowledge, ESCOFILT is one of the first recommender models that employ BERT as a review feature extractor.\n• We also propose a comprehensive set of criteria that assesses the explainability of explanation texts in real life.\n• Our experiments illustrate that the rating prediction accuracy of ESCOFILT is better than the other state-of-the-art models. Moreover, our explainability study shows that summarylevel explanations are superior and more preferred than the other types of explanations."
    }, {
      "heading" : "2 Related Work",
      "text" : "Developing a CF model involves two crucial steps, i.e., learning user and item representations and modeling user-item interactions based on those representations (He et al., 2018). One of the foundational works in utilizing NN for CF is neural collaborative filtering or NCF (He et al., 2017). Originally implemented for implicit feedback datadriven CF, NCF learns non-linear interactions between users and items by employing MLP layers as its interaction function.\nDeepCoNN is the first deep learning-based model representing users and items from reviews in a coordinated manner (Zheng et al., 2017). The model consists of two parallel networks powered by convolutional neural networks (CNN). One network learns user behavior by examining all reviews he has written, and the other network models item properties by exploring all reviews it has received. A shared layer connects these two networks, and factorization machines capture user-item interactions. Another notable model is NARRE, which shares several similarities with DeepCoNN. NARRE is also composed of two parallel CNN-based networks for user and item modeling (Chen et al., 2018). For the first time, this model incorporates the review-level attention mechanism that determines each review’s usefulness or contribution based on attention weights. As a sideeffect, this also leads to review-level explanations; reviews with the highest attention scores are presented as explanations. These weights are then integrated into the representations of users and items to enhance embedding quality and prediction accuracy.\nOther related studies include D-Attn (Seo et al., 2017), MPCN (Tay et al., 2018) DAML (Liu et al., 2019a), and HUITA (Wu et al., 2019). These all employ different types of attention mechanisms to distinguish informative parts of a given data sample, resulting in simultaneous accuracy and explainability improvements. D-Attn integrates global and local attention to score each word to determine its relevance in a review text. MPCN is similar to NARRE, but the former relies solely on attention mechanisms without any need for convolutional layers. DAML utilizes CNN’s local and mutual attention to learn review features, and HUITA incorporates a hierarchical, three-tier attention network.\nMost of these aforementioned models take advantage of CNNs as automatic review feature ex-\ntractors. Coupling them with mainstream word embeddings leads to the formulation of user and item representations. However, such approaches fail to consider global context and word frequency information. The two said factors are crucial as they can affect recommendation performance (Pilehvar and Camacho-Collados, 2019; Wang et al., 2018a). To deal with such dilemmas, NCEM (Feng and Zeng, 2019) and BENEFICT (Pugoy and Kao, 2020) use a pre-trained BERT model to obtain review features. BERT’s advantage lies in its full retention of global context and word frequency information (Feng and Zeng, 2019). For explainability, NCEM similarly adopts NARRE’s review-level attention. On the contrary, BENEFICT utilizes BERT’s self-attention weights in conjunction with a solution to the maximum subarray problem (MSP). BENEFICT’s approach produces an explanation based on a subarray of contiguous tokens with the largest possible sum of self-attention weights.\nIn summary, there appears to be a trend; tackling explainability improves prediction and recommendation performance consequentially. While most recommender models address this via attention mechanisms, our proposed model solves this by unifying representation and explanation in the form of extractive summaries. As evidenced in the succeeding sections of this paper, we argue that our approach can further enhance CF’s accuracy and explainability."
    }, {
      "heading" : "3 Methodology",
      "text" : "ESCOFILT, whose architecture is illustrated in Figure 1, has two parallel components that learn summarization-based user and item representations. From Sections 3.2 to 3.3, we will only discuss the item modeling process as it is nearly identical to user modeling, with their inputs as the only difference."
    }, {
      "heading" : "3.1 Definition and Notation",
      "text" : "The training dataset τ consists of N tuples, with the latter denoting the size of the dataset. Each tuple follows this form: (u, i, rui, vui) where rui and vui respectively refer to the ground-truth rating and review accorded by user u to item i. Moreover, let Vu = {vu1, vu2, ..., vuj} be the set of all j reviews written by user u. Similarly, let Vi = {v1i, v2i, ..., vki} be the set of all k reviews received by item i. Both Vu and Vi are obtained from scanning τ itself.\nThe input of ESCOFILT is a user-item pair (u, i) from each tuple in τ . We particularly feed Vu and Vi to the model as they initially represent u and i. The output is the predicted rating r̂ui ∈ R that user u may give to item i. Thus, the rating prediction task R can be expressed as:\nR(u, i) = (Vu, Vi)→ r̂ui (1)\nIts corresponding objective function, the mean squared error (MSE), is given below:\nMSE = 1 |τ | ∑ u,i∈τ (rui − r̂ui)2 (2)"
    }, {
      "heading" : "3.2 Sentence Extraction and BERT Encoding",
      "text" : "First, the reviews in Vi are concatenated together to form a single document. A sentence segmentation component called Sentencizer (by spaCy) is utilized to split this document into individual sentences (Gupta and Nishu, 2020). The set of all sentences in Vi is now given by Si = {si1, si2, ..., sig} where g refers to the total number of sentences.\nAfterward, Si is fed to a pre-trained BERTLARGE model. It should be noted that we opt not to use [CLS] representations as these may not necessarily provide the best sentence embeddings (Miller, 2019). In this regard, we tap BERT’s penultimate encoder layer to obtain the contextualized word embeddings. The word embeddings of each sentence\nin Si are stored in S̄i ∈ Rg×w×1024; w pertains to the amount of words in a sentence, and 1024 is the embedding size of BERT. Then, we average every sentence’s word embeddings in S̄i to produce the set of sentence embeddings S′i = {s′i1, s′i2, ..., s′ig}, with S′i ∈ Rg×1024."
    }, {
      "heading" : "3.3 Embedding Clustering",
      "text" : "K-Means clustering is next performed to partition the sentence embeddings in S′i into K clusters. Its objective is to minimize the intra-cluster sum of the distances from each sentence to its nearest centroid, given by the following equation (Xia et al., 2020):\nJi = K∑ x=1 ∑ s′iy∈Cx ||s′iy − cx||2 (3)\nwhere cx is the centroid of cluster Cx that is closest to the sentence embedding s′iy. The objective function Ji is optimized for item i by running the assignment and update steps until the cluster centroids stabilize. The assignment step assigns each sentence to a cluster based on the shortest sentence embedding-cluster centroid distance, provided by the formula below:\nd(s′iy) = argminx=1,...,K{||s′iy − cx||2} (4)\nwhere d is a function that obtains the cluster closest to s′iy. Furthermore, the update step recomputes the cluster centroids based on new assignments from the previous step. This is defined as:\ncx = 1\n|Cx| g∑ y=1 {s′iy|d(s′iy) = x} (5)\nwhere |Cx| refers to the number of sentences that cluster Cx contains. By introducing clustering, redundant and related sentences are grouped in the same cluster. Concerning this, K is derived using this equation:\nK = φi × g (6)\nwhere φi pertains to the item summary ratio, i.e., the percentage of sentences that comprise an item’s extractive summary. This subsequently implies that K denotes the actual number of sentences in the summary. Sentences closest to each cluster centroid are selected and combined to form the item’s representation-explanation. This is mathematically\nexpressed as:\ne(Cx) = argminy=1,...,g{||s′iy − cx||2}\nItemRXi = 1\nK K∑ x=1 s′i,e(Cx) (7)\nwhere e is a function that returns the nearest sentence to the centroid cx of cluster Cx, and ItemRXi ∈ R1×1024 is the representationexplanation embedding of item i."
    }, {
      "heading" : "3.4 Fusion Layers",
      "text" : "Inspired by NARRE (Chen et al., 2018), we also draw some principles from the traditional latent factor model by incorporating rating-based hidden vectors that depict users and items to a certain extent. These are represented by UserIV and ItemIV , both in R1×m where m is the dimension of the latent vectors. Such vectors are fused with their respective representation-explanation embeddings. This is facilitated by these fusion levels, illustrated by the following formulas:\nfu = (UserRXu ×Wu + bu) + UserIVu fi = (ItemRXi ×Wi + bi) + ItemIVi\nfui = [fu, fi]\n(8)\nwhere fu and fi pertain to the preliminary fusion layers and both are in R1×m; Wu and Wi are weight matrices in R1024×m; bu and bi refer to bias vectors; and fui ∈ R1×2m denotes the initial user-item interactions from the third fusion layer and is later fed to the MLP."
    }, {
      "heading" : "3.5 Multilayer Perceptron and Rating Prediction",
      "text" : "The MLP is necessary to model the CF effect, i.e., to learn meaningful non-linear interactions between users and items. An MLP with multiple hidden layers typically implies a higher degree of nonlinearity and flexibility. Similar to the strategy of He et al. (2017), ESCOFILT adopts an MLP with a tower pattern; the bottom layer is the widest while every succeeding top layer has fewer neurons. A tower structure enables the MLP to learn more abstractive data features. Specifically, we halve the size of hidden units for each successive higher layer. ESCOFILT’s MLP component is defined as follows:\nh1 = ReLU(fui ×W1 + b1) hL = ReLU(hL−1 ×WL + bL)\n(9)\nwhere hL represents the L-th MLP layer, and WL and bL pertain to the L-th layer’s weight matrix and bias vector, respectively. As far as the MLP’s activation function is concerned, we select the rectified linear unit (ReLU), which yields better performance than other activation functions (He et al., 2017). Finally, the MLP’s output is fed to one more linear layer to produce the predicted rating:\nr̂ui = hL ×WL+1 + bL+1 (10)"
    }, {
      "heading" : "4 Empirical Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Research Questions",
      "text" : "In this section, we detail our experimental setup designed to answer the following research questions (RQs):\n• RQ1: Does ESCOFILT outperform the other state-of-the-art recommender baselines? • RQ2: Is embedding clustering effective? • RQ3: Can our model produce explanations\nacceptable to humans in real life?"
    }, {
      "heading" : "4.2 Datasets, Baselines, and Evaluation Metric",
      "text" : "Table 2 summarizes the four public datasets1 that we utilized in our study. These datasets are Amazon 5-core, wherein users and items are guaranteed to have at least five reviews each (McAuley et al., 2015; He and McAuley, 2016). The ratings across all datasets are in the range of [1, 5]. We split each dataset into training (80%), validation (10%), and test (10%) sets. Next, to validate the effectiveness of ESCOFILT, we compared its prediction performance against four state-of-the-art baselines:\n• BENEFICT (Pugoy and Kao, 2020): This recent recommender model uniquely integrates BERT, MSP, and MLP to learn representations, explanations, and interactions.\n1http://jmcauley.ucsd.edu/data/amazon/\n• DeepCoNN (Zheng et al., 2017): This is the first deep collaborative neural network model that is based on two parallel CNNs to jointly learn user and item features. • MPCN (Tay et al., 2018): Akin to NARRE, this CNN-less model employs a new type of dual attention for identifying relevant reviews. • NARRE (Chen et al., 2018): Similar to DeepCoNN, it is a neural attentional regression model that integrates two parallel CNNs and the review-level attention mechanism.\nAll these recommender models employed the same dataset split. We then computed the root mean square error (RMSE) on the test dataset (τ̄ ), as indicated by the formula below. RMSE is a widely used metric for evaluating a model’s rating prediction accuracy (Steck, 2013).\nRMSE =\n√ 1\n|τ̄ | ∑ u,i∈τ̄ (rui − r̂ui)2 (11)"
    }, {
      "heading" : "4.3 Experimental Settings",
      "text" : "For ESCOFILT, we mainly based its summarization component on BERT Extractive Summarizer2 by Miller (2019). We also utilized the pre-trained BERTLARGE model afforded by the Transformers library of HuggingFace3. In our implementation4, the following hyperparameters were fixed:\n• Learning rate: 0.006 • Quantity of MLP layers: 4 • Item summary ratio (φi): 0.4 • User summary ratio (φu): 0.4\nOn the other hand, we operated an exhaustive grid search over these hyperparameters:\n• Number of epochs: [1, 30] • Latent vector dimension (m): {32, 128, 220}\nDue to its architectural similarity to ESCOFILT, we reimplemented BENEFICT by augmenting it with the pre-trained BERTLARGE model and adopting our model’s fusion and latent vector dimension strategies. For DeepCoNN, MPCN, and NARRE, we employed the extensible NRRec framework5 and retained the other hyperparameters reported in the framework (Liu et al., 2019b).\n2https://github.com/dmmiller612/bert-extractivesummarizer\n3https://github.com/huggingface/transformers 4https://github.com/reinaldncku/ESCOFILT 5https://github.com/ShomyLiu/Neu-Review-Rec\nFor the four baselines, we also performed an exhaustive grid search over the following:\n• Number of epochs: [1, 30] • Learning rates: {0.003, 0.004, 0.006}\nAll models, including ESCOFILT, used the same optimizer, Adam, which leverages the power of adaptive learning rates during training (Kingma and Ba, 2014). This makes the selection of a learning rate less cumbersome, leading to faster convergence (Chen et al., 2018). Without special mention, the models shared the same random seed, batch size (128), and dropout rate (0.5). We selected the model configuration with the lowest RMSE on the validation set. We ran our experiments on NVIDIA GeForce RTX 2080 Ti."
    }, {
      "heading" : "4.4 Prediction Results and Discussion",
      "text" : ""
    }, {
      "heading" : "4.4.1 Performance Comparison",
      "text" : "The overall performances of our model and the other baselines are summarized in Table 3. It is essential to remark that although utilizing information derived from reviews is beneficial, a model’s performance can vary contingent on how the said information is considered. These are our general findings:\nFirst, our proposed model consistently outperforms all baselines across all datasets. This ascertains the effectiveness of ESCOFILT and clearly answers RQ1. Moreover, this validates our case that coupling BERT (a superior review feature extractor) with embedding clustering enables user and item representations to have finer granularity and fewer redundancies.\nSecond, receiving the two lowest average RMSE values, BERT-based models (ESCOFILT and BENEFICT) have generally better prediction accuracies than the rest of the mostly CNN-powered baselines. This particular observation verifies the necessity of integrating BERT in a CF architecture. Unlike its mainstream counterparts, BERT produces more semantically meaningful embeddings that keep essential elements such as global context and word frequency information."
    }, {
      "heading" : "4.4.2 Efficacy of Embedding Clustering",
      "text" : "This section further discusses the efficacy of KMeans embedding clustering, instrumental in producing user and item representative summaries. Concerning this, we prepared three variants of our model. First is ESCOFILT-N, which does not utilize any embedding clustering. Instead, it relies on\ntraditional embeddings that are neither pre-trained nor review-based. They are randomly initialized yet optimized during training. Another variant is ESCOFILT-I, wherein only item reviews undergo embedding clustering while the user component is based on traditional embeddings. ESCOFILTU also operates the same way; the difference is that only user reviews are processed by embedding clustering.\nBased on Figure 2, having the lowest validation RMSE values, the default ESCOFILT configuration is the best across the datasets, while the worst variant is ESCOFILT-N. This gives credence to embedding clustering’s effectiveness and addresses RQ2; it can simultaneously capture user preferences and item characteristics, resulting in precise representations and accurate rating prediction.\nThere appears to be a trend as well: the secondbest and the third-best variants are ESCOFILT-I and ESCOFILT-U, respectively. In some instances, ESCOFILT-I seems to be on par with the default ESCOFILT variant. This implies that items stand to benefit more than users from embedding clustering. One possible explanation is that each item normally receives a far greater quantity of reviews than each user actually writes, translating to more possibly extractable information and features. Hence, item reviews have a more significant influence than user\nreviews in determining ratings. Still, this does not immediately suggest that user embedding clustering is not helpful. It needs to be integrated first with item embedding clustering via the MLP to discover relevant user-item interactions, leading to our original model’s performance."
    }, {
      "heading" : "5 Explainability Study",
      "text" : ""
    }, {
      "heading" : "5.1 Real-Life Explainability Criteria",
      "text" : "The assessment of explanations in existing recommender systems literature is generally limited to specific case studies. Most of these relied on simple qualitative analysis of attention weights and highscoring reviews on selected samples (Liu et al., 2019a; Seo et al., 2017; Wu et al., 2019). The assessment criterion provided in the NARRE and BENEFICT papers went a little further by asking human raters to score each explanation’s helpfulness or usefulness on a given Likert scale (Chen et al., 2018; Pugoy and Kao, 2020). Nevertheless, to the best of our knowledge, there does not appear to be a comprehensive set of criteria that assesses the real-life explainability of explanations. We contend that it is increasingly necessary to measure how people actually perceive explanation texts generated by recommender models; after all, these texts aim to explain entities in real life. Hence, we\npropose the following explainability criteria, which are inspired by Zemla et al. (2017):\n1. Coherence: “Parts of the explanation fit together coherently.”\n2. Completeness: “There are no gaps in the explanation.”\n3. Lack of Alternatives: “There are probably less to no reasonable alternative explanations.”\n4. Novelty: “I learned something new from the explanation.”\n5. Perceived Truth: “I believe this explanation to be true.”\n6. Quality: “This is a good explanation.” 7. Visualization: “It is easy to visualize what\nthe explanation is saying.”"
    }, {
      "heading" : "5.2 Human Assessment of Explanations",
      "text" : "We generated a total of 90 item explanations, 30 each from BENEFICT (token-level), NARRE (review-level), and ESCOFILT (summary-level). For pointwise evaluation, we asked two human judges to assess the explanations based on our proposed real-life explainability criteria on a five-point Likert scale. For listwise evaluation, we instructed them to rank the three explanation types for every text according to helpfulness. We further examined these results by determining the strength of agreement between the two judges, using Cohen’s Kappa coefficient (κ) wherein -1 indicates a less\nthan chance agreement, 0 refers to a random agreement, and 1 denotes a perfect agreement (Borromeo and Toyama, 2015; Landis and Koch, 1977)."
    }, {
      "heading" : "5.3 Explainability Results and Discussion",
      "text" : "Table 4 summarizes the results of the human judges’ pointwise evaluation. For five out of seven criteria, ESCOFILT-derived explanations have the highest explainability scores. Specifically, summarylevel explanations are most coherent, most complete, most novel, and most truthful. ESCOFILT’s strongest aspect is its perceived truth, obtaining a mean rating of 3.92 and κ = 0.28 that indicates a fair inter-judge agreement.\nInterestingly, both ESCOFILT and NARRE have the best quality, with the same mean rating of 3.72. The Kappa coefficient is 0.11, implying that the judges agree with each other to a certain extent. Considering that a review-level explanation is simply the highest weighted review, our modelgenerated explanations are assessed on par with the former. Furthermore, review-level explanations have the highest explainability scores in two other criteria, i.e., lack of alternatives and visualization. NARRE’s strongest aspect is that its explanations are easiest to visualize, having a mean rating of 3.92 and κ = 0.27 that denotes a fair inter-judge agreement.\nLastly, Figure 3 shows the results of the human judges’ listwise evaluation. Our model produces the most helpful explanations; such explanations\nare ranked first for almost 83% of the items. These are followed far behind by NARRE’s explanations, ranked first for nearly 17% of the items. None of BENEFICT’s explanations are ranked first. With κ = 0.45 for ranking consistency, there is a moderate agreement between the judges.\nIn summary, these results clearly illustrate the superiority of summary-level explanations in real life that can present necessary guidance to users in making future purchasing decisions, thereby satisfying RQ3."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this study, unifying representations and explanations, in the form of extractive summaries, have further enhanced collaborative filtering accuracy and explainability. We have successfully developed a model that uniquely integrates BERT, embedding clustering, and MLP. Our experiments on various datasets verify ESCOFILT’s predictive capability, and the human judges’ assessments validate its explainability in real life. In the future, we shall consider expanding our model’s explainability capability by possibly incorporating other NLP principles such as abstractive summarization and natural language generation."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was funded in part by Qualcomm through a Taiwan University Research Collaboration Project and also in part by the Ministry of Science and Technology, Taiwan, under NCKU B109K027D and MOST 109-2221-E-006-173 grants, respectively."
    } ],
    "references" : [ {
      "title" : "Sentence types and functions",
      "author" : [ "Sarah Andersen." ],
      "venue" : "San Jose State University Writing Center.",
      "citeRegEx" : "Andersen.,? 2014",
      "shortCiteRegEx" : "Andersen.",
      "year" : 2014
    }, {
      "title" : "Automatic vs",
      "author" : [ "Ria Mae Borromeo", "Motomichi Toyama." ],
      "venue" : "crowdsourced sentiment analysis. In Proceedings of the 19th International Database Engineering & Applications Symposium, pages 90–95.",
      "citeRegEx" : "Borromeo and Toyama.,? 2015",
      "shortCiteRegEx" : "Borromeo and Toyama.",
      "year" : 2015
    }, {
      "title" : "Neural attentional rating regression with review-level explanations",
      "author" : [ "Chong Chen", "Min Zhang", "Yiqun Liu", "Shaoping Ma." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference, pages 1583– 1592.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Introduction to Logic",
      "author" : [ "Jess Drake." ],
      "venue" : "ED-Tech Press.",
      "citeRegEx" : "Drake.,? 2018",
      "shortCiteRegEx" : "Drake.",
      "year" : 2018
    }, {
      "title" : "Neural collaborative embedding from reviews for recommendation",
      "author" : [ "Xingjie Feng", "Yunze Zeng." ],
      "venue" : "IEEE Access, 7:103263–103274.",
      "citeRegEx" : "Feng and Zeng.,? 2019",
      "shortCiteRegEx" : "Feng and Zeng.",
      "year" : 2019
    }, {
      "title" : "Mapping local news coverage: Precise location extraction in textual news content using fine-tuned bert based language model",
      "author" : [ "Sarang Gupta", "Kumari Nishu." ],
      "venue" : "Proceedings of the 4th Workshop on Natural Language Processing and Computational Social",
      "citeRegEx" : "Gupta and Nishu.,? 2020",
      "shortCiteRegEx" : "Gupta and Nishu.",
      "year" : 2020
    }, {
      "title" : "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "author" : [ "Ruining He", "Julian McAuley." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, pages 507–517.",
      "citeRegEx" : "He and McAuley.,? 2016",
      "shortCiteRegEx" : "He and McAuley.",
      "year" : 2016
    }, {
      "title" : "Outer productbased neural collaborative filtering",
      "author" : [ "Xiangnan He", "Xiaoyu Du", "Xiang Wang", "Feng Tian", "Jinhui Tang", "Tat-Seng Chua." ],
      "venue" : "arXiv preprint arXiv:1808.03912.",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural collaborative filtering",
      "author" : [ "Xiangnan He", "Lizi Liao", "Hanwang Zhang", "Liqiang Nie", "Xia Hu", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, pages 173– 182.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Yehuda Koren", "Robert Bell", "Chris Volinsky." ],
      "venue" : "Computer, 42(8):30–37.",
      "citeRegEx" : "Koren et al\\.,? 2009",
      "shortCiteRegEx" : "Koren et al\\.",
      "year" : 2009
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "Biometrics, pages 159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "DAML: Dual attention mutual learning between ratings and reviews for item recommendation",
      "author" : [ "Donghua Liu", "Jing Li", "Bo Du", "Jun Chang", "Rong Gao." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "NRPA: Neural recommendation with personalized attention",
      "author" : [ "Hongtao Liu", "Fangzhao Wu", "Wenjun Wang", "Xianchen Wang", "Pengfei Jiao", "Chuhan Wu", "Xing Xie." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Image-based recommendations on styles and substitutes",
      "author" : [ "Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information",
      "citeRegEx" : "McAuley et al\\.,? 2015",
      "shortCiteRegEx" : "McAuley et al\\.",
      "year" : 2015
    }, {
      "title" : "Leveraging BERT for extractive text summarization on lectures",
      "author" : [ "Derek Miller." ],
      "venue" : "arXiv preprint arXiv:1906.04165.",
      "citeRegEx" : "Miller.,? 2019",
      "shortCiteRegEx" : "Miller.",
      "year" : 2019
    }, {
      "title" : "A multi-criteria recommender system exploiting aspect-based sentiment analysis of users’ reviews",
      "author" : [ "Cataldo Musto", "Marco de Gemmis", "Giovanni Semeraro", "Pasquale Lops." ],
      "venue" : "Proceedings of the 11th ACM Conference on Recommender Systems,",
      "citeRegEx" : "Musto et al\\.,? 2017",
      "shortCiteRegEx" : "Musto et al\\.",
      "year" : 2017
    }, {
      "title" : "Explanation mining: Post hoc interpretability of latent factor models for recommendation systems",
      "author" : [ "Georgina Peake", "Jun Wang." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2060–",
      "citeRegEx" : "Peake and Wang.,? 2018",
      "shortCiteRegEx" : "Peake and Wang.",
      "year" : 2018
    }, {
      "title" : "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
      "author" : [ "Mohammad Taher Pilehvar", "Jose CamachoCollados." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Pilehvar and CamachoCollados.,? 2019",
      "shortCiteRegEx" : "Pilehvar and CamachoCollados.",
      "year" : 2019
    }, {
      "title" : "BERTbased neural collaborative filtering and fixed-length contiguous tokens explanation",
      "author" : [ "Reinald Adrian Pugoy", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the",
      "citeRegEx" : "Pugoy and Kao.,? 2020",
      "shortCiteRegEx" : "Pugoy and Kao.",
      "year" : 2020
    }, {
      "title" : "Why should I trust you?: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Interpretable convolutional neural networks with dual local and global attention for review rating prediction",
      "author" : [ "Sungyong Seo", "Jing Huang", "Hao Yang", "Yan Liu." ],
      "venue" : "Proceedings of the 11th ACM Conference on Recommender Systems, pages 297–305.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluation of recommendations: rating-prediction and ranking",
      "author" : [ "Harald Steck." ],
      "venue" : "Proceedings of the 7th ACM Conference on Recommender systems, pages 213–220.",
      "citeRegEx" : "Steck.,? 2013",
      "shortCiteRegEx" : "Steck.",
      "year" : 2013
    }, {
      "title" : "Multi-pointer co-attention networks for recommendation",
      "author" : [ "Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2309–2318.",
      "citeRegEx" : "Tay et al\\.,? 2018",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2018
    }, {
      "title" : "Worddriven and context-aware review modeling for recommendation",
      "author" : [ "Qianqian Wang", "Si Li", "Guang Chen." ],
      "venue" : "Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 1859–1862.",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "TEM: Tree-enhanced embedding model for explainable recommendation",
      "author" : [ "Xiang Wang", "Xiangnan He", "Fuli Feng", "Liqiang Nie", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference, pages 1543–1552.",
      "citeRegEx" : "Wang et al\\.,? 2018b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical user and item representation with three-tier attention for recommendation",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Junxin Liu", "Yongfeng Huang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "A fast adaptive k-means with no bounds",
      "author" : [ "Shuyin Xia", "Daowan Peng", "Deyu Meng", "Changqing Zhang", "Guoyin Wang", "Elisabeth Giem", "Wei Wei", "Zizhong Chen." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating everyday explanations",
      "author" : [ "Jeffrey C Zemla", "Steven Sloman", "Christos Bechlivanidis", "David A Lagnado." ],
      "venue" : "Psychonomic Bulletin & Review, 24(5):1488–1500.",
      "citeRegEx" : "Zemla et al\\.,? 2017",
      "shortCiteRegEx" : "Zemla et al\\.",
      "year" : 2017
    }, {
      "title" : "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis",
      "author" : [ "Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma." ],
      "venue" : "Proceedings of the 37th International ACM SIGIR Conference on",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint deep modeling of users and items using reviews for recommendation",
      "author" : [ "Lei Zheng", "Vahid Noroozi", "Philip S Yu." ],
      "venue" : "Proceedings of the 10th ACM International Conference on Web Search and Data Mining, pages 425–434.",
      "citeRegEx" : "Zheng et al\\.,? 2017",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "CF mainly focuses on learning accurate representations of users and items, denoting user preferences and item characteristics, respectively (Chen et al., 2018; Tay et al., 2018).",
      "startOffset" : 140,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "CF mainly focuses on learning accurate representations of users and items, denoting user preferences and item characteristics, respectively (Chen et al., 2018; Tay et al., 2018).",
      "startOffset" : 140,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "The earliest CF models learned such representations based on user-given numeric ratings, but employing them is an oversimplification of user preferences and item characteristics (Koren et al., 2009; Musto et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "The earliest CF models learned such representations based on user-given numeric ratings, but employing them is an oversimplification of user preferences and item characteristics (Koren et al., 2009; Musto et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 218
    }, {
      "referenceID" : 2,
      "context" : "Thus, reviews contain a large quantity of rich latent information that cannot be otherwise acquired solely from ratings (Chen et al., 2018).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "2982 sic black-box nature of neural networks (NN) makes the explainability behind predictions obscure (Ribeiro et al., 2016; Wang et al., 2018b).",
      "startOffset" : 102,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "2982 sic black-box nature of neural networks (NN) makes the explainability behind predictions obscure (Ribeiro et al., 2016; Wang et al., 2018b).",
      "startOffset" : 102,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "The intricate architecture of hidden layers has opaqued the decision-making processes of neural models (Peake and Wang, 2018).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Providing explanations is essential as they could help persuade users to develop further trust in a recommender system and make eventual purchasing decisions (Peake and Wang, 2018; Ribeiro et al., 2016; Zhang et al., 2014).",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 20,
      "context" : "Providing explanations is essential as they could help persuade users to develop further trust in a recommender system and make eventual purchasing decisions (Peake and Wang, 2018; Ribeiro et al., 2016; Zhang et al., 2014).",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 29,
      "context" : "Providing explanations is essential as they could help persuade users to develop further trust in a recommender system and make eventual purchasing decisions (Peake and Wang, 2018; Ribeiro et al., 2016; Zhang et al., 2014).",
      "startOffset" : 158,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "In a review-level explanation, the attention mechanism is applied to measure every review’s contribution to the item (or user) embedding (Chen et al., 2018; Feng and Zeng, 2019).",
      "startOffset" : 137,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : "In a review-level explanation, the attention mechanism is applied to measure every review’s contribution to the item (or user) embedding (Chen et al., 2018; Feng and Zeng, 2019).",
      "startOffset" : 137,
      "endOffset" : 177
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, in a word-level or token-level explanation, informative words in a local window or textual block are selected together (Liu et al., 2019a; Pugoy and Kao, 2020; Seo et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, in a word-level or token-level explanation, informative words in a local window or textual block are selected together (Liu et al., 2019a; Pugoy and Kao, 2020; Seo et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, in a word-level or token-level explanation, informative words in a local window or textual block are selected together (Liu et al., 2019a; Pugoy and Kao, 2020; Seo et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "nation is a set of intelligible statements usually constructed to describe and clarify the causes, context, and consequences of objects, events, or phenomena under examination (Drake, 2018).",
      "startOffset" : 176,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "Word-level explanations may not be intelligible enough due to humans’ natural bias toward sentences, which are defined to express complete thoughts (Andersen, 2014).",
      "startOffset" : 148,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : ", learning user and item representations and modeling user-item interactions based on those representations (He et al., 2018).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "One of the foundational works in utilizing NN for CF is neural collaborative filtering or NCF (He et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "DeepCoNN is the first deep learning-based model representing users and items from reviews in a coordinated manner (Zheng et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "NARRE is also composed of two parallel CNN-based networks for user and item modeling (Chen et al., 2018).",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "Other related studies include D-Attn (Seo et al., 2017), MPCN (Tay et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : ", 2018) DAML (Liu et al., 2019a), and HUITA (Wu et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 24,
      "context" : "The two said factors are crucial as they can affect recommendation performance (Pilehvar and Camacho-Collados, 2019; Wang et al., 2018a).",
      "startOffset" : 79,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "To deal with such dilemmas, NCEM (Feng and Zeng, 2019) and BENEFICT (Pugoy and Kao, 2020) use a pre-trained BERT model to obtain review features.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "To deal with such dilemmas, NCEM (Feng and Zeng, 2019) and BENEFICT (Pugoy and Kao, 2020) use a pre-trained BERT model to obtain review features.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "BERT’s advantage lies in its full retention of global context and word frequency information (Feng and Zeng, 2019).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "A sentence segmentation component called Sentencizer (by spaCy) is utilized to split this document into individual sentences (Gupta and Nishu, 2020).",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "It should be noted that we opt not to use [CLS] representations as these may not necessarily provide the best sentence embeddings (Miller, 2019).",
      "startOffset" : 130,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : "Its objective is to minimize the intra-cluster sum of the distances from each sentence to its nearest centroid, given by the following equation (Xia et al., 2020):",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "Inspired by NARRE (Chen et al., 2018), we also draw some principles from the traditional latent factor model by incorporating rating-based hidden vectors that depict users and items to a certain extent.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "As far as the MLP’s activation function is concerned, we select the rectified linear unit (ReLU), which yields better performance than other activation functions (He et al., 2017).",
      "startOffset" : 162,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "These datasets are Amazon 5-core, wherein users and items are guaranteed to have at least five reviews each (McAuley et al., 2015; He and McAuley, 2016).",
      "startOffset" : 108,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "These datasets are Amazon 5-core, wherein users and items are guaranteed to have at least five reviews each (McAuley et al., 2015; He and McAuley, 2016).",
      "startOffset" : 108,
      "endOffset" : 152
    }, {
      "referenceID" : 19,
      "context" : "• BENEFICT (Pugoy and Kao, 2020): This recent recommender model uniquely integrates BERT, MSP, and MLP to learn representations, explanations, and interactions.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 30,
      "context" : "2986 • DeepCoNN (Zheng et al., 2017): This is the first deep collaborative neural network model that is based on two parallel CNNs to jointly learn user and item features.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "• MPCN (Tay et al., 2018): Akin to NARRE, this CNN-less model employs a new type of dual attention for identifying relevant reviews.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "• NARRE (Chen et al., 2018): Similar to DeepCoNN, it is a neural attentional regression model that integrates two parallel CNNs and",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "RMSE is a widely used metric for evaluating a model’s rating prediction accuracy (Steck, 2013).",
      "startOffset" : 81,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "For DeepCoNN, MPCN, and NARRE, we employed the extensible NRRec framework5 and retained the other hyperparameters reported in the framework (Liu et al., 2019b).",
      "startOffset" : 140,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "All models, including ESCOFILT, used the same optimizer, Adam, which leverages the power of adaptive learning rates during training (Kingma and Ba, 2014).",
      "startOffset" : 132,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "This makes the selection of a learning rate less cumbersome, leading to faster convergence (Chen et al., 2018).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "Most of these relied on simple qualitative analysis of attention weights and highscoring reviews on selected samples (Liu et al., 2019a; Seo et al., 2017; Wu et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 21,
      "context" : "Most of these relied on simple qualitative analysis of attention weights and highscoring reviews on selected samples (Liu et al., 2019a; Seo et al., 2017; Wu et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "Most of these relied on simple qualitative analysis of attention weights and highscoring reviews on selected samples (Liu et al., 2019a; Seo et al., 2017; Wu et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "The assessment criterion provided in the NARRE and BENEFICT papers went a little further by asking human raters to score each explanation’s helpfulness or usefulness on a given Likert scale (Chen et al., 2018; Pugoy and Kao, 2020).",
      "startOffset" : 190,
      "endOffset" : 230
    }, {
      "referenceID" : 19,
      "context" : "The assessment criterion provided in the NARRE and BENEFICT papers went a little further by asking human raters to score each explanation’s helpfulness or usefulness on a given Likert scale (Chen et al., 2018; Pugoy and Kao, 2020).",
      "startOffset" : 190,
      "endOffset" : 230
    }, {
      "referenceID" : 1,
      "context" : "We further examined these results by determining the strength of agreement between the two judges, using Cohen’s Kappa coefficient (κ) wherein -1 indicates a less than chance agreement, 0 refers to a random agreement, and 1 denotes a perfect agreement (Borromeo and Toyama, 2015; Landis and Koch, 1977).",
      "startOffset" : 252,
      "endOffset" : 302
    }, {
      "referenceID" : 11,
      "context" : "We further examined these results by determining the strength of agreement between the two judges, using Cohen’s Kappa coefficient (κ) wherein -1 indicates a less than chance agreement, 0 refers to a random agreement, and 1 denotes a perfect agreement (Borromeo and Toyama, 2015; Landis and Koch, 1977).",
      "startOffset" : 252,
      "endOffset" : 302
    } ],
    "year" : 2021,
    "abstractText" : "We pioneer the first extractive summarizationbased collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user/item explainability. Our experiments illustrate that ESCOFILT’s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types.",
    "creator" : "LaTeX with hyperref"
  }
}