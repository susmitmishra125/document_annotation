{
  "name" : "2021.acl-long.455.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Math Word Problem Solving with Explicit Numerical Values",
    "authors" : [ "Qinzhuo Wu", "Qi Zhang", "Zhongyu Wei", "Xuanjing Huang" ],
    "emails" : [ "(qzwu17@fudan.edu.cn", "qz@fudan.edu.cn", "zywei@fudan.edu.cn", "xjhuang)@fudan.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5859–5869\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5859"
    }, {
      "heading" : "1 Introduction",
      "text" : "Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions\n∗ Corresponding author. 1Code is available at https://github.com/\nqinzhuowu/NumS2T/\nand have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node.\nAlthough promising results have been achieved, previous methods rarely take numerical values into consideration, despite the fact that in math word problem solving, numerical values provide vital information. As an infinite number of numerals can appear in math word problems, it is impossible to list them all in the vocabulary. Previous methods replace all the numbers in the problems with number symbols (e.g., v1, v2) in order in the preprocessing stage. These replaced problems are used as input\nto directly generate expressions containing number symbols. The number symbols in the expressions are then replaced with the numerical values in the original problems to obtain executable expressions. As shown in Figure 1, taking the problem with numerical values {v2=15, v3=10, v4=100, v5=25} as input, the target expression of the problem would be “v4/(v2 − v3) + v5”. However, if the number symbol v5 = 20%, the target expression for the same problem would be “v4/(v2 − v3) ∗ (1 + v5)”. Similarly, without numerical value information, the model can hardly determine whether the number gap between the table and the chair should be v2 − v3 or v3 − v2. As such, it will incorrectly generates the same expression for problems with different numerical values.\nTo address these problems, we propose a novel approach called NumS2T to better capture numerical value information and utilize numerical properties. Specifically, the proposed model uses a sequence-to-tree network with a digit-to-digit number encoder that explicitly incorporates numerical values into the model and captures number-aware problem representations. In addition, we designed a numerical properties prediction mechanism to further utilize the numerical properties. NumS2T predicts the comparative relationship between paired numerical values, determines the category of each numeral, and measures their importance for generating the final expression. With the category and comparison information, the model can better identify the interactive relationship between the numerals, and thus generate better results. With consideration of the importance of the numerals, the model can capture the global relationship between the numerals and target expressions rather than simply focusing on the local relationship between numeral pairs.\nThe main contributions of this paper can be summarized as follows:\n• We explicitly incorporate numerical value information into math word problem solving tasks.\n• We propose a numerical properties prediction mechanism to utilize numerical properties. To incorporate the local relationship between numerals and the global relationship associated with the final expression, NumS2T compares the paired numerical values, determines the category of each numeral, and then measures\nwhether they should appear in the final expression.\n• We conducted experiments on two largescale Math23K and Ape210K datasets to verify the effectiveness of our NumS2T model. The results show that our model achieved better performance than existing state-of-theart methods."
    }, {
      "heading" : "2 Models",
      "text" : "In this section, we present details regarding our proposed NumS2T model. As shown in Figure 2, we use an attention-based sequence-to-tree model with a problem encoder (Section 2.2) and a treestructured decoder to generate math expressions (Section 2.4). In addition, we explicitly incorporate numerical values to obtain number-aware problem representations (Section 2.3). Finally, we propose a numerical properties prediction mechanism to further utilize the numerical properties (Section 2.5)."
    }, {
      "heading" : "2.1 Problem Definition",
      "text" : "A math word problem X = (x1, x2, . . . , xm) is a sequence of m words. Our goal is to generate a math expression Y = (y1, y2, . . . , yn), where Y is the pre-order traversal sequence of a binary math expression tree, which can be executed to produce the answer to problem X.\nHere, we replace all of the numbers in the problem X with a list of number symbols based on their order of appearance. Let Vc = (v1, v2, . . . , vK) be the K numbers that appear in problem X. The numerical value of the k-th number vk is a sequence of l characters (v1k, v 2 k, . . . , v l k). The generated vocabulary Vg is composed of several common numbers (e.g., 1,100,π) and several math operators (e.g., +,-,*,/). At each time step during decoding, the NumS2T model either copies a number from Vc or generates a number from Vg."
    }, {
      "heading" : "2.2 Problem Encoder",
      "text" : "We use a two-layer bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) network as the encoder, which encodes the math word problem X into a sequence of hidden states\nH=(hx1,h x 2, . . . ,h x m) ∈ Rm×2d as follows:\nhxi = [ −→ hxi , ←− hxi ], −→ hxi = BiLSTM(E(xi), −−→ hxi−1), ←− hxi = BiLSTM(E(xi), ←−− hxi−1).\n(1)\nHere, word embedding vectors E(xi) are obtained via a wording embedding layer E(·). d is the dimension of the hidden state and hxi is the concatenation of the forward and backward LSTM hidden states.\nFollowing Wu et al. (2020), we enrich the problem representations with common-sense knowledge information from external knowledge bases. The words in problem sequences X and their categories in external knowledge bases are constructed as an entity graph. In this entity graph, each word is related to its neighbor in the problem. If there are two nouns belonging to the same category in the knowledge base, these two nouns are related\nto their categories. See Wu et al. (2020) for more details.\nThe knowledge-aware problem states hkgi are obtained from a two-layer graph attention network (Veličković et al., 2018) on the entity graph:\nαij = softmax Aij=1\n( f (wTh [Wxh x i : Wxh x j ])),\nhkgi = || t=1,...,T\nσ( ∑\nAij=1 αijWkh\nx j ),\n(2)\nwhere wTh ,Wx,Wk are weight vector and matrices. || and [:] are concatenation functions. f(·) and σ are the LeakyRelu and sigmoid activation functions. T is the number of heads in GAT layer. If the i-th word is related to the j-th word, the score of the adjacent matrix Aij is set to 1, otherwise it is set to 0."
    }, {
      "heading" : "2.3 Number-aware Problem Representations",
      "text" : "To solve the issues mentioned in the introduction section, we need to incorporate explicit numerical value information into NumS2T. However, there are an infinite number of numerals that can appear in math word problems. For example, among the 18,529 problems in the training set of Math23K, there are 3,058 different numerical values. Therefore, rather than list all these numerals in the vocabulary, we encode each numeral value digit by digit.\nAll the digits in the numerical value vk are treated as a sequence (v1k, v 2 k, . . . , v l k) and embedded via the embed layer E(·). Take a 5-digit value vk = (1/3) as an example, we have E(vk) ∈ R5×demb . Similar to the architecture shown in Equation 1, we use a BiLSTM network to encode the numeral values and obtain the numeral hidden states hvk with an average pooling layer:\nhnvk,j = BiLSTM(E(v j k),h n vk,j−1 ),\nhnvk = 1\nl ∑l j=1 hnvk,j . (3)\nTo capture the relations and dependency between numeral pairs, we use a self-attention mechanism (Wang et al., 2017a) on the hidden state of all the numerals Hnv = {hnvk} K k=1 to compute the contextual numeral hidden states hcnvk :\nαvk = softmax( (H n v) TWhh n vk ), hcnvk = αvk ·H n v,\n(4)\nwhere αvk is the attention distribution of vk on all the numerals in the problem X.\nCombining the numeral hidden states hnvk , h cn vk\nwith the original problem hidden states hxi , h kg i , we have number-aware problem states hnumi enhanced with explicit numeral value information:\nhnumi =\n{ [hnvk : h cn vk ] xi = vk\n[hxi : h kg i ] xi is not a number\n(5)\nThe final number-aware problem representations are obtained by concatenating the problem hidden states hxi , the knowledge-aware problem states h kg i and the number-aware problem states hnumi :\nhi = [h x i : h kg i : h num i ]. (6)"
    }, {
      "heading" : "2.4 Tree Structured Decoder",
      "text" : "Previous works (Xie and Sun, 2019; Liu et al., 2019; Wu et al., 2020) have confirmed that a sequence-to-tree model can better represent the expression structures than a sequence-to-sequence model, because a tree structured decoder can capture the global expression information and focus on the features of adjacent nodes.\nThe tree structured decoder takes the final number-aware problem representations hi as input and generates the target expression from top to bottom. The target expression can be regarded as a pre-order traversal of a binary tree, with operators as internal nodes and numbers as leaf nodes. The decoder is a one-layer LSTM, which updates its states as follows:\nst+1 = LSTM([E(yt) : ct : rt], st). (7)\nAt time step t+1, the decoder uses the last generated word embedding E(yt), the problem context state ct and the expression context state rt to update its previous hidden state st.\nThe problem context state ct is computed via attention mechanism as follows:\nαti = softmax(tanh(Whhi+Ws[st : rt])), ct = m∑ i=1 αtihi, (8)\nwhere Wh, Ws are weight matrices. αti is the attention distribution on the number-aware problem representations hi.\nThe expression context state rt is computed via a state aggregation mechanism (Wu et al., 2020). It describes the global representation of the partial expressions y<t = (y1, y2, . . . , yt−1) being generated by the decoder. At time step t, the decoder aggregates each node’s context state with its neighbor nodes in the generated partial expression tree. The aggregation functions are as follows:\nr0t = st,\nrη+1t = σ(Wr[r η t : r η t,p : r η t,l : r η t,r]),\n(9)\nwhere σ is the sigmoid function and Wr is a weight matrix. r0t is initialized with decoder hidden state st when η = 0,. rt,p, rt,l, rt,r are the context state of the parent node, the left child node, and the right child node of yt in the expression tree. r η+1 t represents the expression context state updated with\nglobal information from all nodes in the generated partial expression.\nLastly, the decoder can generate a word from a given vocabulary Vg. It can also generate a number symbol in Vc, and use it to copy a number from the problem X. The final distribution is the combination of the generated probability and copy probability:\nHv = {hvk} K k=1,\npc = σ(Wz[st : ct : rt] +WvHv),\nPc(yt) = softmax(f ([st : ct : rt : Hv])),\nPg(yt) = softmax(f ([st : ct : rt])), P(yt|y<t,X) = pcPc(yt) + (1−pc)Pg(yt).\n(10)\nHere, Hv are the number-aware problem representations of all the numerals vk in X. Wz,Wv are the weight matrices. f(·) is a perception layer. pc is the probability that the current word is a number copied from the problem."
    }, {
      "heading" : "2.5 Numerical Properties Prediction Mechanism",
      "text" : "Our NumS2T model explicitly incorporates numerical values information. Furthermore, utilize the numerical properties to the degree possible through a numerical properties prediction mechanism. We consider three numerical properties to be useful for solving math word problems:\nPairwise Numeral Comparison. If we consider the question “What is the difference between v1 and v2,” the comparative relationship between these two numerals can help the model decide whether to generate v1 − v2 or v2 − v1. In this paper, we compare each numeral vk in the question with the other numerals. Then, we calculate the pairwise comparison scores zkj based on their number-aware problem representations, and we optimize the pairwise comparison loss to assign numerals with larger numerical values higher pairwise comparison scores. The pairwise comparison loss LCR is calculated as follows:\ngvk = σ(Whhvk),\nzkj = { max(0, gvj − gvk) if vk ≥ vj max(0, gvk − gvj ) if vk < vj ,\nLCR = − 1\nK2 K∑ k=1 K∑ j=1 zkj ,\n(11)\nNumeral categories. In the sentence “the number of apples is 5 more than the number of pears,” replacing the numeral 5 with the integer 100 may not affect the structure of the target expression, but replacing the numeral 5 with 20% may change the structure from “+5” to “*(1 + 20%)”. We roughly divide all numbers into four categories: {integer, decimal, fraction, percentage}, and assign a category label C = {1,2,3,4}, respectively. Given the number-aware problem representations hvk for each numeral vk, we calculate the category score distribution P(Cvk |hvk) and then minimize the negative log likelihood:\nP(Cvk |hvk)=softmax(Wchvk),\nLCA = − 1\nK K∑ k=1 log P(Cvk |hvk). (12)\nGlobal relationship with target expressions. Current models tend to focus on the local relationship between numerals, while sometimes these numerals are not related to the target expression. Given “3 bags of rice weighing 60 kg,” the numeral 3 is highly correlated with 60. However, if the problem relates to the total price of the rice rather than the weight of each bag of rice, the numeral 3 is not so important for generating the target expression. The NumS2T model predicts a scalar value g′vk for each numeral that denotes whether this numeral will be used in a math expression. The importance label avk=1 when vk is used in the ground truth math expression, otherwise avk=0. The supervised loss is defined by:\ng′vk = σ(Wghvk),\nLGR=− 1\nK K∑ k=1 ai log g ′ vk +(1−ai) log (1−g′vk).\n(13)"
    }, {
      "heading" : "2.6 Training",
      "text" : "During training, for each question–expression pair (X, Y), we first train the NumS2T by optimizing the maximum likelihood estimation (MLE) loss Ll on the probability distribution P(yt|y<t,X)). Then, the final loss function L is a combination of the MLE loss and three numerical properties loss functions:\nLl = − 1\nn n∑ i=1 logP(yt|y<t,X)),\nL = Ll + β1LCR + β2LCA + β3LGR. (14)\nHere, β1, β2, β3 are hyper-parameters."
    }, {
      "heading" : "3 Experiment",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "We present the experimental results of math word problem solving using our proposed models on the Math23K (Wang et al., 2017b) and Ape210K (Zhao et al., 2020)2 datasets. Following Xie and Sun (2019), we removed the problems that the corresponding expressions could not be executed to obtain the given answers and the problems that omit intermediate calculation expressions. For Math23K, following previous studies (Xie and Sun, 2019; Wu et al., 2020), we randomly split the dataset into a training set, a development set and a test set with 18,529, 2,316, 2,316 problems. For Ape210K, we use the official data partition. There are 166,270, 4,157, and 4,159 problems in our training set, development set and test set, respectively.\nWe report answer accuracy as the main evaluation metrics of the math word problem solving task."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "In this paper, we truncate the problem to a max sequence length of 150, and the expression to a max sequence length of 50. We select 4,000 words that appear most frequently in the training set of each dataset as the vocabulary, and replace the remaining words with a special token UNK. We initialize the word embedding with the pretrained 300-dimension word vectors3. The problem encoder used two external knowledge bases: Cilin (Mei, 1985) and Hownet (Dong et al., 2010). The number of heads T in GAT is 8. The hidden size is 512 and the batch size is 64. We use the Adam optimizer (Kingma and Ba, 2014) to optimize the models an the learning rate is 0.001. We compute the final loss function with β1, β2, β3 of 0.5. Dropout (Srivastava et al., 2014) is set to 0.5. Models are trained in 80 epoches for the Math23K dataset and 50 epoches for the Ape210K dataset. During testing, the beam size is set to 5. Once all internal nodes in the expression tree have two child nodes, the decoder stops generating the next word. The hyper-parameters are tuned on the valid set.\n2https://github.com/yuantiku/ape210k 3https://github.com/Embedding/Chinese-Word-Vectors"
    }, {
      "heading" : "3.3 Baselines",
      "text" : "We compare our proposed NumS2T model with the following baseline models: DNS (Wang et al., 2017b) is a seq2seq model with a two-layer GRU as an encoder and a two-layer LSTM as a decoder. DNS-Retrieval is a variant of DNS that combines a retrieval model. S2S (Wang et al., 2018a) is a standard bidirectional LSTM-based seq2seq model with an attention mechanism. RecursiveNN (Wang et al., 2019) uses a recursive neural network on the predicted tree structure templates TreeDecoder (Liu et al., 2019) is a seq2tree model with a tree structured decoder. The decoder generates each node based on its parent node and its sibling node. GTS (Xie and Sun, 2019) generates each node based on its parent node and its left sibling subtree embedding. The subtree embedding is obtained by merging the embedding of the subtree from bottom to top. KA-S2T (Wu et al., 2020) is a seq2tree model with external knowledge and a state aggregation mechanism. The decoder use a two-layer GCN to recursively aggregate neighbors of each node in the partial expression tree."
    }, {
      "heading" : "3.4 Results Analysis",
      "text" : "The main evaluation results are presented in Table 1. Compared with baseline methods, our model obtains the highest answer accuracy of 78.1% in the Math23K dataset and 70.5% in the APE210K dataset, which is significantly better than other state-of-the-art methods. The experimental results provide the following observations:\n1) The methods with a tree-structured decoder (Tree-Decoder, GTS, KA-S2T) perform better than methods with a sequence-structured decoder (DNS, S2S). These methods treat the math expression as a binary tree and directly use adjacent nodes in the\ntree instead of the previous word in the sequence to generate the next word. In this way, the model can better capture the structure information of the math expressions.\n2) The KAS2T model with external knowledge performs better than GTS, which proves that external knowledge enables the model to obtain better interaction between words.\n3) NumS2T outperforms all the other baselines. This result shows the effectiveness of the explicitly incorporated numerical values and use of a numerical properties prediction mechanism."
    }, {
      "heading" : "3.5 Ablation Study",
      "text" : "Effect of explicitly incorporating numerical values: We designed several NumS2T variants that reduce the numerical values incorporated in the model. Here, “NumS2T w/o Numerals” means that we remove the character-level numeric value encoder. An input example is “Alan bought v1 apples for $ v2”. “NumS2T w/o Symbols” means that we not only remove the character-level numeric value encoder, but also replace the math symbols in math problems with character-level numeric values. An input example is “Alan bought 2 5 apples for $ 1 5 0”.\nTable 2 shows the results of these different variants, from which we can see:\n1)The experimental results show that model performance of “NumS2T w/o Symbols” is significantly reduced in both datasets. We believe this is because directly replacing the number symbols will make it difficult for the model to obtain the overall representation of each number.\n2) The use of a self-attention mechanism significantly improves the accuracy by 0.8% in Math23K and 0.7% in APE210K. This is because the same numerical value may describe different information in different problems. Therefore, the self-attention mechanism combines numerical values with other numerical values in the problem, which helps to model numerical information and the relations between these numerals.\n3) Without numerical values, the answer accuracy of “NumS2T w/o Numerals” would be reduced to 76.6% and 69.2%. The results show the benefit of explicitly incorporating numerical values. Effect of the numerical properties prediction mechanism: Table 3 shows the results of several NumS2T variants designed to measure the effect\nof the numerical properties prediction mechanism. From the table we can observe that:\n1) NumS2T-base is the variant of NumS2T without the numerical properties prediction mechanism. Without numerical properties, the answer accuracy in the Math23K and APE210K datasets are reduced to 77.0% and 69.6%, which show that the numerical properties prediction mechanism contributes considerably to improving performance. In addition, NumS2T-base still outperforms the state-of-the-art baseline KA-S2T, which once again proves the effectiveness of explicitly incorporating numerical values.\n2) The use of pairwise numeral comparison, numeral category and global relationship with a target expression can improve accuracy by approximately 0.6%, 0.4% and 0.3%, respectively. Their combination achieves further improvements in model performance. These results show the effectiveness of the numerical properties prediction mechanism because it enables the model to further utilize numerical properties. Model performance on problems with a different number of numerals: Table 4 shows the results for how accuracy changes as the number of numerals in the problem increases. The NumS2T model outperforms the best-performing baseline with respect to problems with a different number of\nnumerals. In addition, as the number of numerals in the problems increase, the performance gap between NumS2T and KAS2T also increases. This is because with more numerals in the problem, NumS2T, which explicitly incorporate numerical value information, is able to more readily achieve better performance. Meanwhile, NumS2T also achieved a considerable improvement on problems with only one numeral. This further demonstrates the effect of utilizing numerical category information and global relationship information."
    }, {
      "heading" : "3.6 Case Study",
      "text" : "Table 5 shows three cases generated by KA-S2T (Wu et al., 2020) and our NumS2T model. In the first problem, without numerical values, KA-S2T incorrectly uses the smaller value to subtract the larger value when calculating the price difference between footballs and basketballs. This case requires the model to choose the larger value between two numerals. Our NumS2T model can better handle this problem. In the second problem, KA-S2T replaces all of the numerals in the problems with number symbols (v1, v2) and does not know that v2=20% is not an integer. Our proposed method can capture numerical values and numeral category information to generate\ncorrect results. In the third problem, 80 seats and 52 tickets are strongly semantically related, so KA-S2T generates the sub-expression “80-52”. However, this problem is about the fares that have already been sold rather than how many tickets are left. With numerical properties, NumS2T is able to realize that 80 is not related to the target expression and should not appear in the generated result."
    }, {
      "heading" : "4 Related Work",
      "text" : "Math Word Problem Solving: In recent years, Seq2Seq (Sutskever et al., 2014) has been widely used in math word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each\nnumber in the problem with nearby nouns to enrich the problem representations.\nHowever, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this study, we proposed a novel approach called NumS2T, that better captures numerical value information and utilizes numerical properties. In this model, we use a digit-to-digit numerical value encoder to explicitly incorporate numerical values. In addition, we designed a numerical properties prediction mechanism that compares the paired numerical values, determines the category of each numeral, and measures whether they should appear in the final expression. Experimental results show that our proposed NumS2T model outperforms other state-of-the-art baseline methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&D Program (No. 2018YFB1005100), National Natural Science Foundation of China (No. 62076069, 61976056), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103)."
    } ],
    "references" : [ {
      "title" : "Tree-structured decoding for",
      "author" : [ "Kawahara" ],
      "venue" : null,
      "citeRegEx" : "2019.,? \\Q2019\\E",
      "shortCiteRegEx" : "2019.",
      "year" : 2019
    }, {
      "title" : "Tongyi ci cilin",
      "author" : [ "Jiaju Mei." ],
      "venue" : "Shangai cishu chubanshe.",
      "citeRegEx" : "Mei.,? 1985",
      "shortCiteRegEx" : "Mei.",
      "year" : 1985
    }, {
      "title" : "Learning to use formulas to solve simple arithmetic problems",
      "author" : [ "Arindam Mitra", "Chitta Baral." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2144–2153, Berlin, Germany.",
      "citeRegEx" : "Mitra and Baral.,? 2016",
      "shortCiteRegEx" : "Mitra and Baral.",
      "year" : 2016
    }, {
      "title" : "Exploring numeracy in word embeddings",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Carolyn Rose", "Eduard Hovy." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374–3380, Florence, Italy.",
      "citeRegEx" : "Naik et al\\.,? 2019",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2019
    }, {
      "title" : "Solving general arithmetic word problems",
      "author" : [ "Subhro Roy", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752, Lisbon, Portugal. Association for Computational",
      "citeRegEx" : "Roy and Roth.,? 2015",
      "shortCiteRegEx" : "Roy and Roth.",
      "year" : 2015
    }, {
      "title" : "Numeracy for language models: Evaluating and improving their ability to predict numbers",
      "author" : [ "Georgios Spithourakis", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Spithourakis and Riedel.,? 2018",
      "shortCiteRegEx" : "Spithourakis and Riedel.",
      "year" : 2018
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15(56):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Do NLP models know numbers? probing numeracy in embeddings",
      "author" : [ "Eric Wallace", "Yizhong Wang", "Sujian Li", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Translating a math word problem to a expression tree",
      "author" : [ "Lei Wang", "Yan Wang", "Deng Cai", "Dongxiang Zhang", "Xiaojiang Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1064–1069,",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "2018b. Mathdqn: Solving arithmetic word problems via deep reinforcement learning",
      "author" : [ "Lei Wang", "Dongxiang Zhang", "Lianli Gao", "Jingkuan Song", "Long Guo", "Heng Tao Shen" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Template-based math word problem solvers with recursive neural networks",
      "author" : [ "Lei Wang", "Dongxiang Zhang", "Jipeng Zhang", "Xing Xu", "Lianli Gao", "Bing Tian Dai", "Heng Shen." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33:7144–",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Wang et al\\.,? 2017a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep neural solver for math word problems",
      "author" : [ "Yan Wang", "Xiaojiang Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845–854, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wang et al\\.,? 2017b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "A knowledge-aware sequence-to-tree network for math word problem solving",
      "author" : [ "Qinzhuo Wu", "Qi Zhang", "Jinlan Fu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "A goal-driven tree-structured neural model for math word problems",
      "author" : [ "Zhipeng Xie", "Shichao Sun." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299–5305. International Joint",
      "citeRegEx" : "Xie and Sun.,? 2019",
      "shortCiteRegEx" : "Xie and Sun.",
      "year" : 2019
    }, {
      "title" : "Teacher-student networks with multiple decoders for solving math word problem",
      "author" : [ "Jipeng Zhang", "Roy Ka-Wei Lee", "Ee-Peng Lim", "Wei Qin", "Lei Wang", "Jie Shao", "Qianru Sun." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graphto-tree learning for solving math word problems",
      "author" : [ "Jipeng Zhang", "Lei Wang", "Roy Ka-Wei Lee", "Yi Bin", "Yan Wang", "Jie Shao", "Ee-Peng Lim." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Ape210k: A large-scale and template-rich dataset of math word problems",
      "author" : [ "Wei Zhao", "Mingyue Shang", "Yang Liu", "Liang Wang", "Jingming Liu." ],
      "venue" : "arXiv preprint arXiv:2009.11506.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 209
    }, {
      "referenceID" : 2,
      "context" : "With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 209
    }, {
      "referenceID" : 16,
      "context" : "and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "The knowledge-aware problem states h i are obtained from a two-layer graph attention network (Veličković et al., 2018) on the entity graph:",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "To capture the relations and dependency between numeral pairs, we use a self-attention mechanism (Wang et al., 2017a) on the hidden state of all the numerals Hv = {hnvk} K k=1 to compute the contextual numeral hidden states hcn vk :",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Previous works (Xie and Sun, 2019; Liu et al., 2019; Wu et al., 2020) have confirmed that a sequence-to-tree model can better represent the expression structures than a sequence-to-sequence model, because a tree structured decoder can capture the global expression information and focus on the features of adjacent nodes.",
      "startOffset" : 15,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "Previous works (Xie and Sun, 2019; Liu et al., 2019; Wu et al., 2020) have confirmed that a sequence-to-tree model can better represent the expression structures than a sequence-to-sequence model, because a tree structured decoder can capture the global expression information and focus on the features of adjacent nodes.",
      "startOffset" : 15,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "The expression context state rt is computed via a state aggregation mechanism (Wu et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "We present the experimental results of math word problem solving using our proposed models on the Math23K (Wang et al., 2017b) and Ape210K (Zhao et al.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "For Math23K, following previous studies (Xie and Sun, 2019; Wu et al., 2020), we randomly split the dataset into a training set, a development set and a test set with 18,529, 2,316, 2,316 problems.",
      "startOffset" : 40,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "For Math23K, following previous studies (Xie and Sun, 2019; Wu et al., 2020), we randomly split the dataset into a training set, a development set and a test set with 18,529, 2,316, 2,316 problems.",
      "startOffset" : 40,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "The problem encoder used two external knowledge bases: Cilin (Mei, 1985) and Hownet (Dong et al.",
      "startOffset" : 61,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "We compare our proposed NumS2T model with the following baseline models: DNS (Wang et al., 2017b) is a seq2seq model with a two-layer GRU as an encoder and a two-layer LSTM as a decoder.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "S2S (Wang et al., 2018a) is a standard bidirectional LSTM-based seq2seq model with an attention mechanism.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "RecursiveNN (Wang et al., 2019) uses a recursive neural network on the predicted tree structure templates TreeDecoder (Liu et al.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "GTS (Xie and Sun, 2019) generates each node based on its parent node and its left sibling subtree embedding.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "KA-S2T (Wu et al., 2020) is a seq2tree model with external knowledge and a state aggregation mechanism.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "Table 5 shows three cases generated by KA-S2T (Wu et al., 2020) and our NumS2T model.",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Table 5: Three cases of generated expressions by KAS2T (Wu et al., 2020) and NumS2T.",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Math Word Problem Solving: In recent years, Seq2Seq (Sutskever et al., 2014) has been widely used in math word problem solving tasks (Ling et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 182
    } ],
    "year" : 2021,
    "abstractText" : "In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration. Most methods treat the numerical values in the problems as number symbols, and ignore the prominent role of the numerical values in solving the problem. In this paper, we propose a novel approach called NumS2T, which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network. In addition, a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions. Experimental results on the Math23K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models. 1",
    "creator" : "LaTeX with hyperref package"
  }
}