{
  "name" : "2021.acl-long.17.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Modularized Interaction Network for Named Entity Recognition",
    "authors" : [ "Fei Li", "Zheng Wang", "Siu Cheung Hui", "Lejian Liao", "Dandan Song", "Jing Xu", "Guoxiu He", "Meihuizi Jia" ],
    "emails" : [ "lifei926@bit.edu.cn", "liaolj@bit.edu.cn", "sdd@bit.edu.cn", "xujing@bit.edu.cn", "jmhuizi24@bit.edu.cn", "zheng@ntu.edu.sg,", "asschui@ntu.edu.sg,", "guoxiu.he@whu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 200–209\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n200"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is one of the fundamental tasks in natural language processing (NLP) that intends to find and classify the type of a named entity in text such as person (PER), location (LOC) or organization (ORG). It has been widely used for many downstream applications such as relation extraction (Xiong et al., 2018), entity linking (Gupta et al., 2017), question generation (Zhou et al., 2017) and coreference resolution (Barhom et al., 2019).\n∗Corresponding authors.\nCurrently, there are two types of methods for the NER task. The first one is sequence labeling-based methods (Lample et al., 2016; Chiu and Nichols, 2016; Luo et al., 2020), in which each word in a sentence is assigned a special label (e.g., B-PER or IPER). Such methods can capture the dependencies between adjacent word-level labels and maximize the probability of predicted labels over the whole sentence. It has achieved the state-of-the-art performance in various datasets over the years. However, NER is a segment-level recognition task. As such, the sequence labeling-based models which focus only on word-level information do not perform well especially in recognizing long entities (Ye and Ling, 2018). Recently, segment-based methods (Kong et al., 2016; Li et al., 2020b; Yu et al., 2020b; Li et al., 2021) have gained popularity for the NER task. They process segment (i.e., a span of words) instead of single word as the basic unit and assign a special label (e.g., PER, ORG or LOC) to each segment. As these methods adopt segment-level processing, they are capable of recognizing long entities. However, the word-level dependencies within a segment are usually ignored.\nNER aims at detecting the entity boundaries and the type of a named entity in text. As such, the NER task generally contains two separate and independent sub-tasks on boundary detection and type prediction. However, from our experiments, we observe that the boundary detection and type prediction sub-tasks are actually correlated. In other words, the two sub-tasks can interact and mutually reinforce each other by sharing their information. Consider the following example sentence: “Emmy Rossum was from New York University”. If we know “University” is an entity boundary, it will be more accurate to predict the corresponding entity type to be “ORG”. Similarly, if we know an entity has an “ORG” type, it will be more accurate to predict that “University” is the end boundary of\nthe entity “New York University” instead of “York” (which is the end boundary for the entity “New York”). However, sequence labeling-based models consider the boundary and type as labels, and thus such information cannot be shared between the subtasks to improve the accuracy. On the other hand, segment-based models first detect the segments and then classify them into the corresponding types. These methods generally cannot use entity type information in the process of segment detection and may have errors when passing such information from segment detection to segment classification.\nIn this paper, we propose a Modularized Interaction Network (MIN) model which consists of the NER Module, Boundary Module, Type Module and Interaction Mechanism for the NER task. To tackle the issue on recognizing long entities in sequence labeling-based models and the issue of utilizing word-level dependencies within a segment in segment-based models, we incorporate a pointer network (Vinyals et al., 2015) into the Boundary Module as the decoder to capture segment-level information on each word. Then, these segmentlevel information and the corresponding word-level information on each word are concatenated as the input to the sequence labeling-based models.\nTo enable interaction information, we propose to separate the NER task into the boundary detection and type prediction sub-tasks to enhance the performance of the two sub-tasks by sharing the information from each sub-task. Specifically, we use two different encoders to extract their distinct contextual representations from the two sub-tasks and propose an Interaction Mechanism to mutually reinforce each other. Finally, these information are fused into the NER Module to enhance the performance. In addition, the NER Module, Boundary Module and Type Module share the same word representations and we apply multitask training when training the proposed MIN model.\nIn summary, the main contributions of this paper include:\n• We propose a novel Modularized Interaction Network (MIN) model which utilizes both the segment-level information from segmentbased models and word-level dependencies from sequence labeling-based models in order to enhance the performance of the NER task.\n• The proposed MIN model consists of the NER Module, Boundary Module, Type Module and\nInteraction Mechanism. We propose to separate boundary detection and type prediction into two sub-tasks and the Interaction Mechanism is incorporated to enable information sharing between the two sub-tasks to achieve the state-of-the-art performance.\n• We conduct extensive experiments on three NER benchmark datasets, namely CoNLL2003, WNUT2017 and JNLPBA, to evaluate the performance of the proposed MIN model. The experimental results have shown that our MIN model has achieved the state-of-the-art performance and outperforms the existing neural-based NER models."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we review the related work on the current approaches for Named Entity Recognition (NER). These approaches can be categorized into sequence labeling-based NER and segment-based NER."
    }, {
      "heading" : "2.1 Sequence Labeling-based NER",
      "text" : "Sequence labeling-based NER is regarded as a sequence labeling task, where each word in a sentence is assigned a special label (e.g., B-PER, IPER). Huang et al. (Huang et al., 2015) utilized the BiLSTM as an encoder to learn the contextual representation of words, and then Conditional Random Fields (CRFs) was used as a decoder to label the words. It has achieved the state-of-the-art results on various datasets for the past many years. Inspired by the success of the BiLSTM-CRF architecture, many other state-of-the-art models have adopted such architecture. Chiu and Nichols (Chiu and Nichols, 2016) used Convolutional Neural Network (CNN) to capture spelling features, and the character-level and word-level embeddings are concatenated as the input of BiLSTM with CRF network. Further, Lample et al. (Lample et al., 2016) proposed RNN-BiLSTM-CRF as an alternative. More recently, pretrained language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have been adopted to further enhance the performance of NER."
    }, {
      "heading" : "2.2 Segment-based NER",
      "text" : "Segment-based NER identifies segments in a sentence and classifies each segment with a special label (e.g., PER, ORG or LOC). Kong et al. (Kong et al., 2016) used BiLSTM to map arbitrary-length\nsegment into a fixed-length vector, and then these vectors were passed to Semi-Markov Conditional Random Fields (Semi-CRFs) for labeling the segments. Zhuo et al. (Zhuo et al., 2016) adopted a gated recursive Convolutional Neural Network instead of BiLSTM to build a pyramid-like structure for extracting segment-level features in a hierarchical way. In recent years, Ye et al. (Ye and Ling, 2018) exploited the weighted sum of word-level within segment to learn segment-level features with Semi-CRFs which was then trained jointly on word-level with the BiLSTM-CRF network. Li et al. (Li et al., 2020a) used a recurrent neural network encoder-decoder framework with a pointer network to detect entity segments. Li et al. (Li et al., 2020b) treated NER as a machine reading comprehension (MRC) task, where entities were extracted as retrieved answer spans. Yu et al. (Yu et al., 2020b) ranked all the spans in terms of the pairs of start and end tokens in a sentence using a biaffine model."
    }, {
      "heading" : "3 Proposed Model",
      "text" : "This section presents our proposed Modularized Interaction Network (MIN) for NER. The overall model architecture is shown in Figure 1(a), which consists of the NER Module, Boundary Module, Type Module and Interaction Mechanism."
    }, {
      "heading" : "3.1 NER Module",
      "text" : "In the NER Module, we adopt the RNN-BiLSTMCRF model (Lample et al., 2016) as our backbone, which consists of three components: word representation, BiLSTM encoder and CRF decoder. Word Representation Given an input sentence S =< w1, w2, · · · , wn >, each word wi(1 ≤ i ≤ n) is represented by concatenating a word-level embedding xwi and a character-level word embedding xci as follows:\nxi = [x w i ;x c i ] (1)\nwhere xwi is the pre-trained word embedding, and the character-level word embedding xci is obtained with a BiLSTM to capture the orthographic and morphological information. It considers each character in the word as a vector, and then inputs them to a BiLSTM to learn the hidden states. The final hidden states from the forward and backward outputs are concatenated as the character-level word information.\nBiLSTM Encoder The distributed word embeddings X =< x1, x2, · · · , xn > are then fed into the BiLSTM encoder to extract the hidden sequences H =< h1, h2, · · · , hn > of all words as follows:\nhi = [−→ hi ; ←− hi ] −→ hi = LSTM ( xi, −−→ hi−1\n) ←− hi = LSTM ( xi, ←−− hi−1 ) (2)\nIn the NER Module, we fuse the distinct contextual boundary representation and type representation for the NER task. In addition, we also fuse the segment information from the Boundary Module to support the recognition of long entities. Note that the boundary information and type information can mutually reinforce each other. Thus, we use an interaction mechanism to reinforce them before fusing these information in the NER Module. Instead of directly concatenating these information with hidden representations in the NER module, we follow the previous studies (Zhang et al., 2018; Yu et al., 2020a) to use a gate function to dynamically control the amount of information flowing by infusing the expedient part while excluding the irrelevant part. The gate function uses the information from the NER Module to guide the process, which is described formally as follows:\nH Bdy , H Type = interact(HBdy, HType) HB = σ ( W>1 H +W > BH Bdy ) ⊗HBdy\nHT = σ ( W>2 H +W > T H Type ) ⊗HType\nHS = σ ( W>3 H +W > S H Seg ) ⊗HSeg\n(3)\nwhere HBdy and HType represent the distinct representations of hidden sequences from the Boundary Module and Type Module respectively, and HSeg represents the segment information from the Boundary Module. We will discuss them in Section 3.2 and Section 3.3. HBdy and HType represent the distinct representations of hidden sequences from the Boundary Module and Type Module respectively after the interaction using an interaction mechanism interact(·, ·), and we will discuss them in Section 3.4. HB , HT and HS represent the boundary, type and segment information respectively to be injected into the NER Module from the gate function. σ denotes the logistic\nsigmoid function and ⊗ denotes the element-wise multiplication.\nThe final hidden representations in the NER Module are as follows:\nHNER =W>[H;HB;HT ;HS ] + b (4)\nCRF Decoder CRF has been widely used in the state-of-the-art NER models (Chiu and Nichols, 2016; Lample et al., 2016) to model tagging decisions when considering strong connections between output tags. For an input sentence S =< w1, w2, · · · , wn >, the score of a predicted sequence of labels y =< y1, y2, · · · , yn > is defined as follows:\nsc (S, y) = n∑ i=0 Tyi,yi+1 + n∑ i=1 Pi,yi (5)\nwhere Tyi,yi+1 represents the score of a transition from yi to yi+1, and Pi,yi is the score of the yi tag of the ith word in a sentence.\nThe CRF model describes the probability of predicted labels y over all possible tag sequences in the set Y , that is:\np (y|S) = e sc(S,y)∑\nỹ∈Y e sc(S,ỹ)\n(6)\nWe maximize the log-probability of the correct sequence of labels during the training. During decoding, we predict the label sequence with the maximum score:\ny∗ = argmax ỹ∈Y sc (S, ỹ) (7)"
    }, {
      "heading" : "3.2 Boundary Module",
      "text" : "The Boundary Module needs to provide not only distinct contextual boundary information but also\nsegment information for the NER Module. Here, we use another BiLSTM as encoder to extract distinct contextual boundary information. And inspired by BDRYBOT (Li et al., 2020a), a recurrent neural network encoder-decoder framework with a pointer network is used to detect entity segments for segment information. The BDRYBOT model processes the starting boundary word in an entity to point to the corresponding ending boundary word. The other entity words in the entity are skipped. The non-entity words are pointed to a specific position. This method has achieved promising results in the boundary detection task. However, due to the variable length of entities, this model is deprived of the power of batch training. In addition, as the segment information of each word in an entity is the same as the starting boundary word, the segment information for all the words within a segment will be incorrect if the starting boundary word is detected wrongly. To avoid this problem, we improve the training process and propose a novel method to capture the segment information of each word.\nWe train the starting boundary word to point to the corresponding ending boundary word, and the other words in the sentence to a sentinel word inactive. The process is shown in Figure 1(b). Specifically, we use another BiLSTM as encoder to obtain the distinct boundary hidden sequences HBdy =< hBdy1 , h Bdy 2 , · · · , h Bdy n >, and a sentinel vector is padded into the last positions of hidden sequences HBdy for the sentinel word inactive. Then, a unidirectional LSTM is used as a decoder to generate the decoded state dj at each time step j. To add extra information to the input of the LSTM, we follow (Fernández-González and Gómez-Rodrı́guez, 2020) and use the sum of the hidden states of current (hBdyi ), previous (h Bdy i−1 ) and next (hBdyi+1 ) words instead of word embedding\nas the input to the decoder as follows:\nsj = h Bdy j−1 + h Bdy j + h Bdy j+1 dj = LSTM (sj , dj−1) (8)\nNote that the first word and last word do not have hidden states of previous and next, we use zero vectors to represent it which are shown as grey blocks in Figure 1(b).\nAfter that, we use the biaffine attention mechanism (Dozat and Manning, 2017) to generate a feature representation for each possible boundary position i at time step j, and the Softmax function is used to obtain the probability of word wi for determining an entity segment that starts with word wj and ends with word wi.\nuji = dj TWhBdyi + U Tdj + V ThBdyi + b p (wi|wj) = Softmax ( uji ) , i ∈ [j, n+ 1] (9)\nwhere W is the weight matrix of bi-linear term, U and V are the weight matrices of linear terms, b is the bias vector and i ∈ [j, n+ 1] indicates a possible position in decoding.\nDifferent from the existing methods (Zhuo et al., 2016; Sohrab and Miwa, 2018) that enumerate all segments starting with word wj with equal importance, we use the probability p (wi|wj) as the confidence of the segment that starts with word wj and ends with word wi, and then all these segments under the probability p (wi|wj) are summed as the segment information of word wj .\nHSegj = n∑ i=j p (wi|wj)hpj,i\nhpj,i = [h Bdy j ;h Bdy i ;h Bdy i − h Bdy j ;h Bdy i h Bdy j ]\n(10)\nwhere hpj,i is the representation of the segment that starts with word wj and ends with word wi, and is element-wise product."
    }, {
      "heading" : "3.3 Type Module",
      "text" : "For the Type Module, we use the same network structure as in the NER Module. Given the shared input X =< x1, x2, · · · , xn >, BiLSTM is used to extract distinct contextual type information HType =< hType1 , h Type 2 , · · · , h Type n >, and then CRF is used to tag type labels."
    }, {
      "heading" : "3.4 Interaction Mechanism",
      "text" : "As discussed in Section 1, the boundary information and type information can mutually reinforce each other. We first follow (Cui and Zhang, 2019; Qin et al., 2021) and use a self-attention mechanism over each sub-task labels to obtain the explicit label representations. Then, we concatenate these representations and contextual information of corresponding sub-tasks to get label-enhanced contextual information. For the ith label-enhanced boundary contextual representation hB−Ei , we first use the biaffine attention mechanism (Dozat and Manning, 2017) to grasp the attention scores between hB−Ei and the label-enhanced type contextual information < hT−E1 , h T−E 2 , · · · , hT−En >. The attention scores < αB−Ei,1 , α B−E i,2 , · · · , α B−E i,n > are computed in the same way as in Equation (9). Then, we concatenate the ith label-enhanced boundary representation hB−Ei and the interaction representation rB−Ei by considering the type information as its updated boundary representation:\nrB−Ei = n∑ j=1 αB−Ei,j h T−E j\nh Bdy i = [h B−E i , r B−E i ]\n(11)\nSimilarity, we can obtain the updated type representation h\nType i by considering the boundary infor-\nmation."
    }, {
      "heading" : "3.5 Joint Training",
      "text" : "There are three modules in our proposed MIN model: NER Module, Boundary Module and Type Module. They share the same word representations. Thus, the whole model can be trained with multitask training. During training, we minimize the negative log-probability of the correct sequence of labels in Equation (6) for the NER Module and Type Module, while the cross-entropy loss is used for the Boundary Module:\nLNER = −log ( p ( ŷNER|X )) LType = −log ( p ( ŷType|X\n)) LBdy = − 1\nn n∑ i=1 ŷBdyi logp Bdy i\n(12)\nwhere X represents input sequence, and ŷNER and ŷType represent the correct sequence of labels for the NER Module and Type Module respectively. pBdyi is the probability distribution of the gold label and ŷBdyi is the gold one-hot vector for the\nBoundary Module. Then, the final multitask loss is a weighted sum of the three losses:\nL = LNER + LType + LBdy (13)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we first introduce the datasets, baseline models and implementation details. Then, we present the experimental results on three benchmark datasets. Moreover, an ablation study is also conducted. Finally, we give some insights on further analysis."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate the proposed model on three benchmark NER datasets: CoNLL2003 (Sang and De Meulder, 2003), WNUT2017 (Derczynski et al., 2017) and JNLPBA (Kim et al., 2004).\n• CoNLL2003 - It is collected from Reuters news articles. Four different types of named entities including PER, LOC, ORG and MISC are defined by the CoNLL 2003 NER shared task.\n• WNUT2017 - It is a set of noisy usergenerated text including YouTube comments, StackExchange posts, Twitter text, and Reddit comments. Six types of entities including PER, LOC, Group, Creative work, Corporation and Product are annotated.\n• JNLPBA - It is collected from MEDLINE abstracts. Five types of entities including DNA, RNA, protein, cell line and cell type are annotated.\nTable 1 presents the statistics of these datasets."
    }, {
      "heading" : "4.2 Baseline Models",
      "text" : "We compare the proposed MIN model with several baseline models including sequence labeling-based models and segment-based models.\nThe compared sequence labeling-based models include:\n• CNN-BiLSTM-CRF (Chiu and Nichols, 2016) - This model utilizes CNN to capture character-level word features, and then the character-level and word-level embeddings are concatenated as the input to the BiLSTMCRF network. It is a classical baseline for NER.\nDataset train dev test\nCoNLL2003 #sentences 14,987 3,466 3,684#entities 23,499 5,942 5,648 WNUT2017 #sentences 3,394 1,009 1,287#entities 3,160 1,250 1,589\nJNLPBA #sentences 16,691 1,853 3,855#entities 46,388 4,902 8,657\nThe compared segment-based models include:\n• BiLSTM-Pointer1 (Li et al., 2020a) - This model uses BiLSTM as the encoder and another unidirectional LSTM with pointer networks as the decoder for entity boundary detection. Then, the entity segments generated by the decoder are classified with the Softmax classifier for NER.\n• HSCRF (Ye and Ling, 2018) - This model exploits the weighted sum of word-level within segment to learn segment-level features with Semi-CRFs which is then trained jointly on word-level with the BiLSTM-CRF network.\n1In (Li et al., 2020a), the pointer networks is used for detecting entity boundaries only. We reproduce this work and add a Softmax layer for the NER task.\n• MRC+BERT (Li et al., 2020b) - This model formulates the NER task as a machine reading comprehension task.\n• Biaffine+BERT (Yu et al., 2020b) - This model ranks all the spans in terms of the pairs of start and end tokens in a sentence using a biaffine model."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "Our proposed MIN model is implemented with the PyTorch framework. We use 100-dimensional pre-trained Glove word embeddings 2 (Pennington et al., 2014). The char embeddings is initialized randomly as 25-dimensional vectors. When training the model, both of the embeddings are updated along with other parameters. We use Adam optimizer (Kingma and Ba, 2014) for training with a mini-batch. The initial learning rate is set to 0.01 and will shrunk by 5% after each epoch, dropout rate to 0.5, the hidden layer size to 100, and the gradient clipping to 5. We report the results based on the best performance on the development set. All of our experiments are conducted on the same machine with 8-cores of Intel(R) Xeon(R) E5-1630 CPU@3.70GHz and two Nvidia GeForce-GTX GPU. Following the work in (Ye and Ling, 2018), the maximum segment length for segment information discussed in Section 3.2 is set to 6 for better computational efficiency."
    }, {
      "heading" : "4.4 Experimental Results",
      "text" : "Table 2 shows the experimental results of our proposed MIN model and the baseline models. In Table 2, when compared with models without using any language models or external knowledge, we observe that our MIN model outperforms all the compared baseline models in terms of precision, recall and F1 scores, and achieves 0.57%, 4.77% and 3.26% improvements on F1 scores for the CoNLL2003, WNUT2017 and JNLPBA datasets respectively.\nAmong the compared models, the F1 scores of the BiLSTM-Pointer model are generally lower than other models. This is because it does not utilize the word-level dependencies within a segment and also suffers from the problem on boundary error propagation during boundary detection and type prediction. The CNN-BiLSTM-CRF and\n2http://nlp.stanford.edu/projects/ glove/\nRNN-BiLSTM-CRF models have achieved similar performance results on the three datasets, which perform worse than that of HCRA and HSCRF. The HCRA model uses sentence-level and documentlevel representations to augment the contextualized word representation, while the HSCRF model considers the segment-level and word-level information with multitask training. However, the HCRA model does not consider the segment-level information, and the HSCRF model does not model directly the word-level dependencies within a segment. In addition, all the above models do not share information between the boundary detection and type prediction sub-tasks. Our MIN model has achieved the best performance as it is capable of considering all these information.\nWhen pre-trained language models such as ELMo and BERT are incorporated, all the models have achieved better performance results. In particular, we observe that our MIN model has achieved 0.95%, 3.83% and 2.73% improvements on the F1 scores for the CoNLL2003, WNUT2017 and JNLPBA datasets respectively when compared with the other models. The results are consistent with what have been discussed in models without using any pre-trained language models."
    }, {
      "heading" : "4.5 Ablation Study",
      "text" : "To show the importance of each component in our proposed MIN model, we conduct an ablation experiment on the Boundary Module, Type Module and Interaction Mechanism. As shown in Table 3, we can see that all these components contribute significantly to the effectiveness of our MIN model.\nThe discussion on the effectiveness of each component is given with respect to the three datasets. The Boundary Module improves the F1 scores by 1.13%, 3.58% and 2.1% for CoNLL2003, WNUT2017 and JNLPBA respectively. This is because it not only provides segment-level information for the NER Module but also provides the boundary information for the Type Module. As such, it helps recognize long entities and predict the entity types more accurately.\nThe Type Module improves the F1 scores by 1.02%, 2.81% and 1.42% for CoNLL2003, WNUT2017 and JNLPBA respectively. This is because it provides the type information for the Boundary Module which can help detect entity boundaries more accurately. In addition, it can also help obtain more effective segment information.\nThe Interaction Mechanism has achieved 0.54%, 1.86% and 0.72% improvements on F1 scores for CoNLL2003, WNUT2017 and JNLPBA respectively. As it bridges the gap between the Boundary Module and Type Module for information interaction and sharing, it can help improve the performance of boundary detection and type prediction simultaneously.\nOverall, the different components of the proposed model can work effectively with each other with multitask training and enable the model achieve the state-of-the-art performance for the NER task."
    }, {
      "heading" : "4.6 Performance Against Entity Length",
      "text" : "As our proposed MIN model is capable of recognizing long entities, we compare the performance of our MIN model with RNN-BiLSTM-CRF and HSCRF. Note that the RNN-BiLSTM-CRF model is the base model used in our MIN model. And the HSCRF model also considers the segment-level and word-level information with multitask training. The results are shown in Figure 2. The experiment is conducted on the CoNLL2003 test dataset. We follow the setting in (Ye and Ling, 2018) and group the data according to the number of entities from 1 to ≥ 6 in a sentence. We observe that our MIN model and the HSCRF model consistently\noutperform RNN-BiLSTM-CRF in each group. In particular, the improvement is obvious when the entity length is longer than 4 because both our MIN model and the HSCRF model consider the segmentlevel information. However, our MIN model performs better than the HSCRF model in each group. More specifically, when the entity length is longer than 4, our MIN model has great improvement over HSCRF. This is because the HSCRF model directly uses segment-level features with Semi-CRFs to tag the segments, which ignore word-level dependencies within the segment. In contrast, our MIN model combines segment-level information with word-level dependencies within a segment for the NER task."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have proposed a novel Modularized Interaction Network (MIN) model for the NER task. The proposed MIN model utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task. We have conducted extensive experiments on three NER benchmark datasets. The experimental results have shown that our proposed MIN model has achieved the state-ofthe-art performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research has been supported by the National Key R&D Program of China under Grant No. 2020AAA0106600, the National Natural Science Foundation of China under Grants No. 62062012 and 61976021, and the Ministry of Education (MoE) of Singapore under the Academic Research Fund (AcRF) Tier 1 Grant RG135/18."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Revisiting joint modeling of cross-document entity and event coreference resolution",
      "author" : [ "Shany Barhom", "Vered Shwartz", "Alon Eirew", "Michael Bugert", "Nils Reimers", "Ido Dagan." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computa-",
      "citeRegEx" : "Barhom et al\\.,? 2019",
      "shortCiteRegEx" : "Barhom et al\\.",
      "year" : 2019
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Hierarchicallyrefined label attention network for sequence labeling",
      "author" : [ "Leyang Cui", "Yue Zhang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Cui and Zhang.,? 2019",
      "shortCiteRegEx" : "Cui and Zhang.",
      "year" : 2019
    }, {
      "title" : "Results of the wnut2017 shared task on novel and emerging entity recognition",
      "author" : [ "Leon Derczynski", "Eric Nichols", "Marieke van Erp", "Nut Limsopatham." ],
      "venue" : "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140–147.",
      "citeRegEx" : "Derczynski et al\\.,? 2017",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT (1).",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Discontinuous constituent parsing with pointer networks",
      "author" : [ "Daniel Fernández-González", "Carlos GómezRodrı́guez" ],
      "venue" : "In Proceedings of the AAAI conference on artificial intelligence,",
      "citeRegEx" : "Fernández.González and GómezRodrı́guez.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fernández.González and GómezRodrı́guez.",
      "year" : 2020
    }, {
      "title" : "Entity linking via joint encoding of types, descriptions, and context",
      "author" : [ "Nitish Gupta", "Sameer Singh", "Dan Roth." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2681–2690.",
      "citeRegEx" : "Gupta et al\\.,? 2017",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Introduction to the bio-entity recognition task at jnlpba",
      "author" : [ "Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier." ],
      "venue" : "Proceedings of the international joint workshop on natural language processing in biomedicine and its ap-",
      "citeRegEx" : "Kim et al\\.,? 2004",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2004
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Segmental recurrent neural networks",
      "author" : [ "Lingpeng Kong", "Chris Dyer", "Noah A Smith." ],
      "venue" : "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.",
      "citeRegEx" : "Kong et al\\.,? 2016",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective named entity recognition with boundary-aware bidirectional neural networks",
      "author" : [ "Fei Li", "Zheng Wang", "Siu Cheung Hui", "Lejian Liao", "Dandan Song", "Jing Xu." ],
      "venue" : "Proceedings of The Web Conference 2021.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural named entity boundary detection",
      "author" : [ "Jing Li", "Aixin Sun", "Yukun Ma." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "A unified mrc",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical contextualized representation for named entity recognition",
      "author" : [ "Ying Luo", "Fengshun Xiao", "Hai Zhao." ],
      "venue" : "AAAI, pages 8441–8448.",
      "citeRegEx" : "Luo et al\\.,? 2020",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2020
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A co-interactive transformer for joint slot filling and intent detection",
      "author" : [ "Libo Qin", "Tailu Liu", "Wanxiang Che", "Bingbing Kang", "Sendong Zhao", "Ting Liu." ],
      "venue" : "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Qin et al\\.,? 2021",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "Introduction to the conll-2003 shared task: language-independent named entity recognition",
      "author" : [ "Erik F Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142–147.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Deep exhaustive model for nested named entity recognition",
      "author" : [ "Mohammad Golam Sohrab", "Makoto Miwa." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2843–2849.",
      "citeRegEx" : "Sohrab and Miwa.,? 2018",
      "shortCiteRegEx" : "Sohrab and Miwa.",
      "year" : 2018
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages 2692– 2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards better text understanding and retrieval through kernel entity salience modeling",
      "author" : [ "Chenyan Xiong", "Zhengzhong Liu", "Jamie Callan", "Tie-Yan Liu." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information",
      "citeRegEx" : "Xiong et al\\.,? 2018",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2018
    }, {
      "title" : "Hybrid semimarkov crf for neural sequence labeling",
      "author" : [ "Zhixiu Ye", "Zhen-Hua Ling." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 235–240.",
      "citeRegEx" : "Ye and Ling.,? 2018",
      "shortCiteRegEx" : "Ye and Ling.",
      "year" : 2018
    }, {
      "title" : "Improving multimodal named entity recognition via entity span detection with unified multimodal transformer",
      "author" : [ "Jianfei Yu", "Jing Jiang", "Li Yang", "Rui Xia." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Yu et al\\.,? 2020a",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Named entity recognition as dependency parsing",
      "author" : [ "Juntao Yu", "Bernd Bohnet", "Massimo Poesio." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470– 6476.",
      "citeRegEx" : "Yu et al\\.,? 2020b",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptive co-attention network for named entity recognition in tweets",
      "author" : [ "Qi Zhang", "Jinlan Fu", "Xiaoyu Liu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural question generation from text: A preliminary study",
      "author" : [ "Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou." ],
      "venue" : "National CCF Conference on Natural Language Processing and Chinese Computing, pages 662–671.",
      "citeRegEx" : "Zhou et al\\.,? 2017",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "Segment-level sequence modeling using gated recursive semi-markov conditional random fields",
      "author" : [ "Jingwei Zhuo", "Yong Cao", "Jun Zhu", "Bo Zhang", "Zaiqing Nie." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhuo et al\\.,? 2016",
      "shortCiteRegEx" : "Zhuo et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "It has been widely used for many downstream applications such as relation extraction (Xiong et al., 2018), entity linking (Gupta et al.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : ", 2018), entity linking (Gupta et al., 2017), question generation (Zhou et al.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : ", 2017), question generation (Zhou et al., 2017) and coreference resolution (Barhom et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2017) and coreference resolution (Barhom et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "The first one is sequence labeling-based methods (Lample et al., 2016; Chiu and Nichols, 2016; Luo et al., 2020), in which each word in a sentence is assigned a special label (e.",
      "startOffset" : 49,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "The first one is sequence labeling-based methods (Lample et al., 2016; Chiu and Nichols, 2016; Luo et al., 2020), in which each word in a sentence is assigned a special label (e.",
      "startOffset" : 49,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "The first one is sequence labeling-based methods (Lample et al., 2016; Chiu and Nichols, 2016; Luo et al., 2020), in which each word in a sentence is assigned a special label (e.",
      "startOffset" : 49,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "As such, the sequence labeling-based models which focus only on word-level information do not perform well especially in recognizing long entities (Ye and Ling, 2018).",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "Recently, segment-based methods (Kong et al., 2016; Li et al., 2020b; Yu et al., 2020b; Li et al., 2021) have gained popularity for the NER task.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 27,
      "context" : "Recently, segment-based methods (Kong et al., 2016; Li et al., 2020b; Yu et al., 2020b; Li et al., 2021) have gained popularity for the NER task.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "Recently, segment-based methods (Kong et al., 2016; Li et al., 2020b; Yu et al., 2020b; Li et al., 2021) have gained popularity for the NER task.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "To tackle the issue on recognizing long entities in sequence labeling-based models and the issue of utilizing word-level dependencies within a segment in segment-based models, we incorporate a pointer network (Vinyals et al., 2015) into the Boundary Module as the decoder to capture segment-level information on each word.",
      "startOffset" : 209,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "(Huang et al., 2015) utilized the BiLSTM as an encoder to learn the contextual representation of words, and then Conditional Ran-",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Chiu and Nichols (Chiu and Nichols, 2016) used Convolutional Neural Network (CNN) to capture spelling features, and the character-level and word-level embeddings are concatenated as the input of BiLSTM with CRF network.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "(Lample et al., 2016) proposed RNN-BiLSTM-CRF as an alternative.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "More recently, pretrained language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and BERT (Devlin et al., 2019) have been adopted to further enhance the performance of NER.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "(Kong et al., 2016) used BiLSTM to map arbitrary-length",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "(Zhuo et al., 2016) adopted a gated recursive Convolutional Neural Network instead of BiLSTM to build a pyramid-like structure for extracting segment-level features in a hierarchical way.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 25,
      "context" : "(Ye and Ling, 2018) exploited the weighted sum of word-level within segment to learn segment-level features with Semi-CRFs which was then trained jointly on word-level with the BiLSTM-CRF network.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "(Li et al., 2020a) used a recurrent neural network encoder-decoder framework with a pointer network to detect entity segments.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 27,
      "context" : "(Yu et al., 2020b) ranked all the spans in terms of the pairs of start and end tokens in a sentence using a biaffine model.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "In the NER Module, we adopt the RNN-BiLSTMCRF model (Lample et al., 2016) as our backbone, which consists of three components: word representation, BiLSTM encoder and CRF decoder.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 28,
      "context" : "Instead of directly concatenating these information with hidden representations in the NER module, we follow the previous studies (Zhang et al., 2018; Yu et al., 2020a) to use a gate function to dynamically control the amount of information flowing by infusing the expedient part while excluding the irrelevant part.",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 26,
      "context" : "Instead of directly concatenating these information with hidden representations in the NER module, we follow the previous studies (Zhang et al., 2018; Yu et al., 2020a) to use a gate function to dynamically control the amount of information flowing by infusing the expedient part while excluding the irrelevant part.",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "CRF Decoder CRF has been widely used in the state-of-the-art NER models (Chiu and Nichols, 2016; Lample et al., 2016) to model tagging decisions when considering strong connections between output tags.",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "CRF Decoder CRF has been widely used in the state-of-the-art NER models (Chiu and Nichols, 2016; Lample et al., 2016) to model tagging decisions when considering strong connections between output tags.",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "And inspired by BDRYBOT (Li et al., 2020a), a recurrent neural network encoder-decoder framework with a pointer network is used to detect entity segments for segment information.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "After that, we use the biaffine attention mechanism (Dozat and Manning, 2017) to generate a feature representation for each possible boundary position i at time step j, and the Softmax function is used to obtain the probability of word wi for determining an entity segment that starts with",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 30,
      "context" : "Different from the existing methods (Zhuo et al., 2016; Sohrab and Miwa, 2018) that enumerate all",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "Different from the existing methods (Zhuo et al., 2016; Sohrab and Miwa, 2018) that enumerate all",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "We first follow (Cui and Zhang, 2019; Qin et al., 2021) and use a self-attention mechanism",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "We first follow (Cui and Zhang, 2019; Qin et al., 2021) and use a self-attention mechanism",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "For the ith label-enhanced boundary contextual representation hB−E i , we first use the biaffine attention mechanism (Dozat and Manning, 2017) to grasp the attention scores between hB−E i and the label-enhanced type contextual information < hT−E 1 , h T−E 2 , · · · , hT−E n >.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "We evaluate the proposed model on three benchmark NER datasets: CoNLL2003 (Sang and De Meulder, 2003), WNUT2017 (Derczynski et al., 2017) and JNLPBA (Kim et al.",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "• CNN-BiLSTM-CRF (Chiu and Nichols, 2016) - This model utilizes CNN to capture character-level word features, and then the character-level and word-level embeddings are concatenated as the input to the BiLSTMCRF network.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "• RNN-BiLSTM-CRF (Lample et al., 2016) This model uses RNN instead of CNN in CNN-BiLSTM-CRF.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "• ELMo (Peters et al., 2018) - This model uses a deep bidirectional language model to learn contextualized word representation on a large text corpus, which is then fed into BiLSTMCRF for NER.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "• Flair (Akbik et al., 2018) - This model uses BiLSTM-CRF with character-level contextu-",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "• BERT (Devlin et al., 2019) - This model learns contextualized word representation based on a bidirectional Transformer, which is then fed into BiLSTM-CRF for NER.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "• HCRA (Luo et al., 2020) - This model uses sentence-level and document-level representations to augment the contextualized representation based on a funnel-shaped CNN with BiLSTM-CRF for NER.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "• BiLSTM-Pointer1 (Li et al., 2020a) - This model uses BiLSTM as the encoder and another unidirectional LSTM with pointer networks as the decoder for entity boundary detection.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "• HSCRF (Ye and Ling, 2018) - This model exploits the weighted sum of word-level within segment to learn segment-level features with Semi-CRFs which is then trained jointly on word-level with the BiLSTM-CRF network.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "In (Li et al., 2020a), the pointer networks is used for detecting entity boundaries only.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "• Biaffine+BERT (Yu et al., 2020b) - This model ranks all the spans in terms of the pairs of start and end tokens in a sentence using a biaffine model.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "We use 100-dimensional pre-trained Glove word embeddings 2 (Pennington et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "We use Adam optimizer (Kingma and Ba, 2014) for training with a mini-batch.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "Following the work in (Ye and Ling, 2018), the maximum segment length for segment information discussed in Section 3.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "We follow the setting in (Ye and Ling, 2018) and group the data according to the number of entities from 1 to ≥ 6 in a sentence.",
      "startOffset" : 25,
      "endOffset" : 44
    } ],
    "year" : 2021,
    "abstractText" : "Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks. The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two subtasks to mutually reinforce each other by sharing their information. In this paper, we propose a novel Modularized Interaction Network (MIN) model which utilizes both segmentlevel information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task. We have conducted extensive experiments based on three NER benchmark datasets. The performance results have shown that the proposed MIN model has outperformed the current stateof-the-art models.",
    "creator" : "LaTeX with hyperref"
  }
}