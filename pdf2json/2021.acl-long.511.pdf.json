{
  "name" : "2021.acl-long.511.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations",
    "authors" : [ "Pierre Colombo", "Pablo Piantanida", "Chloé Clavel" ],
    "emails" : [ "pierre.colombo@ibm.com", "chloe.clavel@telecom-paris.fr", "pablo.piantanida@centralesupelec.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6539–6550\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6539"
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning disentangled representations hold a central place to build rich embeddings of highdimensional data. For a representation to be disentangled implies that it factorizes some latent cause\nor causes of variation as formulated by (Bengio et al., 2013). For example, if there are two causes for the transformations in the data that do not generally happen together and are statistically distinguishable (e.g., factors occur independently), a maximally disentangled representation is expected to present a sparse structure that separates those causes. Disentangled representations have been shown to be useful for a large variety of data, such as video (Hsieh et al., 2018), image (Sanchez et al., 2019), text (John et al., 2018), audio (Hung et al., 2018), among others, and applied to many different tasks, e.g., robust and fair classification (Elazar and Goldberg, 2018), visual reasoning (van Steenkiste et al., 2019), style transfer (Fu et al., 2017), conditional generation (Denton et al., 2017; Burgess et al., 2018), few shot learning (Kumar Verma et al., 2018), among others.\nIn this work, we focus our attention on learning disentangled representations for text, as it remains overlooked by (John et al., 2018). Perhaps, one of the most popular applications of disentanglement in textual data is fair classification (Elazar and Goldberg, 2018; Barrett et al., 2019) and sentence generation tasks such as style transfer (John et al., 2018) or conditional sentence generation (Cheng et al., 2020b). For fair classification, perfectly disentangled latent representations can be used to ensure fairness as the decisions are taken based on representations which are statistically independent from–or at least carrying limited information about–the protected attributes. However, there exists a trade-offs between full disentangled representations and performances on the target task, as shown by (Feutry et al., 2018), among others. For sequence generation and in particular, for style transfer, learning disentangled representations aim at allowing an easier transfer of the desired style. To the best of our knowledge, a depth study of the relationship between disentangled representa-\ntions based either on adversarial losses solely or on vCLUB − S and quality of the generated sentences remains overlooked. Most of the previous studies have been focusing on either trade-offs between metrics computed on the generated sentences (Tikhonov et al., 2019) or performance evaluation of the disentanglement as part of (or convoluted with) more complex modules. This enhances the need to provide a fair evaluation of disentanglement methods by isolating their individual contributions (Yamshchikov et al., 2019; Cheng et al., 2020b). Methods to enforce disentangled representations can be grouped into two different categories. The first category relies on an adversarial term in the training objective that aims at ensuring that sensitive attribute values (e.g. race, sex, style) as statistically independent as possible from the encoded latent representation. Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown that even though the adversary teacher seems to be performing remarkably well during training, after the training phase, a fair amount of information about the sensitive attributes still remains, and can be extracted from the encoded representation. The second category aim at minimising Mutual Information (MI) between encoded latent representation and the sensitive attribute values, i.e., without resorting to an adversarial discriminator. MI acts as an universal measure of dependence since it captures non-linear and statistical dependencies of high orders between the involved quantities (Kinney and Atwal, 2014). However, estimating MI has been a long-standing challenge, in particular when dealing with high-dimensional data (Paninski, 2003; Pichler et al., 2020). Recent methods rely on variational upper bounds. For instance, (Cheng et al., 2020b) study vCLUB-S (Cheng et al., 2020a) for sentence generation tasks. Although this approach improves on previous state-of-the-art methods, it does not allow to fine-tuning of the desired degree of disentanglement, i.e., it enforces light or strong levels of disentanglement where only few features relevant to the input sentence remain (see Feutry et al. (2018) for further discussion)."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "We develop new tools to build disentangled textual representations and evaluate them on fair classifi-\ncation and two sentence generation tasks, namely, style transfer and conditional sentence generation. Our main contributions are summarized below:\n• A novel objective to train disentangled representations from attributes. To overcome some of the limitations of both adversarial losses and vCLUB-S we derive a novel upper bound to the MI which aims at correcting the approximation error via either the Kullback-Leibler (Ali and Silvey, 1966) or Renyi (Rényi et al., 1961) divergences. This correction terms appears to be a key feature to fine-tuning the degree of disentanglement compared to vCLUB-S.\n• Applications and numerical results. First, we demonstrate that the aforementioned surrogate is better suited than the widely used adversarial losses as well as vCLUB-S as it can provide better disentangled textual representations while allowing fine-tuning of the desired degree of disentanglement. In particular, we show that our method offers a better accuracy versus disentanglement trade-offs for fair classification tasks. We additionally demonstrate that our surrogate outperforms both methods when learning disentangled representations for style transfer and conditional sentence generation while not suffering (or degenerating) when the number of classes is greater than two, which is an apparent limitation of adversarial training. By isolating the disentanglement module, we identify and report existing tradeoffs between different degree of disentanglement and quality of generated sentences. The later includes content preservation between input and generated sentences and accuracy on the generated style."
    }, {
      "heading" : "2 Main Definitions and Related Works",
      "text" : "We introduce notations, tasks, and closely related work. Consider a training set D = {(xi, yi)}ni=1 of n sentences xi ∈ X paired with attribute values yi ∈ Y ≡ {1, . . . , |Y|} which indicates a discrete attribute to be disentangled from the resulting representations. We study the following scenarios:\nDisentangled representations. Learning disentangled representations consists in learning a model M : X → Rd that maps feature inputs X to a vector of dimension d that retains as much as possible information of the original content from the input\nsentence but as little as possible about the undesired attribute Y . In this framework, content is defined as any relevant information present in X that does not depend on Y .\nApplications to binary fair classification. The task of fair classification through disentangled representations aims at building representations that are independent of selective discrete (sensitive) attributes (e.g., gender or race). This task consists in learning a modelM : X → {0, 1} that maps any input x to a label l ∈ {0, 1}. The goal of the learner is to build a predictor that assigns each x to either 0 or 1 “oblivious” of the protected attribute y. Recently, much progress has been made on devising appropriate means of fairness, e.g., (Zemel et al., 2013; Zafar et al., 2017; Mohri et al., 2019). In particular, (Xie et al., 2017; Barrett et al., 2019; Elazar and Goldberg, 2018) approach the problem based on adversarial losses. More precisely, these approaches consist in learning an encoder that maps x into a representation vector hx, a criticCθc which attempts to predict y, and an output classifier fθd used to predict l based on the observed hx. The classifier is said to be fair if there is no statistical information about y that is present in hx (Xie et al., 2017; Elazar and Goldberg, 2018).\nApplications to conditional sentence generation. The task of conditional sentence generation consists in taking an input text containing specific stylistic properties to then generate a realistic (synthetic) text containing potentially different stylistic properties. It requests to learn a modelM : X × Y → X that maps a pair of inputs (x, yt) to a sentence xg, where the outcome sentence should retain as much as possible of the original content from the input sentence while having (potentially a new) attribute yg. Proposed approaches to tackle textual style transfer (Zhang et al., 2020; Xu et al., 2019) can be divided into two main categories. The first category (Prabhumoye et al., 2018; Lample et al., 2018) uses cycle losses based on back translation (Wieting et al., 2017) to ensure that the content is preserved during the transformation. Whereas, the second category look to explicitly separate attributes from the content. This constraint is enforced using either adversarial training (Fu et al., 2017; Hu et al., 2017; Zhang et al., 2018; Yamshchikov et al., 2019) or MI minimisation using vCLUB-S (Cheng et al., 2020b). Traditional adversarial training is based on an encoder that aims to fool the adversary discriminator\nby removing attribute information from the content embedding (Elazar and Goldberg, 2018). As we will observe, the more the representations are disentangled the easier is to transfer the style but at the same time the less the content is preserved. In order to approach the sequence generation tasks, we build on the Style-embedding Model by (John et al., 2018) (StyleEmb) which uses adversarial losses introduced in prior work for these dedicated tasks. During the training phase, the input sentence is fed to a sentence encoder, namely fθe , while the input style is fed to a separated style encoder, namely fsθe . During the inference phase, the desired style–potentially different from the input style–is provided as input along with the input sentence."
    }, {
      "heading" : "3 Model and Training Objective",
      "text" : "This section describes the proposed approach to learn disentangled representations. We first review MI along with the model overview and then, we derive the variational bound we will use, and discuss connections with adversarial losses."
    }, {
      "heading" : "3.1 Model Overview",
      "text" : "The MI is a key concept in information theory for measuring high-order statistical dependencies between random quantities. Given two random variables Z and Y , the MI is defined by\nI(Z;Y ) = EZY [ log pZY (Z, Y )\npZ(Z)pY (Y )\n] , (1)\nwhere pZY is the joint probability density function (pdf) of the random variables (Z, Y ), with pZ and pY representing the respective marginal pdfs. MI is related to entropy h(Y ) and conditional entropy h(Y |Z) as follows:\nI(Z;Y ) = h(Y )− h(Y |Z). (2) Our models for fair classification and sequence generation share a similar structure. These rely on an encoder that takes as input a random sentence X and maps it to a random representation Z using a deep encoder denoted by fθe . Then, classification and sentence generation are performed using either a classifier or an auto-regressive decoder denoted by fθd . We aim at minimizing MI between the latent code represented by the Random Variable (RV) Z = fθe(X) and the desired attribute represented by the RV Y . The objective of interest L(fθe) is defined as:\nL(fθe) ≡ Ldown.(fθe)︸ ︷︷ ︸ downstream task +λ · I(fθe(X);Y )︸ ︷︷ ︸ disentangled , (3)\nwhere Ldown. represents a downstream specific (target task) loss and λ is a meta-parameter that controls the sensitive trade-off between disentanglement (i.e., minimizing MI) and success in the downstream task (i.e., minimizing the target loss). In Sec. 5, we illustrate theses different trade-offs.\nApplications to fair classification and sentence generation. For fair classification, we follow standard practices and optimize the cross-entropy between prediction and ground-truth labels. In the sentence generation task Ldown. represents the negative log-likelihood between individual tokens."
    }, {
      "heading" : "3.2 A Novel Upper Bound on MI",
      "text" : "Estimating the MI is a long-standing challenge as the exact computation (Paninski, 2003) is only tractable for discrete variables, or for a limited family of problems where the underlying datadistribution satisfies smoothing properties, see recent work by (Pichler et al., 2020). Different from previous approaches leading to variational lower bounds (Belghazi et al., 2018; Hjelm et al., 2018; Oord et al., 2018), in this paper we derive an estimator based on a variational upper bound to the MI which control the approximation error based on the Kullback-Leibler and the Renyi divergences (Daudel et al., 2020).\nTheorem 1 (Variational upper bound on MI) Let (Z, Y ) be an arbitrary pair of RVs with (Z, Y ) ∼ pZY according to some underlying pdf, and let q Ŷ |Z be a conditional variational distribution on the attributes satisfying pZY pZ · qŶ |Z , i.e., absolutely continuous. Then, we have that\nI(Z;Y ) ≤ EY [ − log ∫ q Ŷ |Z(Y |z)pZ(z)dz ] +\nEY Z [ log q Ŷ |Z(Y |Z) ] + KL ( pZY ‖pZ · qŶ |Z ) ,\n(4)\nwhere KL ( pZY ‖pZ · qŶ |Z ) denotes the KL divergence. Similarly, we have that for any α > 1,\nI(Z;Y ) ≤ EY [ − log ∫ q Ŷ |Z(Y |z)pZ(z)dz ] +\nEY Z [ log q Ŷ |Z(Y |Z) ] +Dα ( pZY ‖pZ · qŶ |Z ) ,\n(5)\nwhere (α − 1)Dα ( pZY ‖pZ · qŶ |Z ) = logEZY [ Rα−1(Z, Y )] denotes the Renyi divergence and R(z, y) = pY |Z(y|z) q Ŷ |Z(y|z) , for (z, y) ∈ Supp(pZY ).\nProof: The upper bound on H(Y ) is a direct application of the the (Donsker and Varadhan, 1985) representation of KL divergence while the lower bound on H(Y |Z) follows from the monotonicity property of the function: α 7→ Dα ( pZY ‖pZ ·qŶ |Z ) . Further details are relegated to Appendix A. Remark: It is worth to emphasise that the KL divergence in (4) and Renyi divergence in (5) control the approximation error between the exact entropy and its corresponding bound.\nFrom theoretical bounds to trainable surrogates to minimize MI: It is easy to check that the inequalities in (Eq. 4) and (Eq. 5) are tight provided that pZY ≡ pZ · qŶ |Z almost surely for some adequate choice of the variational distribution. However, the evaluation of these bounds requires to obtain an estimate of the density-ratio R(z, y). Density-ratio estimation has been widely studied in the literature (see (Sugiyama et al., 2012) and references therein) and confidence bounds has been reported by (Kpotufe, 2017) under some smoothing assumption on underlying data-distribution pZY . In this work, we will estimate this ratio by using a critic CθR which is trained to differentiate between a balanced dataset of positive i.i.d samples coming from pZY and negative i.i.d samples coming from q Ŷ |Z · pZ . Then, for any pair (z, y), the densityratio can be estimated by R(z, y) ≈ σ(CθR (z,y))1−σ(CθR (z,y)) , where σ(·) indicates the sigmoid function and CθR(z, y) is the unnormalized output of the critic. It is worth to mention that after estimating this ratio, the previous upper bounds may not be strict bounds so we will refer them as surrogates."
    }, {
      "heading" : "3.3 Comparison to existing methods",
      "text" : "Adversarial approaches: In order to enhance our understanding of why the proposed approach based on the minimization of the MI using our variational upper bound in Th. 1 may lead to a better training objective than previous adversarial losses, we discuss below the explicit relationship between MI and cross-entropy loss. Let Y ∈ Y denote a random attribute and let Z be a possibly highdimensional representation that needs to be disentangled from Y . Then,\nI(Z;Y ) ≥H(Y )− EY Z [ log q Ŷ |Z(Y |Z) ]\n=Const− CE(Ŷ |Z), (6)\nwhere CE(Ŷ |Z) denotes the cross-entropy corresponding to the adversarial discriminator q\nŶ |Z , not-\ning that Y comes from an unknown distribution on which we have no influence H(Y ) is an unknown constant, and using that the approximation error: KL ( qZY ‖qŶ |Z ·pZ ) = CE(Ŷ |Z)−H(Y |Z). Eq. 6 shows that the cross-entropy loss leads to a lower bound (up to a constant) on the MI. Although the cross-entropy can lead to good estimates of the conditional entropy, the adversarial approaches for classification and sequence generation by (Barrett et al., 2019; John et al., 2018) which consists in maximizing the cross-entropy, induces a degeneracy (unbounded loss) as λ increases in the underlying optimization problem. As we will observe in next section, our variational upper bound in Th. 1 can overcome this issue, in particular for |Y| > 2. vCLUB-S: Different from our method, Cheng et al. (2020a) introduce IvCLUB which is an upper bound on MI defined by\nIvCLUB(Y ;Z) =EY Z [log pY |Z(Y |Z)] − EY EZ [log pY |Z(Y |Z)]. (7)\nIt would be worth to mention that this bound follows a similar approach to the previously introduced bound in (Feutry et al., 2018)."
    }, {
      "heading" : "4 Experimental Setting",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Fair classification task. We follow the experimental protocol of (Elazar and Goldberg, 2018). The main task consists in predicting a binary label representing either the sentiment (positive/negative) or the mention. The mention task aims at predicting if a tweet is conversational. Here the considered protected attribute is the race. The dataset has been automatically constructed from DIAL corpus (Blodgett et al., 2016) which contained race annotations over 50 Million of tweets. Sentiment tweets are extracted using a list of predefined emojis and mentions are identified using @mentions tokens. The final dataset contains 160k tweets for the training and two splits of 10K tweets for validation and testing. Splits are balanced such that the random estimator is likely to achieve 50% accuracy. Style Transfer For our sentence generation task, we conduct experiments on three different datasets extracted from restaurant reviews in Yelp. The first dataset, referred to as SYelp, contains 444101, 63483, and 126670 labelled short reviews (at most 20 words) for train, validation, and test, respectively. For each review a binary label is assigned\ndepending on its polarity. Following (Lample et al., 2018), we use a second version of Yelp, referred to as FYelp, with longer reviews (at most 70 words). It contains five coarse-grained restaurant category labels (e.g., Asian, American, Mexican, Bars and Dessert). The multi-category FYelp is used to access the generalization capabilities of our methods to a multi-class scenario."
    }, {
      "heading" : "4.2 Metrics for Performance Evaluation",
      "text" : "Efficiency measure of the disentanglement methods. (Barrett et al., 2019) report that offline classifiers (post training) outperform clearly adversarial discriminators. We will re-training a classifier on the latent representation learnt by the model and we will report its accuracy.\nMeasure of performance within the fair classification task. In the fair classification task we aim at maximizing accuracy on the target task and so we will report the corresponding accuracy.\nMeasure of performance within sentence generation tasks. Sentences generated by the model are expected to be fluent, to preserve the input content and to contain the desired style. For style transfer, the desired style is different from the input style while for conditional sentence generation, both input and output styles should be similar. Nevertheless, automatic evaluation of generative models for text is still an open problem. We measure the style of the output sentence by using a fastText classifier (Joulin et al., 2016b). For content preservation, we follow (John et al., 2018) and compute both: (i) the cosine measure between source and generated sentence embeddings, which are the concatenation of min, max, and mean of word embedding (sentiment words removed), and (ii) the BLEU score between generated text and the input using SACREBLEU from (Post, 2018). Motivated by previous work, we evaluate the fluency of the language with the perplexity given by a GPT-2 (Radford et al., 2019) pretrained model performing fine-tuning on the training corpus. We choose to report the logperplexity since we believe it can better reflects the uncertainty of the language model (a small variation in the model loss would induce a large change in the perplexity due to the exponential term). Besides the automatic evaluation, we further test our disentangled representation effectiveness by human evaluation results are presented in Tab. 1. Conventions and abbreviations. Adv refers to a model trained using the adversarial loss;\nvCLUB-S, KL refers to a model trained using the vCLUB-S and KL surrogate (see Eq. 14) respectively; and Dα refers to a model trained based on the α-Renyi surrogate (Eq. 15), for α ∈ {1.3, 1.5, 1.8}."
    }, {
      "heading" : "5 Numerical Results",
      "text" : "In this section, we present our results on the fair classification and binary sequence generation tasks, see Ssec. 5.1 and Ssec. 5.2, respectively. We additionally show that our variational surrogates to the MI–contrarily to adversarial losses–do not suffer in multi-class scenarios (see Ssec. 5.3)."
    }, {
      "heading" : "5.1 Applications to Fairness",
      "text" : "Upper bound on performances. We first examine how much of the protected attribute we can be recovered from an unfair classifier (i.e., trained without adversarial loss) and how well does such classifier perform. Results are reported in Fig. 1. We observe that we achieve similar scores than the ones reported in previous studies (Barrett et al., 2019; Elazar and Goldberg, 2018). This experiment shows that, when training to solve the main task, the classifier learns information about the protected attribute, i.e., the attacker’s accuracy is better than random guessing. In the following, we compare the different proposed methods to disentangle representations and obtain a fairer classifier.\nMethods comparisons. Fig. 1 shows the results of the different models and illustrates the trade-offs between disentangled representations and the target task accuracy. Results are reported on the testset for both sentiment and mention tasks when race is the protected. We observe that the classifier trained with an adversarial loss degenerates for λ > 5 since the adversarial term in Eq. 3 is influencing much the global gradient than the downstream term (i.e., cross-entropy loss between predicted and golden distribution). Remarkably, both models trained to minimize either the KL or the Renyi surrogate do not suffer much from the aforementioned multiclass problem. For both tasks, we observe that the KL and the Renyi surrogates can offer better disentangled representations than those induced by adversarial approaches. In this task, both the KL and Renyi achieve perfect disentangled representations (i.e., random guessing accuracy on protected attributes) with a 5% drop in the accuracy of the target task, when perfectly masking the protected attributes. As a matter of fact, we ob-\nserve that vCLUB-S provides only two regimes: either a “light” protection (attacker accuracy around 60%), with almost no loss in task accuracy (λ < 1), or a strong protection (attacker accuracy around 50%), where a few features relevant to the target task remain.1 On the sentiment task, we can draw similar conclusions. However, the Renyi’s surrogate achieves slightly better-disentangled representations. Overall, we can observe that our proposed surrogate enables good control of the degree of disentangling. Additionally, we do not observe a degenerated behaviour–as it is the case with adversarial losses–when λ increases. Furthermore, our surrogate allows simultaneously better disentangled representations while preserving the accuracy of the target task."
    }, {
      "heading" : "5.2 Applications to binary polarity transfer",
      "text" : "In the previous section, we have shown that the proposed surrogates do not suffer from limitations of adversarial losses and allow to achieve better disentangled representations than existing methods relying on vCLUB-S. Disentanglement modules are a core block for a large number of both style transfer and conditional sentence generation algorithms (Tikhonov et al., 2019; Yamshchikov et al., 2019; Fu et al., 2017) that place explicit constraints to force disentangled representations. First, we assess the disentanglement quality and the control over desired level of disentanglement while changing the downstream term, which for the sentence generation task is the cross-entropy loss on individual token. Then, we exhibit the existing trade-offs between quality of generated sentences, measured by the metric introduced in Ssec. 4.2, and the resulting degree of disentanglement. The results are presented for SYelp"
    }, {
      "heading" : "5.2.1 Evaluating disentanglement",
      "text" : "Fig. 2a shows the adversary accuracy of the different methods as a function of λ. Similarly to the fair classification task, a fair amount of information can be recovered from the embedding learnt with adversarial loss. In addition, we observe a clear degradation of its performance for values λ > 1. In this setting, the Renyi surrogates achieves consistently better results in terms of disentanglement than the one minimizing the KL surrogate. The curve for Renyi’s surrogates shows that exploring different values of λ allows good control of the\n1This phenomenon is also reported in (Feutry et al., 2018) on a picture anonymization task.\ndisentanglement degree. Renyi surrogate generalizes well for sentence generation. Similarly to the fairness task vCLUB-S only offers two regimes: \"light\" disentanglement with very little polarity transfer and \"strong\" disentanglement."
    }, {
      "heading" : "5.2.2 Disentanglement in Polarity Transfer",
      "text" : "The quality of generated sentences are evaluated using the fluency (see Fig. 3c ), the content preservation (see Fig. 3a), additional results using a cosine similarity are given in Appendix D, and polarity accuracy (see Fig. 3b ). For style transfer, and for all models, we observe trade-offs between disentanglement and content preservation (measured by BLEU) and between fluency and disentanglement. Learning disentangled representations leads to poorer content preservation. As a matter of fact, similar conclusions can be drawn while measuring content with the cosine similarity (see Appendix D). For polarity accuracy, in non-degenerated cases (see below), we observe that the model is able to better transfer the sentiment in presence of disentangled representations. Transferring style is easier with disentangled representations, however there is no free lunch here since disentangling also re-\nmoves important information about the content. It is worth noting that even in the \"strong\" disentanglement regime vCLUB-S struggles to transfer the polarity (accuracy of 40% for λ ∈ {1, 2, 10, 15}) where other models reach 80%. It is worth noting that similar conclusions hold for two different sentence generation tasks: style transfer and conditional generation, which tends to validate the current line of work that formulates text generation as generic text-to-text (Raffel et al., 2019). Quality of generated sentences. Examples of generated sentences are given in Tab. 2 , providing qualitative examples that illustrate the previously observed trade-offs. The adversarial loss degenerates for values λ ≥ 5 and a stuttering phenomenon appears (Holtzman et al., 2019). Tab. 1 gathers results of human evaluation and show that our surrogates can better disentangle style while preserving more content than available methods."
    }, {
      "heading" : "5.3 Adversarial Loss Fails to Disentangle",
      "text" : "when |Y| ≥ 3\nIn Fig. 2b we report the adversary accuracy of our different methods for the values of λ using FYelp\ndataset with category label. In the binary setting for λ ≤ 1, models using adversarial loss can learn disentangled representations while in the multi-class setting, the adversarial loss degenerates for small values of λ (i.e sentences are no longer fluent as shown by the increase in perplexity in Fig. 4c). Minimizing MI based on our surrogates seems to mitigate the problem and offer a better control of the disentanglement degree for various values of λ than vCLUB − S. Further results are gathered in Appendix G."
    }, {
      "heading" : "6 Summary and Concluding Remarks",
      "text" : "We devised a new alternative method to adversarial losses capable of learning disentangled textual representation. Our method does not require adversarial training and hence, it does not suffer in presence of multi-class setups. A key feature of this method is to account for the approximation error incurred when bounding the mutual information. Experiments show better trade-offs than both adversarial training and vCLUB-S on two fair classification tasks and demonstrate the efficiency to learn disentangled representations for sequence generation. As a matter of fact, there is no free-lunch for sen-\ntence generation tasks: although transferring style is easier with disentangled representations, it also removes important information about the content. The proposed method can replace the adversary in any kind of algorithms (Tikhonov et al., 2019; Fu et al., 2017) with no modifications. Future work includes testing with other type of labels such as dialog act (Chapuis et al., 2020; Colombo et al., 2020), emotions (Witon et al., 2018), opinion (Garcia et al., 2019) or speaker’s stance and confidence (Dinkar et al., 2020). Since it allows more finegrained control over the amount of disentanglement, we expect it to be easier to tune when combined with more complex models."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "The authors would like to thanks Georg Pichler for the thorough reading. The work of Prof. Pablo Piantanida was supported by the European Commission’s Marie Sklodowska-Curie Actions (MSCA), through the Marie Sklodowska-Curie IF (H2020MSCAIF-2017-EF-797805). The PhD of Pierre is fully founded by IBM GBS France in collaboration with Telecom Paris."
    } ],
    "references" : [ {
      "title" : "A general class of coefficients of divergence of one distribution from another",
      "author" : [ "Syed Mumtaz Ali", "Samuel D Silvey." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodological), 28(1):131–142.",
      "citeRegEx" : "Ali and Silvey.,? 1966",
      "shortCiteRegEx" : "Ali and Silvey.",
      "year" : 1966
    }, {
      "title" : "Generating sentences from disentangled syntactic and semantic spaces",
      "author" : [ "Yu Bao", "Hao Zhou", "Shujian Huang", "Lei Li", "Lili Mou", "Olga Vechtomova", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1907.05789.",
      "citeRegEx" : "Bao et al\\.,? 2019",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial removal of demographic attributes revisited",
      "author" : [ "Maria Barrett", "Yova Kementchedjhieva", "Yanai Elazar", "Desmond Elliott", "Anders Søgaard." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Barrett et al\\.,? 2019",
      "shortCiteRegEx" : "Barrett et al\\.",
      "year" : 2019
    }, {
      "title" : "Mine: mutual information neural estimation",
      "author" : [ "Mohamed Ishmael Belghazi", "Aristide Baratin", "Sai Rajeswar", "Sherjil Ozair", "Yoshua Bengio", "Aaron Courville", "R Devon Hjelm." ],
      "venue" : "arXiv preprint arXiv:1801.04062.",
      "citeRegEx" : "Belghazi et al\\.,? 2018",
      "shortCiteRegEx" : "Belghazi et al\\.",
      "year" : 2018
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Demographic dialectal variation in social media: A case study of African-American English",
      "author" : [ "Su Lin Blodgett", "Lisa Green", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Blodgett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding disentangling in β-vae",
      "author" : [ "Christopher P Burgess", "Irina Higgins", "Arka Pal", "Loic Matthey", "Nick Watters", "Guillaume Desjardins", "Alexander Lerchner." ],
      "venue" : "arXiv preprint arXiv:1804.03599.",
      "citeRegEx" : "Burgess et al\\.,? 2018",
      "shortCiteRegEx" : "Burgess et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical pre-training for sequence labelling in spoken dialog",
      "author" : [ "Emile Chapuis", "Pierre Colombo", "Matteo Manica", "Matthieu Labeau", "Chloé Clavel." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Chapuis et al\\.,? 2020",
      "shortCiteRegEx" : "Chapuis et al\\.",
      "year" : 2020
    }, {
      "title" : "Club: A contrastive log-ratio upper bound of mutual information",
      "author" : [ "Pengyu Cheng", "Weituo Hao", "Shuyang Dai", "Jiachang Liu", "Zhe Gan", "Lawrence Carin." ],
      "venue" : "International Conference on Machine Learning, pages 1779–1788. PMLR.",
      "citeRegEx" : "Cheng et al\\.,? 2020a",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving disentangled text representation learning with information-theoretic guidance",
      "author" : [ "Pengyu Cheng", "Martin Renqiang Min", "Dinghan Shen", "Christopher Malon", "Yizhe Zhang", "Yitong Li", "Lawrence Carin." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cheng et al\\.,? 2020b",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Guiding attention in sequence-to-sequence models for dialogue act prediction",
      "author" : [ "Pierre Colombo", "Emile Chapuis", "Matteo Manica", "Emmanuel Vignon", "Giovanna Varni", "Chloe Clavel." ],
      "venue" : "AAAI, pages 7594–7601.",
      "citeRegEx" : "Colombo et al\\.,? 2020",
      "shortCiteRegEx" : "Colombo et al\\.",
      "year" : 2020
    }, {
      "title" : "Affect-driven dialog generation",
      "author" : [ "Pierre Colombo", "Wojciech Witon", "Ashutosh Modi", "James Kennedy", "Mubbasir Kapadia." ],
      "venue" : "arXiv preprint arXiv:1904.02793.",
      "citeRegEx" : "Colombo et al\\.,? 2019",
      "shortCiteRegEx" : "Colombo et al\\.",
      "year" : 2019
    }, {
      "title" : "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)",
      "author" : [ "Thomas M. Cover", "Joy A. Thomas." ],
      "venue" : "Wiley-Interscience, USA.",
      "citeRegEx" : "Cover and Thomas.,? 2006",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 2006
    }, {
      "title" : "Infinite-dimensional gradient-based descent for alpha-divergence minimisation",
      "author" : [ "Kamélia Daudel", "Randal Douc", "François Portier." ],
      "venue" : "Working paper or preprint.",
      "citeRegEx" : "Daudel et al\\.,? 2020",
      "shortCiteRegEx" : "Daudel et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised learning of disentangled representations from video",
      "author" : [ "Emily L Denton" ],
      "venue" : "Advances in neural information processing systems, pages 4414–4423.",
      "citeRegEx" : "Denton,? 2017",
      "shortCiteRegEx" : "Denton",
      "year" : 2017
    }, {
      "title" : "The importance of fillers for text representations of speech transcripts",
      "author" : [ "Tanvi Dinkar", "Pierre Colombo", "Matthieu Labeau", "Chloé Clavel." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,",
      "citeRegEx" : "Dinkar et al\\.,? 2020",
      "shortCiteRegEx" : "Dinkar et al\\.",
      "year" : 2020
    }, {
      "title" : "Large deviations for stationary gaussian processes",
      "author" : [ "MD Donsker", "SRS Varadhan." ],
      "venue" : "Communications in Mathematical Physics, 97(1-2):187–210.",
      "citeRegEx" : "Donsker and Varadhan.,? 1985",
      "shortCiteRegEx" : "Donsker and Varadhan.",
      "year" : 1985
    }, {
      "title" : "Adversarial removal of demographic attributes from text data",
      "author" : [ "Yanai Elazar", "Yoav Goldberg." ],
      "venue" : "arXiv preprint arXiv:1808.06640.",
      "citeRegEx" : "Elazar and Goldberg.,? 2018",
      "shortCiteRegEx" : "Elazar and Goldberg.",
      "year" : 2018
    }, {
      "title" : "Learning anonymized representations with adversarial neural networks",
      "author" : [ "Clément Feutry", "Pablo Piantanida", "Yoshua Bengio", "Pierre Duhamel" ],
      "venue" : null,
      "citeRegEx" : "Feutry et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Feutry et al\\.",
      "year" : 2018
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "arXiv preprint arXiv:1711.06861.",
      "citeRegEx" : "Fu et al\\.,? 2017",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning deep representations by mutual information estimation and maximization",
      "author" : [ "R Devon Hjelm", "Alex Fedorov", "Samuel LavoieMarchildon", "Karan Grewal", "Phil Bachman", "Adam Trischler", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Hjelm et al\\.,? 2018",
      "shortCiteRegEx" : "Hjelm et al\\.",
      "year" : 2018
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1904.09751.",
      "citeRegEx" : "Holtzman et al\\.,? 2019",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to decompose and disentangle representations for video prediction",
      "author" : [ "Jun-Ting Hsieh", "Bingbin Liu", "De-An Huang", "Li F FeiFei", "Juan Carlos Niebles." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 517–526.",
      "citeRegEx" : "Hsieh et al\\.,? 2018",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2018
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "arXiv preprint arXiv:1703.00955.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning disentangled representations for timber and pitch in music audio",
      "author" : [ "Yun-Ning Hung", "Yi-An Chen", "Yi-Hsuan Yang." ],
      "venue" : "arXiv preprint arXiv:1811.03271.",
      "citeRegEx" : "Hung et al\\.,? 2018",
      "shortCiteRegEx" : "Hung et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised controllable text formalization",
      "author" : [ "Parag Jain", "Abhijit Mishra", "Amar Prakash Azad", "Karthik Sankaranarayanan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6554–6561.",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Heavy-tailed representations, text polarity classification & data augmentation",
      "author" : [ "Hamid Jalalzai", "Pierre Colombo", "Chloé Clavel", "Eric Gaussier", "Giovanna Varni", "Emmanuel Vignon", "Anne Sabourin." ],
      "venue" : "arXiv preprint arXiv:2003.11593.",
      "citeRegEx" : "Jalalzai et al\\.,? 2020",
      "shortCiteRegEx" : "Jalalzai et al\\.",
      "year" : 2020
    }, {
      "title" : "Disentangled representation learning for non-parallel text style transfer",
      "author" : [ "Vineet John", "Lili Mou", "Hareesh Bahuleyan", "Olga Vechtomova." ],
      "venue" : "arXiv preprint arXiv:1808.04339.",
      "citeRegEx" : "John et al\\.,? 2018",
      "shortCiteRegEx" : "John et al\\.",
      "year" : 2018
    }, {
      "title" : "2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Hérve Jégou", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Joulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1607.01759.",
      "citeRegEx" : "Joulin et al\\.,? 2016b",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Equitability, mutual information, and the maximal information coefficient",
      "author" : [ "Justin B Kinney", "Gurinder S Atwal." ],
      "venue" : "Proceedings of the National Academy of Sciences, 111(9):3354–3359.",
      "citeRegEx" : "Kinney and Atwal.,? 2014",
      "shortCiteRegEx" : "Kinney and Atwal.",
      "year" : 2014
    }, {
      "title" : "Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning",
      "author" : [ "Samory Kpotufe." ],
      "venue" : "volume 54 of Proceedings of Machine Learning Research, pages 1320–1328, Fort Lauderdale, FL, USA. PMLR.",
      "citeRegEx" : "Kpotufe.,? 2017",
      "shortCiteRegEx" : "Kpotufe.",
      "year" : 2017
    }, {
      "title" : "Content analysis: An introduction to its methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage publications.",
      "citeRegEx" : "Krippendorff.,? 2018",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2018
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "arXiv preprint arXiv:1804.10959.",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "Generalized zero-shot learning via synthesized examples",
      "author" : [ "Vinay Kumar Verma", "Gundeep Arora", "Ashish Mishra", "Piyush Rai." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4281–4289.",
      "citeRegEx" : "Verma et al\\.,? 2018",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 2018
    }, {
      "title" : "Multiple-attribute text rewriting",
      "author" : [ "Guillaume Lample", "Sandeep Subramanian", "Eric Smith", "Ludovic Denoyer", "Marc’Aurelio Ranzato", "YLan Boureau" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1510.03055.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1804.06437.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Agnostic federated learning",
      "author" : [ "Mehryar Mohri", "Gary Sivek", "Ananda Theertha Suresh." ],
      "venue" : "arXiv preprint arXiv:1902.00146.",
      "citeRegEx" : "Mohri et al\\.,? 2019",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 2019
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "Liam Paninski." ],
      "venue" : "Neural computation, 15(6):1191– 1253.",
      "citeRegEx" : "Paninski.,? 2003",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2003
    }, {
      "title" : "On the estimation of information measures of continuous distributions",
      "author" : [ "Georg Pichler", "Pablo Piantanida", "Günther Koliander" ],
      "venue" : null,
      "citeRegEx" : "Pichler et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pichler et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting bleu scores",
      "author" : [ "Matt Post." ],
      "venue" : "arXiv preprint arXiv:1804.08771.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Style transfer through back-translation",
      "author" : [ "Shrimai Prabhumoye", "Yulia Tsvetkov", "Ruslan Salakhutdinov", "Alan W Black." ],
      "venue" : "arXiv preprint arXiv:1804.09000.",
      "citeRegEx" : "Prabhumoye et al\\.,? 2018",
      "shortCiteRegEx" : "Prabhumoye et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "On measures of entropy and information",
      "author" : [ "Alfréd Rényi" ],
      "venue" : "Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of Califor-",
      "citeRegEx" : "Rényi,? 1961",
      "shortCiteRegEx" : "Rényi",
      "year" : 1961
    }, {
      "title" : "The perceptron: a probabilistic model for information storage and organization in the brain",
      "author" : [ "Frank Rosenblatt." ],
      "venue" : "Psychological review, 65(6):386.",
      "citeRegEx" : "Rosenblatt.,? 1958",
      "shortCiteRegEx" : "Rosenblatt.",
      "year" : 1958
    }, {
      "title" : "Learning disentangled representations via mutual information estimation",
      "author" : [ "Eduardo Hugo Sanchez", "Mathieu Serrurier", "Mathias Ortner." ],
      "venue" : "arXiv preprint arXiv:1912.03915.",
      "citeRegEx" : "Sanchez et al\\.,? 2019",
      "shortCiteRegEx" : "Sanchez et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Are disentangled representations helpful for abstract visual reasoning",
      "author" : [ "Sjoerd van Steenkiste", "Francesco Locatello", "Jürgen Schmidhuber", "Olivier Bachem" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Steenkiste et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Steenkiste et al\\.",
      "year" : 2019
    }, {
      "title" : "Density Ratio Estimation in Machine Learning, 1st edition",
      "author" : [ "Masashi Sugiyama", "Taiji Suzuki", "Takafumi Kanamori." ],
      "venue" : "Cambridge University Press, USA.",
      "citeRegEx" : "Sugiyama et al\\.,? 2012",
      "shortCiteRegEx" : "Sugiyama et al\\.",
      "year" : 2012
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Style transfer for texts: Retrain, report errors, compare with rewrites",
      "author" : [ "Alexey Tikhonov", "Viacheslav Shibaev", "Aleksander Nagaev", "Aigul Nugmanova", "Ivan P Yamshchikov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Tikhonov et al\\.,? 2019",
      "shortCiteRegEx" : "Tikhonov et al\\.",
      "year" : 2019
    }, {
      "title" : "Rényi divergence and kullback-leibler divergence",
      "author" : [ "Tim Van Erven", "Peter Harremos." ],
      "venue" : "IEEE Transactions on Information Theory, 60(7):3797–3820.",
      "citeRegEx" : "Erven and Harremos.,? 2014",
      "shortCiteRegEx" : "Erven and Harremos.",
      "year" : 2014
    }, {
      "title" : "Learning paraphrastic sentence embeddings from back-translated bitext",
      "author" : [ "John Wieting", "Jonathan Mallinson", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:1706.01847.",
      "citeRegEx" : "Wieting et al\\.,? 2017",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2017
    }, {
      "title" : "Disney at IEST 2018: Predicting emotions using an ensemble",
      "author" : [ "Wojciech Witon", "Pierre Colombo", "Ashutosh Modi", "Mubbasir Kapadia." ],
      "venue" : "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,",
      "citeRegEx" : "Witon et al\\.,? 2018",
      "shortCiteRegEx" : "Witon et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "ArXiv, abs/1910.03771.",
      "citeRegEx" : "Scao et al\\.,? 2019",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable invariance through adversarial feature learning",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Yulun Du", "Eduard Hovy", "Graham Neubig." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 585– 596.",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "Empirical evaluation of rectified activations in convolutional network",
      "author" : [ "Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li." ],
      "venue" : "arXiv preprint arXiv:1505.00853.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Formality style transfer with hybrid textual annotations",
      "author" : [ "Ruochen Xu", "Tao Ge", "Furu Wei." ],
      "venue" : "arXiv preprint arXiv:1903.06353.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decomposing textual information for style transfer",
      "author" : [ "Ivan P Yamshchikov", "Viacheslav Shibaev", "Aleksander Nagaev", "Jürgen Jost", "Alexey Tikhonov." ],
      "venue" : "arXiv preprint arXiv:1909.12928.",
      "citeRegEx" : "Yamshchikov et al\\.,? 2019",
      "shortCiteRegEx" : "Yamshchikov et al\\.",
      "year" : 2019
    }, {
      "title" : "Text style transfer via learning style instance supported latent space",
      "author" : [ "Xiaoyuan Yi", "Zhenghao Liu", "Wenhao Li", "Maosong Sun." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3801–3807.",
      "citeRegEx" : "Yi et al\\.,? 2020",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2020
    }, {
      "title" : "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "author" : [ "Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi." ],
      "venue" : "Proceedings of the 26th international",
      "citeRegEx" : "Zafar et al\\.,? 2017",
      "shortCiteRegEx" : "Zafar et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning fair representations",
      "author" : [ "Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork." ],
      "venue" : "volume 28 of Proceedings of Machine Learning Research, pages 325–333, Atlanta, Georgia, USA. PMLR.",
      "citeRegEx" : "Zemel et al\\.,? 2013",
      "shortCiteRegEx" : "Zemel et al\\.",
      "year" : 2013
    }, {
      "title" : "Shaped: Shared-private encoder-decoder for text style adaptation",
      "author" : [ "Ye Zhang", "Nan Ding", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1804.04093.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Parallel data augmentation for formality style transfer",
      "author" : [ "Yi Zhang", "Tao Ge", "Xu Sun." ],
      "venue" : "arXiv preprint arXiv:2005.07522.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE inter-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For a representation to be disentangled implies that it factorizes some latent cause or causes of variation as formulated by (Bengio et al., 2013).",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "Disentangled representations have been shown to be useful for a large variety of data, such as video (Hsieh et al., 2018), image (Sanchez et al.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : ", 2018), audio (Hung et al., 2018), among others, and applied to many different tasks, e.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : ", robust and fair classification (Elazar and Goldberg, 2018), visual reasoning (van Steenkiste et al.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "ditional generation (Denton et al., 2017; Burgess et al., 2018), few shot learning (Kumar Verma et al.",
      "startOffset" : 20,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "In this work, we focus our attention on learning disentangled representations for text, as it remains overlooked by (John et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 19,
      "context" : "Perhaps, one of the most popular applications of disentanglement in textual data is fair classification (Elazar and Goldberg, 2018; Barrett et al., 2019) and sentence generation tasks such as style transfer (John et al.",
      "startOffset" : 104,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Perhaps, one of the most popular applications of disentanglement in textual data is fair classification (Elazar and Goldberg, 2018; Barrett et al., 2019) and sentence generation tasks such as style transfer (John et al.",
      "startOffset" : 104,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and sentence generation tasks such as style transfer (John et al., 2018) or conditional sentence generation (Cheng et al.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : ", 2018) or conditional sentence generation (Cheng et al., 2020b).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "However, there exists a trade-offs between full disentangled representations and performances on the target task, as shown by (Feutry et al., 2018), among others.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 58,
      "context" : "Most of the previous studies have been focusing on either trade-offs between metrics computed on the generated sentences (Tikhonov et al., 2019) or performance evaluation of the disentanglement as part of (or convoluted with) more complex modules.",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 66,
      "context" : "This enhances the need to provide a fair evaluation of disentanglement methods by isolating their individual contributions (Yamshchikov et al., 2019; Cheng et al., 2020b).",
      "startOffset" : 123,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "This enhances the need to provide a fair evaluation of disentanglement methods by isolating their individual contributions (Yamshchikov et al., 2019; Cheng et al., 2020b).",
      "startOffset" : 123,
      "endOffset" : 170
    }, {
      "referenceID" : 29,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 19,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 67,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 27,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 70,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 25,
      "context" : "Interestingly enough, several works (John et al., 2018; Elazar and Goldberg, 2018; Bao et al., 2019; Yi et al., 2020; Jain et al., 2019; Zhang et al., 2018; Hu et al., 2017), Elazar and Goldberg (2018) have recently shown",
      "startOffset" : 36,
      "endOffset" : 173
    }, {
      "referenceID" : 33,
      "context" : "captures non-linear and statistical dependencies of high orders between the involved quantities (Kinney and Atwal, 2014).",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 44,
      "context" : "However, estimating MI has been a long-standing challenge, in particular when dealing with high-dimensional data (Paninski, 2003; Pichler et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 151
    }, {
      "referenceID" : 45,
      "context" : "However, estimating MI has been a long-standing challenge, in particular when dealing with high-dimensional data (Paninski, 2003; Pichler et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "For instance, (Cheng et al., 2020b) study vCLUB-S (Cheng et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2020b) study vCLUB-S (Cheng et al., 2020a) for sentence generation tasks.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "To overcome some of the limitations of both adversarial losses and vCLUB-S we derive a novel upper bound to the MI which aims at correcting the approximation error via either the Kullback-Leibler (Ali and Silvey, 1966) or Renyi (Rényi et al.",
      "startOffset" : 196,
      "endOffset" : 218
    }, {
      "referenceID" : 63,
      "context" : "In particular, (Xie et al., 2017; Barrett et al., 2019; Elazar and Goldberg, 2018) approach the problem based on adversarial losses.",
      "startOffset" : 15,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "In particular, (Xie et al., 2017; Barrett et al., 2019; Elazar and Goldberg, 2018) approach the problem based on adversarial losses.",
      "startOffset" : 15,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "In particular, (Xie et al., 2017; Barrett et al., 2019; Elazar and Goldberg, 2018) approach the problem based on adversarial losses.",
      "startOffset" : 15,
      "endOffset" : 82
    }, {
      "referenceID" : 63,
      "context" : "The classifier is said to be fair if there is no statistical information about y that is present in hx (Xie et al., 2017; Elazar and Goldberg, 2018).",
      "startOffset" : 103,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "The classifier is said to be fair if there is no statistical information about y that is present in hx (Xie et al., 2017; Elazar and Goldberg, 2018).",
      "startOffset" : 103,
      "endOffset" : 148
    }, {
      "referenceID" : 71,
      "context" : "Proposed approaches to tackle textual style transfer (Zhang et al., 2020; Xu et al., 2019) can be divided into two",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 65,
      "context" : "Proposed approaches to tackle textual style transfer (Zhang et al., 2020; Xu et al., 2019) can be divided into two",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 47,
      "context" : "The first category (Prabhumoye et al., 2018; Lample et al., 2018) uses cycle losses based on back translation (Wieting et al.",
      "startOffset" : 19,
      "endOffset" : 65
    }, {
      "referenceID" : 38,
      "context" : "The first category (Prabhumoye et al., 2018; Lample et al., 2018) uses cycle losses based on back translation (Wieting et al.",
      "startOffset" : 19,
      "endOffset" : 65
    }, {
      "referenceID" : 60,
      "context" : ", 2018) uses cycle losses based on back translation (Wieting et al., 2017) to ensure that the content is preserved during the transformation.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "This constraint is enforced using either adversarial training (Fu et al., 2017; Hu et al., 2017; Zhang et al., 2018; Yamshchikov et al., 2019) or MI minimisation using vCLUB-S (Cheng et al.",
      "startOffset" : 62,
      "endOffset" : 142
    }, {
      "referenceID" : 25,
      "context" : "This constraint is enforced using either adversarial training (Fu et al., 2017; Hu et al., 2017; Zhang et al., 2018; Yamshchikov et al., 2019) or MI minimisation using vCLUB-S (Cheng et al.",
      "startOffset" : 62,
      "endOffset" : 142
    }, {
      "referenceID" : 70,
      "context" : "This constraint is enforced using either adversarial training (Fu et al., 2017; Hu et al., 2017; Zhang et al., 2018; Yamshchikov et al., 2019) or MI minimisation using vCLUB-S (Cheng et al.",
      "startOffset" : 62,
      "endOffset" : 142
    }, {
      "referenceID" : 66,
      "context" : "This constraint is enforced using either adversarial training (Fu et al., 2017; Hu et al., 2017; Zhang et al., 2018; Yamshchikov et al., 2019) or MI minimisation using vCLUB-S (Cheng et al.",
      "startOffset" : 62,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : ", 2019) or MI minimisation using vCLUB-S (Cheng et al., 2020b).",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "In order to approach the sequence generation tasks, we build on the Style-embedding Model by (John et al., 2018) (StyleEmb) which uses adversarial losses introduced in prior work for these dedicated tasks.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 44,
      "context" : "Estimating the MI is a long-standing challenge as the exact computation (Paninski, 2003) is only tractable for discrete variables, or for a limited family of problems where the underlying data-",
      "startOffset" : 72,
      "endOffset" : 88
    }, {
      "referenceID" : 45,
      "context" : "distribution satisfies smoothing properties, see recent work by (Pichler et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "Different from previous approaches leading to variational lower bounds (Belghazi et al., 2018; Hjelm et al., 2018; Oord et al., 2018), in this paper we derive an es-",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "Different from previous approaches leading to variational lower bounds (Belghazi et al., 2018; Hjelm et al., 2018; Oord et al., 2018), in this paper we derive an es-",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 43,
      "context" : "Different from previous approaches leading to variational lower bounds (Belghazi et al., 2018; Hjelm et al., 2018; Oord et al., 2018), in this paper we derive an es-",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "timator based on a variational upper bound to the MI which control the approximation error based on the Kullback-Leibler and the Renyi divergences (Daudel et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "plication of the the (Donsker and Varadhan, 1985) representation of KL divergence while the lower bound on H(Y |Z) follows from the monotonicity property of the function: α 7→ Dα ( pZY ‖pZ ·qŶ |Z ) .",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 56,
      "context" : "Density-ratio estimation has been widely studied in the literature (see (Sugiyama et al., 2012) and references therein) and confidence bounds has been reported by (Kpotufe, 2017) under some smoothing assumption on underlying data-distribution pZY .",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : ", 2012) and references therein) and confidence bounds has been reported by (Kpotufe, 2017) under some smoothing assumption on underlying data-distribution pZY .",
      "startOffset" : 75,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "Although the cross-entropy can lead to good estimates of the conditional entropy, the adversarial approaches for classification and sequence generation by (Barrett et al., 2019; John et al., 2018) which consists in maximizing the cross-entropy, induces a degeneracy (unbounded loss) as λ increases in the underlying optimization problem.",
      "startOffset" : 155,
      "endOffset" : 196
    }, {
      "referenceID" : 29,
      "context" : "Although the cross-entropy can lead to good estimates of the conditional entropy, the adversarial approaches for classification and sequence generation by (Barrett et al., 2019; John et al., 2018) which consists in maximizing the cross-entropy, induces a degeneracy (unbounded loss) as λ increases in the underlying optimization problem.",
      "startOffset" : 155,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "The dataset has been automatically constructed from DIAL corpus (Blodgett et al., 2016) which contained race annotations over 50 Million of tweets.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "(Barrett et al., 2019) report that offline classifiers (post training) outperform clearly adversarial discriminators.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : "of the output sentence by using a fastText classifier (Joulin et al., 2016b).",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "For content preservation, we follow (John et al., 2018) and compute both: (i) the cosine measure between source and generated sentence embeddings, which are the concatenation of min, max, and mean of word embedding (sentiment words removed), and (ii) the BLEU score between generated text and the input using SACREBLEU from (Post, 2018).",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 46,
      "context" : ", 2018) and compute both: (i) the cosine measure between source and generated sentence embeddings, which are the concatenation of min, max, and mean of word embedding (sentiment words removed), and (ii) the BLEU score between generated text and the input using SACREBLEU from (Post, 2018).",
      "startOffset" : 276,
      "endOffset" : 288
    }, {
      "referenceID" : 48,
      "context" : "the perplexity given by a GPT-2 (Radford et al., 2019) pretrained model performing fine-tuning on the training corpus.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "ones reported in previous studies (Barrett et al., 2019; Elazar and Goldberg, 2018).",
      "startOffset" : 34,
      "endOffset" : 83
    }, {
      "referenceID" : 19,
      "context" : "ones reported in previous studies (Barrett et al., 2019; Elazar and Goldberg, 2018).",
      "startOffset" : 34,
      "endOffset" : 83
    }, {
      "referenceID" : 58,
      "context" : "Disentanglement modules are a core block for a large number of both style transfer and conditional sentence generation algorithms (Tikhonov et al., 2019; Yamshchikov et al., 2019; Fu et al., 2017) that place explicit constraints",
      "startOffset" : 130,
      "endOffset" : 196
    }, {
      "referenceID" : 66,
      "context" : "Disentanglement modules are a core block for a large number of both style transfer and conditional sentence generation algorithms (Tikhonov et al., 2019; Yamshchikov et al., 2019; Fu et al., 2017) that place explicit constraints",
      "startOffset" : 130,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "Disentanglement modules are a core block for a large number of both style transfer and conditional sentence generation algorithms (Tikhonov et al., 2019; Yamshchikov et al., 2019; Fu et al., 2017) that place explicit constraints",
      "startOffset" : 130,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "This phenomenon is also reported in (Feutry et al., 2018) on a picture anonymization task.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 49,
      "context" : "tional generation, which tends to validate the current line of work that formulates text generation as generic text-to-text (Raffel et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "The adversarial loss degenerates for values λ ≥ 5 and a stuttering phenomenon appears (Holtzman et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "Future work includes testing with other type of labels such as dialog act (Chapuis et al., 2020; Colombo et al., 2020), emotions (Witon et al.",
      "startOffset" : 74,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Future work includes testing with other type of labels such as dialog act (Chapuis et al., 2020; Colombo et al., 2020), emotions (Witon et al.",
      "startOffset" : 74,
      "endOffset" : 118
    }, {
      "referenceID" : 61,
      "context" : ", 2020), emotions (Witon et al., 2018), opinion (Garcia et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : ", 2019) or speaker’s stance and confidence (Dinkar et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 64
    } ],
    "year" : 0,
    "abstractText" : "Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi’s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.",
    "creator" : null
  }
}