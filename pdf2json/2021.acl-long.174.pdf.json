{
  "name" : "2021.acl-long.174.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval",
    "authors" : [ "Zijing Ou", "Qinliang Su", "Jianxing Yu", "Bang Liu", "Jingwen Wang", "Ruihui Zhao", "Changyou Chen", "Yefeng Zheng" ],
    "emails" : [ "ouzj@mail2.sysu.edu.cn,", "yujx26}@mail.sysu.edu.cn,", "bang.liu@umontreal.ca,", "yefengzheng}@tencent.com,", "changyou@buffalo.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2238–2249\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2238"
    }, {
      "heading" : "1 Introduction",
      "text" : "Similarity search plays a pivotal role in a variety of tasks, such as image retrieval (Jing and Baluja, 2008; Zhang et al., 2018), plagiarism detection (Stein et al., 2007) and recommendation systems (Koren, 2008). If the search is carried out in the original continuous feature space directly, the requirements of computation and storage would be\n∗Corresponding author. Qinliang Su is also affiliated with (i) Guangdong Key Lab. of Big Data Analysis and Processing, Guangzhou, China, and (ii) Key Lab. of Machine Intelligence and Advanced Computing, Ministry of Education, China.\n1Our code is available at https://github.com/J-zin/SNUH. The MindSpore code will also be released soon.\nextremely high, especially for large-scale applications. Semantic hashing (Salakhutdinov and Hinton, 2009b) sidesteps this problem by learning a compact binary code for every item such that similar items can be efficiently found according to the Hamming distance of binary codes.\nUnsupervised semantic hashing aims to learn for each item a binary code that can preserve the semantic similarity information of original items, without the supervision of any labels. Motivated by the success of deep generative models (Salakhutdinov and Hinton, 2009a; Kingma and Welling, 2013; Rezende et al., 2014) in unsupervised representation learning, many recent methods approach this problem from the perspective of deep generative models, leading to state-of-the-art performance on benchmark datasets. Specifically, these methods train a deep generative model to model the underlying documents and then use the trained generative model to extract continuous or binary representations from the original documents (Chaidaroon and Fang, 2017; Shen et al., 2018; Dong et al., 2019; Zheng et al., 2020). The basic principle behind these generative hashing methods is to have the hash codes retaining as much semantics information of original documents as possible so that semantically similar documents are more likely to yield similar codes.\nIn addition to semantics information, it is widely observed that neighborhood information among the documents is also useful to generate high-quality hash codes. By constructing an adjacency matrix from the raw features of documents, neighborbased methods seek to preserve the information in the constructed adjacency matrix, such as the locality-preserving hashing (He et al., 2004; Zhao et al., 2014), spectral hashing (Weiss et al., 2009; Li et al., 2012), and etc. However, since the groundtruth neighborhood information is not available and the constructed one is neither accurate nor\ncomplete, neighbor-based methods alone do not perform as well as the semantics-based ones. Despite both semantics and neighborhood information are derived from the original documents, different aspects are emphasized in them. Thus, to obtain higher-quality hash codes, it has been proposed to incorporate the constructed neighborhood information into semantics-based methods. For examples, Chaidaroon et al. (2018) and Hansen et al. (2020) require the hash codes can reconstruct neighboring documents, in addition to the original input. Other works (Shen et al., 2019; Hansen et al., 2019) use an extra loss term, derived from the approximate neighborhood information, to encourage similar documents to produce similar codes. However, all of the aforementioned methods exploit the neighborhood information by using it to design different kinds of regularizers to the original semanticsbased models, lacking a basic principle to unify and leverage them under one framework.\nTo fully exploit the two types of information, in this paper, we propose a hashing method that unifies the semantics and neighborhood information with the graph-driven generative models. Specifically, we first encode the neighborhood information with a multivariate Gaussian distribution. With this Gaussian distribution as a prior in a generative model, the neighborhood information can be naturally incorporated into the semantics-based hashing model. Despite the simplicity of the modeling, the correlation introduced by the neighbor-encoded prior poses a significant challenge to the training since it invalidates the widely used identical-andindependent-distributed (i.i.d.) assumption, making all documents correlated. To address this issue, we propose to use a tree-structured distribution to capture as much as possible the neighborhood information. We prove that under the tree approximation, the evidence lower bound (ELBO) can be decomposed into terms involving only singleton and pairwise documents, enabling the model to be trained as efficiently as the models without considering the document correlations. To capture more neighborhood information, a more accurate approximation by using multiple trees is also developed. Extensive experimental results on three public datasets demonstrate that the proposed method can outperform state-of-the-art methods, indicating the effectiveness of the proposed framework in unifying the semantic and neighborhood information for document hashing."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Semantics-Based Hashing Due to the similarities among the underlying ideas of these methods, we take the variational deep semantic hashing (VDSH) (Chaidaroon and Fang, 2017) as an example to illustrate their working flow. Given a document x , {wj}|x|j=1, VDSH proposes to model a document by a generative model as\np(x, z) = pθ(x|z)p(z), (1)\nwhere p(z) is the prior distribution and is chosen to be the standard Gaussian distribution N (z;0, Id), with Id denoting the d-dimensional identity matrix; and pθ(x|z) is defined to be\npθ(x|z) = ∏ wi∈x pθ(wi|z) (2)\nwith\npθ(wi|z) , exp(zTEwi + bi)∑|V | j=1 exp(zTEwj + bj) , (3)\nin which wj denotes the |V |-dimensional one-hot representation of the j-th word, with |x| and |V | denoting the document and vocabulary size, respectively; and E ∈ Rd×|V | represents the learnable embedding matrix. For a corpus containing N documents X = {x1,x2, · · · ,xN}, due to the i.i.d. assumption for documents, it is modelled by simply multiplying individual document models as\np(X,Z) = N∏ k=1 pθ(xk|zk)p(zk), (4)\nwhereZ , [z1; z2; · · · ; zN ] denotes a long vector obtained by concatenating the individual vectors zi. The model is trained by optimizing the evidence lower bound (ELBO) of the log-likelihood function log p(X). After training, outputs from the trained encoder are used as documents’ representations, from which binary hash codes can be obtained by thresholding the real-valued representations.\nNeighborhood Information The ground-truth semantic similarity information is not available for the unsupervised hashing task in practice. To leverage this information, an affinity N ×N matrixA is generally constructed from the raw features (e.g., the TFIDF) of original documents. For instances, we can construct the matrix as\naij=  e− ||xi−xj|| 2\nσ , xi∈Nk (xj) 0, otherwise\n(5)\nwhere aij denotes the (i, j)-th element of A; and Nk(x) denotes the k-nearest neighbors of document x. Given the affinity matrixA, some methods have been proposed to incorporate the neighborhood information into the semantics-based hashing models. However, as discussed above, these methods generally leverage the information based on some intuitive criteria, lacking theoretical supports behind them."
    }, {
      "heading" : "3 A Hashing Framework with Unified",
      "text" : "Semantics-Neighborhood Information\nIn this section, we present a more effective framework to unify the semantic and neighborhood information for the task of document hashing."
    }, {
      "heading" : "3.1 Reformulating the VDSH",
      "text" : "To introduce the neighborhood information into the semantics-based hashing models, we first rewrite the VDSH model into a compact form as\np(X,Z) = pθ(X|Z)pI(Z), (6)\nwhere pθ(X|Z) = ∏N\nk=1 pθ(xk|zk); and the prior pI(Z) = ∏N k=1 p(zk), which can be shown to be\npI(Z) = N (Z;0, IN ⊗ Id) . (7)\nHere, ⊗ denotes the Kronecker product and the subscript I indicates independence among zk. The ELBO of this model can be expressed as\nL=Eqφ(Z|X)[log pθ(X|Z)]︸ ︷︷ ︸ L1 −KL(qφ(Z|X)||pI(Z))︸ ︷︷ ︸ L2\nwhere KL(·) denotes the Kullback-Leibler (KL) divergence. By restricting the posterior to independent Gaussian form\nqφ(Z|X) = N∏ k=1 N ( zk;µk, diag(σ 2 k) )︸ ︷︷ ︸\nqφ(zk|xk)\n, (8)\nthe L1 can be handled using the reparameterization trick. Thanks to the factorized forms assumed in qφ(Z|X) and pI(Z), the L2 term can also be expressed analytically and evaluated efficiently."
    }, {
      "heading" : "3.2 Injecting the Neighborhood Information",
      "text" : "Given an affinity matrixA, the covariance matrix IN+λA can be used to reveal the neighborhood information of documents, where the hyperparameter λ ∈ [0, 1) is used to control the overall correlation\nstrength. If two documents are neighboring, then the corresponding correlation value in IN + λA will be large; otherwise, the value will be zero. To have the neighborhood information reflected in document representations, we can require that the representations zi are drawn from a Gaussian distribution of the form\npG(Z) = N (Z;0, (IN + λA)⊗ Id) , (9)\nwhere the subscript G denotes that the distribution is constructed from a neighborhood graph. To see why the representations Z ∼ pG(Z) have already reflected the neighborhood information, let us consider an example with three documents {x1,x2,x3}, in which x1 is connected to x2, x2 is connected to x3, and no connection exists between x1 and x3. Under the case that zi is a two-dimensional vector zi ∈ R2, we have the concatenated representations [z1; z2; z3] follow a Gaussian distribution with covariance matrix of\nz1 z2 z3  z1 1 0 λa12 0 0 0 0 1 0 λa12 0 0 z2 λa21 0 1 0 λa23 0 0 λa21 0 1 0 λa23 z3 0 0 λa32 0 1 0 0 0 0 λa32 0 1\nFrom the property of Gaussian distribution, it can be known that z1 is strongly correlated with z2 on the corresponding elements, but not with z3. This suggests that z1 should be similar to z2, but different from z3, which is consistent with the neighborhood relation that x1 is a neighbor of x2, but not of x3.\nNow that the neighborhood information can be modeled by requiring Z being drawn from pG(Z), and the semantic information can be reflected in the likelihood function pθ(X|Z). The two types of information can be taken into account simultaneously by modeling the corpus as\np(X,Z) = pθ(X|Z)pG(Z). (10)\nComparing to the VDSH model in (6), it can be seen that the only difference lies in the employed priors. Here, a neighborhood-preserving prior pG(Z) is employed, while in VDSH, an independent prior pI(Z) is used. Although only a modification to the prior is made from the perspective of modeling, significant challenges are posed for the training. Specifically, by replacing pI(Z) with pG(Z) in the L2 of L, it can be\nshown that the expression of L2 involves the matrix ( (IN+λA)⊗Id )−1. Due to the introduced dependence among documents, for example, if the corpus contains over 100,000 documents and the representation dimension is set to 100, the L2 involves the inverse of matrices with dimension as high as 107, which is computationally prohibitive in practice."
    }, {
      "heading" : "4 Training with Tree Approximations",
      "text" : "Although the prior pG(Z) captures the full neighborhood information, its induced model is not practically trainable. In this section, to facilitate the training, we first propose to use a tree-structured prior to partially capture the neighborhood information, and then extend it to multiple-tree case for more accurate modeling.\n4.1 Approximating the Prior pG(Z) with a Tree-Structured Distribution\nThe matrix A represents a graph G , (V, E), where V = {1, 2, · · · , N} is the set of document indices; and E = {(i, j)|aij 6= 0} is the set of connections between documents. From the graph G, a spanning tree T = (V, ET ) can be obtained easily, where ET denotes the set of connections on the tree.2 Based on the spanning tree, we construct a new distribution as pT (Z) = ∏ i∈V pG(zi) ∏\n(i,j)∈ET\npG(zi, zj)\npG(zi)pG(zj) , (11)\nwhere pG(zi) and pG(zi, zj) represent one- and two-variable marginal distributions of pG(Z), respectively. From the properties of Gaussian distribution, it is known that\npG(zi)=N(zi;0, Id), pG(zi, zj)=N([zi;zj ];0,(I2+λAij)⊗Id) , (12)\nwhere Aij , [ 0 aij aji 0 ] . Because pT (Z) is defined on a tree, as proved in (Wainwright and Jordan, 2008), it is guaranteed to be a valid probability distribution, and more importantly, it satisfies the following two relations: i) pT (zi) = pG(zi); ii) pT (zi, zj) = pG(zi, zj) for any (i, j) ∈ ET , where pT (zi) and pT (zi, zj) denote the marginal distributions of pT (Z). That is, the tree-structured\n2We assume the graph is connected. For more general cases, results can be derived similarly.\ndistribution pT (Z) captures the neighborhood information reflected on the spanning tree T. By using pT (Z) to replace pI(Z) of L2, it can be shown that L2 can be expressed as the summation of terms involving only one or two variables, which can be handled easily. Due to the limitation of space, the concrete expression for the lower bound is given in the Supplementary Material."
    }, {
      "heading" : "4.2 Imposing Correlations on the Posterior",
      "text" : "The posterior distribution qφ(Z|X) in the previous section is assumed to be in independent form, as the form shown in (8). But since a prior pT (Z) considering the correlations among documents is used, assuming an independent posterior is not appropriate. Hence, we follow the tree-structured prior and also construct a tree-structured posterior qT (Z|X)= ∏ i∈V qφ(zi|xi) ∏ (i,j)∈ET qφ (zi, zj |xi,xj) qφ(zi|xi)qφ(zj |xj) ,\nwhere qφ(zi|xi) is the same as that in (8); and qφ (zi, zj |xi,xj) is also defined to be Gaussian, with its mean defined as [µi;µj ] and covariance matrix defined as[\ndiag(σ2i ) diag(γij σi σj) diag(γij σi σj) diag(σ2j )\n] , (13)\nin which γij ∈ Rd controls the correlation strength between zi and zj , whose elements are restricted in (−1, 1) and denotes the Hadamard product. By taking the correlated posterior qT (Z|X) into the ELBO, we obtain\nLT = ∑ i∈V Eqφ[log pθ(xi|zi)]−KL(qφ(zi)||pG(zi))\n− ∑\n(i,j)∈ET\n( KL (qφ(zi, zj |xi,xj)||pG(zi, zj))\n−KL(qφ(zi)||pG(zi))−KL(qφ(zj)||pG(zj)) ) ,\nwhere we briefly denote the variational distribution qφ(zi|xi) as qφ(zi). Since pG(zi), pG(zi, zj), qφ(zi|xi) and qφ(zi, zj |xi,xj) are all Gaussian distributions, the KL-divergence terms above can be derived in closed-form. Moreover, it can be seen that LT involves only single or pairwise variables, thus optimizing it is as efficient as the models without considering document correlation.\nWith the trained model, hash codes can be obtained by binarizing the posterior mean µi with a threshold, as done in (Chaidaroon and Fang,\n2017). However, if without any constraint, the range of mean lies in (−∞,+∞). Thus, if we binarize it directly, lots of information in the original representations will be lost. To alleviate this problem, in our implementation, we parameterize the posterior mean µi by a function of the form µi = sigmoid(nn(xi)/τ), where the outermost sigmoid function forces the mean to look like binary value and thus can effectively reduce the quantization loss, with nn(·) denoting a neural network function and τ controlling the slope of the sigmoid function."
    }, {
      "heading" : "4.3 Extending to Multiple Spanning Trees",
      "text" : "Obviously, approximating the graph with a spanning tree may lose too much information. To alleviate this issue, we propose to capture the similarity information by a mixture of multiple distributions, with each built on a spanning tree. Specifically, we first construct a set of M spanning trees TG = {T1,T2, · · · ,TM} from the original graph G. Based on the set of spanning trees, a mixturedistribution prior and posterior can be constructed as\npMT (Z) = 1\nM ∑ T ∈TG pT (Z), (14)\nqMT (Z|X) = 1\nM ∑ T ∈TG qT (Z|X), (15)\nwhere pT (Z) and qT (Z|X) are the prior and posterior defined on the tree T , as done in (11) and (13). By taking the mixture distributions above into the ELBO of L to replace the prior and posterior, we can obtain a new ELBO, denoted as LMT . Obviously, it is impossible to obtain a closed-form expression for the bound LMT . But as proved in (Tang et al., 2019), by using the log-sum inequality, LMT can be further lower bounded by\nL̃MT = 1\nM ∑ T ∈TG LT . (16)\nGiven the expression of LT , the lower bound of L̃MT can also be expressed in closed-form and optimized efficiently. For detailed derivations and concrete expressions, please refer to the Supplementary."
    }, {
      "heading" : "4.4 Details of Modeling",
      "text" : "The parameters µi,µj ,σi,σj and γij in the approximate posterior distribution qφ(zi|xi) of (8)\nand qφ(zi, zj |xi,xj) of (13) are all defined as the outputs of neural networks, with the parameters denoted as φ. Specifically, the entire model is mainly composed of three components:\ni) The variational encoder qφ(zi|xi), which takes single document as input, and outputs the mean and variance of Gaussian distribution, i.e., [µi;σ 2 i ] = fφ(xi);\nii) The correlated encoder, which takes pairwise documents as input, and outputs the correlation coefficient, i.e., γij = fφ(xi,xj). Note that the correlation encoder is required to be order-irrelevant, that is, fφ(xi,xj) = fφ(xj ,xi), which is achieved in this paper as fφ = 1 2 ( fφ(xi,xj) + fφ(xj ,xi) ) ;\niii) The generative decoder pθ(xi|zi), which takes the latent variable zi as input and output the document xi. The decoder is modeled by a neural network parameterized by θ.\nThe model is trained by optimizing the lower bound L̃MT w.r.t. φ and θ. After training, hash codes are obtained by passing the documents through the variational encoder and binarizing the outputs on every dimension by a the threshold value, which is simply set as 0.5 in our experiments.\nTo intuitively understand the insight behind our model, an illustration is shown in Figure 1. We see that if the two documents are neighbors and semantically similar, the representations will be strongly correlated to each other. But if they are not semantically similar neighbors, the representations become less correlated. If they are neither neighbors nor semantically similar, the representations become not correlated at all. Since our model can simultaneously preserve semantics and neighborhood information, we name it as Semantics-Neighborhood Unified Hahing (SNUH)."
    }, {
      "heading" : "5 Related Work",
      "text" : "Deep generative models (Rezende et al., 2014) have attracted a lot of attention in semanticsbased hashing, due to their successes in unsupervised representation learning. VDSH (Chaidaroon and Fang, 2017) first employed variational autoencoder (VAE) (Kingma and Welling, 2013) to learn continuous representations of documents and then casts them into binary codes. However, for the sake of information leaky problem during binarization step, such a two-stage strategy is prone to result in local optima and undermine the performance. NASH (Shen et al., 2018) tackled this issue by replacing the Gaussian prior with Bernoulli and adopted the straight-through technique (Bengio et al., 2013) to achieve end-to-end training. To further improve the model’s capability, Dong et al. (2019) proposed to employ mixture distribution as a priori knowledge and Zheng et al. (2020) exploited Boltzmann posterior to introduce correlation among bits. Beyond generative frameworks, AMMI (Stratos and Wiseman, 2020) achieved superior performance by maximizing the mutual information between codes and documents. Nevertheless, the aforementioned semantic hashing methods are consistently under the i.i.d. assumption, which means they ignore the neighborhood information.\nSpectral hashing (Weiss et al., 2009) and selftaught hashing (Zhang et al., 2010) are two typical methods of neighbor-based hashing models. But these algorithms generally ignore the rich semantic information associated with documents. Recently, some VAE-based models tried to concurrently take account of semantic and neighborhood information, such as NbrReg (Chaidaroon et al., 2018), RBSH (Hansen et al., 2019) and PairRec(Hansen et al., 2020). However, as mentioned before, all of them simply regarded the proximity as regularization, lacking theoretical principles to guide the incorporation process. Thanks to the virtue of graph-induced distribution, we effectively preserve the two types of information in a theoretical framework."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Experiment Setup",
      "text" : "Datasets We verify the proposed methods on three public datasets which published by VDSH3:\n3https://github.com/unsuthee/VariationalDeepSemantic Hashing/tree/master/dataset\ni) Reuters25178, which contains 10,788 news documents with 90 different categories; ii) TMC, which is a collection of 21,519 air traffic reports with 22 different categories; iii) 20Newsgroups (NG20), which consists of 18,828 news posts from 20 different topics. Note that the category labels of each dataset are only used to compute the evaluation metrics, as we focus on unsupervised scenarios.\nBaselines We compare our method with the following models: SpH (Weiss et al., 2009), STH (Zhang et al., 2010), VDSH (Chaidaroon and Fang, 2017), NASH (Shen et al., 2018), GMSH(Dong et al., 2019), NbrReg (Chaidaroon et al., 2018), CorrSH (Zheng et al., 2020) and AMMI (Stratos and Wiseman, 2020). For all baselines, we take the reported performance from their original papers.\nTraining Details For fair comparisons, we follow the same network architecture used in VDSH, GMSH and CorrSH, using a one-layer feedforward neural network as the variational and the correlated encoder. The graph G is constructed with the K-nearest neighbors (KNN) algorithm based on cosine similarity on the TFIDF features of documents. In our experiments, the correlation strength coefficient λ in (12) is fixed to 0.99. According to the performance observed on the validation set, we choose the learning rate from {0.0005, 0.001, 0.003}, batch size from {32, 64, 128}, the temperature τ in sigmoid function from {0.1, 0.2, · · · , 1}, the number of treesM and neighbors K both form {1,2,. . . ,20}, with the best used for evaluation on the test set. The model is trained using the Adam optimizer (Kingma and Ba, 2014). More detailed experimental settings, along with the generating method of spanning trees, are given in the supplementary materials.\nEvaluation Metrics The retrieval precision is used as our evaluation metric. For each query document, we retrieve 100 documents most similar to it based on the Hamming distance of hash codes. Then, the retrieval precision for a single sample is measured as the percentage of the retrieved documents with the same label as the query. Finally, the average precision over the whole test set is calculated as the performance of the evaluated method."
    }, {
      "heading" : "6.2 Performance and Analysis",
      "text" : "Overall Performance The performances of all the models on the three public datasets are shown in Table 1. We see that our model performs favorably\nto the current state-of-the-art method, yielding best average performance across different datasets and settings. Compared with VDSH and NASH, which simply employ isotropic Gaussian and Bernoulli prior, respectively, we can observe that our model, which leverages correlated prior and posterior distributions, achieves better results on all the three datasets. Although GMSH improves performance by exploiting a more expressive Gaussian mixture prior, our model still outperforms it by a substantial margin, indicating the superiority of incorporating document correlations. It is worth noting that, by unifying semantics and neighborhood information under the generative models, the two types of information can be preserved more effectively. This can be validated by that our model performs significantly better than NbrReg, which naively incorporates the neighborhood information by using a neighbor-reconstruction regularizer. The superiority of our unified method can be further corroborated in the comparisons with RBSH and PairRec, which are given in the Supplementary since they employed a different preprocessing method as the models reported here. Comparing to the current SOTA methods of AMMI and CorrSh, our method\nis still able to achieve better results by exploiting the correlation among documents. Moreover, thanks to the benefit of correlation regularization, remarkable gratuity can be acquired profitably in 64 and 128 bits.\nImpact of Introducing Correlations in Prior and Posterior To understand the influences of the proposed document-correlated prior and posterior, we further experiment with two variants of our model: i) SNUHind: which does not consider document correlations in neither the prior nor the posterior distribution; ii) SNUHprior: which only considers the correlations in the prior, but not in the posterior. Obviously, the proposed SNUH represents the method that leverage the correlations in both of the prior and posterior. As seen from Table 2, SNUHprior achieves better performance than SNUHind, demonstrating the benefit of considering the correlation information of documents only in the prior. By further taking the correlations into account in the posterior, improvements of SNUH can be further observed, which fully corroborates the superiority of considering document correlations in the prior and posterior. Another interesting observation is that the performance gap be-\ntween SNUHind and SNUHprior becomes small as the length of bits increases. This may be attributed to the fact that the increased generalization ability of models brought by large bits is inclined to alleviate the impact of priori knowledge. However, by additionally incorporating correlation constraints on posterior, significant performance gains would be obtained, especially in large bits scenarios.\nEffect of Spanning Trees For more efficient training, spanning trees are utilized to approximate the whole graph by dropping out some edges. To understand its effects, we first investigate the impact of the number of trees. The first row of Figure 2 shows the performance of our method as a function of different numbers of spanning trees. We observe that, compared to not using any correlation, one tree alone can bring significant performance gains. As the tree number increases, the performance rises steadily at first and then converges into a certain level, demonstrating that the document correlations can be mostly captured by several spanning trees. Then, we further explore the impact of the neighbor number when constructing the graphs using the KNN method, as shown in the second row of Figure 2. It can be seen that more neighbors contributes to better performance. We hypothesize that this is partly due to the more diverse correlation information captured by the increasing number of neighbors. However, incorporating too many neighbors may lead to the problem of introducing noise and incorrect correlation information to the hash codes. That explains why no further improvement is observed after the number reaches a level.\nEmpirical Study of Computational Efficiency We also investigate the training complexity by comparing the training duration of our method and VDSH, on Tesla V100-SXM2-32GB. On the Reuters, TMC, 20Newsgroups datasets with 64- bit hash codes, our method finishes one epoch of training respectively in 3.791s, 5.238s, 1.343s and\nVDSH in 2.038s, 4.364s, 1.051s. It can be seen that our model, though with much stronger performance, can be trained almost as efficiently as vanilla VDSH due to the tree approximations.\nCase Study In Table 3, we present a retrieval case of the given query document. It can be observed that as the Hamming distance increases, the semantic (topic) of the retrieved document gradually becomes more irrelevant, illustrating that the Hamming distance can effectively measure the document relevance.\nVisualization of Hash Codes To evaluate the quality of generated hash code more intuitively, we project the latent representations into a 2- dimensional plane with the t-SNE (van der Maaten and Hinton, 2008) technique. As shown in Figure 3, the representations generated by our method are more separable than those of AMMI, demonstrating the superiority of our method."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have proposed an effective and efficient semantic hashing method to preserve both the semantics and neighborhood information of documents. Specifically, we applied a graph-induced Gaussian prior to model the two types of information in a unified framework. To facilitate training, a treestructure approximation was further developed to\ndecompose the ELBO into terms involving only singleton or pairwise variables. Extensive evaluations demonstrated that our model significantly outperforms baseline methods by incorporating both the semantics and neighborhood information."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the National Natural Science Foundation of China (No. 61806223, 61906217, U1811264), Key R&D Program of Guangdong Province (No. 2018B010107005), National Natural Science Foundation of Guangdong Province (No. 2021A1515012299). This work is also supported by MindSpore."
    }, {
      "heading" : "A Derivation of Formulas",
      "text" : "Derivation of KL (qφ(Z|X)||pT (Z)) In the main paper, we propose a tree-type distribution to introduce partial neighborhood information so that the L2 term can be expressed as the summation over terms involving only one or two variables. Here, we provide the detail derivation.\nKL (qφ(Z|X)||pT (Z))\n= ∫ qφ(Z|X)log\n∏ i∈V\nqφ(zi|xi)∏ i∈V pG(zi) ∏\n(i,j)∈ET\npG(zi,zj) pG(zi)pG(zj)\ndZ\n= ∑ i∈V KL (qφ(zi|xi)||pθ(zi))\n− ∑\n(i,j)∈ET\nEqφ(zi,zj |xi,xj) [ log pG(zi)pG(zj)\npG(zi, zj)\n] .\nObviously, the KL divergence is decomposed into the terms involving singleton and pairwise variables, which can be calculated efficiently.\nExpressing LT in Analytical Form For simplification, in the following, we use µ1,Σ1 to represent the mean and variance matrix of qT (zi, zj |xi,xj), respectively, and represent those of pG(zi, zj) as µ2,Σ2, respectively. Besides we denote λaij as τij so we have τij = λaij = λaji. By applying the Cholesky decomposition on the covariance matrix of Σ1 and Σ2\nΣ1=\n[ σi 0d\nγijσj √ 1− γ2ijσj\n][ σi γijσj\n0d √ 1− γ2ijσj\n] ,\nΣ2=\n[ Id 0\nτijId √ 1− τ2ijId\n][ Id τijId\n0 √ 1− τ2ijId\n] ,\nwhere we omit diag(·) for simplifying, we have\nKL (qφ(zi, zj |xi,xj)||pG(zi, zj))\n= 1\n2 d∑ n=1 { log(1− τ2ij)\n− ( logσ2in + logσ 2 jn + log(1− γ2ijn) ) − 2 + σ2in+σ 2 jn−2τijγijnσinσjn+µ2in+µjn−2τijµinµjn\n1− τ2ij\n} .\nAlgorithm 1 Model Training Algorithm Input: Document representationsX; edges list of spanning trees E; batch size b. Output: Optimal parameters (θ,φ). 1: θ,φ← Initialize parameters 2: repeat 3: VM←{x1, · · · ,xb}∼X . Sample nodes 4: EMT ←{e1, · · · , eb}∼E . Sample endges 5: g ← ∇φ,θL̃MMT (θ,φ;VM , EMT ) 6: θ,φ ← Update parameters using gradients g (e.g.,\nThen, we can express LT in an analytical form LT = ∑ i∈V ( Eqφ(zi|xi)[log pθ(xi|zi)]− 1 2 d∑ n=1 (µ2in\n+ σ2in−1−2 logσin) ) − ∑\n(i,j)∈ET\n( 1\n2 d∑ n=1 { log(1− τ2ij)\n− ( µ2in + µ 2 jn + σ 2 in + σ 2 jn + log(1− γ2ijn) ) + σ2in+σ 2 jn−2τijγijnσinσjn+µ2in+µjn−2τijµinµjn\n1− τ2ij\n})\nDerivation of L̃MT With LMT , we extend the single-tree approximation to multi-tree approximation. Although the KL divergence between the mixture distributions does not have a closed-form solution, we can obtain its explicit upper bound by using the log-sum inequality as\nLMT ≥ 1\nM ∑ T ∈TG EqT (Z|X)[log pθ(X|Z)]\n− 1 M ∑ T ∈TG KL (qT (Z|X)||pT (X))\n, L̃MT .\nWe can further express L̃MT in a more intuitive form as∑ i∈V ( Eqφ(zi|xi)[log pθ(xi|zi)]−KL(qφ(zi|xi)||pG(zi))\n) −\n∑ (i,j)∈ET wij ( KL(qφ(zi, zj |xi,xj)||pG(xi,xj))\n−KL(qφ(zi|xi)||pG(zi))−KL(qφ(zj |xj)||pG(zj)) ) ,\nAlgorithm 2 Spanning Tree Generation Algorithm Input: Graph G; number of trees n. Output: Edges list of spanning trees E. 1: procedure TREEGEN(n) . Input: #tree n 2: E = [ ] . Initial edges list 3: for k ← 0, · · · , n− 1 do 4: V = [False]|V| . Visited node list 5: while False in V do 6: i← RC[V==False] . Choose node 7: Q = [i] . Initial queue 8: while len(Q) > 0 do 9: i← Q[0] 10: V [i]← True 11: N = ID[V [N (i)]==False] 12: if len(N) == 0 then 13: POP (Q,−1) 14: break 15: end if 16: j←RC[N ] . Choose neighbor 17: V [j]← True 18: APPEND(Q, j) 19: APPEND(E, [i, j]) 20: end while 21: end while 22: end for 23: end procedure\nwhere wij = |{T ∈TG|(i,j)∈ET }|\nM denotes the proportion of times that the edge (i, j) appears. To optimize this objective, we can construct an estimator of the ELBO, based on the minibatch\nL̃MT ' L̃MMT = ∑ i∈VM LVM (xi)− ∑\n(i,j)∈EMT\nwijLEMT (xi,xj),\nwhere VM is the subset of documents, EMT is the subset of edges and\nLVM (xi) , Eqφ(zi|xi)[log pθ(xi|zi)] −KL (qφ(zi|xi)||pG(zi)) ;\nLEMT (xi,xj),KL(qφ(zi, zj |xi,xj)||pG(xi,xj)) −KL(qφ(zi|xi)||pG(zi))−KL(qφ(zj |xj)||pG(zj)) .\nThen we can update the parameters by using the gradient ∇φ,θL̃MMT . The training procedure is summarized in Algorithm 1."
    }, {
      "heading" : "B Tree Generation Algorithm",
      "text" : "Algorithm 2 shows the spanning tree generation algorithm TreeGen(·) used in our graph-induced generative document hashing model. TreeGen(·) utilizes a depth-first search (DFS) algorithm to generate meaningful neighborhood information for each node. In this algorithm, RC[·] means randomly\nchoosing one index according to the indicator function; ID[·] represents the set of node indexes satisfying the indicator condition and N (i) denotes the neighbors of node i. Due to the importance of edges precision, when choosing a neighbor (line 16 in Algorithm 2), instead of using uniform sampling, we exploit a temperature α to control the trade-off between the precision and diversity of edges. Specifically, the probability of sampling neighbor j of node i is\nexp(cos(xTj xi)/α)∑ n∈N (i) exp(cos(x T nxi)/α) .\nWe find the best configuration ofα on the validation set with the values in {0.1, 0.2, · · · , 1} ."
    }, {
      "heading" : "C Experiment Details",
      "text" : "For fair comparisons, we follow the experimental setting of VDSH. Specifically, the vocabulary size |V | is 7164, 20000, and 10000 for Reuters, TMC and 20Newsgroups, respectively. The split of training, validation, and test set is as follows: 7752, 967, 964 for Reuters; 21286, 3498, 3498 for TMC and 11016, 3667, 3668 for 20Newsgroups, respectively. Moreover, the KL term in Eq. (18) of the main paper is weighted with a coefficient β to avoid posterior collapse. We find the best configuration of β on the validation set with the values in {0.01, 0.02, · · · , 0.1}. To intuitively understand our model, we illustrate the whole architecture in Table 4."
    }, {
      "heading" : "D Additional Experiments",
      "text" : "Comparing with RBSH and PairRec As mentioned before, the reason we do not directly compare our method with RBSH (Hansen et al., 2019) and PairRec (Hansen et al., 2020) is that their data\nprocessing methods are different from the mainstream methods (e.g., VDSH, NASH, GMSH, NbrReg, AMMI and CorrSH). To further compare our method with them, we evaluate our model on three datasets that are published by RBSH4. The results are illustrated in Table 5. We observe that our method achieves the best performances in most experimental settings, which further confirms the superiority of simultaneously preserving the semantics and similarity information in a more principled framework.\nParameter Sensitivity To understand the robustness of our model, we conduct a parameter Sensitivity analysis of τ and β in Figure 4. Compared with β = 0 (without using neighborhood information), models with β 6= 0 improve performance significantly, but gradually performs steadily as β getting larger, which once again confirms the importance of simultaneously modeling semantic and neighborhood information. As for temperature coefficient τ used in variational encoder, our model performs steadily with various values of τ in the Reuters dataset. But in TMC and 20Newsgroups, increasing τ would deteriorate the model performance. Generally speaking, the model can achieve better performance with smaller τ (i.e., steeper sigmoid function). As we utilize 0.5 as the threshold value, steeper sigmoid functions make it easier to distinguish hash codes.\n4https://github.com/casperhansen/RBSH"
    } ],
    "references" : [ {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "Nicholas Léonard", "Aaron Courville." ],
      "venue" : "arXiv preprint arXiv:1308.3432.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep semantic text hashing with weak supervision",
      "author" : [ "Suthee Chaidaroon", "Travis Ebesu", "Yi Fang." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 1109–1112.",
      "citeRegEx" : "Chaidaroon et al\\.,? 2018",
      "shortCiteRegEx" : "Chaidaroon et al\\.",
      "year" : 2018
    }, {
      "title" : "Variational deep semantic hashing for text documents",
      "author" : [ "Suthee Chaidaroon", "Yi Fang." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 75–84.",
      "citeRegEx" : "Chaidaroon and Fang.,? 2017",
      "shortCiteRegEx" : "Chaidaroon and Fang.",
      "year" : 2017
    }, {
      "title" : "Document hashing with mixture-prior generative models",
      "author" : [ "Wei Dong", "Qinliang Su", "Dinghan Shen", "Changyou Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised neural generative semantic hashing",
      "author" : [ "Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in",
      "citeRegEx" : "Hansen et al\\.,? 2019",
      "shortCiteRegEx" : "Hansen et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised semantic hashing with pairwise reconstruction",
      "author" : [ "Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Develop-",
      "citeRegEx" : "Hansen et al\\.,? 2020",
      "shortCiteRegEx" : "Hansen et al\\.",
      "year" : 2020
    }, {
      "title" : "Locality preserving indexing for document representation",
      "author" : [ "Xiaofei He", "Deng Cai", "Haifeng Liu", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the 27th Annual International ACM SIGIR Conference on Research",
      "citeRegEx" : "He et al\\.,? 2004",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2004
    }, {
      "title" : "VisualRank: Applying PageRank to large-scale image search",
      "author" : [ "Yushi Jing", "Shumeet Baluja." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1877–1890.",
      "citeRegEx" : "Jing and Baluja.,? 2008",
      "shortCiteRegEx" : "Jing and Baluja.",
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Factorization meets the neighborhood: A multifaceted collaborative filtering model",
      "author" : [ "Yehuda Koren." ],
      "venue" : "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 426–434.",
      "citeRegEx" : "Koren.,? 2008",
      "shortCiteRegEx" : "Koren.",
      "year" : 2008
    }, {
      "title" : "Spectral hashing with semantically consistent graph for image indexing",
      "author" : [ "Peng Li", "Meng Wang", "Jian Cheng", "Changsheng Xu", "Hanqing Lu." ],
      "venue" : "IEEE Transactions on Multimedia, 15(1):141–152.",
      "citeRegEx" : "Li et al\\.,? 2012",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, pages 2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra." ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, pages 1278–1286.",
      "citeRegEx" : "Rezende et al\\.,? 2014",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey Hinton." ],
      "venue" : "Artificial Intelligence and Statistics, pages 448–455. The Proceedings of Machine Learning Research.",
      "citeRegEx" : "Salakhutdinov and Hinton.,? 2009a",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "Semantic hashing",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey Hinton." ],
      "venue" : "International Journal of Approximate Reasoning, 50(7):969–978.",
      "citeRegEx" : "Salakhutdinov and Hinton.,? 2009b",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "NASH: Toward end-to-end neural architecture for generative semantic hashing",
      "author" : [ "Dinghan Shen", "Qinliang Su", "Paidamoyo Chapfuwa", "Wenlin Wang", "Guoyin Wang", "Ricardo Henao", "Lawrence Carin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised binary representation learning with deep variational networks",
      "author" : [ "Yuming Shen", "Li Liu", "Ling Shao." ],
      "venue" : "International Journal of Computer Vision, 127(11):1614–1628.",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Strategies for retrieving plagiarized documents",
      "author" : [ "Benno Stein", "Sven Meyer zu Eissen", "Martin Potthast." ],
      "venue" : "Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 825–826.",
      "citeRegEx" : "Stein et al\\.,? 2007",
      "shortCiteRegEx" : "Stein et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning discrete structured representations by adversarially maximizing mutual information",
      "author" : [ "Karl Stratos", "Sam Wiseman." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, pages 9144–9154.",
      "citeRegEx" : "Stratos and Wiseman.,? 2020",
      "shortCiteRegEx" : "Stratos and Wiseman.",
      "year" : 2020
    }, {
      "title" : "Correlated variational auto-encoders",
      "author" : [ "Da Tang", "Dawen Liang", "Tony Jebara", "Nicholas Ruozzi." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, pages 6135–6144.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "Martin J Wainwright", "Michael Irwin Jordan." ],
      "venue" : "Now Publishers Inc.",
      "citeRegEx" : "Wainwright and Jordan.,? 2008",
      "shortCiteRegEx" : "Wainwright and Jordan.",
      "year" : 2008
    }, {
      "title" : "Spectral hashing",
      "author" : [ "Yair Weiss", "Antonio Torralba", "Rob Fergus." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1753–1760.",
      "citeRegEx" : "Weiss et al\\.,? 2009",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2009
    }, {
      "title" : "Self-taught hashing for fast similarity search",
      "author" : [ "Dell Zhang", "Jun Wang", "Deng Cai", "Jinsong Lu." ],
      "venue" : "Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 18–25.",
      "citeRegEx" : "Zhang et al\\.,? 2010",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Visual search at Alibaba",
      "author" : [ "Yanhao Zhang", "Pan Pan", "Yun Zheng", "Kang Zhao", "Yingya Zhang", "Xiaofeng Ren", "Rong Jin." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 993–1001.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Locality preserving hashing",
      "author" : [ "Kang Zhao", "Hongtao Lu", "Jincheng Mei." ],
      "venue" : "Twenty-eighth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhao et al\\.,? 2014",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    }, {
      "title" : "Generative semantic hashing enhanced via Boltzmann machines",
      "author" : [ "Lin Zheng", "Qinliang Su", "Dinghan Shen", "Changyou Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 777–788.",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Similarity search plays a pivotal role in a variety of tasks, such as image retrieval (Jing and Baluja, 2008; Zhang et al., 2018), plagiarism detection (Stein et al.",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Similarity search plays a pivotal role in a variety of tasks, such as image retrieval (Jing and Baluja, 2008; Zhang et al., 2018), plagiarism detection (Stein et al.",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : ", 2018), plagiarism detection (Stein et al., 2007) and recommendation systems (Koren, 2008).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : ", 2007) and recommendation systems (Koren, 2008).",
      "startOffset" : 35,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "Semantic hashing (Salakhutdinov and Hinton, 2009b) sidesteps this problem by learning a compact binary code for every item such that similar items can be efficiently found according to the Hamming distance of binary codes.",
      "startOffset" : 17,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Motivated by the success of deep generative models (Salakhutdinov and Hinton, 2009a; Kingma and Welling, 2013; Rezende et al., 2014) in unsupervised representation learning, many recent methods approach this problem from the perspective of deep generative models, leading to state-of-the-art performance on benchmark datasets.",
      "startOffset" : 51,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Motivated by the success of deep generative models (Salakhutdinov and Hinton, 2009a; Kingma and Welling, 2013; Rezende et al., 2014) in unsupervised representation learning, many recent methods approach this problem from the perspective of deep generative models, leading to state-of-the-art performance on benchmark datasets.",
      "startOffset" : 51,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "Motivated by the success of deep generative models (Salakhutdinov and Hinton, 2009a; Kingma and Welling, 2013; Rezende et al., 2014) in unsupervised representation learning, many recent methods approach this problem from the perspective of deep generative models, leading to state-of-the-art performance on benchmark datasets.",
      "startOffset" : 51,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "Specifically, these methods train a deep generative model to model the underlying documents and then use the trained generative model to extract continuous or binary representations from the original documents (Chaidaroon and Fang, 2017; Shen et al., 2018; Dong et al., 2019; Zheng et al., 2020).",
      "startOffset" : 210,
      "endOffset" : 295
    }, {
      "referenceID" : 16,
      "context" : "Specifically, these methods train a deep generative model to model the underlying documents and then use the trained generative model to extract continuous or binary representations from the original documents (Chaidaroon and Fang, 2017; Shen et al., 2018; Dong et al., 2019; Zheng et al., 2020).",
      "startOffset" : 210,
      "endOffset" : 295
    }, {
      "referenceID" : 3,
      "context" : "Specifically, these methods train a deep generative model to model the underlying documents and then use the trained generative model to extract continuous or binary representations from the original documents (Chaidaroon and Fang, 2017; Shen et al., 2018; Dong et al., 2019; Zheng et al., 2020).",
      "startOffset" : 210,
      "endOffset" : 295
    }, {
      "referenceID" : 26,
      "context" : "Specifically, these methods train a deep generative model to model the underlying documents and then use the trained generative model to extract continuous or binary representations from the original documents (Chaidaroon and Fang, 2017; Shen et al., 2018; Dong et al., 2019; Zheng et al., 2020).",
      "startOffset" : 210,
      "endOffset" : 295
    }, {
      "referenceID" : 6,
      "context" : "By constructing an adjacency matrix from the raw features of documents, neighborbased methods seek to preserve the information in the constructed adjacency matrix, such as the locality-preserving hashing (He et al., 2004; Zhao et al., 2014), spectral hashing (Weiss et al.",
      "startOffset" : 204,
      "endOffset" : 240
    }, {
      "referenceID" : 25,
      "context" : "By constructing an adjacency matrix from the raw features of documents, neighborbased methods seek to preserve the information in the constructed adjacency matrix, such as the locality-preserving hashing (He et al., 2004; Zhao et al., 2014), spectral hashing (Weiss et al.",
      "startOffset" : 204,
      "endOffset" : 240
    }, {
      "referenceID" : 22,
      "context" : ", 2014), spectral hashing (Weiss et al., 2009; Li et al., 2012), and etc.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : ", 2014), spectral hashing (Weiss et al., 2009; Li et al., 2012), and etc.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "Other works (Shen et al., 2019; Hansen et al., 2019) use an extra loss term, derived from the approximate neighborhood information, to encourage similar documents to produce similar codes.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "Other works (Shen et al., 2019; Hansen et al., 2019) use an extra loss term, derived from the approximate neighborhood information, to encourage similar documents to produce similar codes.",
      "startOffset" : 12,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Semantics-Based Hashing Due to the similarities among the underlying ideas of these methods, we take the variational deep semantic hashing (VDSH) (Chaidaroon and Fang, 2017) as an example to illustrate their working flow.",
      "startOffset" : 146,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "fined on a tree, as proved in (Wainwright and Jordan, 2008), it is guaranteed to be a valid probability distribution, and more importantly, it satisfies the following two relations: i) pT (zi) = pG(zi); ii) pT (zi, zj) = pG(zi, zj) for any (i, j) ∈ ET , where pT (zi) and pT (zi, zj) denote the marginal distributions of pT (Z).",
      "startOffset" : 30,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "But as proved in (Tang et al., 2019), by using the log-sum inequality, LMT can be further lower bounded by",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "Deep generative models (Rezende et al., 2014) have attracted a lot of attention in semanticsbased hashing, due to their successes in unsupervised representation learning.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "VDSH (Chaidaroon and Fang, 2017) first employed variational autoencoder (VAE) (Kingma and Welling, 2013) to learn continuous representations of documents and then casts them into binary codes.",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "VDSH (Chaidaroon and Fang, 2017) first employed variational autoencoder (VAE) (Kingma and Welling, 2013) to learn continuous representations of documents and then casts them into binary codes.",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "NASH (Shen et al., 2018) tackled this issue by replacing the Gaussian prior with Bernoulli and adopted the straight-through technique (Bengio et al.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", 2018) tackled this issue by replacing the Gaussian prior with Bernoulli and adopted the straight-through technique (Bengio et al., 2013) to achieve end-to-end training.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "Beyond generative frameworks, AMMI (Stratos and Wiseman, 2020) achieved superior performance by maximizing the mutual information between codes and documents.",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Spectral hashing (Weiss et al., 2009) and selftaught hashing (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : ", 2009) and selftaught hashing (Zhang et al., 2010) are two typical methods of neighbor-based hashing models.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Recently, some VAE-based models tried to concurrently take account of semantic and neighborhood information, such as NbrReg (Chaidaroon et al., 2018), RBSH (Hansen et al.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : ", 2018), RBSH (Hansen et al., 2019) and PairRec(Hansen et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "Baselines We compare our method with the following models: SpH (Weiss et al., 2009), STH (Zhang et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : ", 2009), STH (Zhang et al., 2010), VDSH (Chaidaroon and Fang, 2017), NASH (Shen et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : ", 2010), VDSH (Chaidaroon and Fang, 2017), NASH (Shen et al., 2018), GMSH(Dong et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : ", 2018), GMSH(Dong et al., 2019), NbrReg (Chaidaroon et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : ", 2019), NbrReg (Chaidaroon et al., 2018), CorrSH (Zheng et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : ", 2018), CorrSH (Zheng et al., 2020) and AMMI (Stratos and Wiseman, 2020).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "The model is trained using the Adam optimizer (Kingma and Ba, 2014).",
      "startOffset" : 46,
      "endOffset" : 67
    } ],
    "year" : 2021,
    "abstractText" : "With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both semantics and neighborhood information are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model. To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning. Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.1",
    "creator" : "LaTeX with hyperref"
  }
}