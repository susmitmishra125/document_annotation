{
  "name" : "2021.acl-long.149.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Survey of Race, Racism, and Anti-Racism in NLP",
    "authors" : [ "Anjalie Field", "Su Lin Blodgett", "Yulia Tsvetkov" ],
    "emails" : [ "anjalief@cs.cmu.edu", "sulin.blodgett@microsoft.com", "z.w.butt@sheffield.ac.uk", "yuliats@cs.washington.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1905–1925\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1905"
    }, {
      "heading" : "1 Introduction",
      "text" : "Race and language are tied in complicated ways. Raciolinguistics scholars have studied how they are mutually constructed: historically, colonial powers construct linguistic and racial hierarchies to justify violence, and currently, beliefs about the inferiority of racialized people’s language practices continue to justify social and economic exclusion (Rosa and Flores, 2017).1 Furthermore, language is the primary means through which stereotypes and prejudices are communicated and perpetuated (Hamilton and Trolier, 1986; Bar-Tal et al., 2013).\nHowever, questions of race and racial bias have been minimally explored in NLP literature.\n1We use racialization to refer the process of “ascribing and prescribing a racial category or classification to an individual or group of people . . . based on racial attributes including but not limited to cultural and social history, physical features, and skin color” (Hudley, 2017).\nWhile researchers and activists have increasingly drawn attention to racism in computer science and academia, frequently-cited examples of racial bias in AI are often drawn from disciplines other than NLP, such as computer vision (facial recognition) (Buolamwini and Gebru, 2018) or machine learning (recidivism risk prediction) (Angwin et al., 2016). Even the presence of racial biases in search engines like Google (Sweeney, 2013; Noble, 2018) has prompted little investigation in the ACL community. Work on NLP and race remains sparse, particularly in contrast to concerns about gender bias, which have led to surveys, workshops, and shared tasks (Sun et al., 2019; Webster et al., 2019).\nIn this work, we conduct a comprehensive survey of how NLP literature and research practices engage with race. We first examine 79 papers from the ACL Anthology that mention the words ‘race’, ‘racial’, or ‘racism’ and highlight examples of how racial biases manifest at all stages of NLP model pipelines (§3). We then describe some of the limitations of current work (§4), specifically showing that NLP research has only examined race in a narrow range of tasks with limited or no social context. Finally, in §5, we revisit the NLP pipeline with a focus on how people generate data, build models, and are affected by deployed systems, and we highlight current failures to engage with people traditionally underrepresented in STEM and academia.\nWhile little work has examined the role of race in NLP specifically, prior work has discussed race in related fields, including human-computer interaction (HCI) (Ogbonnaya-Ogburu et al., 2020; Rankin and Thomas, 2019; Schlesinger et al., 2017), fairness in machine learning (Hanna et al., 2020), and linguistics (Hudley et al., 2020; Motha, 2020). We draw comparisons and guidance from this work and show its relevance to NLP research. Our work differs from NLP-focused related work on gender bias (Sun et al., 2019), ‘bias’ generally\n(Blodgett et al., 2020), and the adverse impacts of language models (Bender et al., 2021) in its explicit focus on race and racism.\nIn surveying research in NLP and related fields, we ultimately find that NLP systems and research practices produce differences along racialized lines. Our work calls for NLP researchers to consider the social hierarchies upheld and exacerbated by NLP research and to shift the field toward “greater inclusion and racial justice” (Hudley et al., 2020)."
    }, {
      "heading" : "2 What is race?",
      "text" : "It has been widely accepted by social scientists that race is a social construct, meaning it “was brought into existence or shaped by historical events, social forces, political power, and/or colonial conquest” rather than reflecting biological or ‘natural’ differences (Hanna et al., 2020). More recent work has criticized the “social construction” theory as circular and rooted in academic discourse, and instead referred to race as “colonial constituted practices”, including “an inherited western, modern-colonial practice of violence, assemblage, superordination, exploitation and segregation” (Saucier et al., 2016).\nThe term race is also multi-dimensional and can refer to a variety of different perspectives, including racial identity (how you self-identify), observed race (the race others perceive you to be), and reflected race (the race you believe others perceive you to be) (Roth, 2016; Hanna et al., 2020; Ogbonnaya-Ogburu et al., 2020). Racial categorizations often differ across dimensions and depend on the defined categorization schema. For example, the United States census considers Hispanic an ethnicity, not a race, but surveys suggest that 2/3 of people who identify as Hispanic consider it a part of their racial background.2 Similarly, the census does not consider ‘Jewish’ a race, but some NLP work considers anti-Semitism a form of racism (Hasanuzzaman et al., 2017). Race depends on historical and social context—there are no ‘ground truth’ labels or categories (Roth, 2016).\nAs the work we survey primarily focuses on the United States, our analysis similarly focuses on the U.S. However, as race and racism are global constructs, some aspects of our analysis are applicable to other contexts. We suggest that future studies on racialization in NLP ground their analysis in the appropriate geo-cultural context, which may result\n2https://bit.ly/3r9J1fO, https://pewrsr. ch/36vlUEl\nin findings or analyses that differ from our work."
    }, {
      "heading" : "3 Survey of NLP literature on race",
      "text" : ""
    }, {
      "heading" : "3.1 ACL Anthology papers about race",
      "text" : "In this section, we introduce our primary survey data—papers from the ACL Anthology3—and we describe some of their major findings to emphasize that NLP systems encode racial biases. We searched the anthology for papers containing the terms ‘racial’, ‘racism’, or ‘race’, discarding ones that only mentioned race in the references section or in data examples and adding related papers cited by the initial set if they were also in the ACL Anthology. In using keyword searches, we focus on papers that explicitly mention race and consider papers that use euphemistic terms to not have substantial engagement on this topic. As our focus is on NLP and the ACL community, we do not include NLP-related papers published in other venues in the reported metrics (e.g. Table 1), but we do draw from them throughout our analysis.\nOur initial search identified 165 papers. However, reviewing all of them revealed that many do not deeply engage on the topic. For example, 37 papers mention ‘racism’ as a form of abusive language or use ‘racist’ as an offensive/hate speech label without further engagement. 30 papers only mention race as future work, related work, or motivation, e.g. in a survey about gender bias, “Nonbinary genders as well as racial biases have largely been ignored in NLP” (Sun et al., 2019). After discarding these types of papers, our final analysis set consists of 79 papers.4\nTable 1 provides an overview of the 79 papers, manually coded for each paper’s primary NLP task and its focal goal or contribution. We determined task/application labels through an iterative process: listing the main focus of each paper and then collapsing similar categories. In cases where papers could rightfully be included in multiple categories, we assign them to the best-matching one based on stated contributions and the percentage of the paper devoted to each possible category. In the Appendix we provide additional categorizations of the papers\n3The ACL Anthology includes papers from all official ACL venues and some non-ACL events listed in Appendix A, as of December 2020 it included 6, 200 papers\n4We do not discard all papers about abusive language, only ones that exclusively use racism/racist as a classification label. We retain papers with further engagement, e.g. discussions of how to define racism or identification of racial bias in hate speech classifiers.\naccording to publication year, venue, and racial categories used, as well as the full list of 79 papers."
    }, {
      "heading" : "3.2 NLP systems encode racial bias",
      "text" : "Next, we present examples that identify racial bias in NLP models, focusing on 5 parts of a standard NLP pipeline: data, data labels, models, model outputs, and social analyses of outputs. We include papers described in Table 1 and also relevant literature beyond the ACL Anthology (e.g. NeurIPS, PNAS, Science). These examples are not intended to be exhaustive, and in §4 we describe some of the ways that NLP literature has failed to engage with race, but nevertheless, we present them as evidence that NLP systems perpetuate harmful biases along racialized lines.\nData A substantial amount of prior work has already shown how NLP systems, especially word embeddings and language models, can absorb and amplify social biases in data sets (Bolukbasi et al., 2016; Zhao et al., 2017). While most work focuses on gender bias, some work has made similar observations about racial bias (Rudinger et al., 2017; Garg et al., 2018; Kurita et al., 2019). These studies focus on how training data might describe racial minorities in biased ways, for example, by examining words associated with terms like ‘black’ or traditionally European/African American names (Caliskan et al., 2017; Manzini et al., 2019). Some studies additionally capture who is described, revealing under-representation in training data, sometimes tangentially to primary research questions: Rudinger et al. (2017) suggest that gender bias may be easier to identify than racial or ethnic bias in Natural Language Inference data sets because of\ndata sparsity, and Caliskan et al. (2017) alter the Implicit Association Test stimuli that they use to measure biases in word embeddings because some African American names were not frequent enough in their corpora.\nAn equally important consideration, in addition to whom the data describes is who authored the data. For example, Blodgett et al. (2018) show that parsing systems trained on White Mainstream American English perform poorly on African American English (AAE).5 In a more general example, Wikipedia has become a popular data source for many NLP tasks. However, surveys suggest that Wikipedia editors are primarily from whitemajority countries,6 and several initiatives have pointed out systemic racial biases in Wikipedia coverage (Adams et al., 2019; Field et al., 2021).7 Models trained on these data only learn to process the type of text generated by these users, and further, only learn information about the topics these users are interested in. The representativeness of data sets is a well-discussed issue in social-oriented tasks, like inferring public opinion (Olteanu et al., 2019), but this issue is also an important consideration in ‘neutral’ tasks like parsing (Waseem et al., 2021). The type of data that researchers choose to train their models on does not just affect what data the models perform well for, it affects what people the models work for. NLP researchers cannot assume models will be useful or function for marginalized people unless they are trained on data\n5We note that conceptualizations of AAE and the accompanying terminology for the variety have shifted considerably in the last half century; see King (2020) for an overview.\n6https://bit.ly/2Yv07IL 7https://bit.ly/3j2weZA\ngenerated by them.\nData Labels Although model biases are often blamed on raw data, several of the papers we survey identify biases in the way researchers categorize or obtain data annotations. For example: • Annotation schema Returning to Blodgett\net al. (2018), this work defines new parsing standards for formalisms common in AAE, demonstrating how parsing labels themselves were not designed for racialized language varieties. • Annotation instructions Sap et al. (2019)\nshow that annotators are less likely to label tweets using AAE as offensive if they are told the likely language varieties of the tweets. Thus, how annotation schemes are designed (e.g. what contextual information is provided) can impact annotators’ decisions, and failing to provide sufficient context can result in racial biases. • Annotator selection Waseem (2016) show\nthat feminist/anti-racist activists assign different offensive language labels to tweets than figure-eight workers, demonstrating that annotators’ lived experiences affect data annotations.\nModels Some papers have found evidence that model instances or architectures can change the racial biases of outputs produced by the model. Sommerauer and Fokkens (2019) find that the word embedding associations around words like ‘race’ and ‘racial’ change not only depending on the model architecture used to train embeddings, but also on the specific model instance used to extract them, perhaps because of differing random seeds. Kiritchenko and Mohammad (2018) examine gender and race biases in 200 sentiment analysis systems submitted to a shared task and find different levels of bias in different systems. As the training data for the shared task was standardized, all models were trained on the same data. However, participants could have used external training data or pre-trained embeddings, so a more detailed investigation of results is needed to ascertain which factors most contribute to disparate performance.\nModel Outputs Several papers focus on model outcomes, and how NLP systems could perpetuate and amplify bias if they are deployed: • Classifiers trained on common abusive lan-\nguage data sets are more likely to label tweets\ncontaining characteristics of AAE as offensive (Davidson et al., 2019; Sap et al., 2019). • Classifiers for abusive language are more\nlikely to label text containing identity terms like ‘black’ as offensive (Dixon et al., 2018). • GPT outputs text with more negative senti-\nment when prompted with AAE -like inputs (Groenwold et al., 2020).\nSocial Analyses of Outputs While the examples in this section primarily focus on racial biases in trained NLP systems, other work (e.g. included in ‘Social Science/Social Media’ in Table 1) uses NLP tools to analyze race in society. Examples include examining how commentators describe football players of different races (Merullo et al., 2019) or how words like ‘prejudice’ have changed meaning over time (Vylomova et al., 2019).\nWhile differing in goals, this work is often susceptible to the same pitfalls as other NLP tasks. One area requiring particular caution is in the interpretation of results produced by analysis models. For example, while word embeddings have become a common way to measure semantic change or estimate word meanings (Garg et al., 2018), Joseph and Morgan (2020) show that embedding associations do not always correlate with human opinions; in particular, correlations are stronger for beliefs about gender than race. Relatedly, in HCI, the recognition that authors’ own biases can affect their interpretations of results has caused some authors to provide self-disclosures (Schlesinger et al., 2017), but this practice is uncommon in NLP.\nWe conclude this section by observing that when researchers have looked for racial biases in NLP systems, they have usually found them. This literature calls for proactive approaches in considering how data is collected, annotated, used, and interpreted to prevent NLP systems from exacerbating historical racial hierarchies."
    }, {
      "heading" : "4 Limitations in where and how NLP operationalizes race",
      "text" : "While §3 demonstrates ways that NLP systems encode racial biases, we next identify gaps and limitations in how these works have examined racism, focusing on how and in what tasks researchers have considered race. We ultimately conclude that prior NLP literature has marginalized research on race and encourage deeper engagement with other fields, critical views of simplified classification schema,\nand broader application scope in future work (Blodgett et al., 2020; Hanna et al., 2020)."
    }, {
      "heading" : "4.1 Common data sets are narrow in scope",
      "text" : "The papers we surveyed suggest that research on race in NLP has used a very limited range of data sets, which fails to account for the multidimensionality of race and simplifications inherent in classification. We identified 3 common data sources:8\n• 9 papers use a set of tweets with inferred probabilistic topic labels based on alignment with U.S. census race/ethnicity groups (or the provided inference model) (Blodgett et al., 2016). • 11 papers use lists of names drawn from\nSweeney (2013), Caliskan et al. (2017), or Garg et al. (2018). Most commonly, 6 papers use African/European American names from the Word Embedding Association Test (WEAT) (Caliskan et al., 2017), which in turn draws data from Greenwald et al. (1998) and Bertrand and Mullainathan (2004). • 10 papers use explicit keywords like ‘Black\nwoman’, often placed in templates like “I am a ” to test if model performance remains\nthe same for different identity terms. While these commonly-used data sets can identify performance disparities, they only capture a narrow subset of the multiple dimensions of race (§2). For example, none of them capture selfidentified race. While observed race is often appropriate for examining discrimination and some types of disparities, it is impossible to assess potential harms and benefits of NLP systems without assessing their performance over text generated by and directed to people of different races. The corpus from Blodgett et al. (2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.\nFurthermore, names and hand-selected identity terms are not sufficient for uncovering model bias. De-Arteaga et al. (2019) show this in examining gender bias in occupation classification: when overt indicators like names and pronouns are scrubbed from the data, performance gaps and potential allocational harms still remain. Names also\n8We provide further counts of what racial categories papers use and how they operationalize them in Appendix B.\ngeneralize poorly. While identity terms can be examined across languages (van Miltenburg et al., 2017), differences in naming conventions often do not translate, leading some studies to omit examining racial bias in non-English languages (Lauscher and Glavaš, 2019). Even within English, names often fail to generalize across domains, geographies, and time. For example, names drawn from the U.S. census generalize poorly to Twitter (WoodDoughty et al., 2018), and names common among Black and white children were not distinctly different prior to the 1970s (Fryer Jr and Levitt, 2004; Sweeney, 2013).\nWe focus on these 3 data sets as they were most common in the papers we surveyed, but we note that others exist. Preoţiuc-Pietro and Ungar (2018) provide a data set of tweets with self-identified race of their authors, though it is little used in subsequent work and focused on demographic prediction, rather than evaluating model performance gaps. Two recently-released data sets (Nadeem et al., 2020; Nangia et al., 2020) provide crowd-sourced pairs of more- and less-stereotypical text. More work is needed to understand any privacy concerns and the strengths and limitations of these data (Blodgett et al., 2021). Additionally, some papers collect domain-specific data, such as self-reported race in an online community (Loveys et al., 2018), or crowd-sourced annotations of perceived race of football players (Merullo et al., 2019). While these works offer clear contextualization, it is difficult to use these data sets to address other research questions."
    }, {
      "heading" : "4.2 Classification schemes operationalize race as a fixed, single-dimensional U.S.-census label",
      "text" : "Work that uses the same few data sets inevitably also uses the same few classification schemes, often without justification. The most common explicitly stated source of racial categories is the U.S. census, which reflects the general trend of U.S.-centrism in NLP research (the vast majority of work we surveyed also focused on English). While census categories are sometimes appropriate, repeated use of classification schemes and accompanying data sets without considering who defined these schemes and whether or not they are appropriate for the current context risks perpetuating the misconception that race is ‘natural’ across geo-cultural contexts. We refer to Hanna et al. (2020) for a more thorough\noverview of the harms of “widespread uncritical adoption of racial categories,” which “can in turn re-entrench systems of racial stratification which give rise to real health and social inequalities.” At best, the way race has been operationalized in NLP research is only capable of examining a narrow subset of potential harms. At worst, it risks reinforcing racism by presenting racial divisions as natural, rather than the product of social and historical context (Bowker and Star, 2000).\nAs an example of questioning who devised racial categories and for what purpose, we consider the pattern of re-using names from Greenwald et al. (1998), who describe their data as sets of names “judged by introductory psychology students to be more likely to belong to White Americans than to Black Americans” or vice versa. When incorporating this data into WEAT, Caliskan et al. (2017) discard some judged African American names as too infrequent in their embedding data. Work subsequently drawing from WEAT makes no mention of the discarded names nor contains much discussion of how the data was generated and whether or not names judged to be white or Black by introductory psychology students in 1998 are an appropriate benchmark for the studied task. While gathering data to examine race in NLP is challenging, and in this work we ourselves draw from examples that use Greenwald et al. (1998), it is difficult to interpret what implications arise when models exhibit disparities over this data and to what extent models without disparities can be considered ‘debiased’.\nFinally, almost all of the work we examined conducts single-dimensional analyses, e.g. focus on race or gender but not both simultaneously. This focus contrasts with the concept of intersectionality, which has shown that examining discrimination along a single axis fails to capture the experiences of people who face marginalization along multiple axes. For example, consideration of race often emphasizes the experience of genderprivileged people (e.g. Black men), while consideration of gender emphasizes the experience of race-privileged people (e.g. white women). Neither reflect the experience of people who face discrimination along both axes (e.g. Black women) (Crenshaw, 1989). A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in\nany other NLP research areas. Further, several of these papers use NLP technology to examine or validate theories on intersectionality; they do not draw from theory on intersectionality to critically examine NLP models. These omissions can mask harms: Jiang and Fellbaum (2020) provide an example using word embeddings of how failing to consider intersectionality can render invisible people marginalized in multiple ways. Numerous directions remain for exploration, such as how ‘debiasing’ models along one social dimension affects other dimensions. Surveys in HCI offer further frameworks on how to incorporate identity and intersectionality into computational research (Schlesinger et al., 2017; Rankin and Thomas, 2019)."
    }, {
      "heading" : "4.3 NLP research on race is restricted to specific tasks and applications",
      "text" : "Finally, Table 1 reveals many common NLP applications where race has not been examined, such as machine translation, summarization, or question answering.9 While some tasks seem inherently more relevant to social context than others (a claim we dispute in this work, particularly in §5), research on race is compartmentalized to limited areas of NLP even in comparison with work on ‘bias’. For example, Blodgett et al. (2020) identify 20 papers that examine bias in co-reference resolution systems and 8 in machine translation, whereas we identify 0 papers in either that consider race. Instead, race is most often mentioned in NLP papers in the context of abusive language, and work on detecting or removing bias in NLP models has focused on word embeddings.\nOverall, our survey identifies a need for the examination of race in a broader range of NLP tasks, the development of multi-dimensional data sets, and careful consideration of context and appropriateness of racial categories. In general, race is difficult to operationalize, but NLP researchers do not need to start from scratch, and can instead draw from relevant work in other fields."
    }, {
      "heading" : "5 NLP propagates marginalization of racialized people",
      "text" : "While in §4 we primarily discuss race as a topic or a construct, in this section, we consider the role, or more pointedly, the absence, of traditionally underrepresented people in NLP research.\n9We identified only 8 relevant papers on Text Generation, which focus on other areas including chat bots, GPT-2/3, humor generation, and story generation."
    }, {
      "heading" : "5.1 People create data",
      "text" : "As discussed in §3.2, data and annotations are generated by people, and failure to consider who created data can lead to harms. In §3.2 we identify a need for diverse training data in order to ensure models work for a diverse set of people, and in §4 we describe a similar need for diversity in data that is used to assess algorithmic fairness. However, gathering this type of data without consideration of the people who generated it can introduce privacy violations and risks of demographic profiling.\nAs an example, in 2019, partially in response to research showing that facial recognition algorithms perform worse on darker-skinned than lighter-skinned people (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019), researchers at IBM created the “Diversity in Faces” data set, which consists of 1 million photos sampled from the the publicly available YFCC-100M data set and annotated with “craniofacial distances, areas and ratios, facial symmetry and contrast, skin color, age and gender predictions” (Merler et al., 2019). While this data set aimed to improve the fairness of facial recognition technology, it included photos collected from a Flickr, a photo-sharing website whose users did not explicitly consent for this use of their photos. Some of these users filed a lawsuit against IBM, in part for “subjecting them to increased surveillance, stalking, identity theft, and other invasions of privacy and fraud.”10 NLP researchers could easily repeat this incident, for example, by using demographic profiling of social media users to create more diverse data sets. While obtaining diverse, representative, real-world data sets is important for building models, data must be collected with consideration for the people who generated it, such as obtaining informed consent, setting limits of uses, and preserving privacy, as well as recognizing that some communities may not want their data used for NLP at all (Paullada, 2020)."
    }, {
      "heading" : "5.2 People build models",
      "text" : "Research is additionally carried out by people who determine what projects to pursue and how to approach them. While statistics on ACL conferences and publications have focused on geographic\n10https://bit.ly/3r3LuIk https://nbcnews.to/3j5hI39 IBM has since removed the “Diversity in Faces” data set as well as their “Detect Faces” public API and stopped their use of and research on facial recognition. https://bit.ly/3j2Jv4i\nrepresentation rather than race, they do highlight under-representation. Out of 2, 695 author affiliations associated with papers in the ACL Anthology for 5 major conferences held in 2018, only 5 (0.2%) were from Africa, compared with 1, 114 from North America (41.3%).11 Statistics published for 2017 conference attendees and ACL fellows similarly reveal a much higher percentage of people from “North, Central and South America” (55% attendees / 74% fellows) than from “Europe, Middle East and Africa” (19%/13%) or “AsiaPacific” (23%/13%).12 These broad regional categories likely mask further under-representation, e.g. percentage of attendees and fellows from Africa as compared to Europe. According to an NSF report that includes racial statistics rather than nationality, 14% of doctorate degrees in Computer Science awarded by U.S. institutions to U.S. citizens and permanent residents were awarded to Asian students, < 4% to Black or African American students, and 0% to American Indian or Alaska Native students (National Center for Science and Engineering Statistics, 2019).13\nIt is difficult to envision reducing or eliminating racial differences in NLP systems without changes in the researchers building these systems. One theory that exemplifies this challenge is interest convergence, which suggests that people in positions of power only take action against systematic problems like racism when it also advances their own interests (Bell Jr, 1980). Ogbonnaya-Ogburu et al. (2020) identify instances of interest convergence in the HCI community, primarily in diversity initiatives that benefit institutions’ images rather than underrepresented people. In a research setting, interest convergence can encourage studies of incremental and surface-level biases while discouraging research that might be perceived as controversial and force fundamental changes in the field.\nDemographic statistics are not sufficient for avoiding pitfalls like interest convergence, as they fail to capture the lived experiences of researchers. Ogbonnaya-Ogburu et al. (2020) provide several examples of challenges that non-white HCI researchers have faced, including the invisible labor of representing ‘diversity’, everyday microaggres-\n11http://www.marekrei.com/blog/ geographic-diversity-of-nlp-conferences/\n12https://www.aclweb.org/portal/ content/acl-diversity-statistics\n13Results exclude respondents who did not report race or ethnicity or were Native Hawaiian or Other Pacific Islander.\nsions, and altering their research directions in accordance with their advisors’ interests. Rankin and Thomas (2019) further discuss how research conducted by people of different races is perceived differently: “Black women in academia who conduct research about the intersections of race, gender, class, and so on are perceived as ‘doing service,’ whereas white colleagues who conduct the same research are perceived as doing cutting-edge research that demands attention and recognition.” While we draw examples about race from HCI in the absence of published work on these topics in NLP, the lack of linguistic diversity in NLP research similarly demonstrates how representation does not necessarily imply inclusion. Although researchers from various parts of the world (Asia, in particular) do have some numerical representation among ACL authors, attendees, and fellows, NLP research overwhelmingly favors a small set of languages, with a heavy skew towards European languages (Joshi et al., 2020) and ‘standard’ language varieties (Kumar et al., 2021)."
    }, {
      "heading" : "5.3 People use models",
      "text" : "Finally, NLP research produces technology that is used by people, and even work without direct applications is typically intended for incorporation into application-based systems. With the recognition that technology ultimately affects people, researchers on ethics in NLP have increasingly called for considerations of whom technology might harm and suggested that there are some NLP technologies that should not be built at all. In the context of perpetuating racism, examples include criticism of tools for predicting demographic information (Tatman, 2020) and automatic prison term prediction (Leins et al., 2020), motivated by the history of using technology to police racial minorities and related criticism in other fields (Browne, 2015; Buolamwini and Gebru, 2018; McIlwain, 2019). In cases where potential harms are less direct, they are often unaddressed entirely. For example, while low-resource NLP is a large area of research, a paper on machine translation of white American and European languages is unlikely to discuss how continual model improvements in these settings increase technological inequality. Little work on lowresource NLP has focused on the realities of structural racism or differences in lived experience and how they might affect the way technology should be designed.\nDetection of abusive language offers an informative case study on the danger of failing to consider people affected by technology. Work on abusive language often aims to detect racism for content moderation (Waseem and Hovy, 2016). However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like ‘black’ or text containing linguistic markers of AAE as toxic (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Xia et al., 2020). Deploying these models could censor the posts of the very people they purport to help.\nIn other areas of statistics and machine learning, focus on participatory design has sought to amplify the voices of people affected by technology and its development. An ICML 2020 workshop titled “Participatory Approaches to Machine Learning” highlights a number of papers in this area (Kulynych et al., 2020; Brown et al., 2019). A few related examples exist in NLP, e.g. Gupta et al. (2020) gather data for an interactive dialogue agent intended to provide more accessible information about heart failure to Hispanic/Latinx and African American patients. The authors engage with healthcare providers and doctors, though they leave focal groups with patients for future work. While NLP researchers may not be best situated to examine how people interact with deployed technology, they could instead draw motivation from fields that have stronger histories of participatory design, such as HCI. However, we did not identify citing participatory design studies conducted by others as common practice in the work we surveyed. As in the case of researcher demographics, participatory design is not an end-all solution. Sloane et al. (2020) provide a discussion of how participatory design can collapse to ‘participation-washing’ and how such work must be context-specific, long-term, and genuine."
    }, {
      "heading" : "6 Discussion",
      "text" : "We conclude by synthesizing some of the observations made in the preceding sections into more actionable items. First, NLP research needs to explicitly incorporate race. We quote Benjamin (2019): “[technical systems and social codes] operate within powerful systems of meaning that render some things visible, others invisible, and create a vast array of distortions and dangers.”\nIn the context of NLP research, this philosophy implies that all technology we build works in service of some ideas or relations, either by upholding\nthem or dismantling them. Any research that is not actively combating prevalent social systems like racism risks perpetuating or exacerbating them. Our work identifies several ways in which NLP research upholds racism: • Systems contain representational harms and\nperformance gaps throughout NLP pipelines • Research on race is restricted to a narrow sub-\nset of tasks and definitions of race, which can mask harms and falsely reify race as ‘natural’ • Traditionally underrepresented people are ex-\ncluded from the research process, both as consumers and producers of technology\nFurthermore, while we focus on race, which we note has received substantially less attention than gender, many of the observations in this work hold for social characteristics that have received even less attention in NLP research, such as socioeconomic class, disability, or sexual orientation (Mendelsohn et al., 2020; Hutchinson et al., 2020).\nNevertheless, none of these challenges can be addressed without direct engagement with marginalized communities of color. NLP researchers can draw on precedents for this type of engagement from other fields, such as participatory design and value sensitive design models (Friedman et al., 2013). Additionally, numerous organizations already exist that serve as starting points for partnerships, such as Black in AI, Masakhane, Data for Black Lives, and the Algorithmic Justice League.\nFinally, race and language are complicated, and while readers may look for clearer recommendations, no one data set, model, or set of guidelines can ‘solve’ racism in NLP. For instance, while we draw from linguistics, Hudley et al. (2020) in turn call on linguists to draw models of racial justice from anthropology, sociology, and psychology. Relatedly, there are numerous racialized effects that NLP research can have that we do not address in this work; for example, Bender et al. (2021) and Strubell et al. (2019) discuss the environmental costs of training large language models, and how global warming disproportionately affects marginalized communities. We suggest that readers use our work as one starting point for bringing inclusion and racial justice into NLP."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully thank Hanna Kim, Kartik Goyal, Artidoro Pagnoni, Qinlan Shen, and Michael Miller Yoder for their feedback on this work. Z.W. has\nbeen supported in part by the Canada 150 Research Chair program and the UK-Canada Artificial Intelligence Initiative. A.F. has been supported in part by a Google PhD Fellowship and a GRFP under Grant No. DGE1745016. This material is based upon work supported in part by the National Science Foundation under Grants No. IIS2040926 and IIS2007960. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF."
    }, {
      "heading" : "7 Ethical Considerations",
      "text" : "We, the authors of this work, are situated in the cultural contexts of the United States of America and the United Kingdom/Europe, and some of us identify as people of color. We all identify as NLP researchers, and we acknowledge that we are situated within the traditionally exclusionary practices of academic research. These perspectives have impacted our work, and there are viewpoints outside of our institutions and experiences that our work may not fully represent."
    }, {
      "heading" : "A ACL Anthology Venues",
      "text" : "ACL events: AACL, ACL, ANLP, CL, CoNLL, EACL, EMNLP, Findings, NAACL, SemEval, *SEM, TACL, WMT, Workshops, Special Interest Groups\nNon-ACL events: ALTA, AMTA, CCL, COLING, EAMT, HLT, IJCNLP, JEP/TALN/RECITAL, LILT, LREC, MUC, PACLIC, RANLP, ROCLING/IJCLCLP, TINLAP, TIPSTER"
    }, {
      "heading" : "B Additional Survey Metrics",
      "text" : "We show three additional breakdowns of the data set: Figure 1 shows the number of papers published each year, Figure 2 shows the number of papers published in each venue, and Table 2 shows how papers have operationalized race. As expected, given the growth of NLP research in general and the increasing focus on social issues (e.g. “Ethics and NLP” track was added to ACL in 2020) more work has been published on race in more recent years (2019, 2020). In Figure 2, we consider if work on race has been siloed into or out of specific\nvenues. The majority of papers were published in workshops, which is consist with the large number of workshop papers. In 2019, approximately 2,038 papers were published in workshops14 and 1,680 papers were published in conferences (ACL, EMNLP, NAACL, CONLL, CICLing), meaning 54.8% were published in workshops. In our data set, 46.8% of papers surveyed were published in workshops. The most number of papers were published in the largest conferences: ACL and EMNLP. Thus, while Table 1 suggests that discussions of race have been siloed to particular NLP applications, Figure 2 does not show evidence that they have been siloed to particular venues.\nIn Table 2, for all papers that use categorization schema to classify race, we show what racial categories they use. If a paper uses multiple schemes (e.g. collects crowd-sourced annotations of stereotypes associated with different races and also asks annotators to self-report their race), we report each scheme as a separate data point. This table does not include papers that do not specify racial categories (e.g. examine “racist language” without specifying targeted people or analyze semantic change of topics like “racism” and “prejudice”). Finally, we map terms used by papers to the ones in Table 2, e.g. papers examining African American vs. European American names are included in BW.\nThe majority of papers focus on binary\n14https://www.aclweb.org/anthology/ venues/ws/\nBlack/white racial categories. While many papers draw definitions from the U.S. census, very few papers consider less-commonly-selected census categories like Native American or Pacific Islander. The most common method for identifying people’s race uses first or last names (10 papers) or explicit keywords like “black” and “white” (10 papers)."
    }, {
      "heading" : "C Full List of Surveyed Papers",
      "text" : "Year Venue NLP Task Task Type Assimakopoulos et al. (2020) 2020 LREC Abusive Language Collect Corpus Bommasani et al. (2020) 2020 ACL Text Representations Detect Bias Chakravarthi (2020) 2020 Workshop Abusive Language Collect Corpus Groenwold et al. (2020) 2020 EMNLP Text Generation Detect Bias Gupta et al. (2020) 2020 Workshop Sector-spec. NLP apps. Collect Corpus Huang et al. (2020) 2020 LREC Abusive Language Detect Bias Jiang and Fellbaum (2020) 2020 Workshop Text Representations Detect Bias Joseph and Morgan (2020) 2020 ACL Text Representations Detect Bias Kennedy et al. (2020) 2020 ACL Abusive Language Debias Kurrek et al. (2020) 2020 Workshop Abusive Language Collect Corpus Lepori (2020) 2020 COLING Text Representations Detect Bias Liu et al. (2020) 2020 COLING Text Generation Debias Meaney (2020) 2020 Workshop Social Science/Media Survey/Position Nangia et al. (2020) 2020 EMNLP Text Representations Detect Bias Roy and Goldwasser (2020) 2020 EMNLP Social Science/Media Analyze Corpus Sap et al. (2020) 2020 ACL Abusive Language Collect Corpus Shah et al. (2020) 2020 ACL Ethics/Task-indep. Bias Survey/Position Shahid et al. (2020) 2020 Workshop Social Science/Media Analyze Corpus Tan et al. (2020) 2020 ACL Ethics/Task-indep. Bias Develop Model Xia et al. (2020) 2020 Workshop Abusive Language Debias Zhang et al. (2020) 2020 ACL Abusive Language Detect Bias Zhao and Chang (2020) 2020 EMNLP Ethics/Task-indep. Bias Detect Bias Amir et al. (2019) 2019 Workshop Sector-spec. NLP apps. Analyze Corpus Davidson et al. (2019) 2019 Workshop Abusive Language Detect Bias Demszky et al. (2019) 2019 NAACL Social Science/Media Analyze Corpus Gillani and Levy (2019) 2019 Workshop Text Representations Analyze Corpus Jurgens et al. (2019) 2019 ACL Abusive Language Survey/Position Karve et al. (2019) 2019 Workshop Text Representations Debias Kurita et al. (2019) 2019 Workshop Text Representations Detect Bias Lauscher and Glavaš (2019) 2019 Workshop Text Representations Detect Bias Lee et al. (2019) 2019 Workshop Text Generation Detect Bias Liu et al. (2019) 2019 CoNLL Social Science/Media Develop Model Manzini et al. (2019) 2019 NAACL Text Representations Debias May et al. (2019) 2019 ACL Text Representations Detect Bias Mayfield et al. (2019) 2019 Workshop Sector-spec. NLP apps. Survey/Position Merullo et al. (2019) 2019 EMNLP Social Science/Media Analyze Corpus Mostafazadeh Davani et al. (2019) 2019 EMNLP Core NLP Applications Develop Model Parish-Morris (2019) 2019 Workshop Sector-spec. NLP apps. Survey/Position Romanov et al. (2019) 2019 NAACL Sector-spec. NLP apps. Debias Santos and Paraboni (2019) 2019 RANLP Social Science/Media Collect Corpus Sap et al. (2019) 2019 ACL Abusive Language Detect Bias Sharifirad and Matwin (2019) 2019 Workshop Abusive Language Analyze Corpus Sommerauer and Fokkens (2019) 2019 Workshop Text Representations Detect Bias Tripodi et al. (2019) 2019 Workshop Text Representations Analyze Corpus Vylomova et al. (2019) 2019 Workshop Social Science/Media Analyze Corpus Wallace et al. (2019) 2019 EMNLP Text Generation Detect Bias Xu et al. (2019) 2019 INLG Text Generation Develop Model Barbieri and Camacho-Collados (2018) 2018 *SEM Social Science/Media Analyze Corpus\nBlodgett et al. (2018) 2018 ACL Core NLP Applications Debias Castelle (2018) 2018 Workshop Abusive Language Analyze Corpus de Gibert et al. (2018) 2018 Workshop Abusive Language Collect Corpus Elazar and Goldberg (2018) 2018 EMNLP Ethics/Task-indep. Bias Debias Kasunic and Kaufman (2018) 2018 Workshop Text Generation Survey/Position Kiritchenko and Mohammad (2018) 2018 *SEM Social Science/Media Detect Bias Loveys et al. (2018) 2018 Workshop Sector-spec. NLP apps. Analyze Corpus Preoţiuc-Pietro and Ungar (2018) 2018 COLING Social Science/Media Develop Model Sheng et al. (2019) 2018 EMNLP Text Generation Detect Bias Wojatzki et al. (2018) 2018 LREC Social Science/Media Collect Corpus Wood-Doughty et al. (2018) 2018 Workshop Social Science/Media Develop Model Clarke and Grieve (2017) 2017 Workshop Abusive Language Analyze Corpus Gallagher et al. (2017) 2017 TACL Social Science/Media Develop Model Hasanuzzaman et al. (2017) 2017 IJCNLP Abusive Language Develop Model Ramakrishna et al. (2017) 2017 ACL Social Science/Media Analyze Corpus Rudinger et al. (2017) 2017 Workshop Core NLP Applications Detect Bias Schnoebelen (2017) 2017 Workshop Ethics/Task-indep. Bias Survey/Position van Miltenburg et al. (2017) 2017 INLG Image Processing Detect Bias Waseem et al. (2017) 2017 Workshop Abusive Language Survey/Position Wood-Doughty et al. (2017) 2017 Workshop Social Science/Media Analyze Corpus Wright et al. (2017) 2017 Workshop Abusive Language Analyze Corpus Blodgett et al. (2016) 2016 EMNLP Ethics/Task-indep. Bias Collect Corpus Pavlick et al. (2016) 2016 EMNLP Core NLP Applications Collect Corpus Waseem (2016) 2016 Workshop Abusive Language Detect Bias Waseem and Hovy (2016) 2016 Workshop Abusive Language Collect Corpus Mohammady and Culotta (2014) 2014 Workshop Social Science/Media Develop Model Bergsma et al. (2013) 2013 NAACL Social Science/Media Develop Model Herbelot et al. (2012) 2012 Workshop Social Science/Media Analyze Corpus Warner and Hirschberg (2012) 2012 Workshop Abusive Language Develop Model Eisenstein et al. (2011) 2011 ACL Social Science/Media Analyze Corpus Somers (2006) 2006 Workshop Sector-spec. NLP apps. Survey/Position"
    } ],
    "references" : [ {
      "title" : "Who counts as a notable sociologist on Wikipedia? gender, race, and the “professor test",
      "author" : [ "Julia Adams", "Hannah Brückner", "Cambria Naslund." ],
      "venue" : "Socius, 5.",
      "citeRegEx" : "Adams et al\\.,? 2019",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2019
    }, {
      "title" : "Mental health surveillance over social media with digital cohorts",
      "author" : [ "Silvio Amir", "Mark Dredze", "John W. Ayers." ],
      "venue" : "Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 114–120, Minneapolis, Minnesota.",
      "citeRegEx" : "Amir et al\\.,? 2019",
      "shortCiteRegEx" : "Amir et al\\.",
      "year" : 2019
    }, {
      "title" : "Machine bias: There’s software used across the country to predict future criminals and it’s biased against blacks",
      "author" : [ "Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner." ],
      "venue" : "ProPublica.",
      "citeRegEx" : "Angwin et al\\.,? 2016",
      "shortCiteRegEx" : "Angwin et al\\.",
      "year" : 2016
    }, {
      "title" : "Annotating for hate speech: The MaNeCo corpus and some input from critical discourse analysis",
      "author" : [ "Stavros Assimakopoulos", "Rebecca Vella Muskat", "Lonneke van der Plas", "Albert Gatt." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation",
      "citeRegEx" : "Assimakopoulos et al\\.,? 2020",
      "shortCiteRegEx" : "Assimakopoulos et al\\.",
      "year" : 2020
    }, {
      "title" : "Stereotyping and prejudice: Changing conceptions",
      "author" : [ "Daniel Bar-Tal", "Carl F Graumann", "Arie W Kruglanski", "Wolfgang Stroebe." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Bar.Tal et al\\.,? 2013",
      "shortCiteRegEx" : "Bar.Tal et al\\.",
      "year" : 2013
    }, {
      "title" : "How gender and skin tone modifiers affect emoji semantics in Twitter",
      "author" : [ "Francesco Barbieri", "Jose Camacho-Collados." ],
      "venue" : "Proceedings of the Seventh",
      "citeRegEx" : "Barbieri and Camacho.Collados.,? 2018",
      "shortCiteRegEx" : "Barbieri and Camacho.Collados.",
      "year" : 2018
    }, {
      "title" : "Brown v",
      "author" : [ "Derrick A Bell Jr." ],
      "venue" : "board of education and the interest-convergence dilemma. Harvard law review, pages 518–533.",
      "citeRegEx" : "Jr.,? 1980",
      "shortCiteRegEx" : "Jr.",
      "year" : 1980
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big",
      "author" : [ "Emily Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "Proceedings of the 2021 Conference on Fairness, Accountability, and Transparency,",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Race After Technology: Abolitionist Tools for the New Jim Code",
      "author" : [ "Ruha Benjamin." ],
      "venue" : "Wiley.",
      "citeRegEx" : "Benjamin.,? 2019",
      "shortCiteRegEx" : "Benjamin.",
      "year" : 2019
    }, {
      "title" : "Broadly improving user classification via communication-based name and location clustering on Twitter",
      "author" : [ "Shane Bergsma", "Mark Dredze", "Benjamin Van Durme", "Theresa Wilson", "David Yarowsky." ],
      "venue" : "Proceedings of the 2013",
      "citeRegEx" : "Bergsma et al\\.,? 2013",
      "shortCiteRegEx" : "Bergsma et al\\.",
      "year" : 2013
    }, {
      "title" : "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination",
      "author" : [ "Marianne Bertrand", "Sendhil Mullainathan." ],
      "venue" : "American Economic Review, 94(4):991–1013.",
      "citeRegEx" : "Bertrand and Mullainathan.,? 2004",
      "shortCiteRegEx" : "Bertrand and Mullainathan.",
      "year" : 2004
    }, {
      "title" : "Language (technology) is power: A critical survey of “bias” in NLP",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Demographic dialectal variation in social media: A case study of African-American English",
      "author" : [ "Su Lin Blodgett", "Lisa Green", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Blodgett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2016
    }, {
      "title" : "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
      "author" : [ "Su Lin Blodgett", "Gilsinia Lopez", "Alexandra Olteanu", "Robert Sim", "Hanna Wallach." ],
      "venue" : "Proceedings of the Joint Conference of the 59th Annual Meeting of the",
      "citeRegEx" : "Blodgett et al\\.,? 2021",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2021
    }, {
      "title" : "Twitter Universal Dependency parsing for African-American and mainstream",
      "author" : [ "Su Lin Blodgett", "Johnny Wei", "Brendan O’Connor" ],
      "venue" : "American English. In Proceedings of the 56th Annual Meeting",
      "citeRegEx" : "Blodgett et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2018
    }, {
      "title" : "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Zou", "Venkatesh Saligrama", "Adam Kalai." ],
      "venue" : "Proceedings of the 30th International Confer-",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings",
      "author" : [ "Rishi Bommasani", "Kelly Davis", "Claire Cardie." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4758–",
      "citeRegEx" : "Bommasani et al\\.,? 2020",
      "shortCiteRegEx" : "Bommasani et al\\.",
      "year" : 2020
    }, {
      "title" : "Sorting Things Out: Classification and Its Consequences",
      "author" : [ "Geoffrey C. Bowker", "Susan Leigh Star." ],
      "venue" : "Inside Technology. MIT Press.",
      "citeRegEx" : "Bowker and Star.,? 2000",
      "shortCiteRegEx" : "Bowker and Star.",
      "year" : 2000
    }, {
      "title" : "Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making",
      "author" : [ "Anna Brown", "Alexandra Chouldechova", "Emily PutnamHornstein", "Andrew Tobin", "Rhema Vaithianathan" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2019
    }, {
      "title" : "Dark Matters: On the Surveillance of Blackness",
      "author" : [ "Simone Browne." ],
      "venue" : "Duke University Press.",
      "citeRegEx" : "Browne.,? 2015",
      "shortCiteRegEx" : "Browne.",
      "year" : 2015
    }, {
      "title" : "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "author" : [ "Joy Buolamwini", "Timnit Gebru." ],
      "venue" : "Proceedings of the 1st Conference on Fairness, Accountability and Transparency, pages 77–91, New York, NY, USA.",
      "citeRegEx" : "Buolamwini and Gebru.,? 2018",
      "shortCiteRegEx" : "Buolamwini and Gebru.",
      "year" : 2018
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like biases",
      "author" : [ "Aylin Caliskan", "Joanna J. Bryson", "Arvind Narayanan." ],
      "venue" : "Science, 356(6334):183–186.",
      "citeRegEx" : "Caliskan et al\\.,? 2017",
      "shortCiteRegEx" : "Caliskan et al\\.",
      "year" : 2017
    }, {
      "title" : "The linguistic ideologies of deep abusive language classification",
      "author" : [ "Michael Castelle." ],
      "venue" : "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 160–170, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Castelle.,? 2018",
      "shortCiteRegEx" : "Castelle.",
      "year" : 2018
    }, {
      "title" : "HopeEDI: A multilingual hope speech detection dataset for equality, diversity, and inclusion",
      "author" : [ "Bharathi Raja Chakravarthi." ],
      "venue" : "Proceedings of the Third Workshop on Computational Modeling of People’s Opinions, Personality, and Emotion’s in Social Me-",
      "citeRegEx" : "Chakravarthi.,? 2020",
      "shortCiteRegEx" : "Chakravarthi.",
      "year" : 2020
    }, {
      "title" : "Dimensions of abusive language on Twitter",
      "author" : [ "Isobelle Clarke", "Jack Grieve." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 1–10, Vancouver, BC, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Clarke and Grieve.,? 2017",
      "shortCiteRegEx" : "Clarke and Grieve.",
      "year" : 2017
    }, {
      "title" : "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics",
      "author" : [ "Kimberlé Crenshaw." ],
      "venue" : "University of Chicago Legal Forum, 1989(8).",
      "citeRegEx" : "Crenshaw.,? 1989",
      "shortCiteRegEx" : "Crenshaw.",
      "year" : 1989
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Bias in bios: A case study of semantic representation bias",
      "author" : [ "Maria De-Arteaga", "Alexey Romanov", "Hanna Wallach", "Jennifer Chayes", "Christian Borgs", "Alexandra Chouldechova", "Sahin Geyik", "Krishnaram Kenthapadi", "Adam Tauman Kalai" ],
      "venue" : null,
      "citeRegEx" : "De.Arteaga et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "De.Arteaga et al\\.",
      "year" : 2019
    }, {
      "title" : "Analyzing polarization in social media: Method and application to tweets on 21 mass shootings",
      "author" : [ "Dorottya Demszky", "Nikhil Garg", "Rob Voigt", "James Zou", "Jesse Shapiro", "Matthew Gentzkow", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Demszky et al\\.,? 2019",
      "shortCiteRegEx" : "Demszky et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, page 67–73, New York, NY,",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Discovering sociolinguistic associations with structured sparsity",
      "author" : [ "Jacob Eisenstein", "Noah A. Smith", "Eric P. Xing." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Eisenstein et al\\.,? 2011",
      "shortCiteRegEx" : "Eisenstein et al\\.",
      "year" : 2011
    }, {
      "title" : "Adversarial removal of demographic attributes from text data",
      "author" : [ "Yanai Elazar", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 11–21, Brussels, Belgium. Association for Computa-",
      "citeRegEx" : "Elazar and Goldberg.,? 2018",
      "shortCiteRegEx" : "Elazar and Goldberg.",
      "year" : 2018
    }, {
      "title" : "Controlled analyses of social biases in Wikipedia bios",
      "author" : [ "Anjalie Field", "Chan Young Park", "Yulia Tsvetkov." ],
      "venue" : "Computing Research Repository, arXiv:2101.00078. Version 1.",
      "citeRegEx" : "Field et al\\.,? 2021",
      "shortCiteRegEx" : "Field et al\\.",
      "year" : 2021
    }, {
      "title" : "Value sensitive design and information systems",
      "author" : [ "Batya Friedman", "Peter Kahn", "Alan Borning", "Alina Huldtgren." ],
      "venue" : "Neelke Doorn, Daan Schuurbiers, Ibo van de Poel, and Michael Gorman, editors, Early engagement and new technologies: Opening",
      "citeRegEx" : "Friedman et al\\.,? 2013",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2013
    }, {
      "title" : "The causes and consequences of distinctively black names",
      "author" : [ "Roland G Fryer Jr", "Steven D Levitt." ],
      "venue" : "The Quarterly Journal of Economics, 119(3):767–805.",
      "citeRegEx" : "Jr and Levitt.,? 2004",
      "shortCiteRegEx" : "Jr and Levitt.",
      "year" : 2004
    }, {
      "title" : "Anchored correlation explanation: Topic modeling with minimal domain knowledge",
      "author" : [ "Ryan J. Gallagher", "Kyle Reing", "David Kale", "Greg Ver Steeg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:529–542.",
      "citeRegEx" : "Gallagher et al\\.,? 2017",
      "shortCiteRegEx" : "Gallagher et al\\.",
      "year" : 2017
    }, {
      "title" : "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "author" : [ "Nikhil Garg", "Londa Schiebinger", "Dan Jurafsky", "James Zou." ],
      "venue" : "Proceedings of the National Academy of Sciences, 115(16):E3635–E3644.",
      "citeRegEx" : "Garg et al\\.,? 2018",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2018
    }, {
      "title" : "Hate speech dataset from a white supremacy forum",
      "author" : [ "Ona de Gibert", "Naiara Perez", "Aitor Garcı́a-Pablos", "Montse Cuadros" ],
      "venue" : "In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2),",
      "citeRegEx" : "Gibert et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gibert et al\\.",
      "year" : 2018
    }, {
      "title" : "Simple dynamic word embeddings for mapping perceptions in the public sphere",
      "author" : [ "Nabeel Gillani", "Roger Levy." ],
      "venue" : "Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 94–99, Minneapolis,",
      "citeRegEx" : "Gillani and Levy.,? 2019",
      "shortCiteRegEx" : "Gillani and Levy.",
      "year" : 2019
    }, {
      "title" : "Measuring individual differences in implicit cognition: the implicit association test",
      "author" : [ "Anthony G Greenwald", "Debbie E McGhee", "Jordan LK Schwartz." ],
      "venue" : "Journal of personality and social psychology, 74(6):1464.",
      "citeRegEx" : "Greenwald et al\\.,? 1998",
      "shortCiteRegEx" : "Greenwald et al\\.",
      "year" : 1998
    }, {
      "title" : "Investigating AfricanAmerican Vernacular English in transformer-based text generation",
      "author" : [ "Sophie Groenwold", "Lily Ou", "Aesha Parekh", "Samhita Honnavalli", "Sharon Levy", "Diba Mirza", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Confer-",
      "citeRegEx" : "Groenwold et al\\.,? 2020",
      "shortCiteRegEx" : "Groenwold et al\\.",
      "year" : 2020
    }, {
      "title" : "Heart failure education of African American and Hispanic/Latino patients: Data collection and analysis",
      "author" : [ "Itika Gupta", "Barbara Di Eugenio", "Devika Salunke", "Andrew Boyd", "Paula Allen-Meares", "Carolyn Dickens", "Olga Garcia." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Stereotypes and stereotyping: An overview of the cogni",
      "author" : [ "David L Hamilton", "Tina K Trolier" ],
      "venue" : null,
      "citeRegEx" : "Hamilton and Trolier.,? \\Q1986\\E",
      "shortCiteRegEx" : "Hamilton and Trolier.",
      "year" : 1986
    }, {
      "title" : "Towards a critical race methodology in algorithmic fairness",
      "author" : [ "Alex Hanna", "Emily Denton", "Andrew Smart", "Jamila Smith-Loud." ],
      "venue" : "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, page 501–512, New York, NY, USA.",
      "citeRegEx" : "Hanna et al\\.,? 2020",
      "shortCiteRegEx" : "Hanna et al\\.",
      "year" : 2020
    }, {
      "title" : "Demographic word embeddings for racism detection on Twitter",
      "author" : [ "Mohammed Hasanuzzaman", "Gaël Dias", "Andy Way." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
      "citeRegEx" : "Hasanuzzaman et al\\.,? 2017",
      "shortCiteRegEx" : "Hasanuzzaman et al\\.",
      "year" : 2017
    }, {
      "title" : "Distributional techniques for philosophical enquiry",
      "author" : [ "Aurélie Herbelot", "Eva von Redecker", "Johanna Müller." ],
      "venue" : "Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 45–54, Avi-",
      "citeRegEx" : "Herbelot et al\\.,? 2012",
      "shortCiteRegEx" : "Herbelot et al\\.",
      "year" : 2012
    }, {
      "title" : "Multilingual Twitter corpus and baselines for evaluating demographic bias in hate speech recognition",
      "author" : [ "Xiaolei Huang", "Linzi Xing", "Franck Dernoncourt", "Michael J. Paul." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Confer-",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Language and Racialization",
      "author" : [ "Anne H. Charity Hudley." ],
      "venue" : "Ofelia Garcı́a, Nelson Flores, and Massimiliano Spotti, editors, The Oxford Handbook of Language and Society, pages 381–402. Oxford University Press.",
      "citeRegEx" : "Hudley.,? 2017",
      "shortCiteRegEx" : "Hudley.",
      "year" : 2017
    }, {
      "title" : "Toward racial justice in linguistics: Interdisciplinary insights into theorizing race in the discipline and diversifying the profession",
      "author" : [ "Anne H Charity Hudley", "Christine Mallinson", "Mary Bucholtz." ],
      "venue" : "Language, 96(4):e200–e235.",
      "citeRegEx" : "Hudley et al\\.,? 2020",
      "shortCiteRegEx" : "Hudley et al\\.",
      "year" : 2020
    }, {
      "title" : "Social biases in NLP models as barriers for persons with disabilities",
      "author" : [ "Ben Hutchinson", "Vinodkumar Prabhakaran", "Emily Denton", "Kellie Webster", "Yu Zhong", "Stephen Denuyl." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Hutchinson et al\\.,? 2020",
      "shortCiteRegEx" : "Hutchinson et al\\.",
      "year" : 2020
    }, {
      "title" : "Interdependencies of gender and race in contextualized word embeddings",
      "author" : [ "May Jiang", "Christiane Fellbaum." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 17–25, Barcelona, Spain (Online). Asso-",
      "citeRegEx" : "Jiang and Fellbaum.,? 2020",
      "shortCiteRegEx" : "Jiang and Fellbaum.",
      "year" : 2020
    }, {
      "title" : "When do word embeddings accurately reflect surveys on our beliefs about people",
      "author" : [ "Kenneth Joseph", "Jonathan Morgan" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Joseph and Morgan.,? \\Q2020\\E",
      "shortCiteRegEx" : "Joseph and Morgan.",
      "year" : 2020
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "A just and comprehensive strategy for using NLP to address online abuse",
      "author" : [ "David Jurgens", "Libby Hemphill", "Eshwar Chandrasekharan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3658–",
      "citeRegEx" : "Jurgens et al\\.,? 2019",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2019
    }, {
      "title" : "Conceptor debiasing of word representations evaluated on WEAT",
      "author" : [ "Saket Karve", "Lyle Ungar", "João Sedoc." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 40–48, Florence, Italy. Association for Com-",
      "citeRegEx" : "Karve et al\\.,? 2019",
      "shortCiteRegEx" : "Karve et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to listen: Critically considering the role of AI in human storytelling and character creation",
      "author" : [ "Anna Kasunic", "Geoff Kaufman." ],
      "venue" : "Proceedings of the First Workshop on Storytelling, pages 1–13, New Orleans, Louisiana. Association for Computa-",
      "citeRegEx" : "Kasunic and Kaufman.,? 2018",
      "shortCiteRegEx" : "Kasunic and Kaufman.",
      "year" : 2018
    }, {
      "title" : "Contextualizing hate speech classifiers with post-hoc explanation",
      "author" : [ "Brendan Kennedy", "Xisen Jin", "Aida Mostafazadeh Davani", "Morteza Dehghani", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kennedy et al\\.,? 2020",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2020
    }, {
      "title" : "From African American Vernacular English to African American Language: Rethinking the study of race and language in African Americans’ speech",
      "author" : [ "Sharese King." ],
      "venue" : "Annual Review of Linguistics, 6(1):285–300.",
      "citeRegEx" : "King.,? 2020",
      "shortCiteRegEx" : "King.",
      "year" : 2020
    }, {
      "title" : "Examining gender and race bias in two hundred sentiment analysis systems",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43–53, New Orleans,",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2018",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2018
    }, {
      "title" : "Participatory approaches to machine learning",
      "author" : [ "Bogdan Kulynych", "David Madras", "Smitha Milli", "Inioluwa Deborah Raji", "Angela Zhou", "Richard Zemel." ],
      "venue" : "International Conference on Machine Learning Workshop.",
      "citeRegEx" : "Kulynych et al\\.,? 2020",
      "shortCiteRegEx" : "Kulynych et al\\.",
      "year" : 2020
    }, {
      "title" : "Machine translation into low-resource language varieties",
      "author" : [ "Sachin Kumar", "Antonios Anastasopoulos", "Shuly Wintner", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Kumar et al\\.,? 2021",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2021
    }, {
      "title" : "Measuring bias in contextualized word representations",
      "author" : [ "Keita Kurita", "Nidhi Vyas", "Ayush Pareek", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166–172, Florence, Italy. Associ-",
      "citeRegEx" : "Kurita et al\\.,? 2019",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards a comprehensive taxonomy and large-scale annotated corpus for online slur usage",
      "author" : [ "Jana Kurrek", "Haji Mohammad Saleem", "Derek Ruths." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 138–149, Online. As-",
      "citeRegEx" : "Kurrek et al\\.,? 2020",
      "shortCiteRegEx" : "Kurrek et al\\.",
      "year" : 2020
    }, {
      "title" : "Are we consistently biased? multidimensional analysis of biases in distributional word vectors",
      "author" : [ "Anne Lauscher", "Goran Glavaš." ],
      "venue" : "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 85–91,",
      "citeRegEx" : "Lauscher and Glavaš.,? 2019",
      "shortCiteRegEx" : "Lauscher and Glavaš.",
      "year" : 2019
    }, {
      "title" : "Exploring social bias in chatbots using stereotype knowledge",
      "author" : [ "Nayeon Lee", "Andrea Madotto", "Pascale Fung." ],
      "venue" : "Proceedings of the 2019 Workshop on Widening NLP, pages 177–180, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Give me convenience and give her death: Who should decide what uses of NLP are appropriate, and on what basis",
      "author" : [ "Kobi Leins", "Jey Han Lau", "Timothy Baldwin" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Leins et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Leins et al\\.",
      "year" : 2020
    }, {
      "title" : "Unequal representations: Analyzing intersectional biases in word embeddings using representational similarity analysis",
      "author" : [ "Michael Lepori." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1720–1728, Barcelona,",
      "citeRegEx" : "Lepori.,? 2020",
      "shortCiteRegEx" : "Lepori.",
      "year" : 2020
    }, {
      "title" : "Does gender matter? towards fairness in dialogue systems",
      "author" : [ "Haochen Liu", "Jamell Dacon", "Wenqi Fan", "Hui Liu", "Zitao Liu", "Jiliang Tang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4403–4416, Barcelona,",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting frames in news headlines and its application to analyzing news framing trends surrounding U.S. gun violence",
      "author" : [ "Siyi Liu", "Lei Guo", "Kate Mays", "Margrit Betke", "Derry Tanti Wijaya" ],
      "venue" : "In Proceedings of the 23rd Conference on Computational",
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-cultural differences in language markers of depression online",
      "author" : [ "Kate Loveys", "Jonathan Torrez", "Alex Fine", "Glen Moriarty", "Glen Coppersmith." ],
      "venue" : "Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology:",
      "citeRegEx" : "Loveys et al\\.,? 2018",
      "shortCiteRegEx" : "Loveys et al\\.",
      "year" : 2018
    }, {
      "title" : "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "author" : [ "Thomas Manzini", "Lim Yao Chong", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Manzini et al\\.,? 2019",
      "shortCiteRegEx" : "Manzini et al\\.",
      "year" : 2019
    }, {
      "title" : "On measuring social biases in sentence encoders",
      "author" : [ "Chandler May", "Alex Wang", "Shikha Bordia", "Samuel R. Bowman", "Rachel Rudinger." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "May et al\\.,? 2019",
      "shortCiteRegEx" : "May et al\\.",
      "year" : 2019
    }, {
      "title" : "Equity beyond bias in language technologies for education",
      "author" : [ "Elijah Mayfield", "Michael Madaio", "Shrimai Prabhumoye", "David Gerritsen", "Brittany McLaughlin", "Ezekiel Dixon-Román", "Alan W Black." ],
      "venue" : "Proceedings of the Fourteenth Workshop",
      "citeRegEx" : "Mayfield et al\\.,? 2019",
      "shortCiteRegEx" : "Mayfield et al\\.",
      "year" : 2019
    }, {
      "title" : "Black Software: The Internet and Racial Justice, from the AfroNet to Black Lives Matter",
      "author" : [ "Charlton D. McIlwain." ],
      "venue" : "Oxford University Press, Incorporated.",
      "citeRegEx" : "McIlwain.,? 2019",
      "shortCiteRegEx" : "McIlwain.",
      "year" : 2019
    }, {
      "title" : "Crossing the line: Where do demographic variables fit into humor detection? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 176–181, Online",
      "author" : [ "J.A. Meaney." ],
      "venue" : "Associa-",
      "citeRegEx" : "Meaney.,? 2020",
      "shortCiteRegEx" : "Meaney.",
      "year" : 2020
    }, {
      "title" : "A framework for the computational linguistic analysis of dehumanization",
      "author" : [ "Julia Mendelsohn", "Yulia Tsvetkov", "Dan Jurafsky." ],
      "venue" : "Frontiers in Artificial Intelligence, 3:55.",
      "citeRegEx" : "Mendelsohn et al\\.,? 2020",
      "shortCiteRegEx" : "Mendelsohn et al\\.",
      "year" : 2020
    }, {
      "title" : "Diversity in faces",
      "author" : [ "Michele Merler", "Nalini Ratha", "Rogerio S Feris", "John R Smith." ],
      "venue" : "Computing Research Repository, arXiv:1901.10436. Version 6.",
      "citeRegEx" : "Merler et al\\.,? 2019",
      "shortCiteRegEx" : "Merler et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating sports commentator bias within a large corpus of American football broadcasts",
      "author" : [ "Jack Merullo", "Luke Yeh", "Abram Handler", "Alvin Grissom II", "Brendan O’Connor", "Mohit Iyyer" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Merullo et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Merullo et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-linguistic differences and similarities in image descriptions",
      "author" : [ "Emiel van Miltenburg", "Desmond Elliott", "Piek Vossen." ],
      "venue" : "Proceedings of the 10th International Conference on Natural Language Generation, pages 21–30, Santiago de Compostela,",
      "citeRegEx" : "Miltenburg et al\\.,? 2017",
      "shortCiteRegEx" : "Miltenburg et al\\.",
      "year" : 2017
    }, {
      "title" : "Using county demographics to infer attributes of Twitter users",
      "author" : [ "Ehsan Mohammady", "Aron Culotta." ],
      "venue" : "Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 7–16, Baltimore, Maryland. Association",
      "citeRegEx" : "Mohammady and Culotta.,? 2014",
      "shortCiteRegEx" : "Mohammady and Culotta.",
      "year" : 2014
    }, {
      "title" : "Reporting the unreported",
      "author" : [ "Aida Mostafazadeh Davani", "Leigh Yeh", "Mohammad Atari", "Brendan Kennedy", "Gwenyth Portillo Wightman", "Elaine Gonzalez", "Natalie Delong", "Rhea Bhatia", "Arineh Mirinjian", "Xiang Ren", "Morteza Dehghani" ],
      "venue" : null,
      "citeRegEx" : "Davani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Davani et al\\.",
      "year" : 2019
    }, {
      "title" : "Stereoset: Measuring stereotypical bias in pretrained language models",
      "author" : [ "Moin Nadeem", "Anna Bethke", "Siva Reddy." ],
      "venue" : "Computing Research Repository, arXiv:2004.09456. Version 1.",
      "citeRegEx" : "Nadeem et al\\.,? 2020",
      "shortCiteRegEx" : "Nadeem et al\\.",
      "year" : 2020
    }, {
      "title" : "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
      "author" : [ "Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Nangia et al\\.,? 2020",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2020
    }, {
      "title" : "Algorithms of Oppression: How Search Engines Reinforce Racism",
      "author" : [ "Safiya U. Noble." ],
      "venue" : "NYU Press.",
      "citeRegEx" : "Noble.,? 2018",
      "shortCiteRegEx" : "Noble.",
      "year" : 2018
    }, {
      "title" : "Critical race theory for HCI",
      "author" : [ "Ihudiya Finda Ogbonnaya-Ogburu", "Angela D.R. Smith", "Alexandra To", "Kentaro Toyama." ],
      "venue" : "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI ’20, page 1–16, New York, NY, USA.",
      "citeRegEx" : "Ogbonnaya.Ogburu et al\\.,? 2020",
      "shortCiteRegEx" : "Ogbonnaya.Ogburu et al\\.",
      "year" : 2020
    }, {
      "title" : "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "author" : [ "Alexandra Olteanu", "Carlos Castillo", "Fernando Diaz", "Emre Kiciman." ],
      "venue" : "Frontiers in Big Data, 2:13.",
      "citeRegEx" : "Olteanu et al\\.,? 2019",
      "shortCiteRegEx" : "Olteanu et al\\.",
      "year" : 2019
    }, {
      "title" : "Computational linguistics for enhancing scientific reproducibility and reducing healthcare inequities",
      "author" : [ "Julia Parish-Morris." ],
      "venue" : "Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 94–102, Minneapolis, Min-",
      "citeRegEx" : "Parish.Morris.,? 2019",
      "shortCiteRegEx" : "Parish.Morris.",
      "year" : 2019
    }, {
      "title" : "The gun violence database: A new task and data set for NLP",
      "author" : [ "Ellie Pavlick", "Heng Ji", "Xiaoman Pan", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1018–1024, Austin,",
      "citeRegEx" : "Pavlick et al\\.,? 2016",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2016
    }, {
      "title" : "Userlevel race and ethnicity predictors from Twitter text",
      "author" : [ "Daniel Preoţiuc-Pietro", "Lyle Ungar." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1534–1545, Santa Fe, New Mexico, USA. Association for Com-",
      "citeRegEx" : "Preoţiuc.Pietro and Ungar.,? 2018",
      "shortCiteRegEx" : "Preoţiuc.Pietro and Ungar.",
      "year" : 2018
    }, {
      "title" : "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products",
      "author" : [ "Inioluwa Deborah Raji", "Joy Buolamwini." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
      "citeRegEx" : "Raji and Buolamwini.,? 2019",
      "shortCiteRegEx" : "Raji and Buolamwini.",
      "year" : 2019
    }, {
      "title" : "Linguistic analysis of differences in portrayal of movie characters",
      "author" : [ "Anil Ramakrishna", "Victor R. Martı́nez", "Nikolaos Malandrakis", "Karan Singla", "Shrikanth Narayanan" ],
      "venue" : "In Proceedings of the 55th Annual Meeting of the Association",
      "citeRegEx" : "Ramakrishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ramakrishna et al\\.",
      "year" : 2017
    }, {
      "title" : "Straighten up and fly right: Rethinking intersectionality in HCI research",
      "author" : [ "Yolanda A. Rankin", "Jakita O. Thomas." ],
      "venue" : "Interactions, 26(6):64–68.",
      "citeRegEx" : "Rankin and Thomas.,? 2019",
      "shortCiteRegEx" : "Rankin and Thomas.",
      "year" : 2019
    }, {
      "title" : "What’s in a name? Reducing bias in bios without",
      "author" : [ "Alexey Romanov", "Maria De-Arteaga", "Hanna Wallach", "Jennifer Chayes", "Christian Borgs", "Alexandra Chouldechova", "Sahin Geyik", "Krishnaram Kenthapadi", "Anna Rumshisky", "Adam Kalai" ],
      "venue" : null,
      "citeRegEx" : "Romanov et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Romanov et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsettling race and language: Toward a raciolinguistic perspective",
      "author" : [ "Jonathan Rosa", "Nelson Flores." ],
      "venue" : "Language in Society, 46(5):621–647.",
      "citeRegEx" : "Rosa and Flores.,? 2017",
      "shortCiteRegEx" : "Rosa and Flores.",
      "year" : 2017
    }, {
      "title" : "The multiple dimensions of race",
      "author" : [ "Wendy D Roth." ],
      "venue" : "Ethnic and Racial Studies, 39(8):1310–1338.",
      "citeRegEx" : "Roth.,? 2016",
      "shortCiteRegEx" : "Roth.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised learning of nuanced frames for analyzing polarization in news media",
      "author" : [ "Shamik Roy", "Dan Goldwasser." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7698–7716,",
      "citeRegEx" : "Roy and Goldwasser.,? 2020",
      "shortCiteRegEx" : "Roy and Goldwasser.",
      "year" : 2020
    }, {
      "title" : "Social bias in elicited natural language inferences",
      "author" : [ "Rachel Rudinger", "Chandler May", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 74–79, Valencia, Spain. Association for",
      "citeRegEx" : "Rudinger et al\\.,? 2017",
      "shortCiteRegEx" : "Rudinger et al\\.",
      "year" : 2017
    }, {
      "title" : "Moral stance recognition and polarity classification from Twitter and elicited text",
      "author" : [ "Wesley Santos", "Ivandré Paraboni." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 1069–",
      "citeRegEx" : "Santos and Paraboni.,? 2019",
      "shortCiteRegEx" : "Santos and Paraboni.",
      "year" : 2019
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678, Florence,",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Conceptual Aphasia in Black: Displacing Racial Formation",
      "author" : [ "P.K. Saucier", "T.P. Woods", "P. Douglass", "B. Hesse", "T.K. Nopper", "G. Thomas", "C. Wun." ],
      "venue" : "Critical Africana Studies. Lexington Books.",
      "citeRegEx" : "Saucier et al\\.,? 2016",
      "shortCiteRegEx" : "Saucier et al\\.",
      "year" : 2016
    }, {
      "title" : "Intersectional hci: Engaging identity through gender, race, and class",
      "author" : [ "Ari Schlesinger", "W. Keith Edwards", "Rebecca E. Grinter." ],
      "venue" : "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI ’17, page 5412–5427, New",
      "citeRegEx" : "Schlesinger et al\\.,? 2017",
      "shortCiteRegEx" : "Schlesinger et al\\.",
      "year" : 2017
    }, {
      "title" : "Goal-oriented design for ethical machine learning and NLP",
      "author" : [ "Tyler Schnoebelen." ],
      "venue" : "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 88–93, Valencia, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Schnoebelen.,? 2017",
      "shortCiteRegEx" : "Schnoebelen.",
      "year" : 2017
    }, {
      "title" : "Predictive biases in natural language processing models: A conceptual framework and overview",
      "author" : [ "Deven Santosh Shah", "H. Andrew Schwartz", "Dirk Hovy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Shah et al\\.,? 2020",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting and understanding moral biases in news",
      "author" : [ "Usman Shahid", "Barbara Di Eugenio", "Andrew Rojecki", "Elena Zheleva." ],
      "venue" : "Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 120–125, Online. Associa-",
      "citeRegEx" : "Shahid et al\\.,? 2020",
      "shortCiteRegEx" : "Shahid et al\\.",
      "year" : 2020
    }, {
      "title" : "Using attention-based bidirectional LSTM to identify different categories of offensive language directed toward female celebrities",
      "author" : [ "Sima Sharifirad", "Stan Matwin." ],
      "venue" : "Proceedings of the 2019 Workshop on Widening NLP, pages 46–48, Florence,",
      "citeRegEx" : "Sharifirad and Matwin.,? 2019",
      "shortCiteRegEx" : "Sharifirad and Matwin.",
      "year" : 2019
    }, {
      "title" : "The woman worked as a babysitter: On biases in language generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Premkumar Natarajan", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Sheng et al\\.,? 2019",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Participation is not a design fix for machine learning",
      "author" : [ "M Sloane", "E Moss", "O Awomolo", "L Forlano." ],
      "venue" : "Computing Research Repository, arXiv:2007.02423. Version 3.",
      "citeRegEx" : "Sloane et al\\.,? 2020",
      "shortCiteRegEx" : "Sloane et al\\.",
      "year" : 2020
    }, {
      "title" : "Language engineering and the pathway to healthcare: A user-oriented view",
      "author" : [ "Harold Somers." ],
      "venue" : "Proceedings of the First International Workshop on Medical Speech Translation, pages 28–35, New York, New York. Association for Computational Lin-",
      "citeRegEx" : "Somers.,? 2006",
      "shortCiteRegEx" : "Somers.",
      "year" : 2006
    }, {
      "title" : "Conceptual change and distributional semantic models: an exploratory study on pitfalls and possibilities",
      "author" : [ "Pia Sommerauer", "Antske Fokkens." ],
      "venue" : "Proceedings of the 1st International Workshop on Computational Approaches to Historical Language",
      "citeRegEx" : "Sommerauer and Fokkens.,? 2019",
      "shortCiteRegEx" : "Sommerauer and Fokkens.",
      "year" : 2019
    }, {
      "title" : "Energy and policy considerations for deep learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Mitigating gender bias in natural language processing: Literature review",
      "author" : [ "Tony Sun", "Andrew Gaut", "Shirlyn Tang", "Yuxin Huang", "Mai ElSherief", "Jieyu Zhao", "Diba Mirza", "Elizabeth Belding", "Kai-Wei Chang", "William Yang Wang." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Discrimination in online ad delivery: Google ads, black names and white names, racial discrimination, and click advertising",
      "author" : [ "Latanya Sweeney." ],
      "venue" : "Queue, 11(3):10–29.",
      "citeRegEx" : "Sweeney.,? 2013",
      "shortCiteRegEx" : "Sweeney.",
      "year" : 2013
    }, {
      "title" : "It’s morphin’ time! Combating linguistic discrimination with inflectional perturbations",
      "author" : [ "Samson Tan", "Shafiq Joty", "Min-Yen Kan", "Richard Socher." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Tan et al\\.,? 2020",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing social and intersectional biases in contextualized word representations",
      "author" : [ "Yi Chern Tan", "L Elisa Celis." ],
      "venue" : "Proceedings of the 2019 Conference on Advances in Neural Information Processing Systems, volume 32, pages 13230–13241. Curran Asso-",
      "citeRegEx" : "Tan and Celis.,? 2019",
      "shortCiteRegEx" : "Tan and Celis.",
      "year" : 2019
    }, {
      "title" : "What I Won’t Build",
      "author" : [ "Rachael Tatman." ],
      "venue" : "Workshop on Widening NLP.",
      "citeRegEx" : "Tatman.,? 2020",
      "shortCiteRegEx" : "Tatman.",
      "year" : 2020
    }, {
      "title" : "Tracing antisemitic language through diachronic embedding projections: France 1789-1914",
      "author" : [ "Rocco Tripodi", "Massimo Warglien", "Simon Levis Sullam", "Deborah Paci." ],
      "venue" : "Proceedings of the 1st International Workshop on Computational Approaches to",
      "citeRegEx" : "Tripodi et al\\.,? 2019",
      "shortCiteRegEx" : "Tripodi et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluation of semantic change of harm-related concepts in psychology",
      "author" : [ "Ekaterina Vylomova", "Sean Murphy", "Nicholas Haslam." ],
      "venue" : "Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,",
      "citeRegEx" : "Vylomova et al\\.,? 2019",
      "shortCiteRegEx" : "Vylomova et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting hate speech on the world wide web",
      "author" : [ "William Warner", "Julia Hirschberg." ],
      "venue" : "Proceedings of the Second Workshop on Language in Social Media, pages 19–26, Montréal, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Warner and Hirschberg.,? 2012",
      "shortCiteRegEx" : "Warner and Hirschberg.",
      "year" : 2012
    }, {
      "title" : "Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138– 142, Austin, Texas. Association for Computational",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84, Vancouver, BC, Canada.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Disembodied machine learning: On the illusion of objectivity in NLP",
      "author" : [ "Zeerak Waseem", "Smarika Lulz", "Isabelle Bingel", "Joachim Augenstein." ],
      "venue" : "Computing Research Repository, arXiv:2101.11974. Version 1.",
      "citeRegEx" : "Waseem et al\\.,? 2021",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2021
    }, {
      "title" : "Gendered ambiguous pronoun (GAP) shared task at the gender bias in NLP workshop 2019",
      "author" : [ "Kellie Webster", "Marta R. Costa-jussà", "Christian Hardmeier", "Will Radford." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Pro-",
      "citeRegEx" : "Webster et al\\.,? 2019",
      "shortCiteRegEx" : "Webster et al\\.",
      "year" : 2019
    }, {
      "title" : "Quantifying qualitative data for understanding controversial issues",
      "author" : [ "Michael Wojatzki", "Saif Mohammad", "Torsten Zesch", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Wojatzki et al\\.,? 2018",
      "shortCiteRegEx" : "Wojatzki et al\\.",
      "year" : 2018
    }, {
      "title" : "Predicting Twitter user demographics from names alone",
      "author" : [ "Zach Wood-Doughty", "Nicholas Andrews", "Rebecca Marvin", "Mark Dredze." ],
      "venue" : "Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and",
      "citeRegEx" : "Wood.Doughty et al\\.,? 2018",
      "shortCiteRegEx" : "Wood.Doughty et al\\.",
      "year" : 2018
    }, {
      "title" : "How does Twitter user behavior vary across demographic groups",
      "author" : [ "Zach Wood-Doughty", "Michael Smith", "David Broniatowski", "Mark Dredze" ],
      "venue" : "In Proceedings of the Second Workshop on NLP and Computational Social Science,",
      "citeRegEx" : "Wood.Doughty et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wood.Doughty et al\\.",
      "year" : 2017
    }, {
      "title" : "Vectors for counterspeech on Twitter",
      "author" : [ "Lucas Wright", "Derek Ruths", "Kelly P Dillon", "Haji Mohammad Saleem", "Susan Benesch." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 57–62, Vancouver, BC, Canada. Association",
      "citeRegEx" : "Wright et al\\.,? 2017",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2017
    }, {
      "title" : "Demoting racial bias in hate speech detection",
      "author" : [ "Mengzhou Xia", "Anjalie Field", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 7–14, Online. Association for Computa-",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    }, {
      "title" : "Privacy-aware text rewriting",
      "author" : [ "Qiongkai Xu", "Lizhen Qu", "Chenchen Xu", "Ran Cui." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 247–257, Tokyo, Japan. Association for Computational Linguistics.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Demographics should not be the reason of toxicity: Mitigating discrimination in text classifications with instance weighting",
      "author" : [ "Guanhua Zhang", "Bing Bai", "Junqi Zhang", "Kun Bai", "Conghui Zhu", "Tiejun Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "LOGAN: Local group bias detection by clustering",
      "author" : [ "Jieyu Zhao", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1968–1977, Online. Association for Computational",
      "citeRegEx" : "Zhao and Chang.,? 2020",
      "shortCiteRegEx" : "Zhao and Chang.",
      "year" : 2020
    }, {
      "title" : "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 93,
      "context" : "continue to justify social and economic exclusion (Rosa and Flores, 2017).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 42,
      "context" : "1 Furthermore, language is the primary means through which stereotypes and prejudices are communicated and perpetuated (Hamilton and Trolier, 1986; Bar-Tal et al., 2013).",
      "startOffset" : 119,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "1 Furthermore, language is the primary means through which stereotypes and prejudices are communicated and perpetuated (Hamilton and Trolier, 1986; Bar-Tal et al., 2013).",
      "startOffset" : 119,
      "endOffset" : 169
    }, {
      "referenceID" : 47,
      "context" : "based on racial attributes including but not limited to cultural and social history, physical features, and skin color” (Hudley, 2017).",
      "startOffset" : 120,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "(Buolamwini and Gebru, 2018) or machine learning (recidivism risk prediction) (Angwin et al.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "(Buolamwini and Gebru, 2018) or machine learning (recidivism risk prediction) (Angwin et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 112,
      "context" : "Even the presence of racial biases in search engines like Google (Sweeney, 2013; Noble, 2018) has prompted little investigation in the ACL com-",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 83,
      "context" : "Even the presence of racial biases in search engines like Google (Sweeney, 2013; Noble, 2018) has prompted little investigation in the ACL com-",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 111,
      "context" : "Work on NLP and race remains sparse, particularly in contrast to concerns about gender bias, which have led to surveys, workshops, and shared tasks (Sun et al., 2019; Webster et al., 2019).",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 124,
      "context" : "Work on NLP and race remains sparse, particularly in contrast to concerns about gender bias, which have led to surveys, workshops, and shared tasks (Sun et al., 2019; Webster et al., 2019).",
      "startOffset" : 148,
      "endOffset" : 188
    }, {
      "referenceID" : 84,
      "context" : "While little work has examined the role of race in NLP specifically, prior work has discussed race in related fields, including human-computer interaction (HCI) (Ogbonnaya-Ogburu et al., 2020; Rankin and Thomas, 2019; Schlesinger et al., 2017), fairness in machine learning (Hanna et al.",
      "startOffset" : 161,
      "endOffset" : 243
    }, {
      "referenceID" : 91,
      "context" : "While little work has examined the role of race in NLP specifically, prior work has discussed race in related fields, including human-computer interaction (HCI) (Ogbonnaya-Ogburu et al., 2020; Rankin and Thomas, 2019; Schlesinger et al., 2017), fairness in machine learning (Hanna et al.",
      "startOffset" : 161,
      "endOffset" : 243
    }, {
      "referenceID" : 101,
      "context" : "While little work has examined the role of race in NLP specifically, prior work has discussed race in related fields, including human-computer interaction (HCI) (Ogbonnaya-Ogburu et al., 2020; Rankin and Thomas, 2019; Schlesinger et al., 2017), fairness in machine learning (Hanna et al.",
      "startOffset" : 161,
      "endOffset" : 243
    }, {
      "referenceID" : 43,
      "context" : ", 2017), fairness in machine learning (Hanna et al., 2020), and linguistics (Hudley et al.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 111,
      "context" : "Our work differs from NLP-focused related work on gender bias (Sun et al., 2019), ‘bias’ generally",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "language models (Bender et al., 2021) in its explicit focus on race and racism.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 48,
      "context" : "Our work calls for NLP researchers to consider the social hierarchies upheld and exacerbated by NLP research and to shift the field toward “greater inclusion and racial justice” (Hudley et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 43,
      "context" : "It has been widely accepted by social scientists that race is a social construct, meaning it “was brought into existence or shaped by historical events, social forces, political power, and/or colonial conquest” rather than reflecting biological or ‘natural’ differences (Hanna et al., 2020).",
      "startOffset" : 270,
      "endOffset" : 290
    }, {
      "referenceID" : 94,
      "context" : "and reflected race (the race you believe others perceive you to be) (Roth, 2016; Hanna et al., 2020; Ogbonnaya-Ogburu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 131
    }, {
      "referenceID" : 43,
      "context" : "and reflected race (the race you believe others perceive you to be) (Roth, 2016; Hanna et al., 2020; Ogbonnaya-Ogburu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 131
    }, {
      "referenceID" : 84,
      "context" : "and reflected race (the race you believe others perceive you to be) (Roth, 2016; Hanna et al., 2020; Ogbonnaya-Ogburu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 131
    }, {
      "referenceID" : 44,
      "context" : "some NLP work considers anti-Semitism a form of racism (Hasanuzzaman et al., 2017).",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 94,
      "context" : "Race depends on historical and social context—there are no ‘ground truth’ labels or categories (Roth, 2016).",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 111,
      "context" : "in a survey about gender bias, “Nonbinary genders as well as racial biases have largely been ignored in NLP” (Sun et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "Data A substantial amount of prior work has already shown how NLP systems, especially word embeddings and language models, can absorb and amplify social biases in data sets (Bolukbasi et al., 2016; Zhao et al., 2017).",
      "startOffset" : 173,
      "endOffset" : 216
    }, {
      "referenceID" : 133,
      "context" : "Data A substantial amount of prior work has already shown how NLP systems, especially word embeddings and language models, can absorb and amplify social biases in data sets (Bolukbasi et al., 2016; Zhao et al., 2017).",
      "startOffset" : 173,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "These studies focus on how training data might describe racial minorities in biased ways, for example, by examining words associated with terms like ‘black’ or traditionally European/African American names (Caliskan et al., 2017; Manzini et al., 2019).",
      "startOffset" : 206,
      "endOffset" : 251
    }, {
      "referenceID" : 70,
      "context" : "These studies focus on how training data might describe racial minorities in biased ways, for example, by examining words associated with terms like ‘black’ or traditionally European/African American names (Caliskan et al., 2017; Manzini et al., 2019).",
      "startOffset" : 206,
      "endOffset" : 251
    }, {
      "referenceID" : 0,
      "context" : "However, surveys suggest that Wikipedia editors are primarily from whitemajority countries,6 and several initiatives have pointed out systemic racial biases in Wikipedia coverage (Adams et al., 2019; Field et al., 2021).",
      "startOffset" : 179,
      "endOffset" : 219
    }, {
      "referenceID" : 32,
      "context" : "However, surveys suggest that Wikipedia editors are primarily from whitemajority countries,6 and several initiatives have pointed out systemic racial biases in Wikipedia coverage (Adams et al., 2019; Field et al., 2021).",
      "startOffset" : 179,
      "endOffset" : 219
    }, {
      "referenceID" : 85,
      "context" : "The representativeness of data sets is a well-discussed issue in social-oriented tasks, like inferring public opinion (Olteanu et al., 2019), but this issue is also an important consideration in ‘neutral’ tasks like parsing (Waseem et al.",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 123,
      "context" : ", 2019), but this issue is also an important consideration in ‘neutral’ tasks like parsing (Waseem et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : "• Classifiers for abusive language are more likely to label text containing identity terms like ‘black’ as offensive (Dixon et al., 2018).",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 40,
      "context" : "• GPT outputs text with more negative sentiment when prompted with AAE -like inputs (Groenwold et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 77,
      "context" : "Examples include examining how commentators describe football players of different races (Merullo et al., 2019) or how words like ‘prejudice’ have changed meaning over time (Vylomova et al.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 117,
      "context" : ", 2019) or how words like ‘prejudice’ have changed meaning over time (Vylomova et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "a common way to measure semantic change or estimate word meanings (Garg et al., 2018), Joseph and Morgan (2020) show that embedding associations do not always correlate with human opinions; in particular, correlations are stronger for be-",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 101,
      "context" : "Relatedly, in HCI, the recognition that authors’ own biases can affect their interpretations of results has caused some authors to provide self-disclosures (Schlesinger et al., 2017), but this practice is uncommon in NLP.",
      "startOffset" : 156,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "census race/ethnicity groups (or the provided inference model) (Blodgett et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "(WEAT) (Caliskan et al., 2017), which in turn draws data from Greenwald et al.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 98,
      "context" : "(2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.",
      "startOffset" : 120,
      "endOffset" : 220
    }, {
      "referenceID" : 14,
      "context" : "(2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.",
      "startOffset" : 120,
      "endOffset" : 220
    }, {
      "referenceID" : 129,
      "context" : "(2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.",
      "startOffset" : 120,
      "endOffset" : 220
    }, {
      "referenceID" : 130,
      "context" : "(2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.",
      "startOffset" : 120,
      "endOffset" : 220
    }, {
      "referenceID" : 40,
      "context" : "(2016) does serve as a starting point and forms the basis of most current work assessing performance gaps in NLP models (Sap et al., 2019; Blodgett et al., 2018; Xia et al., 2020; Xu et al., 2019; Groenwold et al., 2020), but even this corpus is explicitly not intended to infer race.",
      "startOffset" : 120,
      "endOffset" : 220
    }, {
      "referenceID" : 63,
      "context" : ", 2017), differences in naming conventions often do not translate, leading some studies to omit examining racial bias in non-English languages (Lauscher and Glavaš, 2019).",
      "startOffset" : 143,
      "endOffset" : 170
    }, {
      "referenceID" : 112,
      "context" : ", 2018), and names common among Black and white children were not distinctly different prior to the 1970s (Fryer Jr and Levitt, 2004; Sweeney, 2013).",
      "startOffset" : 106,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "More work is needed to understand any privacy concerns and the strengths and limitations of these data (Blodgett et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 69,
      "context" : "data, such as self-reported race in an online community (Loveys et al., 2018), or crowd-sourced annotations of perceived race of football players (Merullo et al.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 77,
      "context" : ", 2018), or crowd-sourced annotations of perceived race of football players (Merullo et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "At worst, it risks reinforcing racism by presenting racial divisions as natural, rather than the product of social and historical context (Bowker and Star, 2000).",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 45,
      "context" : "A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in any other NLP research areas.",
      "startOffset" : 101,
      "endOffset" : 177
    }, {
      "referenceID" : 71,
      "context" : "A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in any other NLP research areas.",
      "startOffset" : 101,
      "endOffset" : 177
    }, {
      "referenceID" : 114,
      "context" : "A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in any other NLP research areas.",
      "startOffset" : 101,
      "endOffset" : 177
    }, {
      "referenceID" : 66,
      "context" : "A small selection of papers have examined intersectional biases in embeddings or word co-occurrences (Herbelot et al., 2012; May et al., 2019; Tan and Celis, 2019; Lepori, 2020), but we did not identify mentions of intersectionality in any other NLP research areas.",
      "startOffset" : 101,
      "endOffset" : 177
    }, {
      "referenceID" : 101,
      "context" : "Surveys in HCI offer further frameworks on how to incorporate identity and intersectionality into computational research (Schlesinger et al., 2017; Rankin and Thomas, 2019).",
      "startOffset" : 121,
      "endOffset" : 172
    }, {
      "referenceID" : 91,
      "context" : "Surveys in HCI offer further frameworks on how to incorporate identity and intersectionality into computational research (Schlesinger et al., 2017; Rankin and Thomas, 2019).",
      "startOffset" : 121,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "gorithms perform worse on darker-skinned than lighter-skinned people (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019), researchers at IBM created the “Diversity in Faces” data set, which consists of 1 million photos sampled from",
      "startOffset" : 69,
      "endOffset" : 124
    }, {
      "referenceID" : 89,
      "context" : "gorithms perform worse on darker-skinned than lighter-skinned people (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019), researchers at IBM created the “Diversity in Faces” data set, which consists of 1 million photos sampled from",
      "startOffset" : 69,
      "endOffset" : 124
    }, {
      "referenceID" : 76,
      "context" : "the the publicly available YFCC-100M data set and annotated with “craniofacial distances, areas and ratios, facial symmetry and contrast, skin color, age and gender predictions” (Merler et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 52,
      "context" : "authors, attendees, and fellows, NLP research overwhelmingly favors a small set of languages, with a heavy skew towards European languages (Joshi et al., 2020) and ‘standard’ language varieties (Kumar et al.",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 60,
      "context" : ", 2020) and ‘standard’ language varieties (Kumar et al., 2021).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 115,
      "context" : "In the context of perpetuating racism, examples include criticism of tools for predicting demographic information (Tatman, 2020) and automatic prison term prediction (Leins et al.",
      "startOffset" : 114,
      "endOffset" : 128
    }, {
      "referenceID" : 65,
      "context" : "In the context of perpetuating racism, examples include criticism of tools for predicting demographic information (Tatman, 2020) and automatic prison term prediction (Leins et al., 2020), motivated by the history of using technology to police racial minorities and related criticism in other fields (Browne, 2015; Buo-",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 122,
      "context" : "Work on abusive language often aims to detect racism for content moderation (Waseem and Hovy, 2016).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like ‘black’ or text containing linguistic markers of AAE as toxic (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Xia et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 278
    }, {
      "referenceID" : 98,
      "context" : "However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like ‘black’ or text containing linguistic markers of AAE as toxic (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Xia et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 278
    }, {
      "referenceID" : 26,
      "context" : "However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like ‘black’ or text containing linguistic markers of AAE as toxic (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Xia et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 278
    }, {
      "referenceID" : 129,
      "context" : "However, more recent work has show that existing hate speech classifiers are likely to falsely label text containing identity terms like ‘black’ or text containing linguistic markers of AAE as toxic (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Xia et al., 2020).",
      "startOffset" : 199,
      "endOffset" : 278
    }, {
      "referenceID" : 59,
      "context" : "An ICML 2020 workshop titled “Participatory Approaches to Machine Learning” highlights a number of papers in this area (Kulynych et al., 2020; Brown et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "An ICML 2020 workshop titled “Participatory Approaches to Machine Learning” highlights a number of papers in this area (Kulynych et al., 2020; Brown et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 162
    }, {
      "referenceID" : 33,
      "context" : "draw on precedents for this type of engagement from other fields, such as participatory design and value sensitive design models (Friedman et al., 2013).",
      "startOffset" : 129,
      "endOffset" : 152
    } ],
    "year" : 2021,
    "abstractText" : "Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed singledimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",
    "creator" : "LaTeX with hyperref"
  }
}