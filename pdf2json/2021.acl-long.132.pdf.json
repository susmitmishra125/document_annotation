{
  "name" : "2021.acl-long.132.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
    "authors" : [ "Bertie Vidgen", "Tristan Thrush", "Zeerak Waseem", "Douwe Kiela" ],
    "emails" : [ "bvidgen@turing.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1667–1682\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1667"
    }, {
      "heading" : "1 Introduction",
      "text" : "Accurate detection of online hate speech is important for ensuring that such content can be found and tackled scalably, minimizing the risk that harm will be inflicted on victims and making online spaces more accessible and safe. However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020). To address these challenges, we present a human-and-model-in-the-loop process for collecting data and training hate detection models.\nOur approach encompasses four rounds of data generation and model training. We first trained a classification model using previously released hate speech datasets. We then tasked annotators with\npresenting content that would trick the model and yield misclassifications. At the end of the round we trained a new model using the newly presented data. In the next round the process was repeated with the new model in the loop for the annotators to trick. We had four rounds but this approach could, in principle, be continued indefinitely.\nRound 1 contains original content created synthetically by annotators. Rounds 2, 3 and 4 are split into half original content and half perturbations. The perturbations are challenging ‘contrast sets’, which manipulate the original text just enough to flip the label (e.g. from ‘Hate’ to ‘Not Hate’) (Kaushik et al., 2019; Gardner et al., 2020). In Rounds 3 and 4 we also tasked annotators with exploring specific types of hate and taking close inspiration from real-world hate sites to make content as adversarial, realistic, and varied as possible.\nModels have lower accuracy when evaluated on test sets from later rounds as the content becomes more adversarial. Similarly, the rate at which annotators trick models also decreases as rounds progress (see Table 3). At the same time, models trained on data from later rounds achieve higher accuracy, indicating that their performance improves (see Table 4). We verify improved model performance by evaluating them against the HATECHECK functional tests (Röttger et al., 2020), with accuracy improving from 60% in Round 1 to 95% in Round 4. In this way the models ‘learn from the worst’ because as the rounds progress (a) they become increasingly accurate in detecting hate which means that (b) annotators have to provide more challenging content in order to trick them.\nWe make three contributions to online hate classification research. First, we present a human-andmodel-in-the-loop process for training online hate detection models. Second, we present a dataset of 40, 000 entries, of which 54% are hate. It includes fine-grained annotations by trained annotators for\nlabel, type and target (where applicable). Third, we present high quality and robust hate detection models. All data, code and annotation guidelines are available.1"
    }, {
      "heading" : "2 Background",
      "text" : "Benchmark datasets Several benchmark datasets have been put forward for online hate classification (Waseem and Hovy, 2016; Waseem, 2016; Davidson et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019, 2020; Vidgen et al., 2020, 2021). These datasets offer a comparison point for detection systems and have focused the field’s attention on important subtasks, such as classification across different languages, domains and targets of hate. Performance on some benchmark datasets has increased substantially through the use of more advanced models. For instance, in the original Waseem and Hovy (2016) paper in 2016, the authors achieved an F1 of 0.74. By 2018 this had increased to 0.93 (Pitsilis et al., 2018).\nNumerous problems have been identified with hate speech training datasets, such as lacking linguistic variety, being inexpertly annotated and degrading over time (Vidgen et al., 2019a; Poletto et al., 2020). Vidgen and Derczynski (2020) examined 63 open-source abusive language datasets and found that 27 (43%) were sourced from Twitter (Vidgen and Derczynski, 2020). In addition, many datasets are formed with bootstrapped sampling, such as keyword searches, due to the low prevalence of hate speech ‘in the wild’ (Vidgen et al., 2019b). Such bootstrapping can substantially bias the nature and coverage of datasets (Wiegand et al., 2019). Models trained on historical data may also not be effective for present-day hate classification models given how quickly online conversations evolve (Nobata et al., 2016).\nModel limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020). These errors often make models unsuitable for use in downstream tasks, such as moderating online content or measuring online hate.\n1https://github.com/bvidgen/ Dynamically-Generated-Hate-Speech-Dataset\nFalse positives are non-hateful entries which are incorrectly classified as hateful. Vidgen et al. (2020) report that 29% of errors from a classifier for East Asian prejudice are due to lexical similarities between hateful and non-hateful entries, such as abuse directed towards out-of-scope targets being misclassified as Sinophobic. Other research shows that some identity terms (e.g. ‘gay’) are substantially more likely to appear in toxic content in training datasets, leading models to overfit on them (Dixon et al., 2018; Kennedy et al., 2020). Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irrespective of how they are used (Waseem et al., 2018; Davidson et al., 2017; Kurrek et al., 2020; Palmer et al., 2020). This is problematic when the terms are used as part of counter speech (Wright et al., 2017; Chung et al., 2019) or have been reclaimed by the targeted group (Waseem et al., 2018; Sap et al., 2019). Models can also misclassify interpersonal abuse and incivil language as hateful (Wulczyn et al., 2017a; Zampieri et al., 2019; Palmer et al., 2020).\nFalse negatives are hateful entries which are incorrectly classified as non-hateful. Gröndahl et al. (2018) show that making simple changes such as inserting spelling errors, using leetspeak2, changing word boundaries, and appending words can lead to misclassifications of hate. Hosseini et al. (2017) also investigate how detection models can be attacked and report similar findings. In other cases, false negatives can be provoked by changing the ‘sensitive’ attribute of hateful content, such as changing the target from ‘gay’ to ‘black’ people (Garg et al., 2019). This can happen when models are trained on data which only contains hate directed against a limited set of targets (Salminen et al., 2020). Another source of false negatives is when classification systems are applied to out-ofdomain settings, such as system trained on Twitter data being applied to data from Gab (Karan and Šnajder, 2018; Pamungkas et al., 2020; Swamy et al., 2019; Basile et al., 2019; Salminen et al., 2020). Subtle and implicit forms of hate speech can also create false negatives (Vidgen and Yasseri, 2019; Palmer et al., 2020; Mathew et al., 2020), as well as more ‘complex’ forms of speech such as sarcasm, irony, adjective nominalization and rhetorical questions (Caselli et al., 2020; Vidgen et al.,\n2Leetspeak refers to the obfuscation of words by replacing letters with similar looking numbers and symbols.\n2019a).\nDynamic benchmarking and contrast sets Addressing the numerous flaws of hate detection models is a difficult task. The problem may partly lie in the use of static benchmark datasets and fixed model evaluations. In other areas of Natural Language Processing, several alternative model training and dataset construction paradigms have been presented, involving dynamic and iterative approaches. In a dynamic dataset creation setup, annotators are incentivised to produce high-quality ‘adversarial’ samples which are challenging for baseline models, repeating the process over multiple rounds (Nie et al., 2020). This offers a more targeted way of collecting data. Dinan et al. (2019) ask crowd-workers to ‘break’ a BERT model trained to identify toxic comments and then retrain it using the new examples. Their final model is more robust to complex forms of offensive content, such as entries with figurative language and without profanities.\nAnother way of addressing the limitations of static datasets is through creating ‘contrast sets’ of perturbations (Kaushik et al., 2019; Gardner et al., 2020). By making minimal label-changing modifications that preserve ‘lexical/syntactic artifacts present in the original example’ (Gardner et al., 2020, p. 1308) the risk of overfitting on spurious correlations is minimized. Perturbations have only received limited attention in the context of hate detection. Samory et al. (2020) create 2, 000 ‘hard-to-classify’ not-sexist examples which contrast sexist examples in their dataset. They show that fine-tuning a BERT model with the contrast set produces more robust classification system.\nDynamic benchmarking and contrast sets highlight the effectiveness of developing datasets in a directed and adaptive way, ensuring that models learn from and are evaluated on the most challenging content. However, to date, these approaches remain under-explored for hate speech detection and to the best of our knowledge no prior work in hate speech detection has combined the two approaches within one system."
    }, {
      "heading" : "3 Dataset labels",
      "text" : "Previous research shows the limitations of using only a binary labelling schema (i.e., ‘Hate’ and ‘Not Hate’). However, there are few established taxonomies and standards in online hate research, and most of the existing datasets have been labelled\nwith very different schemas. The hierarchical taxonomy we present aims for a balance between granularity versus conceptual distinctiveness and annotation simplicity, following the guidance of Nickerson et al. (2013). All entries are assigned to either ‘Hate’ or ‘Not Hate’. ‘Hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” (Warner and Hirschberg, 2012). For ‘Hate’, we also annotate secondary labels for the type and target of hate. The taxonomy for the type of hate draws on and extends previous work, including Waseem and Hovy (2016); Vidgen et al. (2019a); Zampieri et al. (2019)."
    }, {
      "heading" : "3.1 Types of hate",
      "text" : "Derogation Content which explicitly attacks, demonizes, demeans or insults a group. This resembles similar definitions from Davidson et al. (2017), who define hate as content that is ‘derogatory’, Waseem and Hovy (2016) who include ‘attacks’ in their definition, and Zampieri et al. (2019) who include ‘insults’.\nAnimosity Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies (Waseem et al., 2017; Vidgen and Yasseri, 2019; Kumar et al., 2018).\nThreatening language Content which expresses intention to, support for, or encourages inflicting harm on a group, or identified members of the group. This category is used in datasets by Hammer (2014), Golbeck et al. (2017) and Anzovino et al. (2018).\nSupport for hateful entities Content which explicitly glorifies, justifies or supports hateful actions, events, organizations, tropes and individuals (collectively, ‘entities’).\nDehumanization Content which ‘perceiv[es] or treat[s] people as less than human’ (Haslam and Stratemeyer, 2016). It often involves describing groups as leeches, cockroaches, insects, germs or rats (Mendelsohn et al., 2020)."
    }, {
      "heading" : "3.2 Targets of hate",
      "text" : "Hate can be targeted against any vulnerable, marginalized or discriminated-against group. We provided annotators with a non-exhaustive list of 29 identities to focus on (e.g., women, black people, Muslims, Jewish people and gay people), as well\nas a small number of intersectional variations (e.g., ‘Muslim women’). They are given in Appendix A. Some identities were considered out-of-scope for Hate, including men, white people, and heterosexuals."
    }, {
      "heading" : "4 Annotation",
      "text" : "Data was annotated using an open-source web platform for dynamic dataset creation and model benchmarking.3 The platform supports human-andmodel-in-the-loop dataset creation for a variety of NLP tasks. Annotation was overseen by two experts in online hate. The annotation process is described in the following section. Annotation guidelines were created at the start of the project and then updated after each round in response to the increased need for detail from annotators. We followed the guidance for protecting and monitoring annotator well-being provided by Vidgen et al. (2019a). 20 annotators were recruited. They received extensive training and feedback during the project. Full details on the annotation team are given in Appendix E. The small pool of annotators was driven by the logistical constraints of hiring and training them to the required standard and protecting their welfare given the sensitivity and complexity of the topic. Nonetheless, it raises the potential for bias. We take steps to address this in our test set construction and provide an annotator ID with each entry in our publicly-released dataset to enable further research into this issue."
    }, {
      "heading" : "5 Dataset formation",
      "text" : "The dataset was generated over four rounds, each of which involved ∼10, 000 entries. The final dataset comprises 41, 255 entries, as shown in Table 1. The ten groups that are targeted most often are given in Table 2. Entries could target multiple groups. After each round, the data was split into training, dev and test splits of 80%, 10% and 10%, respectively. Approximately half of the entries in the test sets are produced by annotators who do not appear in the training and dev sets (between 1 and 4 in each round). This makes the test sets more challenging and minimizes the risk of annotator bias given our relatively small pool of annotators (Geva et al., 2019). The other half of each test set consists of content from annotators who do appear in the training and dev sets.\n3https://anonymized-url\nRounds 2, 3 and 4 contain perturbations. In 18 cases the perturbation does not flip the label. This mistake was only identified after completion of the paper and is left in the dataset. These cases can be identified by checking whether original and perturbed entries that have been linked together have the same labels (e.g., whether an original and perturbation are both assigned to ’Hate’).\nTarget model implementation Every round has a model in the loop, which we call the ‘target model’. The target model is always trained on a combination of data collected in the previous round(s). For instance, M2 is the target model used in R2, and was trained on R1 and R0 data. For consistency, we use the same model architecture everywhere, specifically RoBERTa (Liu et al., 2019) with a sequence classification head. We use the implementation from the Transformers (Wolf et al., 2019) library. More details are available in appendix D.\nFor each new target model, we identify the best sampling ratio of previous rounds’ data using the dev sets. M1 is trained on R0 data. M2 is trained on R0 data and R1 upsampled to a factor of five. M3 is trained on the data used for M2 and R2 data upsampled to a factor of one hundred. M4 is trained on the data used for M3 and one lot of the R3 data."
    }, {
      "heading" : "5.1 Round 1 (R1)",
      "text" : "The target model in R1 is M1, a RoBERTa model trained on R0 which consists of 11 English language training datasets for hate and toxicity taken from hatespeechdata.com, as reported in Vidgen and Derczynski (2020). It includes widely-used datasets provided by Waseem (2016), Davidson et al. (2017) and Founta et al. (2018). It comprises 468, 928 entries, of which 22% are hateful/toxic. The dataset was anonymized by replacing usernames, indicated by the ‘@’ symbol. URLs were also replaced with a special token. In R1, annotators were instructed to enter synthetic content into the model that would trick M1 using their own creativity and by exploiting any model weaknesses they identified through the real-time feedback.\nAll entries were validated by one other annotator and entries marked as incorrect were sent for review by expert annotators. This happened with 1, 011 entries. 385 entries were excluded for being entirely incorrect. In the other cases, the expert annotator decided the final label and/or made minor\nadjustments to the text. The final dataset comprises 11, 157 entries of which 7, 197 are ‘Hate’ (65%) and 3, 960 are ‘Not Hate’ (35%). The type and target of Hate was not recorded by annotators in R1."
    }, {
      "heading" : "5.2 Round 2 (R2)",
      "text" : "A total of 9, 996 entries were entered in R2. The hateful entries are split between Derogation (3, 577, 72%), Dehumanization (255, 5%), Threats (380, 8%), Support for hateful entities (39, 1%) and Animosity (759, 15%). In R2 we gave annotators adversarial ‘pivots’ to guide their work, which we identified from a review of previous literature (see Section 2). The 10 hateful and 12 not hateful adversarial pivots, with examples and a description, are given in Appendix B. Half of R2 comprises originally entered content and the other half comprises perturbed contrast sets.\nFollowing Gardner et al. (2020), perturbations were created offline without feedback from a model-in-the-loop. Annotators were given four\nmain points of guidance: (1) ensure perturbed entries are realistic, (2) firmly meet the criteria of the flipped label and type, (3) maximize diversity within the dataset in terms of type, target and how entries are perturbed and (4) make the least changes possible while meeting (1), (2) and (3). Common strategies for perturbing entries included changing the target (e.g., from ‘black people’ to ‘the local council’), changing the sentiment (e.g. ‘It’s wonderful having gay people round here’), negating an attack (e.g. ‘Muslims are not a threat to the UK’) and quoting or commenting on hate.\nOf the original entries, those which fooled M1 were validated by between three and five other annotators. Every perturbation was validated by one other annotator. Annotators could select: (1) correct if they agreed with the label and, for Hate, the type/target, (2) incorrect if the label was wrong or (3) flag if they thought the entry was unrealistic and/or they agreed with the label for hate but disagreed with the type or target. Krippendorf’s alpha is 0.815 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating extremely high levels of agreement (Hallgren, 2012). All of the original entries identified by at least two validators as incorrect/flagged, and perturbations which were identified by one validator as incorrect/flagged, were sent for review by an expert annotator. This happened in 760 cases in this round.\nLessons from R2 The validation and review process identified some limitations of the R2 dataset. First, several ‘template’ statements were entered by annotators. These are entries which have a standardized syntax and/or lexicon, with only the identity changed, such as ‘[Identity] are [negative attribute]’. When there are many cases of each tem-\nplate they are easy for the model to correctly classify because they create a simple decision boundary. Discussion sessions showed that annotators used templates (i) to ensure coverage of different identities (an important consideration in making a generalisable online hate classifier) and (ii) to maximally exploit model weaknesses to increase their model error rate. We banned the use of templates. Second, in attempting to meet the ‘pivots’ they were assigned, some annotators created unrealistic entries. We updated guidance to emphasize the importance of realism. Third, the pool of 10 trained annotators is large for a project annotating online hate but annotator biases were still produced. Model performance was high in R2 when evaluated on a training/dev/test split with all annotators stratified. We then held out some annotators’ content and performance dropped substantially. We use this setup for all model evaluations."
    }, {
      "heading" : "5.3 Round 3 (R3)",
      "text" : "In R3 annotators were tasked with finding realworld hateful online content to inspire their entries. All real-world content was subject to at least one substantial adjustment prior to being presented to the model. 9, 950 entries were entered in R3. The hateful entries are split between Derogation (3, 205, 64%), Dehumanization (315, 6%), Threats (147, 3%), Support for hateful entities (104, 2%) and Animosity (1, 210, 24%). Half of R3 comprises originally entered content (4, 975) and half comprises perturbed contrast sets (4, 975).\nThe same validation procedure was used as with R2. Krippendorf’s alpha was 0.55 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating moderate levels of agreement (Hallgren, 2012). This is lower than R2, but still comparable with other hate speech datasets (e.g., Wulczyn et al. (2017b) achieve Krippnedorf’s alpha of 0.45). Note that more content is labelled as Animosity compared with R2 (24% compared with 15%), which tends to have higher levels of disagreement. 981 entries were reviewed by the expert annotators."
    }, {
      "heading" : "5.4 Round 4 (R4)",
      "text" : "As with R3, annotators searched for real-world hateful online content to inspire their entries. In addition, each annotator was given a target identity to focus on (e.g., Muslims, women, Jewish people). The annotators (i) investigated hateful online forums and communities relevant to the target\nidentity to find the most challenging and nuanced content and (ii) looked for challenging non-hate examples, such as neutral discussions of the identity. 10, 152 entries were entered in R4, comprising 5, 076 ‘Hate’ and 5, 076 ‘Not Hate’. The hateful entries are split between Derogation (3, 128, 62%), Dehumanization (331, 7%), Threats (82, 2%), Support for hateful entities (61, 1%) and Animosity (1, 474, 29%). Half of R4 comprises originally entered content (5, 076) and half comprises perturbed contrast sets (5, 076). The same validation procedure was used as in R2 and R3. Krippendorf’s alpha was 0.52 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating moderate levels of agreement (Hallgren, 2012). This is similar to R2. 967 entries were reviewed by the expert annotators following the validation process."
    }, {
      "heading" : "6 Model performance",
      "text" : "In this section, we examine the performance of models on the collected data, both when used inthe-loop during data collection (measured by the model error rate on new content shown by annotators), as well as when separately evaluated against the test sets in each round’s data. We also examine how models generalize by evaluating them on the out-of-domain suite of diagnostic functional tests in HATECHECK."
    }, {
      "heading" : "6.1 Model error rate",
      "text" : "The model error rate is the rate at which annotatorgenerated content tricks the model. It decreases as the rounds progress, as shown in Table 3. M1, which was trained on a large set of public hate speech datasets, was the most easily tricked, even though many annotators were learning and had not been given advice on its weaknesses. 54.7% of entries tricked it, including 64.6% of Hate and 49.2% of Not Hate. Only 27.7% of content tricked the final model (M4), including 23.7% of Hate and 31.7% of Not Hate. The type of hate affected how frequently entries tricked the model. In general, more explicit and overt forms of hate had the lowest model error rates, with threatening language and dehumanization at 18.2% and 24.8% on average, whereas support for hateful entities and animosity had the highest error (55.4% and 46.4% respectively). The model error rate falls as the rounds progress but nonetheless this metric potentially still underestimates the increasing difficulty of the rounds and the improvement in the models.\nAnnotators became more experienced and skilled over the annotation process, and entered progressively more adversarial content. As such the content that annotators enter becomes far harder to classify in the later rounds, which is also reflected in all models’ lower performance on the later round test sets (see Table 4)."
    }, {
      "heading" : "6.2 Test set performance",
      "text" : "Table 4 shows the macro F1 of models trained on different combinations of data, evaluated on the test sets from each round (see Appendix C for dev set performance). The target models achieve lower scores when evaluated on test sets from the later rounds, demonstrating that the dynamic approach to data collection leads to increasingly more challenging data. The highest scores for R3 and R4 data are in the mid-70s, compared to the high 70s in R2 and low 90s in R1. Generally, the target models from the later rounds have higher performance across the test sets. For instance, M4 is the best performing model on R1, R2 and R4 data. It achieves 75.97 on the R4 data whereas M3 achieves 74.83 and M2 only 60.87. A notable exception is M1 which outperforms M2 on the R3 and R4 test sets.\nTable 4 presents the results for models trained on just the training sets from each round (with no upsampling), indicated by M(RX only). In general the performance is lower than the equivalent target model. For instance, M4 achieves macro F1 of 75.97 on the R4 test data. M(R3 only) achieves 73.16 on that test set and M(R4 only) just 69.6. In other cases, models which are trained on just one round perform well on some rounds but are far worse on others. Overall, building models cumulatively leads to more consistent performance. Table 4 also shows models trained on the cumulative rounds of data with no upsampling, indicated by M(RX+RY). In general, performance is lower with-\nout upsampling; the F1 of M3 is 2 points higher on the R3 test set than the equivalent non-upsampled model (M(R0+R1+R2))."
    }, {
      "heading" : "6.3 HateCheck",
      "text" : "To better understand the weaknesses of the target models from each round, we apply them to HATECHECK, as presented by Röttger et al. (2020). HATECHECK is a suite of functional tests for hate speech detection models, based on the testing framework introduced by Ribeiro et al. (2020). It comprises 29 tests, of which 18 correspond to distinct expressions of hate and the other 11 are non-hateful contrasts. The selection of functional tests is motivated by a review of previous literature and interviews with 21 NGO workers. From the 29 tests in the suite, 3, 728 labelled entries are generated in the dataset of which 69% are ‘Hate’ and 31% are ‘Not Hate’.\nPerformance of target models trained on later rounds is substantially higher, increasing from 60% (on both ‘Hate’ and ‘Not Hate’) combined for M1\nto 95% for M4. Performance is better than all four models evaluated by Röttger et al. (2020), of which Perspective’s toxicity classifier4 is best performing with 77% overall accuracy, including 90% on ‘Hate’ and 48% on ‘Not Hate’. Notably, the performance of M4 is consistent across both ‘Hate’ and ‘Not Hate’, achieving 95% and 93% respectively. This is in contrast to earlier target models, such as M2 which achieves 91% on ‘Hate’ but only 67% on ‘Not Hate’ (note that this is actually a reduction in performance from M1 on ‘Not Hate’). Note that HATECHECK only has negative predictive power. These results indicate the absence of particular weaknesses in models rather than necessarily characterising generalisable strengths.\nA further caveat is that in R2 the annotators were given adversarial pivots to improve their ability to trick the models (See above). These pivots exploit similar model weaknesses as the functional tests in HATECHECK expose, which creates a risk that this gold standard is not truly independent. We did not identify any exact matches, although after lowering case and removing punctuation there are 21 matches. This is just 0.05% of our dataset but indicates a risk of potential overlap and cross-dataset similarity."
    }, {
      "heading" : "7 Discussion",
      "text" : "Online hate detection is a complex and nuanced problem, and creating systems that are accurate,\n4See: https://www.perspectiveapi.com/#/ home.\nrobust and generalisable across target, type and domain has proven difficult for AI-based solutions. It requires having datasets which are large, varied, expertly annotated and contain challenging content. Dynamic dataset generation offers a powerful and scalable way of creating these datasets, and training and evaluating more robust and high performing models. Over the four rounds of model training and evaluation we show that the performance of target models improves, as measured by their accuracy on the test sets. The robustness of the target models from later rounds also increases, as shown by their better performance on HATECHECK.\nDynamic data creation systems offer several advantages for training better performing models. First, problems can be addressed as work is conducted – rather than creating the dataset and then discovering any inadvertent design flaws. For instance, we continually worked with annotators to improve their understanding of the guidelines and strategies for tricking the model. We also introduced perturbations to ensure that content was more challenging. Second, annotators can input more challenging content because their work is guided by real-time feedback from the target model. Discussion sessions showed that annotators responded to the models’ feedback in each round, adjusting their content to find better ways to trick it. This process of people trying to find ways to circumvent hate speech models such that their content goes undetected is something that happens often in the real world. Third, dynamic datasets can be\nconstructed to better meet the requirements of machine learning; our dataset is balanced, comprising ∼54% hate. It includes hate targeted against a large number of targets, providing variety for the model to learn from, and many entries were constructed to include known challenging content, such as use of slurs and identity referents.\nHowever, our approach also presents some challenges. First, it requires substantial infrastructure and resources. This project would not have been possible without the use of an online interface and a backend that can serve up state-of-the-art hate speech detection models with relatively low latency. Second, it requires substantial domain expertise from dataset creators as well as annotators, such as knowing where to find real-world hate to inspire synthetic entries. This requires a cross-disciplinary team, combining social science with linguistics and machine learning expertise. Third, evaluating and validating content in a time-constrained dynamic setting can introduce new pressures on the annotation process. The perturbation process also requires additional annotator training, or else might introduce other inadvertent biases."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We presented a human-and-model-in-the-loop process for training an online hate detection system. It was employed dynamically to collect four rounds of hate speech datasets. The datasets are large and high quality, having been obtained using only expert annotators. They have fine-grained annotations for the type and target of hate, and include perturbations to increase the dataset difficulty. We demonstrated that the models trained on these dynamically generated datasets are much better at the task of hate speech detection, including evaluation on out-of-domain functional test suites.\nIn future work we aim to expand the size and diversity of the annotator pool for further rounds of dynamic adversarial data collection. We would like to evaluate different models in-the-loop beyond RoBERTa. The datasets also open many new avenues of investigation, including training models on only original entries and evaluating against perturbations (and vice versa) and training multi-label results for type and target of hate. Data collection for future rounds is ongoing.\nImpact Statement & Ethical Considerations\nIn the Impact Statement we address relevant ethical considerations that were not explicitly discussed in the main body of the paper.\nData The entries in the dataset were created by the annotation team and, where needed, reviewed by the expert annotators. In no cases did annotators enter content that they found on online sites. All entries which were closely inspired by real-world content (e.g., data entered during round 4) had substantial adjustments made to them. As such, the data is synthetic.\nAnnotator Compensation We employed a team of twenty annotators to enter content who worked varying hours on a flexible basis over four months. Annotators were compensated at a rate of £16 per hour. The rate was set 50% above the local living wage (£10.85), even though all work was completed remotely. All training time and meetings were paid.\nIntended Use The approach, dataset and models presented here are intended to support more accurate and robust detection and classification of online hate. We anticipate that the high-quality and fine-grained labels in the dataset will advance research in online hate in other ways, such as enabling multiclass classification of types and targets of online hate.\nPotential Misuse The dataset and models we present could in principle be used to train a generative hate speech model. Alternatively, the dataset and models could be used to better understand the limitations of current detection tools and then attack them. For instance, if a malicious actor investigated our models then they could better understand what content tricks content moderation tools and then use this knowledge to avoid their content being flagged on social media platforms. However, we believe that these outcomes are unlikely. We do not report any new weaknesses that have not been established in previous research, and the models we present still contain several limitations. Further, it is unlikely that a malicious actor would be able to train a powerful enough generative model from this dataset (given its size and composition) to affect their activities. Overall, the scientific and social benefits of the present research arguably outweighs the small risk of their misuse."
    }, {
      "heading" : "A List of identities",
      "text" : "See Table 5 for a list of identities focused on during data collection."
    }, {
      "heading" : "B List of pivots in R2",
      "text" : "Hate pivots\nConvoluted statements Statements which express hate through a convoluted sentence structure, e.g. ‘I’ll make sure that X don’t exist anymore because that’s what I want to do and I dont care if nobody knows it”.\nNegation of neutrality in the second clause Statements which initially appear to be neutral (or positive) about an identity, but then undermine it with a caveat, e.g. ‘some people say it’s not fair to call X dumb. . . but 9 times out of 10 it’s true!’.\nLong phrases Statements which are long, often involving multiple clauses and sentences. In such cases the hate may use more subtle forms of language and/or rely more heavily on tone, e.g. ‘Italy is a beautiful country and I love it so much, from its food to its fashion, from its history to its arts, but i am very worried about the current trends which remind me of the 1930s, So let’s import X to change up the food, fashion, history and arts?’\nRhetorical questions Statements which ask a rhetorical question in order to make a hateful point. Their goal is to express hate rather than to elicit an actual answer, e.g. ‘Aren’t all X penny pinching, after all?’\nMisspelling (swapped characters) Words which are intentionally misspelt so that, although the meaning is clear to a human, the model misclassifiers it because the hate is ‘obfuscated’.5 e.g. n*gga: ‘N!gg@’, ‘n!gga’, ‘n1gg4”, f*g: ‘f@g’, ‘f4g’, ‘f@gz’, k*ke: ‘k1ke’, ‘k1k3’.\nMisspelling (elongation) Words which are misspelt so that they are far longer, with many repeated letters, e.g. ‘I think letting gaaaaaaaay people have rights is just soooooooo awful’.\nMisspelling (spaces) Words which are misspelt so that there are spaces between letters. Misspelling of terms includes: ‘T R A N N Y S’, ‘J e w s’, ‘N i g g a’.\n5Note we use * for the first vowel in each slur to minimize use of harmful language.\nSynonym (hate expressed with infrequently used terms) Statements which contain rarely used terms,, e.g. rather than saying ‘X are scum’ they use the phrase, ‘X are all sour misanthropes’. Equally, such statements might just direct unusual / less common forms of abuse at the targeted identity. Infrequently used terms to insult a group include: ‘Obnoxious’, ‘Heinous’, ‘Abhorrent’, ‘Unlawful’, ‘Dishonorable’.\nTerms with positive sentiment Statements which contain positive phrases and terms but still express hate. In particular, phrases which use the term ’love’ are often identified as not hateful, e.g. ‘I just love it when all of the brown people leave my office so I dont have to look at them’.\nRandom statement Statements which are ’random’ and do not content any features which are likely to trick the model.\nNot Hate pivots\nCounter speech (by another identity) Statements which challenge, counter or dismiss hate, which are expressed by someone who does not identify with the group being attacked. This often involves referring to or quoting hateful content – which models may mistake for genuine hate.\nCounter speech (by target identity) Statements which challenge, counter or dismiss hate, which are expressed by someone who is from the group being attacked.\nCounter speech (use of negation) Statements which are neutral and state that a group does not have a (negative or neutral) attribute. This is presented primarily as a contrast set to Negated nonhate (in the Hateful pivots), e.g. ‘Jews are not money lovers’.\nNot hateful use of a term that is also a referent for an identity (polysemy) Statements that mention a referent for an identity (in a way that is not meant to refer to the identity). For instance, using ‘black’ as a colour, e.g. ‘the black cat down the road really ruins the neighbourhood’.\nUse of profanities Statements which contain a profanity in a not hateful way. They are often used as an adjective or adverb to describe an emotion or to place emphasis on what is being said, e.g. ‘fucking hell today was a lot of bullshit’.\nNegativity against objects Statements which attack, criticise or express negativity against inanimate objects, such as sofa or a car, e.g. ‘this cup is totally rubbish’.\nPersonal abuse (direct) Statements which are aggressive, insulting or abusive against an individual using a direct personal pronoun (i.e. ‘you’), e.g. ‘you are a complete joke and no-one respects you, loser’.\nPersonal abuse (indirect) Statements which are aggressive, insulting or abusive against an individual who is not part of the conversation and as such is referred to with an indirect personal pronoun (i.e. ‘he’, ‘she’, ‘they’), e.g. ‘he is such a waste of space. I hope he dies’.\nNegativity against concepts Statements which attack, criticise or express negativity against concepts and ideologies, such as political ideologies, economic ideas and philosophical ideals, e.g. ‘I’ve never trusted capitalism. It’s bullshit and it fucks society over’.\nNegativity against animals Statements which attack, criticise or express negativity against animals, e.g. ‘dogs are just beasts, kick them if they annoy you’.\nNegativity against institutions Statements which attack, criticise or express negativity against institutions; such as large organisations, governments and bodies, e.g. ‘the NHS is a badly run and pointless organisation which is the source of so much harm’.\nNegativity against others Statements which attack, criticise or express negativity against something that is NOT an identity – and the targets are not identified elsewhere in this typology, e.g. ‘the air round here is toxic, it smells like terrible’."
    }, {
      "heading" : "C Development set performance",
      "text" : "Table 6 shows dev set performance numbers."
    }, {
      "heading" : "D Model, Training, and Evaluation Details",
      "text" : "The model architecture was the roberta-base model from Huggingface (https://huggingface.co/), with a sequence classification head. This model has approximately 125 million parameters. Training each model took no longer than approximately a\nday, on average, with 8 GPUs on the FAIR cluster. All models were trained with a learning rate of 2e-5 with the default optimizer that Huggingface’s sequence classification routine uses. Target model hyperparameter search was as follows: the R2 target was trained for 3 epochs on the R1 target training data, plus multiples of the round 1 data from {1, 5, 10, 20, 40, 100} (the best was 5). The R3 target was trained for 3 epochs on the R2 target training data, plus multiples of the round 2 data from {1, 5, 10, 20, 40, 100} (the best was 100). The R4 target was trained on the R3 target training data for 4 epochs, plus multiples of the round 3 data from {1, 5, 10, 20, 40, 100, 200} (the best was 1); early stopping based on loss on the dev set (measured multiple times per epoch) was performed. The dev set we used for tuning target models was the latest dev set we had at each round. We did not perform hyperparameter search on the non-target models, with the exception of training 5 seeds of each and early stopping based on dev set loss throughout 4 training epochs. We recall that model performance typically did not vary by much more than 5% through our hyperparameter searches."
    }, {
      "heading" : "E Data statement",
      "text" : "Following Bender and Friedman (2018) we provide a data statement, which documents the process and provenance of the final dataset.\nA. CURATION RATIONALE In order to study the potential of dynamically generated datasets for improving online hate detection, we used an online interface to generate a large-scale synthetic dataset of 40,000 entries, collected over 4 rounds, with a ‘model-in-the-loop’ design. Data was not sampled. Instead a team of trained annotators created synthetic content to enter into the interface.\nB. LANGUAGE VARIETY All of the content is in English. We opted for English language due to the available annotation team, and resources and the project leaders’ expertise. The system that we developed could, in principle, be applied to other languages.\nC. SPEAKER DEMOGRAPHICS Due to the synthetic nature of the dataset, the speakers are the same as the annotators.\nD. ANNOTATOR DEMOGRAPHICS Annotator demographics are reported in the paper, and\nare reproduced here for fullness. Annotation guidelines were created at the start of the project and then updated after each round. Annotations guidelines will be publicly released with the dataset. We followed the guidance for protecting and monitoring annotator well-being provided by Vidgen et al. (2019a). 20 annotators were recruited. Ten were recruited to work for 12 weeks and ten were recruited for the final four weeks. Annotators completed different amounts of work depending on their availability, which is recorded in the dataset.\nAll annotators attended a project onboarding session, half day training session, at least one one-toone session and a daily ’standup’ meeting when working. They were given a test assignment and guidelines to review before starting work and received feedback after each round. Annotators could ask the experts questions in real-time over a messaging platform.\nOf the 20 annotators, 35% were male and 65% were female. 65% were 18-29 and 35% were 30- 39. 10% were educated to high school level, 20% to undergraduate, 45% to taught masters and 25% to research degree (i.e. PhD). 70% were native English speakers and 30% were non-native but fluent. Respondents had a range of nationalities, including British (60%), as well as Polish, Spanish and Iraqi. Most annotators identified as ethnically white (70%), followed by Middle Eastern (20%) and two or more ethnicities (10%). Participants all used social media regularly, including 75% who used it more than once per day. All participants had seen other people targeted by online abuse before, and 55% had been targeted personally.\nE. SPEECH SITUATION All data was created from 21st September 2020 until 14th January 2021. During the project annotators visited a range of online platforms, with adequate care and supervision from the project leaders, to better understand online hate as it appears online.\nF. TEXT CHARACTERISTICS The composition of the dataset, including the distribution of the Primary label (Hate and Not) and the type (Derogation, Animosity, Threatening, Support, Dehumanization and None Given) is described in the paper."
    } ],
    "references" : [ {
      "title" : "Automatic identification and classification of misogynistic language on Twitter",
      "author" : [ "Maria Anzovino", "Elisabetta Fersini", "Paolo Rosso." ],
      "venue" : "NLDB, pages 57–64.",
      "citeRegEx" : "Anzovino et al\\.,? 2018",
      "shortCiteRegEx" : "Anzovino et al\\.",
      "year" : 2018
    }, {
      "title" : "2019. Sem Eval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women",
      "author" : [ "Valerio Basile", "Cristina Bosco", "Elisabetta Fersini", "Debora Nozza", "Viviana Patti", "Francisco Manuel Rangel Pardo", "Paolo Rosso", "Manuela Sanguinetti" ],
      "venue" : null,
      "citeRegEx" : "Basile et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2019
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "I Feel Offended, Don’t Be Abusive! Implicit/Explicit Messages in Offensive and Abusive Language",
      "author" : [ "Tommaso Caselli", "Valerio Basile", "Jelena Mitrović", "Inga Kartoziya", "Michael Granitzer." ],
      "venue" : "Proceedings of the 12th Conference on Language Re-",
      "citeRegEx" : "Caselli et al\\.,? 2020",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2020
    }, {
      "title" : "CONAN COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech",
      "author" : [ "Yi-Ling Chung", "Elizaveta Kuzmenko", "Serra Sinem Tekiroglu", "Marco Guerini." ],
      "venue" : "Proceedings of the 57th Annual Meeting",
      "citeRegEx" : "Chung et al\\.,? 2019",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated Hate Speech Detection and the Problem of Offensive Language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of the 11th ICWSM, pages 1–4.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and Mitigating Unintended Bias in Text Classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73.",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating NLP Models’ Local Decision Boundaries via Contrast Sets",
      "author" : [ "Singh", "Noah A. Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou." ],
      "venue" : "Findings of the Assocation for Computational Linguistics: EMNLP",
      "citeRegEx" : "Singh et al\\.,? 2020",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual fairness in text classification through robustness",
      "author" : [ "Sahaj Garg", "Ankur Taly", "Vincent Perot", "Ed H. Chi", "Nicole Limtiaco", "Alex Beutel." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 219–226.",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "A Large Labeled Corpus for Online Harassment Research",
      "author" : [ "Ryan Lau", "Vichita Jienjitlert." ],
      "venue" : "Proceedings of the ACM Conference on Web Science, pages 229–233.",
      "citeRegEx" : "Lau and Jienjitlert.,? 2017",
      "shortCiteRegEx" : "Lau and Jienjitlert.",
      "year" : 2017
    }, {
      "title" : "All You Need is ”Love”: Evading Hate-speech Detection",
      "author" : [ "Tommi Gröndahl", "Luca Pajola", "Mika Juuti", "Mauro Conti", "N. Asokan." ],
      "venue" : "Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security, pages 2–12.",
      "citeRegEx" : "Gröndahl et al\\.,? 2018",
      "shortCiteRegEx" : "Gröndahl et al\\.",
      "year" : 2018
    }, {
      "title" : "Computing Inter-Rater Reliability for Observational Data: An Overview and Tutorial",
      "author" : [ "Kevin A Hallgren." ],
      "venue" : "Tutorials in quantitative methods for psychology, 8(1):23–34.",
      "citeRegEx" : "Hallgren.,? 2012",
      "shortCiteRegEx" : "Hallgren.",
      "year" : 2012
    }, {
      "title" : "Detecting threats of violence in online discussions using bigrams of important words",
      "author" : [ "Hugo Lewi Hammer." ],
      "venue" : "Proceedings - 2014 IEEE Joint Intelligence and Security Informatics Conference, JISIC 2014, page 319.",
      "citeRegEx" : "Hammer.,? 2014",
      "shortCiteRegEx" : "Hammer.",
      "year" : 2014
    }, {
      "title" : "Recent research on dehumanization",
      "author" : [ "N. Haslam", "M. Stratemeyer." ],
      "venue" : "Current Opinion in Psychology, 11:25–29.",
      "citeRegEx" : "Haslam and Stratemeyer.,? 2016",
      "shortCiteRegEx" : "Haslam and Stratemeyer.",
      "year" : 2016
    }, {
      "title" : "Deceiving Google’s Perspective API Built for Detecting Toxic Comments",
      "author" : [ "Hossein Hosseini", "Sreeram Kannan", "Baosen Zhang", "Radha Poovendran." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Hosseini et al\\.,? 2017",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-Domain Detection of Abusive Language Online",
      "author" : [ "Mladen Karan", "Jan Šnajder." ],
      "venue" : "2nd Workshop on Abusive Language Online, pages 132– 137.",
      "citeRegEx" : "Karan and Šnajder.,? 2018",
      "shortCiteRegEx" : "Karan and Šnajder.",
      "year" : 2018
    }, {
      "title" : "Learning the difference that makes a difference with counterfactually-augmented data",
      "author" : [ "Divyansh Kaushik", "Eduard Hovy", "Zachary C Lipton." ],
      "venue" : "arXiv preprint arXiv:1909.12434.",
      "citeRegEx" : "Kaushik et al\\.,? 2019",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextualizing Hate Speech Classifiers with Post-hoc Explanation",
      "author" : [ "Brendan Kennedy", "Xisen Jin", "Aida Mostafazadeh Davani", "Morteza Dehghani", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Kennedy et al\\.,? 2020",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2020
    }, {
      "title" : "Benchmarking Aggression Identification in Social Media",
      "author" : [ "Ritesh Kumar", "Atul Kr Ojha", "Shervin Malmasi", "Marcos Zampieri." ],
      "venue" : "Proceedings ofthe First Workshop on Trolling, Aggression and Cyberbullying,, 1, pages 1–11.",
      "citeRegEx" : "Kumar et al\\.,? 2018",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage",
      "author" : [ "Jana Kurrek", "Haji Mohammad Saleem", "Derek Ruths." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 138–149.",
      "citeRegEx" : "Kurrek et al\\.,? 2020",
      "shortCiteRegEx" : "Kurrek et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the HASOC track at FIRE 2019: Hate speech and offensive content identification in Indo-European languages",
      "author" : [ "Thomas Mandl", "Sandip Modha", "Prasenjit Majumder", "Daksh Patel", "Mohana Dave", "Chintak Mandlia", "Aditya Patel." ],
      "venue" : "FIRE",
      "citeRegEx" : "Mandl et al\\.,? 2019",
      "shortCiteRegEx" : "Mandl et al\\.",
      "year" : 2019
    }, {
      "title" : "Hatexplain: A benchmark dataset for explainable hate speech detection",
      "author" : [ "Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee" ],
      "venue" : null,
      "citeRegEx" : "Mathew et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mathew et al\\.",
      "year" : 2020
    }, {
      "title" : "A Framework for the Computational Linguistic Analysis of Dehumanization",
      "author" : [ "Julia Mendelsohn", "Yulia Tsvetkov", "Dan Jurafsky." ],
      "venue" : "Frontiers in Artificial Intelligence, 3(August):1–24.",
      "citeRegEx" : "Mendelsohn et al\\.,? 2020",
      "shortCiteRegEx" : "Mendelsohn et al\\.",
      "year" : 2020
    }, {
      "title" : "Tackling online abuse: A survey of automated abuse detection methods",
      "author" : [ "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "arXiv:1908.06024v2, pages 1–17.",
      "citeRegEx" : "Mishra et al\\.,? 2019",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2019
    }, {
      "title" : "A method for taxonomy development and its application in information systems",
      "author" : [ "Robert C Nickerson", "Upkar Varshney", "Jan Muntermann." ],
      "venue" : "European Journal of Information Systems, 22(3):336– 359.",
      "citeRegEx" : "Nickerson et al\\.,? 2013",
      "shortCiteRegEx" : "Nickerson et al\\.",
      "year" : 2013
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Abusive Language Detection in Online User Content",
      "author" : [ "Chikashi Nobata", "Achint Thomas", "Yashar Mehdad", "Yi Chang", "Joel Tetreault." ],
      "venue" : "World Wide Web Conference, pages 145–153.",
      "citeRegEx" : "Nobata et al\\.,? 2016",
      "shortCiteRegEx" : "Nobata et al\\.",
      "year" : 2016
    }, {
      "title" : "COLD: Annotation scheme and evaluation data set for complex offensive language in English",
      "author" : [ "Alexis Palmer", "Christine Carr", "Melissa Robinson", "Jordan Sanders." ],
      "venue" : "The Journal for Language Technology and Computational Linguistics, 34(1):1–28.",
      "citeRegEx" : "Palmer et al\\.,? 2020",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2020
    }, {
      "title" : "Misogyny Detection in Twitter: a Multilingual and Cross-Domain Study",
      "author" : [ "Endang Pamungkas", "Valerio Basile", "Viviana Patti." ],
      "venue" : "Information Processing and Management, 57(6).",
      "citeRegEx" : "Pamungkas et al\\.,? 2020",
      "shortCiteRegEx" : "Pamungkas et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting Offensive Language in Tweets Using Deep Learning",
      "author" : [ "Georgios K. Pitsilis", "Heri Ramampiaro", "Helge Langseth." ],
      "venue" : "arXiv:1801.04433v1, pages 1–17.",
      "citeRegEx" : "Pitsilis et al\\.,? 2018",
      "shortCiteRegEx" : "Pitsilis et al\\.",
      "year" : 2018
    }, {
      "title" : "Resources and benchmark corpora for hate speech detection: a systematic review",
      "author" : [ "Fabio Poletto", "Valerio Basile", "Manuela Sanguinetti", "Cristina Bosco", "Viviana Patti." ],
      "venue" : "Language Resources and Evaluation, pages 1–47.",
      "citeRegEx" : "Poletto et al\\.,? 2020",
      "shortCiteRegEx" : "Poletto et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Hatecheck: Functional tests for hate speech detection models",
      "author" : [ "Paul Röttger", "Bertram Vidgen", "Dong Nguyen", "Zeerak Waseem", "Helen Margetts", "Janet Pierrehumbert" ],
      "venue" : null,
      "citeRegEx" : "Röttger et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Röttger et al\\.",
      "year" : 2020
    }, {
      "title" : "Developing an online hate classifier for multiple social media platforms",
      "author" : [ "Joni Salminen", "Maximilian Hopf", "Shammur A. Chowdhury", "Soon gyo Jung", "Hind Almerekhi", "Bernard J. Jansen." ],
      "venue" : "Human-centric Computing and Information",
      "citeRegEx" : "Salminen et al\\.,? 2020",
      "shortCiteRegEx" : "Salminen et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsex me here: Revisiting sexism detection using psychological scales and adversarial samples",
      "author" : [ "Mattia Samory", "Indira Sen", "Julian Kohne", "Fabian Flock", "Claudia Wagner." ],
      "venue" : "arXiv preprint:2004.12764v1, pages 1–11.",
      "citeRegEx" : "Samory et al\\.,? 2020",
      "shortCiteRegEx" : "Samory et al\\.",
      "year" : 2020
    }, {
      "title" : "The Risk of Racial Bias in Hate Speech Detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A Smith", "Paul G Allen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the ACL, pages 1668–1678.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "A Survey on Hate Speech Detection using Natural Language Processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10. Association for Com-",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Studying generalisability across abusive language detection datasets",
      "author" : [ "Steve Durairaj Swamy", "Anupam Jamatia", "Björn Gambäck." ],
      "venue" : "CoNLL 2019 - 23rd Conference on Computational Natural Language Learning, Proceedings of the Conference,",
      "citeRegEx" : "Swamy et al\\.,? 2019",
      "shortCiteRegEx" : "Swamy et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting East Asian Prejudice on Social Media",
      "author" : [ "Bertie Vidgen", "Austin Botelho", "David Broniatowski", "Ella Guest", "Matthew Hall", "Helen Margetts", "Rebekah Tromble", "Zeerak Waseem", "Scott Hale." ],
      "venue" : "arXiv:2005.03909v1, pages 1–12.",
      "citeRegEx" : "Vidgen et al\\.,? 2020",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2020
    }, {
      "title" : "Directions in Abusive Language Training Data: Garbage In, Garbage Out",
      "author" : [ "Bertie Vidgen", "Leon Derczynski." ],
      "venue" : "Plos ONE, pages 1–26.",
      "citeRegEx" : "Vidgen and Derczynski.,? 2020",
      "shortCiteRegEx" : "Vidgen and Derczynski.",
      "year" : 2020
    }, {
      "title" : "Challenges and frontiers in abusive content detection",
      "author" : [ "Bertie Vidgen", "Alex Harris", "Dong Nguyen", "Rebekah Tromble", "Scott Hale", "Helen Margetts." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online (ACL), pages 80–93.",
      "citeRegEx" : "Vidgen et al\\.,? 2019a",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2019
    }, {
      "title" : "How much online abuse is there? A systematic review of evidence for the UK",
      "author" : [ "Bertie Vidgen", "Helen Margetts", "Alex Harris." ],
      "venue" : "The Alan Turing Institute, London.",
      "citeRegEx" : "Vidgen et al\\.,? 2019b",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2019
    }, {
      "title" : "Introducing CAD: the contextual abuse dataset",
      "author" : [ "Bertie Vidgen", "Dong Nguyen", "Helen Margetts", "Patricia Rossini", "Rebekah Tromble." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Vidgen et al\\.,? 2021",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2021
    }, {
      "title" : "Detecting weak and strong Islamophobic hate speech on social media",
      "author" : [ "Bertie Vidgen", "Taha Yasseri." ],
      "venue" : "Journal of Information Technology & Politics, 17(1):66–78.",
      "citeRegEx" : "Vidgen and Yasseri.,? 2019",
      "shortCiteRegEx" : "Vidgen and Yasseri.",
      "year" : 2019
    }, {
      "title" : "Detecting hate speech on the world wide web",
      "author" : [ "William Warner", "Julia Hirschberg." ],
      "venue" : "Proceedings of the Second Workshop on Language in Social Media, pages 19–26.",
      "citeRegEx" : "Warner and Hirschberg.,? 2012",
      "shortCiteRegEx" : "Warner and Hirschberg.",
      "year" : 2012
    }, {
      "title" : "Are you a racist or am I seeing things? annotator influence on hate speech detection on twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138– 142, Austin, Texas. Association for Computational",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "NAACL-HLT, pages 88–93.",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Bridging the gaps: Multi task learning for domain transfer of hate speech detection",
      "author" : [ "Zeerak Waseem", "James Thorne", "Joachim Bingel." ],
      "venue" : "Jennifer Golbeck, editor, Online Harassment, pages 29–55. Springer International Publishing, Cham.",
      "citeRegEx" : "Waseem et al\\.,? 2018",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2018
    }, {
      "title" : "Detection of Abusive Language: the Problem of Biased Datasets",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Thomas Kleinbauer." ],
      "venue" : "NAACL-HLT, pages 602–608, Minneapolis. ACL.",
      "citeRegEx" : "Wiegand et al\\.,? 2019",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2019
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Vectors for counterspeech on Twitter",
      "author" : [ "Lucas Wright", "Derek Ruths", "Kelly P Dillon", "Haji Mohammad Saleem", "Susan Benesch." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 57–62, Vancouver, BC, Canada. Association",
      "citeRegEx" : "Wright et al\\.,? 2017",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2017
    }, {
      "title" : "Ex Machina: Personal Attacks Seen at Scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the International World Wide Web Conference, pages 1391–1399.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017a",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Ex Machina: Personal Attacks Seen at Scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the International World Wide Web Conference, pages 1391–1399.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017b",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of NAACL HLT 2019, volume 1, pages 1415–1420.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offen",
      "author" : [ "Marcos Zampieri", "Preslav Nakov", "Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Hamdy Mubarak", "Leon Derczynski", "Zeses Pitenis", "Çağrı Çöltekin" ],
      "venue" : null,
      "citeRegEx" : "Zampieri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 51,
      "context" : "However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020).",
      "startOffset" : 189,
      "endOffset" : 297
    }, {
      "referenceID" : 43,
      "context" : "However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020).",
      "startOffset" : 189,
      "endOffset" : 297
    }, {
      "referenceID" : 3,
      "context" : "However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020).",
      "startOffset" : 189,
      "endOffset" : 297
    }, {
      "referenceID" : 26,
      "context" : "However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020).",
      "startOffset" : 189,
      "endOffset" : 297
    }, {
      "referenceID" : 33,
      "context" : "However, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalisability and fairness of even stateof-the-art models (Waseem et al., 2018; Vidgen et al., 2019a; Caselli et al., 2020; Mishra et al., 2019; Poletto et al., 2020).",
      "startOffset" : 189,
      "endOffset" : 297
    }, {
      "referenceID" : 35,
      "context" : "We verify improved model performance by evaluating them against the HATECHECK functional tests (Röttger et al., 2020), with accuracy improving from 60% in Round 1 to 95% in Round 4.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 50,
      "context" : "Benchmark datasets Several benchmark datasets have been put forward for online hate classification (Waseem and Hovy, 2016; Waseem, 2016; Davidson et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019, 2020; Vidgen et al., 2020, 2021).",
      "startOffset" : 99,
      "endOffset" : 256
    }, {
      "referenceID" : 48,
      "context" : "Benchmark datasets Several benchmark datasets have been put forward for online hate classification (Waseem and Hovy, 2016; Waseem, 2016; Davidson et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019, 2020; Vidgen et al., 2020, 2021).",
      "startOffset" : 99,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "Benchmark datasets Several benchmark datasets have been put forward for online hate classification (Waseem and Hovy, 2016; Waseem, 2016; Davidson et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019, 2020; Vidgen et al., 2020, 2021).",
      "startOffset" : 99,
      "endOffset" : 256
    }, {
      "referenceID" : 23,
      "context" : "Benchmark datasets Several benchmark datasets have been put forward for online hate classification (Waseem and Hovy, 2016; Waseem, 2016; Davidson et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019, 2020; Vidgen et al., 2020, 2021).",
      "startOffset" : 99,
      "endOffset" : 256
    }, {
      "referenceID" : 43,
      "context" : "hate speech training datasets, such as lacking linguistic variety, being inexpertly annotated and degrading over time (Vidgen et al., 2019a; Poletto et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 162
    }, {
      "referenceID" : 33,
      "context" : "hate speech training datasets, such as lacking linguistic variety, being inexpertly annotated and degrading over time (Vidgen et al., 2019a; Poletto et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 162
    }, {
      "referenceID" : 42,
      "context" : "and found that 27 (43%) were sourced from Twitter (Vidgen and Derczynski, 2020).",
      "startOffset" : 50,
      "endOffset" : 79
    }, {
      "referenceID" : 44,
      "context" : "In addition, many datasets are formed with bootstrapped sampling, such as keyword searches, due to the low prevalence of hate speech ‘in the wild’ (Vidgen et al., 2019b).",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 52,
      "context" : "Such bootstrapping can substantially bias the nature and coverage of datasets (Wiegand et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "Models trained on historical data may also not be effective for present-day hate classification models given how quickly online conversations evolve (Nobata et al., 2016).",
      "startOffset" : 149,
      "endOffset" : 170
    }, {
      "referenceID" : 39,
      "context" : "Model limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 298
    }, {
      "referenceID" : 26,
      "context" : "Model limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 298
    }, {
      "referenceID" : 42,
      "context" : "Model limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 298
    }, {
      "referenceID" : 35,
      "context" : "Model limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 298
    }, {
      "referenceID" : 24,
      "context" : "Model limitations Systems trained on existing datasets have been shown to lack accuracy, robustness and generalisability, creating a range of false positives and false negatives (Schmidt and Wiegand, 2017; Mishra et al., 2019; Vidgen and Derczynski, 2020; Röttger et al., 2020; Mathew et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 298
    }, {
      "referenceID" : 7,
      "context" : "‘gay’) are substantially more likely to appear in toxic content in training datasets, leading models to overfit on them (Dixon et al., 2018; Kennedy et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : "‘gay’) are substantially more likely to appear in toxic content in training datasets, leading models to overfit on them (Dixon et al., 2018; Kennedy et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 162
    }, {
      "referenceID" : 51,
      "context" : "Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irrespective of how they are used (Waseem et al., 2018; Davidson et al., 2017; Kurrek et al., 2020; Palmer et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irrespective of how they are used (Waseem et al., 2018; Davidson et al., 2017; Kurrek et al., 2020; Palmer et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 21,
      "context" : "Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irrespective of how they are used (Waseem et al., 2018; Davidson et al., 2017; Kurrek et al., 2020; Palmer et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 30,
      "context" : "Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irrespective of how they are used (Waseem et al., 2018; Davidson et al., 2017; Kurrek et al., 2020; Palmer et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 218
    }, {
      "referenceID" : 51,
      "context" : ", 2019) or have been reclaimed by the targeted group (Waseem et al., 2018; Sap et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : ", 2019) or have been reclaimed by the targeted group (Waseem et al., 2018; Sap et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "cases, false negatives can be provoked by changing the ‘sensitive’ attribute of hateful content, such as changing the target from ‘gay’ to ‘black’ people (Garg et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 36,
      "context" : "This can happen when models are trained on data which only contains hate directed against a limited set of targets (Salminen et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 46,
      "context" : "Subtle and implicit forms of hate speech can also create false negatives (Vidgen and Yasseri, 2019; Palmer et al., 2020; Mathew et al., 2020), as well as more ‘complex’ forms of speech such as sarcasm, irony, adjective nominalization and rhetorical questions (Caselli et al.",
      "startOffset" : 73,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "Subtle and implicit forms of hate speech can also create false negatives (Vidgen and Yasseri, 2019; Palmer et al., 2020; Mathew et al., 2020), as well as more ‘complex’ forms of speech such as sarcasm, irony, adjective nominalization and rhetorical questions (Caselli et al.",
      "startOffset" : 73,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Subtle and implicit forms of hate speech can also create false negatives (Vidgen and Yasseri, 2019; Palmer et al., 2020; Mathew et al., 2020), as well as more ‘complex’ forms of speech such as sarcasm, irony, adjective nominalization and rhetorical questions (Caselli et al.",
      "startOffset" : 73,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "‘adversarial’ samples which are challenging for baseline models, repeating the process over multiple rounds (Nie et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "Another way of addressing the limitations of static datasets is through creating ‘contrast sets’ of perturbations (Kaushik et al., 2019; Gardner et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "Dehumanization Content which ‘perceiv[es] or treat[s] people as less than human’ (Haslam and Stratemeyer, 2016).",
      "startOffset" : 81,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : "It often involves describing groups as leeches, cockroaches, insects, germs or rats (Mendelsohn et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "This makes the test sets more challenging and minimizes the risk of annotator bias given our relatively small pool of annotators (Geva et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "For consistency, we use the same model architecture everywhere, specifically RoBERTa (Liu et al., 2019) with a sequence classification head.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 53,
      "context" : "We use the implementation from the Transformers (Wolf et al., 2019) library.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "815 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating extremely high levels of agreement (Hallgren, 2012).",
      "startOffset" : 128,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "55 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating moderate levels of agreement (Hallgren, 2012).",
      "startOffset" : 121,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "52 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating moderate levels of agreement (Hallgren, 2012).",
      "startOffset" : 121,
      "endOffset" : 137
    } ],
    "year" : 2021,
    "abstractText" : "We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of ∼40, 000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes∼15, 000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HATECHECK, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.",
    "creator" : "LaTeX with hyperref"
  }
}