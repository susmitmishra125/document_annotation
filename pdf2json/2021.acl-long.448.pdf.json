{
  "name" : "2021.acl-long.448.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Coreference Reasoning in Machine Reading Comprehension",
    "authors" : [ "Mingzhu Wu", "Nafise Sadat Moosavi", "Dan Roth", "Iryna Gurevych" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5768–5781\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5768"
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine reading comprehension is the ability to read and understand the given passages and answer questions about them. Coreference resolution is the task of finding different expressions that refer to the same real-world entity. The tasks of coreference resolution and machine reading comprehension have moved closer to each other. Converting coreference-related datasets into an MRC format\n1The code and the resulting dataset are available at https://github.com/UKPLab/ coref-reasoning-in-qa.\nimproves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019). There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al., 2018), etc.\nQuoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models. Figure 1 shows a QA sample from Quoref in which the model needs to resolve the coreference relation between “his” and “John Motteux” to answer the question.\nRecent large pre-trained language models reached high performance on Quoref. However, our results and analyses suggest that this dataset contains artifacts and does not reflect the natural distribution and, therefore, the challenges of coreference reasoning. As a result, high performances on Quoref do not necessarily reflect the coreference reasoning capabilities of the examined models and answering questions that require coreference reasoning might be a greater challenge than current scores suggest.\nIn this paper, we propose two solutions to address this issue. First, we propose a methodology for creating MRC datasets that better reflect the coreference reasoning challenge. We release a sample challenging evaluation set containing 200 examples by asking an annotator to create new question-\nanswer pairs using our methodology and based on existing passages in Quoref. We show that this dataset contains fewer annotation artifacts, and its distribution of biases is closer to a coreference resolution dataset. The performance of state-of-the-art models on Quoref considerably drops on our evaluation set suggesting that (1) coreference reasoning is still an open problem for MRC models, and (2) our methodology opens a promising direction to create future challenging MRC datasets.\nSecond, we propose to directly use coreference resolution datasets for training MRC models to improve their coreference reasoning. We automatically create a question whose answer is a coreferring expression m1 using the BART model (Lewis et al., 2020). We then consider this question, m1’s antecedent, and the corresponding document as a new (question, answer, context) tuple. This data helps the model learning to resolve the coreference relation between m1 and its antecedent to answer the question. We show that incorporating these additional data improves the performance of the state-of-the-art models on our new evaluation set.\nOur main contributions are as follows:\n• We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge.\n• We release a sample challenging dataset that is manually created by an annotator using our methodology. The results of state-of-the-art MRC models on our evaluation set show that, despite the high performance of MRC models on Quoref, answering questions based on coreference reasoning is still an open challenge.\n• We propose an approach to use existing coreference resolution datasets for training MRC models. We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Artifacts in NLP datasets",
      "text" : "One of the known drawbacks of many NLP datasets is that they contain artifacts.2 Models tend to ex-\n2I.e., the conditional distribution of the target label based on specific attributes of the training domain diverges while testing on other domains.\nploit these easy-to-learn patterns in the early stages of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models.\nThere are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020).\nIn the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b).\nWhile there is an emerging trend of including adversarial models in data collection, their effectiveness is not yet compared with using debiasing methods, e.g., whether they are still beneficial when we use debiasing methods or vice versa."
    }, {
      "heading" : "2.2 Joint QA and Coreference Reasoning",
      "text" : "There are a few studies on the joint understanding of coreference relations and reading comprehension. Wu et al. (2020b) propose to formulate coreference resolution as a span-prediction task by generating a query for each mention using the surrounding context, thus converting coreference resolution to a reading comprehension problem. They leverage the plethora of existing MRC datasets for data augmentation and improve the generalization of the coreference model. In parallel to Wu et al. (2020b), Aralikatte et al. (2019) also cast ellipsis and coreference resolution as reading comprehension tasks. They leverage the existing neural archi-\ntectures designed for MRC for ellipsis resolution and outperform the previous best results. In a similar direction, Hou (2020) propose to cast bridging anaphora resolution as question answering and present a question answering framework for this task. However, none of the above works investigate the impact of using coreference data on QA.\nDua et al. (2020) use Amazon Mechanical Turkers to annotate the corresponding coreference chains of the answers in the passages of Quoref for 2,000 QA pairs. They then use this additional coreference annotation for training a model on Quoref. They show that including these additional coreference annotations improves the overall performance on Quoref. The proposed method by Dua et al. (2020) requires annotating additional coreference relations on every new coreference-aware QA dataset. Contrary to this, our approach uses existing coreference resolution datasets, and therefore, applies to any new QA dataset without introducing any additional cost."
    }, {
      "heading" : "3 How Well Quoref Presents Coreference Reasoning?",
      "text" : "For creating the Quoref dataset, annotators first identify coreferring expressions and then ask questions that connect the two coreferring expressions. Dasigi et al. (2019) use a BERT-base model (Devlin et al., 2019) that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) as an adversarial model to exclude QA samples that the adversarial model can already answer. The goal of using this adversarial model is to avoid including questionanswer pairs that can be solved using surface cues. They claim that most examples in Quoref cannot be answered without coreference reasoning.\nIf we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples.\nIn this section, we investigate whether Quoref contains the known artifacts of QA datasets, and therefore, models can solve some of the QA pairs without performing coreference reasoning. Figure 2 shows such an example where simple lexical cues are enough to answer the question despite the fact that coreference expressions “Frankie” and “his” were included in the corresponding context.\nmost frequent named entity in the context as the answer.\nFor wh-word, empty question, and short distance reasoning, we use the TASE model (Segal et al., 2020) to learn the bias. Biased examples are then those that can be correctly solved by these models. We only change the training data for biased example detection, if necessary, and the development set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set.\nWe also investigate whether these biases have similar ratios in a coreference resolution dataset. We use the CoNLL-2012 coreference resolution dataset (Pradhan et al., 2012a) and convert it to a reading comprehension format, i.e., CoNLLbart in Section 5.5 This data contains question-answer pairs in which the question is created based on a coreferring expression in CoNLL-2012, and the answer is its closest antecedent. We split this data into training and test sets and train bias models on the training split. The CoNLLbart column in Table 1 shows the bias proportions on this data.\nAs we see, the short distance reasoning is the most prominent bias in the Quoref dataset. However, the ratio of such biased examples is only around 10% in CoNLL-2012. Therefore, apart from the examples that can be solved without coreference reasoning,6 the difficulty of the required coreference reasoning in the remaining examples is also not comparable with naturally occurring coreference relations in a coreference resolution dataset.\nAs a result, high performance on Quoref does not necessarily indicate that the model is adept at performing coreference reasoning.\n5We report the bias ratios of CoNLLdec in Section 5 in the appendix.\n6E.g., about 20% of examples can be answered without considering the question."
    }, {
      "heading" : "4 Creating an MRC Dataset that Better Reflects Coreference Reasoning",
      "text" : "There is a growing trend in using adversarial models for data creation to make the dataset more challenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).\nQuoref is also created using an adversarial data collection method to discard examples that can be solved using simple lexical cues. The assumption is that it is hard to avoid simple lexical cues by which the model can answer questions without coreference reasoning. Therefore, an adversarial model (A) is used to discard examples that contain such lexical cues. While this adversarial filtering removes examples that are easy to solve by A, it does not ensure that the remaining examples do not contain shortcuts that are not explored by A. First, the adversarial model in Quoref is trained on another dataset, i.e., SQuAD. Thus, the failure of A on Quoref examples may be due to (1) Quoref having different lexical cues than those in SQuAD, or (2) domain shift. Second, and more importantly, as argued by Dunietz et al. (2020), making the task challenging by focusing on examples that are more difficult for existing models is not a solution for more useful reading comprehension.7\nWe instead propose a methodology for creating question-answer pairs as follows:\n• Annotators should create a question that connects the referring expression m1 to its antecedent m2 so that (1) m2 is more informative than m1,8 and (2) m1 and m2 reside in a different sentence.\n• Candidate passages for creating QA pairs are selected according to their number of named entities and pronouns. The number of distinct named entities is an indicator of the number of entities in the text. Therefore, there would be more candidate entities for resolving referring expressions. The number of pronouns indicates that we have enough candidate m1s that have more informative antecedents.\nWe provide this guideline to a student from the 7As put by them: “the dominant MRC research paradigm is like trying to become a professional sprinter by glancing around the gym and adopting any exercises that look hard”.\n8Proper names are more informative than common nouns, and they are more informative than pronouns (Lee et al., 2013).\nComputer Science department for generating new QA pairs from the existing passages in the Quoref development set. We use Quoref passages to ensure that the source of performance differences on our dataset vs. Quoref is not due to domain differences. This results in 200 new QA pairs. Table 2 presents examples from our dataset.\nTable 3 shows the results of the examined biases on our dataset. By comparing Table 3 and Table 1, we observe that the examined biases are less strong in our dataset, and their distribution is closer to those in CoNLL-2012. As we will see in Table 5, the performance of state-of-the-art models on Quoref drops more than 10 points, i.e., 13-18 points, on our challenge dataset.9"
    }, {
      "heading" : "5 Improving Coreference Reasoning",
      "text" : "While we do not have access to many coreference annotations for the task of coreference-aware MRC, there are various datasets for the task of coreference resolution. Coreference resolution datasets contain the annotation of expressions that refer to the same entity. In this paper, we hypothesize that we can directly use coreference resolution corpora to improve the coreference reasoning of MRC models. We propose an effective approach to convert coreference annotations into QA pairs so that models learn to perform coreference resolution by answering those questions. In our experiments, we use the\n9We examine 50 randomly selected examples from our challenge set, and they were all answerable by a human.\nCoNLL-2012 dataset (Pradhan et al., 2012b) that is the largest annotated dataset with coreference information."
    }, {
      "heading" : "5.1 Coreference-to-QA Conversion",
      "text" : "The existing approach to convert coreference annotations into (question, context, answer) tuples, which is used to improve coreference resolution performance (Wu et al., 2020b; Aralikatte et al., 2019), is to use the sentence of the anaphor as a declarative query, and its closest antecedent as the answer. The format of these queries is not compatible with questions in MRC datasets, and therefore, the impact of this data on MRC models may be limited. In this work, we instead generate questions from those declarative queries using an automatic question generation model. We use the BART model (Lewis et al., 2020) that is one of the state-of-the-art text generation models. Below we explain the details of each of these two approaches for creating QA data from CoNLL-2012. Table 4 shows examples from both approaches.\nCoNLLdec: Wu et al. (2020b) and Aralikatte et al. (2019) choose a sentence that contains an anaphor as a declarative query, the closest nonpronominal antecedent of that anaphor as the answer, and the corresponding document of the expressions as the context.10 We remove the tuples in which the anaphor and its antecedent are identical. The reason is that (1) Quoref already contains many examples in which the coreference relation is between two mentions with the same string, and (2) even after removing such examples, CoNLLdec contains around four times more QA pairs than the Quoref training data.\nCoNLLbart: we use a fine-tuned BART model (Lewis et al., 2020) released by Durmus et al.\n10We use the code provided by Aralikatte et al. (2019).\n(2020) for question generation and apply it on the declarative queries in CoNLLdec. The BART model specifies potential answers by masking noun phrases or named entities in the query and then generates questions for each masked text span. We only keep questions whose answer, i.e., the masked expression, is a coreferring expression and replace that answer with its closest non-pronominal antecedent. We only keep questions in which the masked expression and its antecedent are not identical. Such QA pairs enforce the model to resolve the coreference relation between the two coreferring expressions to answer generated questions."
    }, {
      "heading" : "5.2 Experimental Setups",
      "text" : "We use two recent models from the Quoref leaderboard: RoBERTa (Liu et al., 2019) and TASE (Segal et al., 2020), from which TASE has the state-ofthe-art results. We use RoBERTa-large from HuggingFace (Wolf et al., 2020). TASE casts MRC as a sequence tagging problem to handle questions with multi-span answers. It assigns a tag to every token of the context indicating whether the token is a part of the answer. We use the TASEIO+SSE setup that is a combination of their multi-span architecture and single-span extraction with IO tagging.We use the same configuration and hyper-parameters for TASEIO+SSE as described in Segal et al. (2020). We train all models for two epochs in all experiments.11 We use the F1 score that calculates the number of shared words between predictions and gold answers for evaluation.\nTraining Strategies. To include the additional training data that we create from CoNLL-2012 using coreference-to-CoNLL conversion methods, we use two different strategies:\n• Joint: we concatenate the training examples from Quoref and CoNLL-to-QA converted\n11The only difference of TASE in our experiments and the reported results in Segal et al. (2020) is the number of training epochs. For a fair comparison, we train all models for the same number of iterations.\ndatasets. Therefore, the model is jointly trained on the examples from both datasets.\n• Transfer: Since the CoNLL-to-QA data is automatically created and is noisy, we also examine a sequential fine-tuning setting in which we first train the model on the CoNLL-to-QA converted data, and then fine-tune it on Quoref."
    }, {
      "heading" : "5.3 Data",
      "text" : "We evaluate all the models on four different QA datasets.\n• Quoref : the official development and test sets of Quoref, i.e., Quorefdev and Quoreftest, respectively.\n• Our challenge set: our new evaluation set described in Section 4.\n• Contrast set: the evaluation set by Gardner et al. (2020) that is created based on the official Quoref test set. For creating this evaluation set, the authors manually performed small but meaningful perturbations to the test examples in a way that it changes the gold label. This dataset is constructed to evaluate whether models decision boundaries align to true decision boundaries when they are measured around the same point.\n• MultiRC: Multi-Sentence Reading Comprehension set (Khashabi et al., 2018) is created in a way that answering questions requires a more complex understanding from multiple sentences. Therefore, coreference reasoning can be one of the sources for improving the performance on this dataset. Note that MultiRC is from a different domain than the rest of evaluation sets.12\n12To use the MultiRC development set, which is in a multichoice answer selection format, we convert it to a reading comprehension format by removing QA pairs whose answers cannot be extracted from the context.\nThe Contrast set and MultiRC datasets are not designed to explicitly evaluate coreference reasoning. However, we include them among our evaluation sets to have a broader view about the impact of using our coreference data in QA.\nTable 6 reports the statistics of these QA datasets. In addition, it reports the number of examples in CoNLLdec and CoNLLbart datasets that we create by converting the CoNLL-2012 training data into QA examples. Since the question generation model cannot generate a standard question for every declarative sentence, CoNLLbart contains a smaller number of examples. We also include the statistics of SQuAD in Table 6 as we use it for investigating whether the resulting performance changes are due to using more training data or using coreference-aware additional data.\nThe language of all the datasets is English."
    }, {
      "heading" : "5.4 Results",
      "text" : "Table 5 presents the results of evaluating the impact of using coreference annotations to improve coreference reasoning in MRC. We report the re-\nsults for both of the examined state-of-the-art models, i.e., TASE and RoBERTa-large, using both training settings: (1) training the model jointly on Quoref and CoNLL-to-QA converted data (Joint), and (2) pre-training the model on CoNLL-to-QA data first and fine-tuning it on Quoref (Transfer). Baseline represents the results of the examined models that are only trained on Quoref. CoNLLbart represents the results of the models that are only trained on the CoNLLbart data. Transfer-SQuAD reports the results of the sequential training when the model is first trained on the SQuAD training dataset (Rajpurkar et al., 2016) and is then finetuned on Quoref.\nBased on the results of Table 5, we make the following observations.\nFirst, the most successful setting for improving coreference reasoning, i.e., improving the performance on our challenge evaluation set, is TransferCoNLLbart. Pre-training the TASE model on CoNLLbart improves its performance on all of the examined evaluation sets. However, it only improves the performance of RoBERTa on our challenge set.\nSecond, SQuAD contains well-formed QA pairs while CoNLLbart and CoNLLdec contain noisy QA. Also, SQuAD and Quoref are both created based on Wikipedia articles, and therefore, have similar domains. However, the genres of the documents in CoNLL-2012 include newswire, broadcast news, broadcast conversations, telephone conversations,\nweblogs, magazines, and Bible, which are very different from those in Quoref. As a result, pretraining on SQuAD has a positive impact on the majority of datasets. However, this impact is less pronounced on our challenge dataset, as it requires coreference reasoning while this skill is not present in SQuAD examples.\nFinally, while using the sentence of coreferring mentions as a declarative query (CONLLdec) is the common method for converting coreference resolution datasets into QA format in previous studies, our results show using CoNLLbart has a more positive impact compared to using CoNLLdec."
    }, {
      "heading" : "5.5 Analysis",
      "text" : "To analyze what kind of examples benefit more from incorporating the coreference data, we split Quorefdev and our dataset into different subsets based on the semantic overlap and short distance reasoning biases, which are the most common types of biases in both datasets.\nThe semantic overlap column in Table 7 represents the results on the subset of the data in which answers reside in the most similar sentence of the context, and the ¬semantic overlap column contains the rest of the examples in each of the examined datasets. The short reasoning column presents the results on the subset of the data containing examples that can be solved by the short distance reasoning bias model, and ¬ short reasoning presents the results on the rest of the examples.\nTable 7 shows the performance differences of the TASE and RoBERTa models on these four subsets for each of the two datasets.\nSurprisingly, the performance of the baseline models is lower on the semantic overlap subset compared to ¬semantic overlap on Quorefdev. This can indicate that examples in the ¬semantic overlap subset of Quorefdev contain other types of biases that make QA less challenging on this subset.\nThe addition of the coreference resolution annotations in all four training settings reduces the performance gap of the TASE model on the semantic overlap and ¬semantic overlap subsets for both datasets. Incorporating coreference data for RoBERTa, on the other hand, has a positive impact using the CoNLLbart data and on the harder subsets of our challenge evaluation set, i.e., ¬semantic overlap and ¬short reasoning.\nFinally, there is still a large performance gap between short reasoning and ¬ short reasoning subsets. In our coreference-to-QA conversion methods, we consider the closest antecedent of each anaphor as the answer. A promising direction for future work is to also create QA pairs based on longer distance coreferring expressions, e.g., to create two QA pairs based on each anaphor, one in which the answer is the closest antecedent, and the other with the first mention of the entity in the text as the answer."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We show that the high performance of recent models on the Quoref dataset does not necessarily indicate that they are adept at performing coreference reasoning, and that QA based on coreference reasoning is a greater challenge than current scores\nsuggest. We then propose a methodology for creating a dataset that better presents the coreference reasoning challenge for MRC. We provide our methodology to an annotator and create a sample dataset. Our analysis shows that our dataset contains fewer biases compared to Quoref, and the performance of state-of-the-art Quoref models drops considerably on this evaluation set.\nTo improve the coreference reasoning of QA models, we propose to use coreference resolution datasets to train MRC models. We propose a method to convert coreference annotations into an MRC format. We examine the impact of incorporating this coreference data on improving the coreference reasoning of QA models using two topperforming QA systems from the Quoref leaderboard. We show that using coreference datasets improves the performance of both examined models on our evaluation set, indicating their improved coreference reasoning. The results on our evaluation set suggest that there is still room for improvement, and reading comprehension with coreference understanding remains a challenge for existing QA models, especially if the coreference relation is between two distant expressions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been supported by the German Research Foundation (DFG) as part of the QASciInf project (grant GU 798/18-3), and the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. Dan Roth’s work is partly supported by contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). The authors would like to thank Michael Bugert, Max Glockner, Yevgeniy Puzikov, Nils Reimers, Andreas Rücklé, and the anonymous reviewers for their valuable feedback."
    }, {
      "heading" : "A Additional Statistics about Biased Examples",
      "text" : "Table 8 shows the proportion of biased examples in the CoNLLdec set. We can see that the results are similar to that of the CoNLLbart set.\nTo compare the ratio of biased examples between Quorefdev and our challenge set when considering the same number of examples in both datasets, we randomly sample 10 different subsets from Quorefdev and our challenge set with 100 samples in each subset and compute the rations in each subset. Figure 3 shows the results. As we see, in this setting the ratio of all bias types in our evaluation set is still lower than those in Quorefdev."
    }, {
      "heading" : "B Additional Experiments",
      "text" : "Table 9 shows additional experiments for pretraining the examined models on coreference data. We examine an additional setting for pre-training on both CoNLLdec and CoNLLbart by first training the models on CoNLLdec, then on CoNLLbart, and finally on Quoref (Transfer-CoNLLbart+dec). By comparing the results of Transfer-CoNLLbart+dec with Transfer-CoNLLbart from Table 5, we observe that pre-training the models on both CoNLLdec and CoNLLbart does not result in any additional advantage compared to only using CoNLLbart."
    }, {
      "heading" : "C Additional Examples",
      "text" : "Table 10 presents more examples from CoNLLdec and CoNLLbart."
    } ],
    "references" : [ {
      "title" : "A Simple Transfer Learning Baseline for Ellipsis Resolution",
      "author" : [ "Rahul Aralikatte", "Matthew Lamm", "Daniel Hardt", "Anders Søgaard." ],
      "venue" : "arXiv preprint arXiv:1908.11141.",
      "citeRegEx" : "Aralikatte et al\\.,? 2019",
      "shortCiteRegEx" : "Aralikatte et al\\.",
      "year" : 2019
    }, {
      "title" : "A closer look at memorization",
      "author" : [ "Devansh Arpit", "Stanisław Jastrzundefinedbski", "Nicolas Ballas", "David Krueger", "Emmanuel Bengio", "Maxinder S. Kanwal", "Tegan Maharaj", "Asja Fischer", "Aaron Courville", "Yoshua Bengio", "Simon LacosteJulien" ],
      "venue" : null,
      "citeRegEx" : "Arpit et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Arpit et al\\.",
      "year" : 2017
    }, {
      "title" : "Beat the AI: Investigating adversarial human annotation for reading comprehension",
      "author" : [ "Max Bartolo", "Alastair Roberts", "Johannes Welbl", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:662–678.",
      "citeRegEx" : "Bartolo et al\\.,? 2020",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2020
    }, {
      "title" : "CODAH: An adversarially-authored question answering dataset for common sense",
      "author" : [ "Michael Chen", "Mike D’Arcy", "Alisa Liu", "Jared Fernandez", "Doug Downey" ],
      "venue" : "In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations",
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases",
      "author" : [ "Christopher Clark", "Mark Yatskar", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Clark et al\\.,? 2019a",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019b",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "author" : [ "Pradeep Dasigi", "Nelson F. Liu", "Ana Marasović", "Noah A. Smith", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Benefits of intermediate annotations in reading comprehension",
      "author" : [ "Dheeru Dua", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5627–5634, Online. Association for Computa-",
      "citeRegEx" : "Dua et al\\.,? 2020",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2020
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "To test machine comprehension, start by defining comprehension",
      "author" : [ "Jesse Dunietz", "Greg Burnham", "Akash Bharadwaj", "Owen Rambow", "Jennifer Chu-Carroll", "Dave Ferrucci." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Dunietz et al\\.,? 2020",
      "shortCiteRegEx" : "Dunietz et al\\.",
      "year" : 2020
    }, {
      "title" : "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "author" : [ "Esin Durmus", "He He", "Mona Diab." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–",
      "citeRegEx" : "Durmus et al\\.,? 2020",
      "shortCiteRegEx" : "Durmus et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "F. Liu", "Phoebe Mulcaire", "Qiang Ning", "Sameer Singh", "Noah A. Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unlearn dataset bias in natural language inference by fitting the residual",
      "author" : [ "He He", "Sheng Zha", "Haohan Wang." ],
      "venue" : "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132–142, Hong Kong, China.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "An improved non-monotonic transition system for dependency parsing",
      "author" : [ "Matthew Honnibal", "Mark Johnson." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373–1378, Lisbon, Portugal. As-",
      "citeRegEx" : "Honnibal and Johnson.,? 2015",
      "shortCiteRegEx" : "Honnibal and Johnson.",
      "year" : 2015
    }, {
      "title" : "Bridging anaphora resolution as question answering",
      "author" : [ "Yufang Hou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1428–1438, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Hou.,? 2020",
      "shortCiteRegEx" : "Hou.",
      "year" : 2020
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
      "author" : [ "Daniel Khashabi", "Snigdha Chaturvedi", "Michael Roth", "Shyam Upadhyay", "Dan Roth." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Khashabi et al\\.,? 2018",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial filters of dataset biases",
      "author" : [ "Ronan Le Bras", "Swabha Swayamdipta", "Chandra Bhagavatula", "Rowan Zellers", "Matthew Peters", "Ashish Sabharwal", "Yejin Choi." ],
      "venue" : "International Conference on Machine Learning, pages 1078–1088. PMLR.",
      "citeRegEx" : "Bras et al\\.,? 2020",
      "shortCiteRegEx" : "Bras et al\\.",
      "year" : 2020
    }, {
      "title" : "Deterministic coreference resolution based on entity-centric, precision-ranked rules",
      "author" : [ "Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Computational Linguistics, 39(4):885–916.",
      "citeRegEx" : "Lee et al\\.,? 2013",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2013
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Early-learning regularization prevents memorization of noisy labels",
      "author" : [ "Sheng Liu", "Jonathan Niles-Weed", "Narges Razavian", "Carlos Fernandez-Granda." ],
      "venue" : "Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020).",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Y. Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang." ],
      "venue" : "Joint Conference on EMNLP and CoNLL - Shared Task, pages",
      "citeRegEx" : "Pradhan et al\\.,? 2012a",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2012
    }, {
      "title" : "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang." ],
      "venue" : "Joint Conference on EMNLP and CoNLL - Shared Task, pages",
      "citeRegEx" : "Pradhan et al\\.,? 2012b",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2012
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "DuoRC: Towards complex language understanding with paraphrased reading comprehension",
      "author" : [ "Amrita Saha", "Rahul Aralikatte", "Mitesh M. Khapra", "Karthik Sankaranarayanan." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Saha et al\\.,? 2018",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple and effective model for answering multi-span questions",
      "author" : [ "Elad Segal", "Avia Efrat", "Mor Shoham", "Amir Globerson", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Segal et al\\.,? 2020",
      "shortCiteRegEx" : "Segal et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing the benchmarking capacity of machine reading comprehension datasets",
      "author" : [ "Saku Sugawara", "Pontus Stenetorp", "Kentaro Inui", "Akiko Aizawa." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI 2020). Associ-",
      "citeRegEx" : "Sugawara et al\\.,? 2020",
      "shortCiteRegEx" : "Sugawara et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance",
      "author" : [ "Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Utama et al\\.,? 2020a",
      "shortCiteRegEx" : "Utama et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards debiasing NLU models from unknown biases",
      "author" : [ "Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7597–7610, On-",
      "citeRegEx" : "Utama et al\\.,? 2020b",
      "shortCiteRegEx" : "Utama et al\\.",
      "year" : 2020
    }, {
      "title" : "Making neural QA as simple as possible but not simpler",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 271–280, Vancouver, Canada.",
      "citeRegEx" : "Weissenborn et al\\.,? 2017",
      "shortCiteRegEx" : "Weissenborn et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving QA generalization by concurrent modeling of multiple biases",
      "author" : [ "Mingzhu Wu", "Nafise Sadat Moosavi", "Andreas Rücklé", "Iryna Gurevych." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 839–853, Online.",
      "citeRegEx" : "Wu et al\\.,? 2020a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "CorefQA: Coreference resolution as query-based span prediction",
      "author" : [ "Wei Wu", "Fei Wang", "Arianna Yuan", "Fei Wu", "Jiwei Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6953–6963, Online. As-",
      "citeRegEx" : "Wu et al\\.,? 2020b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Em-",
      "citeRegEx" : "Yang et al\\.,? 2018a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Mastering the dungeon: Grounded language learning by mechanical turker descent",
      "author" : [ "Zhilin Yang", "Saizheng Zhang", "Jack Urbanek", "Will Feng", "Alexander H Miller", "Arthur Szlam", "Douwe Kiela", "Jason Weston." ],
      "venue" : "Proceedings of 6th International",
      "citeRegEx" : "Yang et al\\.,? 2018b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : ", 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Quoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "We automatically create a question whose answer is a coreferring expression m1 using the BART model (Lewis et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task.",
      "startOffset" : 12,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task.",
      "startOffset" : 12,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task.",
      "startOffset" : 12,
      "endOffset" : 71
    }, {
      "referenceID" : 40,
      "context" : "In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua",
      "startOffset" : 117,
      "endOffset" : 203
    }, {
      "referenceID" : 38,
      "context" : "In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua",
      "startOffset" : 117,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua",
      "startOffset" : 117,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "weight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "weight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 32,
      "context" : ", 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 36,
      "context" : "across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b).",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b).",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "(2019) use a BERT-base model (Devlin et al., 2019) that is fine-tuned on the SQuAD dataset (Rajpurkar et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : ", 2019) that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) as an adversarial",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "• Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "• Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "• Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "reasoning, we use the TASE model (Segal et al., 2020) to learn the bias.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "dataset (Pradhan et al., 2012a) and convert it to a reading comprehension format, i.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 23,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 38,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 40,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 39,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 9,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "lenging or discard examples that can be solved using surface cues (Bartolo et al., 2020; Nie et al., 2020; Yang et al., 2018a; Zellers et al., 2018; Yang et al., 2018b; Dua et al., 2019; Chen et al., 2019; Dasigi et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 226
    }, {
      "referenceID" : 19,
      "context" : "(8)Proper names are more informative than common nouns, and they are more informative than pronouns (Lee et al., 2013).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "CoNLL-2012 dataset (Pradhan et al., 2012b) that is the largest annotated dataset with coreference information.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "We use the BART model (Lewis et al., 2020) that is one of the state-of-the-art text generation models.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "CoNLLbart: we use a fine-tuned BART model (Lewis et al., 2020) released by Durmus et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "We use two recent models from the Quoref leaderboard: RoBERTa (Liu et al., 2019) and TASE (Segal et al.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and TASE (Segal et al., 2020), from which TASE has the state-ofthe-art results.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "• MultiRC: Multi-Sentence Reading Comprehension set (Khashabi et al., 2018) is created in a way that answering questions requires a more complex understanding from multiple sentences.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "Transfer-SQuAD reports the results of the sequential training when the model is first trained on the SQuAD training dataset (Rajpurkar et al., 2016) and is then finetuned on Quoref.",
      "startOffset" : 124,
      "endOffset" : 148
    } ],
    "year" : 2021,
    "abstractText" : "Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about coreference. However, as we show, coreference reasoning in MRC is a greater challenge than earlier thought; MRC datasets do not reflect the natural distribution and, consequently, the challenges of coreference reasoning. Specifically, success on these datasets does not reflect a model’s proficiency in coreference reasoning. We propose a methodology for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set. The results on our dataset show that state-ofthe-art models still struggle with these phenomena. Furthermore, we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models. This allows us to show an improvement in the coreference reasoning abilities of state-of-theart models.1",
    "creator" : "LaTeX with hyperref package"
  }
}