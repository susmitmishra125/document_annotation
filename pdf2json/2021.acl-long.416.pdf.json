{
  "name" : "2021.acl-long.416.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "StereoSet: Measuring stereotypical bias in pretrained language models",
    "authors" : [ "Moin Nadeem", "Anna Bethke", "Siva Reddy" ],
    "emails" : [ "mnadeem@mit.edu", "anna.bethke@intel.com,", "siva.reddy@mila.quebec" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5356–5371\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5356"
    }, {
      "heading" : "1 Introduction",
      "text" : "A key idea behind the current success of neural network models for language is pretrained representations such as word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019). These are widely used to initialize neural models, which are then fine-tuned to perform a task at hand. Typically, these are learned from massive text cor-\npora using variants of language modeling objective (i.e., predicting a word given its surrounding context). In the recent years, these representations empowered neural models to attain unprecedented levels of performance gains on multiple language tasks. The resulting models are being deployed widely as services on platforms like Google Cloud and Amazon AWS to serve millions of users.\nWhile this growth is commendable, there are concerns about the fairness of these models. Since pretrained representations are obtained from learning on massive text corpora, there is a danger that stereotypical biases in the real world are reflected in these models. For example, GPT2 (Radford et al., 2019), a pretrained language model, has shown to generate unpleasant stereotypical text when prompted with context containing certain races such as African-Americans (Sheng et al., 2019). In this work, we assess the stereotypical biases of popular pretrained language models.\nThe seminal works of Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) contain stereotypical biases using diagnostic methods like word analogies and association tests. For example, Caliskan et al. show that male names are more likely to be associated with career terms than female names where the association is measured using embedding similarity.\nRecently, studies have attempted to evaluate bias in contextual word embeddings where a word is provided with artificial context (May et al., 2019; Kurita et al., 2019), e.g., the contextual embedding of man is obtained from the embedding of man in the sentence This is a man. However, these have limitations. First, the context does not reflect the natural usage of a word. Second, they require stereotypical attribute terms to be predefined (e.g., pleasant and unpleasant terms). Third, they focus on single word terms and ignore multiword terms like construction worker. Lastly, they study bias of a model independent of its language modeling ability which could lead to undeserved trust in a model if it is a poor language model.\nIn this work, we propose methods to evaluate stereotypical bias of pretrained language models. These methods do not have the aforementioned limitations. Specifically, we design two different association tests, one for measuring bias at sentence level (intrasentence), and the other at discourse level (intersentence) as shown in Figure 1.. In these tests, each target term (e.g., Arab) is provided with a natural context in which it appears, along with three possible associative contexts. The associative contexts help us to evaluate the biases of the model, as well as measure its language modeling performance. We crowdsource StereoSet, a dataset for associative contexts in English containing 4 target domains, 321 target terms and 16,995 test instances (triplets)."
    }, {
      "heading" : "2 Task Definition & Formulation",
      "text" : ""
    }, {
      "heading" : "2.1 Definition",
      "text" : "Following previous literature (Greenwald and Banaji, 1995; Bolukbasi et al., 2016; Caliskan et al., 2017), we define a stereotype as an overgeneralized belief about a particular group of people, e.g., Asians are good at math. Our primary focus is on detecting the presence of stereotypes in pretrained language models. We leave the de-\ntails of mitigating bias from pretrained language models to future work."
    }, {
      "heading" : "2.2 Formulation",
      "text" : "We design our formulation around the desiderata of an ideal language model. An ideal language model should be able to perform the task of language modeling, i.e., it should rank meaningful contexts higher than meaningless contexts. For example, it should tell us that Our housekeeper is a Mexican is more probable than Our housekeeper is a banana. Second, it should not exhibit stereotypical bias, i.e., it should avoid ranking stereotypical contexts higher than anti-stereotypical contexts, e.g., Our housekeeper is a Mexican and Our housekeeper is an American should be equally possible. We desire equally possible instead of anti-stereotype over stereotype because any kind of overgeneralized belief is known to hurt target groups (Czopp et al., 2015). If the model consistently prefers stereotypes over anti-stereotypes, we say that the model exhibits stereotypical bias. Another approach would be to rank a neutral context higher over stereotypical or anti-stereotypical context. In practice, we found that collecting neutral contexts are prone to implicit biases and has low inter-annotator agreement (Section 4).\nBased on these observations, we develop the Context Association Test (CAT), a test that measures the language modeling ability as well as the stereotypical bias of pretrained language models. Although language modeling has standard evaluation metrics such as perplexity, due to varying vocabulary sizes of different pretrained models, this metric becomes incomparable across models. In order to analyse the relationship between language modeling ability and stereotypical bias, we define a simple metric that is appropriate for our task. Evaluating the full language modeling ability of models is beyond the scope of this work.\nIn CAT, given a context containing a target group (e.g., housekeeper), we provide three different ways to instantiate this context. Each instantiation corresponds to either a stereotypical, anti-stereotypical, or a meaningless association. The stereotypical and anti-stereotypical associations are used to measure stereotypical bias, and the meaningless association is used to ensure that an unbiased language model still retains language modeling ability. We include the meaningless association in order to provide a standardized bench-\nmark across both masked and autoregressive language models, which cannot be done with common metrics such as perplexity.\nSpecifically, we design two types of association tests, intrasentence and intersentence CATs, to assess language modeling and stereotypical bias at sentence level and discourse level. Figure 1 shows an example for each."
    }, {
      "heading" : "2.3 Intrasentence",
      "text" : "Our intrasentence task measures the bias and the language modeling ability at sentence-level. We create a fill-in-the-blank style context sentence describing the target group, and a set of three attributes, which correspond to a stereotype, an antistereotype, and a meaningless option (Figure 1a). In order to measure language modeling and stereotypical bias, we determine which attribute has the greatest likelihood of filling the blank, i.e., which of the instantiated contexts is more likely."
    }, {
      "heading" : "2.4 Intersentence",
      "text" : "Our intersentence task measures the bias and the language modeling ability at the discourse-level. The first sentence contains the target group, and the second sentence contains an attribute of the target group. Figure 1b shows the intersentence task. We create a context sentence with a target group that can be succeeded with three attribute sentences corresponding to a stereotype, an antistereotype and a meaningless option. We measure the bias and language modeling ability based on which attribute sentence is likely to follow the context sentence."
    }, {
      "heading" : "3 Related Work",
      "text" : "Our work is inspired from related attempts that aim to measure bias in pretrained representations such as word embeddings and language models."
    }, {
      "heading" : "3.1 Bias in word embeddings",
      "text" : "The two popular methods of testing bias in word embeddings are word analogy tests and word association tests. In word analogy tests, given two words in a certain syntactic or semantic relation (man → king), the goal is generate a word that is in similar relation to a given word (woman → queen). Mikolov et al. (2013) showed that word embeddings capture syntactic and semantic word analogies, e.g., gender, morphology etc. Bolukbasi et al. (2016) build on this observation to study\ngender bias. They show that word embeddings capture several undesired gender biases (semantic relations) e.g. doctor : man :: woman : nurse. Manzini et al. (2019) extend this to show that word embeddings capture several stereotypical biases such as racial and religious biases.\nIn the word embedding association test (WEAT, Caliskan et al. 2017), the association of two complementary classes of words, e.g., European and African names, with two other complementary classes of attributes that indicate bias, e.g., pleasant and unpleasant attributes, are studied to quantify the bias. The bias is defined as the difference in the degree with which European names are associated with pleasant and unpleasant attributes in comparison with African names being associated with those attributes. Here, the association is defined as the similarity between the name and attribute word embeddings. This is the first large scale study that showed word embeddings exhibit several stereotypical biases and not just gender bias. Our inspiration for CAT comes from WEAT."
    }, {
      "heading" : "3.2 Bias in pretrained language models",
      "text" : "May et al. (2019) extend WEAT to sentence encoders, calling it the Sentence Encoder Association Test (SEAT). For a target term and its attribute, they create artificial sentences using generic context of the form \"This is [target].\" and \"They are [attribute].\" and obtain contextual word embeddings of the target and the attribute terms. They repeat Caliskan et al. (2017)’s study using these embeddings and cosine similarity as the association metric but their study was inconclusive. Later, Kurita et al. (2019) show that cosine similarity is not the best association metric and define a new association metric based on the probability of predicting an attribute given the target in generic sentential context, e.g., [target] is [mask], where [mask] is the attribute. They show that similar observations of Caliskan et al. (2017) are observed on contextual word embeddings too. Our intrasentence CAT is similar to their setting but with natural context. We also go beyond intrasentence to propose intersentence CATs, since language modeling is not limited at sentence level.\nConcurrent to our work, Nangia et al. (2020) introduced CrowS-Pairs, which examines stereotypical bias via minimal pairs. However, CrowSPairs only studies bias within a single sentence (intrasentence) and ignores discourse-level (inter-\nsentence) measurements. Furthermore, StereoSet contains an order of magnitude of data that contains greater variety, and hence, has the potential to detect a wider range of biases that may be otherwise overlooked. Lastly, StereoSet measures bias across both masked and autoregressive language models, while CrowS-Pairs only measures bias in masked language models."
    }, {
      "heading" : "3.3 Measuring bias through extrinsic tasks",
      "text" : "Another method to evaluate bias in pretrained representations is to measure bias on extrinsic tasks like coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) and sentiment analysis (Kiritchenko and Mohammad, 2018). This method fine-tunes pretrained representations on the target task. The bias in pretrained representations is estimated by the target task’s performance. However, it is hard to segregate the bias of task-specific training data from the pretrained representations. Our CATs are an intrinsic way to evaluate bias in pretrained models."
    }, {
      "heading" : "4 Dataset Creation",
      "text" : "In StereoSet, we select four domains as the target domains of interest for measuring bias: gender, profession, race and religion. For each domain, we select terms (e.g., Asian) that represent a social group. For collecting target term contexts and their associative contexts, we employ crowdworkers via Amazon Mechanical Turk.1 We restrict ourselves to crowdworkers in USA since stereotypes could change based on the country. Table 1 shows the overall statistics of StereoSet. We also provide a full data statement in Section 9 (Bender and Friedman, 2018)."
    }, {
      "heading" : "4.1 Target terms selection",
      "text" : "We curate diverse set of target terms for the target domains using Wikidata relation triples (Vrandečić and Krötzsch, 2014). A Wikidata triple is of the form <subject, relation, object> (e.g., <Brad Pitt, P106, Actor>). We collect all objects occurring with the relations P106 (profession), P172 (race), and P140 (religion) as the target terms. We manually filter terms that are either infrequent or too fine-grained (assistant producer is merged with producer). We collect gender terms from\n1Screenshots of our Mechanical Turk interface and details about task setup are available in the Section 9.6.\nNosek et al. (2002). A list of target terms is available in Appendix A.1."
    }, {
      "heading" : "4.2 CATs collection",
      "text" : "In the intrasentence CAT, for each target term, a crowdworker writes attribute terms that correspond to stereotypical, anti-stereotypical and meaningless associations of the target term. Then, they provide a context sentence containing the target term. The context is a fill-in-the-blank sentence, where the blank can be filled either by the stereotype term or the anti-stereotype term but not the meaningless term.\nIn the intersentence CAT, they first provide a sentence containing the target term. Then, they provide three associative sentences corresponding to stereotypical, anti-stereotypical and meaningless associations. These associative sentences are such that the stereotypical and the antistereotypical sentences can follow the target term sentence but the meaningless ones cannot follow the target term sentence.\nWe also experimented with a variant that asked crowdworkers to provide a neutral association for the target term, but found that crowdworkers had significant trouble remaining neutral. In the validation step (next section), we found that many of these neutral associations are often classified as stereotype or anti-stereotype by multiple validators. We conjecture that attaining neutrality is hard is due to anchoring bias (Tversky and Kahneman, 1974), i.e., stereotypical associations are easy to think and access and could implicitly affect crowdworkers to tilt towards them. Therefore, we discard the notion of neutrality. Some examples are shown in Appendix A.4."
    }, {
      "heading" : "4.3 CATs validation and human agreement",
      "text" : "In order to ensure that stereotypes reflect common views, we validate the data collected in the above step with additional workers. For each context and its associations, we ask five validators to classify each association into a stereotype, an anti-stereotype or a meaningless association. We only retain CATs where at least three validators agree on the labels.2 This filtering results in selecting 83% of the CATs, indicating that there is regularity in stereotypical views among the workers. Table 10 shows detailed agreement scores for\n2One can increase the quality of the data further by selecting examples where four or more workers agree upon.\nstereotypes computed using the average of annotator agreement per example."
    }, {
      "heading" : "4.4 Dataset analysis",
      "text" : "Are people prone to view stereotypes negatively? To answer this question, we classify stereotypes into positive and negative sentiment classes using a sentiment classifier (details in Appendix A.2). As evident in Table 2, people do not always associate stereotypes with negative associations (e.g., Asians are good at math has positive sentiment). However, people associate stereotypes with relatively more negative associations than antistereotypes (41% vs. 33%).\nWe also extract keywords in StereoSet to analyze which words are most commonly associated with target groups. We define a keyword as a word that is more frequent in StereoSet than the natural distribution of words (Kilgarriff, 2009; Jakubicek et al., 2013). Table 3 shows the top keywords of each domain. These keywords indicate that target terms in gender and race are associated with physical attributes such as beautiful, feminine, masculine, etc., professional terms are associated with behavioural attributes such as pushy, greedy, hardwork, etc., and religious terms are associated with belief attributes such as diety, forgiving, reborn, etc. This aligns with expectations and indicates that multiple annotators use similar attributes.\nGender\nstepchild masculine bossy ma uncare breadwinner immature naggy feminine rowdy possessive manly polite studious homemaker burly\nProfession\nnerdy uneducated bossy hardwork pushy unintelligent studious dumb rude snobby greedy sloppy disorganize talkative uptight dishonest\nRace\npoor beautiful uneducated smelly snobby immigrate wartorn rude industrious wealthy dangerous accent impoverish lazy turban scammer\nReligion"
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "In this section, we describe the data splits, evaluation metrics and the baselines."
    }, {
      "heading" : "5.1 Development and test sets",
      "text" : "We split StereoSet based on the target terms: 25% of the target terms and their instances for the development set and 75% for the test set. We ensure terms in the development set and test set are disjoint. We do not have a training set since this defeats the purpose of StereoSet, which is to measure the biases of pretrained language models (and not the models fine-tuned on StereoSet)."
    }, {
      "heading" : "5.2 Evaluation Metrics",
      "text" : "Our desiderata of an ideal language model is that it excels at language modeling while not exhibiting stereotypical biases. In order to determine success at both these goals, we evaluate both language modeling and stereotypical bias of a given model. We pose both problems as ranking problems.\nLanguage Modeling Score (lms) In the language modeling case, given a target term context and two possible associations of the context, one meaningful and the other meaningless, the model has to rank the meaningful association higher than meaningless association. The meaningful association corresponds to either the stereotype or the anti-stereotype option.\nWe define the language modeling score (lms) of a target term as the percentage of instances in which a language model prefers the meaningful over meaningless association. We define the overall lms of a dataset as the average lms of the target terms in the split. The lms of an ideal language model is 100, i.e., for every target term in a dataset, the model always prefers the meaningful association of the term.\nAs discussed in Section 2.2, the goal of this metric is not to evaluate the full scale language modeling ability, but only to provide an reasonable metric that allows comparison between different models to analyze the relationship between language modeling ability and stereotypical bias.\nStereotype Score (ss) Similarly, we define the stereotype score (ss) of a target term as the percentage of examples in which a model prefers a stereotypical association over an anti-stereotypical association. We define the overall ss of a dataset as the average ss of the target terms in the dataset. The ss of an ideal language model is 50, for every target term, the model prefers neither stereotypical associations nor anti-stereotypical associations.\nIdealized CAT Score (icat) StereoSet motivates a question around how practitioners should prefer models for real-world deployment. Just because a model has low stereotypical bias does not mean it is preferred over others. For example, although a random language model exhibits the lowest stereotypical bias (ss = 50) it is the worst language model (lms = 50). While model selection desiderata is often task-specific, we introduce a simple point-estimate called the idealized CAT (icat) score for model comparison assuming equal importance to language modeling ability and stereotypical bias. We define the icat score as lms ∗ min(ss,100−ss)50 centered around the idea that an ideal language model has an icat score of 100 and a stereotyped model has a score of 0. Appendix A.6 presents a detailed formulation and Figure 2 (Appendix) highlights this idea."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "IDEALLM We define this hypothetical model as the one that always picks correct associations for a given target term context. It also picks equal number of stereotypical and anti-stereotypical associations over all the target terms. So the resulting lms and ss scores are 100 and 50 respectively.\nSTEREOTYPEDLM We define this hypothetical model as the one that always picks a stereotypical association over an anti-stereotypical association. So its ss is 100 irrespective of its lms.\nRANDOMLM We define this model as the one that picks associations randomly, and therefore its lms and ss scores are both 50.\nSENTIMENTLM In Section 4.4, we saw that stereotypical instantiations are more frequently associated with negative sentiment than antistereotypes. In this baseline, we assess if sentiment can be used to detect a stereotypical association. For a given a pair of context associations, the model always picks the association with the most negative sentiment."
    }, {
      "heading" : "6 Main Experiments",
      "text" : "In this section, we evaluate pretrained models such as BERT (Devlin et al., 2019), ROBERTA (Liu et al., 2019), XLNET (Yang et al., 2019) and GPT2 (Radford et al., 2019) on StereoSet."
    }, {
      "heading" : "6.1 Masked Language Models",
      "text" : "While scoring sentences using autoregressive language models is well-defined, there is no corresponding scoring mechanism for masked language models. As a result, we evaluate our models using both likelihood-based scoring and psuedolikelihood scoring (Nangia et al., 2020).\nLikelihood-based Scoring For intrasentence CATs, we define the score as the log probability of an attribute term to fill the blank. If the attribute consists of multiple subwords, we iteratively unmask the subwords from left to right, and compute the average per-subword probability. We rank a given pair of attribute terms based on these probabilities (the one with higher probability is preferred). In intersentence CATs, inspired by Devlin et al. (2019), we use a Next Sentence Prediction (NSP) task to rank the possible associations. For all models, we train identical Next Sentence Prediction heads on identical datasets (details given\nin Appendix A.5), and compute the log likelihood that any given target sentence follows the context. Given a pair of associations, we rank each association using this score.\nPsuedo-likelihood Scoring Nangia et al. (2020) adopts psuedo-likelihood based scoring (Salazar et al., 2020) that does not penalize less frequent attribute terms. In intrasentence CAT, we choose to never mask the attribute term but mask each context term one at a time and measure the psuedo-probability of the sentence given the attribute term. We refer the reader to Nangia et al. (2020) for more information on this scoring mechanism. In intersentence CATs, we measure the psuedolikelihood of the context sentence conditioned on the attribute sentence by iteratively masking the tokens in the context sentence while keeping the attribute sentence unchanged."
    }, {
      "heading" : "6.2 Autoregressive Language Models",
      "text" : "Unlike above models, GPT2 is a generative model in an auto-regressive setting. For the intrasentence CAT, we instantiate the blank with an attribute term and compute the probability of the full sentence. Given a pair of associations, we rank each association using this score. For the intersentence CAT, our scoring mechanism mirrors that for masked language models. If the likelihoodbased scoring mechanism is used, then we train an NSP head on identical datasets (details given in Appendix A.5) and compute the log likelihood that any given target sentence follows the context. If the masked language models are scored with psuedo-likelihood, then we measure the effect of the context sentence by measuring the joint probability of the attribute sentence with and without the context. Given a pair of associations, we rank each association by the ratio of these probabilities."
    }, {
      "heading" : "7 Results and discussion",
      "text" : "Table 4 shows the overall results of baselines and models on StereoSet test set when using likelihood-based scoring, and Table 5 shows the results when using psuedo-likelihood based scoring. The results exhibit similar trends on the development and test sets. Since the initial version of this paper3 used likelihood-based scoring, we mainly center the discussion around it as the trends are similar to pseudo-likelihood.\n3Apr 2020 arXiv:2004.09456\nBaselines vs. Models As seen in Table 4, all pretrained models have higher lms values than RANDOMLM indicating that these are better language models as expected. Among models, GPT2-large is the best performing language model (88.3) followed by GPT2-medium (85.9).\nComing to stereotypical bias, all pretrained models demonstrate more stereotypical behavior than RANDOMLM. While GPT2-large is the most stereotypical model of all pretrained models (60.1), ROBERTA-base is the least stereotypical model (50.5). SENTIMENTLM achieves the highest stereotypical score compared to all pretrained models, indicating that sentiment can indeed be exploited to detect stereotypical associations. However, its language model performance is worse, which is expected, since sentiment alone isn’t sufficient to distinguish meaningful and meaningless sentences.\nRelation between lms and ss All models exhibit a strong correlation between lms and ss (Spearman rank correlation ρ of 0.87). As the language model becomes stronger, its stereotypical bias (ss) does too. We build the strongest language model, ENSEMBLE, using a linear weighted combination of BERT-large, GPT2-medium, and GPT2-large, which is also found to be the most biased model (ss = 62.5). The correlation between lms and ss is unfortunate and perhaps un-\navoidable as long as we rely on the real world distribution of corpora to train language models since these corpora are likely to reflect stereotypes. Amongst the models, GPT2 exhibits more unbiased behavior than other models (icat score of 73.0). However, this metric is not intended as the sole criterion for model selection. Further research is required in designing better metrics.\nImpact of model size For a given architecture, all of its pretrained models are trained on the same corpora but with different number of parameters. For example, both BERT-base and BERT-large are trained on Wikipedia and BookCorpus (Zhu et al., 2015) with 110M and 340M parameters respectively. As the model size increases, we see that its language modeling ability (lms) increases, and correspondingly its stereotypical score.\nImpact of scoring mechanism We evaluate models using both likelihood based scoring and psuedo-likelihood based scoring. First, we note that likelihood-based (ll) scoring is higher than psuedo-likelihood-based (pll) scoring by a narrow margin (avg lmsll = 79.88, avg lmspll = 79.68). For intrasentence CATs, psuedo-likelihood outperforms likelihood scoring by a wide margin (avg lmsll = 75.7, avg lmspll = 79.4). However, psuedo-likelihood scoring is significantly degraded for intersentence CATs (avg lmsll =\n78.82, avg lmspll = 75.98). This suggests that psuedo-likelihood has trouble scoring longer sequences. Moreover, Aribandi et al. (2021) has shown that psuedo-likelihood has higher variance than likelihood scoring.\nImpact of pretraining corpora BERT, ROBERTA, XLNET and GPT2 are trained on 16GB, 160GB, 158GB and 40GB of text corpora. Surprisingly, the corpora size does not correlate with either lms or ss. This could be due to the differences in architectures and corpora types. A better way to verify this would be to train the same model on increasing amounts of corpora. Due to lack of computing resources, we leave this work for the community. We conjecture that the high performance of GPT2 (high lms and high ss) is due to the nature of its training data. GPT2 is trained on documents linked from Reddit. Since Reddit has several subreddits related to target terms in StereoSet (e.g., relationships, religion), GPT2 is likely to be exposed to contextual\nassociations that contain real-world bias.\nDomain-wise bias Table 8 shows domain-wise results of the ENSEMBLE model on the test set. The model is relatively less biased on race than on others (ss = 61.8). We also show the most and least biased target terms for each domain from the development set (see Table 10 for humanagreement scores, a proxy for most and least biased terms). We conjecture that the most biased terms are those that have well established stereotypes and are also frequent in language. This is the case with mother (attributes: caring, cooking), software developer (attributes: geek, nerd), and Africa (attributes: poor, dark). The least biased are those that do not have well established stereotypes, for example, producer and Crimean. The outlier is Muslim, although it has established stereotypes indicated by the high human agreement (see Table 10). This requires further investigation.\nIntrasentence vs Intersentence CATs Table 6 shows the results of intrasentence and intersen-\ntence CATs on the test set. Since intersentence tasks has more number of words per instance, we expect intersentence language modeling task to be harder than intrasentence, especially results computed using psuedo-likelihood (Table 7)."
    }, {
      "heading" : "8 Conclusions",
      "text" : "In this work, we develop the Context Association Test (CAT) to measure the stereotypical biases of pretrained language models in contrast with their language modeling ability. We crowdsource StereoSet, a dataset containing 16,995 CATs to test biases in four domains: gender, profession, race and religion. We show that current pretrained language models exhibit strong stereotypical biases. We also find that language modeling ability correlates with the degree of stereotypical bias. This dependence has to be broken if we are to achieve unbiased language models.\nWe hope that StereoSet will spur further research in evaluating and mitigating bias in language models. We also note that achieving an ideal performance on StereoSet does not guarantee that a model is unbiased since bias can manifest in many ways (Gonen and Goldberg, 2019; Bender et al., 2021)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers, Yonatan Belinkov, Vivek Kulkarni, and Spandana Gella for their helpful comments in reviewing this paper. This work was completed in part while MN and AB were at Intel AI."
    }, {
      "heading" : "9 Ethics and Data Statement",
      "text" : "Following Bender and Friedman (2018), we provide the following ethics and data statement."
    }, {
      "heading" : "9.1 Curation Rationale",
      "text" : "StereoSet is a crowdsourced dataset that was created as a benchmark for stereotypical biases in pretrained language models. This dataset consists of 4 target domains, 321 target terms, and 16,995 test instances. StereoSet is in English and is tailored for the stereotypes that exist in the United States. The data was explicitly curated with a goal of creating a set of stereotypical and antistereotypical examples.\nEach example in the dataset consists of a triple. Each triple consists of a target context, with a corresponding stereotypical, anti-stereotypical, or unrelated association that stereotypes the target or combats stereotypes about the target.\nWe collected this data via Amazon Mechanical Turk (AMT), where each example was written by one crowdworker and validated by four other crowdworkers. We required all crowdworkers to be in the United States and have a HIT acceptance rate greater than 97%. We paid all workers with a minimum wage of $15 an hour in compliance with our funding agencies’ AMT policy."
    }, {
      "heading" : "9.2 Language Variety",
      "text" : "We require crowdworkers to be within the United States, and all examples are written in US English (en-US). However, we do not enforce any constraints on, nor do we collect, the dialect that is used."
    }, {
      "heading" : "9.3 Annotator Demographics",
      "text" : "Our annotators came from Amazon Mechanical Turk (AMT), and we provided no filters beyond the 97% HIT acceptance rate. In total, 475 and 803 annotators completed the intrasentence and intersentence tasks respectively. Difallah et al. (2018) shows that the Amazon Mechanical Turk population is 55% women and 45% men, with 80% of the populous under the age of 50. The median income of workers on AMT is $47k; in contrast, the United States has a median income of $57k."
    }, {
      "heading" : "9.3.1 Speech Situation",
      "text" : "All text was written in English, and was never edited after the speaker wrote it. The time and place were unconstrained. We prompted the\nspeaker to stereotype and anti-stereotype a given target word. We informed them that their work would be used for a scientific study and they were encouraged to explicitly stereotype target groups."
    }, {
      "heading" : "9.4 Text Characteristics",
      "text" : "StereoSet measures stereotypical biases in gender, profession, race, and religion. The intrasentence task lends itself to a \"fill-in-the-blank\" nature, while the intersentence task asks annotators to contextualize a pair of sentences."
    }, {
      "heading" : "9.5 Recording Quality",
      "text" : "The data was only written, and never recorded."
    }, {
      "heading" : "9.6 Interface",
      "text" : "Our Mechanical Turk interface is shown in Figure 3 and Figure 4 for the intrasentence and intersentence tasks respectively."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 List of Target Words\nTable 10 list our target terms used in the dataset collection task.\nA.2 Fine-Tuning BERT for Sentiment Analysis\nIn order to evaluate sentiment, we fine-tune BERT (Devlin et al., 2019) on movie reviews (Maas et al., 2011) for seven epochs. We used a maximum sequence length of 256 WordPieces, batch size 32, and used Adam with a learning rate of 1e−4. Our fine-tuned model achieves an 92% test accuracy on the Large Movie Review dataset. It is important to note that the classifier also classifies neutral sentiment such as My housekeeper is a Mexican as positive.\nA.3 Reproducibility Checklist\nWe used 2 RTX 2080 Ti to fine-tune all of the models for the next sentence prediction task, although this could have been done with one GPU as well. The only exception to this is GPT2large, which required 2 Tesla V100s (with 32GB of RAM) to fine-tune. We will release all pretrained models to encourage reproducibility, and each model’s biases is able to be evaluated within 5 minutes.\nSince we are using standardized models, the sizes of every single model is available online. For convenience, we report it here:\n1. BERT-base: 110M parameters\n2. BERT-large: 340M parameters\n3. GPT2-small: 117M parameters\n4. GPT2-medium: 345M parameters\n5. GPT2-large: 774M parameters\n6. ROBERTA-base: 125M parameters\n7. ROBERTA-large: 355M parameters\n8. XLNET-base: 110M parameters\n9. XLNET-large: 340M parameters\nA.4 Collecting Neutral Associations\nWe also experimented with testing whether models endorse a contextually neutral association over the stereotypical or anti-stereotypical context. However, we found that crowd workers had a poor inter-annotator agreement when asked to write neutral continuations. Table 9 highlights some collected sentences that were neutral.\nMany of the contextually neutral associations in Table 9 show highly stereotypical behavior. For instance, \"the ethiopian woman had never been so angry\" characterizes the Angry Black Woman stereotype (Collins, 2004). Furthermore, we hold that some of these neutral sentences aren’t truly neutral; the chess player was bearded may inadvertently conceal stereotypes, since both chess players and bearded men are commonly seen as wise. Hence, a model may endorse a neutral sentence for the wrong reasons.\nA.5 General Methods for Training a Next Sentence Prediction Head\nGiven some context c, and some sentence s, our intersentence task requires calculating the likelihood p(s|c), for some sentence s and context sentence c.\nWhile BERT has been trained with a Next Sentence Prediction classification head to provide p(s|c), the other models have not. In this section, we detail our creation of a Next Sentence Prediction classification head as a downstream task.\nFor some sentences A and B, our task is simply determining if Sentence A follows Sentence B, or if Sentence B follows Sentence A. We trivially generate this corpus from Wikipedia by sampling some ith sentence, i + 1th sentence, and a randomly chosen negative sentence from any other\narticle. We maintain a maximum sequence length of 256 tokens, and our training set consists of 9.5 million examples.\nWe train with a batch size of 80 sequences until convergence (80 sequences / batch * 256 tokens / sequence = 20,480 tokens/batch) for 10 epochs over the corpus. For BERT, We use BertAdam as the optimizer, with a learning rate of 1e-5, a linear warmup schedule from 50 steps to 500 steps, and minimize cross entropy for our loss function. Our results are comparable to Devlin et al. (2019), with each model obtaining 93-98% accuracy against the test set of 3.5 million examples.\nAdditional models maintain the same experimental details. Our NSP classifier achieves an 94.6% accuracy with ROBERTA-base, a 97.1% accuracy with ROBERTA-large, a 93.4% accuracy with XLNET-base and 94.1% accuracy with XLNET-large.\nIn order to evaluate GPT-2 on intersentence tasks, we feed the mean-pooled representations across the entire sequence length into the classification head. Our NSP classifier obtains a 92.5% accuracy on GPT2-small, 94.2% on GPT2-medium, and 96.1% on GPT2-large. In order to fine-tune GPT2-large on our machines, we utilized gradient accumulation with a step size of 10, and mixed precision training from Apex.\nA.6 Motivating the ICAT score\nTo address situations where a point estimate that combines lms and ss is required (ie. ranking models), we develop the idealized CAT (icat) score. We recognize that various applications have different trade-offs between fairness and accuracy. We address a generic case where accuracy and fairness are equally important. We derive the icat score from the following axioms:\n• An ideal model has an icat score of 100, i.e., when its lms is 100 and ss is 50, its icat score is 100.\n• A fully biased model has an icat score of 0, i.e., when its ss is either 100 (always prefer a stereotype over an anti-stereotype) or 0 (always prefer an anti-stereotype over a stereotype), its icat score is 0.\n• A random model has an icat score of 50, i.e., when its lms is 50 and ss is 50, its icat score must be 50.\nTherefore we define icat score as\nicat = lms ∗ min(ss, 100− ss) 50\nThis equation satisfies all the axioms. Here min(ss,100−ss) 50 ∈ [0, 1] is maximized when the model prefers neither stereotypes nor antistereotypes for each target term and is minimized when the model favours one over the other. We scale this value using the language modeling score. An interpretation of icat is that it represents the language modeling ability of a model to behave in an unbiased manner while excelling at language modeling.\nFigure 2 depicts the values that the icat score may take on."
    } ],
    "references" : [ {
      "title" : "How reliable are model diagnostics",
      "author" : [ "Vamsi Aribandi", "Yi Tay", "Donald Metzler" ],
      "venue" : "In Proceedings of ACL Findings",
      "citeRegEx" : "Aribandi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Aribandi et al\\.",
      "year" : 2021
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Y. Zou", "Venkatesh Saligrama", "Adam T. Kalai." ],
      "venue" : "Proceedings of Neural Information Processing Systems",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like biases",
      "author" : [ "Aylin Caliskan", "Joanna J. Bryson", "Arvind Narayanan." ],
      "venue" : "Science, 356(6334):183–186.",
      "citeRegEx" : "Caliskan et al\\.,? 2017",
      "shortCiteRegEx" : "Caliskan et al\\.",
      "year" : 2017
    }, {
      "title" : "Black sexual politics: African Americans, gender, and the new racism",
      "author" : [ "Patricia Hill Collins." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Collins.,? 2004",
      "shortCiteRegEx" : "Collins.",
      "year" : 2004
    }, {
      "title" : "Positive stereotypes are pervasive and powerful",
      "author" : [ "Alexander M Czopp", "Aaron C Kay", "Sapna Cheryan." ],
      "venue" : "Perspectives on Psychological Science, 10(4):451–463.",
      "citeRegEx" : "Czopp et al\\.,? 2015",
      "shortCiteRegEx" : "Czopp et al\\.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Demographics and dynamics of mechanical turk workers",
      "author" : [ "Djellel Difallah", "Elena Filatova", "Panos Ipeirotis." ],
      "venue" : "Proceedings of the ACM International Conference on Web Search and Data Mining, WSDM ’18, pages 135 – 143, New York, NY, USA.",
      "citeRegEx" : "Difallah et al\\.,? 2018",
      "shortCiteRegEx" : "Difallah et al\\.",
      "year" : 2018
    }, {
      "title" : "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
      "author" : [ "Hila Gonen", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Gonen and Goldberg.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gonen and Goldberg.",
      "year" : 2019
    }, {
      "title" : "Implicit social cognition: attitudes, self-esteem, and stereotypes",
      "author" : [ "Anthony G. Greenwald", "Mahzarin R. Banaji." ],
      "venue" : "Psychological review, 102(1):4.",
      "citeRegEx" : "Greenwald and Banaji.,? 1995",
      "shortCiteRegEx" : "Greenwald and Banaji.",
      "year" : 1995
    }, {
      "title" : "Universal Language Model Fine-tuning for Text Classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Proceedings of the Association for Computational Linguistics, pages 328–339, Melbourne, Australia. Association for Computational Linguistics.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "The tenten corpus family",
      "author" : [ "Milos Jakubicek", "Adam Kilgarriff", "Vojtech Kovar", "Pavel Rychly", "Vit Suchomel." ],
      "venue" : "Proceedings of the International Corpus Linguistics Conference CL.",
      "citeRegEx" : "Jakubicek et al\\.,? 2013",
      "shortCiteRegEx" : "Jakubicek et al\\.",
      "year" : 2013
    }, {
      "title" : "Simple maths for keywords",
      "author" : [ "Adam Kilgarriff." ],
      "venue" : "Proceedings of the Corpus Linguistics Conference 2009 (CL2009), page 171.",
      "citeRegEx" : "Kilgarriff.,? 2009",
      "shortCiteRegEx" : "Kilgarriff.",
      "year" : 2009
    }, {
      "title" : "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of Joint Conference on Lexical and Computational Semantics, pages 43–53.",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2018",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2018
    }, {
      "title" : "Measuring bias in contextualized word representations",
      "author" : [ "Keita Kurita", "Nidhi Vyas", "Ayush Pareek", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166–172, Florence,",
      "citeRegEx" : "Kurita et al\\.,? 2019",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the Association for Computational Linguistics, pages 142–150, Portland, Oregon, USA.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "author" : [ "Thomas Manzini", "Lim Yao Chong", "Alan W Black", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the North American Chapter of the Association",
      "citeRegEx" : "Manzini et al\\.,? 2019",
      "shortCiteRegEx" : "Manzini et al\\.",
      "year" : 2019
    }, {
      "title" : "On measuring social biases in sentence encoders",
      "author" : [ "Chandler May", "Alex Wang", "Shikha Bordia", "Samuel R. Bowman", "Rachel Rudinger." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 622–628,",
      "citeRegEx" : "May et al\\.,? 2019",
      "shortCiteRegEx" : "May et al\\.",
      "year" : 2019
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of Neural Information Processing Systems (NeurIPS), NIPS 13, pages 3111 –",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
      "author" : [ "Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel R. Bowman." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Nangia et al\\.,? 2020",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2020
    }, {
      "title" : "Math = male, me = female, therefore math != me",
      "author" : [ "Brian Nosek", "Mahzarin Banaji", "Anthony Greenwald." ],
      "venue" : "Journal of personality and social psychology, 83:44–59.",
      "citeRegEx" : "Nosek et al\\.,? 2002",
      "shortCiteRegEx" : "Nosek et al\\.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Gender bias in coreference resolution",
      "author" : [ "Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme." ],
      "venue" : "Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), pages 8–14.",
      "citeRegEx" : "Rudinger et al\\.,? 2018",
      "shortCiteRegEx" : "Rudinger et al\\.",
      "year" : 2018
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "The woman worked as a babysitter: On biases in language generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Premkumar Natarajan", "Nanyun Peng." ],
      "venue" : "Proceedings of the Empirical Methods in Natural Language Processing and the International",
      "citeRegEx" : "Sheng et al\\.,? 2019",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Judgment under uncertainty: Heuristics and biases",
      "author" : [ "Amos Tversky", "Daniel Kahneman." ],
      "venue" : "science, 185(4157):1124–1131.",
      "citeRegEx" : "Tversky and Kahneman.,? 1974",
      "shortCiteRegEx" : "Tversky and Kahneman.",
      "year" : 1974
    }, {
      "title" : "Wikidata: A free collaborative knowledgebase",
      "author" : [ "Denny Vrandečić", "Markus Krötzsch." ],
      "venue" : "Commun. ACM, 57(10):78–85.",
      "citeRegEx" : "Vrandečić and Krötzsch.,? 2014",
      "shortCiteRegEx" : "Vrandečić and Krötzsch.",
      "year" : 2014
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "H. Wallach, H. Larochelle, A. Beygelzimer, F. d’e Buc, E. Fox,",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Gender Bias in Coreference Resolution: Evaluation and Debiasing",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE In-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "Reproducibility Checklist We used 2 RTX 2080 Ti to fine-tune all of the models for the next sentence prediction",
      "author" : [ "positive. A" ],
      "venue" : null,
      "citeRegEx" : "A.3,? \\Q2080\\E",
      "shortCiteRegEx" : "A.3",
      "year" : 2080
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "A key idea behind the current success of neural network models for language is pretrained representations such as word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (Peters et al.",
      "startOffset" : 130,
      "endOffset" : 177
    }, {
      "referenceID" : 22,
      "context" : "A key idea behind the current success of neural network models for language is pretrained representations such as word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (Peters et al.",
      "startOffset" : 130,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : ", 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : ", 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : ", 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : ", 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : ", 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "For example, GPT2 (Radford et al., 2019), a pretrained language model, has shown to generate unpleasant stereotypical text when prompted with context containing certain races such as African-Americans (Sheng et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : ", 2019), a pretrained language model, has shown to generate unpleasant stereotypical text when prompted with context containing certain races such as African-Americans (Sheng et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 188
    }, {
      "referenceID" : 19,
      "context" : "(2017) show that word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : ", 2013) and GloVe (Pennington et al., 2014) contain stereotypical biases using diagnostic methods like word analogies and association tests.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "Recently, studies have attempted to evaluate bias in contextual word embeddings where a word is provided with artificial context (May et al., 2019; Kurita et al., 2019), e.",
      "startOffset" : 129,
      "endOffset" : 168
    }, {
      "referenceID" : 14,
      "context" : "Recently, studies have attempted to evaluate bias in contextual word embeddings where a word is provided with artificial context (May et al., 2019; Kurita et al., 2019), e.",
      "startOffset" : 129,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "Following previous literature (Greenwald and Banaji, 1995; Bolukbasi et al., 2016; Caliskan et al., 2017), we define a stereotype as an overgeneralized belief about a particular group of people, e.",
      "startOffset" : 30,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Following previous literature (Greenwald and Banaji, 1995; Bolukbasi et al., 2016; Caliskan et al., 2017), we define a stereotype as an overgeneralized belief about a particular group of people, e.",
      "startOffset" : 30,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "Following previous literature (Greenwald and Banaji, 1995; Bolukbasi et al., 2016; Caliskan et al., 2017), we define a stereotype as an overgeneralized belief about a particular group of people, e.",
      "startOffset" : 30,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "We desire equally possible instead of anti-stereotype over stereotype because any kind of overgeneralized belief is known to hurt target groups (Czopp et al., 2015).",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "Another method to evaluate bias in pretrained representations is to measure bias on extrinsic tasks like coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) and sentiment analysis (Kiritchenko and Mohammad, 2018).",
      "startOffset" : 128,
      "endOffset" : 170
    }, {
      "referenceID" : 31,
      "context" : "Another method to evaluate bias in pretrained representations is to measure bias on extrinsic tasks like coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) and sentiment analysis (Kiritchenko and Mohammad, 2018).",
      "startOffset" : 128,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "We also provide a full data statement in Section 9 (Bender and Friedman, 2018).",
      "startOffset" : 51,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "We curate diverse set of target terms for the target domains using Wikidata relation triples (Vrandečić and Krötzsch, 2014).",
      "startOffset" : 93,
      "endOffset" : 123
    }, {
      "referenceID" : 28,
      "context" : "We conjecture that attaining neutrality is hard is due to anchoring bias (Tversky and Kahneman, 1974), i.",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "We define a keyword as a word that is more frequent in StereoSet than the natural distribution of words (Kilgarriff, 2009; Jakubicek et al., 2013).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "We define a keyword as a word that is more frequent in StereoSet than the natural distribution of words (Kilgarriff, 2009; Jakubicek et al., 2013).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "In this section, we evaluate pretrained models such as BERT (Devlin et al., 2019), ROBERTA (Liu et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : ", 2019), ROBERTA (Liu et al., 2019), XLNET (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 30,
      "context" : ", 2019), XLNET (Yang et al., 2019) and GPT2 (Radford et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "As a result, we evaluate our models using both likelihood-based scoring and psuedolikelihood scoring (Nangia et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "(2020) adopts psuedo-likelihood based scoring (Salazar et al., 2020) that does not penalize less frequent attribute terms.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 32,
      "context" : "For example, both BERT-base and BERT-large are trained on Wikipedia and BookCorpus (Zhu et al., 2015) with 110M and 340M parameters respectively.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "We also note that achieving an ideal performance on StereoSet does not guarantee that a model is unbiased since bias can manifest in many ways (Gonen and Goldberg, 2019; Bender et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 190
    } ],
    "year" : 2021,
    "abstractText" : "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data; 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT2, ROBERTA, and XLNET. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset. mit.edu.",
    "creator" : "LaTeX with hyperref"
  }
}