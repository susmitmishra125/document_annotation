{
  "name" : "2021.acl-long.121.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition",
    "authors" : [ "Shuang Wu", "Xiaoning Song", "Zhenhua Feng" ],
    "emails" : [ "shuangwu@stu.jiangnan.edu.cn", "x.song@jiangnan.edu.cn,", "z.feng@surrey.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1529–1539\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1529"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) plays an essential role in structuring of unstructured text. It is a sequence tagging task that extracts named entities from unstructured text. Common categories of NER include names of people, places, organizations, time, quantity, currency, and some proper nouns. NER is the basis for many Natural Language Processing (NLP) tasks such as event extraction (Chen et al., 2015), question answering (Diefenbach et al., 2018), information re-\n∗Corresponding author. 1The source code of the proposed method is publicly available at https://github.com/CoderMusou/ MECT4CNER.\ntrieval (Khalid et al., 2008), knowledge graph construction (Riedel et al., 2013), etc.\nCompared with English, there is no space between Chinese characters as word delimiters. Chinese word segmentation is mostly distinguished by readers through the semantic information of sentences, posing many difficulties to Chinese NER (Duan and Zheng, 2011; Ma et al., 2020). Besides, the task also has many other challenges, such as complex combinations, entity nesting, and indefinite length (Dong et al., 2016).\nIn English, different words may have the same root or affix that better represents the word’s semantics. For example, physiology, psychology, sociology, technology and zoology contain the same suffix, ‘-logy’, which helps identify the entity of a subject name. Besides, according to the information of English words, root or affixes often determine general meanings (Yadav et al., 2018). The root, such as ‘ophthalmo-’ (ophthalmology), ‘esophage’ (esophagus) and ‘epithelio-’ (epithelium), can help human or machine to better recognize professional nouns in medicine. Therefore, even the stateof-the-art methods, such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), trained on large-scale datasets, adopt this delicate word segmentation method for performance boost.\nFor Chinese characters, there is also a structure\nsimilar to the root and affixes in English. According to the examples in Table 1, we can see that the structure of Chinese characters has different decomposition methods, including the Chinese radical (CR), head and tail (HT) and structural components (SC). Chinese characters have evolved from hieroglyphs since ancient times, and their structure often reflects more information about them. There are some examples in Table 2. The glyph structure can enrich the semantics of Chinese characters and improve the performance of NER. For example, the Bi-LSTM-CRF method (Dong et al., 2016) firstly obtains character-level embedding through the disassembly of Chinese character structure to improve the performance of NER. However, LSTM is based on time series modeling, and the input of each cell depends on the output of the previous cell. So the LSTM-based model is relatively complicated and the parallel ability is limited.\nTo address the aforementioned issues, we take the advantages of Flat-Lattice Transformer (FLAT) (Li et al., 2020) in efficient parallel computing and excellent lexicon learning, and introduce the radical stream as an extension on its basis. By combining the radical information, we propose a Multi-metadata Embedding based CrossTransformer (MECT). MECT has the lattice- and radical-streams, which not only possesses FLAT’s word boundary and semantic learning ability but also increases the structure information of Chinese character radicals. This is very effective for NER tasks, and has improved the baseline method on different benchmarks. The main contributions of the proposed method include:\n• The use of multi-metadata feature embedding of Chinese characters in Chinese NER.\n• A novel two-stream model that combines the radicals, characters and words of Chinese characters to improve the performance of the proposed MECT method.\n• The proposed method is evaluated on several well-known Chinese NER benchmarking\ndatasets, demonstrating the merits and superiority of the proposed approach over the stateof-the-art methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "The key of the proposed MECT method is to use the radical information of Chinese characters to enhance the Chinese NER model. So we focus on the mainstream information enhancement methods in the literature. There are two main types of Chinese NER enhancement methods, including lexical information fusion and glyph-structural information fusion.\nLexical Enhancement In Chinese NER, many recent studies use word matching methods to enhance character-based models. A typical method is the Lattice-LSTM model (Zhang and Yang, 2018) that improves the NER performance by encoding and matching words in the lexicon. Recently, some lexical enhancement methods were proposed using CNN models, such as LR-CNN (Gui et al., 2019a), CAN-NER (Zhu and Wang, 2019). Graph networks have also been used with lexical enhancement. The typical one is LGN (Gui et al., 2019b). Besides, there are Transformer-based lexical enhancement methods, such as PLT (Xue et al., 2019) and FLAT. And SoftLexicon (Ma et al., 2020) introduces lexical information through label and probability methods at the character representation layer.\nGlyph-structural Enhancement Some studies also use the glyph structure information in Chinese NER. For example, Dong et al. (2016) were the first to study the application of radical-level information in Chinese NER. They used Bi-LSTM to extract radical-level embedding and then concatenated it with the embedding of characters as the final input. The radical information used in Bi-LSTM is structural components (SC) as shown in Table 1, which achieved state-of-the-art performance on the MSRA dataset. The Glyce (Meng et al., 2019) model used Chinese character images to extract features such as strokes and structure of Chinese characters, achieving promising perfor-\nmance in Chinese NER. Some other methods (Xu et al., 2019; Song et al., 2020) also proposed to use radical information and Tencent’s pre-trained embedding2 to improve the performance. In these works, the structural components of Chinese characters have been proven to be able to enrich the semantics of the characters, resulting in better NER performance."
    }, {
      "heading" : "3 Background",
      "text" : "The proposed method is based on the Flat-Lattice Transformer (FLAT) model. Thus, we first briefly introduce FLAT that improves the encoder structure of Transformer by adding word lattice information, including semantic and position boundary information. These word lattices are obtained through dictionary matching.\nFigure 1 shows the input and output of FLAT. It uses the relative position encoding transformed by head and tail position to fit the word’s boundary information. The relative position encoding, Rij , is calculated as follows:\nRij = ReLU(Wr(phi−hj ⊕ phi−tj ⊕ pti−hj ⊕ pti−tj )),\n(1)\nwhere Wr is a learnable parameter, hi and ti represent the head position and tail position of the i-th character, ⊕ denotes the concatenation operation, and pspan is obtained as in Vaswani et al. (2017):\np(2k)span = sin( span\n100002k/dmodel ), (2)\np(2k+1)span = cos( span\n100002k/dmodel ), (3)\nwhere pspan corresponds to p in Eq. (1), and span denotes hi − hj , hi − tj , ti − hj and ti − tj . Then the scaled dot-product attention is obtained by:\nAtt(A,V ) = softmax(A)V , (4)\nAij = (Qi + u) >Kj + (Qi + v) >R∗ij , (5)\n[Q,K,V ] = Ex[Wq,Wk,Wv], (6)\n2https://ai.tencent.com/ailab/nlp/en/ embedding.html\nwhere R∗ij = Rij ·WR. u, v and W2 are learnable parameters."
    }, {
      "heading" : "4 The Proposed MECT Method",
      "text" : "To better integrate the information of Chinese character components, we use Chinese character structure as another metadata and design a two-stream form of multi-metadata embedding network. The architecture of the proposed network is shown in Figure 2a. The proposed method is based on the encoder structure of Transformer and the FLAT method, in which we integrate the meaning and boundary information of Chinese words. The proposed two-stream model uses a Cross-Transformer module similar to the self-attention structure to fuse the information of Chinese character components. In our method, we also use the multi-modal collaborative attention method that is widely used in vision-language tasks (Lu et al., 2019). The difference is that we add a randomly initialized attention matrix to calculate the attention bias for the two types of metadata embedding."
    }, {
      "heading" : "4.1 CNN for Radical-level Embedding",
      "text" : "Chinese characters are based on pictographs, and their meanings are expressed in the shape of objects. In this case, the structure of Chinese characters has certain useful information for NER. For example, the radicals such as ‘艹’ (grass) and ‘木’ (wood) generally represent plants, enhancing Chinese medicine entity recognition. For another example, ‘月’ (body) represents human body parts or organs, and ‘疒’ (disease) represents diseases, which benefits Chinese NER for the medical field. Besides, the Chinese have their own culture and belief in naming. Radicals ‘钅’ (metal), ‘木’ (wood), ‘氵’ (water), ‘火’ (fire), and ‘土’ (earth) represented by the Wu-Xing (Five Elements) theory are often used as names of people or companies. But ‘锈’ (rust), ‘杀’ (kill), ‘污’ (dirt), ‘灾’ (disaster) and ‘堕’ (fall) are usually not used as names, even if they contain some elements of the Wu-Xing theory. It is because the other radical components also determine the semantics of Chinese characters. Radicals that generally appear negative or conflict with Chinese cultural beliefs are usually not used for naming.\nTherefore, we choose the more informative Structural Components (SC) in Table 1 as radicallevel features of Chinese characters and use Convolutional Neural Network (CNN) to extract character\nfeatures. The structure diagram of the CNN network is shown in Figure 3. We first disassemble the Chinese characters into SC and then input the radicals into CNN. Last, we use the max-pooling and fully connected layers to get the feature embedding of Chinese characters at the radical-level."
    }, {
      "heading" : "4.2 The Cross-Transformer Module",
      "text" : "After radical feature extraction, we propose a CrossTransformer network to obtain the supplementary semantic information of the structure of Chinese characters. It also uses contextual and lexical information to enrich the semantics of Chinese characters. The Cross-Transformer network is illustrated in Figure 2b. We use two Transformer encoders to cross the lattice and radical information of Chinese characters, which is different from the selfattention method in Transformer.\nThe input QL(QR),KL(KR),VL(VR) are ob-\ntained by the linear transformation of lattice and radical-level feature embedding:QL(R),iKL(R),i\nVL(R),i > = EL(R),i WL(R),QI WL(R),V > , (7) where EL and ER are lattice embedding and radical-level embedding, I is the identity matrix, and each W is a learnable parameter. Then we use the relative position encoding in FLAT to represent the boundary information of a word and calculate the attention score in our Cross-Transformer:\nAttL(AR,VL) = Softmax(AR)VL, (8)\nAttR(AL,VR) = Softmax(AL)VR, (9)\nAL(R),ij = (QL(R),i + uL(R)) >ER(L),j\n+ (QL(R),i + vL(R)) >R∗L(R),ij ,\n(10)\nwhere u and v are learnable parameters for attention bias in Eq. (10), AL is the lattice attention score, and AR denotes the radical attention score. And R∗ij = Rij ·WR. WR are learnable parameters. The relative position encoding, Rij , is calculated as follows:\nRij = ReLU(Wr(phi−hj ⊕ pti−tj )). (11)"
    }, {
      "heading" : "4.3 Random Attention",
      "text" : "We empirically found that the use of random attention in Cross-Transformer can improve the performance of the proposed method. This may be due to the requirement of attention bias in lattice and radical feature embedding, which can better adapt to the scores of two subspaces. Random attention is a randomly initialized parameter matrix\nBmax len×max len that is added to the previous attention score to obtain a total attention score:\nV ∗L = Softmax(AR +B)VL, (12) V ∗R = Softmax(AL +B)VR. (13)"
    }, {
      "heading" : "4.4 The Fusion Method",
      "text" : "To reduce information loss, we directly concatenate the lattice and radical features and input them into a fully connected layer for information fusion:\nFusion(V ∗L ,V ∗ R) = (V ∗ R ⊕ V ∗L )W o + b, (14)\nwhere ⊕ denotes the concatenation operation, W o and b are learnable parameters.\nAfter the fusion step, we mask the word part and pass the fused feature to a Conditional Random Field (CRF) (Lafferty et al., 2001) module."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we evaluate the proposed MECT method on four datasets. To make the experimental results more reasonable, we also set up two additional working methods for assessing the performance of radicals in a two-stream model. We use the span method to calculate F1-score (F1), precision (P), and recall (R) as the evaluation metrics."
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "We use four mainstream Chinese NER benchmarking datasets: Weibo (Peng and Dredze, 2015; He and Sun, 2016), Resume (Zhang and Yang, 2018), MSRA (Levow, 2006), and Ontonotes 4.0 (Weischedel and Consortium, 2013). The corpus of MSRA and Ontonotes 4.0 comes from news, the corpus of Weibo comes from social media, and the corpus of Resume comes from the resume data in Sina Finance. Table 3 shows the statistical information of these datasets. Among them, the Weibo dataset has four types of entities, including PER, ORG, LOC, and GPE. Resume has eight types of entities, including CONT, EDU, LOC, PER, ORG, PRO, RACE, and TITLE. OntoNotes 4.0 has four types of entities: PER, ORG, LOC, and GPE. The MSRA dataset contains three types of entities, i.e., ORG, PER, and LOC.\nWe use the state of the art method, FLAT, as the baseline model. FLAT is a Chinese NER model based on Transformer and combined with lattice. Besides, we also compared the proposed method with both classic and innovative Chinese NER models. We use the more informative ‘SC’ as the radical feature, which comes from the online Xinhua\nDictionary3. The pre-trained embedding of characters and words are the same as FLAT.\nFor hyper-parameters, we used 30 1-D convolution kernels with the size of 3 for CNN. We used the SMAC (Hutter et al., 2011) algorithm to search for the optimal hyper-parameters. Besides, we set a different learning rate for the training of the radicallevel embedding with CNN. Readers can refer to the appendix for our hyper-parameter settings."
    }, {
      "heading" : "5.2 Comparison with SOTA Methods",
      "text" : "In this section, we evaluate and analyze the proposed MECT method with a comparison to both the classic and state of the art methods. The experimental results are reported in Tables 4−74. Each table is divided into four blocks. The first block includes classical Chinese NER methods. The second one reports the results obtained by state of the art approaches published recently. The third and\n3http://tool.httpcn.com/Zi/. 4In Tables 4−7, ‘∗’ denotes the use of external labeled data for semi-supervised learning and ‘†’ denotes the use of discrete features.\nfourth ones are the results obtained by the proposed MECT method as well as the baseline models.\nWeibo: Table 4 shows the results obtained on Weibo in terms of the F1 scores of named entities (NE), nominal entities (NM), and both (Overall). From the results, we can observe that MECT achieves the state-of-the-art performance. Compared with the baseline method, MECT improves 2.98% in terms of the F1 metric. For the NE metric, the proposed method achieves 61.91%, beating all the other approaches.\nResume: The results obtained on the Resume dataset are reported in Table 5. The first block shows Zhang and Yang (2018) comparative results on the character-level and word-level models. We can observe that the performance of incorporating word features into the character-level model is better than other models. Additionally, MECT combines lexical and radical features, and the F1 score is higher than the other models and the baseline method.\nOntonotes 4.0: Table 6 shows the results obtained on Ontonotes 4.0. The symbol ‘§’ indicates gold segmentation, and the symbol ‘¶’ denotes automated segmentation. Other models have no segmentation and use lexical matching. Compared to the baseline method, the F1 score of MECT is increased by 0.47%. MECT also achieves a high recall rate, keeping the precision rate and recall rate relatively stable.\nMSRA: Table 7 shows the experimental results obtained on MSRA. In the first block, the result proposed by Dong et al. (2016) is the first method using radical information in Chinese NER. From the table, we can observe that the overall performance of MECT is higher than the existing SOTA methods. Similarly, our recall rate achieves a higher performance so that the final F1 has a certain performance boosting.\nWith BERT: Besides the single-model evaluation on the four datasets, we also evaluated the proposed method when combining with the SOTA method, BERT. The BERT model is the same as FLAT using the ‘BERT-wwm’ released by Cui et al. (2020). The results are shown in the fourth block of each table. The results of BERT are taken from the FLAT paper. We can find that MECT further improves the performance of BERT significantly."
    }, {
      "heading" : "5.3 Effectiveness of Cross-Transformer",
      "text" : "There are two sub-modules in the proposed CrossTransformer method: lattice and radical attentions. Figure 4 includes two heatmaps for the normalization of the attention scores of the two modules. From the two figures, we can see that lattice attention pays more attention to the relationship between words and characters so that the model can obtain the position information and boundary information of words. Radical attention focuses on global information and corrects the semantic information of\neach character through radical features. Therefore, lattice and radical attentions provide complementary information for the performance-boosting of the proposed MECT method in Chinese NER."
    }, {
      "heading" : "5.4 Impact of Radicals",
      "text" : "We visualized the radical-level embedding obtained by the CNN network and found that the cosine distance of Chinese characters with the same radical or similar structure is smaller. For example, Figure 5 shows part of the Chinese character embedding trained on the Resume dataset. The highlighted dots represent Chinese characters that are close to the character ‘华’. We can see that they have the same radicals or similar structure. It can enhance the semantic information of Chinese characters to a certain extent.\nWe also examined the inference results of MECT and FLAT on Ontonotes 4.0 and found many exciting results. For example, some words with a\npercentage like ‘百分之四十三点二 (43.2%)’ is incorrectly labelled as PER in the training dataset, which causes FLAT to mark the percentage of words with PER on the test dataset, while MECT avoids this situation. There are also some words such as ‘田时’ and ‘以国’ that appear in the lexicon, which was mistakenly identified as valid words by FLAT, leading to recognition errors. Our MECT addresses these issues by paying global attention to the radical information. Besides, in FLAT, some numbers and letters are incorrectly marked as PER, ORG, or others. We compared the PER label accuracy of FLAT and MECT on the test dataset. FLAT achieves 81.6%, and MECT reaches 86.96%, which is a very significant improvement."
    }, {
      "heading" : "5.5 Analysis in Efficiency and Model Size",
      "text" : "We use the same FLAT method to evaluate the parallel and non-parallel inference speed of MECT on an NVIDIA GeForce RTX 2080Ti card, using batch size = 16 and batch size = 1. We use the non-parallel version of FLAT as the standard and calculate the other models’ relative inference speed. The results are shown in Figure 6. According to the figure, even if MECT adds a Transformer encoder to FLAT, the speed is only reduced by 0.15 in terms of the parallel inference speed. Our model’s speed is considerable relative to LSTM, CNN, and some graph-based network models. Because Transformer can make full use of the GPU’s parallel computing power, the speed of MECT does not drop too much, but it is still faster than other models. The model’s parameter is between 2 and 4 million, determined by the max sentence length in the dataset and the dmodel size in the model."
    }, {
      "heading" : "5.6 Ablation Study",
      "text" : "To validate the effectiveness of the main components of the proposed method, we set up two experiments in Figure 7. In Experiment A, we only use a\nsingle-stream model with a modified self-attention, which is similar to the original FLAT model. The difference is that we use a randomly initialized attention matrix (Random Attention) for the attention calculation. We combine lattice embedding and radical-level embedding as the input of the model. The purpose is to verify the performance of the twostream model relative to the single-stream model. In Experiment B, we do not exchange the query’s feature vector. We replace the cross-attention with two sets of modified self-attention and follow the two modules’ output with the same fusion method as MECT. The purpose of experiment B is to verify the effectiveness of MECT relative to the twostream model without crossover. Besides, we evaluate the proposed MECT method by removing the random attention module.\nTable 8 shows the ablation study results. 1) By comparing the results of Experiment A with the results of Experiment B and MECT, we can find that the two-stream model works better. The use of lattice-level and radical-level features as the two streams of the model helps the model to better understand and extract the semantic features of Chinese characters. 2) Based on the results of Experiment B and MECT, we can see that by exchanging the two query feature vectors, the model can extract features more effectively at the lattice and radical levels. They have different attention mechanisms to obtain contextual information, resulting in global and local attention interaction. This provides better information extraction capabilities for the proposed method in a complementary way. 3) Last, the performance of MECT drops on all the datasets by\nremoving the random attention module (the last row). This indicates that, as an attention bias, random attention can eliminate the differences caused by different embeddings, thereby improving the model’s performance further."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presented a novel two-stream network, namely MECT, for Chinese NER. The proposed method uses multi-metadata embedding that fuses the information of radicals, characters and words through a Cross-Transformer network. Additionally, random attention was used for further performance boost. Experimental results obtained on four benchmarks demonstrate that the radical information of Chinese characters can effectively improve the performance for Chinese NER.\nThe proposed MECT method with the radical stream increases the complexity of a model. In the future, we will consider how to integrate the characters, words and radical information of Chinese characters with a more efficient way in two-stream or multi-stream networks to improve the performance of Chinese NER and extend it to other NLP tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Key Research and Development Program of China\n(2017YFC1601800), the National Natural Science Foundation of China (61876072, 61902153) and the Six Talent Peaks Project of Jiangsu Province (XYDXX-012). We also thank Xiaotong Xiang and Jun Quan for their help on editing the manuscript."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Range of Hyper-parameters We manually selected parameters on the two largescale datasets, including Ontonotes 4.0 and MSRA. For the two small datasets, Weibo and Resume, we used the SMAC algorithm to search for the best hyper-parameters. The range of parameters is listed in Table 9."
    } ],
    "references" : [ {
      "title" : "Adversarial transfer learning for Chinese named entity recognition with selfattention mechanism",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition with bilingual constraints",
      "author" : [ "Wanxiang Che", "Mengqiu Wang", "Christopher D. Manning", "Ting Liu." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Che et al\\.,? 2013",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2013
    }, {
      "title" : "Chinese named entity recognition with conditional probabilistic models",
      "author" : [ "Aitao Chen", "Fuchun Peng", "Roy Shan", "Gordon Sun." ],
      "venue" : "SIGHAN Workshop on Chinese Language Processing.",
      "citeRegEx" : "Chen et al\\.,? 2006",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2006
    }, {
      "title" : "Event extraction via dynamic multi-pooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "ACL—IJCNLP, volume 1, pages 167–176.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657–668,",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Core techniques of question answering systems over knowledge bases: a survey",
      "author" : [ "Dennis Diefenbach", "Vanessa Lopez", "Kamal Singh", "Pierre Maret." ],
      "venue" : "KAIS, 55(3):529–569.",
      "citeRegEx" : "Diefenbach et al\\.,? 2018",
      "shortCiteRegEx" : "Diefenbach et al\\.",
      "year" : 2018
    }, {
      "title" : "Characterbased lstm-crf with radical-level features for chinese named entity recognition",
      "author" : [ "Chuanhai Dong", "Jiajun Zhang", "Chengqing Zong", "Masanori Hattori", "Hui Di." ],
      "venue" : "Natural Language Understanding and Intelligent Applications, pages 239–",
      "citeRegEx" : "Dong et al\\.,? 2016",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2016
    }, {
      "title" : "A study on features of the crfs-based chinese named entity recognition",
      "author" : [ "Huanzhong Duan", "Yan Zheng." ],
      "venue" : "International Journal of Advanced Intelligence, 3(2):287–294.",
      "citeRegEx" : "Duan and Zheng.,? 2011",
      "shortCiteRegEx" : "Duan and Zheng.",
      "year" : 2011
    }, {
      "title" : "Cnn-based chinese ner with lexicon rethinking",
      "author" : [ "Tao Gui", "Ruotian Ma", "Qi Zhang", "Lujun Zhao", "Yu-Gang Jiang", "Xuanjing Huang." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4982–4988.",
      "citeRegEx" : "Gui et al\\.,? 2019a",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "A lexicon-based graph neural network for chinese ner",
      "author" : [ "Tao Gui", "Yicheng Zou", "Qi Zhang", "Minlong Peng", "Jinlan Fu", "Zhongyu Wei", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Gui et al\\.,? 2019b",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "F-score driven max margin neural network for named entity recognition in chinese social media",
      "author" : [ "Hangfeng He", "Xu Sun." ],
      "venue" : "CoRR, abs/1611.04234.",
      "citeRegEx" : "He and Sun.,? 2016",
      "shortCiteRegEx" : "He and Sun.",
      "year" : 2016
    }, {
      "title" : "F-score driven max margin neural network for named entity recognition in Chinese social media",
      "author" : [ "Hangfeng He", "Xu Sun." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short",
      "citeRegEx" : "He and Sun.,? 2017a",
      "shortCiteRegEx" : "He and Sun.",
      "year" : 2017
    }, {
      "title" : "A unified model for cross-domain and semi-supervised named entity recognition in chinese social media",
      "author" : [ "Hangfeng He", "Xu Sun." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page 3216–3222. AAAI Press.",
      "citeRegEx" : "He and Sun.,? 2017b",
      "shortCiteRegEx" : "He and Sun.",
      "year" : 2017
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration",
      "author" : [ "Frank Hutter", "Holger H Hoos", "Kevin LeytonBrown." ],
      "venue" : "International conference on learning and intelligent optimization, pages 507–523. Springer.",
      "citeRegEx" : "Hutter et al\\.,? 2011",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2011
    }, {
      "title" : "The impact of named entity normalization on information retrieval for question answering",
      "author" : [ "Mahboob Alam Khalid", "Valentin Jijkoun", "Maarten de Rijke." ],
      "venue" : "Advances in Information Retrieval, pages 705–710, Berlin, Heidelberg. Springer Berlin Hei-",
      "citeRegEx" : "Khalid et al\\.,? 2008",
      "shortCiteRegEx" : "Khalid et al\\.",
      "year" : 2008
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition",
      "author" : [ "Gina-Anne Levow." ],
      "venue" : "Proceedings of the Fifth SIGHAN Workshop on Chinese",
      "citeRegEx" : "Levow.,? 2006",
      "shortCiteRegEx" : "Levow.",
      "year" : 2006
    }, {
      "title" : "FLAT: Chinese NER using flat-lattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6836–6842, Online. Association for",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 13–23. Curran Asso-",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multiprototype chinese character embedding",
      "author" : [ "Yanan Lu", "Yue Zhang", "Dong-Hong Ji." ],
      "venue" : "LREC.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Simplify the usage of lexicon in Chinese NER",
      "author" : [ "Ruotian Ma", "Minlong Peng", "Qi Zhang", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5951–5960, Online. As-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Glyce: Glyph-vectors for chinese character representations",
      "author" : [ "Yuxian Meng", "Wei Wu", "Fei Wang", "Xiaoya Li", "Ping Nie", "Fan Yin", "Muyu Li", "Qinghong Han", "Xiaofei Sun", "Jiwei Li." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages",
      "citeRegEx" : "Meng et al\\.,? 2019",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2019
    }, {
      "title" : "Named entity recognition for Chinese social media with jointly trained embeddings",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal.",
      "citeRegEx" : "Peng and Dredze.,? 2015",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2015
    }, {
      "title" : "Improving named entity recognition for Chinese social media with word segmentation representation learning",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Peng and Dredze.,? 2016",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2016
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M Marlin." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Riedel et al\\.,? 2013",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint self-attention and multi-embeddings for chinese named entity recognition",
      "author" : [ "C. Song", "Y. Xiong", "W. Huang", "L. Ma." ],
      "venue" : "2020 6th International Conference on Big Data Computing and Communications (BIGCOM), pages 76–80.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Effective bilingual constraints for semi-supervised learning of named entity recognizers",
      "author" : [ "Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI’13,",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting multiple embeddings for chinese named entity recognition",
      "author" : [ "Canwen Xu", "Feiyang Wang", "Jialong Han", "Chenliang Li." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM ’19, page",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Porous lattice-based transformer encoder for chinese ner",
      "author" : [ "Mengge Xue", "Bowen Yu", "Tingwen Liu", "Bin Wang", "Erli Meng", "Quangang Li." ],
      "venue" : "arXiv preprint arXiv:1911.02733.",
      "citeRegEx" : "Xue et al\\.,? 2019",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep affix features improve neural named entity recognizers",
      "author" : [ "Vikas Yadav", "Rebecca Sharp", "Steven Bethard." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 167–172, New Orleans, Louisiana. Associa-",
      "citeRegEx" : "Yadav et al\\.,? 2018",
      "shortCiteRegEx" : "Yadav et al\\.",
      "year" : 2018
    }, {
      "title" : "Combining discrete and neural features for sequence labeling",
      "author" : [ "Jie Yang", "Zhiyang Teng", "Meishan Zhang", "Yue Zhang." ],
      "venue" : "Computational Linguistics and Intelligent Text Processing, pages 140– 154, Cham. Springer International Publishing.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Word segmentation and named entity recognition for sighan bakeoff3",
      "author" : [ "Suxiang Zhang", "Ying Qin", "Juan Wen", "Xiaojie Wang." ],
      "venue" : "SIGHAN Workshop on Chinese Language Processing, pages 158– 161.",
      "citeRegEx" : "Zhang et al\\.,? 2006",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554– 1564, Melbourne, Australia. Association for Compu-",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    }, {
      "title" : "Chinese named entity recognition via joint identification and categorization",
      "author" : [ "Junsheng Zhou", "Weiguang Qu", "Fen Zhang." ],
      "venue" : "Chinese journal of electronics, 22(2):225–230.",
      "citeRegEx" : "Zhou et al\\.,? 2013",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2013
    }, {
      "title" : "CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition",
      "author" : [ "Yuying Zhu", "Guoxin Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Zhu and Wang.,? 2019",
      "shortCiteRegEx" : "Zhu and Wang.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "NER is the basis for many Natural Language Processing (NLP) tasks such as event extraction (Chen et al., 2015), question answering (Diefenbach et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : ", 2015), question answering (Diefenbach et al., 2018), information re-",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "trieval (Khalid et al., 2008), knowledge graph construction (Riedel et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 26,
      "context" : ", 2008), knowledge graph construction (Riedel et al., 2013), etc.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Chinese word segmentation is mostly distinguished by readers through the semantic information of sentences, posing many difficulties to Chinese NER (Duan and Zheng, 2011; Ma et al., 2020).",
      "startOffset" : 148,
      "endOffset" : 187
    }, {
      "referenceID" : 21,
      "context" : "Chinese word segmentation is mostly distinguished by readers through the semantic information of sentences, posing many difficulties to Chinese NER (Duan and Zheng, 2011; Ma et al., 2020).",
      "startOffset" : 148,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "Besides, the task also has many other challenges, such as complex combinations, entity nesting, and indefinite length (Dong et al., 2016).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 32,
      "context" : "Besides, according to the information of English words, root or affixes often determine general meanings (Yadav et al., 2018).",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Therefore, even the stateof-the-art methods, such as BERT (Devlin et al., 2019) and GPT (Radford et al.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : ", 2019) and GPT (Radford et al., 2018), trained on large-scale datasets, adopt this delicate word segmentation method for performance boost.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "For example, the Bi-LSTM-CRF method (Dong et al., 2016) firstly obtains character-level embedding through the disassembly of Chinese character structure to improve the performance of NER.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "To address the aforementioned issues, we take the advantages of Flat-Lattice Transformer (FLAT) (Li et al., 2020) in efficient parallel computing and excellent lexicon learning, and introduce the radical stream as an extension on its basis.",
      "startOffset" : 96,
      "endOffset" : 113
    }, {
      "referenceID" : 35,
      "context" : "A typical method is the Lattice-LSTM model (Zhang and Yang, 2018) that improves the NER performance by encoding and matching words in the lexicon.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "Recently, some lexical enhancement methods were proposed using CNN models, such as LR-CNN (Gui et al., 2019a), CAN-NER (Zhu and Wang, 2019).",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : "Besides, there are Transformer-based lexical enhancement methods, such as PLT (Xue et al., 2019) and FLAT.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "And SoftLexicon (Ma et al., 2020) introduces lexical information through label and probability methods at the character representation layer.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "The Glyce (Meng et al., 2019) model used Chinese character images to extract features such as strokes and structure of Chinese characters, achieving promising perfor-",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 30,
      "context" : "Some other methods (Xu et al., 2019; Song et al., 2020) also proposed to use radical information and Tencent’s pre-trained embedding2 to improve the performance.",
      "startOffset" : 19,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "Some other methods (Xu et al., 2019; Song et al., 2020) also proposed to use radical information and Tencent’s pre-trained embedding2 to improve the performance.",
      "startOffset" : 19,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "In our method, we also use the multi-modal collaborative attention method that is widely used in vision-language tasks (Lu et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "After the fusion step, we mask the word part and pass the fused feature to a Conditional Random Field (CRF) (Lafferty et al., 2001) module.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "We use four mainstream Chinese NER benchmarking datasets: Weibo (Peng and Dredze, 2015; He and Sun, 2016), Resume (Zhang and Yang, 2018), MSRA (Levow, 2006), and Ontonotes 4.",
      "startOffset" : 64,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "We use four mainstream Chinese NER benchmarking datasets: Weibo (Peng and Dredze, 2015; He and Sun, 2016), Resume (Zhang and Yang, 2018), MSRA (Levow, 2006), and Ontonotes 4.",
      "startOffset" : 64,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "We use four mainstream Chinese NER benchmarking datasets: Weibo (Peng and Dredze, 2015; He and Sun, 2016), Resume (Zhang and Yang, 2018), MSRA (Levow, 2006), and Ontonotes 4.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "We use four mainstream Chinese NER benchmarking datasets: Weibo (Peng and Dredze, 2015; He and Sun, 2016), Resume (Zhang and Yang, 2018), MSRA (Levow, 2006), and Ontonotes 4.",
      "startOffset" : 143,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "We used the SMAC (Hutter et al., 2011) algorithm to search for the optimal hyper-parameters.",
      "startOffset" : 17,
      "endOffset" : 38
    } ],
    "year" : 2021,
    "abstractText" : "Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based CrossTransformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method.1",
    "creator" : "LaTeX with hyperref"
  }
}