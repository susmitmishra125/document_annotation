{
  "name" : "2021.acl-long.354.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ONE2SET: Generating Diverse Keyphrases as a Set",
    "authors" : [ "Jiacheng Ye", "Tao Gui", "Yichao Luo", "Yige Xu", "Qi Zhang" ],
    "emails" : [ "qz}@fudan.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4598–4608\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4598"
    }, {
      "heading" : "1 Introduction",
      "text" : "Keyphrase generation (KG) aims to generate of a set of keyphrases that expresses the high-level semantic meaning of a document. These keyphrases can be further categorized into present keyphrases that appear in the document and absent keyphrases that do not. Meng et al. (2017) proposed a sequence-to-sequence (Seq2Seq) model with a copy mechanism (Gu et al., 2016) to predict both present and absent keyphrases. However, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases. To\n∗ Corresponding authors.\naddress this, Yuan et al. (2020) proposed the ONE2SEQ training paradigm where each source text corresponds to a sequence of keyphrases that are concatenated with a delimiter 〈sep〉 and a terminator 〈eos〉. As keyphrases must be ordered before being concatenated, Yuan et al. (2020) sorted the present keyphrases by their order of the first occurrence in the source text and appended the absent keyphrases to the end. During inference, the decoding process terminates when generating 〈eos〉, and the final keyphrase predictions are obtained after splitting the sequence by 〈sep〉. Thus, a model trained with ONE2SEQ paradigm can generate a sequence of multiple keyphrases with dynamic numbers as well as considering the dependency between keyphrases.\nHowever, as the keyphrases are inherently an unordered set rather than an ordered sequence, imposing a predefined order usually leads to the following intractable problems. First, the predefined order will give wrong bias during training, which\ncan highly penalize shifts in the order between keyphrases. As shown in Figure 1 (a), the model makes correct predictions in each keyphrase but can still receive a large loss during training. Second, this increases the difficulty of model training. For example, the absent keyphrases are appended to the end in an author-defined order in Yuan et al. (2020), however, different authors can have various sorting bases, which makes it difficult for the model to learn a unified pattern. Third, the model is highly sensitive to the predefined order, as shown in Meng et al. (2019), and can suffer from error propagation during inference when previously having generated keyphrases with an incorrect order. Lately, Chan et al. (2019) proposed a reinforcement learningbased fine-tuning method, which fine-tunes the pre-trained models with metric-based rewards (i.e., recall and F1) for generating more sufficient and accurate keyphrases. However, this method can alleviate the impact of the order problems when fine-tuning but needs to be pre-trained under the ONE2SEQ paradigm to initialize the model, which can still introduce wrong biases.\nTo address this problem, we propose a new training paradigm ONE2SET where the ground-truth target is a set rather than a keyphrase-concatenated sequence. However, the vanilla Seq2Seq model can generate a sequence but not a set. Hence, we introduce a set prediction model that adopts Transformer (Vaswani et al., 2017) as the main architecture together with a fixed set of learned control codes as additional decoder inputs to perform controllable generation. For each code, the model generates a corresponding keyphrase for the source document or a special ∅ token that represents the meaning of “no corresponding keyphrase”. During training, the cross-entropy loss cannot be directly used since we do not know the correspondence between each prediction and target. Hence, we introduce a K-step target assignment mechanism, where we first auto-regressively generate K words for each code and then assign targets via bipartite matching based on the predicted words. After that, we can train each code using teacher forcing as before. Compared with the previous models, the proposed method has the following advantages: (a) there is no need to predefine an order to concatenate the keyphrases, thus the model will not be affected by the wrong biases in the whole training stage; and (b) the bipartite matching forces unique predictions for each code, which greatly reduces the duplication\nratio and increases the diversity of predictions. We summarize our main contributions as follows: (1) we propose a new training paradigm ONE2SET without predefining an order to concatenate the keyphrases; (2) we propose a novel set prediction model that can generate a set of diverse keyphrases in parallel and a dynamic target assignment mechanism to solve the intractable training problem under the ONE2SET paradigm; (3) our method consistently outperforms all the state-of-the-art methods and greatly reduces the duplication ratio. Our codes are publicly available at Github1."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Keyphrase Extraction",
      "text" : "Existing approaches for keyphrase prediction can be broadly divided into extraction and generation methods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017)."
    }, {
      "heading" : "2.2 Keyphrase Generation",
      "text" : "Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016). Many works are proposed based on the CopyRNN architecture (Chen et al., 2018; Zhao and Zhang, 2019; Chen et al., 2019b,a).\nIn previous CopyRNN based works, each source text corresponds to a single target keyphrase. Thus, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases and consider the inter-relation among keyphrases.\n1https://github.com/jiacheng-ye/kg_ one2set\nTo this end, Yuan et al. (2020) proposed an ONE2SEQ training paradigm where each source text corresponds to a sequence of concatenated keyphrases. Thus, the model can capture the contextual information between the keyphrases as well as determines the dynamic number of keyphrases for different source texts. The recent works (Chan et al., 2019; Chen et al., 2020; Swaminathan et al., 2020) mostly follow the ONE2SEQ training paradigm. Chan et al. (2019) proposed an RL-based fine-tuning method using F1 and Recall metrics as rewards. Swaminathan et al. (2020) proposed an RL-based fine-tuning method using a discriminator to produce rewards. All the above models need to be trained or pre-trained under the ONE2SEQ paradigm. As keyphrases must be ordered before concatenating and keyphrases are inherently an unordered set, the model can be trained with wrong signal. Our ONE2SET training paradigm aims to solve this problem."
    }, {
      "heading" : "3 Methodology",
      "text" : "This paper proposes a new training paradigm ONE2SET for keyphrase generation. A set prediction model based on Transformer (SETTRANS) is proposed to fit this paradigm, as shown in Figure 2. Given a fixed set of learned control codes as input conditions, the model generates a keyphrase or a special ∅ token for each code in parallel. During training, a K-step target assignment mechanism is proposed to dynamically determine the target corresponding to each code. The main idea is that the model first freely predicts K steps without any supervision to see what keyphrase each code can roughly generate, and then use bipartite matching to find the optimal allocation based on the model’s\nconjecture and target. Given the correspondence of each code and target, a separate set loss is then used to correct the model’s conjecture, where half of the codes are trained to predict the present keyphrase set and the others are trained to predict the absent keyphrase set."
    }, {
      "heading" : "3.1 The ONE2SET Training Paradigm",
      "text" : "We first formally describe the keyphrase generation task as follows. Given a document x, it’s aimed to predict a set of keyphrases Y = {yi}i=1,...,|Y|, where |Y| is the number of keyphrases. To solve the KG task, previous works typically adopted an ONE2ONE training paradigm (Meng et al., 2017) or ONE2SEQ training paradigm (Yuan et al., 2020). The difference between the two training paradigms is that the form of training samples is different. Specifically, in the ONE2ONE training paradigm, each original sample pair (x,Y) is divided into multiple pairs {(x,yi)}i=1,...,|Y| to perform training independently. In the ONE2SEQ training paradigm, each original sample pair is processed as (x, f(Y)), where f(Y) is a sequence of keyphrases after the reordering and concatenating operation.\nTo solve the wrong bias problem caused by the ONE2SEQ training paradigm, we propose the ONE2SET training paradigm, where each original sample pair is kept still as (x,Y). Hence, the sample used in training is consistent with the original sample, which avoids the intractable problem introduced by the additional processing (i.e., dividing or concatenating)."
    }, {
      "heading" : "3.2 The SETTRANS Model",
      "text" : "We adopt the Transformer (Vaswani et al., 2017) as the backbone encoder-decoder framework. How-\never, the vanilla Transformer can only generate a sequence but not a set. To predict a set of keyphrases, we propose SETTRANS model that utilizes a set of learned control codes as additional decoder inputs. By performing generation conditioned on each control code, we can generate a set of keyphrases in parallel. To decide suitable numbers of keyphrases for different given documents, we fix the total length of the control codes to a sufficient number N , and introduce a special ∅ token that represents the meaning of “no corresponding keyphrase”. Hence, we can determine the appropriate number of keyphrases for an input document after removing all the ∅ tokens from the N predictions.\nFormally, the decoder input at time step t for control code n is defined as follows:\ndnt = e w ynt−1 + ept + c n, (1)\nwhere ewynt−1 is the embedding of word y n t−1, e p t is the t-th sinusoid positional embedding as in (Vaswani et al., 2017) and cn is the n-th learned control code embedding. The decoder outputs the predictive distribution pnt , which is used to get the next word ynt . As some keyphrases contain words that do not exist in the predefined vocabulary but appear in the input document, we also employ a copy mechanism (See et al., 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020)."
    }, {
      "heading" : "3.3 Training",
      "text" : "The main difficulty of training under the ONE2SET paradigm is that the correspondence between each prediction and ground-truth keyphrase is unknown, so that the cross-entropy loss cannot be directly used. Hence, we introduce a K-step target assignment mechanism to assign the ground-truth keyphrase for each prediction, and a separate set loss to train the model in an end-to-end way.\n3.3.1 K-step Target Assignment We first generate K words for each control code and collect the corresponding predictive probability distributions of each step. Formally, we denote P = {Pn}n=1,...,N , where Pn = {pnt }t=1,...,K and pnt is the predictive distribution at time step t for control code n.\nThen, we find a bipartite matching between the ground-truth keyphrases and predictions. Assuming the predefined number of control codes N is\nlarger than the number of ground-truth keyphrases, we consider the ground-truth keyphrases also as a set of size N padded with ∅. Note that the bipartite matching enforces permutation-invariance, and guarantees that each target element has a unique match. Thus, it reduces the duplication ratio of predictions. Specifically, as shown in Figure 2, both the fifth and eighth control code predict the same keyphrase “neural model”, but one of them is assigned with ∅. The eighth code can perceive that this keyphrase has been generated by another code. Hence, the control codes can learn their mutual dependency during training and not generate duplicated keyphrases.\nFormally, to find a bipartite matching between sets of ground-truth keyphrases and predictions, we search for a permutation π̂ with the lowest cost:\nπ̂ = arg min π∈Π(N) N∑ n=1 Cmatch ( yn,Pπ(n) ) , (2)\nwhere Π(N) is the space of all N -length permutations, Cmatch ( yn,Pπ(n) ) is a pair-wise matching cost between the ground truth yn and distributions of a prediction sequence with index π(n). This optimal assignment is computed efficiently with the Hungarian algorithm (Kuhn, 1955). The matching cost takes into account the class predictions, which can be defined as follows: Cmatch ( yn,Pπ(n) ) = −\ns∑ t=1 1{ynt 6=∅}p π(n) t (y n t ) ,\n(3) where s = min(|yn|,K) is the minimum shared length between the target and predicted sequence, p π(n) t (y n t ) denotes the probability of word y n t in p π(n) t , and we ignore the score from matching predictions with ∅, which ensures that valid targets (i.e., non-∅ targets) can be allocated to predictions with as higher predictive probability as possible."
    }, {
      "heading" : "3.3.2 Separate Set Loss",
      "text" : "Given the correspondence between each code and target, we can train the model to predict a single target set, which is defined as follows:\nL(θ) = − N∑ n=1 |yn|∑ t=1 logp π̂(n) t (y n t ) , (4)\nwhere pπ̂(n)t is the predictive probability distribution using teacher forcing. However, predicting present and absent keyphrases requires the model\nto have different capabilities, we propose a separate set loss to flexibly take this bias into account in a unified model. Specifically, we first separate the control codes into two fixed sets with equal size of N/2, which is denoted as C1 and C2, and the target keyphrase set Y into present target keyphrase set Ypre and absent target keyphrase set Yabs. Finally, the bipartite matching is performed on the two sets separately, namely, we find a permutation π̂pre using Ypre and predictions from C1, and π̂abs using Yabs and predictions from C2. Thus, we can modify the final loss in Equal 4 as follows:\nL(θ) = −( N/2∑ n=1 |yn|∑ t=1 logp π̂pre(n) t (y n t )\n+ N∑ n=N/2+1 |yn|∑ t=1 logp π̂abs(n) t (y n t )).\n(5)\nIn practice, we down-weight the log-probability term when ynt = ∅ by scale factors λpre and λabs for present keyphrase set and absent keyphrase set to account for the class imbalance."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct our experiments on five scientific article datasets, including Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin et al., 2009), SemEval (Kim et al., 2010) and KP20k (Meng et al., 2017). Each sample from these datasets consists of a title, an abstract, and some keyphrases. Following previous works (Meng et al., 2017; Chen et al., 2019b,a; Yuan et al., 2020), we concatenate the title and abstract as a source document. We use the largest dataset (i.e., KP20k) to train all the models. After preprocessing (i.e., lowercasing, replacing all the digits with the symbol 〈digit〉 and removing the duplicated data), the final KP20k dataset contains 509,818 samples for training, 20,000 for validation, and 20,000 for testing. The dataset statistics are shown in Table 1."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We focus on the comparisons with the following state-of-the-art methods as our baselines: • catSeq (Yuan et al., 2020). The RNN-based\nseq2seq model with copy mechanism trained under ONE2SEQ paradigm. • catSeqTG (Chen et al., 2019b). An extension\nof catSeq with additional title encoding and cross-attention.\n• catSeqTG-2RF1 (Chan et al., 2019). An extension of catSeqTG with RL-based finetuning using F1 and Recall metrics as rewards. • GANMR (Swaminathan et al., 2020). An\nextension of catSeq with RL-based fine-tuning using a discriminator to produce rewards. • ExHiRD-h (Chen et al., 2020). An extension\nof catSeq with a hierarchical decoding method and an exclusion mechanism to avoid generating duplicated keyphrases. In this paper, we propose two Transformer-based models that are denoted as follows: • Transformer. A Transformer-based\nmodel with copy mechanism trained under ONE2SEQ paradigm. • SETTRANS. An extension of Transformer\nwith additional control codes trained under ONE2SET paradigm."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "Following previous works (Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020), when training under the ONE2SEQ paradigm, the target keyphrase sequence is the concatenation of present and absent keyphrases, with the present keyphrases are sorted according to the orders of their first occurrences in the document and the absent keyphrase kept in their original order. We use a Transformer structure similar to Vaswani et al. (2017), with six layers and eight self-attention heads, 2048 dimensions for hidden states. In the training stage, we choose the top 50,002 frequent words to form the predefined vocabulary and set the embedding dimension to 512. We use the Adam optimization algorithm (Kingma and Ba, 2015) with a learning rate of 0.0001, and a batch size of 12. During testing, we use greedy search as the decoding algorithm. We set the number of control codes to 20 as we find it covers 99.5% of the samples in the validation set. We use a number of two for target assignment steps K based on the average keyphrase length on the validation set, a factor of 0.2 and 0.1 for λpre\nand λabs respectively based on the validation set. We conduct the experiments on a GeForce RTX 2080Ti GPU, repeat three times using different random seeds, and report the averaged results."
    }, {
      "heading" : "4.4 Evaluation Metrics",
      "text" : "We follow previous works (Chan et al., 2019; Chen et al., 2020) and use macro-averaged F1@5 and F1@M for both present and absent keyphrase predictions. F1@M compares all the keyphrases predicted by the model with the ground-truth keyphrases, which means it considers the number of predictions. For F1@5, when the prediction number is less than five, we randomly append incorrect keyphrases until it obtains five predictions. If we do not adopt such an appending operation, F1@5 will become the same with F1@M when the prediction number is less than five as shown in Chan et al. (2019). We apply the Porter Stemmer before determining whether two keyphrases are identical and remove all the duplicated keyphrases after stemming."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Present and Absent Keyphrase Predictions",
      "text" : "Table 2 and Table 3 show the performance evaluations of the present and absent keyphrase, respectively. We observe that the proposed SETTRANS model consistently outperforms almost all the\nprevious state-of-the-art models on both F1@5 and F1@M metrics by a large margin, which demonstrates the effectiveness of our methods. As noted by previous works (Chan et al., 2019; Yuan et al., 2020) that predicting absent keyphrases for a document is an extremely challenging task, thus the performance is much lower than that of present keyphrase prediction. Regarding the comparison of our Transformer model trained under ONE2SEQ paradigm and SETTRANS model trained under ONE2SET paradigm, we find SETTRANS model consistently improves both keyphrase extractive and generative ability by a large margin on almost all the datasets, and maintains the performance of present keyphrase prediction on the Inspec and Krapivin datasets, which demonstrates the advantages of ONE2SET training paradigm."
    }, {
      "heading" : "5.2 Diversity of Predicted Keyphrases",
      "text" : "To investigate the model’s ability to generate diverse keyphrases, we measure the average numbers of unique present and absent keyphrases, and the average duplication ratio of all the predicted keyphrases. The results are reported in Table 4. Based on the results, we observe that our SETTRANS model generates more unique keyphrases than other baselines by a large margin, as well as achieves a significantly lower duplication ratio. Note that ExHiRD-h specifically designed a deduplication mechanism to remove duplication in the inference stage. In contrast, our model achieves\na lower duplication ratio without any deduplication mechanism, which proves its effectiveness. However, we also observe that our model tends to overgenerate more present keyphrases than the ground-truth on the Krapivin and KP20k datasets. We analyze that different datasets have different preferences for the number of keyphrases, which we leave as our future work."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "To understand the effects of each component of the SETTRANS model, we conduct an ablation study on it and report the results on the KP20k dataset in Table 5.\nEffects of Model Architecture To verify the effectiveness of the model architecture of SETTRANS, we remove the control codes and find the model is completely broken. The duplication ratio increases to 0.95, which means all the 20 control codes predict the same keyphrase. This occurs because when the control codes are removed, all the predictions depend on the same condition (i.e., the source document) without any distinction. This demonstrates that the control codes play an extremely important role in the SETTRANS model.\nEffects of Target Assignment The major difficulty for successfully training under ONE2SET paradigm is the target assignment between predictions and targets. An attempt is first made to remove the K-step target assignment mechanism, which means that we employ a fixed sequential matching strategy as in the ONE2SEQ paradigm. From the results, we observe that both the present and absent keyphrase performances degrade, the number of predicted keyphrases also drops dramatically, and the duplication ratio increased greatly by 18%. We analyze the reasons as follows: (1)\nThe dynamic characteristics of the K-step target assignment remove unnecessary position constraint during training, which encourages the model to generate more keyphrases. Specifically, the model can generate a keyphrase in any location rather than only in the given position. Thus, the model does not need to consider the position constraint during the generation and encourages all the control codes to predict keyphrases rather than only the first few codes, which will be verified in Section 5.6. (2) The bipartite characteristics of the Kstep target assignment forces the model to predict unique keyphrases, which reduces the duplication ratio of predictions. When predictions from two codes are similar, only one code may be assigned a target keyphrase, and the other is assigned a ∅ token. Thus, the model can be very careful about each prediction to prevent duplication. We further experiment that replacing the K-step target assignment with a random assignment, and we find that the results are similar to those when removing the control codes. This is because the random assignment misleads the learning of the control codes and causes them to become invalid.\nEffects of Set Loss As discussed in Section 3.3.2, teacher forcing and a separate set loss are used to train the model after assigning a target for each prediction. We investigate their effects in detail. The results show the following. (1) Teaching forcing can alleviate the cold start problem. After removing teaching forcing, the model faces a cold start problem, in other words, the lack of supervision information leads to a poor prediction, and the target assignment is therefore not ideal, which causes the model to fail at the early stage of training. (2) A separate set loss helps in both present and absent keyphrase predictions\nbut also increases the duplication ratio slightly compared with a single set loss. As producing correct present keyphrases is an easier task, the model tends to generate present keyphrases only when using a single set loss. Our separate set loss can infuse different inductive biases into the two sets of control codes, which makes them more focused on generating one type of keyphrase (i.e., the present one or absent one). Thus, it increases the accuracy of the predictions and encourages more absent keyphrase predictions. However, because bipartite matching is performed separately, the constraint of unique prediction does not exist between the two sets, which leads to a slight increase in the duplication ratio."
    }, {
      "heading" : "5.4 Performance over Scale Factors",
      "text" : "In this section, we conduct experiments on KP20k dataset to evaluate performance under different loss scale factors λ for ∅ token. The results are shown in Figure 3.\nThe left part of the figure shows that when λ = 0.2, the performances on both present and absent keyphrases are consistently better than the results when λ = 0.1. However, a scale factor larger than 0.1 improves the present keyphrase performance, but also harms the absent keyphrase performance. As we can see from the right part of the figure, the number of predictions decreases consistently for both the present and absent keyphrases when the scale factor becomes larger. This is because a larger scale factor causes the model to predict more ∅ tokens to reduce the loss penalty during training. Moreover, we also find that the precision metric P@M will increases when the number of predictions decreases. While the effect of the decrease in the recall metric R@M is even greater when the number is too small, which leads to a degradation in the overall metric F1@M ."
    }, {
      "heading" : "5.5 Efficiency over Assignment Steps",
      "text" : "In this section, we study the influence of target assignment steps K on the prediction performance and efficiency compared with Transformer.\nAs shown in the left part of Figure 4, we note that when K is equal to 1, the improvement of SETTRANS over Transformer is relatively lower than when it is equal to 2 (i.e., the average length of keyphrase). This is mainly because some keyphrases that have the same first word cannot be distinguished during training, which could interfere with the learning of control codes. The right part of Figure 4 shows the training and inference speedup with various K compared with the Transformer. We note SETTRANS could be slower than Transformer at the training stage, and a smaller K could alleviate this problem. For performance and efficiency considerations, we consider 2 to be an appropriate value for steps K. Moreover, as K is only used in the training stage, SETTRANS is 6.44 times invariably faster than Transformer on the inference stage. This is because that with different control codes as input condition, all the keyphrases can be generated in parallel on the GPU. Hence, in addition to better performance than Transformer, SETTRANS also has great advantages in the inference efficiency."
    }, {
      "heading" : "5.6 Analysis of Learned Control Codes",
      "text" : "Our analysis here is driven by two questions from Section 5.3:\n(1) Whether the K-step target assignment mechanism encourages all the control codes to predict keyphrases rather than only the first few codes?\n(2) Whether the separate set loss makes the control codes more focused on generating one type of keyphrase (i.e., present or absent) compared to the single set loss?\nTo investigate these two questions, we measure the ratio of present and absent keyphrase predic-\ntions for all the control codes on the KP20k dataset, which is shown in Figure 5. As shown in the top and middle subfigures, we observe that without the target assignment mechanism, many control codes are invalid (i.e., only predicting ∅), and only the first small part performs valid predictions. Moreover, when there are already very few valid predictions, the model still has a duplication ratio of up to 26%, as shown in Table 5, resulting in an even smaller number of final predictions. After the introduction of the target assignment mechanism, most of the codes can generate valid keyphrases, which increases the number of predictions.\nHowever, as shown in the middle subfigure, most of the control code tends to generate more present keyphrases than absent keyphrases when using a single set loss. When using a separate set loss in the bottom subfigure, the two parts are more inclined to predict only present and absent keyphrases respectively, which also increases the number of absent keyphrase predictions."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we propose a new training paradigm ONE2SET without predefining an order to concatenate the keyphrases, and a novel model SETTRANS that predicts a set of keyphrases in parallel. To successfully train under ONE2SET paradigm, we propose a K-step target assignment mechanism and a separate set loss, which greatly increases the number and diversity of the generated keyphrases. Experiments show that our method gains significantly huge performance improvements against\nexisting state-of-the-art models. We also show that SETTRANS has great advantages in the inference efficiency compared with the Transformer under ONE2SEQ paradigm."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&D Program (No. 2017YFB1002104), National Natural Science Foundation of China (No. 62076069, 61976056), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural keyphrase generation via reinforcement learning with adaptive rewards",
      "author" : [ "Hou Pong Chan", "Wang Chen", "Lu Wang", "Irwin King." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Chan et al\\.,? 2019",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2019
    }, {
      "title" : "Keyphrase generation with correlation constraints",
      "author" : [ "Jun Chen", "Xiaoming Zhang", "Yu Wu", "Zhao Yan", "Zhoujun Li." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4057–4066,",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "An integrated approach for keyphrase generation via exploring the power of retrieval and extraction",
      "author" : [ "Wang Chen", "Hou Pong Chan", "Piji Li", "Lidong Bing", "Irwin King." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Chen et al\\.,? 2019a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Exclusive hierarchical decoding for deep keyphrase generation",
      "author" : [ "Wang Chen", "Hou Pong Chan", "Piji Li", "Irwin King." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1095–1105,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Title-guided encoding for keyphrase generation",
      "author" : [ "Wang Chen", "Yifan Gao", "Jiani Zhang", "Irwin King", "Michael R. Lyu." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI, pages 6268–6275. AAAI Press.",
      "citeRegEx" : "Chen et al\\.,? 2019b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating expert knowledge into keyphrase extraction",
      "author" : [ "Sujatha Das Gollapalli", "Xiaoli Li", "Peng Yang." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 3180–",
      "citeRegEx" : "Gollapalli et al\\.,? 2017",
      "shortCiteRegEx" : "Gollapalli et al\\.",
      "year" : 2017
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved automatic keyword extraction given more linguistic knowledge",
      "author" : [ "Anette Hulth." ],
      "venue" : "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216–223.",
      "citeRegEx" : "Hulth.,? 2003",
      "shortCiteRegEx" : "Hulth.",
      "year" : 2003
    }, {
      "title" : "SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
      "author" : [ "Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21–26,",
      "citeRegEx" : "Kim et al\\.,? 2010",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Large dataset for keyphrases extraction",
      "author" : [ "Mikalai Krapivin", "Aliaksandr Autaeu", "Maurizio Marchese." ],
      "venue" : "Technical report, University of Trento.",
      "citeRegEx" : "Krapivin et al\\.,? 2009",
      "shortCiteRegEx" : "Krapivin et al\\.",
      "year" : 2009
    }, {
      "title" : "The hungarian method for the assignment problem",
      "author" : [ "Harold W Kuhn." ],
      "venue" : "Naval research logistics quarterly, 2(1-2):83–97.",
      "citeRegEx" : "Kuhn.,? 1955",
      "shortCiteRegEx" : "Kuhn.",
      "year" : 1955
    }, {
      "title" : "Automatic keyphrase extraction by bridging vocabulary gap",
      "author" : [ "Zhiyuan Liu", "Xinxiong Chen", "Yabin Zheng", "Maosong Sun." ],
      "venue" : "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135–144, Port-",
      "citeRegEx" : "Liu et al\\.,? 2011",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2011
    }, {
      "title" : "Human-competitive tagging using automatic keyphrase extraction",
      "author" : [ "Olena Medelyan", "Eibe Frank", "Ian H. Witten." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327, Singapore.",
      "citeRegEx" : "Medelyan et al\\.,? 2009",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2009
    }, {
      "title" : "Does Order Matter? An Empirical Study on Generating Multiple Keyphrases as a Sequence",
      "author" : [ "Rui Meng", "Xingdi Yuan", "Tong Wang", "Peter Brusilovsky", "Adam Trischler", "Daqing He." ],
      "venue" : "arXiv:1909.03590 [Cs].",
      "citeRegEx" : "Meng et al\\.,? 2019",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep keyphrase generation",
      "author" : [ "Rui Meng", "Sanqiang Zhao", "Shuguang Han", "Daqing He", "Peter Brusilovsky", "Yu Chi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Meng et al\\.,? 2017",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2017
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Keyphrase extraction in scientific publications",
      "author" : [ "Thuy Dung Nguyen", "Min-Yen Kan." ],
      "venue" : "International conference on Asian digital libraries, pages 317–326. Springer.",
      "citeRegEx" : "Nguyen and Kan.,? 2007",
      "shortCiteRegEx" : "Nguyen and Kan.",
      "year" : 2007
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A preliminary exploration of GANs for keyphrase generation",
      "author" : [ "Avinash Swaminathan", "Haimin Zhang", "Debanjan Mahata", "Rakesh Gosangi", "Rajiv Ratn Shah", "Amanda Stent." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Swaminathan et al\\.,? 2020",
      "shortCiteRegEx" : "Swaminathan et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Single document keyphrase extraction using neighborhood knowledge",
      "author" : [ "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "AAAI, volume 8, pages 855–860.",
      "citeRegEx" : "Wan and Xiao.,? 2008",
      "shortCiteRegEx" : "Wan and Xiao.",
      "year" : 2008
    }, {
      "title" : "One size does not fit all: Generating and evaluating variable number of keyphrases",
      "author" : [ "Xingdi Yuan", "Tong Wang", "Rui Meng", "Khushboo Thaker", "Peter Brusilovsky", "Daqing He", "Adam Trischler." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Keyphrase extraction using deep recurrent neural networks on Twitter",
      "author" : [ "Qi Zhang", "Yang Wang", "Yeyun Gong", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 836–845,",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating linguistic constraints into keyphrase generation",
      "author" : [ "Jing Zhao", "Yuxiang Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5224–5233, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Zhao and Zhang.,? 2019",
      "shortCiteRegEx" : "Zhao and Zhang.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "(2017) proposed a sequence-to-sequence (Seq2Seq) model with a copy mechanism (Gu et al., 2016) to predict both present and absent keyphrases.",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "Hence, we introduce a set prediction model that adopts Transformer (Vaswani et al., 2017) as the main architecture together with a fixed set of learned control codes as additional decoder inputs to perform controllable generation.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008).",
      "startOffset" : 106,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008).",
      "startOffset" : 106,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008).",
      "startOffset" : 106,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : "Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008).",
      "startOffset" : 106,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007).",
      "startOffset" : 77,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007).",
      "startOffset" : 77,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007).",
      "startOffset" : 146,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007).",
      "startOffset" : 146,
      "endOffset" : 181
    }, {
      "referenceID" : 25,
      "context" : "Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "(2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al.",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : ", 2014) with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "The recent works (Chan et al., 2019; Chen et al., 2020; Swaminathan et al., 2020) mostly follow the ONE2SEQ training paradigm.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "The recent works (Chan et al., 2019; Chen et al., 2020; Swaminathan et al., 2020) mostly follow the ONE2SEQ training paradigm.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "The recent works (Chan et al., 2019; Chen et al., 2020; Swaminathan et al., 2020) mostly follow the ONE2SEQ training paradigm.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "To solve the KG task, previous works typically adopted an ONE2ONE training paradigm (Meng et al., 2017)",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "We adopt the Transformer (Vaswani et al., 2017) as the backbone encoder-decoder framework.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "is the t-th sinusoid positional embedding as in (Vaswani et al., 2017) and cn is the n-th learned control code embedding.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "a copy mechanism (See et al., 2017), which is generally adopted for many previous KG works (Meng et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : ", 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : ", 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : ", 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : ", 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "This optimal assignment is computed efficiently with the Hungarian algorithm (Kuhn, 1955).",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "We conduct our experiments on five scientific article datasets, including Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin et al.",
      "startOffset" : 81,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "We conduct our experiments on five scientific article datasets, including Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin et al.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "We conduct our experiments on five scientific article datasets, including Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin et al., 2009), SemEval (Kim et al.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : ", 2009), SemEval (Kim et al., 2010) and KP20k (Meng et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Following previous works (Meng et al., 2017; Chen et al., 2019b,a; Yuan et al., 2020), we concatenate the title and abstract as a source document.",
      "startOffset" : 25,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "Following previous works (Meng et al., 2017; Chen et al., 2019b,a; Yuan et al., 2020), we concatenate the title and abstract as a source document.",
      "startOffset" : 25,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "We focus on the comparisons with the following state-of-the-art methods as our baselines: • catSeq (Yuan et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Following previous works (Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020), when training under the ONE2SEQ paradigm, the target keyphrase sequence is the concatenation of present and absent keyphrases, with the present keyphrases are sorted according to the orders of their first occurrences in the document and the absent keyphrase kept in their original order.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Following previous works (Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020), when training under the ONE2SEQ paradigm, the target keyphrase sequence is the concatenation of present and absent keyphrases, with the present keyphrases are sorted according to the orders of their first occurrences in the document and the absent keyphrase kept in their original order.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Following previous works (Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020), when training under the ONE2SEQ paradigm, the target keyphrase sequence is the concatenation of present and absent keyphrases, with the present keyphrases are sorted according to the orders of their first occurrences in the document and the absent keyphrase kept in their original order.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "We use the Adam optimization algorithm (Kingma and Ba, 2015) with a learning rate of 0.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M catSeq (Yuan et al., 2020) 0.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M catSeq (Yuan et al., 2020) 0.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "We follow previous works (Chan et al., 2019; Chen et al., 2020) and use macro-averaged F1@5 and F1@M for both present and absent keyphrase predictions.",
      "startOffset" : 25,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "We follow previous works (Chan et al., 2019; Chen et al., 2020) and use macro-averaged F1@5 and F1@M for both present and absent keyphrase predictions.",
      "startOffset" : 25,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "As noted by previous works (Chan et al., 2019; Yuan et al., 2020) that predicting absent keyphrases for a document is an extremely challenging task, thus the performance is much lower than that of present keyphrase prediction.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "As noted by previous works (Chan et al., 2019; Yuan et al., 2020) that predicting absent keyphrases for a document is an extremely challenging task, thus the performance is much lower than that of present keyphrase prediction.",
      "startOffset" : 27,
      "endOffset" : 65
    } ],
    "year" : 2021,
    "abstractText" : "Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm ONE2SET without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step target assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the duplication ratio of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.",
    "creator" : "LaTeX with hyperref"
  }
}