{
  "name" : "2021.acl-long.188.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Unified Generative Framework for Aspect-Based Sentiment Analysis",
    "authors" : [ "Hang Yan", "Junqi Dai", "Tuo Ji", "Xipeng Qiu", "Zheng Zhang" ],
    "emails" : [ "hyan19@fudan.edu.cn", "jqdai19@fudan.edu.cn", "tji19@fudan.edu.cn", "xpqiu@fudan.edu.cn", "zz@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2416–2429\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2416"
    }, {
      "heading" : "1 Introduction",
      "text" : "Aspect-based Sentiment Analysis (ABSA) is the fine-grained Sentiment Analysis (SA) task, which aims to identify the aspect term (a), its corresponding sentiment polarity (s), and the opinion term (o). For example, in the sentence “The drinks are always well made and wine selection is fairly priced”, the aspect terms are “drinks” and “wine selection”, and their sentiment polarities are both “positive”, and the opinion terms are “well made” and “fairly priced”. Based on the combination of the a, s, o, there exist seven subtasks in ABSA. We summarize these subtasks in Figure 1. Specifically, their definitions are as follows:\n∗Equal contribution. †Corresponding author.\n1Code is available at https://github.com/yhcc/ BARTABSA.\n•Aspect Term Extraction(AE): Extracting all the aspect terms from a sentence. • Opinion Term Extraction (OE): Extracting all the opinion terms from a sentence. • Aspect-level Sentiment Classification (ALSC): Predicting the sentiment polarities for every given aspect terms in a sentence. • Aspect-oriented Opinion Extraction (AOE): Extracting the paired opinion terms for every given aspect terms in a sentence. • Aspect Term Extraction and Sentiment Classification (AESC): Extracting the aspect terms as well as the corresponding sentiment polarities simultaneously. • Pair Extraction (Pair): Extracting the aspect terms as well as the corresponding opinion terms simultaneously. • Triplet Extraction (Triplet): Extracting all aspects terms with their corresponding opinion terms and sentiment polarity simultaneously.\nAlthough these ABSA subtasks are strongly related, most of the existing work only focus 1∼3 subtasks individually. The following divergences make it difficult to solve all subtasks in a unified framework.\n1. Input: Some subtasks ( AE, OE, AESC, Pair\nand Triplet) only take the text sentence as input, while the remained subtasks ( ALSC and AOE) take the text and a given aspect term as input.\n2. Output: Some tasks (AE, OE, ALSC, AOE) only output a certain type from a, s or o, while the remained tasks (AESC, Pair and Triplet) return compound output as the combination of a, s and o.\n3. Task Type: There are two kinds of tasks: extraction task (extracting aspect and opinion) and classification task (predicting sentiment).\nBecause of the above divergences, a myriad of previous works only focus on the subset of these subtasks. However, the importance of solving the whole ABSA subtasks in a unified framework remains significant. Recently, several works make attempts on this track. Some methods(Peng et al., 2020; Mao et al., 2021) apply the pipeline model to output the a, s, o from the inside sub-models separately. However, the pipeline process is not end-to-end. Another line follows the sequence tagging method by extending the tagging schema (Xu et al., 2020). However, the compositionality of candidate labels hinders the performance. In conclusion, the existing methods can hardly solve all the subtasks by a unified framework without relying on the sub-models or changing the model structure to adapt to all ABSA subtasks.\nMotivated by the above observations, we propose a unified generative framework to address all the ABSA subtasks. We first formulate all these subtasks as a generative task, which could handle the obstacles on the input, output, and task type sides and adapt to all the subtasks without any model structure changes. Specifically, we model the extraction and classification tasks as the pointer indexes and class indexes generation, respectively. Based on the unified task formulation, we use the sequence-to-sequence pre-trained model BART (Lewis et al., 2020) as our backbone to generate the target sequence in an end-to-end process. To validate the effectiveness of our method, we conduct extensive experiments on public datasets. The comparison results demonstrate that our proposed framework outperforms most state-of-the-art (SOTA) models in every subtask.\nIn summary, our main contributions are as follows: •We formulate both the extraction task and classification task of ABSA into a unified index gen-\neration problem. Unlike previous unified models, our method needs not to design specific decoders for different output types. • With our re-formulation, all ABSA subtasks can be solved in sequence-to-sequence framework, which is easy-to-implement and can be built on the pre-trained models, such as BART. •We conduct extensive experiments on four public datasets, and each dataset contains a subset of all ABSA subtasks. To the best of our knowledge, it is the first work to evaluate a model on all ABSA tasks. • The experimental results show that our proposed framework significantly outperforms recent SOTA methods."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 ABSA Subtasks",
      "text" : "In this section, we first review the existing studies on single output subtasks, and then turn to studies focusing on the compound output subtasks."
    }, {
      "heading" : "2.1.1 Single Output Subtasks",
      "text" : "Some researches mainly focus on the single output subtasks. The AE, OE, ALSC and AOE subtasks only output one certain type from a, s or o.\nAE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b). Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020).\nOE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019). Most works can only extract the unpaired aspect and opinion terms2. In this case, opinion terms are independent of aspect terms.\nALSC Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. Wang et al. (2016b); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based neural network models to model relations of aspects and their contextual words. Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural\n2It is also referred to as the AE-OE co-Extraction.\nnetwork (Tang et al., 2016b; Chen et al., 2017) have also been applied.\nAOE This subtask is first introduced by Fan et al. (2019) and they propose the datasets for this subtask. Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020)."
    }, {
      "heading" : "2.1.2 Compound Output Subtasks",
      "text" : "Some researchers pay more attention and efforts to the subtasks with compound output. We review them as follows:\nAESC. One line follows pipeline method to solve this problem. Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018). Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema.\nPairs Zhao et al. (2020) propose to extract all (a, o) pair-wise relations from scratch. They propose a multi-task learning framework based on the spanbased extraction method to handle this subtask.\nTriplet This subtask is proposed by Peng et al. (2020) and gains increasing interests recently. Xu et al. (2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004). However, the time complexity limits the model to detect the aspect term with longdistance opinion terms. Mao et al. (2021) formulate Triplet as a two-step MRC problem, which applies the pipeline method."
    }, {
      "heading" : "2.2 Sequence-to-Sequence Models",
      "text" : "The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015). Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al. (2019); Raffel et al. (2020); Lewis et al. (2020) try to pre-train sequence-tosequence models. Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al., 2015), such as MASS (Song et al., 2019).\nBART is a strong sequence-to-sequence pretrained model for Natural Language Generation (NLG). BART is a denoising autoencoder composed of several transformer (Vaswani et al., 2017) encoder and decoder layers. It is worth noting that the BART-Base model contains a 6-layer encoder and 6-layer decoder, which makes it similar number of parameters3 with the BERT-Base model. BART is pretrained on denoising tasks where the input sentence is noised by some methods, such as masking and permutation. The encoder takes the noised sentence as input, and the decoder will restore the original sentence in an autoregressive manner."
    }, {
      "heading" : "3 Methodology",
      "text" : "Although there are two types of tasks among the seven ABSA subtasks, they can be formulated under a generative framework. In this part, we first introduce our sequential representation for each ABSA subtask. Then we detail our method, which utilizes BART to generate these sequential representations."
    }, {
      "heading" : "3.1 Task Formulation",
      "text" : "As depicted in Figure 1, there are two types of tasks, namely the extraction and classification, whose target can be represented as a sequence of pointer indexes and class indexes, respectively. Therefore, we can formulate these two types of tasks in a unified generative framework. We use a, s, o, to represent the aspect term, sentiment polarity,and opinion term, respectively. Moreover, we use the superscript s and e to denote the start index and end index of a term. For example, os, ae represent the start index of an opinion term o and the end index of an aspect term a. We use the sp to denote the index of sentiment polarity class. The target sequence for each subtask is as follows: • AE : Y = [as1, ae1, ..., asi , aei , ...], • OE : Y = [os1, oe1, ..., osi , oei , ...], • AESC : Y = [as1, ae1, s p 1, ..., a s i , a e i , s p i , ...],\n• Pair: Y = [as1, ae1, os1, oe1, ..., asi , aei , osi , oei ,...], • Triplet : Y = [as1, ae1, os1, oe1, s p 1, ..., a s i , a e i , o s i ,\noei , s p i , ...],\nThe above subtasks only rely on the input sentence, while for the ALSC and AOE subtasks, they also depend on a specific aspect term a. Instead of putting the aspect term on the input side, we put\n3Because of the cross-attention between encoder and decoder, the number of parameters of BART is about 10% larger than its counterpart of BERT (Lewis et al., 2020).\nand the class index will be converted to corresponding class tokens. Embedding vectors in ll boxes are retrieved from same embedding matrix. We use different position embeddings in the source and target for better generation performance.\nthem on the target side so that the target sequences are as follows: • ALSC : Y = [as, ae, sp], • AOE : Y = [as, ae, os1, oe1, ..., osi , oei , ...], where the underlined tokens are given during inference. Detailed target sequence examples for each subtask are presented in Figure 3."
    }, {
      "heading" : "3.2 Our Model",
      "text" : "As our discussion in the last section, all subtasks can be formulated as taking the X = [x1, ..., xn] as input and outputting a target sequence Y =\n[y1, ..., ym], where y0 is the start-of-the-sentence token. Therefore, different ABSA subtasks can be formulated as:\nP (Y |X) = m∏ t=1 P (yt|X,Y<t). (1)\nTo get the index probability distribution Pt = P (yt|X,Y<t) for each step, we use a model composed of two components: (1) Encoder; (2) Decoder.\nEncoder The encoder part is to encode X into vectors He. We use the BART model, therefore, the start of sentence (<s>) and the end of sentence (</s>) tokens will be added to the start and end of X , respectively. We ignore the <s> token in our equations for simplicity. The encoder part is as follows:\nHe = BARTEncoder([x1, ..., xn]), (2)\nwhere He ∈ Rn×d, and d is the hidden dimension. Decoder The decoder part takes the encoder outputs He and previous decoder outputs Y<t as inputs to get Pt. However, the Y<t is an index sequence. Therefore, for each yt in Y<t, we first need to use the following Index2Token module to conduct a\nconversion\nŷt = { Xyt , if yt is a pointer index, Cyt−n, if yt is a class index,\n(3)\nwhere C = [c1, ..., cl] is the class token list4. After that, we use the BART decoder to get the last hidden state\nhdt = BARTDecoder(H e; Ŷ<t), (4)\nwhere hdt ∈ Rd. With hdt , we predict the token probability distribution Pt as follows:\nEe = BARTTokenEmbed(X), (5)\nĤe = MLP(He), (6)\nH̄e = αĤe + (1− α)Ee, (7) Cd = BARTTokenEmbed(C), (8)\nPt = Softmax([H̄ e;Cd]hdt ), (9)\nwhere Ee,He, Ĥe, H̄e ∈ Rn×d; Cd ∈ Rl×d; and Pt ∈ R(n+l) is the final distribution on all indexes.\nDuring the training phase, we use the teacher forcing to train our model and the negative loglikelihood to optimize the model. Moreover, during the inference, we use the beam search to get the target sequence Y in an autoregressive manner. After that, we need to use the decoding algorithm to convert this sequence into the term spans and sentiment polarity. We use the Triplet task as an example and present the decoding algorithm in Algorithm 1, the decoding algorithm for other tasks are much depicted in the Supplementary Material.\nAlgorithm 1 Decoding Algorithm for the Triplet Subtask Input: Number of tokens in the input sentence\nn, target sequence Y = [y1, ..., ym] and yi ∈ [1, n+ |C|] Output: Target span set L = {(as1, ae1, os1, oe1, s1), ..., (asi , aei , osi , oei , si), ...}\n1: L = {}, e = [], i = 1 2: while i <= m do 3: yi = Y [i] 4: if yi > n then 5: L.add((e, Cyi−n)) 6: e = [] 7: else 8: e.append(yi) 9: end if\n10: i+ = 1 11: end while 12: return L"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our method on four ABSA datasets. All of them are originated from the Semeval Challenges (Pontiki et al., 2014a,b,c), where only the aspect terms and their sentiment polarities are labeled.\nThe first dataset(D175) is annotated by Wang et al. (2017), where the unpaire opinion terms are labeled. The second dataset(D19) is annotated by Fan et al. (2019), where they pair opinion terms with\n4In our implement, yt ∈ [1, n+ l]. The x1 has the pointer index 1.\n5Each dataset only contains a subset of all ABSA subtasks. We use the published year of the dataset to distinguish them.\ncorresponding aspects. The third dataset(D20a) is from Peng et al. (2020). They refine the data in <a, o, s> triplet form. The fourth dataset(D20b) from Xu et al. (2020) is the revised variant of Peng et al. (2020), where the missing triplets with overlapping opinions are corrected. We present the statistics for these four datasets in Table 1."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "To have a fair comparison, we summarize topperforming baselines of all ABSA subtasks. Given different ABSA subtasks, datasets, and experimental setups, existing baselines can be separated into three groups roughly as shown in Table 2.\nThe baselines in the first group are conducted on D17 dataset, covering the AE, OE, ALSC, and AESC subtasks. Span-based method SPAN-BERT (Hu et al., 2019) and sequence tagging method, IMNBERT (He et al., 2019) and RACL-BERT (Chen and Qian, 2020), are selected. Specifically, the IMN-BERT model is reproduced by Chen and Qian (2020). All these baselines are implemented on BERT-Large.\nThe baselines of the second group are conducted on D19 dataset, mainly focusing on AOE subtask. Interestingly, we find that sequence tagging method is the main solution for this subtask (Fan et al., 2019; Wu et al., 2020; Pouran Ben Veyseh et al., 2020).\nThe baselines of the third group are mainly conducted on D20a and D20b datasets, which could\ncover almost all the ABSA subtasks except for one certain subtask depending on the baseline structures. For the following baselines: RINANTE (Dai and Song, 2019), CMLA (Wang et al., 2017), Liunified (Li et al., 2019), the suffix “+” in Table 2 denotes the corresponding model variant modified by Peng et al. (2020) for being capable of AESC, Pair and Triplet."
    }, {
      "heading" : "4.3 Implement Details",
      "text" : "Following previous studies, we use different metrics according to different subtasks and datasets. Specifically, for the single output subtasks AE, OE, and AOE, the prediction span would be considered as correct only if it exactly matches the start and the end boundaries. For the ALSC subtask, we require the generated sentiment polarity of the given aspect should be the same as the ground truth. As for compound output subtasks, AESC, Pair and Triplet, a prediction result is correct only when all the span boundaries and the generated sentiment polarity are accurately identified. We report the precision (P), recall (R), and F1 scores for all experiments6."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "On D17 dataset (Wang et al., 2017), we compare our method for AE, OE, ALSC, and AESC. The comparison results are shown in Table 3. Most of our results achieve better or comparable results to\n6Due to the limited space, we would present detailed experiments for each dataset in the Supplementary Material.\nbaselines. However, these baselines yield competitive results based on the BERT-Large pre-trained models. While our results are achieved on the BART-Base model with almost half parameters. This shows that our framework is more suitable for\nthese ABSA subtasks.\nOn D19 dataset (Fan et al., 2019), we compare our method for AOE. The comparison results are shown in Table 4. We can observe that our method achieves significant P/R/F1 improvements on 14res,\n15res, and 16res. Additionally, we notice that our F1 score on 14lap is close to the previous SOTA result. This is probably caused by the dataset domain difference as the 14lap is the laptop comments while the others are restaurant comments.\nOn D20a dataset (Peng et al., 2020), we compare our method for AESC, Pair, and Triplet. The comparison results are shown in Table 5. We can observe that our proposed method is able to outperform other baselines on all datasets. Specifically, we achieve the better results for Triplet, which demonstrates the effectiveness of our method on capturing interactions among aspect terms, opinion terms, and sentiment polarities. We also observe that the Span-based methods show superior performance to sequence tagging methods. This may be caused by the higher compositionality of candidate labels in sequence tagging methods (Hu et al., 2019). As the previous SOTA method, the DualMRC shows competitive performance by utilizing the span-based extraction method and the MRC mechanism. However, their inference process is not an end-to-end process.\nOn D20b dataset (Xu et al., 2020), we compare our method for Triplet. The comparison results can be found in Table 6. Our method achieves the best results with nearly 7 F1 points improvements on 14res, 15res, and 16res. Our method achieves nearly 13, 9, 7, 12 points improvements on each dataset for the recall scores compared with other baselines. This also explains the drop performance of the precision score. Since D20b is refined from D20a, we specifically compare the Triplet results of the corresponding dataset in D20a and D20b. Interestingly, we discover that all baselines have a much bigger performance change on 15res. We conjecture the distribution differences may be the cause reason. In conclusion, all the experiment results confirm that our proposed method, which unifies the training and the inference to an end-to-end generative framework, provides a new SOTA solution for the whole ABSA task."
    }, {
      "heading" : "5 Framework Analysis",
      "text" : "To better understand our proposed framework, we conduct analysis experiments on the D20b dataset (Xu et al., 2020).\nTo validate whether our proposed framework could adapt to the generative ABSA task, we metric the invalid predictions for the Triplet. Specifically, since the Triplet requires the prediction for-\nmat like [as, ae, os, oe, sp], it is mandatory that one valid triplet prediction should be in length 5, noted as “5-len”, and obviously all end index should be larger than the corresponding start index, noted as “ordered prediction”. We calculate number of non−5−len\ntotal prediction , referred to as the “Invalid size”, and the number of non−ordered predictiontotal 5−len prediction , referred to as the “Invalid order”. The “Invalid token” means the as is not the start of a token, instead, it is the index of an inside subword. From Table 7, we can observe that BART could learn this task form easily as the low rate for all the three metrics, which demonstrate that the generative framework for ABSA is not only a theoretically unified task form but also a realizable framework in practical. We remove these invalid predictions in our implementation of experiments.\nAs shown in Table 4, we give some analysis on the impact of the beam size, as we are a generation method. However, the beam size seems to have little impact on the F1 scores."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper summarizes the seven ABSA subtasks and previous studies, which shows that there exist divergences on all the input, output, and task type sides. Previous studies have limitations on handling all these divergences in a unified framework. We propose to convert all the ABSA subtasks to a unified generative task. We implement the BART\nto generate the target sequence in an end-to-end process based on the unified task formulation. We conduct massive experiments on public datasets for seven ABSA subtasks and achieve significant improvements on most datasets. The experimental results demonstrate the effectiveness of our method. Our work leads to several promising directions, such as sequence-to-sequence framework on other tasks, and data augmentation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their insightful comments. The discussion with colleagues in AWS Shanghai AI Lab was quite fruitful. We also thank the developers of fastNLP7 and fitlog8. This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106700) and National Natural Science Foundation of China (No. 62022027).\nEthical Considerations\nFor the consideration of ethical concerns, we would make detailed description as follows:\n(1) All the experiments are conducted on existing datasets, which are derived from public scientific papers.\n(2) We describe the characteristics of the datasets in a specific section. Our analysis is consistent with the results.\n(3) Our work does not contain identity characteristics. It does not harm anyone.\n(4) Our experiments do not need a lot of computer resources compared to pre-trained models.\n(5) We will open source all our code."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Experimental Environment\nWe use the triangular learning rate warmup. All experiments are conducted in the Nvidia Ge-Force RTX-3090 Graphical Card with 24G graphical memory.\nThe averages running time for experiments on each dataset is less than 15 minutes. The number of parameters is as follows: • BART-Base model: 12 layers, 768 hidden dimensions and 16 heads with the total number of parameters, 139M; • BERT-Base model: 12 layers, 768 hidden dimensions and 12 heads with the total number of parameters, 110M.\nA.2 Decoding Algorithm for Different Datasets\nIn this part, we introduce the decoding algorithm we used to convert the predicted target sequence Y into the target span set L. These algorithm can be found in Algorithm 2, 3, 4.\nAlgorithm 2 Decoding Algorithm for the AOE subtask Input: Number of tokens in the input sentence\nn, target sequence Y = [y1, ..., ym] and yi ∈ [1, n+ |C|], LT is a given length for different tasks. Output: Target span set L = {(os1, oe1, ..., osi , oei )} 1: L = {}, e = [], i = 3 2: while i <= m do 3: yi = Y [i] 4: e.append(yi) 5: i+ = 1 6: end while 7: L.add(e) 8: return L\nA.3 Detailed Experimental Setup\nExperiments on each dataset As the different subtasks are conducted on different datasets, specifically, we conduct the following experiments on each dataset: • On the D17 dataset, we conduct the AESC and the OE in multi-task learning method. To that end, we feed the pre-defined task tags “<AESC>” and “<OE>” to the decoder first. For example, for the input “The drinks are always\n:::: well ::::: made and wine\nselection is ::::: fairly :::::: priced” from D17 dataset, we\nAlgorithm 3 Decoding Algorithm for the AESC Subtask Input: Number of tokens in the input sentence\nn, target sequence Y = [y1, ..., ym] and yi ∈ [1, n+ |C|] Output: Target span set L = {(as1, ae1, s1), ..., (asi , aei , si)}\n1: L = {}, e = [], i = 1 2: while i <= m do 3: yi = Y [i] 4: if yi > n then 5: L.add((e, Cyi−n)) 6: e = [] 7: else 8: e.append(yi) 9: end if\n10: i+ = 1 11: end while 12: return L\nAlgorithm 4 Decoding Algorithm for the AE/OE/Pair subtasks Input: Number of tokens in the input sentence\nn, target sequence Y = [y1, ..., ym] and yi ∈ [1, n+ |C|], LT is a given length for different tasks. Output: Target span set L = {x1, ..., xi}(xi is (asi , a e i ), (o s i , o e i ) and (a s i , a e i , o s i , o e i ) for\nAE/OE/Pair, respectively) 1: L = {}, e = [], i = 1 2: while i <= m do 3: yi = Y [i] 4: if len(e) == LT then 5: L.add((e, Cyi−n)) 6: e = [] 7: end if 8: e.append(yi) 9: i+ = 1\n10: end while 11: return L\ndefine the AESC sequence and the OE target sequence as “<AESC>, 1, 1, POS, 7, 8, POS, </s>” and “<OE>, 4, 5, 10, 11, </s>”. • On the D19 dataset, we conduct the AOE. As the AOE subtask requires to detect the opinion terms given aspect terms in advance, the aspect terms need to be fed to our decoder first. For the aforementioned example sentence from D19 dataset, we define the AOE target sequence as “ 1, 1, 4, 5, </s>” and the “ 7, 8, 10, 11, </s>”.\n• On theD20a andD20b datasets, we conduct the Triplet Extraction. For the aforementioned example sentence fromD20a andD20b dataset, we define the Triplet target sequence as “1, 1, 4, 5, POS, 7, 8, 10, 11, POS, </s>”. Specific Subtask Metrics • On the D17 dataset, we get the AESC and OE results directly. Following previous work, we only calculate the metrics for AESC and ALSC from those true positive AE predictions. Specifically, the F1 • On the D19 dataset, we get the AOE results directly. The metrics for AOE are standard Precision, Recall and the F1 score. • On the D20a and D20b datasets, we get the Triplet results directly. We preserve the <AT,OT> for Pair metric and <AT, SP> for AESC metric. The metrics for them are standard Precision, Recall and the F1 score."
    } ],
    "references" : [ {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "7https://github.com/fastnlp/fastNLP.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Recurrent attention network on memory for aspect sentiment analysis",
      "author" : [ "Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461, Copen-",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3685–3694, Online. Association for",
      "citeRegEx" : "Chen and Qian.,? 2020",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2020
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural aspect and opinion term extraction with mined rules as weak supervision",
      "author" : [ "Hongliang Dai", "Yangqiu Song." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5268–5277, Florence, Italy. Asso-",
      "citeRegEx" : "Dai and Song.,? 2019",
      "shortCiteRegEx" : "Dai and Song.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Target-oriented opinion words extraction with target-fused neural sequence labeling",
      "author" : [ "Zhifang Fan", "Zhen Wu", "Xin-Yu Dai", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Open-domain targeted sentiment analysis via span-based extraction and classification",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Zhen Huang", "Dongsheng Li", "Yiwei Lv." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional augmentation for aspect term extraction via masked sequence-tosequence generation",
      "author" : [ "Kun Li", "Chengbo Chen", "Xiaojun Quan", "Qing Ling", "Yan Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformation networks for target-oriented sentiment classification",
      "author" : [ "Xin Li", "Lidong Bing", "Wai Lam", "Bei Shi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946–",
      "citeRegEx" : "Li et al\\.,? 2018a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "A unified model for opinion target extraction and target sentiment prediction",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect term extraction with history attention and selective transformation",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam", "Zhimou Yang." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July",
      "citeRegEx" : "Li et al\\.,? 2018b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep multi-task learning for aspect term extraction with memory interaction",
      "author" : [ "Xin Li", "Wai Lam." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2886–2892, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Li and Lam.,? 2017",
      "shortCiteRegEx" : "Li and Lam.",
      "year" : 2017
    }, {
      "title" : "Attention modeling for targeted sentiment",
      "author" : [ "Jiangming Liu", "Yue Zhang." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Liu and Zhang.,? 2017",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2017
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lis-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint learning for targeted sentiment analysis",
      "author" : [ "Dehong Ma", "Sujian Li", "Houfeng Wang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4737–4742, Brussels, Belgium. Association for Computational",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring sequence-tosequence learning in aspect term extraction",
      "author" : [ "Dehong Ma", "Sujian Li", "Fangzhao Wu", "Xing Xie", "Houfeng Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Interactive attention networks for aspect-level sentiment classification",
      "author" : [ "Dehong Ma", "Sujian Li", "Xiaodong Zhang", "Houfeng Wang." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Mel-",
      "citeRegEx" : "Ma et al\\.,? 2017",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2017
    }, {
      "title" : "A joint training dual-mrc framework for aspect based sentiment analysis",
      "author" : [ "Yue Mao", "Yi Shen", "Chao Yu", "Longjun Cai." ],
      "venue" : "CoRR, abs/2101.00816.",
      "citeRegEx" : "Mao et al\\.,? 2021",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2021
    }, {
      "title" : "Open domain targeted sentiment",
      "author" : [ "Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington,",
      "citeRegEx" : "Mitchell et al\\.,? 2013",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
      "author" : [ "Haiyun Peng", "Lu Xu", "Lidong Bing", "Fei Huang", "Wei Lu", "Luo Si." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evalua-",
      "citeRegEx" : "Pontiki et al\\.,? 2014a",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evalua-",
      "citeRegEx" : "Pontiki et al\\.,? 2014b",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evalua-",
      "citeRegEx" : "Pontiki et al\\.,? 2014c",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Introducing syntactic structures into target opinion word extraction with deep learning",
      "author" : [ "Amir Pouran Ben Veyseh", "Nasim Nouri", "Franck Dernoncourt", "Dejing Dou", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Veyseh et al\\.,? 2020",
      "shortCiteRegEx" : "Veyseh et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "TianXiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "SCIENCE CHINA Technological Sciences, 63(10):1872–1897.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Semimarkov conditional random fields for information extraction",
      "author" : [ "Sunita Sarawagi", "William W. Cohen." ],
      "venue" : "Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Van-",
      "citeRegEx" : "Sarawagi and Cohen.,? 2004",
      "shortCiteRegEx" : "Sarawagi and Cohen.",
      "year" : 2004
    }, {
      "title" : "MASS: masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Effective LSTMs for target-dependent sentiment classification",
      "author" : [ "Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3298–",
      "citeRegEx" : "Tang et al\\.,? 2016a",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Aspect level sentiment classification with deep memory network",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214–224, Austin, Texas. Association for Com-",
      "citeRegEx" : "Tang et al\\.,? 2016b",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis",
      "author" : [ "Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova-",
      "citeRegEx" : "Tay et al\\.,? 2018",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive neural structural correspondence network for crossdomain aspect and opinion co-extraction",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Wang and Pan.,? 2018",
      "shortCiteRegEx" : "Wang and Pan.",
      "year" : 2018
    }, {
      "title" : "Recursive neural conditional random fields for aspect-based sentiment analysis",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Wang et al\\.,? 2016a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Coupled multi-layer attentions",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-based LSTM for aspectlevel sentiment classification",
      "author" : [ "Yequan Wang", "Minlie Huang", "Xiaoyan Zhu", "Li Zhao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615, Austin,",
      "citeRegEx" : "Wang et al\\.,? 2016b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Latent opinions transfer network for target-oriented opinion words extraction",
      "author" : [ "Zhen Wu", "Fei Zhao", "Xin-Yu Dai", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Double embeddings and CNN-based sequence labeling for aspect extraction",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip S. Yu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 592–",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Position-aware tagging for aspect sentiment triplet extraction",
      "author" : [ "Lu Xu", "Hao Li", "Wei Lu", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2339–2349, Online. Associa-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspect based sentiment analysis with gated convolutional networks",
      "author" : [ "Wei Xue", "Tao Li." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2514–2523, Melbourne, Australia.",
      "citeRegEx" : "Xue and Li.,? 2018",
      "shortCiteRegEx" : "Xue and Li.",
      "year" : 2018
    }, {
      "title" : "Neural networks for open domain targeted sentiment",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Duy-Tin Vo." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, Lisbon, Portugal. Association for Compu-",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Gated neural networks for targeted sentiment analysis",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Duy-Tin Vo." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages 3087–3093.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "SpanMlt: A span-based multi-task learning framework for pair-wise aspect and opinion terms extraction",
      "author" : [ "He Zhao", "Longtao Huang", "Rong Zhang", "Quan Lu", "Hui Xue." ],
      "venue" : "Proceedings of the 58th Annual",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Some methods(Peng et al., 2020; Mao et al., 2021) apply the pipeline model to output the a, s, o from the inside sub-models separately.",
      "startOffset" : 12,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Some methods(Peng et al., 2020; Mao et al., 2021) apply the pipeline model to output the a, s, o from the inside sub-models separately.",
      "startOffset" : 12,
      "endOffset" : 49
    }, {
      "referenceID" : 45,
      "context" : "Another line follows the sequence tagging method by extending the tagging schema (Xu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "Based on the unified task formulation, we use the sequence-to-sequence pre-trained model BART (Lewis et al., 2020) as our backbone to generate the target sequence in an end-to-end process.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b).",
      "startOffset" : 63,
      "endOffset" : 116
    }, {
      "referenceID" : 44,
      "context" : "AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b).",
      "startOffset" : 63,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "AE Most studies treat AE subtask as a sequence tagging problem (Li and Lam, 2017; Xu et al., 2018; Li et al., 2018b).",
      "startOffset" : 63,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "Recent works explore sequence-to-sequence learning on AE subtask, which obtain promissing results especially with the pre-training language models (Ma et al., 2019; Li et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 181
    }, {
      "referenceID" : 39,
      "context" : "OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "OE Most studies treat OE subtask as an auxiliary task (Wang et al., 2016a, 2017; Wang and Pan, 2018; Chen and Qian, 2020; He et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al.",
      "startOffset" : 66,
      "endOffset" : 102
    }, {
      "referenceID" : 46,
      "context" : "Other model structures such as convolutional neural network (CNN) (Li et al., 2018a; Xue and Li, 2018), gated neural network (Zhang et al.",
      "startOffset" : 66,
      "endOffset" : 102
    }, {
      "referenceID" : 48,
      "context" : ", 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural",
      "startOffset" : 49,
      "endOffset" : 87
    }, {
      "referenceID" : 46,
      "context" : ", 2018a; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memory neural",
      "startOffset" : 49,
      "endOffset" : 87
    }, {
      "referenceID" : 35,
      "context" : "2418 network (Tang et al., 2016b; Chen et al., 2017) have also been applied.",
      "startOffset" : 13,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "2418 network (Tang et al., 2016b; Chen et al., 2017) have also been applied.",
      "startOffset" : 13,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "Most studies apply sequence tagging method for this subtask (Wu et al., 2020; Pouran Ben Veyseh et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 47,
      "context" : "Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Other works utilize unified tagging schema (Mitchell et al., 2013; Zhang et al., 2015; Li et al., 2019) or multi-task learning (He et al.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : ", 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al.",
      "startOffset" : 31,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : ", 2019) or multi-task learning (He et al., 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al.",
      "startOffset" : 31,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : ", 2019; Chen and Qian, 2020) to avoid the error-propagation problem (Ma et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "Spanbased AESC works are also proposed recently (Hu et al., 2019), which can tackle the sentiment inconsistency problem in the unified tagging schema.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "(2020) design the position-aware tagging schema and apply model based on CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 182
    }, {
      "referenceID" : 38,
      "context" : "The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "The sequence-to-sequence framework has been long studied in the NLP field to tackle various tasks (Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015; Luong et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 182
    }, {
      "referenceID" : 29,
      "context" : "Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the success of PTMs (pre-trained models) (Qiu et al., 2020; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), Song et al.",
      "startOffset" : 53,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "Among them, we use the BART (Lewis et al., 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 38,
      "context" : ", 2020) as our backbone, while the other sequence-to-sequence pre-training models can also be applied in our architecture to use the pointer mechanism (Vinyals et al., 2015), such as MASS (Song et al.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 37,
      "context" : "BART is a denoising autoencoder composed of several transformer (Vaswani et al., 2017) encoder and decoder layers.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Because of the cross-attention between encoder and decoder, the number of parameters of BART is about 10% larger than its counterpart of BERT (Lewis et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "Span-based method SPAN-BERT (Hu et al., 2019) and sequence tagging method, IMNBERT (He et al.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and sequence tagging method, IMNBERT (He et al., 2019) and RACL-BERT (Chen and Qian, 2020), are selected.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : ", 2019) and RACL-BERT (Chen and Qian, 2020), are selected.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Interestingly, we find that sequence tagging method is the main solution for this subtask (Fan et al., 2019; Wu et al., 2020; Pouran Ben Veyseh et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 157
    }, {
      "referenceID" : 43,
      "context" : "Interestingly, we find that sequence tagging method is the main solution for this subtask (Fan et al., 2019; Wu et al., 2020; Pouran Ben Veyseh et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "For the following baselines: RINANTE (Dai and Song, 2019), CMLA (Wang et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 41,
      "context" : "For the following baselines: RINANTE (Dai and Song, 2019), CMLA (Wang et al., 2017), Liunified (Li et al.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : ", 2017), Liunified (Li et al., 2019), the suffix “+” in Table 2 denotes the corresponding model variant modified by Peng et al.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 41,
      "context" : "On D17 dataset (Wang et al., 2017), we compare our method for AE, OE, ALSC, and AESC.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : "Table 3: Comparison F1 scores for AE, OE, SC, and AESC on the D17 dataset (Wang et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Table 4: Comparison results for AOE on the D19 dataset (Fan et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Table 5: Comparison F1 scores for AESC, Pair and Triplet on the D20a dataset (Peng et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 45,
      "context" : "Table 6: Comparison results for Triplet on the D20b dataset (Xu et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "On D19 dataset (Fan et al., 2019), we compare our method for AOE.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "On D20a dataset (Peng et al., 2020), we compare our method for AESC, Pair, and Triplet.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "This may be caused by the higher compositionality of candidate labels in sequence tagging methods (Hu et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 45,
      "context" : "On D20b dataset (Xu et al., 2020), we compare our method for Triplet.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 45,
      "context" : "To better understand our proposed framework, we conduct analysis experiments on the D20b dataset (Xu et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 114
    } ],
    "year" : 2021,
    "abstractText" : "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an endto-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks1.",
    "creator" : "LaTeX with hyperref"
  }
}