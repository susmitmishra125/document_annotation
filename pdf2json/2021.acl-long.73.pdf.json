{
  "name" : "2021.acl-long.73.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation",
    "authors" : [ "Dongqin Xu", "Junhui Li", "Muhua Zhu", "Min Zhang", "Guodong Zhou" ],
    "emails" : [ "xdqck@live.com,", "gdzhou}@suda.edu.cn", "zhumuhua@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 896–907\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n896\nXLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation\nDongqin Xu1 Junhui Li1∗ Muhua Zhu2 Min Zhang1 Guodong Zhou1 1School of Computer Science and Technology, Soochow University, Suzhou, China\n2Tencent News, Beijing, China xdqck@live.com, {lijunhui, minzhang, gdzhou}@suda.edu.cn\nzhumuhua@gmail.com\nAbstract\nDue to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-toX parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation. We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages. With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning. Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art. In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively. We make our code available on github https:// github.com/xdqkid/XLPT-AMR."
    }, {
      "heading" : "1 Introduction",
      "text" : "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as\n∗Corresponding Author: Junhui Li.\n“doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few).\nRestricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiêta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajič et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately.\nIn this paper, we formalize both AMR parsing and AMR-to-text generation as sequence-tosequence (seq2seq) learning and propose a novel and effective approach to cross-lingual AMR, which is illustrated in Figure 1. Upon the availability of the English AMR dataset and English-toX parallel datasets (X ∈ {German, Spanish, Italian} in this paper), our purpose is to boost the performance of zero-shot AMR parsing and text generation in\nX-language. To this end, we borrow the idea of joint pre-training from Xu et al. (2020) and explore three types of relevant tasks, including machine translation tasks, AMR parsing and AMR-to-text generation tasks. We conjecture that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research.\nWe also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help\nof another relevant yet stronger task. Experimental results on the cross-lingual AMR dataset (LDC2020T07) show that the proposed approach greatly advances the state of the art of cross-lingual AMR.\nOverall, we make the following contributions.\n• We propose an effective cross-lingual pretraining approach for zero-shot AMR parsing and AMR-to-text generation. Our pre-trained models could be used for both AMR parsing and AMR-to-text generation.\n• We explore and compare different fine-tuning methods. We also propose a teacher-studentbased fine-tuning method that achieves the best performance.\n• We evaluate our approach in three zero-shot languages of AMR and our approach greatly advances the state of the art."
    }, {
      "heading" : "2 Related Work",
      "text" : "We describe related studies on AMR from three perspectives: English AMR parsing, English AMRto-text generation, and cross-lingual AMR.\nEnglish AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a).\nEnglish AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq\ntask (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) .\nCross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajič et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate that a simplified AMR can be used across languages and for the first time they study crosslingual AMR parsing for languages rather than English. Blloshmi et al. (2020) employ large-scale silver parallel AMR data to bridge the gap between different languages and greatly advance the performance of cross-lingual AMR parsing. Sheth et al. (2021) explore annotation projection to leverage existing English AMR and overcome resource shortage in the target language. Furthermore, Fan and Gardent (2020) explore cross-lingual AMR-to-text based on pre-trained cross-lingual language model (XLM) (Lample and Conneau, 2019). In this paper we build strong cross-lingual pre-trained models for both AMR parsing and AMR-to-text generation. Moreover, a nice property of our approach is that for AMR parsing, unlike related studies (Damonte and Cohen, 2018; Blloshmi et al., 2020), we do not need to perform lemmatization, POS tagging, NER, or re-categorization of entities, thus require no language specific toolkits in pre-processing."
    }, {
      "heading" : "3 Cross-Lingual Pre-Training",
      "text" : "In this section, we first present the background of our pre-training approach (Section 3.1), followed by the description of cross-lingual pre-training tasks (Section 3.2). Then we present our joint\npre-training (Section 3.3). For simplicity, in the following we use German as a representative to describe our approach to German AMR parsing and AMR-to-text generation."
    }, {
      "heading" : "3.1 Background",
      "text" : "Transformer-based Seq2Seq Learning. Our models are built on the Transformer framework (Vaswani et al., 2017). The encoder in Transformer consists of a stack of multiple identical layers, each of which has two sub-layers: one implements the multi-head self-attention mechanism and the other is a position-wise fully-connected feedforward network. The decoder is also composed of a stack of multiple identical layers. Each layer in the decoder consists of the same sub-layers as in the encoder plus an additional sub-layer that performs multi-head attention to the distributional representation produced by the encoder. See Vaswani et al. (2017) for more details.\nAMR Graph Linearization and Recovering. To make Transformer applicable to AMR parsing and AMR-to-text generation, on the one hand we follow van Noord and Bos (2017) to linearize AMR graphs into sequences by removing variables, wiki links and duplicating the co-referring nodes. On the other hand, for AMR parsing we need to recover the graph representation from linearized AMRs by assigning a unique variable to each concept, pruning duplicated and redundant materials, restoring co-referring nodes, fixing incomplete concepts and performing Wikification.1 In this paper, we adopt linearization and recovering scripts provided by van Noord and Bos (2017).2"
    }, {
      "heading" : "3.2 Cross-Lingual Pre-Training Tasks",
      "text" : "Due to the unavailability of gold training data of German AMR parsing and AMR-to-text generation, we view English as a pivot and hope that knowledge gained while learning for English AMR parsing and text generation could be helpful for the German counterparts. Specifically, given an EN-DE parallel dataset ( T EN , T DE ) , we use an English AMR parser trained on annotated English AMRs (i.e., AMR2.0) to parse the English sentences into AMR graphs, thus obtain a trilingual parallel dataset T = ( T EN , T DE , T AMR ) . Then\n1We extract a term-wiki list from English AMR training dataset. When performing Wikification, we simply just look up the list.\n2https://github.com/RikVN/AMR\non the trilingual parallel dataset, we propose crosslingual pre-training via multi-task learning. We consider three types of tasks, i.e., AMR parsing, AMR-to-text generation, and machine translation.\nAMR Parsing Tasks, which include both English AMR parsing on the training data( T EN , T AMR ) and German AMR parsing on(\nT DE , T AMR ) . Note that both AMR parsing tasks\nare trained on silver AMR graphs.\nAMR-to-Text Generation Tasks, which include both English AMR-to-text generation and German AMR-to-text generation. Similar to AMR parsing, these two AMR-to-text generation tasks are also trained on silver AMR graphs( T AMR, T EN ) and ( T AMR, T DE ) , respectively.\nMachine Translation Tasks, which include both English-to-German and German-to-English machine translation tasks on ( T EN , T DE ) . The advantage of including the bi-directional translation tasks is three-fold. First, English-to-German translation will enable the decoder to generate fluent German sentence, which is beneficial to German AMR-to-text generation. Second, German-toEnglish translation will enable the encoder to capture syntax and semantic information from German sentences, which is beneficial to German AMR parsing. Third, translation tasks can serve as regularization to the training of AMR parsing and AMR-to-text generation, both of which are apt to overfit to the training data.\nOverall speaking, in our pre-training there exist three types of (six) pre-training tasks in total. The pre-training is conducted on a trilingual parallel dataset ( T EN , T DE , T AMR ) , where T EN and T DE are parallel gold sentence pairs while T AMR is the set of corresponding silver AMR graphs."
    }, {
      "heading" : "3.3 Jointly MTL Pre-Training",
      "text" : "To train the above six pre-training tasks with a single model, we follow the strategy used in Xu et al. (2020) and add preceding language tags to both source and target sides of training data to distinguish the inputs and outputs of each training task. As illustrated in Table 1, we use <en>, <de>, and <amr> as the tags of begin-of-sentence for English sentences, German sentences, and linearized AMRs, respectively.\nOur joint pre-training on multiple tasks falls into the paradigm of multi-task learning (MTL). In the training stage, we take turns to load the training\ndata of these pre-training tasks. For example, we update model parameters on a batch of training instances from the first task, and then update parameters on a batch of training instances of the second task, and the process repeats. We also note that, according to our preliminary experimentation, the effect of different orders of carrying out these pre-training tasks is negligible."
    }, {
      "heading" : "4 Fine-Tuning Methods",
      "text" : "To fine-tune a pre-trained model, we create a fine-tuning dataset from English annotated AMRs (i.e.,AMR2.0). Given English-AMR parallel data( FEN ,FAMR ) , we use an English-to-German translator to translate the English sentences into German sentences, thus obtain trilingual parallel dataset F = ( FEN ,FDE ,FAMR ) . As our goal is to improve the performance of zero-shot AMR parsing and AMR-to-text generation, our primary fine-tuning tasks are German AMR parsing and AMR-to-text generation. Moreover, we could include the other four fine-tuning tasks as auxiliary tasks when necessary, i.e., English AMR parsing and AMR-to-text generation, as well as English-toGerman and German-to-English translation.\nOnce the fine-tuning dataset is ready, we can finetune a pre-trained model with different methods. The vanilla fine-tuning method that fine-tunes a pretrained model on the dataset of a primary task is a natural choice. We can also fine-tune a pre-trained model jointly over all fine-tuning tasks, or over the primary tasks plus specifically chosen fine-tuning tasks that are relevant. In the following we explore and compare four different fine-tuning methods."
    }, {
      "heading" : "4.1 Vanilla Fine-Tuning",
      "text" : "Given a pre-trained model, vanilla fine-tuning updates the parameters of the pre-trained model solely on the dataset of the downstream task. For example, for German AMR parsing, we fine-tune the pre-trained model on the fine-tuning dataset of the German AMR parsing task. In other words, vanilla fine-tuning involves only a single-task learning."
    }, {
      "heading" : "4.2 One-for-All MTL Fine-Tuning",
      "text" : "We fine-tune a pre-trained model synchronously for all six fine-tuning tasks, which are the same as the pre-training tasks. Related studies (Li and Hoiem, 2018; Xu et al., 2020) have shown that it is important to optimize for high accuracy of a primary fine-tuning task while preserving the performance of other tasks. Preserving the performance of various pre-training tasks could be viewed as a regularizer for each fine-tuning task. Similarly to joint pre-training, we take turns to load the fine-tuning data of these fine-tuning tasks. Consequently, we obtain a single fine-tuned model for all tasks."
    }, {
      "heading" : "4.3 Targeted MTL Fine-Tuning",
      "text" : "Rather than including all fine-tuning tasks within a single model, we can selectively choose relevant fine-tuning tasks. For German AMR parsing, we use AMR parsing on German as the primary finetuning task and German-to-English translation as an auxiliary fine-tuning task. The auxiliary task will enhance the encoder to capture semantic information from German sentences. This is also consistent with the fine-tuning tasks designed for English AMR parsing in (Xu et al., 2020). For German AMR-to-text generation, we choose Englishto-German as the auxiliary fine-tuning task, which is beneficial for the decoder to generate fluent German sentences."
    }, {
      "heading" : "4.4 Teacher-Student-based MTL Fine-Tuning",
      "text" : "One notable property of the fine-tuning dataset is that the German sentences are produced automatically through machine translation. Noises in such silver fine-tuning dataset may degrade the performance of fine-tuned models. Inspired by the teacher-student framework (Kim and Rush, 2016; Chen et al., 2017), we propose to solve this problem by using a stronger fine-tuning task to help improve fine-tuning tasks on such noisy data. For example, we can use English AMR parsing (as the teacher) to help German AMR parsing (as the student), since English AMR parsing that is fine-tuned on gold data tends to have stronger performance.\nFine-Tuning for German AMR Parsing. We use E, G, A to denote English-side, German-side, and AMR-side, respectively, and (e,g,a) as a triple instance. For German AMR parsing (i.e., G → A), we regard English AMR parsing (i.e.,\nE → A) as its teacher and assume that the probability of generating a target AMR token ai from g should be close to that from its counterpart e, given the already obtained partial AMR a<i. On this assumption, the student model can acquire knowledge from the teacher by applying word-level knowledge distillation for multi-class cross-entropy with the following joint training objective:\nJ (θG→A) =∑ (e,g,a) J ( e,g,a, θ̂E→A, θG→A ) + LθG→A (a | g) , (1)\nwhere (e,g,a) ∈ DE,G,A, i.e., ( FEN ,FDE ,FAMR ) , the fine-tuning data for English/German AMR parsing, θ̂E→A denotes the already learned model parameters for English AMR parsing,3 and LθG→A (a | g) denotes the log-likelihood function for translating g into a. The function J in Eq. 1 is defined as:\nJ ( e,g,a, θ̂E→A, θG→A ) =\n|a|∑ i=1 KL ( P (a|e,a<i; θ̂E→A) ‖ P (a|g,a<i; θG→A) )\n= |a|∑ i=1 ∑ a∈Va P (a|e,a<i; θ̂E→A) log P (a|e,a<i; θ̂E→A) P (a|g,a<i; θG→A) ,\n(2)\nwhere KL (· ‖ ·) denotes the KL divergence between two distributions, and Va is the vocabulary set.4\nTo sum up, in MTL fine-tuning we use Eq. 1 as the objective for the fine-tuning task of German AMR parsing while we still use the log-likelihood function for the auxiliary fine-tuning task, i.e., German-to-English translation.\nFine-Tuning for German AMR-to-Text Generation. Considering the fact that the performance of English-to-German translation is also better than that of German AMR-to-text generation, we view English-to-German translation as the teacher and assume that the probability of generating a target German token gi from a should be close to that from its counterpart e, given the already obtained partial German sentence g<i. The joint training objective for German AMR-to-text generation is similar to the aforementioned objective function for German AMR parsing. Due to limited space, we omit definition details of the objective function.\n3The English AMR parser is learned by fine-tuning the pretrained model on fine-tuning tasks of English AMR parsing and English-to-German translation.\n4To avoid overfitting, the method additionally fine-tunes 80K steps on the pre-training dataset at the beginning."
    }, {
      "heading" : "5 Experimentation",
      "text" : "In this section, we report the performance of our approach to AMR parsing and AMR-to-text generation for non-English languages, including German (DE), Spanish (ES), and Italian (IT). The models are pre-trained and fine-tuned on English data and one of either DE, ES, or IT, and are evaluated in the target language."
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "Pre-Training Datasets. For German, we use the WMT14 English-German translation dataset 5 which consists of 3.9M sentence pairs after preprocessing. For Spanish and Italian, we use Europarl parallel datasets,6 which consist of 1.9M English-Spanish and 1.9M English-Italian sentence pairs, respectively. The English sentences of all the datasets are all parsed into AMR graphs via an English AMR parser trained on AMR 2.0 (LDC2017T10) (Appendix A provides more details on the English AMR parser). We merge English, German (Spanish/Italian) sentences and linearized AMRs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 40K (or 30K for both Spanish and Italian) operations.\nIn addition, we also train NMT models to translate English into German, Spanish, and Italian on above parallel datasets with Transformer-big settings (Vaswani et al., 2017). These NMT models will be used in preparing fine-tuning datasets (Appendix B provides more implementation details on the NMT models).\nFine-Tuning Datasets. We use English AMR2.0 which contains 36,521, 1,368, and 1,371 EnglishAMR pairs for training, development, and testing, respectively. We translate the English sentences into German, Spanish, and Italian, respectively. We segment all the tokens into subwords by using the BPE model trained on pre-training datasets.\nPre-Training and Fine-Tuning Model Settings. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017). 7 For simplicity, we use the same hyperparameter settings to train all the models in both pre-training and fine-tuning\n5https://www.statmt.org/wmt14/ translation-task.html\n6https://www.statmt.org/europarl/index. html\n7https://github.com/OpenNMT/OpenNMT-py\nby just following the settings for the Transformerbase model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden state size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.9 and β2 of 0.98. Warm up step, learning rate, dropout rate, and label smoothing epsilon are set to 16000, 2.0, 0.1 and 0.1 respectively. We set the batch size to 4,096 (8,196) in pre-training (finetuning). We pre-train (fine-tune) the models for 250K (10K) steps and save them at every 10K (1K) steps. Finally, we obtain final pre-trained (finetuned) models by averaging the last 10 checkpoints.\nEvaluation. We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sentences from the AMR 2.0, in German, Spanish, Italian, and Chinese. This data is designed for use in cross-lingual AMR research. Following Fan and Gardent (2020), we only evaluate on languages of German, Spanish and Italian where we have training data from EUROPARL. For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002)."
    }, {
      "heading" : "5.2 Baseline Systems",
      "text" : "We compare the performance of our approach against two baseline systems.\nBaselinescratch. To build this baseline system, we directly train models from scratch on the finetuning datasets. Taking German AMR parsing as example, we train the model on its fine-tuning dataset ( FDE,FAMR ) to get Baselinescratch.\nBaselinepre-trained. Rather than training models from scratch, we pre-train the models on largescale silver datasets. Taking German AMR parsing as example, we first pre-train the model on the pretraining dataset, i.e., ( T DE, T AMR ) , then we finetune the pre-trained model on the corresponding fine-tuning dataset, i.e., ( FDE,FAMR ) ."
    }, {
      "heading" : "5.3 Main Results",
      "text" : "Table 2 shows the performance of AMR parsing and AMR-to-text generation for German (DE), Spanish (ES), and Italian (IT).\nFrom the performance comparison of the two baseline approaches, it is not surprising to find out that pre-training on silver datasets is a very effective way to boost performance (Konstas et al., 2017; Xu et al., 2020). By using silver datasets, we obtain improvements of 6.80 ∼ 7.87 Smatch F1, and 6.21 ∼ 10.54 BLEU for parsing and text generation, respectively.\nWith any of our fine-tuning methods, our cross-lingual pre-training approach further improves the performance over the strong baseline Baselinepre-trained in both parsing and generation tasks over all languages. It shows that like other fine-tuning methods, vanilla fine-tuning significantly boosts the performance of both parsing and generation. However, it still underperforms any of the MTL fine-tuning methods. This confirms that it is important to optimize for high accuracy of a certain fine-tuning task while preserving the performance of other pre-training. The performance comparison between XLPT-AMRone4all and XLPT-AMRtargeted suggests that selectively choosing relevant fine-tuning tasks, rather than including all fine-tuning tasks, could further boost parsing and generation performance with the exception of Spanish generation task.\nThe XLPT-AMRT-S models perform the best, which reveals that using the teacher-student framework to guide the decoding process also helps the student task. This is owing to fact that the teacher\nmodels achieve better performance than the student models. See more in Section 5.4 for performance comparison of teacher and student models.\nFinally, we compare our approach to the previous studies. Among them, both Blloshmi et al. (2020) and Fan and Gardent (2020) adopt pretrained models which cover either the encoder part, or the decoder part. From the results we can see even our baseline Baselinepre-trained outperforms them by pre-training the encoder and the decoder simultaneously. The results also show that our XLPT-AMRT-S models greatly advance the state of art. For example, our XLPT-AMRT-S models outperform Sheth et al. (2021) by 3.4∼7.8 Smatch F1 on AMR parsing of the three languages while surpass Fan and Gardent (2020) by around 10 BLEU on AMR-to-text generation.\nTable 3 compares the performance of finegrained metrics for AMR parsing. It shows that our XLPT-AMRT-S models achieve the best performance on all the metrics with the only exception of Concepts for Italian AMR parsing. It shows that like English AMR parsing, all models predict Reentrancies poorly (Szubert et al., 2020). It also demonstrates that Negations is another metric which is hard to predict. In future work, we will pay particular attention to the two metrics."
    }, {
      "heading" : "5.4 Discussion",
      "text" : "In this section, we try to answer the following three questions:\n• First, what is the performance of teacher models when we use teacher models to guide student ones in teacher-student-based MTL finetuning?\n• Second, what is the effect of the two machine translation tasks in pre-training?\n• Third, in our approach we take English as pivot language by taking advantage of large scale English-to-German (or Spanish, Italian) dataset. What is the performance of English AMR parsing and AMT-to-text generation?\nPerformance of teacher models in teacherstudent-based MTL fine-tuning. Table 4 compares the performance of teacher and student models. It shows that the performance of teacher models for English AMR parsing and English-to-X translation is much higher than the counterparts of student models (i.e., Stu.(before) in the table). The table also shows that the student models beneift from receiving guidance from the teachers. For example, while the English AMR parsing model (i.e., the teacher) achieves 78.62 Smatch F1 on the test set, it improves the performance of the German AMR parsing model (i.e., the student) from 68.31 Smatch F1 to 70.45. Similarly, while the Englishto-German model (i.e., the teacher) achieves 39.40 BLEU on the test set, it boosts the performance of the German AMR-to-text generation model (i.e., the student) from 24.15 BLEU to 25.69.\nEffect of machine translation tasks in pretraining. We use German as a representative.\nNote that when machine translation tasks are not involved in pre-training, the targeted MTL finetuning method is not applicable since we cannot use machine translation as the auxiliary task. Therefore, we use the vanilla fine-tuning method to finetune the pre-trained models. Table 5 compares the performance with/without machine translation tasks in pre-training. From it, we observe that including machine translation tasks in pre-training achieves improvements of 2.77 Smatch F1 and 2.46 BLEU on German AMR parsing and text generation, respectively. This suggests the necessity to have machine translation tasks in pre-training.\nPerformance of English AMR parsing and AMR-to-Text generation. Based on the pretrained models, we take the targeted MTL finetuning method (Section 4.3) as a representative. Specifically, for English AMR parsing, we choose English-to-X (X ∈ {German, Spanish, Italian}) as the auxiliary fine-tuning task while for English test generation, we choose X-to-English as the auxiliary task.\nTable 6 shows that the performance of English parsing and generation is much higher than that of other languages. Moreover, we find that the results of English AMR parsing are quite close when combining English with any of other languages whereas the results of English AMR-to-text generation are considerably different. One possible reason for the phenomenon is that English AMR-to-text generation is relevant to the sizes of machine translation datasets used in pre-training (i.e., 3.9M for EN-DE translation whereas 1.9M for both EN-ES and ENIT, respectively) while English parsing seems to be less affected by the sizes of (silver) datasets. It indicates that with more English sentences in pretraining, it helps the generation models to generate\nmore fluent and correct English sentences."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper we proposed a cross-lingual pretraining approach via multi-task learning for zeroshot AMR parsing and AMR-to-text generation. Upon English AMR dataset and English-to-X parallel datasets, we pre-trained models on three types of relevant tasks, including AMR parsing, AMRto-text generation, and machine translation. We also explored and compared four different finetuning methods. Experimentation on the multilingual AMR dataset shows that our approach greatly advances the state of the art."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Key R&D Program of China under Grant No. 2020AAA0108600 and by the National Natural Science Foundation of China under Grant No. 61876120."
    }, {
      "heading" : "A English AMR Parser on AMR 2.0",
      "text" : "Our English AMR parser is learned in a seq2seq framework and trained on AMR2.0, which consists of 36,521 training AMRs, 1,368 development AMRs and 1,371 testing AMRs. We share vocabulary for the input and the output by segmenting tokens into pieces by byte pair encoding (BPE) with 20K merge operations.\nWe use OpenNMT-py as the implementation of Transformer. In model setting, we use Transformer base model setting. We use Adam with β1 = 0.9, β2 = 0.98 for optimization. Batch size, learning rate, warm-up step, and dropout rate are set to 4096, 2.0, 16000 and 0.1 respectively. We train the model for 250K steps on 1 GPUs and save models every 10K steps. Finally, we obtain final model by averaging the last 10 checkpoints.\nThe English AMR parser achieves 73.68 and 73.24 Smatch F1 on the dev and test set, respectively."
    }, {
      "heading" : "B NMT Models for English-to-German, English-to-Spanish, English-to-Italian",
      "text" : "In pre-processing, we tokenize all of MT corpus with Moses scripts.8 Then we segment words into pieces by BPE with 32K (30K) BPE merge operations for EN-DE (both EN-ES and EN-IT). After filtering long and imbalanced pairs, we get 3.9M parallel sentence pairs for EN-DE and 1.9M for both EN-ES and EN-IT.\nWe again use OpenNMT-py as the implementation of Transformer. In model setting, we use Transformer big model setting. We use Adam with β1 = 0.9, β2 = 0.998 for optimization. Batch size, learning rate, warm-up step, and dropout rate are set to 8192, 2.0, 8000 (16000 for both EN-ES and EN-IT) and 0.1, respectively. We train the model for 100K (110K for EN-ES and 150K for EN-IT) steps on 4 GPUs and save models very 5000 steps. For each translation task, we obtain final model by\n8https://github.com/moses-smt/ mosesdecoder\naveraging the last 5 (20 for both EN-ES and EN-IT) checkpoints.\nFor evaluation, we use case-sensitive BLEU measured by multi-bleu script. Table 7 shows the performance of the three translation models on the test sets, i.e., newstest2014 for EN-DE and newstest2009 for both EN-ES and EN-IT."
    } ],
    "references" : [ {
      "title" : "Semantically inspired amr alignment for the Portuguese language",
      "author" : [ "Rafael Anchiêta", "Thiago Pardo." ],
      "venue" : "Proceedings of EMNLP, pages 1595–1600.",
      "citeRegEx" : "Anchiêta and Pardo.,? 2020",
      "shortCiteRegEx" : "Anchiêta and Pardo.",
      "year" : 2020
    }, {
      "title" : "Online back-parsing for AMR-to-text generation",
      "author" : [ "Xuefeng Bai", "Linfeng Song", "Yue Zhang." ],
      "venue" : "Proceedings of EMNLP, pages 1206–1219.",
      "citeRegEx" : "Bai et al\\.,? 2020",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Amr parsing using stack-lstms",
      "author" : [ "Miguel Ballesteros", "Yaser Al-Onaizan." ],
      "venue" : "Proceedings of EMNLP, pages 1269–1275.",
      "citeRegEx" : "Ballesteros and Al.Onaizan.,? 2017",
      "shortCiteRegEx" : "Ballesteros and Al.Onaizan.",
      "year" : 2017
    }, {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguis-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Graph-to-sequence learning using gated graph neural networks",
      "author" : [ "Daniel Beck", "Gholamreza Haffari", "Trevor Cohn." ],
      "venue" : "Proceedings of ACL, pages 273–283.",
      "citeRegEx" : "Beck et al\\.,? 2018",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2018
    }, {
      "title" : "One spring to rule them both: Symmetric amr semantic parsing and generation without a complex pipeline",
      "author" : [ "Michele Bevilacqua", "Rexhina Blloshmi", "Roberto Navigli." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Bevilacqua et al\\.,? 2021",
      "shortCiteRegEx" : "Bevilacqua et al\\.",
      "year" : 2021
    }, {
      "title" : "XL-AMR: Enabling cross-lingual AMR parsing with transfer learning techniques",
      "author" : [ "Rexhina Blloshmi", "Rocco Tripodi", "Roberto Navigli." ],
      "venue" : "Proceedings of EMNLP, pages 2487–2500.",
      "citeRegEx" : "Blloshmi et al\\.,? 2020",
      "shortCiteRegEx" : "Blloshmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Core semantic first: A top-down approach for AMR parsing",
      "author" : [ "Deng Cai", "Wai Lam." ],
      "venue" : "Proceedings of EMNLP, pages 3799–3809.",
      "citeRegEx" : "Cai and Lam.,? 2019",
      "shortCiteRegEx" : "Cai and Lam.",
      "year" : 2019
    }, {
      "title" : "AMR parsing via graph sequence iterative inference",
      "author" : [ "Deng Cai", "Wai Lam." ],
      "venue" : "Proceedings of ACL, pages 1290–1301.",
      "citeRegEx" : "Cai and Lam.,? 2020a",
      "shortCiteRegEx" : "Cai and Lam.",
      "year" : 2020
    }, {
      "title" : "Graph transformer for graph-to-sequence learning",
      "author" : [ "Deng Cai", "Wai Lam." ],
      "venue" : "Proceedings of AAAI, pages 7464–7471.",
      "citeRegEx" : "Cai and Lam.,? 2020b",
      "shortCiteRegEx" : "Cai and Lam.",
      "year" : 2020
    }, {
      "title" : "Smatch: an evaluation metric for semantic feature structure",
      "author" : [ "Shu Cai", "Kevin Knight." ],
      "venue" : "Proceedings of ACL, pages 748–752.",
      "citeRegEx" : "Cai and Knight.,? 2013",
      "shortCiteRegEx" : "Cai and Knight.",
      "year" : 2013
    }, {
      "title" : "Factorising AMR generation through syntax",
      "author" : [ "Kris Cao", "Stephen Clark." ],
      "venue" : "Proceedings of NAACL, pages 2157–2163.",
      "citeRegEx" : "Cao and Clark.,? 2019",
      "shortCiteRegEx" : "Cao and Clark.",
      "year" : 2019
    }, {
      "title" : "A teacher-student framework for zeroresource neural machine translation",
      "author" : [ "Yun Chen", "Yang Liu", "Yong Cheng", "Victor O.K. Li." ],
      "venue" : "Proceedings of ACL, pages 1925–1935.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual abstract meaning representation parsing",
      "author" : [ "Marco Damonte", "Shay B. Cohen." ],
      "venue" : "Proceedings of NAACL, pages 1146–1155.",
      "citeRegEx" : "Damonte and Cohen.,? 2018",
      "shortCiteRegEx" : "Damonte and Cohen.",
      "year" : 2018
    }, {
      "title" : "Structural neural encoders for AMR-to-text generation",
      "author" : [ "Marco Damonte", "Shay B. Cohen." ],
      "venue" : "Proceedings of NAACL, pages 3649–3658.",
      "citeRegEx" : "Damonte and Cohen.,? 2019",
      "shortCiteRegEx" : "Damonte and Cohen.",
      "year" : 2019
    }, {
      "title" : "An incremental parser for abstract meaning representation",
      "author" : [ "Marco Damonte", "Shay B. Cohen", "Giorgio Satta." ],
      "venue" : "Proceedings of EACL, pages 536– 546.",
      "citeRegEx" : "Damonte et al\\.,? 2017",
      "shortCiteRegEx" : "Damonte et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilingual AMR-to-text generation",
      "author" : [ "Angela Fan", "Claire Gardent." ],
      "venue" : "Proceedings of EMNLP, pages 2889–2901.",
      "citeRegEx" : "Fan and Gardent.,? 2020",
      "shortCiteRegEx" : "Fan and Gardent.",
      "year" : 2020
    }, {
      "title" : "Linguistic realisation as machine translation: Comparing different mt models for amr-to-text generation",
      "author" : [ "Thiago Castro Ferreira", "Iacer Calixto", "Sander Wubben", "Emiel Krahmer." ],
      "venue" : "Proceedings of INLG, pages 1–10.",
      "citeRegEx" : "Ferreira et al\\.,? 2017",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2017
    }, {
      "title" : "Generation from abstract meaning representation using tree transducers",
      "author" : [ "Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell." ],
      "venue" : "Proceedings of NAACL, pages 731–739.",
      "citeRegEx" : "Flanigan et al\\.,? 2016",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2016
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL, pages 1426– 1436.",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling source syntax and semantics for neural AMR parsing",
      "author" : [ "Donglai Ge", "Junhui Li", "Muhua Zhu", "Shoushan Li." ],
      "venue" : "Proceedings of IJCAI, pages 4975–4981.",
      "citeRegEx" : "Ge et al\\.,? 2019",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2019
    }, {
      "title" : "AMR dependency parsing with a typed semantic algebra",
      "author" : [ "Jonas Groschwitz", "Matthias Lindemann", "Meaghan Fowlie", "Mark Johnson", "Alexander Koller." ],
      "venue" : "Proceedings of ACL, pages 1831–1841.",
      "citeRegEx" : "Groschwitz et al\\.,? 2018",
      "shortCiteRegEx" : "Groschwitz et al\\.",
      "year" : 2018
    }, {
      "title" : "Better transitionbased AMR parsing with a refined search space",
      "author" : [ "Zhijiang Guo", "Wei Lu." ],
      "venue" : "Proceedings of EMNLP, pages 1712–1722.",
      "citeRegEx" : "Guo and Lu.,? 2018",
      "shortCiteRegEx" : "Guo and Lu.",
      "year" : 2018
    }, {
      "title" : "Densely connected graph convolutional networks for graph-to-sequence learning",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Zhiyang Teng", "Wei Lu." ],
      "venue" : "TACL, 7:297– 312.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Comparing Czech and English AMRs",
      "author" : [ "Jan Hajič", "Ondšová Bojar", "Zdeňka Urešová." ],
      "venue" : "Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64.",
      "citeRegEx" : "Hajič et al\\.,? 2014",
      "shortCiteRegEx" : "Hajič et al\\.",
      "year" : 2014
    }, {
      "title" : "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
      "author" : [ "Hamza Harkous", "Isabel Groves", "Amir Saffari." ],
      "venue" : "Proceedings of COLING, pages 2410–2424.",
      "citeRegEx" : "Harkous et al\\.,? 2020",
      "shortCiteRegEx" : "Harkous et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of EMNLP, pages 1317–1327.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "OpenNMT: Open-source toolkit for neural machine translation",
      "author" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush." ],
      "venue" : "Proceedings of ACL, System Demonstrations, pages 67–72.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural AMR: Sequence-to-sequence models for parsing and generation",
      "author" : [ "Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of ACL, pages 146–157.",
      "citeRegEx" : "Konstas et al\\.,? 2017",
      "shortCiteRegEx" : "Konstas et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Learning without forgetting",
      "author" : [ "Zhizhong Li", "Derek Hoiem." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947.",
      "citeRegEx" : "Li and Hoiem.,? 2018",
      "shortCiteRegEx" : "Li and Hoiem.",
      "year" : 2018
    }, {
      "title" : "AMR parsing as graph prediction with latent alignment",
      "author" : [ "Chunchuan Lyu", "Ivan Titov." ],
      "venue" : "Proceedings of ACL, pages 397–407.",
      "citeRegEx" : "Lyu and Titov.,? 2018",
      "shortCiteRegEx" : "Lyu and Titov.",
      "year" : 2018
    }, {
      "title" : "GPT-too: A language-model-first approach for AMR-to-text generation",
      "author" : [ "Manuel Mager", "Ramón Fernandez Astudillo", "Tahira Naseem", "Md Arafat Sultan", "Young-Suk Lee", "Radu Florian", "Salim Roukos." ],
      "venue" : "Proceedings of ACL, pages 1846–1852.",
      "citeRegEx" : "Mager et al\\.,? 2020",
      "shortCiteRegEx" : "Mager et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural semantic parsing by character-based translation: Experiments with abstract meaning representation",
      "author" : [ "Rik van Noord", "Johan Bos." ],
      "venue" : "Computational Linguistics in the Netherlands Journal, 7:93–108.",
      "citeRegEx" : "Noord and Bos.,? 2017",
      "shortCiteRegEx" : "Noord and Bos.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Ward Todd", "WeiJing Zhu." ],
      "venue" : "Proceedings of ACL, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Addressing the data sparsity issue in neural AMR parsing",
      "author" : [ "Xiaochang Peng", "Chuang Wang", "Daniel Gildea", "Nianwen Xue." ],
      "venue" : "Proceedings of EACL, pages 366–375.",
      "citeRegEx" : "Peng et al\\.,? 2017",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Generating English from abstract meaning representations",
      "author" : [ "Nima Pourdamghani", "Kevin Knight", "Ulf Hermjakob." ],
      "venue" : "Proceedings of INLG, pages 21–25.",
      "citeRegEx" : "Pourdamghani et al\\.,? 2016",
      "shortCiteRegEx" : "Pourdamghani et al\\.",
      "year" : 2016
    }, {
      "title" : "Enhancing AMR-to-text generation with dual graph representations",
      "author" : [ "Leonardo F.R. Ribeiro", "Claire Gardent", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 3183–3194.",
      "citeRegEx" : "Ribeiro et al\\.,? 2019",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating pretrained language models for graph-to-text generation",
      "author" : [ "Leonardo F.R. Ribeiro", "Martin Schmitt", "Hinrich Schütze", "Iryna Gurevych." ],
      "venue" : "Computing Research Repository, arXiv:2007.08426.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of ACL, pages 1715– 1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Bootstrapping multilingual amr with contextual word alignments",
      "author" : [ "Janaki Sheth", "Young-Suk Lee", "Ramón Fernandez Astudillo", "Tahira Naseem", "Radu Florian", "Salim Roukos", "Todd Ward." ],
      "venue" : "Proceedings of EACL, pages 394–404.",
      "citeRegEx" : "Sheth et al\\.,? 2021",
      "shortCiteRegEx" : "Sheth et al\\.",
      "year" : 2021
    }, {
      "title" : "Back-translation as strategy to tackle the lack of corpus in natural language generation from semantic representations",
      "author" : [ "Marco Antonio Sobrevilla Cabezudo", "Simon Mille", "Thiago Pardo." ],
      "venue" : "Proceedings of MSR, pages 94–103.",
      "citeRegEx" : "Cabezudo et al\\.,? 2019",
      "shortCiteRegEx" : "Cabezudo et al\\.",
      "year" : 2019
    }, {
      "title" : "AMR-to-text generation with synchronous node replacement grammar",
      "author" : [ "Linfeng Song", "Xiaochang Peng", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of ACL, pages 7–13.",
      "citeRegEx" : "Song et al\\.,? 2017",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2017
    }, {
      "title" : "Structural information preserving for graph-to-text generation",
      "author" : [ "Linfeng Song", "Ante Wang", "Jinsong Su", "Yue Zhang", "Kun Xu", "Yubin Ge", "Dong Yu." ],
      "venue" : "Proceedings of ACL, pages 7987–7998.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "A graph-to-sequence model for AMRto-text generation",
      "author" : [ "Linfeng Song", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of ACL, pages 1616–1626.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "The role of reentrancies in abstract meaning representation parsing",
      "author" : [ "Ida Szubert", "Marco Damonte", "Shay B. Cohen", "Mark Steedman." ],
      "venue" : "Findings of EMNLP, pages 2198–2207.",
      "citeRegEx" : "Szubert et al\\.,? 2020",
      "shortCiteRegEx" : "Szubert et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N.Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS, pages 5998– 6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Transition-based Chinese amr parsing",
      "author" : [ "Chuan Wang", "Bin Li", "Nianwen Xue." ],
      "venue" : "Proceedings of NAACL, pages 247–252.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Boosting transition-based AMR parsing with refined actions and auxiliary analyzers",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan." ],
      "venue" : "Proceedings of ACL, pages 857–862.",
      "citeRegEx" : "Wang et al\\.,? 2015a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "A transition-based algorithm for AMR parsing",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan." ],
      "venue" : "Proceedings of NAACL, pages 366–375.",
      "citeRegEx" : "Wang et al\\.,? 2015b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust subgraph generation improves abstract meaning representation parsing",
      "author" : [ "Keenon Werling", "Gabor Angeli", "Christoerpher D. Manning." ],
      "venue" : "Proceedings of ACL, pages 982–991.",
      "citeRegEx" : "Werling et al\\.,? 2015",
      "shortCiteRegEx" : "Werling et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving AMR parsing with sequence-to-sequence pre-training",
      "author" : [ "Dongqin Xu", "Junhui Li", "Muhua Zhu", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 2501–2511.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Not an interlingua, but close: Comparison of English AMRs to Chinese and Czech",
      "author" : [ "Nianwen Xue", "Ondšová Bojar", "Jan Hajič", "Martha Palmer", "Zdeňka Urešová", "Xiuhong Zhang." ],
      "venue" : "Proceedings of LREC, pages 1765–1772.",
      "citeRegEx" : "Xue et al\\.,? 2014",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2014
    }, {
      "title" : "Heterogeneous graph transformer for graphto-sequence learning",
      "author" : [ "Shaowei Yao", "Tianming Wang", "Xiaojun Wan." ],
      "venue" : "Proceedings of ACL, pages 7145–7154.",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "AMR parsing as sequence-to-graph transduction",
      "author" : [ "Sheng Zhang", "Xutai Ma", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "Proceedings of ACL, pages 80–94.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Broad-coverage semantic parsing as transduction",
      "author" : [ "Sheng Zhang", "Xutai Ma", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 3786–3798.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Line graph enhanced AMR-to-text generation with mix-order graph attention networks",
      "author" : [ "Yanbin Zhao", "Lu Chen", "Zhi Chen", "Ruisheng Cao", "Su Zhu", "Kai Yu." ],
      "venue" : "Proceedings of ACL, pages 732– 741.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Amr parsing with action-pointer transformer",
      "author" : [ "Jiawei Zhou", "Tahira Naseem", "Ramón Fernandez Astudillo", "Radu Florian." ],
      "venue" : "Proceedings of NAACL, pages 5585–5598.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "AMR parsing with an incremental joint model",
      "author" : [ "Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang Qu", "Ran Li", "Yanhui Gu." ],
      "venue" : "Proceedings of EMNLP, pages 680–689.",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling graph structure in Transformer for better AMR-to-text generation",
      "author" : [ "Jie Zhu", "Junhui Li", "Muhua Zhu", "Longhua Qian", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5459–5468.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 48,
      "context" : "pora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiêta and Pardo, 2020).",
      "startOffset" : 109,
      "endOffset" : 188
    }, {
      "referenceID" : 0,
      "context" : "pora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiêta and Pardo, 2020).",
      "startOffset" : 109,
      "endOffset" : 188
    }, {
      "referenceID" : 53,
      "context" : "In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajič et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 121
    }, {
      "referenceID" : 24,
      "context" : "In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajič et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 121
    }, {
      "referenceID" : 52,
      "context" : "We conjecture that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020).",
      "startOffset" : 206,
      "endOffset" : 223
    }, {
      "referenceID" : 26,
      "context" : "Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help of another relevant yet stronger task.",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help of another relevant yet stronger task.",
      "startOffset" : 51,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013).",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 50,
      "context" : "glish can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al.",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "glish can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al.",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : ", 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 51,
      "context" : ", 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : ", 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al.",
      "startOffset" : 32,
      "endOffset" : 96
    }, {
      "referenceID" : 59,
      "context" : ", 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : ", 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 22,
      "context" : ", 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 58,
      "context" : ", 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 36,
      "context" : ", 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al.",
      "startOffset" : 51,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : ", 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al.",
      "startOffset" : 51,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : ", 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al.",
      "startOffset" : 51,
      "endOffset" : 176
    }, {
      "referenceID" : 52,
      "context" : ", 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al.",
      "startOffset" : 51,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : ", 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al.",
      "startOffset" : 51,
      "endOffset" : 176
    }, {
      "referenceID" : 32,
      "context" : ", 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a).",
      "startOffset" : 54,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : ", 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a).",
      "startOffset" : 54,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 43,
      "context" : "Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 45,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 4,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 14,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 23,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 38,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 60,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 9,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 57,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 44,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 54,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 1,
      "context" : "To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 367
    }, {
      "referenceID" : 33,
      "context" : "By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) .",
      "startOffset" : 104,
      "endOffset" : 193
    }, {
      "referenceID" : 25,
      "context" : "By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) .",
      "startOffset" : 104,
      "endOffset" : 193
    }, {
      "referenceID" : 39,
      "context" : "By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) .",
      "startOffset" : 104,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) .",
      "startOffset" : 104,
      "endOffset" : 193
    }, {
      "referenceID" : 53,
      "context" : "Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajič et al., 2014).",
      "startOffset" : 68,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajič et al., 2014).",
      "startOffset" : 68,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "Furthermore, Fan and Gardent (2020) explore cross-lingual AMR-to-text based on pre-trained cross-lingual language model (XLM) (Lample and Conneau, 2019).",
      "startOffset" : 126,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "Moreover, a nice property of our approach is that for AMR parsing, unlike related studies (Damonte and Cohen, 2018; Blloshmi et al., 2020), we do not need to perform lemmatization, POS tagging, NER, or re-categorization of entities, thus require no language specific toolkits in pre-processing.",
      "startOffset" : 90,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Moreover, a nice property of our approach is that for AMR parsing, unlike related studies (Damonte and Cohen, 2018; Blloshmi et al., 2020), we do not need to perform lemmatization, POS tagging, NER, or re-categorization of entities, thus require no language specific toolkits in pre-processing.",
      "startOffset" : 90,
      "endOffset" : 138
    }, {
      "referenceID" : 47,
      "context" : "Our models are built on the Transformer framework (Vaswani et al., 2017).",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "Related studies (Li and Hoiem, 2018; Xu et al., 2020) have shown that it is important to optimize for high accuracy of a primary",
      "startOffset" : 16,
      "endOffset" : 53
    }, {
      "referenceID" : 52,
      "context" : "Related studies (Li and Hoiem, 2018; Xu et al., 2020) have shown that it is important to optimize for high accuracy of a primary",
      "startOffset" : 16,
      "endOffset" : 53
    }, {
      "referenceID" : 52,
      "context" : "This is also consistent with the fine-tuning tasks designed for English AMR parsing in (Xu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "Inspired by the teacher-student framework (Kim and Rush, 2016; Chen et al., 2017), we propose to solve this problem by using a stronger fine-tuning task to help improve fine-tuning tasks on such noisy data.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the teacher-student framework (Kim and Rush, 2016; Chen et al., 2017), we propose to solve this problem by using a stronger fine-tuning task to help improve fine-tuning tasks on such noisy data.",
      "startOffset" : 42,
      "endOffset" : 81
    }, {
      "referenceID" : 40,
      "context" : "subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 40K (or 30K for both Spanish and Italian) operations.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 47,
      "context" : "late English into German, Spanish, and Italian on above parallel datasets with Transformer-big settings (Vaswani et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sen-",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017).",
      "startOffset" : 77,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017).",
      "startOffset" : 77,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : "For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002).",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "out that pre-training on silver datasets is a very effective way to boost performance (Konstas et al., 2017; Xu et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 52,
      "context" : "out that pre-training on silver datasets is a very effective way to boost performance (Konstas et al., 2017; Xu et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 46,
      "context" : "It shows that like English AMR parsing, all models predict Reentrancies poorly (Szubert et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 101
    } ],
    "year" : 2021,
    "abstractText" : "Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-toX parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation. We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages. With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning. Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art. In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively. We make our code available on github https:// github.com/xdqkid/XLPT-AMR.",
    "creator" : "LaTeX with hyperref"
  }
}