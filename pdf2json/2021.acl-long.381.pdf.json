{
  "name" : "2021.acl-long.381.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "WARP: Word-level Adversarial ReProgramming",
    "authors" : [ "Karen Hambardzumyan", "Hrant Khachatrian", "Jonathan May" ],
    "emails" : [ "mahnerak@yerevann.com,", "hrant@yerevann.com,", "jonmay@isi.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4921–4933\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4921"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language model pretraining has had a tremendous impact on solving many natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). The most popular two approaches take a pretrained model and use a straightforward supervised learning objective. In the first approach, the parameters of the language model are frozen and a task-specific head is trained on top of them (Peters et al., 2018). The second approach fine-tunes all model parameters (Radford et al., 2018). The latter can sometimes yield better results (Peters et al., 2019), while the first one usually offers better stability for smaller datasets. The approach based on frozen features does not require storing task-specific language models.\nA recent alternative is based on so called adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), a technique that adds new weights at every layer of the pretrained language model while the original parameters are kept frozen. This enables a smaller set of task-specific parameters while achieving results comparable to the fine-tuning approach.\nAnother approach of leveraging pretrained language models for downstream tasks, introduced by Radford et al. (2019), provides “task descriptions” without using any labeled examples. GPT3 (Brown et al., 2020) demonstrates impressive few-shot learning performance with priming: by providing the language model a few inputs and outputs (“analogies”) as a context. The language model contextually “learns” from these examples and outputs the answer with a single forward pass without any trainable parameters. These methods, however, require huge language models (1.5B and 175B parameters, respectively).\nThe success of task reformulation-based approaches suggest that language models are capable of solving various natural language processing tasks given a well-crafted prompt. We hypothesize that it is possible to find such prompts. In other words, we can discover extra tokens that, when added to the input, can exploit language model capabilities better than the manually-designed ones.\nIn this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming1. The method is inspired by adversarial reprogramming (Elsayed et al., 2019) — a method of adding adversarial perturbations to an input image that reprograms a pretrained neural network to perform classification on a task other than the one it was originally trained for.\n1Our implementation is publicly available at: https: //github.com/YerevaNN/WARP\nWe show that our method, using up to 25K trainable parameters per task, achieves 81.6 test score on the GLUE Leaderboard, outperforming all the other submissions that use up to three orders of magnitude more trainable parameters. We show that it is possible to inject knowledge into WARP models using manually designed initialization of the prompt, which is especially useful on tasks with a small number of examples. Moreover, WARP shows impressive few-shot performance on two tasks from the SuperGLUE benchmark with just 32 examples, outperforming GPT-3 results. Finally, we discuss the advantages of our method in real-life applications."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Towards Fewer Trainable Parameters",
      "text" : "Jiao et al. (2020) show that knowledge distillation may help reduce the size of their model 7.5 times while almost preserving the performance, but finetuning such models still requires storage of separate task-specific models. As seen in Section 6, this approach does not scale when we want to apply it to many tasks at once.\nAnother approach, called Adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), introduces new task-specific parameters that are added at every layer of the Transformer network. Only these newly initialized weights are trained, which allows separation of general and task-specific knowledge. In contrast, our method does not inject taskspecific knowledge inside the body of the pretrained language model. Instead, it focuses on learning task-specific input-level prompts."
    }, {
      "heading" : "2.2 Task Reformulation",
      "text" : "In GPT-2, Radford et al. (2019) introduce a completely unsupervised way for transferring knowledge to downstream tasks by reformulating various natural language understanding tasks into language modeling problems. This approach does not make use of the available training examples. Brown et al. (2020) demonstrate an effective fewshot transfer by reformulating downstream tasks into input-output analogies in the context without a need for further fine-tuning. Nonetheless, the number of training examples is limited to the context size and is not scalable to a traditional supervised learning scenario.\nSchick and Schütze (2021b) show the effectiveness of reformulating a number of tasks into Cloze-style tasks by fine-tuning masked language models (Devlin et al., 2019). The method, called Pattern Exploited Training (PET), additionally uses training samples and performs few-shot learning even without huge models such as GPT-3.\nOur method is also based on masked language models, but unlike PET, we focus on finding the best prompt using the training examples. This eliminates the need for manually-designed prompts, however, our method can also benefit from similar prior knowledge about the task by careful initialization of the prompts."
    }, {
      "heading" : "2.3 Adversarial Reprogramming",
      "text" : "Adversarial Reprogramming (Elsayed et al., 2019) demonstrates the reprogramming of pretrained ImageNet classifiers by adding input-level adversarial perturbations to make them perform well on MNIST and CIFAR-10 image classification tasks. The adversarial perturbation is designed to be image padding added to the original input, as illus-\ntrated in Figure 1. Then the perturbation parameter is trained to optimize the target classification task objective using the annotated image data.\nWhile in the case of image classification it is not obvious why adversarial reprogramming should ever work, e.g. why a network trained on ImageNet should have the capacity to solve MNIST when surrounded with a particular bitmap, for NLP tasks, there is more intuition. Many NLP tasks can be reformulated as language models, a shared space for both program and data.\nAdversarial reprogramming has been adapted to text classification tasks with LSTM networks in (Neekhara et al., 2019). They operate in the vocabulary space and reprogram a model trained for one task to perform another task. More recently, AutoPrompt (Shin et al., 2020a) attempts to find prompts for large language models automatically without adding any parameters to the model. Unlike AutoPrompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks (Section 6.2).\nIn a more general sense, guiding an NLP model with special tokens appended to the input is an even older idea. In particular, multilingual neural machine translation models use special tokens in the input to control the target language (Ha et al., 2016; Johnson et al., 2017) or politeness\nof the translation (Sennrich et al., 2016). Another method to reprogram a BERT-based model is proposed by Artetxe et al. (2020), where a model tuned on an English version of a particular task is transformed to work in another language by changing only the embedding matrices.\nIn parallel work, Li and Liang (2021) propose a similar method and successfully apply it on two text generation tasks. Apart from the different types of tasks and our characterization of the task as a form of Adversarial Reprogramming, the main difference between their approach and ours is that they use an additional parameterization trick to stabilize the training."
    }, {
      "heading" : "3 WARP",
      "text" : "We follow a setup similar to Elsayed et al. (2019) with some NLP-specific modifications depicted in Figure 2.\nOur goal is to find the best prompt that will make a pretrained masked language model predict the desired answer (verbalizer token) for a training example’s masked token2. We search for such prompts in the (continuous) embedding space. In other words, we want to find parameters Θ = {ΘP ,ΘV } for prompt and verbalizer embed-\n2This approach can be easily extended to autoregressive language modeling.\ndings, respectively, such that:\nΘ∗ = arg max Θ (− logPΘ(y|x))\nand the probabilities are given by:\nPΘ(y|x) = exp ΘVy f(TΘP (x))∑\ni∈C exp ΘVi f(TΘP (x))\nwhere TΘP (x) is the template that inserts the prompt embeddings ΘP into predefined positions, C is the set of classes, and f(x) is the masked language model output (without the last decoder layer, which is simply the transposed word embedding matrix). Both ΘP and ΘV are vectors in the same embeddings space as the word embeddings.\nIn Figure 2, the template TΘP (x) prepends ΘP1 and appends ΘP2 , Θ P 3 , Θ P 4 parameters to the word embeddings and uses ΘV+ and Θ V − to calculate the probabilities on the masked token position for positive and negative classes."
    }, {
      "heading" : "3.1 Method",
      "text" : "Similar to Elsayed et al. (2019), we employ stochastic gradient descent to find the best adversarial perturbation on the text that will minimize the task objective. First, we insert special prompt tokens [P 1], [P 2], ... [P K] and an additional [MASK] token into the input sequence. These tokens might be placed before or after the sentences, depending on the prompt template.\nWe set the optimization objective to a crossentropy loss between the head output of the masked language model and the verbalizer tokens [V 1], [V 2], ..., [V C] for classes 1...C accordingly.\nThe only trainable parameters are the word embeddings for [P 1], ..., [P K] and [V 1], ... [V C]. In case we want to train models for multiple tasks, these are the only task-specific parameters we need to store. The entire “body” of the large language model (all attention layers, feedforward layers, and all other word embeddings) remains untouched.\nNote that, unlike most adversarial attacks, we do not update the embeddings of the original tokens of the input. This follows the intuition from Elsayed et al. (2019), when the pixels of MNIST or CIFAR images are left untouched, and only padding pixels are updated.\nWe train these parameters by minimizing the loss on the training set of the downstream task."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "WARP is implemented in the AllenNLP framework. For all the GLUE benchmark tasks we use the roberta-large (Liu et al., 2019) model from the PyTorch implementation of huggingface transformers (Wolf et al., 2020) library. For the few-shot experiments, we use albert-xxlarge-v2 in order to directly compare to iPET (Schick and Schütze, 2021b). For the GLUE and SuperGLUE tasks we use dataset loaders and metrics implementations from the huggingface datasets library.\nThe prompt tokens are initialized either with word embeddings of [MASK] or similar to the vectors from the word embedding layer. For the answer prompts, we use the masked language model head, which usually consists of a feedforward network and a decoder on top of it, where the weights of the decoder are shared with the word embeddings used for the input. We calculate the softmax over the verbalizer tokens [V 1], ... [V C].\nWe choose the Adam optimizer with a slanted triangular schedule for the learning rate with 6% warm-up steps and train for 10-20 epochs on each task. Each batch consists of examples containing at most 1024 tokens and 8 examples.\nIn order to speed up the training, we disable the dropout of the pretrained language model. All the experiments are performed on two Titan Vs and two RTX 3080 GPUs, with mixed precision training. In practice, WARP is 2.5-3 times faster than regular fine-tuning and 2 times slower than frozenfeatures experiments in terms of epoch duration with the same batch sizes.\nDetails about the hyperparameters can be found in the Supplementary material."
    }, {
      "heading" : "4 Experiments on GLUE",
      "text" : "Following prior work, we evaluate our method on the GLUE Benchmark (Wang et al., 2019b), which consists of 9 natural language understanding tasks. Generally, we perform single-task WARP training, with early stopping and model selection using the original validation sets, if not stated otherwise."
    }, {
      "heading" : "4.1 Tasks",
      "text" : "Almost all the tasks from the GLUE Benchmark are either sentence classification or sentence pair classification tasks, so WARP requires very few modifications to adapt to each of the tasks.\nSST-2 (Sentence Sentiment Treebank, Socher et al., 2013) is a single sentence binary classification task. For the prompt, we put a [MASK] token after the sentence, and the trainable prompt tokens are both appended and prepended to the sentence.\nCoLA (Corpus of Linguistic Acceptability, Warstadt et al., 2019) is a single sentence classification task as well, so we treat both the same way with the only difference that as a validation metric we use accuracy for SST-2, and Matthew’s correlation for CoLA.\nMNLI (MultiNLI, Multi-Genre Natural Language Inference, Williams et al., 2018), QNLI (Question Natural Language Inference, Rajpurkar et al., 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks. Similar to Schick and Schütze (2021a), we may have prompt tokens before, after and between the two sentences, but the [MASK] token is always put between the sentences. For MNLI, we use matched accuracy as a validation metric and use the same model for the mismatched version. In our few-shot attempt for the RTE task, we use a different training and evaluation setup discussed in Section 5.2. QQP (Quora Question Pairs4) and MRPC (Microsoft Research Paraphrase Corpus, Dolan and Brockett, 2005) follow the same prompt pattern as NLI tasks. As a validation metric F1 score is used.\nSTS-B (Semantic Textual Similarity Bench-\n4https://www.quora.com/q/quoradata/First-QuoraDataset-Release-Question-Pairs\nmark, Cer et al., 2017), unlike the other tasks in the benchmark, is formulated as a regression task. The prompt pattern is the same, but instead of introducing new embeddings for [V 1], [V 2], ..., [V C] verbalizer tokens, we add a regression head to the last hidden state of MLM head and use Mean Squares Error optimization objective, similar to (Liu et al., 2019). Pearson Correlation is used as the validation metric. During inference, we clip the scores within [1, 5].\nWe follow Liu et al. and train models for MRPC, STS-B, and RTE tasks initialized with the parameters from the best MNLI model but do not apply any task-specific tricks to WNLI (Winograd Schema Challenge NLI, Levesque et al., 2011) and always predict the majority label."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 1 presents the results on the test set obtained from the GLUE evaluation server. Besides our best WARP models, we also include the human baselines, current state-of-the-art model (He et al., 2020), the regular fine-tuned pretrained model we use, and also include relatively small language models, including (Jiao et al., 2020), (Clark et al., 2020), (Houlsby et al., 2019).\nWith the GLUE Score, WARP outperforms all the models that train less than 25 million parameters on the leaderboard. We explain the relatively strong WARP results on textual entailment tasks by the easier reformulation of such tasks. Likewise, we explain the relatively weak performance on CoLA by the difficulties of reformulating the\ntask into a Cloze task. To further analyze WARP, we conduct several experiments and focus on dev set results. In order to directly compare WARP with existing methods, we report in Table 2 different methods that use RoBERTa, including fine-tuning, linear classifiers on top, AutoPrompt, and Adapters.5 For WARP experiments, we compare performance with different numbers of prompt tokens.\nThe WARP0 model does not introduce any prompt parameters. The only difference between WARP0 and Linear Classifier is that for WARP0, [MASK] is added to the input of each sample, and we get sentence representations from the MLM head at the masked position. By contrast, in the case of the Linear Classifier, we use the average of non-special token embeddings as sentence representations. As we can see, pooling with MLM is significantly better.\nTable 2 shows that, as we decrease the number of trainable prompt parameters, the performance decreases, but the model still works. Similar behavior was observed by Elsayed et al. (2019) in experiments with different padding parameter sizes. However, in contrast to WARP, the number of trainable parameters in that work are much greater than the size of the input.\nAn important benefit of using WARP is that\n5Unlike in Table 2, Adapters in Table 1 are built on bert-large-uncased model.\nit can be initialized with manual prompts. In addition to the regular models where we initialize with [MASK] tokens, we performed a run on the GLUE datasets with the same prompt [CLS] “S1”? [MASK]. “S2”! [SEP] for all the tasks (without S2 for single-sentence tasks). We denote these results as WARPinit in Table 2. WARPinit outperforms WARP8 on tasks with relatively few training examples — RTE, MRPC and STSB, which indicates its potential in the low-data regime."
    }, {
      "heading" : "5 Few-Shot Experiments",
      "text" : "The fact that WARP can be initialized using manually designed natural prompts suggests that we can similarly benefit from such human attribution similar to iPET (Schick and Schütze, 2021b), especially in scenarios with limited training data."
    }, {
      "heading" : "5.1 Setup",
      "text" : "For our few-shot experiments we build WARP on top of ALBERT (Lan et al., 2020), the same pretrained model used by PET and iPET. To initialize WARP prompts, we use the same Prompt-Verbalizer Patterns (PVP) from Schick and Schütze (2021b): the embeddings for [P 1], [P 2]... [P N] are initialized with PVP’s prompt token embeddings, and embeddings for [V 1], [V 2]... [V C] are initialized with verbalizer token embeddings for their corre-\nsponding classes. Unlike roberta-large, the alberta-xxlarge-v2 uses word embeddings of size 128 (8 times smaller than RoBERTa)."
    }, {
      "heading" : "5.2 Tasks",
      "text" : "In order to compare with GPT-3, PET, and iPET, we use two tasks from FewGLUE (Schick and Schütze, 2021b), which is a few-shot subset of the SuperGLUE benchmark (Wang et al., 2019a) consisting of 32 examples for each task. The dataset also provides 20000 additional unlabeled examples, however, we do not make use of them and work in a purely supervised setup.\nCB: CommitmentBank (de Marneffe et al., 2019) is a textual entailment task which we treat like the other sentence pair classification tasks. To initialize the prompt we use the template [CLS] “h”? [MASK]. “p” [SEP] . We also initialize [V 1], [V 2], [V 3] token embeddings with yes, no and maybe (respectively for entailment, contradiction and neutral).\nRTE: Unlike experiments on the RTE task for the full-sized training in the GLUE benchmark, we do not initialize the model with vectors from MNLI. Instead, the prompt is initialized exactly the same way as in the CB task. The only difference is that we have only the two tokens [V 1] and [V 2] initialized with yes and instead (for entailment and not entailment, respectively)."
    }, {
      "heading" : "5.3 Model Selection",
      "text" : "Although all trainable parameters are manually initialized in this setup, different random seeds can yield different results because of the order the training examples appear during an epoch.\nIn the few-shot setup we cannot access the original validation set. Thus, we disable early stopping and simply pick the last checkpoint.\nIn order to find the best initial learning rate, we conduct 20 runs of WARP with the same learning rate each time by randomly choosing 16 training examples and taking the rest for a development set. We repeat this for all candidate learning rates and choose the one with the best average validation performance across all the random seeds.\nFinally, in order to eliminate the effect of different random seeds, we build an ensemble model from 20 WARP runs using simple majority vote."
    }, {
      "heading" : "5.4 Results",
      "text" : "As seen in Table 3, WARP outperforms PET and GPT-3 baselines, but stays behind iPET on both tasks. GPT-3 has 170B parameters, but none of them is being trained for the given tasks. PET and iPET have 255M parameters, and all of them are trained for these tasks. Additionally, they leverage unlabeled examples using distillation. WARP has roughly the same 255M parameters, but only 1024 of them are trained for any single model. An ensemble of 20 WARP models has slightly more than 20K trainable parameters."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Interpreting tokens learned by WARP",
      "text" : "WARP learns prompt embeddings in a continuous space. In this section, we explore those embeddings by looking at the nearby token vectors. Table 6 in the Supplementary material lists the closest tokens (in terms of cosine similarity) to the learned embeddings. All GLUE tasks are initialized with [MASK] token, except for RTE, MRPC, and STS-B, which are initialized from the pretrained MNLI model. The prompt tokens of the solutions for those three tasks are quite close to the ones from the MNLI solution. We have seen similar behavior on SuperGLUE experiments with manual initializations. The solution for CoLA (which is one of the worst-performing tasks) is close to the initialized point.\nWe do not see any prompt tokens that are meaningful in the context of the tasks. As expected, the verbalized tokens are more interpretable. For\n.\nexample, the embedding for the “contradiction” class of MNLI is close to the token “Unless”. The embeddings for “negative” and “positive” classes of SST-2 task are close to “defective” and “important”, respectively. Other verbalized tokens are non-interpretable (e.g. “470” or word pieces with non-Latin characters)."
    }, {
      "heading" : "6.2 Comparison with AutoPrompt",
      "text" : "AutoPrompt (Shin et al., 2020b) learns a prompt for the given task in the finite space of vocabulary tokens. Their best version uses 3 or 6 prompt tokens and reaches 91.2% accuracy on the development set of SST-2. The search space of WARP is significantly larger, which allows WARP to get better performance with just a single prompt token (93.8%).\nAutoPrompt does not achieve meaningful results on RTE or CB tasks. WARP succeeds on both without manual initialization. Moreover, with manual initialization, WARP gets good performance on both tasks even with just 32 examples (Table 3).\nFigure 4 shows the dependence of the accuracy on SST-2 development set from the number of training samples. Both WARP and AutoPrompt use 10 prompt tokens. With a few hundred training samples or fewer, the difference between the two algorithms is not significant. WARP starts to perform better with more training samples.\nShin et al. (2020b) include results with a manually designed prompt6 which performs pretty well (shown as a dashed line). We also compare with the manually initialized7 version of WARP, which performs very well with just 100 examples."
    }, {
      "heading" : "6.3 Real-world applications",
      "text" : "The importance of NLP systems like WARP can be demonstrated by the following application. Suppose we want to build a system that needs to serve N >> 1 classification tasks simultaneously. Let the number of classes for each task be bounded by C. The system can be based on a large pretrained language model with M parameters, using word embedding size E. How many parameters should the system store in the device memory to be able to serve all N tasks?\nIf we take the approach with frozen features, we can reuse M parameters for all tasks and store additional ECN task-specific parameters. This is optimal in terms of storage but will not perform well. The other extreme is to fine-tune the whole model for each task and store at least MN parameters. Table 4 shows the trade-offs offered by the other solutions. Methods like TinyBERT decrease the number of parameters from MN by only M . WARP, on the other hand, needs to store only M + NE(C + K) parameters, where K is the number of trainable prompt tokens.\n6 SENT. this movie was . as a prompt, and “terrible” and “fantastic” as verbalizer tokens\n7 SENT, and finally, the movie overall was very ! as a prompt, and “good” and “bad” as verbalizer tokens\nIn practice, WARP additionally allows performing inference on inputs for different tasks in parallel, using samples of multiple tasks in the same batch. Every input sentence can be concatenated with task-specific pretrained prompts in advance. Then, the forward pass of the network is identical for all tasks. The final task-specific linear layers can be concatenated to form a single large linear layer with at most NC output neurons.\nThis approach can be especially useful in the systems that provide machine learning models as a service. By storing one copy of a pretrained language model, it is possible to serve a large number of user-specific models in parallel with little overhead."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we have proposed an alternative way to transfer knowledge from large pretrained language models to downstream tasks by appending carefully optimized embeddings to the input text. The method outperforms existing methods with significantly more trainable parameters on GLUE benchmark tasks and shows an impressive performance in a few-shot setting on two SuperGLUE tasks. On the sentiment analysis task, the performance is comparable to the fully fine-tuned language models. This method can save a lot of storage in software applications designed to serve large numbers of sentence classification tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is based in part on research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Air Force Laboratory, DARPA or the U.S. Government.\nThe work was supported by the RA Science Committee, in the frames of the research project No. 20TTAT-AIa024. Most experiments were performed on GPUs donated by NVIDIA."
    }, {
      "heading" : "A Hyperparameters",
      "text" : "For each of the tasks, we performed hyperparameter search in the following space:\n• Learning rate is chosen from the set {10−2, 3 · 10−3, 10−3, 3 · 10−4, 10−4, 3 · 10−5},\n• Number of epochs is chosen as either 10 or 20. This determines the behavior of the slanted triangular learning rate scheduler.\n• Initialization is performed either with the embedding of the [MASK] token, or randomly initialized from a normal distribution, with the mean and variance taken from the matrix of RoBERTa’s word embeddings.\nThe hyperparameter search took roughly 4 days on two Titan V GPUs. The final choices for each task are shown in Table 5. Initialization with [MASK] performed better than the random initialization.\nWe disable all dropouts inside Transformer. We use huggingface implementation of AdamW optimizer with weight decay disabled. The gradient is normalized to the value 1.0. For the batch sampling we use bucketing with padding noise of 0.1. In order to use the device memory more effectively, we also set maximum number of tokens per batch to 2048. The maximum sequence length is truncated to 512 tokens. We enable mixed precision and pad all sequence lengths to the multiples of 8 for the effective usage of TensorCores8."
    }, {
      "heading" : "B Learned Tokens",
      "text" : "Table 6 lists the closest vocabulary words to the learned embeddings. Most tasks have two input sentences, so the prompts consist of three parts: one is added before the first sentence, the second one is added between the sentences and the third one is appended next to the second sentence. For the single-sentence tasks, the second and third parts of the prompt are simply concatenated. Each task has trainable verbalizer tokens, one per output class.\nThe prompts of RTE, MRPC and STS-B are pretty similar to MNLI’s prompts, as the models for these tasks were initialized from pretrained MNLI models. The other tasks were initialized with [MASK] tokens. The final model for CoLA didn’t move too far from its initialization."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "The second PASCAL recognising textual entailment challenge",
      "author" : [ "Roy Bar Haim", "Ido Dagan", "Bill Dolan", "Lisa Ferro", "Danilo Giampiccolo", "Bernardo Magnini", "Idan Szpektor" ],
      "venue" : null,
      "citeRegEx" : "Haim et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Haim et al\\.",
      "year" : 2006
    }, {
      "title" : "The fifth PASCAL recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo", "Bernardo Magnini" ],
      "venue" : null,
      "citeRegEx" : "Bentivogli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "Language models are few-shot",
      "author" : [ "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "E. Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "J. Clark", "Christopher Berner", "Sam McCandlish", "A. Radford", "Ilya Sutskever", "Dario Amodei" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Pre-training transformers as energy-based cloze models",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc Le", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177–",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the International Workshop on Paraphrasing.",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Adversarial reprogramming of neural networks",
      "author" : [ "Gamaleldin F. Elsayed", "Ian Goodfellow", "Jascha Sohl-Dickstein." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Elsayed et al\\.,? 2019",
      "shortCiteRegEx" : "Elsayed et al\\.",
      "year" : 2019
    }, {
      "title" : "The third PASCAL recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Com-",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "Toward multilingual neural machine translation with universal encoder and decoder",
      "author" : [ "Thanh-Le Ha", "Jan Niehues", "Alexander Waibel" ],
      "venue" : null,
      "citeRegEx" : "Ha et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2016
    }, {
      "title" : "DeBERTa: Decodingenhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "N. Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "S. Gelly." ],
      "venue" : "ICML.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "TinyBERT: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "The Winograd schema challenge",
      "author" : [ "Hector J Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, page 47.",
      "citeRegEx" : "Levesque et al\\.,? 2011",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2011
    }, {
      "title" : "PrefixTuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:2101.00190.",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "The CommitmentBank: Investigating projection in naturally occurring discourse",
      "author" : [ "Marie-Catherine de Marneffe", "Mandy Simons", "Judith Tonhauser." ],
      "venue" : "Proceedings of Sinn und Bedeutung, 23(2):107–124.",
      "citeRegEx" : "Marneffe et al\\.,? 2019",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial reprogramming of text classification neural networks",
      "author" : [ "Paarth Neekhara", "Shehzeen Hussain", "Shlomo Dubnov", "Farinaz Koushanfar." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Neekhara et al\\.,? 2019",
      "shortCiteRegEx" : "Neekhara et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "To tune or not to tune? adapting pretrained representations to diverse tasks",
      "author" : [ "Matthew E. Peters", "Sebastian Ruder", "Noah A. Smith." ],
      "venue" : "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14, Flo-",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "AdapterFusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Associ-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2021",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Schick and Schütze.,? 2021a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Schick and Schütze.,? 2021b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Controlling politeness in neural machine translation via side constraints",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Meth-",
      "citeRegEx" : "Shin et al\\.,? 2020a",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Meth-",
      "citeRegEx" : "Shin et al\\.,? 2020b",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP, pages 1631–1642.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Language model pretraining has had a tremendous impact on solving many natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : "Language model pretraining has had a tremendous impact on solving many natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "Language model pretraining has had a tremendous impact on solving many natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "In the first approach, the parameters of the language model are frozen and a task-specific head is trained on top of them (Peters et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "The second approach fine-tunes all model parameters (Radford et al., 2018).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "The latter can sometimes yield better results (Peters et al., 2019), while the first one usually offers better stability for smaller datasets.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "A recent alternative is based on so called adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), a technique that adds new weights at every layer of the pretrained language model while the original parameters are kept frozen.",
      "startOffset" : 52,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "A recent alternative is based on so called adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), a technique that adds new weights at every layer of the pretrained language model while the original parameters are kept frozen.",
      "startOffset" : 52,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "The method is inspired by adversarial reprogramming (Elsayed et al., 2019) — a method of adding adversarial perturbations to an input image that reprograms a pretrained neural network to perform classification on a task other than the one it was originally trained for.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "Another approach, called Adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), introduces new task-specific parameters that are added at every layer of the Transformer network.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Another approach, called Adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), introduces new task-specific parameters that are added at every layer of the Transformer network.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "Schick and Schütze (2021b) show the effectiveness of reformulating a number of tasks into Cloze-style tasks by fine-tuning masked language models (Devlin et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Adversarial Reprogramming (Elsayed et al., 2019) demonstrates the reprogramming of pretrained ImageNet classifiers by adding input-level adversarial perturbations to make them perform well on MNIST and CIFAR-10 image classification tasks.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "Adversarial reprogramming has been adapted to text classification tasks with LSTM networks in (Neekhara et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 29,
      "context" : "More recently, AutoPrompt (Shin et al., 2020a) attempts to find prompts for large language models automatically without adding any parameters to the model.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "In particular, multilingual neural machine translation models use special tokens in the input to control the target language (Ha et al., 2016; Johnson et al., 2017) or politeness of the translation (Sennrich et al.",
      "startOffset" : 125,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : ", 2017) or politeness of the translation (Sennrich et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "For the few-shot experiments, we use albert-xxlarge-v2 in order to directly compare to iPET (Schick and Schütze, 2021b).",
      "startOffset" : 92,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : "Following prior work, we evaluate our method on the GLUE Benchmark (Wang et al., 2019b), which consists of 9 natural language understanding tasks.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : ", 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks.",
      "startOffset" : 16,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : ", 2016) and RTE (Recognizing Textual Entailment, Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) are sentence pair classification tasks.",
      "startOffset" : 16,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "Besides our best WARP models, we also include the human baselines, current state-of-the-art model (He et al., 2020), the regular fine-tuned pretrained model we use, and also include relatively small language models, including (Jiao et al.",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : ", 2020), the regular fine-tuned pretrained model we use, and also include relatively small language models, including (Jiao et al., 2020), (Clark et al.",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 27,
      "context" : "The fact that WARP can be initialized using manually designed natural prompts suggests that we can similarly benefit from such human attribution similar to iPET (Schick and Schütze, 2021b), especially in scenarios with limited training data.",
      "startOffset" : 161,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "For our few-shot experiments we build WARP on top of ALBERT (Lan et al., 2020), the same pretrained model used by PET and iPET.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "In order to compare with GPT-3, PET, and iPET, we use two tasks from FewGLUE (Schick and Schütze, 2021b), which is a few-shot subset of the SuperGLUE benchmark (Wang et al.",
      "startOffset" : 77,
      "endOffset" : 104
    }, {
      "referenceID" : 32,
      "context" : "In order to compare with GPT-3, PET, and iPET, we use two tasks from FewGLUE (Schick and Schütze, 2021b), which is a few-shot subset of the SuperGLUE benchmark (Wang et al., 2019a) consisting of 32 examples for each task.",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 30,
      "context" : "The results for AutoPrompt and fine-tuning are taken from (Shin et al., 2020b)",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "AutoPrompt (Shin et al., 2020b) learns a prompt for the given task in the finite space of vocabulary tokens.",
      "startOffset" : 11,
      "endOffset" : 31
    } ],
    "year" : 2021,
    "abstractText" : "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
    "creator" : "LaTeX with hyperref"
  }
}