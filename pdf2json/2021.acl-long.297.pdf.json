{
  "name" : "2021.acl-long.297.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection",
    "authors" : [ "Lingwei Wei", "Dou Hu", "Wei Zhou", "Zhaojuan Yue", "Songlin Hu" ],
    "emails" : [ "hudou18}@mails.ucas.edu.cn", "husonglin}@iie.ac.cn", "yuezhaojuan@cnic.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3845–3854\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3845"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the ever-increasing popularity of social media sites, user-generated messages can quickly reach a wide audience. However, social media can also enable the spread of false rumor information (Vosoughi et al., 2018). Rumors are now viewed as one of the greatest threats to democracy, journalism, and freedom of expression. Therefore, detecting rumors on social media is highly desirable and socially beneficial (Ahsan et al., 2019).\n* Corresponding author.\nAlmost all the previous studies on rumor detection leverage text content including the source tweet and all user retweets or replies. As time goes on, rumors form their specific propagation structures after being retweeted or replied to. Vosoughi (2015); Vosoughi et al. (2018) have confirmed rumors spread significantly farther, faster, deeper, and more broadly than the truth. They provide the possibility of detecting rumors through the propagation structure. Some works (Ma et al., 2016; Kochkina et al., 2018) typically learn temporal features alone from propagation sequences, ignoring the internal topology. Recent approaches (Ma et al., 2018; Khoo et al., 2020) model the propagation structure as trees to capture structural features. Bian et al. (2020); Wei et al. (2019) construct graphs and aggregate neighbors’ features through edges based on reply or retweet relations.\nHowever, most of them only work well in a narrow scope since they treat these relations as reliable edges for message-passing. As shown in Figure 1, the existence of inaccurate relations brings uncertainty in the propagation structure. The neglect of unreliable relations would lead to severe error accumulation through multi-layer message-passing and limit the learning of effective features.\nWe argue such inherent uncertainty in the propagation structure is inevitable for two aspects: i)\nIn the real world, rumor producers are always wily. They tend to viciously manipulate others to create fake supporting tweets or remove opposing voices to evade detection (Yang et al., 2020). In these common scenarios, relations can be manipulated, which provides uncertainty in the propagation structure. ii) Some annotations of spread relations are subjective and fragmentary (Ma et al., 2017; Zubiaga et al., 2016). The available graph would be a portion of the real propagation structure as well as contain noisy relations, resulting in uncertainty. Therefore, it is very challenging to handle inherent uncertainty in the propagation structure to obtain robust detection results.\nTo alleviate this issue, we make the first attempt to explore the uncertainty in the propagation structure. Specifically, we propose a novel Edgeenhanced Bayesian Graph Convolutional Network (EBGCN) for rumor detection to model the uncertainty issue in the propagation structure from a probability perspective. The core idea of EBGCN is to adaptively control the message-passing based on the prior belief of the observed graph to surrogate the fixed edge weights in the propagation graph. In each iteration, edge weights are inferred by the posterior distribution of latent relations according to the prior belief of node features in the observed graph. Then, we utilize graph convolutional layers to aggregate node features by aggregating various adjacent information on the refining edges. Through the above network, EBGCN can handle the uncertainty in the propagation structure and promote the robustness of rumor detection.\nMoreover, due to the unavailable of missing or inaccurate relations for training the proposed model, we design a new edge-wise consistency training framework. The framework combines unsupervised consistency training on these unlabeled relations into the original supervised training on labeled samples, to promote better learning. We further ensure the consistency between the latent distribution of edges and the distribution of node features in the observed graph by computing KLdivergence between two distributions. Ultimately, both the cross-entropy loss of each claim and the Bayes by Backprop loss of latent relations will be optimized to train the proposed model.\nWe conduct experiments on three real-world benchmark datasets (i.e., Twitter15, Twitter16, and PHEME). Extensive experimental results demonstrate the effectiveness of our model. EBGCN of-\nfers a superior uncertainty representation strategy and boosts the performance for rumor detection. The main contributions of this work are summarized as follows:\n• We propose novel Edge-enhanced Bayesian Graph Convolutional Networks (EBGCN) to handle the uncertainty in a probability manner. To the best of our knowledge, this is the first attempt to consider the inherent uncertainty in the propagation structure for rumor detection.\n• We design a new edge-wise consistency training framework to optimize the model with unlabeled latent relations.\n• Experiments on three real-world benchmark datasets demonstrate the effectiveness of our model on both rumor detection and early rumor detection tasks1."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Rumor Detection",
      "text" : "Traditional methods on rumor detection adopted machine learning classifiers based on handcrafted features, such as sentiments (Castillo et al., 2011), bag of words (Enayet and El-Beltagy, 2017) and time patterns (Ma et al., 2015). Based on salient features of rumors spreading, Wu et al. (2015); Ma et al. (2017) modeled propagation trees and then used SVM with different kernels to detect rumors.\nRecent works have been devoted to deep learning methods. Ma et al. (2016) employed Recurrent Neural Networks (RNN) to sequentially process each timestep in the rumor propagation sequence. To improve it, many researchers captured more long-range dependency via attention mechanisms (Chen et al., 2018), convolutional neural networks (Yu et al., 2017; Chen et al., 2019), and Transformer (Khoo et al., 2020). However, most of them focused on learning temporal features alone, ignoring the internal topology structure.\nTo capture topological-structural features, Ma et al. (2018) presented two recursive neural network (RvNN) based on bottom-up and top-down propagation trees. Yuan et al. (2019); Lu and Li (2020); Nguyen et al. (2020) formulated the propagation structure as graphs. Inspired by Graph Convolutional Network (GCN) (Kipf and Welling, 2017), Bian et al. (2020) first applied two GCNs\n1The source code is available at https://github. com/weilingwei96/EBGCN.\nbased on the propagation and dispersion graphs. Wei et al. (2019) jointly modeled the structural property by GCN and the temporal evolution by RNN.\nHowever, most of them treat the edge as the reliable topology connection for message-passing. Ignoring the uncertainty caused by unreliable relations could lead to lacking robustness and make it risky for rumor detection. Inspired by valuable research (Zhang et al., 2019a) that modeled uncertainty caused by finite available textual contents, this paper makes the first attempt to consider the uncertainty caused by unreliable relations in the propagation structure for rumor detection."
    }, {
      "heading" : "2.2 Graph Neural Networks",
      "text" : "Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Schlichtkrull et al., 2018; Velickovic et al., 2018) have demonstrated remarkable performance in modeling structured data in a wide variety of fields, e.g., text classifcation (Yao et al., 2019), recommendation system (Wu et al., 2019) and emotion recognition (Ghosal et al., 2019). Although promising, they have limited capability to handle uncertainty in the graph structure. While the graphs employed in real-world applications are themselves derived from noisy data or modeling assumptions. To alleviate this issue, some valuable works (Luo et al., 2020; Zhang et al., 2019b) provide an approach for incorporating uncertain graph information by exploiting a Bayesian framework (Maddox et al., 2019). Inspired by them, this paper explores the uncertainty in the propagation structure from a probability perspective, to obtain more robust rumor detection results."
    }, {
      "heading" : "3 Problem Statement",
      "text" : "This paper develops EBGCN which processes text contents and propagation structure of each claim for rumor detection. In general, rumor detection commonly can be regarded as a multi-classification task, which aims to learn a classifier from training claims for predicting the label of a test claim.\nFormally, let C = {c1, c2, ..., cm} be the rumor detection dataset, where ci is the i-th claim and m is the number of claims. For each claim ci = {ri, xi1, xi2, ..., xini−1, G\ni}, Gi indicates the propagation structure, ri is the source tweet, xij refers to the j-th relevant retweet, and ni represents the number of tweets in the claim ci. Specifically, Gi is defined as a propagation graph Gi = 〈Vi, Ei〉\nwith the root node ri (Ma et al., 2018; Bian et al., 2020), where Vi = {ri, xi1, xi2, ..., xini−1} refers to the node set and Ei = {eist|s, t = 0, ..., ni − 1} represent a set of directed edges from a tweet to its corresponding retweets. Denote Ai ∈ Rni×ni as an adjacency matrix where the initial value is\nαst = { 1, if eist ∈ Ei 0, otherwise .\nBesides, each claim ci is annotated with a ground-truth label yi ∈ Y , where Y represents finegrained classes. Our goal is to learn a classifier from the labeled claimed set, that is f : C → Y ."
    }, {
      "heading" : "4 The Proposed Model",
      "text" : "In this section, we propose a novel edge-enhanced bayesian graph convolutional network (EBGCN) for rumor detection in Section 4.2. For better training, we design an edge-wise consistency training framework to optimize EBGCN in Section 4.3."
    }, {
      "heading" : "4.1 Overview",
      "text" : "The overall architecture of EBGCN is shown in Figure 2. Given the input sample including text contents and its propagation structure, we first formulate the propagation structure as directed graphs with two opposite directions, i.e., a top-down propagation graph and a bottom-up dispersion graph. Text contents are embedded by the text embedding layer. After that, we iteratively capture rich structural characteristics via two main components, node update module, and edge inference module. Then, we aggregate node embeddings to generate graph embedding and output the label of the claim.\nFor training, we incorporate unsupervised consistency training on the Bayes by Backprop loss of unlabeled latent relations. Accordingly, we optimize the model by minimizing the weighted sum of the unsupervised loss and supervised loss."
    }, {
      "heading" : "4.2 Edge-enhanced Bayesian Graph Convolutional Networks",
      "text" : ""
    }, {
      "heading" : "4.2.1 Graph Construction and Text Embedding",
      "text" : "The initial graph construction is similar to the previou work (Bian et al., 2020), i.e., build two distinct directed graphs for the propagation structure of each claim ci. The top-down propagation graph and bottom-up dispersion graph are denoted as GTDi and GBUi , respectively. Their corresponding initial adjacency matrices are ATDi = Ai and ABUi = A>i .\nHere, we leave out the superscript i in the following description for better presenting our method.\nThe initial feature matrix of postings in the claim c can be extracted Top-5000 words in terms of TFIDF values, denoted as X = [x0, x1, ..., xn−1] ∈ Rn×d0 , where x0 ∈ Rd0 is the vector of the source tweet and d0 is the dimensionality of textual features. The initial feature matrices of nodes in propagation graph and dispersion graph are the same, i.e., XTD = XBU = X."
    }, {
      "heading" : "4.2.2 Node Update",
      "text" : "Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are able to extract graph structure information and better characterize a node’s neighborhood. They define multiple Graph Conventional Layers (GCLs) to iteratively aggregate features of neighbors for each node and can be formulated as a simple differentiable message-passing framework. Motivated by GCNs, we employ the GCL to update node features in each graph. Formally, node features at the l-th layer H(l) = [h(l)0 ,h (l) 1 , ...,h (l) n−1] can be defined as,\nH(l) = σ(Â (l−1) H(l−1)W(l) + b(l)), (1)\nwhere Â (l−1)\nrepresents the normalization of adjacency matrix A(l−1) (Kipf and Welling, 2017). We initialize node representations by textual features, i.e., H(0) = X."
    }, {
      "heading" : "4.2.3 Edge Inference",
      "text" : "To alleviate the negative effects of unreliable relations, we rethink edge weights based on the cur-\nrently observed graph by adopting a soft connection.\nSpecifically, we adjust the weight between two nodes by computing a transformation fe(·; θt) based on node representations at the previous layer. Then, the adjacency matrix will be updated, i.e.,\ng(l)t = fe ( ‖h(l−1)i − h (l−1) j ‖; θt ) ,\nA(l) = T∑ t=1 σ(W(l)t g (l) t + b (l) t ) · A(l−1).\n(2)\nIn practice, fe(·; θt) consists an convolutional layer and an activation function. T refers to the number of latent relation types. σ(·) refers to a sigmoid function. W(l)t and W (l) t are learnable parameters.\nWe perform share parameters to the edge inference layer in two graphs GTD and GBU . After the stack of transformations in two layers, the model can effectively accumulate a normalized sum of features of the neighbors driven by latent relations, denoted as HTD and HBU ."
    }, {
      "heading" : "4.2.4 Classification",
      "text" : "We regard the rumor detection task as a graph classification problem. To aggregate node representations in the graph, we employ aggregator to form the graph representations. Given the node representations in the propagation graph HTD and the node representations in the dispersion graph HBU , the graph representations can be computed as:\nCTD = meanpooling(HTD), CBU = meanpooling(HBU ), (3)\nwhere meanpooling(·) refers to the mean-pooling aggregating function. Based on the concatenation of two distinct graph representations, label probabilities of all classes can be defined by a full connection layer and a softmax function, i.e.,\nŷ = softmax ( Wc[CTD; CBU ] + bc ) , (4)\nwhere Wc and bc are learnable parameter matrices."
    }, {
      "heading" : "4.3 Edge-wise Consistency Training Framework",
      "text" : "For the supervised learning loss Lc, we compute the cross-entropy of the predictions and ground truth distributions C = {c1, c2, ..., cm}, i.e.,\nLc = − |Y|∑ i yilogŷi, (5)\nwhere yi is a vector representing distribution of ground truth label for the i-th claim sample.\nFor the unsupervised learning loss Le, we amortize the posterior distribution of the classification weight p(ϕ) as q(ϕ) to enable quick prediction at the test stage and learn parameters by minimizing the average expected loss over latent relations, i.e., ϕ∗ = arg minϕ Le, where\nLe = E [ DKL ( p(r̂(l)|H(l−1), G)‖qϕ(r̂(l)|H(l−1), G) )] ,\nϕ∗ = arg max ϕ\nE[log ∫ p(r̂(l)|H(l−1), ϕ)qϕ(ϕ|H(l−1), G)dϕ], (6)\nwhere r̂ is the prediction distribution of latent relations. To ensure likelihood tractably, we model the prior distribution of each latent relation rt, t ∈ [1, T ] independently. For each relation, we define a factorized Gaussian distribution for each latent relation qϕ(ϕ|H(l−1), G; Θ) with means µt and variances δ2t set by the transformation layer,\nqϕ(ϕ|H(l−1), G; Θ)) = T∏ t=1 qϕ(ϕt|{g(l)t }Tt=1)\n= T∏ t=1 N (µt, δ2t ),\nµt = fµ({g(l)t }Tt=1; θµ), δ2t = fδ({g (l) t }\nT t=1; θδ),\n(7)\nwhere fµ(·; θµ) and fδ(·; θµ) refer to compute the mean and variance of input vectors, parameterized by θµ and θδ, respectively. Such that amounts to set the weight of each latent relation.\nBesides, we also consider the likelihood of latent relations when parameterizing the posterior\ndistribution of prototype vectors. The likelihood of latent relations from the l-th layer based on node embeddings can be adaptively computed by,\np(r̂(l)|H(l−1), ϕ) = T∏ t=1 p(r̂(l)t |H(l−1), ϕt),\np(r̂(l)t |H(l−1), ϕt) = exp\n( Wtg (l) t + bt ) ∑T\nt=1 exp ( Wtg (l) t + bt ) . (8)\nIn this way, the weight of edges can be adaptively adjusted based on the observed graph, which can thus be used to effectively pass messages and learn more discriminative features for rumor detection.\nTo sum up, in training, we optimize our model EBGCN by minimizing the cross-entropy loss of labeled claims Lc and Bayes by Backprop loss of unlabeled latent relations Le, i.e.,\nΘ∗ = arg min Θ γLc + (1− γ)Le, (9)\nwhere γ is the trade-off coefficient."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We evaluate the model on three real-world benchmark datasets: Twitter15 (Ma et al., 2017), Twitter16 (Ma et al., 2017), and PHEME (Zubiaga et al., 2016). The statistics are shown in Table 1. Twitter15 and Twitter162 contain 1,490 and 818 claims, respectively. Each claim is labeled as Nonrumor (NR), False Rumor (F), True Rumor (T), or Unverified Rumor (U). Following (Ma et al., 2018; Bian et al., 2020), we randomly split the dataset into five parts and conduct 5-fold cross-validation to obtain robust results. PHEME dataset3 provides 2,402 claims covering nine events and contains three labels, False Rumor (F), True Rumor (T), and Unverified Rumor (U). Following the previous work (Wei et al., 2019), we conduct leave-oneevent-out cross-validation, i.e., in each fold, one event’s samples are used for testing, and all the rest are used for training."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "For Twitter15 and Twitter16, we compare our proposed model with the following methods. DTC\n2https://www.dropbox.com/s/ 7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0\n3https://figshare.com/articles/ dataset/PHEME_dataset_for_Rumour_ Detection_and_Veracity_Classification/ 6392078\n(Castillo et al., 2011) adopted a decision tree classifier based on information credibility. SVM-TS (Ma et al., 2015) leveraged time series to model the chronological variation of social context features via a linear SVM classifier. SVM-TK (Ma et al., 2017) applied an SVM classifier with a propagation tree kernel to model the propagation structure of rumors. GRU-RNN (Ma et al., 2016) employed RNNs to model the sequential structural features. RvNN (Ma et al., 2018) adopted two recursive neural models based on a bottom-up and a top-down propagation tree. StA-PLAN (Khoo et al., 2020) employed transformer networks to incorporate long-distance interactions among tweets with propagation tree structure. BiGCN (Bian et al., 2020) utilized bi-directional GCNs to model bottom-up propagation and top-down dispersion.\nFor PHEME, we compare with several representative state-of-the-art baselines. NileTMRG (Enayet and El-Beltagy, 2017) used linear support vector classification based on bag of words. BranchLSTM (Kochkina et al., 2018) decomposed the propagation tree into multiple branches and adopted a shared LSTM to capture structural features. RvNN (Ma et al., 2018) consisted of two recursive neural networks to model propagation trees. Hierarchical GCN-RNN (Wei et al., 2019) modeled structural property based on GCN and RNN. BiGCN (Bian et al., 2020) consisted of propagation and dispersion GCNs to learn structural features from propagation graph."
    }, {
      "heading" : "5.3 Evaluation Metrics",
      "text" : "For Twitter15 and Twitter16, we follow (Ma et al., 2018; Bian et al., 2020; Khoo et al., 2020) and evaluate the accuracy (Acc.) over four categories and F1 score (F1) on each class. For PHEME, following (Enayet and El-Beltagy, 2017; Kochkina et al., 2018; Wei et al., 2019), we apply the accuracy (Acc.), macro-averaged F1 (mF1) as evaluation metrics. Also, we report the weighted-averaged F1 (wF1) because of the imbalanced class problem."
    }, {
      "heading" : "5.4 Parameter Settings",
      "text" : "Following comparison baselines, the dimension of hidden vectors in the GCL is set to 64. The number of latent relations T and the coefficient weight γ are set to [1, 5] and [0.0, 1.0], respectively. we train the model via backpropagation and a wildly used stochastic gradient descent named Adam (Kingma and Ba, 2015). The learning rate is set to {0.0002, 0.0005, 0.02} for Twitter15, Twitter16, and PHEME, respectively. The training process is iterated upon 200 epochs and early stopping (Yuan et al., 2007) is applied when the validation loss stops decreasing by 10 epochs. The optimal set of hyperparameters are determined by testing the performance on the fold-0 set of Twitter15 and Twitter16, and the class-balanced charlie hebdo event set of PHEME.\nBesides, on PHEME, following (Wei et al., 2019), we replace TF-IDF features with word embeddings by skip-gram with negative sampling (Mikolov et al., 2013) and set the dimension of textual features to 200. We implement this variant of BiGCN and EBGCN, denoted as BiGCN(SKP) and EBGCN(SKP), respectively.\nFor results of baselines, we implement BiGCN according to their public project4 under the same environment. Other results of baselines are referenced from original papers (Khoo et al., 2020; Wei et al., 2019; Ma et al., 2018)."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Performance Comparison with Baselines",
      "text" : "Table 2 shows results of rumor detection on Twitter15, Twitter16, and PHEME datasets. Our proposed model EBGCN obtains the best performance among baselines. Specifically, for Twitter15, EBGCN outperforms state-of-the-art models 2.4% accuracy and 3.6% F1 score of false rumor. For Twitter16, our model obtains 3.4% and 6.0% improvements on accuracy and F1 score of non-rumor, respectively. For PHEME, EBGCN significantly outperforms previous work by 40.2% accuracy, 34.7% mF1 , and 18.0% wF1.\nDeep learning-based (RvNN, StA-PLAN, BiGCN and EBGCN) outperform conventional methods using hand-crafted features (DTC, SVMTS), which reveals the superiority of learning high-level representations for detecting rumors.\n4https://github.com/TianBian95/BiGCN\nTwitter15\nMethod Acc. NR F T U F1 F1 F1 F1\nDTC 45.5 73.3 35.5 31.7 41.5 SVM-TS 54.4 79.6 47.2 40.4 48.3 GRU-RNN 64.1 68.4 63.4 68.8 57.1 SVM-TK 66.7 61.9 66.9 77.2 64.5 RvNN 72.3 68.2 75.8 82.1 65.4 StA-PLAN 85.2 84.0 84.6 88.4 83.7 BiGCN 87.1 86.0 86.7 91.4 85.4 EBGCN 89.2 86.9 89.7 93.4 86.7\nTwitter16\nMoreover, compared with sequence-based models GRU-RNN, and StA-PLAN, EBGCN outperform them. It can attribute that they capture temporal features alone but ignore internal topology structures, which limit the learning of structural features. EBGCN can aggregate neighbor features in the graph to learn rich structural features.\nFurthermore, compared with state-of-the-art graph-based BiGCN, EBGCN also obtains better performance. We discuss the fact for two main reasons. First, BiGCN treats relations among tweet nodes as reliable edges, which may introduce inaccurate or irrelevant features. Thereby their performance lacks robustness. EBGCN considers the inherent uncertainty in the propagation structure. In the model, the unreliable relations can be refined\nin a probability manner, which boosts the bias of express uncertainty. Accordingly, the robustness of detection is enhanced. Second, the edge-wise consistency training framework ensures the consistency between uncertain edges and the current nodes, which is also beneficial to learn more effective structural features for rumor detection.\nBesides, EBGCN(SKP) and BiGCN(SKP) outperforms EBGCN and BiGCN that use TF-IDF features in terms of Acc. and wF1. It shows the superiority of word embedding to capture textual features. Our model consistently obtains better performance in different text embedding. It reveals the stability of EBGCN."
    }, {
      "heading" : "6.2 Model Analysis",
      "text" : "In this part, we further evaluate the effects of key components in the proposed model.\nThe Effect of Edge Inference. The number of latent relation types T is a critical parameter in the edge inference module. Figure 3(a) shows the accuracy score against T . The best performance is obtained when T is 2, 3, and 4 on Twitter15, Twitter16, and PHEME, respectively. Besides, these best settings are different. An idea explanation is that complex relations among tweets are various in different periods and gradually tend to be more sophisticated in the real world with the development\nof social media. The edge inference module can adaptively refine the reliability of these complex relations by the posterior distribution of latent relations. It enhances the bias of uncertain relations and promotes the robustness of rumor detection.\nThe Effect of Unsupervised Relation Learning Loss. The trade-off parameter γ controls the effect of the proposed edge-wise consistency training framework. γ = 0.0 means this framework is omitted. The right in Figure 3 shows the accuracy score against γ. When this framework is removed, the model gains the worst performance. The optimal γ is 0.4, 0.3, and 0.3 on Twitter15, Twitter16, and PHEME, respectively. The results proves the effectiveness of this framework. Due to wily rumor producers and limited annotations of spread information, it is common and inevitable that datasets contains unreliable relations. This framework can ensure the consistency between edges and the corresponding node pairs to avoid the negative features."
    }, {
      "heading" : "6.3 Early Rumor Detection",
      "text" : "Rumor early detection is to detect a rumor at its early stage before it wide-spreads on social media so that one can take appropriate actions earlier. It is especially critical for a real-time rumor detection system. To evaluate the performance on rumor early detection, we follow (Ma et al., 2018) and control the detection deadline or tweet count since the source tweet was posted. The earlier the detec-\ntion deadline or the less the tweet count, the less propagation information can be available.\nFigure 4 shows the performance of early rumor detection. First, all models climb as the detection deadline elapses or tweet count increases. Particularly, at each deadline or tweet count, our model EBGCN reaches a relatively high accuracy score than other comparable models.\nSecond, compared with RvNN that captures temporal features alone and STM-TK based on handcrafted features, the superior performance of EBGCN and BiGCN that explored rich structural features reveals that structural features are more beneficial to the early detection of rumors.\nThird, EBGCN obtains better early detection results than BiGCN. It demonstrates that EBGCN can learn more conducive structural features to identify rumors by modeling uncertainty and enhance the robustness for early rumor detection.\nOverall, our model not only performs better longterm rumor detection but also boosts the performance of detecting rumors at an early stage."
    }, {
      "heading" : "6.4 The Case Study",
      "text" : "In this part, we perform the case study to show the existence of uncertainty in the propagation structure and explain why EBGCN performs well. We randomly sample a false rumor from PHEME, as depicted in Figure 5. The tweets are formulated as nodes and relations are modeled as edges in the graph, where node 1 refers to the source tweet and node 2-8 refer to the following retweets.\nAs shown in the left of Figure 5, we observe that tweet 5 is irrelevant with tweet 1 although replying, which reveals the ubiquity of unreliable relations among tweets in the propagation structure and it is reasonable to consider the uncertainty caused by these unreliable relations.\nRight of Figure 5 indicates constructed graphs where the color shade indicates the value of edge weights. The darker the color, the greater the edge weight. The existing graph-based models always generate the representation of node 1 by aggregating the information of its all neighbors (node 2, 5, and 6) according to seemingly reliable edges. However, edge between node 1 and 5 would bring noise features and limit the learning of useful features for rumor detection. Our model EBGCN successfully weakens the negative effect of this edge by both the edge inference layer under the ingenious edge-wise consistency training framework. Accordingly, the\nmodel is capable of learning more conducive characteristics and enhances the robustness of results."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we have studied the uncertainty in the propagation structure from a probability perspective for rumor detection. Specifically, we propose Edge-enhanced Bayesian Graph Convolutional Networks (EBGCN) to handle uncertainty with a Bayesian method by adaptively adjusting weights of unreliable relations. Besides, we design an edge-wise consistency training framework incorporating unsupervised relation learning to enforce the consistency on latent relations. Extensive experiments on three commonly benchmark datasets have proved the effectiveness of modeling uncertainty in the propagation structure. EBGCN significantly outperforms baselines on both rumor detection and early rumor detection tasks."
    } ],
    "references" : [ {
      "title" : "Rumors detection, verification and controlling mechanisms in online social networks: A survey",
      "author" : [ "Mohammad Ahsan", "Madhu Kumari", "T.P. Sharma." ],
      "venue" : "Online Soc. Networks Media, 14.",
      "citeRegEx" : "Ahsan et al\\.,? 2019",
      "shortCiteRegEx" : "Ahsan et al\\.",
      "year" : 2019
    }, {
      "title" : "Rumor detection on social media with bi-directional graph convolutional networks",
      "author" : [ "Tian Bian", "Xi Xiao", "Tingyang Xu", "Peilin Zhao", "Wenbing Huang", "Yu Rong", "Junzhou Huang." ],
      "venue" : "AAAI, pages 549– 556. AAAI Press.",
      "citeRegEx" : "Bian et al\\.,? 2020",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2020
    }, {
      "title" : "Information credibility on twitter",
      "author" : [ "Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete." ],
      "venue" : "WWW, pages 675–684. ACM.",
      "citeRegEx" : "Castillo et al\\.,? 2011",
      "shortCiteRegEx" : "Castillo et al\\.",
      "year" : 2011
    }, {
      "title" : "2018. Call attention to rumors: Deep attention based recurrent neural networks for early rumor detection",
      "author" : [ "Tong Chen", "Xue Li", "Hongzhi Yin", "Jun Zhang" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention-residual network with CNN for rumor detection",
      "author" : [ "Yixuan Chen", "Jie Sui", "Liang Hu", "Wei Gong." ],
      "venue" : "CIKM, pages 1121–1130. ACM.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Niletmrg at semeval-2017 task 8: Determining rumour and veracity support for rumours on twitter",
      "author" : [ "Omar Enayet", "Samhaa R. El-Beltagy." ],
      "venue" : "pages 470–474. Association for Computational Linguistics.",
      "citeRegEx" : "Enayet and El.Beltagy.,? 2017",
      "shortCiteRegEx" : "Enayet and El.Beltagy.",
      "year" : 2017
    }, {
      "title" : "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "author" : [ "Deepanway Ghosal", "Navonil Majumder", "Soujanya Poria", "Niyati Chhaya", "Alexander F. Gelbukh." ],
      "venue" : "EMNLP/IJCNLP (1), pages 154–164. Association",
      "citeRegEx" : "Ghosal et al\\.,? 2019",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpretable rumor detection in microblogs by attending to user interactions",
      "author" : [ "Ling Min Serena Khoo", "Hai Leong Chieu", "Zhong Qian", "Jing Jiang." ],
      "venue" : "AAAI, pages 8783–8790. AAAI Press.",
      "citeRegEx" : "Khoo et al\\.,? 2020",
      "shortCiteRegEx" : "Khoo et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "All-in-one: Multi-task learning for rumour verification",
      "author" : [ "Elena Kochkina", "Maria Liakata", "Arkaitz Zubiaga." ],
      "venue" : "COLING, pages 3402–3413. Association for Computational Linguistics.",
      "citeRegEx" : "Kochkina et al\\.,? 2018",
      "shortCiteRegEx" : "Kochkina et al\\.",
      "year" : 2018
    }, {
      "title" : "GCAN: graph-aware co-attention networks for explainable fake news detection on social media",
      "author" : [ "Yi-Ju Lu", "Cheng-Te Li." ],
      "venue" : "ACL, pages 505–514. Association for Computational Linguistics.",
      "citeRegEx" : "Lu and Li.,? 2020",
      "shortCiteRegEx" : "Lu and Li.",
      "year" : 2020
    }, {
      "title" : "Learning from the past: Continual meta-learning with bayesian graph neural networks",
      "author" : [ "Yadan Luo", "Zi Huang", "Zheng Zhang", "Ziwei Wang", "Mahsa Baktashmotlagh", "Yang Yang." ],
      "venue" : "AAAI, pages 5021–5028. AAAI Press.",
      "citeRegEx" : "Luo et al\\.,? 2020",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting rumors from microblogs with recurrent neural networks",
      "author" : [ "Jing Ma", "Wei Gao", "Prasenjit Mitra", "Sejeong Kwon", "Bernard J. Jansen", "Kam-Fai Wong", "Meeyoung Cha." ],
      "venue" : "IJCAI, pages 3818– 3824. IJCAI/AAAI Press.",
      "citeRegEx" : "Ma et al\\.,? 2016",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2016
    }, {
      "title" : "Detect rumors using time series of social context information on microblogging websites",
      "author" : [ "Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong." ],
      "venue" : "CIKM, pages 1751–1754. ACM.",
      "citeRegEx" : "Ma et al\\.,? 2015",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Detect rumors in microblog posts using propagation structure via kernel learning",
      "author" : [ "Jing Ma", "Wei Gao", "Kam-Fai Wong." ],
      "venue" : "ACL (1), pages 708–717. Association for Computational Linguistics.",
      "citeRegEx" : "Ma et al\\.,? 2017",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2017
    }, {
      "title" : "Rumor detection on twitter with tree-structured recursive neural networks",
      "author" : [ "Jing Ma", "Wei Gao", "Kam-Fai Wong." ],
      "venue" : "ACL (1), pages 1980–1989. Association for Computational Linguistics.",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple baseline for bayesian uncertainty in deep learning",
      "author" : [ "Wesley J. Maddox", "Pavel Izmailov", "Timur Garipov", "Dmitry P. Vetrov", "Andrew Gordon Wilson." ],
      "venue" : "NeurIPS, pages 13132–13143.",
      "citeRegEx" : "Maddox et al\\.,? 2019",
      "shortCiteRegEx" : "Maddox et al\\.",
      "year" : 2019
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomás Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "NIPS, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "FANG: leveraging social context for fake news detection using graph representation",
      "author" : [ "Van-Hoang Nguyen", "Kazunari Sugiyama", "Preslav Nakov", "Min-Yen Kan." ],
      "venue" : "CIKM, pages 1165–1174. ACM.",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Sejr Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "ESWC, volume 10843 of Lecture Notes in Computer Science, pages 593–607.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Velickovic", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Velickovic et al\\.,? 2018",
      "shortCiteRegEx" : "Velickovic et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic detection and verification of rumors on twitter",
      "author" : [ "Soroush Vosoughi" ],
      "venue" : null,
      "citeRegEx" : "Vosoughi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vosoughi.",
      "year" : 2015
    }, {
      "title" : "The spread of true and false news online",
      "author" : [ "Soroush Vosoughi", "Deb Roy", "Sinan Aral." ],
      "venue" : "Science, 359(6380):1146–1151.",
      "citeRegEx" : "Vosoughi et al\\.,? 2018",
      "shortCiteRegEx" : "Vosoughi et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling conversation structure and temporal dynamics for jointly predicting rumor stance and veracity",
      "author" : [ "Penghui Wei", "Nan Xu", "Wenji Mao." ],
      "venue" : "EMNLP/IJCNLP (1), pages 4786–4797. Association for Computational Linguistics.",
      "citeRegEx" : "Wei et al\\.,? 2019",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    }, {
      "title" : "False rumors detection on sina weibo by propagation structures",
      "author" : [ "Ke Wu", "Song Yang", "Kenny Q. Zhu." ],
      "venue" : "ICDE, pages 651–662.",
      "citeRegEx" : "Wu et al\\.,? 2015",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    }, {
      "title" : "Session-based recommendation with graph neural networks",
      "author" : [ "Shu Wu", "Yuyuan Tang", "Yanqiao Zhu", "Liang Wang", "Xing Xie", "Tieniu Tan." ],
      "venue" : "AAAI, pages 346–353. AAAI Press.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Rumor detection on social media with graph structured adversarial learning",
      "author" : [ "Xiaoyu Yang", "Yuefei Lyu", "Tian Tian", "Yifei Liu", "Yudong Liu", "Xi Zhang." ],
      "venue" : "IJCAI, pages 1417–1423. ijcai.org.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "AAAI, pages 7370–7377. AAAI Press.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "A convolutional approach for misinformation identification",
      "author" : [ "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang", "Tieniu Tan." ],
      "venue" : "IJCAI, pages 3901–3907.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly embedding the local and global relations of heterogeneous graph for rumor detection",
      "author" : [ "Chunyuan Yuan", "Qianwen Ma", "Wei Zhou", "Jizhong Han", "Songlin Hu." ],
      "venue" : "ICDM, pages 796–805. IEEE.",
      "citeRegEx" : "Yuan et al\\.,? 2019",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2019
    }, {
      "title" : "On early stopping in gradient descent learning",
      "author" : [ "Yao Yuan", "Lorenzo Rosasco", "Andrea Caponnetto." ],
      "venue" : "Constructive Approximation, 26(2):289 – 315.",
      "citeRegEx" : "Yuan et al\\.,? 2007",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2007
    }, {
      "title" : "Reply-aided detection of misinformation via bayesian deep learning",
      "author" : [ "Qiang Zhang", "Aldo Lipani", "Shangsong Liang", "Emine Yilmaz." ],
      "venue" : "WWW, pages 2333–2343. ACM.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bayesian graph convolutional neural networks for semi-supervised classification",
      "author" : [ "Yingxue Zhang", "Soumyasundar Pal", "Mark Coates", "Deniz Üstebay." ],
      "venue" : "AAAI, pages 5829–5836. AAAI Press.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
      "author" : [ "Arkaitz Zubiaga", "Geraldine Wong Sak Hoi", "Maria Liakata", "Rob Procter", "Peter Tolmie." ],
      "venue" : "PLoS ONE, 11(3):e0150989.",
      "citeRegEx" : "Zubiaga et al\\.,? 2016",
      "shortCiteRegEx" : "Zubiaga et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "However, social media can also enable the spread of false rumor information (Vosoughi et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Therefore, detecting rumors on social media is highly desirable and socially beneficial (Ahsan et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Some works (Ma et al., 2016; Kochkina et al., 2018) typically learn temporal features alone from propagation sequences, ignoring the internal topology.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "Some works (Ma et al., 2016; Kochkina et al., 2018) typically learn temporal features alone from propagation sequences, ignoring the internal topology.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "Recent approaches (Ma et al., 2018; Khoo et al., 2020) model the propagation structure as trees to capture structural features.",
      "startOffset" : 18,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Recent approaches (Ma et al., 2018; Khoo et al., 2020) model the propagation structure as trees to capture structural features.",
      "startOffset" : 18,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "They tend to viciously manipulate others to create fake supporting tweets or remove opposing voices to evade detection (Yang et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "ii) Some annotations of spread relations are subjective and fragmentary (Ma et al., 2017; Zubiaga et al., 2016).",
      "startOffset" : 72,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : "ii) Some annotations of spread relations are subjective and fragmentary (Ma et al., 2017; Zubiaga et al., 2016).",
      "startOffset" : 72,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Traditional methods on rumor detection adopted machine learning classifiers based on handcrafted features, such as sentiments (Castillo et al., 2011), bag of words (Enayet and El-Beltagy, 2017) and time patterns (Ma et al.",
      "startOffset" : 126,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : ", 2011), bag of words (Enayet and El-Beltagy, 2017) and time patterns (Ma et al.",
      "startOffset" : 22,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : ", 2011), bag of words (Enayet and El-Beltagy, 2017) and time patterns (Ma et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "To improve it, many researchers captured more long-range dependency via attention mechanisms (Chen et al., 2018), convolutional neural networks (Yu et al.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : ", 2018), convolutional neural networks (Yu et al., 2017; Chen et al., 2019), and Transformer (Khoo et al.",
      "startOffset" : 39,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : ", 2018), convolutional neural networks (Yu et al., 2017; Chen et al., 2019), and Transformer (Khoo et al.",
      "startOffset" : 39,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "Inspired by Graph Convolutional Network (GCN) (Kipf and Welling, 2017), Bian et al.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "Inspired by valuable research (Zhang et al., 2019a) that modeled uncertainty caused by finite available textual contents, this paper makes the first attempt to consider the uncertainty caused by unreliable relations in the propagation structure for rumor detection.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Schlichtkrull et al., 2018; Velickovic et al., 2018) have demonstrated remarkable performance in modeling structured data in a wide variety of fields, e.",
      "startOffset" : 29,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Schlichtkrull et al., 2018; Velickovic et al., 2018) have demonstrated remarkable performance in modeling structured data in a wide variety of fields, e.",
      "startOffset" : 29,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Schlichtkrull et al., 2018; Velickovic et al., 2018) have demonstrated remarkable performance in modeling structured data in a wide variety of fields, e.",
      "startOffset" : 29,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : ", text classifcation (Yao et al., 2019), recommendation system (Wu et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : ", 2019), recommendation system (Wu et al., 2019) and emotion recognition (Ghosal et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "To alleviate this issue, some valuable works (Luo et al., 2020; Zhang et al., 2019b) provide an approach for incorporating uncertain graph information by exploiting a Bayesian framework (Maddox et al.",
      "startOffset" : 45,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "To alleviate this issue, some valuable works (Luo et al., 2020; Zhang et al., 2019b) provide an approach for incorporating uncertain graph information by exploiting a Bayesian framework (Maddox et al.",
      "startOffset" : 45,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : ", 2019b) provide an approach for incorporating uncertain graph information by exploiting a Bayesian framework (Maddox et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Specifically, Gi is defined as a propagation graph Gi = 〈Vi, Ei〉 with the root node ri (Ma et al., 2018; Bian et al., 2020), where Vi = {ri, x1, x2, .",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "Specifically, Gi is defined as a propagation graph Gi = 〈Vi, Ei〉 with the root node ri (Ma et al., 2018; Bian et al., 2020), where Vi = {ri, x1, x2, .",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "The initial graph construction is similar to the previou work (Bian et al., 2020), i.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are able to extract graph structure information and better characterize a node’s neighborhood.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "where Â (l−1) represents the normalization of adjacency matrix A(l−1) (Kipf and Welling, 2017).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "We evaluate the model on three real-world benchmark datasets: Twitter15 (Ma et al., 2017), Twitter16 (Ma et al.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : ", 2017), Twitter16 (Ma et al., 2017), and PHEME (Zubiaga et al.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "Following (Ma et al., 2018; Bian et al., 2020), we randomly split the dataset into five parts and conduct 5-fold cross-validation to obtain robust results.",
      "startOffset" : 10,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Following (Ma et al., 2018; Bian et al., 2020), we randomly split the dataset into five parts and conduct 5-fold cross-validation to obtain robust results.",
      "startOffset" : 10,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "Following the previous work (Wei et al., 2019), we conduct leave-oneevent-out cross-validation, i.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "(Castillo et al., 2011) adopted a decision tree classifier based on information credibility.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "SVM-TS (Ma et al., 2015) leveraged time series to model the chronological variation of social context features via a linear SVM classifier.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "SVM-TK (Ma et al., 2017) applied an SVM classifier with a propagation tree kernel to model the propagation structure of rumors.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "GRU-RNN (Ma et al., 2016) employed RNNs to model the sequential structural features.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "RvNN (Ma et al., 2018) adopted two recursive neural models based on a bottom-up and a top-down propagation tree.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "StA-PLAN (Khoo et al., 2020) employed transformer networks to incorporate long-distance interactions among tweets with propagation tree structure.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "BiGCN (Bian et al., 2020) utilized bi-directional GCNs to model bottom-up propagation and top-down dispersion.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "NileTMRG (Enayet and El-Beltagy, 2017) used linear support vector classification based on bag of words.",
      "startOffset" : 9,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "BranchLSTM (Kochkina et al., 2018) decomposed the propagation tree into multiple branches and adopted a shared LSTM to capture structural features.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "RvNN (Ma et al., 2018) consisted of two recursive neural networks to model propagation trees.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "Hierarchical GCN-RNN (Wei et al., 2019) modeled structural property based on GCN and RNN.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "BiGCN (Bian et al., 2020) consisted of propagation and dispersion GCNs to learn structural features from propagation graph.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "For Twitter15 and Twitter16, we follow (Ma et al., 2018; Bian et al., 2020; Khoo et al., 2020) and evaluate the accuracy (Acc.",
      "startOffset" : 39,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "For Twitter15 and Twitter16, we follow (Ma et al., 2018; Bian et al., 2020; Khoo et al., 2020) and evaluate the accuracy (Acc.",
      "startOffset" : 39,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "For Twitter15 and Twitter16, we follow (Ma et al., 2018; Bian et al., 2020; Khoo et al., 2020) and evaluate the accuracy (Acc.",
      "startOffset" : 39,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "For PHEME, following (Enayet and El-Beltagy, 2017; Kochkina et al., 2018; Wei et al., 2019), we apply the accuracy (Acc.",
      "startOffset" : 21,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "For PHEME, following (Enayet and El-Beltagy, 2017; Kochkina et al., 2018; Wei et al., 2019), we apply the accuracy (Acc.",
      "startOffset" : 21,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "For PHEME, following (Enayet and El-Beltagy, 2017; Kochkina et al., 2018; Wei et al., 2019), we apply the accuracy (Acc.",
      "startOffset" : 21,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "we train the model via backpropagation and a wildly used stochastic gradient descent named Adam (Kingma and Ba, 2015).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "The training process is iterated upon 200 epochs and early stopping (Yuan et al., 2007) is applied when the validation loss stops decreasing by 10 epochs.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "Besides, on PHEME, following (Wei et al., 2019), we replace TF-IDF features with word embeddings by skip-gram with negative sampling (Mikolov et al.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : ", 2019), we replace TF-IDF features with word embeddings by skip-gram with negative sampling (Mikolov et al., 2013) and set the dimension of textual features to 200.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "Other results of baselines are referenced from original papers (Khoo et al., 2020; Wei et al., 2019; Ma et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "Other results of baselines are referenced from original papers (Khoo et al., 2020; Wei et al., 2019; Ma et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Other results of baselines are referenced from original papers (Khoo et al., 2020; Wei et al., 2019; Ma et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "To evaluate the performance on rumor early detection, we follow (Ma et al., 2018) and control the detection deadline or tweet count since the source tweet was posted.",
      "startOffset" : 64,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach. Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.",
    "creator" : "LaTeX with hyperref"
  }
}