{
  "name" : "2021.acl-long.379.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling",
    "authors" : [ "Xiang Hu", "Haitao Mi", "Zujie Wen", "Yafang Wang", "Yi Su", "Jing Zheng", "Gerard de Melo" ],
    "emails" : [ "@alibaba-inc.com", "gdm@demelo.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4897–4908\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4897"
    }, {
      "heading" : "1 Introduction",
      "text" : "The idea of devising a structural model of language capable of learning both representations and meaningful syntactic structure without any humanannotated trees has been a long-standing but challenging goal. Across a diverse range of linguistic theories, human language is assumed to possess a recursive hierarchical structure (Chomsky, 1956, 2014; de Marneffe et al., 2006) such that lowerlevel meaning is combined to infer higher-level semantics. Humans possess notions of characters, words, phrases, and sentences, which children naturally learn to segment and combine.\nPretrained language models such as BERT (Devlin et al., 2019) have achieved substantial gains\n∗Equal contribution. 1The code is available at: https://github.com/\nalipay/StructuredLM_RTDT\nacross a range of tasks. However, they simply apply layer-stacking with a fixed depth to increase the modeling power (Bengio, 2009; Salakhutdinov, 2014). Moreover, as the core Transformer component (Vaswani et al., 2017) does not capture positional information, one also needs to incorporate additional positional embeddings. Thus, pretrained language models do not explicitly reflect the hierarchical structure of linguistic understanding.\nInspired by Le and Zuidema (2015), Maillard et al. (2017) proposed a fully differentiable CKY parser to model the hierarchical process explicitly. To make their parser differentiable, they primarily introduce an energy function to combine all possible derivations when constructing each cell representation. However, their model is based on Tree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and requires O(n3) time complexity. Hence, it is hard to scale up to large training data.\nIn this paper, we revisit these ideas, and propose a model applying recursive Transformers along differentiable trees (R2D2). To obtain differentiability, we adopt Gumbel-Softmax estimation (Jang et al., 2017) as an elegant solution. Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps, thanks to a novel pruned tree induction algorithm. As a training objective, the model seeks to recover each word in a sentence given its left and right syntax nodes. Thus, our model does not require any positional embedding and does not need to mask any words during training. Figure 1 presents an example binary tree induced by our method: Without any syntactic supervision, it acquires a model of hierarchical construction from the word-piece level to words, phrases, and finally the sentence level.\nwhat ’ s more , such short - term cat #ac #ly #sms are sur #vi #vable and are no cause for panic selling .\nFigure 1: An example output tree emerging from our proposed method.\nWe make the following contributions: • Our novel CKY-based recursive Transformer on\ndifferentiable trees model is able to learn both representations and tree structure (Section 2.1). • We propose an efficient optimization algorithm\nto scale up our approach to a linear number of composition steps (Section 2.2). • We design an effective pre-training objective,\nwhich predicts each word given its left and right syntactic nodes (Section 2.3). For simplicity and efficiency reasons, in this paper we conduct experiments only on the tasks of language modeling and unsupervised tree induction. The experimental results on language modeling show that our model significantly outperforms baseline models with same parameter size even in fewer training epochs. At unsupervised parsing, our model as well obtains competitive results."
    }, {
      "heading" : "2 Methodology",
      "text" : ""
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "Differentiable Tree. We follow Maillard et al. (2017) in defining a differentiable binary parser using a CKY-style (Cocke, 1969; Kasami, 1966; Younger, 1967) encoder. Informally, given a sentence S = {s1, s2, ..., sn} with n words or wordpieces, Figure 2 shows the chart data structure T , where each cell Ti,j is a tuple 〈ei,j , pi,j , p̃i,j〉, ei,j is a vector representation, pi,j is the probability of a single composition step, and p̃i,j is the probability of the subtree at span [i, j] over sub-string si:j . At\nthe lowest level, we have terminal nodes Ti,i with ei,i initialized as embeddings of inputs si, while pi,i and p̃i,i are set to one. When j > i, the representation ei,j is a weighted sum of intermediate combinations cki,j , defined as:\ncki,j , p k i,j = f(ei,k, ek+1,j) (1) p̃ki,j = p k i,j p̃i,k p̃k+1,j (2)\nαi,j = GUMBEL(log(p̃i,j)) (3)\nei,j = [c i i,j , c i+1 i,j , ..., c j−1 i,j ]αi,j (4) [pi,j , p̃i,j ] = α ᵀ i,j [pi,j , p̃i,j ] (5)\nHere, k is a split point from i to j − 1, f(·) is a composition function that we shall further define later on, pki,j and p̃ k i,j denote the single step combination probability and the subtree probability, respectively, at split point k, pi,j and p̃i,j are the concatenation of all pki,j or p̃ k i,j values, and GUMBEL is the Straight-Through Gumbel-Softmax operation of Jang et al. (2017) with temperature set to one. The [, ] notation denotes stacking of tensors.\nRecursive Transformer. Figure 3 provides a schematic overview of the composition function f(·), comprising N Transformer layers. Taking cki,j and p k i,j as an example, the input is a concatenation of two special tokens [SUM] and [CLS], the left cell ei,k, and the right cell ek+1,j . We also\nadd role embeddings ([LEFT] and [RIGHT]) to the left and right inputs, respectively. Thus, the input consists of four vectors in Rd. We denote as h[SUM], h[CLS], hi,k, hk+1,j ∈ Rd the hidden state of the output of N Transformer layers. This is followed by a linear layer over h[SUM] to obtain\npki,j = σ(Wph[SUM] + bp), (6)\nwhereWp ∈ R1×d, bp ∈ R, and σ refers to sigmoid activation. Then, cki,j is computed as\nwki,j = softmax(Wwh[CLS] + bw) cki,j = [hi,k, hk+1,j ]w k i,j ,\n(7)\nwhere Ww ∈ R2×d with wki,j ∈ R2 capturing the respective weights of the left and right hidden states hi,k and hk+1,j , and the final cki,j is a weighted sum of hi,k and hk+1,j .\nTree Recovery. As the Straight-Through Gumbel-Softmax picks the optimal splitting point k at each cell in practice, it is straightforward to recover the complete derivation tree, Tree(T1,n), from the root node T1,n in a top-down manner recursively."
    }, {
      "heading" : "2.2 Complexity Optimization",
      "text" : "Algorithm 1 Pruned Tree Induction Algorithm Require: T = 2-d array holding cell references Require: m = pruning threshold 1: function PRUNING(T , m) 2: u← FIND (T , m) . Find optimal merge point 3: n← T .len 4: T ′ ← [n− 1][n− 1] . Create a new 2-d array 5: for i ∈ 1 to n− 1 do 6: for j ∈ i to n− 1 do 7: i′ ← i ≥ u+ 1 ? i+ 1 : i 8: j′ ← j ≥ u ? j + 1 : j 9: T ′i,j ← Ti′,j′ . Skip dark gray cells in Fig. 4 10: return T ′ 11: function TREEINDUCTION(T , m) 12: T ′ ← T 13: for t ∈ 1 to T .len− 1 do 14: if t ≥ m then 15: T ′ ← PRUNING (T ′,m) 16: l← min(t+ 1, m) . Clamp the span length 17: for i ∈ 1 to T ′.len− l + 1 do 18: j ← i+ l − 1 19: if T ′i,j is empty then 20: Compute cell T ′i,j with Equation 1 21: return T\nAs the core computation comes from the composition function f(·), our pruned tree induction algorithm aims to reduce the number of composition calls from O(n3) in the original CKY algorithm to linear.\nOur intuition is based on the conjecture that locally optimal compositions are likely to be retained and participate in higher-level feature combination. Specifically, taking T 2 in Figure 4 (c) as an example, we only pick locally optimal nodes from the second row of T 2. If T 24,5 is locally optimal and non-splittable, then all the cells highlighted in dark gray in (d) may be pruned, as they break span [4, 5]. For any later encoding, including higher-level ones, we can merge the nodes and treat T 24,5 as a new non-splittable terminal node (see (e) to (g)).\nAlgorithm 2 Find the best merge point Require: T = 2-d array holding cell references Require: m = pruning threshold 1: function FIND(T , m) 2: n← T .len 3: L ← [n− 1] . Create an array 4: for i ∈ 1 to n− 1 do 5: L[i]← Ti,i+1 . Collect cells on the 2nd row 6: τ ← ∅ 7: for i ∈ 1 to n−m+ 1 do . Iterate to m-th row 8: j = i+m− 1 9: τ ← τ ∪ {c | c ∈ Tree(Ti,j) ∧ c ∈ L} 10: l← new List() 11: for cell x ∈ τ do 12: i← L.index(x) 13: pl ← 1− L[i− 1].p 14: pr ← 1− L[i+ 1].p 15: . If index out of boundary then set to 0 16: l.append(x.p · pl · pr) 17: return argmaxi l[i]\nFigure 4 walks through the steps of processing a sentence of length 6, where si:j denotes a substring from si to sj . Algorithm 1 constructs our chart table T sequentially row-by-row. Let t be the time step and m be the pruning threshold. First, we invoke TREEINDUCTION (T ,m), and compute a row of cells at each time step when t < m as in regular CKY parsing, leading to result (b) in Figure 4. When t ≥ m, we call PRUNING (T ,m) in Line 15. As mentioned, the PRUNING function aims to find the locally optimal combination node in T , prunes some cells, and returns a new table omitting the pruned cells. Algorithm 2 shows how we FIND the locally optimal combination node. Again, the candidate set for the locally optimal node is the second row of T , and we also take advantage of the subtrees derived from all nodes in the m-th row to limit the candidate set. Lines 6 to 9 in Algorithm 2 generate the candidate set. Each candidate must be in the second row of T and also must be used in a subtree of any node in the m-th row. Given the candidate set, we find the least ambiguous one as the optimal selection (Lines 11 to\n17), i.e., the node with maximum own probability while adjacent bi-gram node probabilities (Lines 13 and 14 ) are as low as possible. After selecting the best merge point u, cells in {T ti,j | j = u} ∪ {T ti,j | i = u + 1} are pruned (highlighted in dark gray in (d)), and we generate a new table T t+1 by removing pruned nodes (Lines 4 to 9 in Algorithm 1). Then we obtain (e), and compute the empty cells on them-th row of T 3 to obtain (f). We continue with the loop in Line 13, trigger PRUNING again, and obtain a new table T t+1, and then fill empty cells on the m-th row T t+1. Continuing with the process until all cells are computed, as shown in (g), we finally obtain a discrete chart table as given in (h).\nIn terms of the time complexity, when t ≥ m, there are at most m cells to update, so the complexity of each step is less than O(m2). When t ≤ m, the complexity is O(t3) ≤ O(m2t). Thus, the overall times to call the composition function is O(m2n), which is linear considering m is a constant."
    }, {
      "heading" : "2.3 Pretraining",
      "text" : "Different from the masked language model training of BERT, we directly minimize the sum of all negative log probabilities of all words or word-pieces\nsi given their left and right contexts.\nmin θ n∑ i=1 − log pθ(si | s1:i−1, si+1:n) (8)\nAs shown in Figure 5, after invoking our recursive encoder on a sentence S, we directly use e1,i−1 and ei+1,n as the left and right contexts, respectively, for each word si. To distinguish from the encoding task, the input consists of a concatenation of a special token [MASK], e1,i−1, and ei+1,n. We apply the same composition function f(·) as in Figure 3, and feed h[MASK] through an output softmax to predict the distribution of si over the complete\nvocabulary. Finally, we compute the cross-entropy over the prediction and ground truth distributions.\nIn cases where e1,i−1 or ei+1,n is missing due to the pruning algorithm in Section 2.2, we simply use the left or right longest adjacent non-empty cell. For example, Tx,i−1 means the longest nonempty cell assuming we cannot find any non-empty Tx′,i−1 for all x′ < x. Analogously, Ti+1,y is defined as the longest non-empty right cell. Note that although the final table is sparse, the sentence representation e1,n is always established."
    }, {
      "heading" : "3 Experiments",
      "text" : "As our approach (R2D2) is able to learn both representations and intermediate structure, we evaluate its representation learning ability on bidirectional language modeling and evaluate the intermediate structures on unsupervised parsing."
    }, {
      "heading" : "3.1 Bidirectional Language Modeling",
      "text" : ""
    }, {
      "heading" : "3.1.1 Setup",
      "text" : "Baselines and Evaluation. As the objective of our model is to predict each word with its left and right context, we use the pseudo-perplexity (PPPL) metric of Salazar et al. (2020) to evaluate bidirectional language modeling.\nL(S) = 1 n n∑ i=1 logP (si | s1:i−1, si+1:n, θ)\nPPPL(S) = exp − 1 N N∑ j=1 L(Sj)  PPPL is a bidirectional version of perplexity, establishing a macroscopic assessment of the model’s ability to deal with diverse linguistic phenomena.\nWe compared our approach with SOTA autoencoding and autoregressive language models capable of capturing bidirectional contexts, including BERT, XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020). For a fair apples to apples comparison, all models use the same vocabulary and are trained from scratch on a language modeling corpus. The models are all based on the open source Transformers library2. To compute PPPL for models based on sequential Transformers, for each word si, we only mask si while others remain visible to predict si. When we evaluate our R2D2 model, for each word si, we treat the left s1:i−1 and right si+1:n as two complete sentences separately, then encode them separately, and pick the\n2https://github.com/huggingface/transformers\nroot nodes as the final representations of left and right contexts. In the end, we predict word si by running our Transformers as in Figure 5.\nData. The English language WikiText-2 corpus (Merity et al., 2017) serves as training data. The dataset is split at the sentence level, and sentences longer than 128 after tokenization are discarded (about 0.03% of the original data). The total number of sentences is 68,634, and the average sentence length is 33.4.\nHyperparameters. The tree encoder of our model uses 3-layer Transformers with 768- dimensional embeddings, 3,072-dimensional hidden layer representations, and 12 attention heads. Other models based on the Transformer share the same setting but vary on the number of layers. Training is conducted using Adam optimization with weight decay with a learning rate of 5× 10−5. The batch size is set to 8 for m=8 and 32 for m=4, though we also limit the maximum total length for each batch, such that excess sentences are moved to the next batch. The limit is set to 128 for m=8 and 512 for m=4. It takes about 43 hours for 10 epochs of training with m = 8 and about 9 hours with m=4, on 8 v100 GPUs."
    }, {
      "heading" : "3.1.2 Results and Discussion",
      "text" : "Table 1 presents the results of all models with different parameters. Our model outperforms other models with the same parameter size and number\nof training epochs. These results suggest that our model architecture utilizes the training data more efficiently. Comparing the different pruning thresholdsm=4 andm=8 (last two rows), the two models actually converge to a similar place after 60 epochs, confirming the effectiveness of the pruned tree induction algorithm. We also replace Transformers with Tree-LSTMs as in Jang et al. (2017), denoted as T-LSTM, finding that the perplexity is significantly higher compared to other models.\nThe best score is from the BERT model with 12 layers at epoch 60. Although our model has a linear time complexity, it is still a sequential encoding model, and hence its training time is not comparable to that of fully parallelizable models. Thus, we do not have results of 12-layer Transformers in Table 1. The experimental results comparing models with the same parameter size suggest that our model may perform even better with further deep layers.\nTable 2 shows the training time of our R2D2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning."
    }, {
      "heading" : "3.2 Unsupervised Constituency Parsing",
      "text" : "We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees."
    }, {
      "heading" : "3.2.1 Setup",
      "text" : "Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: BERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation\nmetric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set.\nAs our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-piece level (WP)3. To support word-piece level evaluation, we convert gold trees to wordpiece level trees by simply breaking each terminal node into a non-terminal node with its word-pieces as terminals, e.g., (NN discrepancy) into (NN (WP disc) (WP ##re) (WP ##pan) (WP ##cy). We set the pruning threshold m to 8 for our tree encoder.\nTo support a word-level evaluation, since our model uses word-pieces, we force it to not prune or select spans that conflict with word spans during prediction, and then merge word-pieces into words in the final output. However, note that this constraint is only used for word-level prediction.\nFor training, we use the same hyperparameters as in Section 3.1.1. Our model pretrained on WikiText-2 is finetuned on the training set with the same unsupervised loss objective. For Chinese, we use a subset of Chinese Wikipedia for pretraining, specifically the first 100,000 sentences shorter than 150 characters.\nData. We test our approach on the Penn Treebank (PTB) (Marcus et al., 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work (Kim et al., 2019a), where we discard punctuation and lower-case all tokens. To explore the universality of the model across languages, we also run experiments on Chinese Penn Treebank (CTB) 8 (Xue et al., 2005), on which we also remove punctuation. Note that in all settings, the training is conducted entirely on raw unannotated text."
    }, {
      "heading" : "3.2.2 Results and Discussion",
      "text" : "Table 3 provides the unlabeled F1 scores of all systems on the WSJ and CTB test sets. It is clear that all systems perform better than left/right branching and random trees. Word-level C-PCFG (W) performs best on both the WSJ and CTB test sets when measuring against word-level gold standard trees. Our system performs better than ON-LSTM (W), but worse than DIORA (W) and C-PCFG (W). Still,\n3As DIORA relies on ELMO word embeddings, to support word-piece level inputs, we use BERT word-piece embeddings instead.\nthis is a remarkable result. Note that models such as C-PCFG are specially designed for unsupervised parsing, e.g., adopting 30 nonterminals, 60 preterminals, and a training objective that is well-aligned with unsupervised parsing. In contrast, the objective of our model is that of bi-directional language modeling, and the derived binary trees are merely a by-product of our model that happen to emerge naturally from the model’s preference for structures that are conducive to better language modeling.\nAnother factor is the mismatch between our training and evaluation, where we train our model at the word-piece level, but evaluate against word-level gold trees. For comparison, we thus also considered DIORA (WP), C-PCFG (WP), and our system all trained on word-piece inputs, and evaluated against word-piece level gold trees. The last three lines show the results, with our system achieving the best F1. As breaking words into word-pieces introduces word boundaries as new spans, while word boundaries are easier to recognize, the overall F1 score may increase, especially on Chinese.\nAnalysis. In order to better understand why our model works better when evaluating on word-piece level golden trees, we compute the recall of constituents following Kim et al. (2019b) and Drozdov et al. (2020). Besides standard constituents, we also compare the recall of word-piece chunks and\nproper noun chunks. Proper noun chunks are extracted by finding adjacent unary nodes with same parent and tag NNP.\nTable 4 reports the recall scores for constituents and words on the WSJ and CTB test sets. Our model and DIORA perform better for small semantic units, while C-PCFG better matches larger semantic units such as VP and SBAR. The recall of word chunks (WD) of our system is almost perfect and significantly better than for other algorithms. Please note that all word-piece level models are trained fairly without using any boundary information. Although it is trivial to recognize English word boundaries among word-pieces using rules, this is non-trivial for Chinese. Additionally, the recall of proper noun segments is as well significantly better for our model compared to other algorithms."
    }, {
      "heading" : "3.3 Dependency Tree Compatibility",
      "text" : "We compared examples of trees inferred by our model with the corresponding ground truth constituency trees (see Appendix), encountering reasonable structures that are different from the constituent structure posited by the manually defined gold trees. Experimental results of previous work (Drozdov et al., 2020; Kim et al., 2019a) also show significant variance with different random seeds. Thus, we hypothesize that an isomorphy-focused F1 evaluation with respect to gold constituency trees is insufficient to evaluate how reasonable the induced structures are. In contrast, dependency grammar encodes semantic and syntactic relations directly, and has the best interlingual phrasal cohesion properties (Fox, 2002). Therefore, we introduce dependency compatibility as an additional metric and re-evaluate all system outputs."
    }, {
      "heading" : "3.3.1 Setup",
      "text" : "Baselines and Data. As our approach is a wordpiece level pretrained model, to enable a fair comparison, we train all models on word-pieces and\nlearn models with the same settings as in the original papers. Evaluation at the word-piece level reveals the model’s ability to learn structure from a smaller granularity. In this section, we keep the word-level gold trees unchanged and invoke Stanford CoreNLP (Manning et al., 2014) to convert the WSJ and CTB into dependency trees.\nEvaluation. Our metric is based on the notion of quantifying the compatibility of a tree by counting how many spans comply with dependency relations in the gold dependency tree. Specifically, as illustrated in Figure 6, a span is deemed compatible with the ground truth if and only if this span forms an independent subtree.\nFormally, given a gold dependency tree D, we denote as S(D) the raw token sequence forD. Considering predicting a binary tree for word-level input, predicted spans in the binary tree are denoted as Z . For any span z ∈ Z , the subgraph of D including nodes in z and directional edges between them is referred to as Gz . O(Gz) is defined as the set of nodes with parent nodes not in Gz and I(Gz) denotes the set of nodes whose child nodes are not in Gz . Thus, |O(Gz)| and |I(Gz)| are the outdegree and in-degree of the subgraph Gz . Let I(z) denote whether z is valid, defined as\nI(z) { 1, |O(Gz)| = 1 and I(Gz) ⊆ O(Gz) 0, otherwise. (9)\nFor binary tree spans for word-piece level input, if z breaks word-piece spans, then I(z) = 0. Otherwise, word-pieces are merged to words and the word-level logic is followed. Specifically, to make the results at the word and word-piece levels comparable, I(z) is forced to be zero if z only covers a single word. The final compatibility for Z is∑\nz∈Z I(z)\n|S(D)| − 1 ."
    }, {
      "heading" : "3.3.2 Results and Discussion",
      "text" : "Table 5 lists system results on the WSJ and CTB test sets. %all refers to the accuracy on all test sentences, while %n≤x is the accuracy on sentences with up to xwords. It is clear that the smaller granularity at the word-piece level makes this task harder. Our model performs better than other systems at the word-piece level on both English and Chinese and even outperforms the baselines in many cases at the word level. It is worth noting that the result is evaluated on the same binary predicted trees as we use for unsupervised constituency parsing, yet our model outperforms baselines that perform better in Table 3. One possible interpretation is that our approach learns to prefer structures different from human-defined phrase structure grammar but self-consistent and compatible with a tree structure. To further understand the strengths and weaknesses of each baseline, we analyzed the compatibility of different sentence length ranges. Interestingly, we find that our approach performs better on long sentences compared with C-PCFG at the word-piece level. This shows that a bidirectional language modeling objective can learn to induce accurate structures even on very long sentences, on which custom-tailored methods may not work as well."
    }, {
      "heading" : "4 Related Work",
      "text" : "Pre-trained models. Pre-trained models have achieved significant success across numerous tasks. ELMo (Peters et al., 2018), pretrained on bidirectional language modeling based on bi-LSTMs, was the first model to show significant improvements across many downstream tasks. GPT (Radford et al., 2018) replaces bi-LSTMs with a Transformer (Vaswani et al., 2017). As the global attention mechanism may reveal contextual information, it uses a left-to-right Transformer to predict the next word given the previous context. BERT (Devlin et al., 2019) proposes masked language modeling (MLM) to enable bidirectional modeling while avoiding contextual information leakage by directly masking part of input tokens. As masking input tokens results in missing semantics, XLNET (Yang et al., 2019) proposes permuted language modeling (PLM), where all bi-directional tokens are visible when predicting masked tokens. However, all aforementioned Transformer based models do not naturally capture positional information on their own and do not have explicit interpretable structural information, which is an essential feature of natural language. To alleviate the above shortcomings, we extend pre-training and the Transformer model to structural language models.\nRepresentation with structures. In the line of work on learning a sentence representation with structures, Socher et al. (2011) proposed the first neural network model applying recursive autoencoders to learn sentence representations, but their approach constructs trees in a greedy way, and it is still unclear how autoencoders can perform against large pre-trained models (e.g., BERT). Yogatama et al. (2017) jointly train their shift-reduce parser and sentence embedding components. As their parser is not differentiable, they have to resort to reinforcement training, but the learned structures collapse to trivial left/right branching trees. The work of URNNG (Kim et al., 2019b) applies variational inference over latent trees to perform unsupervised optimization of the RNNG (Dyer et al., 2016), an RNN model that estimates a joint distribution over sentences and trees based on shiftreduce operations. Maillard et al. (2017) propose an alternative approach, based on CKY parsing. The algorithm is made differentiable by using a soft-gating approach, which approximates discrete candidate selection by a probabilistic mixture of the constituents available in a given cell of the chart.\nThis makes it possible to train with backpropagation. However, their model runs in O(n3) and they use Tree-LSTMs."
    }, {
      "heading" : "5 Conclusion and Outlook",
      "text" : "In this paper, we have proposed an efficient CKYbased recursive Transformer to directly model hierarchical structure in linguistic utterances. We have ascertained the effectiveness of our approach on language modeling and unsupervised parsing. With the help of our efficient linear pruned tree induction algorithm, our model quickly learns interpretable tree structures without any syntactic supervision, which yet prove highly compatible with human-annotated trees. As future work, we are investigating pre-training our model on billion word corpora as done for BERT, and fine-tuning our model on downstream tasks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Liqian Sun, the wife of Xiang Hu, for taking care of their newborn baby during critical phases, which enabled Xiang to polish the work and perform experiments."
    }, {
      "heading" : "A Appendix: Tree Examples",
      "text" : "System Tree\nR2D2 when the price of plastics took off in 1987 quantum chemical corp . went along for the ride\nGOLD when the price of plastics took off in 1987 quantum chemical corp. went along for the ride\nR2D2 pricing cycles to be sure are nothing new for plastics producers\nGOLD pricing cycles to be sure are nothing new for plastics producers\nR2D2 we were all wonderful heroes last year says an executive at one of quantum ’ s competitors\nGOLD we were all wonderful heroes last year says an executive at one of quantum ’s competitors\nR2D2 in the u . s . poly ##eth ##yle ##ne market quantum has claimed the largest share about 20 %\nGOLD in the u.s. polyethylene market quantum has claimed the largest share about 20 %\nR2D2 noting others ’ estimates of when price increases can be sustained he remarks some say october\nGOLD noting others ’ estimates of when price increases can be sustained he remarks some say october"
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Yoshua Bengio." ],
      "venue" : "Now Publishers Inc.",
      "citeRegEx" : "Bengio.,? 2009",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "Three models for the description of language",
      "author" : [ "Noam Chomsky." ],
      "venue" : "IRE Trans. Inf. Theory, 2(3):113– 124.",
      "citeRegEx" : "Chomsky.,? 1956",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1956
    }, {
      "title" : "Aspects of the Theory of Syntax, volume 11",
      "author" : [ "Noam Chomsky." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Chomsky.,? 2014",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 2014
    }, {
      "title" : "Programming Languages and Their Compilers: Preliminary Notes",
      "author" : [ "John Cocke." ],
      "venue" : "New York University, USA.",
      "citeRegEx" : "Cocke.,? 1969",
      "shortCiteRegEx" : "Cocke.",
      "year" : 1969
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders",
      "author" : [ "Andrew Drozdov", "Subendhu Rongali", "Yi-Pei Chen", "Tim O’Gorman", "Mohit Iyyer", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 2020 Con-",
      "citeRegEx" : "Drozdov et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Drozdov et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised latent tree induction with deep inside-outside recursive auto-encoders",
      "author" : [ "Andrew Drozdov", "Patrick Verga", "Mohit Yadav", "Mohit Iyyer", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Drozdov et al\\.,? 2019",
      "shortCiteRegEx" : "Drozdov et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Phrasal cohesion and statistical machine translation",
      "author" : [ "Heidi Fox." ],
      "venue" : "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 304–3111. Association for Computational Linguistics.",
      "citeRegEx" : "Fox.,? 2002",
      "shortCiteRegEx" : "Fox.",
      "year" : 2002
    }, {
      "title" : "Grammar induction with neural language models: An unusual replication",
      "author" : [ "Phu Mon Htut", "Kyunghyun Cho", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4998–5003, Brus-",
      "citeRegEx" : "Htut et al\\.,? 2018",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2018
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "An efficient recognition and syntax-analysis algorithm for context-free languages",
      "author" : [ "Tadao Kasami." ],
      "venue" : "Coordinated Science Laboratory Report no. R-257.",
      "citeRegEx" : "Kasami.,? 1966",
      "shortCiteRegEx" : "Kasami.",
      "year" : 1966
    }, {
      "title" : "Compound probabilistic context-free grammars for grammar induction",
      "author" : [ "Yoon Kim", "Chris Dyer", "Alexander Rush." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385, Florence, Italy. Asso-",
      "citeRegEx" : "Kim et al\\.,? 2019a",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised recurrent neural network grammars",
      "author" : [ "Yoon Kim", "Alexander Rush", "Lei Yu", "Adhiguna Kuncoro", "Chris Dyer", "Gábor Melis." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Kim et al\\.,? 2019b",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization",
      "author" : [ "Phong Le", "Willem Zuidema." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Le and Zuidema.,? 2015",
      "shortCiteRegEx" : "Le and Zuidema.",
      "year" : 2015
    }, {
      "title" : "Jointly learning sentence embeddings and syntax with unsupervised tree-lstms",
      "author" : [ "Jean Maillard", "Stephen Clark", "Dani Yogatama." ],
      "venue" : "CoRR, abs/1705.09189.",
      "citeRegEx" : "Maillard et al\\.,? 2017",
      "shortCiteRegEx" : "Maillard et al\\.",
      "year" : 2017
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "Proceedings of 52nd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep learning",
      "author" : [ "Ruslan Salakhutdinov." ],
      "venue" : "The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, page 1973. ACM.",
      "citeRegEx" : "Salakhutdinov.,? 2014",
      "shortCiteRegEx" : "Salakhutdinov.",
      "year" : 2014
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Compu-",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "Ordered neurons: Integrating tree structures into recurrent neural networks",
      "author" : [ "Yikang Shen", "Shawn Tan", "Alessandro Sordoni", "Aaron C. Courville." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Perturbed masking: Parameter-free probing for analyzing and interpreting BERT",
      "author" : [ "Zhiyong Wu", "Yun Chen", "Ben Kao", "Qun Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online. As-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "The penn chinese treebank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer." ],
      "venue" : "Nat. Lang. Eng., 11(2):207–238.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Con-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to compose words into sentences with reinforcement learning",
      "author" : [ "Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France,",
      "citeRegEx" : "Yogatama et al\\.,? 2017",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2017
    }, {
      "title" : "Recognition and parsing of context-free languages in time n3",
      "author" : [ "Daniel H Younger." ],
      "venue" : "Information and control, 10(2):189–208.",
      "citeRegEx" : "Younger.,? 1967",
      "shortCiteRegEx" : "Younger.",
      "year" : 1967
    }, {
      "title" : "Long short-term memory over recursive structures",
      "author" : [ "Xiao-Dan Zhu", "Parinaz Sobhani", "Hongyu Guo." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Work-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Pretrained language models such as BERT (Devlin et al., 2019) have achieved substantial gains",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "ply layer-stacking with a fixed depth to increase the modeling power (Bengio, 2009; Salakhutdinov, 2014).",
      "startOffset" : 69,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "ply layer-stacking with a fixed depth to increase the modeling power (Bengio, 2009; Salakhutdinov, 2014).",
      "startOffset" : 69,
      "endOffset" : 104
    }, {
      "referenceID" : 27,
      "context" : "Moreover, as the core Transformer component (Vaswani et al., 2017) does not capture positional information, one also needs to incorporate additional positional embeddings.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "However, their model is based on Tree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and requires O(n3) time complexity.",
      "startOffset" : 44,
      "endOffset" : 80
    }, {
      "referenceID" : 33,
      "context" : "However, their model is based on Tree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and requires O(n3) time complexity.",
      "startOffset" : 44,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "To obtain differentiability, we adopt Gumbel-Softmax estimation (Jang et al., 2017) as an elegant solution.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "(2017) in defining a differentiable binary parser using a CKY-style (Cocke, 1969; Kasami, 1966; Younger, 1967) encoder.",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "(2017) in defining a differentiable binary parser using a CKY-style (Cocke, 1969; Kasami, 1966; Younger, 1967) encoder.",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 32,
      "context" : "(2017) in defining a differentiable binary parser using a CKY-style (Cocke, 1969; Kasami, 1966; Younger, 1967) encoder.",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "We compared our approach with SOTA autoencoding and autoregressive language models capable of capturing bidirectional contexts, including BERT, XLNet (Yang et al., 2019), and ALBERT (Lan et al.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "The English language WikiText-2 corpus (Merity et al., 2017) serves as training data.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 28,
      "context" : "For comparison, we further include four recent strong models for unsupervised parsing with open source code: BERT masking (Wu et al., 2020), Ordered Neurons (Shen et al.",
      "startOffset" : 122,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : ", 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : ", 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : "We test our approach on the Penn Treebank (PTB) (Marcus et al., 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work (Kim et al.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : ", 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work (Kim et al., 2019a), where we discard punctuation and",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "To explore the universality of the model across languages, we also run experiments on Chinese Penn Treebank (CTB) 8 (Xue et al., 2005), on which we also remove punctuation.",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Experimental results of previous work (Drozdov et al., 2020; Kim et al., 2019a) also show significant variance with different random seeds.",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "Experimental results of previous work (Drozdov et al., 2020; Kim et al., 2019a) also show significant variance with different random seeds.",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "In contrast, dependency grammar encodes semantic and syntactic relations directly, and has the best interlingual phrasal cohesion properties (Fox, 2002).",
      "startOffset" : 141,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "word-level gold trees unchanged and invoke Stanford CoreNLP (Manning et al., 2014) to convert the WSJ and CTB into dependency trees.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "ELMo (Peters et al., 2018), pretrained on bidirectional language modeling based on bi-LSTMs, was the first model to show significant improvements across many downstream tasks.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "GPT (Radford et al., 2018) replaces bi-LSTMs with a Trans-",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "BERT (Devlin et al., 2019) proposes masked language model-",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 30,
      "context" : "As masking input tokens results in missing semantics, XLNET (Yang et al., 2019) proposes permuted language modeling (PLM), where all bi-directional tokens are visi-",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "The work of URNNG (Kim et al., 2019b) applies variational inference over latent trees to perform unsupervised optimization of the RNNG (Dyer et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : ", 2019b) applies variational inference over latent trees to perform unsupervised optimization of the RNNG (Dyer et al., 2016), an RNN model that estimates a joint distribution over sentences and trees based on shiftreduce operations.",
      "startOffset" : 106,
      "endOffset" : 125
    } ],
    "year" : 2021,
    "abstractText" : "Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. This paper proposes a recursive Transformer model based on differentiable CKY style binary trees to emulate the composition process. We extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruned tree induction algorithm to enable encoding in just a linear number of composition steps. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.1",
    "creator" : "LaTeX with hyperref"
  }
}