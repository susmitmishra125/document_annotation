{
  "name" : "2021.acl-long.479.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling",
    "authors" : [ "Xiaoxue Zang", "Lijuan Liu", "Maria Wang", "Yang Song", "Hao Zhang", "Jindong Chen" ],
    "emails" : [ "jdchen}@google.com,", "yangsong@kuaishou.com", "call@1" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6142–6152\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6142"
    }, {
      "heading" : "1 Introduction",
      "text" : "As instant messaging tools gain enormous popularity in the recent decades, sharing photos as an approach to enhance the engagement of an online messaging conversation has become a pervasive routine communicative act (Lobinger, 2016). A survey conducted in 2010 reveals that 74% of teenagers in the US reported messaging a photo or video using their cell phone (Lenhart et al., 2010). In Britain, almost 70% of the internet users shared photos in 2013 (Dutton and Blank, 2013). Considering the proliferation of photo sharing, it’s desirable to have an intelligent system that can assist users efficiently engaging in this process, i.e. suggesting the most relevant photos in correct timings. In order to achieve this goal, the intelligent system is expected to not only understand how humans\n∗Research conducted while working at Google.\ncommunicate with each other, e.g. the natural language human speak, but also perceive images as human do. How to facilitate building such multimodal system is the goal of this paper.\nThough recently many image-text tasks have been proposed and are being actively studied to bridge language and vision, the majority of them are formulated as choosing or composing the text based on the understanding of given images, e.g. image captioning (Anderson et al., 2018), visual question answering (Antol et al., 2015), visual commonsense reasoning (Zellers et al., 2019), and image-grounded dialogue generation (Shuster et al., 2020). Contrary to these tasks, the photo sharing task focuses on the reverse process, i.e. selecting the image based on the understanding of text, as well as proposing different and unique challenges.\nFirstly, different from the above popular multimodal tasks, in photo-sharing task, the dialogue doesn’t often explicitly mention the main visible content in the image. Instead of the main object of the photo, sometimes the background story, complemented by human imaginations, can be the focus of the chat. Figure 1 shows such an example, in which the person who shares the photo describes the event location “court” and the occupation “attorney” instead of the main object “lady” in the image. Secondly, the dialogue is not guaranteed\nto be relevant to the image. For instance, it often contains greetings and chit-chats of other topics, as the first two turns in Figure 1 shows. In order to suggest the relevant photo, a smart system needs to decide which part of the dialogue can be used for suggesting the image. In contrast, in the traditional image-text tasks, the correct text is designed to be highly correlated with the image and has few distracting content. These photo sharing characteristics makes inferring the connection between the image and textual utterances challenging.\nTo highlight these challenges, we create PhotoChat - a human-human dialogue dataset in which one photo is shared from one person to the other during the conversation1. It is, as far as we know, the first dataset that captures the photo sharing activities. We selected images from OpenImage V4 dataset (Kuznetsova et al., 2020) as shared photos and used crowdsourcing plugins to generate 12,286 dialogues with an average of 10 turns per dialogue. During the dialogue collection, the photo is only visible to the side who is instructed to share the photo and then to both sides after it is being shared. Based on the collected dataset, we propose two tasks that are essential for building a photo suggest system: photo-sharing intent prediction task that predicts whether one intends to share the photo in the next conversation turn, and dialogue-based image retrieval task that retrieves the most relevant photo given the dialogue context. For both, we build baseline models, report and analyze their performances. The best photo-sharing intent prediction baseline model achieves 58.1% F1 score with 58.2% precision and 57.9% recall. The best cross-attention image retrieval model achieves 10.4% recall@1 out of 1000 candidates. We also propose a dual-encoder model that leverages object labels to encode image features, which achieves the best performance among all the models w/o cross-attention mechanisms.\nIn summary, our main contributions are: • We create the first human-human dialogue\nwith photo sharing acts via crowd-sourcing. • We propose two new tasks to promote build-\ning an intelligent photo suggest system. • We build baseline models and provide bench-\nmarks for the new tasks. Our proposed image retrieval model outperforms all the prior models w/o cross-attention mechanisms. We im-\n1https://github.com/google-research/googleresearch/tree/master/multimodalchat/\nplement comprehensive analysis and ablation study to provide more insights."
    }, {
      "heading" : "2 Related Work",
      "text" : "With the recent advances in deep learning, plenty of image-text datasets have been created and new image-text tasks are proposed based on them. These datasets have greatly stimulated the development of joint image-text models. In this section, we review the widely used image-text datasets and the state-of-the-art (SOTA) approaches for solving the image-text problems."
    }, {
      "heading" : "2.1 Image-text Dataset",
      "text" : "Image-captioning datasets are first widely used for joint image-text modeling. MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014) that both contain five written caption descriptions for each image are the representative ones used for automated caption generation and cross-modal retrieval tasks. Conceptual Caption (Sharma et al., 2018) is yet another popular image caption dataset but contains an order of magnitude more images than MSCOCO. Because image captions usually only describe the main objects in the image and omit details, to facilitate understanding details of an image along with the reasoning behind them, Antol et al. (2015) introduced VQA which contains three question answer pairs for each image. A further work is VCR (Zellers et al., 2019) that not only requires a model to answer the question derived from the image but also provides a rationale explaining why its answer is right. It was created to teach the model to learn higher-order cognition and commonsense reasoning about the world.\nCompared to the work above, Image-Chat (Shuster et al., 2020) and IGA (Mostafazadeh et al., 2017), which focus on the dialogues grounded in the image, are the most related work to ours. IGA includes 4k dialogues where each contains an image with a textual description of it, along with the questions and responses around the image. Due to its small scale, IGA can only be used for evaluation. Image-Chat is a larger scale dataset that consists of 202k image-grounded dialogues. However, both of them were created by asking the crowd workers to talk about a shared image to generate engaging conversation, which is different from the scenario of photo sharing where only one side can access the photo at the start of the conversation. Thus, neither can be used to build a photo-suggest system. In our\nwork, we build a new dataset that highlights the challenges of building a photo-suggest system and is the first of its kind to the best of our knowledge."
    }, {
      "heading" : "2.2 Image-text Modeling",
      "text" : "As the challenge for the photo-suggest system is to retrieve the most relevant image based on the textual utterances, we only review the related work on cross-modal retrieval.\nMany models have been proposed for imagecaption retrieval where one is required to retrieve the most relevant caption given an image or vice versa. The typical architecture consists of two separate encoders for image and text to first generate visual and textual embeddings. On top of them, a fusion layer, which can simply be a dot product, is used to generate the relevance score for each pair (Frome et al., 2013; Kiros et al., 2014; Parekh et al., 2020; Karpathy and Fei-Fei, 2015; Faghri et al., 2018). Then a triplet ranking loss or cross-entropy loss is employed to learn the latent visual-semantic alignment. VSE++ (Faghri et al., 2018) emphasizes on the hardest negatives by using the max of the hinge loss as the objectives and yielded a significant performance improvement. Stacked Cross Attention Network (SCAN) (Lee et al., 2018) further improves the performance by introducing the cross attention between image regions and word features. Recently, cross-modal transformer based architecture that are pretrained on large-scale image-text datasets via self-supervised learning has shown great advantages in bridging visual and textual embeddings. Multiple concurrent work (Lu et al., 2019; Chen et al., 2020; Li et al., 2019) have refreshed the best records on the benchmark datasets for the image-text retrieval tasks."
    }, {
      "heading" : "3 Dataset Creation",
      "text" : "We select photos from Open Image Dataset V4 (OID) (Kuznetsova et al., 2020) and collect openended conversations on Amazon Mechanical Turk. Below describes the detailed image filtering, conversation generation, and data verification steps to ensure data quality."
    }, {
      "heading" : "3.1 Image-based Filtering",
      "text" : "Since OID is large-scale and comprehensive, it contains images that are unlikely to be shared in the daily dialogue, such as images only about remote controls or fire hydrants. To create a dataset that is close to the reality, we filter images based on the\nannotated object labels provided with OID. Based on our investigation of the imagegrounded dialogues and daily experiences, photos about four themes are commonly shared: people, food, animal, and product (in the shopping scenario), which are our focus in the dataset creation. From all the 600 object labels that appear in OID, we first enlist the labels that both belong to one of the four themes and have a high chance to appear in the commonly-shared photos. Labels like “traffic light”, “nail”, and “reptile” are excluded and labels like “girl”, “bagel”, and “camera” are included. This process selects 89 object labels (Appendix). We then generate an image pool by selecting those that contain any of the objects in the list. Note that for the objects of the people category, we add another criteria that it must be the main object, i.e. neither positioned in the margin of the image2 nor extremely small 3 to exclude images that only have people as the background. Images are randomly selected from the image pool to generate conversations in the next step."
    }, {
      "heading" : "3.2 Conversation Generation",
      "text" : "We randomly assigned two crowd workers to generate a conversation based on a given image. The image comes with an image description which presents the list of objects labels in the image. When the image contains humans, we assign a random name and relationship to one of the humans to help the workers refer to it and unfold the story. They are instructed to imagine talking with their friend. At the start of the task, only one side has access to the image and is instructed to drive the dialogue until it is fit to share the image with the other (website interfaces are shown in the Appendix). It is not restricted that they must message alternatively but the worker with the photo can’t share the photo until the total number of the conversation turns reaches five. After sharing the photo, they can continue to chat until they wish to end the conversation and submit the dialogue."
    }, {
      "heading" : "3.3 Image&text-based Verification",
      "text" : "Lastly, we use another set of in-house professional crowd workers to filter out the invalid dialogues generated in the above step. Dialogues are discarded if the association between the image and the dialogue is in-evident before the photo sharing act\n2Center of the object is located within 0.1 of the image width/height to the border.\n3Object width/length < 0.3 × (image width/length).\nis excluded in the verification step. Share the photo denotes the photo sharing act.\nor the content is unnatural, contains inappropriate words, too many typos or broken English. Figure 2 displays examples of qualified and unqualified data. Note that the third unqualified dialogue can happen in a real conversation, yet the content/event of the image is not mentioned until the photo being shared, making it impossible for a model to learn the connection between the dialogue and the images and to suggest a photo in advance. Such dialogues are removed from the dataset in this step."
    }, {
      "heading" : "4 Dataset Statistics",
      "text" : "The collected dataset consists of 10,917 unique images and 12,286 dialogues. One image is shared in each dialogue. Based on the object labels of the shared image, we classify the dialogues into four categories: people, food, animals, and daily products. We split the dialogues into 10,086 train, 1,000 dev, and 1,000 test sets while keeping roughly the same distribution of the category across the splits. The detailed statistics of each split and in total are shown in Table 1. Note that the dialogue can have multiple category labels. For instance, if the shared image is about a girl playing with dogs, the dialogue belongs to both people and animals categories. Thus, the sum of the dialogues of each category (people/animal/food/product dial #) exceeds the total number of the dialogues (dial #) in\nthe table. In addition, some images in the training set are used in multiple dialogues.\nBased on the statistics in the table, the average number of turns per dialogue is 12.7 and the average number of tokens per turn is 6.3. Since two sides are not restricted to speak alternatively, if the consecutive turns from the same side are combined as one turn, which is the conventional setting of other dialogue datasets, the average number of turns per dialogue and the average number of tokens per turn become 9.5 and 8.5. On average, people converse for 7 turns before sharing the photo."
    }, {
      "heading" : "5 Task Definition",
      "text" : "We decompose the problem of building a smart photo-suggest system into two separate tasks. The first is to detect if the user has the intent to share the photo in the next turn, which we call photo-sharing intent prediction task. The second is to retrieve the photo based on the dialogue context, which we call image retrieval task. Below describes the formal formulation of the problem settings.\nLet P = {p1, p2, ..., pM} be the photo set where each pi = (ai, li), i ∈ [1,M ] consists of image ai and a list of objects li in it. Given the dialogueD = {t1, ..., th, pk, th+1, ..., tN} where two participants speak alternatively, tj (j ∈ [1, N ]) and pk ∈ P respectively represent the utterance of turn j and\nthe shared image. th is the turn immediately before a photo sharing act. We also define the speaker information S = {s1, s2, ..., sN} where sj (j ∈ [1, N ]), either 0 or 1, denotes the speaker of turn j.\nPhoto-sharing intent prediction: The goal of the intent prediction task is to predict whether a photo will be shared in the next turn for any tj given all the turns before. In equation, it’s formulated as a binary classification task:\n∀j ∈ [1, h], C(t1:j , s1:j) ∈ {0, 1}, (1)\nwhere C is the intent prediction model taking the utterances and the speaker information of all the previous turns as the input and outputs a binary value. In the above case, it should only predicts 1 when j = h, otherwise 0. Note that whether the model make use of all the previous turns and the speaker information depends on the model design. We use F1 score, precision, and recall as the evaluation metrics for this task.\nImage retrieval: Under the same settings, model R of the image retrieval task is expected to correctly retrieve pk from P given the dialogue:\nR(t1:h, s1:h, P ) ∈ [1,M ]. (2)\nDuring training, the candidate pool P is usually comprised of in-batch images while during evaluation, P contains all images in the test set. Following Karpathy and Fei-Fei (2015), we use Recall@K (R@K), computed as “the fraction of times a correct item was found among the top K results” as the evaluation metrics. Specifically, we choose R@1, R@5, and R@10, as well as the sum of them which we denote as “sum(R@1, 5, 10)” to evaluate the models."
    }, {
      "heading" : "6 Baselines",
      "text" : ""
    }, {
      "heading" : "6.1 Photo-sharing Intent Prediction Model",
      "text" : "To establish the baselines, we fine-tune three SOTA pretrained models - BERT (Devlin et al., 2018a),\nALBERT (Lan et al., 2020), and T5 (Raffel et al., 2020), as the pretrained models have achieved remarkable performance in many NLP tasks.\nTo adapt BERT and ALBERT to our settings, we concatenate all the previous turns (t1:j in Equation 1) by [SEP] and prepend the concatenated text with [CLS] to generate the input to the model. We use the speaker information s1:j as the segment id of the input. The output of [CLS] token is fed into two fully-connected layers, of which the output dimensions are respectively 128 and 2 to generate the final prediction. To utilize T5, we concatenate t1:j by [SEP] and prepend the text with “predict share intent:” as the model input. We use cross entropy loss for all three models."
    }, {
      "heading" : "6.2 Image Retrieval Model",
      "text" : "Our baselines consists of both statistical and neural network-based approaches, as elaborated below:\nDual encoder: We built a dual-encoder model similar to Parekh et al. (2020); Gillick et al. (2018), which separately encodes image and text leveraging SOTA pre-trained models. Its entire architecture is shown in Figure 3.\nTo encode the image, for each pi = (ai, li) we first resize the image ai to 224 × 224 and feed it into a pretrained ResNet (He et al., 2016) to generate Ai. A pretrained BERT is used to encode li to achieve the label embedding Li which is the output of [CLS] token. Li is concatenated with Ai to generate the image embedding. For encoding the dialogue context, we use a second pretrained BERT (Devlin et al., 2018b). Its input is the concatenation of all the prior utterances of the speaker who shares the photo. The output of [CLS] token is used as the contextual text embedding. Two fully connected layers are then used to separately project image and text embeddings into a joint image-text embedding space of dimension H . Then, the dot product of the normalized image embedding Bi\nand text embedding Tj is used as the similarity score S(Bi, Tj). Following Young et al. (2014); Gillick et al. (2018), bidirectional in-batch sampled cross entropy loss is employed:\nlsm(Bi, Tj) = −(S(Bi, Tj)− log ∑ T̂j eS(Bi,T̂j))\n−(S(Bi, Tj)− log ∑ B̂i eS(B̂i,Tj)),\nwhere B̂i and T̂j are the image embeddings and text embeddings of the other examples in the batch.\nWe also experiment with bidirectional in-batch hinge loss, defined as:\nlsh(Bi, Tj) = ∑ T̂j [α− S(Bi, Tj) + S(Bi, T̂j)]+\n+ ∑ B̂i [α− S(Bi, Tj) + S(B̂i, Tj)]+,\nwhere α is the margin parameter and [x]+ ≡ max(x, 0) . In our preliminary experiments, we observe cross entropy loss works better and implement most experiments with cross entropy loss.\nVSE++: VSE++ (Faghri et al., 2018) is a simple and effective dual encoder model. It encodes the image and the text, which is the concatenation of all the previous utterances of the person who shares the photo in our case, separately by ResNet152 (He et al., 2016) and GRU (Cho et al., 2014). It is then followed by linear projections to map them into the joint embedding space. Finally, dot products of the normalized embeddings are used to compute the ranking scores. They innovatively make use of the hardest negatives, which are the negatives closest to the query, in the ranking loss function:\nlmh(Bi, Tj) = [α− S(Bi, Tj) + S(Bi, T̂ hj )]+\n+[α− S(Bi, Tj) + S(B̂hi , Tj)]+,\nwhere T̂ hj = argmax(S(Bi, T̂j)) and B̂ h i = argmax(S(B̂i, Tj)) are the hardest negatives.\nSCAN: SCAN (Lee et al., 2018) is a full cross attention model that captures the fine-grained interplay between image regions and text tokens to infer image-text similarity. It uses fasterRCNN (Ren et al., 2017) in conjucntion with ResNet-101 to compute image region embeddings and bidirectional GRU to achieve text embeddings. Same as VSE++, SCAN uses hard negatives in the triple ranking loss function. Though it beats VSE++ on the image captioninig tasks, it doesn’t scale well to large-scale retrieval problems due to the high computational cost of cross attention.\nBM25: BM25 (Amati, 2009) is a probabilistic retrieval function widely used for document retrieval. To adapt it to our settings, we directly utilize the object labels of each image lj , j ∈ [1,m] as the document term. All the utterances before photo is shared are concatenated, tokenized and used as the query term to retrieve the image."
    }, {
      "heading" : "7 Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 Setup",
      "text" : "The maximum sequence length of BERT, ALBERT, and T5 for the photo-sharing intent prediction task is 512. We choose checkpoints that achieve the best F1 score on the dev set for evaluation on the test set.\nFor our dual encoder model, the maximum sequence length of BERT is 128, the dimension of the joint image-text embedding space H is 512, and margin parameter α is 0.2 for all the experiments. All parameters are trainable. We use the Adam optimizer (β1 = 0.9, β2 = 0.999) and a learning rate that starts at 5e-5 and decays by 0.1% every 1000 steps. The models are trained on 32-core pod slices of Cloud TPU V3 Pod, with a per-replica batch size of 4. The loss is computed on item pairs aggregated from all replicas, which is ovegr the global batch of 128 samples in this case.\nFor VSE++ and SCAN models, as GRU is not a pretrained encoder, directly training them on Pho-\ntoChat yields unpleasant results. As such, we first train them on MSCOCO and finetune them on PhotoChat for 20 epochs. We utilize the same setting as the single models that are reported to perform the best on the image-retrieval task on MSCOCO; more specifically, VSE++ (ResNet, FT) and SCAN t-i AVG (λ1 = 9) following the annotations in the original papers."
    }, {
      "heading" : "7.2 Results of intent prediction",
      "text" : "Table 2 presents model performance on the test set. We observe that T5 outperforms BERT and ALBERT in all metrics. Note that our dataset suffers from class imbalance that the negative examples outnumber the positive examples 3 , which we suspect causes the low precision across all the models.\nFigure 4 shows examples of the prediction by T53B model. Though a few turns are falsely predicted as positive (e.g. “They were really pretty.” and the second to last turn in example 2), it’s possible for the speaker to share the photo after this turn in real life, indicating that when to share a photo is subjective and the model may be more viable than the low precision would suggest. We also anticipate if the model has access to the set of photos the speaker can share, the accuracy can be elevated. In this case, the model will be able to infer that the photo in example 1 and 2 of Figure 4 are more likely to follow utterances about food and statues."
    }, {
      "heading" : "7.3 Results of image retrieval",
      "text" : "Table 4 lists the experimental results on PhotoChat. Our dual encoder model is denoted as DE. DEimg and DElabel are the ablation models that only take the image ai or image labels li as the input compared to the default architecture in Figure 3. CE, SH, MH represents cross entropy loss, hinge loss,\nand hinge loss using hard negatives. We attempt training DE on MSCOCO first and finetuning it on PhotoChat. These models are specially annotated with *. We also experiment with different image encoders: ResNet-50 and ResNet-152, in combination with different label encoders: Bert-base and Bert-tiny. They are annotated in the brackets after the model names in Table 4. Among all the models, SCAN achieves the best performance with 10.4% R@1, 27% R@5, and 37.1% R@10, which is consistent with the prior work (Lee et al., 2018), demonstrating the power of the bottom-up cross attention. Among all the models that don’t have cross-attention, our model DE*(ResNet-152, Berttiny) performs the best and beats a strong prior work VSE++, indicating the effectiveness of using image labels in the retrieval task.\nAblation study: By comparing DElabel(Bertbase) and DEimg(ResNet-152), we find that using image features is more effective than using image label features, which is expected as images contain more information. Compared to the model using only image pixel values (DEimg(ResNet152)), adding the label features contributes to an increase of 1.3% in sum(R@1, 5, 10) to 66.4% (DE(ResNet-152, Bert-base)). Pretraining the model on MSCOCO further boosts it by 3.5% to\nTable 4: Experimental results of the baseline models on image retrieval task. DE stands for our proposing dual encoders. DEimg only uses the image pixel values and DElabel only uses image labels to extract image features. DE* is the model pretrained on MSCOCO. All numbers are in percentage.\nModel Loss function R@1 ↑ R@5 ↑ R@10 ↑ Sum(R@1, 5, 10)↑ BM25 - 6.6 15.4 23.0 45.0 DElabel(Bert-base) CE 6.7 22.1 31.2 60.0 DEimg(ResNet-50) CE 6.7 21.9 32.3 60.9 DEimg(ResNet-152) CE 6.8 24.0 34.3 65.1 DE(ResNet-152, Bert-base) CE 8.1 23.7 34.6 66.4 DE*(ResNet-152, Bert-base) SH 8.0 22.0 31.0 61.0 DE*(ResNet-152, Bert-tiny) SH 7.1 23.3 33.0 63.4 DE*(ResNet-152, Bert-base) CE 8.5 26.1 35.3 69.9 DE*(ResNet-152, Bert-tiny) CE 9.0 26.4 35.7 71.1 VSE++ MH 10.2 25.4 34.2 69.8 SCAN MH 10.4 27 37.1 74.5\n69.9% (DE*(ResNet-152, Bert-base)).\nEffect of encoders: We observe that using a smaller model (Bert-tiny) to encode image labels yields better performance regardless of the loss function. DE*(ResNet-152, Bert-tiny) improves sum(R@1, 5, 10) by 1.2% compared to DE*(ResNet-152, Bert-base) when using cross entropy loss and 2.4% when using hinge loss. The reason might be that labels are a compact list of tokens and thus, using a smaller model alleviate the problem of overfitting. On the other hand, using a larger image encoder ResNet-152 produces better results that DEimg(ResNet-152) beats DEimg(ResNet-50) in sum(R@1, 5, 10) by 4.2%.\nEffect of loss function: Our dual encoders work significantly better with cross entropy loss than hinge loss and their gap is about 8% in sum(R@1, 5, 10) as we compare the results of DE*(ResNet152, Bert-base) and DE*(ResNet-152, Bert-tiny) models under different loss functions.\nError analysis: Figure 5 shows the qualitative results of DE*(ResNet-152, Bert-tiny) given a text query. In the first example, the model ranks the relevant images of wine glasses and black tea at top instead of the groundtruth image where a man is holding a wine glass, which is easy to be neglected. In the second example, the model fails to distinguish puffins with ducks and infer the background from keyword “atlantic”. It illustrates the challenge of the image retrieval task under the dialogue context that it requires a model to pay attention to the details and the event, as discussed in Section 1. Figure 6 presents more prediction results including some wrong predictions by the model.\nA: We're missing you over here at the bar! B: Oh..That was unfair A: yeah, sorry you couldn't make it. We're having wine B: Oh..That is interesting to know Enjoy guys.. A: I forget, do you prefer red or white? We got a nice red for the table\nA: do you like puffins in atlantic B: never heard of that A: i had a photo of puffin B: cool A: i thought you would like that\nFigure 5: Predictions by DE*(ResNet-152, Bert-tiny) for the image retrieval task. For each dialogue query, we show the groundtruth (first image in green) and the top-2 ranked images (in red). Best viewed in color."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We collected a 12k high-quality dialogue dataset that contains photo sharing activity via crowdsourcing. To facilitate research on building intelligent photo-suggest system, we have introduced two new challenging tasks that aim at improving the photo-sharing experience: photo-sharing intent prediction task and image retrieval task. That is, when given a dialogue, the system should predict whether the user has the intention to share the photo and which photo is suitable to be shared. We built baseline models for both tasks and report their performance with detailed analysis.\nBesides the proposed two new tasks, our dataset can potentially be used in other dialogue related tasks, such as dialogue generation in the multimodal dialogues, as well as inspiring new research\ntopics, such as composing automatic reply to the photos sent from others. We hope our dataset and modeling work can be beneficial for studies that focus on the interplay between image and dialogue."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Pranav Khaitan and Blaise Aguera y Arcas for the support and assistance; Yinfei Yang, David Bieber for reviewing the draft and providing the feedback; Janel Thamkul and Tulsee Doshi for doing the legal review of the dataset."
    }, {
      "heading" : "A Dataset Creation & Details",
      "text" : "The website interfaces used to collect dialogues are presented in Figure 7 and 8.\nTable 5 shows the 89 object labels that we used to select the photos from Open Image Dataset for generating dialogues."
    } ],
    "references" : [ {
      "title" : "BM25, pages 257–260",
      "author" : [ "Giambattista Amati." ],
      "venue" : "Springer US, Boston, MA.",
      "citeRegEx" : "Amati.,? 2009",
      "shortCiteRegEx" : "Amati.",
      "year" : 2009
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Anderson et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "2018a. Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018b",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Cultures of the Internet: The Internet in Britain",
      "author" : [ "William Dutton", "Grant Blank." ],
      "venue" : "Oxford Internet Survey 2013 Report.",
      "citeRegEx" : "Dutton and Blank.,? 2013",
      "shortCiteRegEx" : "Dutton and Blank.",
      "year" : 2013
    }, {
      "title" : "Vse++: Improving visualsemantic embeddings with hard negatives",
      "author" : [ "Fartash Faghri", "David J Fleet", "Jamie Ryan Kiros", "Sanja Fidler" ],
      "venue" : null,
      "citeRegEx" : "Faghri et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Faghri et al\\.",
      "year" : 2018
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc Aurelio Ranzato", "Tomas Mikolov." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,",
      "citeRegEx" : "Frome et al\\.,? 2013",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end retrieval in continuous space",
      "author" : [ "Daniel Gillick", "Alessandro Presta", "Gaurav Singh Tomar" ],
      "venue" : null,
      "citeRegEx" : "Gillick et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei." ],
      "venue" : "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel" ],
      "venue" : null,
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "The open images dataset v4",
      "author" : [ "Alina Kuznetsova", "Hassan Rom", "Neil Alldrin", "Jasper Uijlings", "Ivan Krasin", "Jordi Pont-Tuset", "Shahab Kamali", "Stefan Popov", "Matteo Malloci", "Alexander Kolesnikov", "Tom Duerig", "Vittorio Ferrari" ],
      "venue" : null,
      "citeRegEx" : "Kuznetsova et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kuznetsova et al\\.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut" ],
      "venue" : null,
      "citeRegEx" : "Lan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Stacked cross attention for image-text matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He." ],
      "venue" : "arXiv preprint arXiv:1803.08024.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Teens and mobile phones: Text messaging explodes as teens embrace it as the centerpiece of their communication strategies with friends",
      "author" : [ "A. Lenhart", "Rich Ling", "S. Campbell", "K. Purcell" ],
      "venue" : null,
      "citeRegEx" : "Lenhart et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lenhart et al\\.",
      "year" : 2010
    }, {
      "title" : "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Ming Gong", "Daxin Jiang", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "Lecture Notes in Computer Science, page 740–755.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Photographs as things – photographs of things",
      "author" : [ "Katharina Lobinger." ],
      "venue" : "a texto-material perspective on photo-sharing practices. Information, Communication & Society, 19(4):475–488.",
      "citeRegEx" : "Lobinger.,? 2016",
      "shortCiteRegEx" : "Lobinger.",
      "year" : 2016
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Image-grounded conversations: Multimodal context for natural question and response generation",
      "author" : [ "Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios Spithourakis", "Lucy Vanderwende." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for ms-coco",
      "author" : [ "Zarana Parekh", "Jason Baldridge", "Daniel Cer", "Austin Waters", "Yinfei Yang" ],
      "venue" : null,
      "citeRegEx" : "Parekh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Parekh et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137–1149.",
      "citeRegEx" : "Ren et al\\.,? 2017",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2017
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "Image-chat: Engaging grounded conversations",
      "author" : [ "Kurt Shuster", "Samuel Humeau", "Antoine Bordes", "Jason Weston." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2414–2429, Online. Association for",
      "citeRegEx" : "Shuster et al\\.,? 2020",
      "shortCiteRegEx" : "Shuster et al\\.",
      "year" : 2020
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "As instant messaging tools gain enormous popularity in the recent decades, sharing photos as an approach to enhance the engagement of an online messaging conversation has become a pervasive routine communicative act (Lobinger, 2016).",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 18,
      "context" : "A survey conducted in 2010 reveals that 74% of teenagers in the US reported messaging a photo or video using their cell phone (Lenhart et al., 2010).",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "In Britain, almost 70% of the internet users shared photos in 2013 (Dutton and Blank, 2013).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "image captioning (Anderson et al., 2018), visual question answering (Antol et al.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : ", 2018), visual question answering (Antol et al., 2015), visual commonsense reasoning (Zellers et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : ", 2015), visual commonsense reasoning (Zellers et al., 2019), and image-grounded dialogue generation (Shuster et al.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 28,
      "context" : ", 2019), and image-grounded dialogue generation (Shuster et al., 2020).",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "We selected images from OpenImage V4 dataset (Kuznetsova et al., 2020) as shared photos and used crowdsourcing plugins to generate 12,286 dialogues with an average of 10 turns per dialogue.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "MSCOCO (Lin et al., 2014) and Flickr30k (Young et al.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 29,
      "context" : ", 2014) and Flickr30k (Young et al., 2014) that both contain five written caption descriptions for each image are the representative ones used for automated caption generation and cross-modal retrieval tasks.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 30,
      "context" : "A further work is VCR (Zellers et al., 2019) that not only requires a model to answer the question derived from the im-",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "Compared to the work above, Image-Chat (Shuster et al., 2020) and IGA (Mostafazadeh et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : ", 2020) and IGA (Mostafazadeh et al., 2017), which focus on the dialogues grounded in the image, are the most related work to ours.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "VSE++ (Faghri et al., 2018) emphasizes on the hardest negatives by using the max of the hinge loss as the objectives and yielded a significant",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "Stacked Cross Attention Network (SCAN) (Lee et al., 2018) further improves the performance by introducing the cross attention between image regions and word features.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "We select photos from Open Image Dataset V4 (OID) (Kuznetsova et al., 2020) and collect openended conversations on Amazon Mechanical Turk.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : ", 2018a), ALBERT (Lan et al., 2020), and T5 (Raffel et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2020), and T5 (Raffel et al., 2020), as the pretrained models have achieved remarkable performance in many NLP tasks.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "To encode the image, for each pi = (ai, li) we first resize the image ai to 224 × 224 and feed it into a pretrained ResNet (He et al., 2016) to generate Ai.",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "For encoding the dialogue context, we use a second pretrained BERT (Devlin et al., 2018b).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "VSE++: VSE++ (Faghri et al., 2018) is a simple and effective dual encoder model.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "It encodes the image and the text, which is the concatenation of all the previous utterances of the person who shares the photo in our case, separately by ResNet152 (He et al., 2016) and GRU (Cho et al.",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "SCAN: SCAN (Lee et al., 2018) is a full cross attention model that captures the fine-grained interplay between image regions and text tokens to infer image-text similarity.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "1% R@10, which is consistent with the prior work (Lee et al., 2018),",
      "startOffset" : 49,
      "endOffset" : 67
    } ],
    "year" : 2021,
    "abstractText" : "We present a new human-human dialogue dataset PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% F1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community.",
    "creator" : "LaTeX with hyperref"
  }
}