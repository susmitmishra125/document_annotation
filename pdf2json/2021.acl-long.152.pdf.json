{
  "name" : "2021.acl-long.152.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks",
    "authors" : [ "Weicheng Ma", "Kai Zhang", "Renze Lou", "Lili Wang", "Soroush Vosoughi" ],
    "emails" : [ "1{first.last}.gr@dartmouth.edu", "2drogozhang@gmail.com", "3marionojump0722@gmail.com", "4soroush.vosoughi@dartmouth.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1956–1966\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1956\nContributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks\nWeicheng Ma1*, Kai Zhang2*†, Renze Lou3†, Lili Wang1, and Soroush Vosoughi4\n1,4Department of Computer Science, Dartmouth College 2Department of Computer Science and Technology, Tsinghua University\n3Department of Computer Science, Zhejiang University City College 1{first.last}.gr@dartmouth.edu\n2drogozhang@gmail.com 3marionojump0722@gmail.com\n4soroush.vosoughi@dartmouth.edu Abstract\nThis paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multilingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings."
    }, {
      "heading" : "1 Introduction",
      "text" : "Prior research on mono-lingual Transformer-based (Vaswani et al., 2017) models reveals that a subset of their attention heads makes key contributions to each task, and the models perform comparably well (Voita et al., 2019; Michel et al., 2019) or even better (Kovaleva et al., 2019) with the remaining heads pruned 1. While multi-lingual Transformer∗Equal contribution. †Work done when interning at the Minds, Machines, and Society Lab at Dartmouth College. 1We regard single-source machine translation as a monolingual task since the inputs to the models are mono-lingual.\nbased models, e.g. mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), are widely applied in cross-lingual and multi-lingual NLP tasks 2 (Wang et al., 2019; Keung et al., 2019; Eskander et al., 2020), no attempt has been made to extend the findings on the aforementioned mono-lingual research to this context. In this paper, we explore the roles of attention heads in cross-lingual and multi-lingual tasks for two reasons. First, better understanding and interpretability of Transformerbased models leads to efficient model designs and parameter tuning. Second, head-pruning makes Transformer-based models more applicable to truly resource-scarce languages if it does not negatively affect model performance significantly.\nThe biggest challenge we face when studying the roles of attention heads in cross-lingual and multi-lingual tasks is locating the heads to prune. Existing research has shown that each attention head is specialized to extract a collection of linguistic features, e.g., the middle layers of BERT mainly extract syntactic features (Vig and Belinkov, 2019; Hewitt and Manning, 2019) and the fourth head on the fifth layer of BERT greatly contributes to the coreference resolution task (Clark et al., 2019). Thus, we hypothesize that important feature extractors for a task should be shared across languages and the remaining heads can be pruned. We evaluate two approaches used to rank attention heads, the first of which is layer-wise relevance propagation (LRP, Ding et al. (2017)). Voita et al. (2019) interpreted the adaptation of LRP in Transformerbased models on machine translation. Motivated by Feng et al. (2018) and Serrano and Smith (2019), we design a second ranking method based on gradients since the gradients on each attention head\n2We define a cross-lingual task as a task whose test set is in a different language from its training set. A multi-lingual task is a task whose training set is multi-lingual and the languages of its test set belong to the languages of the training set.\nreflect its contribution to the predictions.\nWe study the effects of pruning attention heads on three sequence labeling tasks, namely part-ofspeech tagging (POS), named entity recognition (NER), and slot filling (SF). We focus on sequence labeling tasks since they are more difficult to annotate than document- or sentence-level classification datasets and require more treatment in crosslingual and multi-lingual research. We choose POS and NER datasets in 9 languages, where English (EN), Chinese (ZH), and Arabic (AR) are candidate source languages. The MultiAtis++ corpus (Xu et al., 2020) is used in the SF evaluations with EN as the source language. We do not include syntactic chunking and semantic role labeling tasks due to lack of availability of manually written and annotated corpora. In these experiments, we rank attention heads based only on the source language(s) to ensure the extensibility of the learned knowledge to cross-lingual tasks and resource-poor languages. In our preliminary experiments comparing the gradient-based method and LRP, the average F1 score improvements on NER with mBERT are 0.69 (cross-lingual) and 0.24 (multi-lingual) for LRP and 0.81 (cross-lingual) and 0.31 (multi-lingual) for the gradient-based method, though both methods rank attention heads similarly. Thus we choose the gradient-based method to rank attention heads in all our experiments.\nOur evaluations confirm that only a subset of attention heads in each Transformer-based model makes key contributions to each cross-lingual or multi-lingual task and that these heads are shared across languages. Performance of models generally drop when the highest-ranked or randomly selected heads are pruned, validating the head rankings generated by our gradient-based method. We also observe performance improvements on tasks with multiple source languages by pruning attention heads. Our findings potentially apply to truly resource-scarce languages since we show that the models perform better with attention heads pruned when fewer training instances are available in the target languages.\nThe contributions of this paper are three-fold:\n• We explore the roles of attention heads in multilingual Transformer-based models and find that pruning certain heads leads to comparable or better performance in cross-lingual and multilingual sequence labeling tasks. • We adapt a gradient-based method to locate atten-\ntion heads that can be pruned without exhaustive experiments on all possible combinations. • We show the correctness, robustness, and ex-\ntensibility of the findings and our head ranking method under a wide range of settings through comprehensive experiments."
    }, {
      "heading" : "2 Datasets",
      "text" : "We use human-written and manually annotated datasets in experiments to avoid noise from machine translation and automatic label projection.\nWe choose POS and NER datasets in 9 languages, namely EN, ZH, AR, Hebrew (HE), Japanese (JA), Persian (FA), German (DE), Dutch (NL), and Urdu (UR). As Table 1 shows, these languages fall in diverse language families and the datasets are very different in size. EN, ZH, and AR are used as candidate source languages since they are resource-rich in many NLP tasks. Our POS datasets are all from Universal Dependencies (UD) v2.7 3. These datasets are labeled with a common label set containing 17 POS tags.\nFor NER, we use NL, EN, and DE datasets from CoNLL-2002 and 2003 challenges (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Additionally, we use the People’s Daily dataset 4, iob2corpus 5, AQMAR (Mohit et al., 2012), ArmanPerosNERCorpus (Poostchi et al., 2016), MK-PUCIT (Kanwal et al., 2020), and a news-based NER dataset (Mordecai and Elhadad, 2012) for the languages CN, JA, AR, FA, UR, and\n3http://universaldependencies.org/ 4http://github.com/OYE93/Chinese-NLP-C\norpus/tree/master/NER/People’sDaily 5http://github.com/Hironsan/IOB2Corpus\nHE, respectively. Since the NER datasets are individually constructed in each language, their label sets do not fully agree. As there are four NE types (PER, ORG, LOC, MISC) in the three sourcelanguage datasets, we merge other NE types into the MISC class to allow cross-lingual evaluations.\nWe evaluate SF models on MultiAtis++ with EN as the source language and Spanish (ES), Portuguese (PT), DE, French (FR), ZH, JA, Hindi (HI), and Turkish (TR) as target languages. There are 71 slot types in the TR dataset, 75 in the HI dataset, and 84 in the other datasets. We do not use the intent labels in our evaluations since we study only sequence labeling tasks. Thus our results are not directly comparable with Xu et al. (2020)."
    }, {
      "heading" : "3 Methodology",
      "text" : "Here, we introduce the gradient-based method we use in the experiments to rank the attention heads. Feng et al. (2018) claim that gradients measure the importance of features to predictions. Since each head functions similarly as a standalone feature extractor in a Transformer-based model, we use gradients to approximate the importance of the feature set extracted by each head and rank the heads accordingly. Michel et al. (2019) determine importance of heads with accumulated gradients at each head in a training epoch. Different from their approach, we fine-tune the model on the training set and rank the heads using gradients on the development set to ensure that the head importance rankings are not significantly correlated with the training instances in one source language. Specifically, our method generates head rankings for each language in three steps: (1) We fine-tune a Transformer-based model on a mono-lingual task for three epochs. (2) We re-run the fine-tuned model on the development partition of the dataset with back-propagation but not parameter updates to obtain gradients. (3) We sum up the absolute gradients on each head, layer-wise normalize the accumulated gradients, and scale them into the range [0, 1] globally.\nWe show Spearman’s rank correlation coefficients (Spearman’s ρ) between head rankings of each language pair generated by our method on POS, NER, and SF in Figure 1. The highestranked heads largely overlap in all three tasks, while the rankings of unimportant heads vary more in mBERT than XLM-R.\nAfter ranking the attention heads, we fine-tune the model, with the lowest-ranked head in the source language pruned. We keep increasing the number of heads to prune until it reaches a preset limit or when the performance starts to drop. We limit the number of trials to 12 since the models mostly show improved performance within 12 attempts 6."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "This section displays and explains experimental results on cross-lingual and multi-lingual POS, NER, and SF tasks. Training sets in target languages are not used to train the model under the cross-lingual setting. Our experiments are based on the Huggingface (Wolf et al., 2020) implementations of mBERT\n6On average 7.52 and 6.58 heads are pruned for POS, 7.54 and 7.28 heads for NER, and 6.19 and 6.31 heads for SF, respectively in mBERT and XLM-R models.\nand XLM-R. Specifically, we use the pre-trained bert-base-multilingual-cased and xlm-roberta-base models for their comparable model sizes. The models are fine-tuned for 3 epochs with a learning rate of 5e-5 in all the experiments. We use the official dataset splits and load training instances with sequential data samplers, so the reported evaluation scores are robust to randomness."
    }, {
      "heading" : "4.1 POS",
      "text" : "Table 2 shows the evaluation scores on POS with three source language choices. In the majority (88 out of 96 pairs) of experiments, pruning up to 12 attention heads improves mBERT and XLM-R performance. Results are comparable in the other 8 experiments with and without head pruning. Average F-1 score improvements are 0.91 for mBERT and 1.78 for XLM-R in cross-lingual tasks, and 0.15 for mBERT and 0.17 for XLM-R in multi-\nlingual tasks. These results support that pruning heads generally has positive effects on model performance in cross-lingual and multi-lingual tasks, and that our method correctly ranks the heads.\nConsistent with Conneau et al. (2020), XLMR usually outperforms mBERT, with exceptions in cross-lingual experiments where ZH and JA datasets are involved. Word segmentation in ZH and JA is different from the other languages we choose, e.g. words are not separated by white spaces and unpaired adjacent word pieces often make up a new word. As XLM-R applies the SentencePiece tokenization method (Kudo and Richardson, 2018), it is more likely to detect wrong word boundaries and make improper predictions than mBERT in cross-lingual experiments involving ZH or JA datasets. We note that the performance improvements are solid regardless of the\nsource language selection and severe differences of training data sizes in EN, ZH, and AR. This demonstrates the correctness of the head rankings our method generates and that the important attention heads for a task are almost language invariant.\nWe also examine to what extent the score improvements are affected by the relationships between source and target languages, e.g. language families, URIEL language distance scores (Littell et al., 2017), and the similarity of the head ranking matrices. There are three non-exclusive clusters of language families (containing more than one language) in our choice of languages, namely IndoEuropean (IE), Germanic, and Semitic languages. Average score improvements between models with and without head pruning are 0.40 (IE), 0.16 (Germanic), and 0.91 (Semitic) for mBERT and 0.19 (IE), 0.18 (Germanic), and 0.19 (Semitic) for XLMR. In comparison, the overall average score improvements are 0.53 for mBERT and 0.97 for XLMR. Despite the generally higher performance of models when the source and target languages are in the same family, the score improvements by pruning heads are not necessarily associated with language families. Additionally, we use Spearman’s ρ to measure the correlations between improved F-1 scores and URIEL language distances. The correlation scores are 0.11 (cross-lingual) and 0.12 (multi-lingual) for mBERT, and -0.40 (crosslingual) and 0.23 (multi-lingual) for XLM-R. Similarly, the Spearman’s ρ between score improvements and similarities in head ranking matrices shown in Figure 1 are -0.34 (cross-lingual) and 0.25 (multi-lingual) for mBERT, and -0.52 (crosslingual) and -0.10 (multi-lingual) for XLM-R. This indicate that except in the cross-lingual XLM-R model which faces word segmentation issues on ZH or JA experiments, pruning attention heads\nimproves model performance regardless of the distances between source and target languages. Thus our findings are potentially applicable to all crosslingual and multi-lingual POS tasks."
    }, {
      "heading" : "4.2 NER",
      "text" : "As Table 3 shows, pruning attention heads generally has positive effects on our cross-lingual and multi-lingual NER models. Even in the multilingual AR-UR experiment where the full mBERT model achieves an F-1 score of 99.26, the score is raised to 99.31 by pruning heads. Scores are comparable with and without head pruning in the 19 cases where model performances are not improved. This also lends support to the specialized role of important attention heads and the consistency of head rankings across languages. In NER experiments, performance drops mostly happen when the source and target languages are from different families. This is likely caused by the difference between named entity (NE) representations across language families. We show in Section 5.2 that the gap is largely bridged when a language from the same family as the target language is added to the source languages.\nAverage score improvements are comparable on mBERT (0.81 under cross-lingual and 0.31 under multi-lingual settings) and XLM-R (1.08 under cross-lingual and 0.67 under multi-lingual settings) in NER experiments. The results indicate that the performance improvements introduced by headpruning are not sensitive to the pre-training corpora of models. The correlations between F-1 score improvements and URIEL language distances are small, with Spearman’s ρ of -0.05 (cross-lingual)\nand -0.27 (multi-lingual) for mBERT and 0.10 (cross-lingual) and 0.12 (multi-lingual) for XLM-R. Similarities between head ranking matrices do not greatly affect score improvements either, the Spearman’s ρ of which are -0.08 (cross-lingual) and 0.06 (multi-lingual) for mBERT and 0.05 (cross-lingual) and 0.12 (multi-lingual) for XLM-R. The findings in POS and NER experiments are consistent, supporting our hypothesis that important heads for a task are shared by arbitrary source-target language selections."
    }, {
      "heading" : "4.3 Slot Filling",
      "text" : "We report SF evaluation results in Table 4. In 31 out of 34 pairs of experiments, pruning up to 12 heads results in performance improvements, while the scores are comparable in the other three cases. These results agree with those in POS and NER experiments, showing that only a subset of heads in each model makes key contributions to crosslingual or multi-lingual tasks.\nWe also evaluate the correlations between score changes and the closeness of source and target languages. In terms of URIEL language distances, the Spearman’s ρ are 0.69 (cross-lingual) and 0.14 (multi-lingual) for mBERT and -0.59 (cross-lingual) and 0.14 (multi-lingual) for XLMR. The coefficients are -0.25 (cross-lingual) and -0.73 (multi-lingual) for mBERT and -0.70 (crosslingual) and -0.14 (multi-lingual) between score improvements and similarities in head ranking matrices. While these coefficients are generally higher than those in POS and NER evaluations, their pvalues are also high (0.55 to 0.74), indicating the correlations between the score changes and source-"
    }, {
      "heading" : "ZH -1.74 +0.08 -2.44 +0.26",
      "text" : ""
    }, {
      "heading" : "AR -3.17 -2.42 -2.09 -0.43",
      "text" : ""
    }, {
      "heading" : "DE +0.88 -0.62 +0.57 -0.38",
      "text" : ""
    }, {
      "heading" : "NL -2.76 -0.23 +0.29 +0.36",
      "text" : ""
    }, {
      "heading" : "FA -0.86 -0.31 -2.52 -0.74",
      "text" : ""
    }, {
      "heading" : "HE -2.50 -2.15 -0.49 -4.21",
      "text" : ""
    }, {
      "heading" : "JA -1.48 -1.08 -2.65 -2.40",
      "text" : ""
    }, {
      "heading" : "UR -0.15 -0.10 -0.60 -0.12 POS",
      "text" : ""
    }, {
      "heading" : "ZH +0.03 -0.39 -0.14 -0.20",
      "text" : ""
    }, {
      "heading" : "AR -0.65 -0.04 -0.66 -0.12",
      "text" : ""
    }, {
      "heading" : "DE -0.64 -0.04 -0.64 -0.14",
      "text" : ""
    }, {
      "heading" : "NL -0.13 -0.13 -0.11 -0.16",
      "text" : ""
    }, {
      "heading" : "FA -0.75 -0.03 -0.53 -0.25",
      "text" : ""
    }, {
      "heading" : "HE -1.27 -0.28 -1.06 +0.05",
      "text" : ""
    }, {
      "heading" : "JA -22.29 -0.05 -1.23 -0.05",
      "text" : ""
    }, {
      "heading" : "UR -1.78 -0.11 -0.77 -0.07",
      "text" : "target language closeness are not statistically significant. 7"
    }, {
      "heading" : "5 Discussions",
      "text" : "In this section, we perform case studies to confirm the validity of our head ranking method. We also illustrate the extensibility of the knowledge we learn from the main experiments to a wider range of settings, e.g. when the training dataset is limited in size or constructed over multiple source languages."
    }, {
      "heading" : "5.1 Correctness of Head Rankings",
      "text" : "We evaluate the correctness of our head ranking method through comparisons between results in Tables 2 and 3 and those produced by pruning (1) randomly sampled heads and (2) highest ranked heads. Specifically, we repeat the head-pruning experiments with mBERT on NER and POS using\n7The p-values for all the other Spearman’s ρ we report are lower than 0.01, showing that those correlation scores are statistically significant.\nEN as the source language and display the score differences from the the full models in Table 5. Same as in the main experiments, we pick the best score from pruning 1 to 12 heads in each experiment. A random seed of 42 is used for sampling attention heads to prune under the random sampling setting.\nIn 14 out of 16 NER experiments, pruning the heads ranked highest by our method results in noticeable performance drops compared to the full model. Consistently, pruning the highest-ranked attention heads harms the performance of mBERT in 15 out of 16 POS experiments. Though score changes are slightly positive for cross-lingual ENDE and multi-lingual EN-ZH NER tasks and in the cross-lingual EN-ZH POS experiment, improvements introduced by pruning lowest-ranked heads are more significant, as Table 2 and Table 3 show. Pruning random attention heads also has mainly negative effects on the performance of mBERT. These results indicate that while pruning attention heads potentially boosts the performance of models, reasonably choosing the heads to prune is important. Our gradient-based method properly ranks the heads by their priority to prune."
    }, {
      "heading" : "5.2 Multiple Source Languages",
      "text" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012). We also explore the effects of pruning attention heads under the multi-source settings. In this section, we experiment with mBERT on EN, DE, AR, HE, and ZH datasets for both NER and POS tasks. These languages fall into three mutually exclusive language families, enabling our analysis on the influence of training cross-lingual models with source languages belonging to the same family as the target language. Similar to related research, the model is fine-tuned on the concatenation of training datasets in all the languages but the one on which the model is tested.\nSince the head ranking matrices are not identical across languages, we design three heuristics to rank the heads in the multi-source experiments. The first method merges the head ranking matrices of all the source languages into one matrix and re-generates the rankings. The second method ranks the attention heads after summing up the head ranking\nmatrices. We also examine the efficacy of pruning heads based on the head rankings from a single language. For this heuristic, we run experiments using the head ranking matrix from each language and report the highest score. We refer to the three heuristics as MD, SD, and EC, respectively.\nTable 6 displays the results. We note that in the NER evaluations, the performance of mBERT on all the languages but ZH are higher than those in the single-source experiments. This supports our hypothesis that supervision from languages in the\nsame family as the target language helps improve model performance. Different from NER, the evaluation results on POS are not much higher than the single-source evaluation scores, implying that syntactic features are more consistent across languages than appearances of named entities. However, it is consistent on both tasks that pruning attention heads brings performance boosts to all the multisource experiments. While the EC heuristic provides the largest improvement margin in 3 out of 5 experiments, it requires a lot more trial experiments. MD and SD perform comparably well in most cases so they are also promising heuristics for ranking attention heads under the multi-source setting. The results support that pruning attention heads is beneficial to Transformer-based models in cross-lingual tasks even if the training dataset is already large and diverse in languages."
    }, {
      "heading" : "5.3 Extension to Resource-poor Languages",
      "text" : "While the languages we use in the main experiments are not truly resource-poor, we examine our findings when training sets in the target languages are smaller. We design experiments under the multilingual setting with subsampled training datasets in target languages. Specifically, we randomly divide the training set of each target language into 10 disjoint subsets and compare model performance, with and without head pruning, using 1 to 9 sub-\nsets. We do not use 0 or 10 subsets since they correspond to cross-lingual and fully multi-lingual settings, respectively. We run the evaluations on NER and POS tasks. These datasets vary greatly in size, allowing us to validate our findings on targetlanguage datasets with as few as 80 training examples. The UR NER dataset is excluded from this case study since its training set is overly large. We note that the score differences with and without head pruning are, in the main experiments, consistent for all the choices of models and source languages. Thus, we only display the mBERT performance with EN as the source language on NER in Figure 2 and that on POS in Figure 3.\nThe evaluation results are consistent with those in our main experiments, where the model with up to 12 attention heads pruned generally outperforms the full mBERT model. This further supports our hypothesis that pruning lower-ranked attention heads has positive effects on the performance of Transformer-based models in truly resource-scarce languages. It is also worth noting that pruning attention heads often causes the mBERT model to reach peak evaluation scores with less training data in the target language. For example, in the EN-JA NER experiments, the full model achieves the highest F-1 score when all the 800 training instances in the JA dataset are used while the model with heads pruned achieves a comparable score with\n20% less data. This suggests that pruning attention heads makes deep Transformer-based models easier to train with less training data and thus more applicable to truly resource-poor languages."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "This paper studied the contributions of attention heads in Transformer-based models. Past research has shown that in mono-lingual tasks, pruning a large number of attention heads can achieve comparable or higher performance than the full models. However, we were the first to extend these findings to cross-lingual and multi-lingual sequence labeling tasks. Using a gradient-based method, we identified the heads to prune and showed that pruning attention heads generally has positive effects on mBERT and XLM-R performances. Additional case studies empirically demonstrated the validity of our findings and showed further extensibility of them to a wider range of task settings. In addition to better understanding of Transformerbased models under cross- and multi-lingual settings, our findings can be applied to existing models to achieve better performance with reduced training data and resource consumption. Future work could include improving model interpretability in other cross-lingual and multi-lingual tasks, e.g. XNLI (Conneau et al., 2018) and other passage-level classification tasks."
    } ],
    "references" : [ {
      "title" : "Multisource cross-lingual model transfer: Learning what to share",
      "author" : [ "Xilun Chen", "Ahmed Hassan Awadallah", "Hany Hassan", "Wei Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and understanding neural machine translation",
      "author" : [ "Yanzhuo Ding", "Yang Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1150–",
      "citeRegEx" : "Ding et al\\.,? 2017",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised cross-lingual part-ofspeech tagging for truly low-resource scenarios",
      "author" : [ "Ramy Eskander", "Smaranda Muresan", "Michael Collins." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Eskander et al\\.,? 2020",
      "shortCiteRegEx" : "Eskander et al\\.",
      "year" : 2020
    }, {
      "title" : "Pathologies of neural models make interpretations",
      "author" : [ "Shi Feng", "Eric Wallace", "Alvin Grissom II", "Mohit Iyyer", "Pedro Rodriguez", "Jordan Boyd-Graber" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Urdu named entity recognition: Corpus generation and deep learning applications",
      "author" : [ "Safia Kanwal", "Kamran Malik", "Khurram Shahzad", "Faisal Aslam", "Zubair Nawaz." ],
      "venue" : "ACM Trans. Asian Low Resour. Lang. Inf. Process., 19(1):8:1–8:13.",
      "citeRegEx" : "Kanwal et al\\.,? 2020",
      "shortCiteRegEx" : "Kanwal et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER",
      "author" : [ "Phillip Keung", "Yichao Lu", "Vikas Bhardwaj." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Keung et al\\.,? 2019",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2019
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
      "author" : [ "Patrick Littell", "David R. Mortensen", "Ke Lin", "Katherine Kairis", "Carlisle Turner", "Lori Levin." ],
      "venue" : "Proceedings of the 15th Conference of the European",
      "citeRegEx" : "Littell et al\\.,? 2017",
      "shortCiteRegEx" : "Littell et al\\.",
      "year" : 2017
    }, {
      "title" : "Are sixteen heads really better than one? In Advances in Neural Information Processing Systems, volume 32, pages 14014–14024",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig." ],
      "venue" : "Curran Associates, Inc.",
      "citeRegEx" : "Michel et al\\.,? 2019",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Recalloriented learning of named entities in Arabic",
      "author" : [ "Behrang Mohit", "Nathan Schneider", "Rishav Bhowmick", "Kemal Oflazer", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Mohit et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohit et al\\.",
      "year" : 2012
    }, {
      "title" : "Towards lingua franca named entity recognition with bert",
      "author" : [ "Taesun Moon", "Parul Aswathy", "Jian Ni", "Radu Florian." ],
      "venue" : "arXiv preprint arXiv:1912.01389.",
      "citeRegEx" : "Moon et al\\.,? 2019",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2019
    }, {
      "title" : "PersoNER: Persian named-entity recognition",
      "author" : [ "Hanieh Poostchi", "Ehsan Zare Borzeshi", "Mohammad Abdous", "Massimo Piccardi." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Pa-",
      "citeRegEx" : "Poostchi et al\\.,? 2016",
      "shortCiteRegEx" : "Poostchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy",
      "author" : [ "Sofia Serrano", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Serrano and Smith.,? 2019",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Nudging the envelope of direct transfer methods for multilingual named entity recognition",
      "author" : [ "Oscar Täckström." ],
      "venue" : "Proceedings of the NAACLHLT Workshop on the Induction of Linguistic Structure, pages 55–63, Montréal, Canada. Association",
      "citeRegEx" : "Täckström.,? 2012",
      "shortCiteRegEx" : "Täckström.",
      "year" : 2012
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing the structure of attention in a transformer language model",
      "author" : [ "Jesse Vig", "Yonatan Belinkov." ],
      "venue" : "Proceedings of the 2019 ACL Workshop",
      "citeRegEx" : "Vig and Belinkov.,? 2019",
      "shortCiteRegEx" : "Vig and Belinkov.",
      "year" : 2019
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual BERT transformation for zero-shot dependency parsing",
      "author" : [ "Yuxuan Wang", "Wanxiang Che", "Jiang Guo", "Yijia Liu", "Ting Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Single-/multi-source cross-lingual NER via teacher-student learning on unlabeled data in target language",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje Karlsson", "Jian-Guang Lou", "Biqing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end slot alignment and recognition for crosslingual NLU",
      "author" : [ "Weijia Xu", "Batool Haider", "Saab Mansour." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5052–5063, Online. As-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Prior research on mono-lingual Transformer-based (Vaswani et al., 2017) models reveals that a subset of their attention heads makes key contributions to each task, and the models perform comparably well (Voita et al.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : ", 2017) models reveals that a subset of their attention heads makes key contributions to each task, and the models perform comparably well (Voita et al., 2019; Michel et al., 2019) or even better (Kovaleva et al.",
      "startOffset" : 139,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : ", 2017) models reveals that a subset of their attention heads makes key contributions to each task, and the models perform comparably well (Voita et al., 2019; Michel et al., 2019) or even better (Kovaleva et al.",
      "startOffset" : 139,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : ", 2019) or even better (Kovaleva et al., 2019) with the remaining heads pruned 1.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : ", 2019) and XLM-R (Conneau et al., 2020), are widely applied in cross-lingual and multi-lingual NLP tasks 2 (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : ", 2020), are widely applied in cross-lingual and multi-lingual NLP tasks 2 (Wang et al., 2019; Keung et al., 2019; Eskander et al., 2020), no attempt has been made to extend the findings on the aforementioned mono-lingual research to this context.",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : ", 2020), are widely applied in cross-lingual and multi-lingual NLP tasks 2 (Wang et al., 2019; Keung et al., 2019; Eskander et al., 2020), no attempt has been made to extend the findings on the aforementioned mono-lingual research to this context.",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : ", 2020), are widely applied in cross-lingual and multi-lingual NLP tasks 2 (Wang et al., 2019; Keung et al., 2019; Eskander et al., 2020), no attempt has been made to extend the findings on the aforementioned mono-lingual research to this context.",
      "startOffset" : 75,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : ", the middle layers of BERT mainly extract syntactic features (Vig and Belinkov, 2019; Hewitt and Manning, 2019) and the fourth head on the fifth layer of BERT greatly contributes to the coreference resolution task (Clark et al.",
      "startOffset" : 62,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : ", the middle layers of BERT mainly extract syntactic features (Vig and Belinkov, 2019; Hewitt and Manning, 2019) and the fourth head on the fifth layer of BERT greatly contributes to the coreference resolution task (Clark et al.",
      "startOffset" : 62,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : ", the middle layers of BERT mainly extract syntactic features (Vig and Belinkov, 2019; Hewitt and Manning, 2019) and the fourth head on the fifth layer of BERT greatly contributes to the coreference resolution task (Clark et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 235
    }, {
      "referenceID" : 29,
      "context" : "The MultiAtis++ corpus (Xu et al., 2020) is used in the SF evaluations with EN as the source language.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "Additionally, we use the People’s Daily dataset 4, iob2corpus 5, AQMAR (Mohit et al., 2012), ArmanPerosNERCorpus (Poostchi et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : ", 2012), ArmanPerosNERCorpus (Poostchi et al., 2016), MK-PUCIT (Kanwal et al.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : ", 2016), MK-PUCIT (Kanwal et al., 2020), and a news-based NER dataset (Mordecai and Elhadad, 2012) for the languages CN, JA, AR, FA, UR, and",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "As XLM-R applies the SentencePiece tokenization method (Kudo and Richardson, 2018), it is more likely to detect wrong word boundaries and make improper predictions than mBERT in cross-lingual experiments involving ZH or JA datasets.",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "language families, URIEL language distance scores (Littell et al., 2017), and the similarity of the head ranking matrices.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012).",
      "startOffset" : 209,
      "endOffset" : 302
    }, {
      "referenceID" : 16,
      "context" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012).",
      "startOffset" : 209,
      "endOffset" : 302
    }, {
      "referenceID" : 0,
      "context" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012).",
      "startOffset" : 209,
      "endOffset" : 302
    }, {
      "referenceID" : 18,
      "context" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012).",
      "startOffset" : 209,
      "endOffset" : 302
    }, {
      "referenceID" : 20,
      "context" : "Training cross-lingual models on multiple source languages is a practical way to improve their performance, due to enlarged training data size and supervision from source-target languages closer to each other (Wu et al., 2020; Moon et al., 2019; Chen et al., 2019; Rahimi et al., 2019; Täckström, 2012).",
      "startOffset" : 209,
      "endOffset" : 302
    }, {
      "referenceID" : 3,
      "context" : "XNLI (Conneau et al., 2018) and other passage-level classification tasks.",
      "startOffset" : 5,
      "endOffset" : 27
    } ],
    "year" : 2021,
    "abstractText" : "This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multilingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.",
    "creator" : "LaTeX with hyperref"
  }
}