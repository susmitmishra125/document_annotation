{
  "name" : "2021.acl-long.489.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation",
    "authors" : [ "Zixuan Zhang", "Nikolaus Parulian", "Heng Ji", "Ahmed S. Elsayed", "Skatje Myers", "Martha Palmer" ],
    "emails" : [ "hengji}@illinois.edu", "martha.palmer}@colorado.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6261–6270\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6261"
    }, {
      "heading" : "1 Introduction",
      "text" : "The task of Biomedical Information Extraction (IE) aims to extract structured knowledge from biomedical literature, which is usually represented by an information network composed of scientific named\n1Data and source code are publicly available at https: //github.com/zhangzx-uiuc/Knowledge-AMR.\nentities, relations, and key events. It is an essential task for accelerating practical applications of the results and achievements from scientific research. For example, practical progress on combating COVID-19 depends highly on efficient transmission, assessment and extension of cutting-edge scientific research discovery (Wang et al., 2020a; Lybarger et al., 2020; Möller et al., 2020). In this scenario, a powerful biomedical IE system will be able to create a dynamic knowledge base from the surging number of relevant papers, making it more efficient to get access to the latest knowledge and use it for scientific discovery, as well as diagnosis and treatment of patients.\nIE from biomedical scientific papers presents two unique and non-trivial challenges. First, the authors of scientific papers tend to compose long sentences, where the event triggers and entity mentions are usually located far away from each other within the sentence. As shown in Table 1, we can see that compared to the ACE05 dataset in news domain, the average distance between triggers and entities is much longer in biomedical scientific papers. Therefore, it is more difficult for IE models to capture the global context with only flat sequential sentence encoders such as BioBERT (Lee et al., 2020) and SciBERT (Beltagy et al., 2019).\nMoreover, comprehending sentences from scientific papers urgently requires external knowledge, because there are a number of domain-specific un-\nexplained common expressions, acronyms, and abbreviations that are difficult for the model to understand. For instance, as shown in Figure 1, it is nearly impossible for a typical end-to-end model, which only takes in the sentence as input, to get clear understanding of CTF, OTF-1, and OTF-2 without background knowledge. Moreover, the complex biomedical and chemical interactions between multifarious chemicals, genes, and proteins are even harder to understand in addition to the entities themselves.\nTo tackle these two challenges, we propose a novel framework for biomedical IE that integrates Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and external knowledge graphs. AMR is a semantic representation language that converts the meaning of each input sentence into a rooted, directed, labeled, acyclic graph structure. AMR semantic representation includes PropBank (Palmer et al., 2005) frames, non-core semantic roles, coreference, entity typing and linking, modality, and negation. The nodes in AMR are concepts instead of words, and the edge types are much more fine-grained compared with traditional semantic languages like dependency parsing and semantic role labeling. We train a transformer-based AMR semantic parser (Fernandez Astudillo et al., 2020) on biomedical scientific texts and use it in our biomedical IE model. To better handle long\nsentences with distant trigger and entity pairs, we use AMR parsing to compress each sentence and to better capture global interactions between tokens. For example, as shown in Figure 1, the Positive Regulation event trigger “changes” is located far away from its arguments CTF, OTF-1, OTF-2 in the original sentence. However, in the AMR graph, such trigger-entity pairs are linked within two hops. Therefore, it will be much easier for the model to identify such kinds of events with the guidance of AMR parsing.\nIn addition, to make better use of the external knowledge, we extract a global knowledge graph from the Comparative Toxicogenomics Database (CTDB) that covers all biomedical entities in the corpus. For each sentence, we select a minimal connected subgraph as the sentence-level KG. We use this sentence KG to enrich AMR nodes and edges to give the model additional prior domain knowledge, especially the biomedical and chemical interactions between different genes and proteins. These fine-grained relations are important for biomedical event extraction. For example, as in Figure 1, the incorporation of the external KG can indicate that Mono Mac 6 can result in leukemia, which will affect the expression of CTF, OTF-1, and OFT-2 proteins. With this external knowledge, it will be much easier for the model to identify such proteins as the arguments of a Positive Regulation event. We encode the knowledge-enriched AMR graph using an edge-conditioned graph attention network (GAT) that is able to incorporate finegrained edge features before conducting IE tasks. We evaluate our model on the existing benchmark GENIA-2011 dataset where our model greatly outperforms our baseline model by 4.8%. In addition to the existing GENIA-2011 benchmark, we also aim to evaluate the effectiveness of our framework on topic-specific literature. We develop a new ontology for entities and events with a large corpus from COVID-19 research papers, which is specifically annotated by medical professionals and can serve as a new benchmark for the biomedical IE community.\nThe major contributions of this paper are summarized as follows.\n• We are the first to enrich the AMR graph with the external knowledge and use a graph neural network to incorporate the fine-grained edge features.\n• We evaluate our model and create a new state-\nof-the-art for biomedical event extraction on the GENIA-2011 corpus.\n• We develop a new dataset from COVID-19 related research papers based on a new ontology that contains 25 fine-grained entity types and 14 event types."
    }, {
      "heading" : "2 Approach",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "As shown in Figure 2, our proposed biomedical information extraction framework mainly consists of four steps. First, we extract a global knowledge graph (KG) that contains all the entities from the corpus, and select out a sentence-level knowledge subgraph for the input sentence. Then, we perform AMR parsing and construct the sentencelevel AMR graph, and use the sentence knowledge subgraph to enrich the AMR graph by adding additional nodes and edges. After that, given the contextualized word embeddings, we first identify entity and trigger spans, and then conduct message passing on the knowledge enriched AMR graph based on an edge-conditioned GAT. Finally, we use feed-forward neural networks based classifiers for trigger and argument labeling."
    }, {
      "heading" : "2.2 Knowledge Graph Construction",
      "text" : "Global Knowledge Graph We use the Comparative Toxicogenomics Database (CTDB)2 which contains fine-grained biomedical and chemical interactions between chemicals, genes, and diseases. We construct a global knowledge graph that involves all entities from the corpus with their pairwise chemical interactions. We extract these entity pairs with their biomedical interactions as triples, e.g., in Figure 1, (Mono Mac 6, results, leukumia) indicates that Mono Mac 6 cell can result in the disease of leukemia. We merge all the extracted triples and form a global knowledge graph Gg = (V g, Eg). Our extracted global KG consists of 39,436 nodes and 590,235 edges.\nSentence-level Knowledge Graph Given an input sentence, we aim to generate a sentence-level KG by selecting out a subgraph from the global KG, which contains the external knowledge between all entities within the sentence. Given an input sentence S, we use SciSpacy3 to obtain all the related biomedical entities, including genes,\n2http://ctdbase.org/ 3https://allenai.github.io/scispacy/\nchemicals, cells, and proteins. We then link each entity mention from the sentence to the nodes in global KG Gg = (V g, Eg). To select the sentence subgraph from the global KG, given the set of entity mentions E = {ε1, · · · , ε|E|} (where each εi is a word span), we select the connected subgraph that covers all entity mentions in E with the minimal number of nodes as the sentence KG. Note that such a sentence KG construction procedure can be accomplished in linear time complexity in terms of the number of nodes |V g|. This can be done by first traversing all the nodes in the global KG using depth-first search and obtaining all connected subgraphs of Gg in linear time. After that, we select the set of subgraphs that can cover E and then choose the one Gs = (V s, Es) with the minimal number of nodes as the sentence KG."
    }, {
      "heading" : "2.3 KG-enriched AMR parsing",
      "text" : "AMR Parsing After obtaining the sentence KG, we fuse it with the AMR graph as an external knowledge enrichment procedure. Given an input sentence S = {w1, w2, · · · , wN}, we first perform AMR parsing and obtain a sentence-level AMR graph GA = (V A, EA) with an alignment between AMR nodes and the spans in the original sentence. We employ the transformer-based AMR parser4 (Fernandez Astudillo et al., 2020) pretrained on the Biomedical AMR corpus5 released from the AMR official website. Each node vAi = (m A i , n A i ) ∈ V a represents an AMR concept or predicate, and we use (mAi , n A i ) to denote the corresponding span for such an AMR node. For AMR edges, we use eAi,j to denote the specific relation type between nodes vAi and v A j in AMR annotations (e.g., ARG-x, :time, :location, etc.). We randomly initialize the edge embeddings as a lookup embedding matrix EAMR, which is optimized in end-to-end training.\nEnrich AMR with sentence KG Given a pair of AMR graph GA and sentence KG GS , we fuse them into an enriched AMR graph G = (V,E) as the external reference for the subsequent information extraction tasks. In general, there are three cases for fusing each sentence’s KG nodes vsi ∈ V s into the AMR graph. First, if vsi represents an entity within the sentence, and there is also an AMR\n4https://github.com/IBM/ transition-amr-parser\n5https://amr.isi.edu/download/2018-01-25/ amr-release-bio-v3.0.txt\nnode vAj with the same span, we then match v s i to vAj and add all KG edges linked to v s i into the AMR graph. Second, if vsi represents an entity within the sentence, but there is not any AMR node vAj with a matched span, we then add a new node (as well as all related edges) into the AMR graph. Third, if vsi is an additional KG node that does not represent any entity in the sentence, we directly add this node into the AMR graph with all related KG edges. After we match and link all the sentence KG nodes towards the AMR graph, we obtain the fused graph G = (V,E). Note that such a graph fusion procedure could result in multiple edges between a pair of nodes. We keep all these edges with their embeddings for the subsequent message passing procedure. The illustration for the graph fusion procedure is shown in Figure 2."
    }, {
      "heading" : "2.4 Node Identification and Message Passing",
      "text" : "Contextualized Encoder Given an input sentence S, we use the BERT model pretrained on biomedical scientific texts (Lee et al., 2020) to obtain the contextualized word representations {x1,x2, · · · ,xN}. If one word is split into multiple pieces by the BERT tokenizer, we take the average of the representation vectors for all pieces as the final word representation.\nNode Identification After encoding the input sentence using BERT, we first identify the entity and trigger spans as the candidate nodes. Similar to (Wadden et al., 2019), given the contextualized word representations, we first enumerate all possible spans up to a fixed lengthK, and calculate each span representation according to the concatenation of the left and right endpoints and a trainable fea-\nture vector characterizing the span length6. Specifically, given each span si = [start(i), end(i)], the span representation vector is:\nsi = [ xstart(i),xend(i), z(si) ] , (1)\nwhere z(si) denotes a trainable feature vector that is only determined by the span length. We use separate binary classifiers for each specific entity and trigger type to handle the spans with multiple labels. Each binary classifier is a feed-forward neural network with ReLU activation in the hidden layer, which is trained with binary cross-entropy loss jointly with the whole model. In the diagnostic setting of using gold-standard entity mentions, we only employ span enumeration for event trigger identification, and use the gold-standard entity set for the following event extraction steps.\nEdge-conditioned GAT To fully exploit the information of external knowledge and AMR semantic structure, similar to (Zhang and Ji, 2021), we use an L-layer graph attention network to let the model aggregate neighbor information from the fused graph G = (V,E). We use hli to denote the node feature for vi ∈ V in layer l, and ei,j to represent the edge feature vector for ei,j ∈ E. To update the node feature from l to l + 1, we first calculate the attention score for each neighbor j ∈ Ni based on the concatenation of node features hli, h l j and edge features ei,j .\nαli,j = exp\n( σ ( f l[Whli : Weei,j : Wh l j ] ))\n∑ k∈Ni exp ( σ ( f l[Whli : Weei,k : Wh l k] )) ,\n6We use different maximum span length K for entity and trigger spans.\nwhere W, We are trainable parameters, and f l and σ(·) are a single layer feed-forward neural network and LeakyReLU activation function respectively. Then we obtain the neighborhood information h∗i by the weighted sum of all neighbor features:\nh∗i = ∑ k∈Ni αli,jW ∗hlk,\nwhere W∗ is a trainable parameter. The updated node feature is calculated by a combination of the original node feature and its neighborhood information, where γ controls the level of message passing between neighbors.\nhl+1i = h l i + γ · h∗i (2)\nNote that our edge-conditioned GAT structure is similar to (Huang et al., 2020). The main difference is that (Huang et al., 2020) only uses edge features for calculating the attention score αli,j , while we use the concatenation of the feature vectors of each edge and its involved pair of nodes. Such a method can better characterize differing importance levels for neighbor nodes, and thus yield better model performance. We select the last layer hLi as the final representation for each entity or trigger.\nMessage Passing Given the knowledge enriched AMR graphG = (V,E) and representation vectors of extracted trigger and entity spans, we initialize the feature vectors for nodes and edges as follows. For each KG node vsi which does not belong to any AMR node, we initialize its feature vectors vsi using KG embeddings pre-trained on the global KG using TransE (Bordes et al., 2013). For each original AMR node vAi = (m A i , n A i ), we first calculate its span representation vAi according to Eq. (1), and then use a linear transformation WAvAi + b\nA to initialize the node feature vector h0i . For edge features, we use pre-trained TransE embeddings for KG edges, and use the trainable embedding matrix EAMR for AMR relations. We use our proposed edge-conditioned GAT to conduct message passing and get the feature vectors from the final layer as the updated node representations. We obtain the final representation vectors for the trigger and entity nodes and denote them as {τ1, · · · , τ|T |} and {ε1, · · · , ε|E|} respectively."
    }, {
      "heading" : "2.5 Biomedical Event Extraction",
      "text" : "Model Training Given the event trigger set T with the event trigger representations τi, and the\nentity set E with the representations εi, we use LI to denote the loss for binary classifiers for event trigger and entity extraction in the node identification step. For event argument role labeling, we concatenate candidate trigger-entity pairs or triggertrigger pairs (for nested events) and feed them into two separate FFNs (with softmax activation function in the output layer) for role type classification, where we have ytti,j = FFNtt ([τi : τj ]) or ytei,j = FFNte ([τi : εj ]). The overall training objective is defined in a multi-task setting, which includes the cross-entropy loss for trigger and argument classification, as well as the binary classification loss LI .\nL = LI− ∑ i,j ytti,j log ŷ tt i,j− ∑ i,j ytei,j log ŷ te i,j . (3)"
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al., 2011) dataset consisting of both abstracts and main body texts from biomedical scientific papers. Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types. We do not incorporate event ontology or training data from the newer versions of the BioNLP GENIA shared tasks (e.g., GENIA 2013) to ensure fair comparisons with previous models. The statistics of this dataset are shown in Table 2. The original GENIA dataset\nis annotated in paragraphs. Following (Li et al., 2019), we focus on sentence-level event extraction and only keep events and argument roles within each sentence (around 94% of the events).\nImplementation Details For pretrained KG embeddings, we use 600-dim embedding vectors pre-trained on the global knowledge graph using\nTransE. We use a two-layer edge-conditioned GAT and the feature dimensions are 2048 for nodes and 256 for edges. Specifically, the FFNs consist of two layers with a dropout rate of 0.4, where the numbers of hidden units are 150 for entity extraction and 600 for event extraction. We train our model with Adam (Kingma and Ba, 2015) on NVIDIA Tesla V100 GPUs for 80 epochs (approximately takes 4 minutes for 1 training epoch) with learning rate 1e-5 for BERT parameters and 5e-3 for other parameters. We select the model checkpoint with optimal F1-Score on the development set to evaluation on the test set from the official website."
    }, {
      "heading" : "3.2 Baselines and Ablation Variants",
      "text" : "We consider the most recent models on biomedical event extraction: KB-Tree-LSTM (Li et al., 2019), GEANet (Huang et al., 2020), BEESL (Ramponi et al., 2020), and DeepEventMine (Trieu et al., 2020) for comparison in our experiments, and we report the precision, recall, and F1 score from the GENIA 2011 online test set evaluation service7. In addition to the previous models, we also conduct ablation studies to evaluate the contributions of different parts in our model. We adopt the model variants BERT-Flat and BERT-AMR, where BERTFlat only uses the BERT representations without any help from AMR and KG, and BERT-AMR denotes the model with an edge-conditioned GAT to encode the AMR graph without incorporating external knowledge."
    }, {
      "heading" : "3.3 Overall Performance",
      "text" : "We report the performance of our model and compare it with the most recent biomedical IE models KB-Tree-LSTM (Li et al., 2019), GEANet (Huang et al., 2020), BEESL (Ramponi et al., 2020), and DeepEventMine (Trieu et al., 2020) in Table 3. In general, our KG enriched AMR model can achieve slightly higher performance compared with the state-of-the-art model DeepEventMine. Besides, our model greatly outperforms all other previous models for biomedical event extraction. To further measure the impact of each individual part in our model, we also introduce two model variants for the ablation study. We can see that compared with simply finetuning a flat BERT model, the AMR parsing contributes a 1.84% absolute gain on F1-Score, while the incorporation of external\n7http://bionlp-st.dbcls.jp/GE/2011/ eval-test/\nknowledge graph contributes 2.95%. We also report the overall development set F1 scores without using gold-standard entities, and compare the performance with BEESL in Table 4. We can discover that our model performs significantly better than the BEESL model, which proves that our model can better handle practical scenarios without goldstandard entities."
    }, {
      "heading" : "3.4 Case Study on COVID-19 Dataset",
      "text" : "COVID-19 Dataset In order to evaluate the impact of our approach on real-world problems, besides the GENIA dataset, we also develop a new dataset specifically labeled by medical professionals from research papers related to COVID-19. We select out 186 full-text articles with 12,916 sentences from PubMed and PMC. Three experienced annotators who are biomedical domain experts have participated in the annotation, and the Cohen’s Kappa scores for pairwise agreement between the annotators are 0.79, 0.84, and 0.74 respectively. The pre-defined entity and event type distributions in this dataset are shown in Table 6.\nResults We evaluate our proposed model by removing the event argument labeling procedure to accommodate a scenario limited to entity and event trigger labeling, that is, we remove the argument role classifiers FFNtt and FFNte while the overall training loss in Eq. (3) only contains the first two terms for span identification and event trigger classification. As shown in Table 5, our model achieves 78.05% overall F1 score with 83.60% F1 on entity extraction task and 72.37% F1 on event extraction.\nThe entity extraction performance on the COVID dataset is lower than typical coarse-grained entity extraction model performance for BERT-like models on other datasets (e.g., our model can get around 86% F1 score for entity extraction on GENIA-2011 development set). This is probably because our proposed COVID-19 dataset is challenging with more find-grained biomedical entity and event types."
    }, {
      "heading" : "3.5 Qualitative Analysis",
      "text" : "We select two typical examples in Table 7 to show how KG enriched AMR parsing helps to improve the performance of biomedical IE.\nIn the first example, we can see that the flat model fails to identify CAII as an entity of the bind event, which is probably due to the long distance between the trigger bind and the argument CAII (the model successfully detects the other two arguments V-erbA and C-erbA because they are much nearer). With the help of AMR parsing, the model successfully links CAII to the bind event since in the AMR graph, the three entities C-erbA, V-erbA, and CAII are located within the same number of hops from the bind trigger. But the model still cannot recognize CAII as the theme of transcription. This\nis probably because the model is not clear what whose refers to in the sentence. However, with the help of external knowledge, the model knows in advance that V-erbA could inhibit the transcription of CAII, thus it is able to identify CAII as the theme of the transcription event.\nIn the second example, the flat model is confused about which entity belongs to which event between two binding events in the same sentence. Here, the AMR parsing provides a clear tree structure and guides the model to correctly link the event-entity pairs (i.e., heterodimers with RAR beta, binding with VDR). However, the BERT-AMR model still fails to identify heterodimers as the theme of stimulated. With the further help of the external KG, the model knows in advance that RA can stimulate the generation of RAR beta heterodimers, and thus it is able to correctly identify a positive regulation between these two triggers."
    }, {
      "heading" : "3.6 Remaining Challenges",
      "text" : "We compare the predictions from our model with the gold-standard annotations on the development set and discover the following typical remaining error cases.\nNon-verb Event Triggers Most of the biomedical events are triggered by verbs (bind, express, etc.) or their noun forms (binding, expression, etc.). However, there are also events triggered by adjectives (e.g., subsequent), proper nouns (e.g., mRNA, SiRNA), and even prepositions (e.g., from) and conjunctions (e.g., rather than). Our model misses a lot of these non-verb event triggers due to the insufficient training examples.\nMisleading Verb Prefix We also find that the prefix of a verb can sometimes be misleading for event trigger classification, especially for Negative Regulation events. Many Negative Regulation events are triggered by words with certain styles of prefix (in- or de-), e.g., inactivation, inactivated, decrease, degradation, etc., representing some negative interactions. As a result, the model mistakenly labels many other words with the same prefixes as Negative Regulation event triggers. For example, in the sentence: Dephosphorylation of 4E-BP1 was also observed ..., the word dephosphorylation should not be classified as a Negative Regulation event although it has a de- prefix. Because dephosphorylation denotes an inverse chemical process of phosphorylation rather than negative regulation between different events or proteins. This is\nprobably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns."
    }, {
      "heading" : "4 Related Work",
      "text" : "Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR semantic parsing and external knowledge to improve the performance.\nUtilization of External Knowledge In terms of utilization of external knowledge, (Li et al., 2019) proposes a knowledge-driven Tree-LSTM framework to capture dependency structures and entity properties from an external knowledge base. More recently, GEANet (Huang et al., 2020) in-\ntroduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019).\nBiomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020). Our COVID-19 dataset will further advance the field in developing effective IE techniques specifically for the COVID-19 domain."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "In this paper, we propose a novel biomedical Information Extraction framework to effectively tackle two unique challenges for scientific domain IE:\ncomplex sentence structure and unexplained concepts. We utilize AMR parsing to compress wide contexts, and incorporate external knowledge into the AMR. Our proposed model produces significant performance gains compared with most stateof-the-art methods. In the future, we intend to exploit tables and figures in the scientific literature for multimedia representation. We also plan to further incorporate coreference graphs among sentences to further enrich contexts. We will also continue exploring the use of richer information from an external knowledge base to further improve the model’s performance."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This research is based upon work supported by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, NSF No. 2034562, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract No. FA8650-17C-9116, and Air Force No. FA8650-17-C-7715. Any opinions, findings and conclusions or recommendations expressed in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Implementation Details For pretrained KG embeddings, we use 600-dim embedding vectors pre-trained on the global knowledge graph using TransE. We use a two-layer edgeconditioned GAT and the feature dimensions are 2048 for nodes and 256 for edges. Specifically, the FFNs consist of two layers with a dropout rate of 0.4, where the numbers of hidden units are 150 for entity extraction and 600 for event extraction. We train our model with Adam (Kingma and Ba, 2015) on NVIDIA Tesla V100 GPUs for 80 epochs (approximately takes 4 minutes for 1 training epoch) with learning rate 1e-5 for BERT parameters and 5e-3 for other parameters. We select the model checkpoint with optimal F1-Score on the development set to evaluation on the test set from the official website. The detailed hyper-parameter settings are shown in Table 8."
    } ],
    "references" : [ {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguis-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting background knowledge for relation extraction",
      "author" : [ "Yee Seng Chan", "Dan Roth." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 152–160.",
      "citeRegEx" : "Chan and Roth.,? 2010",
      "shortCiteRegEx" : "Chan and Roth.",
      "year" : 2010
    }, {
      "title" : "Relational inference for wikification",
      "author" : [ "Xiao Cheng", "Dan Roth." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1787–1796, Seattle, Washington, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Cheng and Roth.,? 2013",
      "shortCiteRegEx" : "Cheng and Roth.",
      "year" : 2013
    }, {
      "title" : "A search-based neural model for biomedical nested and overlapping event detection",
      "author" : [ "Kurt Junshean Espinosa", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Espinosa et al\\.,? 2019",
      "shortCiteRegEx" : "Espinosa et al\\.",
      "year" : 2019
    }, {
      "title" : "Transition-based parsing with stacktransformers",
      "author" : [ "Ramón Fernandez Astudillo", "Miguel Ballesteros", "Tahira Naseem", "Austin Blodgett", "Radu Florian." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Astudillo et al\\.,? 2020",
      "shortCiteRegEx" : "Astudillo et al\\.",
      "year" : 2020
    }, {
      "title" : "Biomedical event extraction with hierarchical knowledge graphs",
      "author" : [ "Kung-Hsiang Huang", "Mu Yang", "Nanyun Peng." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1277–1285, Online. Association for Compu-",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of Genia event task in BioNLP shared task 2011",
      "author" : [ "Jin-Dong Kim", "Yue Wang", "Toshihisa Takagi", "Akinori Yonezawa." ],
      "venue" : "Proceedings of BioNLP Shared Task 2011 Workshop, pages 7–15, Portland, Oregon, USA. Association for Computa-",
      "citeRegEx" : "Kim et al\\.,? 2011",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinform., 36(4):1234–",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Biomedical event extraction based on knowledgedriven tree-LSTM",
      "author" : [ "Diya Li", "Lifu Huang", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Cord-19: The covid-19 open research dataset. Arxiv",
      "author" : [ "K Lo", "Y Chandrasekhar", "R Reas", "J Yang", "D Eide", "K Funk", "R Kinney", "Z Liu", "W Merrill", "P Mooney" ],
      "venue" : null,
      "citeRegEx" : "Lo et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting covid-19 diagnoses and symptoms from clinical text: A new annotated corpus and neural event extraction framework",
      "author" : [ "Kevin Lybarger", "Mari Ostendorf", "Matthew Thompson", "Meliha Yetisgen" ],
      "venue" : null,
      "citeRegEx" : "Lybarger et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lybarger et al\\.",
      "year" : 2020
    }, {
      "title" : "Event extraction as dependency parsing",
      "author" : [ "David McClosky", "Mihai Surdeanu", "Christopher D. Manning." ],
      "venue" : "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference,",
      "citeRegEx" : "McClosky et al\\.,? 2011",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2011
    }, {
      "title" : "COVID-QA: A question answering dataset for COVID-19",
      "author" : [ "Timo Möller", "Anthony Reina", "Raghavan Jayakumar", "Malte Pietsch." ],
      "venue" : "Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Lin-",
      "citeRegEx" : "Möller et al\\.,? 2020",
      "shortCiteRegEx" : "Möller et al\\.",
      "year" : 2020
    }, {
      "title" : "The proposition bank: A corpus annotated with semantic roles",
      "author" : [ "Martha Palmer", "Dan Gildea", "Paul Kingsbury." ],
      "venue" : "Computational Linguistics Journal, 31(1).",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "Improving question answering with external knowledge",
      "author" : [ "Xiaoman Pan", "Kai Sun", "Dian Yu", "Jianshu Chen", "Heng Ji", "Claire Cardie", "Dong Yu." ],
      "venue" : "arXiv preprint arXiv:1902.00993.",
      "citeRegEx" : "Pan et al\\.,? 2019",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint inference for knowledge extraction from biomedical literature",
      "author" : [ "Hoifung Poon", "Lucy Vanderwende." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Poon and Vanderwende.,? 2010",
      "shortCiteRegEx" : "Poon and Vanderwende.",
      "year" : 2010
    }, {
      "title" : "Biomedical event extraction as sequence labeling",
      "author" : [ "Alan Ramponi", "Rob van der Goot", "Rosario Lombardo", "Barbara Plank." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5357–5367,",
      "citeRegEx" : "Ramponi et al\\.,? 2020",
      "shortCiteRegEx" : "Ramponi et al\\.",
      "year" : 2020
    }, {
      "title" : "Biomedical event extraction using Abstract Meaning Representation",
      "author" : [ "Sudha Rao", "Daniel Marcu", "Kevin Knight", "Hal Daumé III." ],
      "venue" : "BioNLP 2017, pages 126–135, Vancouver, Canada,. Association for Computational Linguistics.",
      "citeRegEx" : "Rao et al\\.,? 2017",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end qa on covid-19: Domain adaptation with synthetic training",
      "author" : [ "Revanth Gangi Reddy", "Bhavani Iyer", "Md Arafat Sultan", "Rong Zhang", "Avi Sil", "Vittorio Castelli", "Radu Florian", "Salim Roukos." ],
      "venue" : "arXiv preprint arXiv:2012.01414.",
      "citeRegEx" : "Reddy et al\\.,? 2020",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2020
    }, {
      "title" : "Deepeventmine: end-to-end neural nested event extraction from biomedical texts",
      "author" : [ "Hai-Long Trieu", "Thy Thy Tran", "Anh-Khoa Duong Nguyen", "Anh Nguyen", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Bioinformatics, 36(19):4910–4917.",
      "citeRegEx" : "Trieu et al\\.,? 2020",
      "shortCiteRegEx" : "Trieu et al\\.",
      "year" : 2020
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Covid-19 literature knowledge graph construction and drug repurposing report generation",
      "author" : [ "Chang", "James Pustejovsky", "David Liem", "Ahmed Elsayed", "Martha Palmer", "Jasmine Rah", "Cynthia Schneider", "Boyan Onyshkevych." ],
      "venue" : "arXiv:2007.00576.",
      "citeRegEx" : "Chang et al\\.,? 2020a",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2020
    }, {
      "title" : "Biomedical event extraction as multi-turn question answering",
      "author" : [ "Xing David Wang", "Leon Weber", "Ulf Leser." ],
      "venue" : "Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis, pages 88–96, Online. Association for Com-",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic textual evidence mining in covid-19 literature",
      "author" : [ "Xuan Wang", "Weili Liu", "Aabhas Chauhan", "Yingjun Guan", "Jiawei Han." ],
      "venue" : "arXiv preprint arXiv:2004.12563.",
      "citeRegEx" : "Wang et al\\.,? 2020c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Covid-19 knowledge graph: accelerating information retrieval and discovery for scientific liter",
      "author" : [ "Colby Wise", "Vassilis N Ioannidis", "Miguel Romero Calvo", "Xiang Song", "George Price", "Ninad Kulkarni", "Ryan Brand", "Parminder Bhatia", "George Karypis" ],
      "venue" : null,
      "citeRegEx" : "Wise et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wise et al\\.",
      "year" : 2020
    }, {
      "title" : "Abstract Meaning Representation guided graph encoding and decoding for joint information extraction",
      "author" : [ "Zixuan Zhang", "Heng Ji." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Zhang and Ji.,? 2021",
      "shortCiteRegEx" : "Zhang and Ji.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "For example, practical progress on combating COVID-19 depends highly on efficient transmission, assessment and extension of cutting-edge scientific research discovery (Wang et al., 2020a; Lybarger et al., 2020; Möller et al., 2020).",
      "startOffset" : 167,
      "endOffset" : 231
    }, {
      "referenceID" : 14,
      "context" : "For example, practical progress on combating COVID-19 depends highly on efficient transmission, assessment and extension of cutting-edge scientific research discovery (Wang et al., 2020a; Lybarger et al., 2020; Möller et al., 2020).",
      "startOffset" : 167,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "Therefore, it is more difficult for IE models to capture the global context with only flat sequential sentence encoders such as BioBERT (Lee et al., 2020) and SciBERT (Beltagy et al.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "To tackle these two challenges, we propose a novel framework for biomedical IE that integrates Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and external knowledge graphs.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "AMR semantic representation includes PropBank (Palmer et al., 2005) frames, non-core semantic roles, coreference, entity typing and linking, modality, and negation.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "Contextualized Encoder Given an input sentence S, we use the BERT model pretrained on biomedical scientific texts (Lee et al., 2020) to obtain the contextualized word representations {x1,x2, · · · ,xN}.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Similar to (Wadden et al., 2019), given the contextualized word representations, we first enumerate all possible spans up to a fixed lengthK, and calculate each span representation according to the concatenation of the left and right endpoints and a trainable feature vector characterizing the span length6.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "Edge-conditioned GAT To fully exploit the information of external knowledge and AMR semantic structure, similar to (Zhang and Ji, 2021), we use an L-layer graph attention network to let the model aggregate neighbor information from the fused graph G = (V,E).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Note that our edge-conditioned GAT structure is similar to (Huang et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "The main difference is that (Huang et al., 2020) only uses edge features for calculating the attention score αl i,j , while we use the concatenation of the feature vectors of each edge and its involved pair of nodes.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al.",
      "startOffset" : 34,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al.",
      "startOffset" : 34,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al.",
      "startOffset" : 34,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : ", 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al., 2011) dataset consisting of both abstracts and main body texts from biomedical scientific papers.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Following (Li et al., 2019), we focus on sentence-level event extraction and only keep events and argument roles within each sentence (around 94% of the events).",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "We train our model with Adam (Kingma and Ba, 2015) on NVIDIA Tesla V100 GPUs for 80 epochs (approximately takes 4 minutes for 1 training epoch) with learning rate 1e-5 for BERT parameters and 5e-3 for other parameters.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "We consider the most recent models on biomedical event extraction: KB-Tree-LSTM (Li et al., 2019), GEANet (Huang et al.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : ", 2019), GEANet (Huang et al., 2020), BEESL (Ramponi et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : ", 2020), BEESL (Ramponi et al., 2020), and DeepEventMine (Trieu et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : ", 2020), and DeepEventMine (Trieu et al., 2020) for comparison in our experiments, and we",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "We report the performance of our model and compare it with the most recent biomedical IE models KB-Tree-LSTM (Li et al., 2019), GEANet (Huang et al.",
      "startOffset" : 109,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : ", 2019), GEANet (Huang et al., 2020), BEESL (Ramponi et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : ", 2020), BEESL (Ramponi et al., 2020), and DeepEventMine (Trieu et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : ", 2020), and DeepEventMine (Trieu et al., 2020) in Table 3.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al.",
      "startOffset" : 31,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al.",
      "startOffset" : 31,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : ", 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al.",
      "startOffset" : 51,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : ", 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al.",
      "startOffset" : 51,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : ", 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural mod-",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "els (Espinosa et al., 2019), and multi-turn question answering (Wang et al.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 24,
      "context" : ", 2019), and multi-turn question answering (Wang et al., 2020b).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "Utilization of External Knowledge In terms of utilization of external knowledge, (Li et al., 2019) proposes a knowledge-driven Tree-LSTM framework to capture dependency structures and entity properties from an external knowledge base.",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al.",
      "startOffset" : 96,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al.",
      "startOffset" : 96,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019).",
      "startOffset" : 187,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : ", 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : ", 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al.",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : ", 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 132
    } ],
    "year" : 2021,
    "abstractText" : "Biomedical Information Extraction from scientific literature presents two unique and nontrivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model’s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledgeenriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.1",
    "creator" : "LaTeX with hyperref"
  }
}