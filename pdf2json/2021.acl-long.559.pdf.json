{
  "name" : "2021.acl-long.559.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling",
    "authors" : [ "Yikang Shen", "Yi Tay", "Che Zheng", "Dara Bahri", "Aaron Courville" ],
    "emails" : [ "yikang.shn@gmail.ca." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7196–7209\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7196"
    }, {
      "heading" : "1 Introduction",
      "text" : "Human languages have a rich latent structure. This structure is multifaceted, with the two major classes of grammar being dependency and constituency structures. There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a). The core principle behind recent methods that induce structure from data is simple - provide an inductive bias that is conducive for structure to emerge as a byproduct of some self-supervised training, e.g., language modeling. To this end, a wide range of models have been proposed that are able to successfully learn grammar structures (Shen et al., 2018a,c;\n∗ Corresponding author: yikang.shn@gmail.ca. Work done while interning at Google Reseach.\nWang et al., 2019; Kim et al., 2019b,a). However, most of these works focus on inducing either constituency or dependency structures alone.\nIn this paper, we make two important technical contributions. First, we introduce a new neural model, StructFormer, that is able to simultaneously induce both dependency structure and constituency structure. Specifically, our approach aims to unify latent structure induction of different types of grammar within the same framework. Second, StructFormer is able to induce dependency structures from raw data in an end-to-end unsupervised fashion. Most existing approaches induce dependency structures from other syntactic information like gold POS tags (Klein and Manning, 2004; Cohen and Smith, 2009; Jiang et al., 2016). Previous works, having trained from words alone, often requires additional information, like pre-trained word clustering (Spitkovsky et al., 2011), pre-trained word embedding (He et al., 2018), acoustic cues (Pate and Goldwater, 2013), or annotated data from related languages (Cohen et al., 2011).\nWe introduce a new inductive bias that enables the Transformer models to induce a directed dependency graph in a fully unsupervised manner. To avoid the necessity of using grammar labels during training, we use a distance-based parsing mechanism. The parsing mechanism predicts a sequence of Syntactic Distances T (Shen et al., 2018b) and a sequence of Syntactic Heights ∆ (Luo et al., 2019) to represent dependency graphs and constituency trees at the same time. Examples of ∆ and T are illustrated in Figure 1a. Based on the syntactic distances (T) and syntactic heights (∆), we provide a new dependency-constrained self-attention layer to replace the multi-head self-attention layer in standard transformer model. More specifically, the new attention head can only attend its parent (to avoid confusion with self-attention head, we use “parent” to denote “head” in dependency graph) or\n(a) An example of Syntactic Distances T (grey bars) and Syntactic Heights ∆ (white bars). In this example, like is the parent (head) of constituent (like cats) and (I like cats).\n(b) Two types of dependency relations. The parent distribution allows each token to attend on its parent. The dependent distribution allows each token to attend on its dependents. For example the parent of cats is like. Cats and I are dependents of like Each attention head will receive a different weighted sum of these relations.\nFigure 1: An example of our parsing mechanism and dependency-constrained self-attention mechanism. The parsing network first predicts the syntactic distance T and syntactic height ∆ to represent the latent structure of the input sentence I like cats. Then the parent and dependent relations are computed in a differentiable manner from T and ∆.\nits dependents in the predicted dependency structure, through a weighted sum of relations shown in Figure 1b. In this way, we replace the complete graph in the standard transformer model with a differentiable directed dependency graph. During the process of training on a downstream task (e.g. masked language model), the model will gradually converge to a reasonable dependency graph via gradient descent.\nIncorporating the new parsing mechanism, the dependency-constrained self-attention, and the Transformer architecture, we introduce a new model named StructFormer. The proposed model can perform unsupervised dependency and constituency parsing at the same time, and can leverage the parsing results to achieve strong performance on masked language model tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous works on unsupervised dependency parsing are primarily based on the dependency model with valence (DMV) (Klein and Manning, 2004) and its extension (Daumé III, 2009; Gillenwater et al., 2010). To effectively learn the DMV model for better parsing accuracy, a variety of inductive biases and handcrafted features, such as correlations between parameters of grammar rules involving different part-of-speech (POS) tags, have been proposed to incorporate prior information into learning. The most recent progress is the neural DMV model (Jiang et al., 2016), which uses a neural network model to predict the grammar rule probabilities\nbased on the distributed representation of POS tags. However, most existing unsupervised dependency parsing algorithms require the gold POS tags to ge provided as inputs. These gold POS tags are labeled by humans and can be potentially difficult (or prohibitively expensive) to obtain for large corpora. Spitkovsky et al. (2011) proposed to overcome this problem with unsupervised word clustering that can dynamically assign tags to each word considering its context. He et al. (2018) overcame the problem by combining DMV model with invertible neural network to jointly model discrete syntactic structure and continuous word representations.\nUnsupervised constituency parsing has recently received more attention. PRPN (Shen et al., 2018a) and ON-LSTM (Shen et al., 2018c) induce tree structure by introducing an inductive bias to recurrent neural networks. PRPN proposes a parsing network to compute the syntactic distance of all word pairs, while a reading network uses the syntactic structure to attend to relevant memories. ON-LSTM allows hidden neurons to learn longterm or short-term information by a novel gating mechanism and activation function. In URNNG (Kim et al., 2019b), amortized variational inference was applied between a recurrent neural network grammar (RNNG) (Dyer et al., 2016) decoder and a tree structure inference network, which encourages the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) proposed using inside-outside dynamic programming to compose latent representations from all possible binary\ntrees. The representations of inside and outside passes from the same sentences are optimized to be close to each other. The compound PCFG (Kim et al., 2019a) achieves grammar induction by maximizing the marginal likelihood of the sentences which are generated by a probabilistic context-free grammar (PCFG). Tree Transformer (Wang et al., 2019) adds extra locality constraints to the Transformer encoder’s self-attention to encourage the attention heads to follow a tree structure such that each token can only attend on nearby neighbors in lower layers and gradually extend the attention field to further tokens when climbing to higher layers. Neural L-PCFG (Zhu et al., 2020) demonstrated that PCFG can benefit from modeling lexical dependencies. Similar to StructFormer, the Neural L-PCFG induces both constituents and dependencies within a single model.\nThough large scale pre-trained models have dominated most natural language processing tasks, some recent work indicates that neural network models can see accuracy gains by leveraging syntactic information rather than ignoring it (Marcheggiani and Titov, 2017; Strubell et al., 2018). Strubell et al. (2018) introduces syntacticallyinformed self-attention that force one attention head to attend on the syntactic governor of the input token. Omote et al. (2019) and Deguchi et al. (2019) argue that dependency-informed selfattention can improve Transformer’s performance on machine translation. Kuncoro et al. (2020) shows that syntactic biases help large scale pretrained models, like BERT, to achieve better language understanding."
    }, {
      "heading" : "3 Syntactic Distance and Height",
      "text" : "In this section, we first reintroduce the concepts of syntactic distance and height, then discuss their relations in the context of StructFormer."
    }, {
      "heading" : "3.1 Syntactic Distance",
      "text" : "Syntactic distance is proposed in Shen et al. (2018b) to quantify the process of splitting sentences into smaller constituents.\nDefinition 3.1. Let T be a constituency tree for sentence (w1, ..., wn). The height of the lowest common ancestor for consecutive words xi and xi+1 is τ̃i. Syntactic distances T = (τ1, ..., τn−1) are defined as a sequence of n− 1 real scalars that share the same rank as (τ̃1, ..., τ̃n−1).\nIn other words, each syntactic distance di is associated with a split point (i, i+ 1) and specify the relative order in which the sentence will be split into smaller components. Thus, any sequence of n − 1 real values can unambiguously map to an unlabeled binary constituency tree with n leaves through the Algorithm 1 (Shen et al., 2018b). As Shen et al. (2018c,a); Wang et al. (2019) pointed out, the syntactic distance reflects the information communication between constituents. More concretely, a large syntactic distance τi represents that short-term or local information should not be communicated between (x≤i) and (x>i). While cooperating with appropriate neural network architectures, we can leverage this feature to build unsupervised dependency parsing models.\nAlgorithm 1 Distance to binary constituency tree 1: function CONSTITUENT(w, d) 2: if d = [] then 3: T⇐ Leaf(w) 4: else 5: i⇐ arg maxi(d) 6: childl⇐ Constituent(w≤i, d<i) 7: childr ⇐ Constituent(w>i, d>i) 8: T⇐ Node(childl, childr) 9: return T\nAlgorithm 2 Converting binary constituency tree to dependency graph 1: function DEPENDENT(T, ∆) 2: if T = w then 3: D⇐ [],parent⇐ w 4: else 5: childl, childr ⇐ T 6: Dl,parentl ⇐ Dependent(childl,∆) 7: Dr, parentr ⇐ Dependent(childr,∆) 8: D⇐ Union(Dl,Dr) 9: if ∆(parentl) > ∆(parentr) then 10: D.add(parentl ← parentr) 11: parent⇐ parentl 12: else 13: D.add(parentr ← parentl) 14: parent⇐ parentr 15: return D, parent"
    }, {
      "heading" : "3.2 Syntactic Height",
      "text" : "Syntactic height is proposed in Luo et al. (2019), where it is used to capture the distance to the root node in a dependency graph. A word with high syntactic height means it is close to the root node. In this paper, to match the definition of syntactic distance, we redefine syntactic height as:\nDefinition 3.2. Let D be a dependency graph for sentence (w1, ..., wn). The height of a token wi in D is δ̃i. The syntactic heights of D can be any\nsequence of n real scalars ∆ = (δ1, ..., δn) that share the same rank as (δ̃1, ..., δ̃n).\nAlthough the syntactic height is defined based on the dependency structure, we cannot rebuild the original dependency structure by syntactic heights alone, since there is no information about whether a token should be attached to the left side or the right side. However, given an unlabelled constituent tree, we can convert it into a dependency graph with the help of syntactic distance. The converting process is similar to the standard process of converting constituency treebank to dependency treebank (Gelbukh et al., 2005). Instead of using the constituent labels and POS tags to identify the parent of each constituent, we simply assign the token with the largest syntactic height as the parent of each constituent. The conversion algorithm is described in Algorithm 2. In Appendix A.1, we also propose a joint algorithm, that takes T and ∆ as inputs and jointly outputs a constituency tree and dependency graph."
    }, {
      "heading" : "3.3 The relation between Syntactic Distance and Height",
      "text" : "As discussed previously, the syntactic distance controls information communication between the two sides of the split point. The syntactic height quantifies the centrality of each token in the dependency graph. A token with large syntactic height tends to have more long-term dependency relations to connect different parts of the sentence together. In StructFormer, we quantify the syntactic distance and height on the same scale. Given a split point (i, i+ 1) and it’s syntactic distance δi, only tokens\nxj with τj > δi can attend across the split point (i, i+ 1). Thus tokens with small syntactic height are limited to attend to nearby tokens. Figure 2 provides an example of T, ∆ and respective dependency graph D.\nHowever, if the left and right boundary syntactic distance of a constituent [l, r] are too large, all words in [l, r] will be forced to only attend to other words in [l, r]. Their contextual embedding will not be able to encode the full context. To avoid this phenomena, we propose calibrating T according to ∆ in Appendix A.2"
    }, {
      "heading" : "4 StructFormer",
      "text" : "In this section, we present the StructFormer model. Figure 3a shows the architecture of StructFormer, which includes a parser network and a Transformer module. The parser network predicts T and ∆, then passes them to a set of differentiable functions to generate dependency distributions. The Transformer module takes these distributions and the sentence as input to computes a contextual embedding for each position. The StructFormer can be trained in an end-to-end fashion on a Masked Language Model task. In this setting, the gradient back propagates through the relation distributions into the parser."
    }, {
      "heading" : "4.1 Parsing Network",
      "text" : "As shown in Figure 3b, the parsing network takes word embeddings as input and feeds them into several convolution layers:\nsl,i = tanh (Conv (sl−1,i−W , ..., sl−1,i+W )) (1)\nwhere sl,i is the output of l-th layer at i-th position, s0,i is the input embedding of tokenwi, and 2W+1 is the convolution kernel size.\nGiven the output of the convolution stack sN,i, we parameterize the syntactic distance T as:\nτi =  Wτ1 tanh ( Wτ2 [ sN,i sN,i+1 ]) ,\n1 ≤ i ≤ n− 1 ∞, i = 0 or i = n\n(2)\nwhere τi is the contextualized distance for the ith split point between token wi and wi+1. The syntactic height ∆ is parameterized in a similar way:\nδi = W δ 1 tanh ( Wδ2sN,i + b δ 2 ) + bδ1 (3)"
    }, {
      "heading" : "4.2 Estimate the Dependency Distribution",
      "text" : "Given T and ∆, we now explain how to estimate the probability p(xj |xi) such that the j-th token is the parent of the i-th token. The first step is identifying the smallest legal constituent C(xi), that contains xi and xi is not C(xi)’s parent. The second step is identifying the parent of the constituent xj = Pr(C(xi)). Given the discussion in section 3.2, the parent of C(xi) must be the parent of xi. Thus, the two-stages of identifying the parent of xi can be formulated as:\nD(xi) = Pr(C(xi)) (4)\nIn StructFormer, C(xi) is represented as constituent [l, r], where l is the starting index (l ≤ i) of C(xi) and r is the ending index (r ≥ i) of C(xi).\nIn a dependency graph, xi is only connected to its parent and dependents. This means that xi does not have direct connection to the outside of C(xi). In other words, C(xi) = [l, r] is the smallest constituent that satisfies:\nδi < τl−1, δi < τr (5)\nwhere τl−1 is the first τ<i that is larger then δi while looking backward, and τr is the first τ≥i that is larger then δi while looking forward. For example, in Figure 2, δ4 = 3.5, τ3 = 4 > δ4 and τ8 = ∞ > δ4, thus C(x4) = [4, 8]. To make this process differentiable, we define τk as a real value and δi as a probability distribution p(δ̃i). For the simplicity and efficiency of computation, we directly parameterize the cumulative distribution function p(δ̃i > τk) with sigmoid function:\np(δ̃i > τk) = σ((δi − τk)/µ1) (6)\nwhere σ is the sigmoid function, δi is the mean of distribution p(δ̃i) and µ1 is a learnable temperature\nterm. Thus the probability that the l-th (l < i) token is inside C(xi) is equal to the probability that δ̃i is larger then the maximum distance τ between l and i:\np(l ∈ C(xi)) = p(δ̃i > max(τi−1, ..., τl)) (7) = σ((δi −max(τl, ..., τi−1))/µ)\nThen we can compute the probability distribution for l:\np(l|i) = p(l ∈ C(xi))− p(l − 1 ∈ C(xi)) = σ((δi −max(τl, ..., τi−1))/µ)−\nσ((δi −max(τl−1, ..., τi−1))/µ) (8)\nSimilarly, we can compute the probability distribution for r:\np(r|i) = σ((δi −max(τi, ..., τr−1))/µ)− σ((δi −max(τi, ..., τr))/µ) (9)\nThe probability distribution for [l, r] = C(xi) can be computed as:\npC([l, r]|i) = { p(l|i)p(r|i), l ≤ i ≤ r\n0, otherwise (10)\nThe second step is to identify the parent of [l, r]. For any constituent [l, r], we choose the j = argmaxk∈[l,r](δk) as the parent of [l, r]. In the previous example, given constituent [4, 8], the maximum syntactic height is δ6 = 4.5, thus Pr([4, 8]) = x6. We use softmax function to parameterize the probability pPr(j|[l, r]):\npPr(j|[l, r]) =\n{ exp(hj/µ2)∑\nl≤k≤r exp(hk/µ2) , l ≤ t ≤ r\n0, otherwise (11)\nGiven probability p(j|[l, r]) and p([l, r]|i), we can compute the probability that xj is the parent of xi:\npD(j|i) = {∑\n[l,r] pPr(j|[l, r])pC([l, r]|i), i 6= j 0, i = j\n(12)"
    }, {
      "heading" : "4.3 Dependency-Constrained Multi-head Self-Attention",
      "text" : "The multi-head self-attention in the transformer can be seen as a information propagation mechanism on the complete graph G = (X,E), where the set of vertices X contains all n tokens in the sentence, and the set of edges E contains all possible word pairs (xi, xj). StructFormer replace the\ncomplete graph G with a soft dependency graph D = (X,A), where A is the matrix of n× n probabilities. Aij = pD(j|i) is the probability of the j-th token depending on the i-th token. The reason that we called it a directed edge is that each specific head is only allow to propagate information either from parent to dependent or from from dependent to parent. To do so, structformer associate each attention head with a probability distribution over parent or dependent relation.\npparent = exp(wparent)\nexp(wparent) + exp(wdep) (13)\npdep = exp(wdep)\nexp(wparent) + exp(wdep) (14)\nwhere wparent and wdep are learnable parameters that associated with each attention head, pparent is the probability that this head will propagate information from parent to dependent, vice versa. The model will learn to assign this association from the downstream task via gradient descent. Then we can compute the probability that information can be propagated from node j to node i via this head:\npi,j = pparentpD(j|i) + pdeppD(i|j) (15)\nHowever, Htut et al. (2019) pointed out that different heads tend to associate with different type of universal dependency relations (including nsubj, obj, advmod, etc), but there is no generalist head can that work with all different relations. To accommodate this observation, we compute a individual probability for each head and pair of tokens (xi, xj):\nqi,j = sigmoid ( QKT√ dk ) (16)\nwhere Q and K are query and key matrix in a standard transformer model and dk is the dimension of attention head. The equation is inspired by the scaled dot-product attention in transformer. We replace the original softmax function with a sigmoid function, so qi,j became an independent probability that indicates whether xi should attend on xj through the current attention head. In the end, we propose to replace transformer’s scaled dotproduct attention with our dependency-constrained self-attention:\nAttention(Qi,Kj , Vj ,D) = pi,jqi,jVj (17)"
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the proposed model on three tasks: Masked Language Modeling, Unsupervised Constituency Parsing and Unsupervised Dependency Parsing.\nOur implementation of StructFormer is close to the original Transformer encoder (Vaswani et al., 2017). Except that we put the layer normalization in front of each layer, similar to the T5 model (Raffel et al., 2019). We found that this modification allows the model to converges faster. For all experiments, we set the number of layers L = 8, the embedding size and hidden size to be dmodel = 512, the number of self-attention heads h = 8, the feedforward size dff = 2048, dropout rate as 0.1, and the number of convolution layers in the parsing network as Lp = 3."
    }, {
      "heading" : "5.1 Masked Language Model",
      "text" : "Masked Language Modeling (MLM) has been widely used as a pretraining object for larger-scale pretraining models. In BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019), authors found that MLM perplexities on held-out evaluation set have a positive correlation with the end-task performance. We trained and evaluated our model on 2 different datasets: the Penn TreeBank (PTB) and BLLIP. In our MLM experiments, each token has an independent chance to be replaced by a mask token <mask>, except that we never replace < unk > token. The training and evaluation object for Masked Language Model is to predict the replaced tokens. The performance of MLM is evaluated by measuring perplexity on masked words.\nPTB is a standard dataset for language modeling (Mikolov et al., 2012) and unsupervised constituency parsing (Shen et al., 2018c; Kim et al., 2019a). Following the setting proposed in Shen et al. (2018c), we use Mikolov et al. (2012)’s prepossessing process, which removes all punctuations, and replaces low frequency tokens with <unk>. The preprocessing results in a vocabulary size of 10001 (including <unk>, <pad> and <mask>). For PTB, we use a 30% mask rate.\nBLLIP is a large Penn Treebank-style parsed corpus of approximately 24 million sentences. We train and evaluate StructFormer on three splits of BLLIP: BLLIP-XS (40k sentences, 1M tokens), BLLIP-SM (200K sentences, 5M tokens), and BLLIP-MD (600K sentences, 14M tokens). They are obtained by randomly sampling sections from\nBLLIP 1987-89 Corpus Release 1. All models are tested on a shared held-out test set (20k sentences, 500k tokens). Following the settings provided in (Hu et al., 2020), we use subword-level vocabulary extracted from the GPT-2 pre-trained model rather than the BLLIP training corpora. For BLLIP, we use a 15% mask rate.\nThe masked language model results are shown in Table 1. StructFormer consistently outperforms our Transformer baseline. This result aligns with previous observations that linguistically informed selfattention can help Transformers achieve stronger performance. We also observe that StructFormer converges much faster than the standard Transformer model."
    }, {
      "heading" : "5.2 Unsupervised Constituency Parsing",
      "text" : "The unsupervised constituency parsing task compares the latent tree structure induced by the model with those annotated by human experts. We use the Algorithm 1 to predict the constituency trees from T predicted by StructFormer. Following the experiment settings proposed in Shen et al. (2018c), we take the model trained on PTB dataset and evaluate it on WSJ test set. The WSJ test set is section 23 of WSJ corpus, it contains 2416 human expert labeled sentences. Punctuation is ignored during the evaluation.\nTable 2 shows that our model achieves strong results on unsupervised constituency parsing. While\nthe C-PCFG (Kim et al., 2019a) achieve a stronger parsing performance with its strong linguistic constraints (e.g. a finite set of production rules), StructFormer may have a border domain of application. For example, it can replace the standard transformer encoder in most of the popular large-scale pre-trained language models (e.g. BERT and ReBERTa) and transformer based machine translation models. Different from the transformer-based TreeT (Wang et al., 2019), we did not directly use constituents to restrict the self-attention receptive field. But StructFormer achieves a stronger constituency parsing performance. This result may suggest that dependency relations are more suitable for grammar induction in transformer-based models. Table 3 shows that our model achieves strong accuracy while predicting Noun Phrase (NP), Preposition Phrase (PP), Adjective Phrase (ADJP), and Adverb Phrase (ADVP)."
    }, {
      "heading" : "5.3 Unsupervised Dependency Parsing",
      "text" : "The unsupervised dependency parsing evaluation compares the induced dependency relations with those in the reference dependency graph. The most common metric is the Unlabeled Attachment Score (UAS), which measures the percentage that a token is correctly attached to its parent in the reference tree. Another widely used metric for unsupervised dependency parsing is Undirected Unlabeled Attachment Score (UUAS) measures the percentage that the reference undirected and unlabeled connections are recovered by the induced tree. Similar to the unsupervised constituency parsing, we take the model trained on PTB dataset and evaluate it on WSJ test set (section 23). For the WSJ test set, reference dependency graphs are converted from its human-annotated constituency trees. However, there are two different sets of rules for the conversion: the Stanford dependencies and the CoNLL dependencies. While Stanford dependencies are used as reference dependencies in previous unsupervised\nparsing papers, we noticed that our model sometimes output dependency structures that are closer to the CoNLL dependencies. Therefore, we report UAS and UUAS for both Stanford and CoNLL dependencies. Following the setting of previous papers (Jiang et al., 2016), we ignored the punctuation during evaluation. To obtain the dependency relation from our model, we compute the argmax for dependency distribution:\nk = argmaxj 6=ipD(j|i) (18)\nand assign the k-th token as the parent of i-th token. Table 5 shows that our model achieves competitive dependency parsing performance while comparing to other models that do not require gold POS tags. While most of the baseline models still rely on some kind of latent POS tags or pre-trained word embeddings, StructFormer can be seen as an easy-to-use alternative that works in an end-to-end fashion. Table 6 shows that our model recovers 61.6% of undirected dependency relations. Given\nthe strong performances on both dependency parsing and masked language modeling, we believe that the dependency graph schema could be a viable substitute for the complete graph schema used in the standard transformer. Appendix A.4 provides examples of parent distribution.\nSince our model uses a mixture of the relation probability distribution for each self-attention head, we also studied how different combinations of relations affect the performance of our model. Table 6 shows that the model can achieve the best performance while using both parent and dependent relations. The model suffers more on dependency parsing if the parent relation is removed. And if the dependent relationship is removed, the model will suffer more on the constituency parsing. Appendix A.3 shows the weight for parent and dependent relations learnt from MLM tasks. It’s interesting to observe that Structformer tends to focus on the parent relations in the first layer, and start to use both relations from the second layer."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we introduce a novel dependency and constituency joint parsing framework. Based on the framework, we propose StructFormer, a new unsupervised parsing algorithm that does unsupervised dependency and constituency parsing at the same time. We also introduced a novel dependencyconstrained self-attention mechanism that allows each attention head to focus on a specific mixture of dependency relations. This brings Transformers closer to modeling a directed dependency graph. The experiments show promising results that StructFormer can induce meaningful dependency and constituency structures and achieve better performance on masked language model tasks. This research provides a new path to build more linguistic bias into a pre-trained language model."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Joint Dependency and Constituency Parsing\nAlgorithm 3 The joint dependency and constituency parsing algorithm. Inputs are a sequence of words w, syntactic distances d, syntactic heights h. Outputs are a binary constituency tree T, a dependency graph D that is represented as a set of dependency relations, the parent of dependency graph D, and the syntactic height of parent. 1: function BUILDTREE(w,d,h) 2: if d = [] and w = [w] and h = [h] then 3: T ⇐ Leaf(w), D ⇐ [], parent ⇐ w, height ⇐ h\n4: else 5: i⇐ arg max(d) 6: Tl,Dl, parentl, heightl ⇐\nBuildTree(d<i,w≤i,h≤i) 7: Tr,Dr, parentr,heightr ⇐\nBuildTree(d>i,w>i,h>i) 8: T⇐ Node(childl ⇐ Tl, childr ⇐ Tr) 9: D⇐ Union(Dl, Dr)\n10: if heightl > heightr then 11: D.add(parentl ← parentr) 12: parent⇐ parentl, height⇐ heightl 13: else 14: D.add(parentr ← parentl) 15: parent⇐ parentr , height⇐ heightr 16: return T, D, parent, height\nA.2 Calibrating the Syntactic Distance and Height\nIn Section 3.3, we explained the relation between ∆ and T, that if δi < τj , the i-th word won’t be able to attend beyond the j-th split point. However, in a specific case, the constraint will isolate a constituent [l, r] from the rest of the sentence. If τl−1 and τr are larger then all height δl,...,r in the constituent, then all words in [l, r] won’t be able to attend on the outside of the constituent. This phenomenon will prevent their output contextual embedding from encoding the full context. To avoid this phenomenon, we propose to calibrate the syntactic distance T according to the syntactic height ∆. First, we compute the maximum syntactic height for each constituent:\nδ[l,r] = max (δl, ..., δr) , l < r (19)\nThen we compute the minimum difference between δ[l,r] and [l, r]’s left and right boundary distance. Since we only care about constituents that the boundary distance is larger than its maximum\nheight, we use a ReLU activation function to keep only the positive values:\n[l,r] = ReLU ( min ( τl−1 − δ[l,r], τr − δ[l,r] )) (20) To make sure all constituent are not isolated and maintain the rank of T, we subtract all T by the maximum of :\nδ̂i = δi − max {[l,r]}/[1,n]\n( [l,r] ) (21)\nA.3 Dependency Relation Weights for Self-attention Heads\nA.4 Dependency Distribution Examples\nA.5 The Performance of StructFormer with different mask rates"
    } ],
    "references" : [ {
      "title" : "Crf autoencoder for unsupervised dependency parsing",
      "author" : [ "Jiong Cai", "Yong Jiang", "Kewei Tu." ],
      "venue" : "arXiv preprint arXiv:1708.01018.",
      "citeRegEx" : "Cai et al\\.,? 2017",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised structure prediction with non-parallel multilingual guidance",
      "author" : [ "Shay B Cohen", "Dipanjan Das", "Noah A Smith." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61.",
      "citeRegEx" : "Cohen et al\\.,? 2011",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2011
    }, {
      "title" : "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction",
      "author" : [ "Shay B Cohen", "Noah A Smith." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the As-",
      "citeRegEx" : "Cohen and Smith.,? 2009",
      "shortCiteRegEx" : "Cohen and Smith.",
      "year" : 2009
    }, {
      "title" : "Unsupervised search-based structured prediction",
      "author" : [ "Hal Daumé III." ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, pages 209–216.",
      "citeRegEx" : "III.,? 2009",
      "shortCiteRegEx" : "III.",
      "year" : 2009
    }, {
      "title" : "Dependency-based self-attention for transformer nmt",
      "author" : [ "Hiroyuki Deguchi", "Akihiro Tamura", "Takashi Ninomiya." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 239–",
      "citeRegEx" : "Deguchi et al\\.,? 2019",
      "shortCiteRegEx" : "Deguchi et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised latent tree induction with deep inside-outside recursive auto-encoders",
      "author" : [ "Andrew Drozdov", "Patrick Verga", "Mohit Yadav", "Mohit Iyyer", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Drozdov et al\\.,? 2019",
      "shortCiteRegEx" : "Drozdov et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith." ],
      "venue" : "Proceedings of NAACL-HLT, pages 199–209.",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Transforming a constituency treebank into a dependency treebank",
      "author" : [ "Alexander Gelbukh", "Sulema Torres", "Hiram Calvo." ],
      "venue" : "Procesamiento del lenguaje natural, (35):145–152.",
      "citeRegEx" : "Gelbukh et al\\.,? 2005",
      "shortCiteRegEx" : "Gelbukh et al\\.",
      "year" : 2005
    }, {
      "title" : "Sparsity in dependency grammar induction",
      "author" : [ "Jennifer Gillenwater", "Kuzman Ganchev", "João Graça", "Fernando Pereira", "Ben Taskar." ],
      "venue" : "ACL 2010, page 194.",
      "citeRegEx" : "Gillenwater et al\\.,? 2010",
      "shortCiteRegEx" : "Gillenwater et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependency grammar induction with neural lexicalization and big training data",
      "author" : [ "Wenjuan Han", "Yong Jiang", "Kewei Tu." ],
      "venue" : "arXiv preprint arXiv:1708.00801.",
      "citeRegEx" : "Han et al\\.,? 2017",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised learning of syntactic structure with invertible neural projections",
      "author" : [ "Junxian He", "Graham Neubig", "Taylor BergKirkpatrick." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving unsupervised dependency parsing with richer contexts and smoothing",
      "author" : [ "William P Headden III", "Mark Johnson", "David McClosky." ],
      "venue" : "Proceedings of human language technologies: the 2009 annual conference of the North American chapter of",
      "citeRegEx" : "III et al\\.,? 2009",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2009
    }, {
      "title" : "Do attention heads in bert track syntactic dependencies? arXiv preprint arXiv:1911.12246",
      "author" : [ "Phu Mon Htut", "Jason Phang", "Shikha Bordia", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Htut et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2019
    }, {
      "title" : "A systematic assessment of syntactic generalization in neural language models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger P Levy." ],
      "venue" : "arXiv preprint arXiv:2005.03692.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised neural dependency parsing. Association for Computational Linguistics (ACL)",
      "author" : [ "Yong Jiang", "Wenjuan Han", "Kewei Tu" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2016
    }, {
      "title" : "Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction",
      "author" : [ "Taeuk Kim", "Jihun Choi", "Daniel Edmiston", "Sanggoo Lee." ],
      "venue" : "arXiv preprint arXiv:2002.00737.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Compound probabilistic context-free grammars for grammar induction",
      "author" : [ "Yoon Kim", "Chris Dyer", "Alexander M Rush." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385.",
      "citeRegEx" : "Kim et al\\.,? 2019a",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised recurrent neural network grammars",
      "author" : [ "Yoon Kim", "Alexander M Rush", "Lei Yu", "Adhiguna Kuncoro", "Chris Dyer", "Gábor Melis." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Kim et al\\.,? 2019b",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "The unsupervised learning of natural language structure",
      "author" : [ "Dan Klein." ],
      "venue" : "Stanford University Stanford.",
      "citeRegEx" : "Klein.,? 2005",
      "shortCiteRegEx" : "Klein.",
      "year" : 2005
    }, {
      "title" : "A generative constituent-context model for improved grammar induction",
      "author" : [ "Dan Klein", "Christopher D Manning." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135.",
      "citeRegEx" : "Klein and Manning.,? 2002",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2002
    }, {
      "title" : "Corpusbased induction of syntactic structure: Models of dependency and constituency",
      "author" : [ "Dan Klein", "Christopher D Manning." ],
      "venue" : "Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04), pages 478–485.",
      "citeRegEx" : "Klein and Manning.,? 2004",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2004
    }, {
      "title" : "Syntactic structure distillation pretraining for bidirectional encoders",
      "author" : [ "Adhiguna Kuncoro", "Lingpeng Kong", "Daniel Fried", "Dani Yogatama", "Laura Rimell", "Chris Dyer", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:2005.13482.",
      "citeRegEx" : "Kuncoro et al\\.,? 2020",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised dependency parsing: Let’s use supervised parsers",
      "author" : [ "Phong Le", "Willem Zuidema." ],
      "venue" : "arXiv preprint arXiv:1504.04666.",
      "citeRegEx" : "Le and Zuidema.,? 2015",
      "shortCiteRegEx" : "Le and Zuidema.",
      "year" : 2015
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural language models by segmenting, attending, and predicting the future",
      "author" : [ "Hongyin Luo", "Lan Jiang", "Yonatan Belinkov", "James Glass." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1483–",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoding sentences with graph convolutional networks for semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:1703.04826.",
      "citeRegEx" : "Marcheggiani and Titov.,? 2017",
      "shortCiteRegEx" : "Marcheggiani and Titov.",
      "year" : 2017
    }, {
      "title" : "Statistical language models based on neural networks",
      "author" : [ "Tomáš Mikolov" ],
      "venue" : "Presentation at Google, Mountain View, 2nd April, 80:26.",
      "citeRegEx" : "Mikolov,? 2012",
      "shortCiteRegEx" : "Mikolov",
      "year" : 2012
    }, {
      "title" : "Dependency-based relative positional encoding for transformer nmt",
      "author" : [ "Yutaro Omote", "Akihiro Tamura", "Takashi Ninomiya." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 854–",
      "citeRegEx" : "Omote et al\\.,? 2019",
      "shortCiteRegEx" : "Omote et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised dependency parsing with acoustic cues",
      "author" : [ "John K Pate", "Sharon Goldwater." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:63–74.",
      "citeRegEx" : "Pate and Goldwater.,? 2013",
      "shortCiteRegEx" : "Pate and Goldwater.",
      "year" : 2013
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural language modeling by jointly learning syntax and lexicon",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Chin-wei Huang", "Aaron Courville." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2018a",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Straight to the tree: Constituency parsing with neural syntactic distance",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Athul Paul Jacob", "Alessandro Sordoni", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Shen et al\\.,? 2018b",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Ordered neurons: Integrating tree structures into recurrent neural networks",
      "author" : [ "Yikang Shen", "Shawn Tan", "Alessandro Sordoni", "Aaron Courville." ],
      "venue" : "In",
      "citeRegEx" : "Shen et al\\.,? 2018c",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised dependency parsing without gold part-of-speech tags",
      "author" : [ "Valentin I Spitkovsky", "Hiyan Alshawi", "Angel Chang", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281–",
      "citeRegEx" : "Spitkovsky et al\\.,? 2011",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2011
    }, {
      "title" : "Breaking out of local optima with count transforms and model recombination: A study in grammar induction",
      "author" : [ "Valentin I Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi" ],
      "venue" : null,
      "citeRegEx" : "Spitkovsky et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistically-informed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1804.08199.",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Unambiguity regularization for unsupervised learning of probabilistic grammars",
      "author" : [ "Kewei Tu", "Vasant Honavar." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Tu and Honavar.,? 2012",
      "shortCiteRegEx" : "Tu and Honavar.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Tree transformer: Integrating tree structures into self-attention",
      "author" : [ "Yaushian Wang", "Hung-Yi Lee", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "The return of lexical dependencies: Neural lexicalized pcfgs",
      "author" : [ "Hao Zhu", "Yonatan Bisk", "Graham Neubig." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:647–661.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 222
    }, {
      "referenceID" : 19,
      "context" : "There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 222
    }, {
      "referenceID" : 23,
      "context" : "There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 222
    }, {
      "referenceID" : 33,
      "context" : "There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 222
    }, {
      "referenceID" : 17,
      "context" : "There has been an exciting breath of recent work targeted at learning this structure in a data-driven unsupervised fashion (Klein and Manning, 2002; Klein, 2005; Le and Zuidema, 2015; Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 123,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "Most existing approaches induce dependency structures from other syntactic information like gold POS tags (Klein and Manning, 2004; Cohen and Smith, 2009; Jiang et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "Most existing approaches induce dependency structures from other syntactic information like gold POS tags (Klein and Manning, 2004; Cohen and Smith, 2009; Jiang et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 174
    }, {
      "referenceID" : 15,
      "context" : "Most existing approaches induce dependency structures from other syntactic information like gold POS tags (Klein and Manning, 2004; Cohen and Smith, 2009; Jiang et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 174
    }, {
      "referenceID" : 34,
      "context" : "works, having trained from words alone, often requires additional information, like pre-trained word clustering (Spitkovsky et al., 2011), pre-trained word embedding (He et al.",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : ", 2011), pre-trained word embedding (He et al., 2018), acoustic cues (Pate and Goldwater, 2013), or annotated data from",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 29,
      "context" : ", 2018), acoustic cues (Pate and Goldwater, 2013), or annotated data from",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 32,
      "context" : "of Syntactic Distances T (Shen et al., 2018b) and a sequence of Syntactic Heights ∆ (Luo et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : ", 2018b) and a sequence of Syntactic Heights ∆ (Luo et al., 2019) to represent dependency graphs and constituency trees at the same time.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "ing are primarily based on the dependency model with valence (DMV) (Klein and Manning, 2004) and its extension (Daumé III, 2009; Gillenwater et al.",
      "startOffset" : 67,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "ing are primarily based on the dependency model with valence (DMV) (Klein and Manning, 2004) and its extension (Daumé III, 2009; Gillenwater et al., 2010).",
      "startOffset" : 111,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "The most recent progress is the neural DMV model (Jiang et al., 2016), which uses a neural network model to predict the grammar rule probabilities based on the distributed representation of POS tags.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : ", 2018a) and ON-LSTM (Shen et al., 2018c) induce tree structure by introducing an inductive bias to recurrent neural networks.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "In URNNG (Kim et al., 2019b), amortized variational inference was applied between a recurrent neural network grammar (RNNG) (Dyer et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : ", 2019b), amortized variational inference was applied between a recurrent neural network grammar (RNNG) (Dyer et al., 2016) decoder and a tree structure inference network, which encourages the decoder to generate reasonable tree structures.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "DIORA (Drozdov et al., 2019) proposed using inside-outside dynamic programming to compose latent representations from all possible binary",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "The compound PCFG (Kim et al., 2019a) achieves grammar induction by maximizing the marginal likelihood of the sentences which are generated by a probabilistic context-free grammar (PCFG).",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 39,
      "context" : "Tree Transformer (Wang et al., 2019) adds extra locality constraints to the Transformer encoder’s self-attention to encourage the attention heads to follow a tree structure such that each token can only attend on nearby neighbors in lower layers and gradually extend the attention field to further tokens when climbing to higher layers.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 40,
      "context" : "Neural L-PCFG (Zhu et al., 2020) demonstrated that PCFG can benefit from modeling lexical dependencies.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 32,
      "context" : "Thus, any sequence of n − 1 real values can unambiguously map to an unlabeled binary constituency tree with n leaves through the Algorithm 1 (Shen et al., 2018b).",
      "startOffset" : 141,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : "The converting process is similar to the standard process of converting constituency treebank to dependency treebank (Gelbukh et al., 2005).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 38,
      "context" : "Our implementation of StructFormer is close to the original Transformer encoder (Vaswani et al., 2017).",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "Except that we put the layer normalization in front of each layer, similar to the T5 model (Raffel et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : ", 2018) and RoBERTa (Liu et al., 2019), authors found that MLM perplexities on held-out evaluation set have a positive correlation with the end-task performance.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : ", 2012) and unsupervised constituency parsing (Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 46,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : ", 2012) and unsupervised constituency parsing (Shen et al., 2018c; Kim et al., 2019a).",
      "startOffset" : 46,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "Following the settings provided in (Hu et al., 2020), we use subword-level vocabulary extracted from the GPT-2 pre-trained model rather than the BLLIP training corpora.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "the C-PCFG (Kim et al., 2019a) achieve a stronger parsing performance with its strong linguistic constraints (e.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 39,
      "context" : "Different from the transformer-based TreeT (Wang et al., 2019), we did not directly use constituents to restrict the self-attention receptive field.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "w/ gold POS tags (for reference only) DMV (Klein and Manning, 2004) 39.",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Following the setting of previous papers (Jiang et al., 2016), we ignored the punctuation during evaluation.",
      "startOffset" : 41,
      "endOffset" : 61
    } ],
    "year" : 2021,
    "abstractText" : "There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can simultaneously induce dependency and constituency structure. To achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph. Then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time.",
    "creator" : "LaTeX with hyperref"
  }
}