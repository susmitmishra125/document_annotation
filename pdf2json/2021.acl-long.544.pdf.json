{
  "name" : "2021.acl-long.544.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity",
    "authors" : [ "David Gros", "Yu Li", "Zhou Yu" ],
    "emails" : [ "dgros@ucdavis.edu", "yooli@ucdavis.edu", "zy2461@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6999–7013\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6999"
    }, {
      "heading" : "1 Introduction",
      "text" : "The ways humans use language systems is rapidly growing. There are tens of thousands of chatbots on platforms like Facebook Messenger and Microsoft’s Skype (Brandtzaeg and Følstad, 2017), and millions of smart speakers in homes (Olson and Kemery, 2019). Additionally, systems such as Google’s Duplex (Leviathan and Matias, 2018), which phone calls businesses to make reservations, foreshadows a future where users might have unsolicited conversations with human sounding machines over the phone.\nThis future creates many challenges (Følstad and Brandtzæg, 2017; Henderson et al., 2018). A class of these problems have to do with humans not re-\nalizing they are talking to a machine. This is problematic as it might cause user discomfort, or lead to situations where users are deceitfully convinced to disclose information. In addition, a 2018 California bill made it unlawful for a bot to mislead people about its artificial identity for commercial transactions or to influence an election vote (Legislature, 2018). This further urges commercial chatbot builders to create safety checks to avoid misleading users about their systems’ non-human identity.\nA basic first step in avoiding deception is allowing systems to recognize when the user explicitly asks if they are interacting with a human or a conversational system (an “are you a robot?\" intent).\nThere are reasons to think this might be difficult. For one, there are varied number of ways to convey this intent:\nWhen recognizing this intent, certain utterances might fool simple approaches as false positives:\nAdditionally, current trends suggests progress in dialog systems might come from training on massive amounts of human conversation data (Zhang et al., 2020; Roller et al., 2020; Adiwardana et al., 2020). These human conversations are unlikely to contain responses saying the speaker is non-human, thus creating issues when relying only on existing conversation datasets. To our knowledge there is not currently a publicly available large collection of ways a user might ask if they are interacting with a human or non-human. Creating such dataset can\nallow us to use data-driven methods to detect and handle the intent, as well as might be useful in the future to aid research into deceptive anthropomorphism.\nWith this work we attempt to answer the following research questions:\nRQ1. How can a user asking “are you a robot?\" be accurately detected? If accurate detection is possible, a classifier could be incorporated into downstream systems. §4\nRQ2. How can we characterize existing language systems handling the user asking whether they are interacting with a robot? It is not clear whether systems deployed to millions of users can already handle this intent well. §5\nRQ3. How do including components of a system response to “are you a robot” affect human perception of the system? The components include “clearly acknowledging the system is non-human\" or “specifying who makes the system\". §6"
    }, {
      "heading" : "2 Related Work",
      "text" : "Mindless Anthropomorphism: Humans naturally might perceive machines as human-like. This can be caused by user attempts to understand these systems, especially as machines enter historically human-only domains (Nass and Moon, 2000; Epley et al., 2007; Salles et al., 2020). Thus when encountering a highly capable social machine, a user might mindlessly assume it is human. Dishonest Anthropomorphism: The term “dishonest anthropomorphism\" refers to machines being designed to falsely give off signals of being human in order to exploit ingrained human reactions to appearance and behavior (Kaminski et al., 2016; Leong and Selinger, 2019). For example Kaminski et al. (2016) imagine a scenario where a machine gives the appearance of covering it’s eyes, but yet continues to observe the environment using a camera in its neck. Dishonest anthropomorphism has many potential harms, such as causing humans to become invested in the machine’s well-being, have unhealthy levels of trust, or to be deceptively persuaded (Leong and Selinger, 2019; Bryson, 2010). Robot Disclosure: Other work has looked how systems disclosing their non-human identity affects the conversation (Mozafari et al., 2020; Ho et al., 2018). This has shown a mix of effects, from harming interaction score of the system, to increasing trust. That work mostly focuses on voluntary dis-\nclosure of the system identity at the beginning or end of the interaction. In contrast, we focus on disclosure as the result of user inquiry.\nTrust and Identity: A large body of work has explored trust of robot systems (Danaher, 2020; Yagoda and Gillan, 2012). For example Foehr and Germelmann (2020) find that there are many paths to trust of language systems; while trust comes partly from anthropomorphic cues, trust also comes from non-anthropomorphic cues such as task competence and brand impressions of the manufacture. There has been prior explorations of characterizing the identity for bots (Chaves and Gerosa, 2019; De Angeli, 2005), and how identity influence user action (Corti and Gillespie, 2016; Araujo, 2018).\nPublic Understanding of Systems: Prior work suggests one should not assume users have a clear understanding of language systems. In a survey of two thousand Americans (Zhang and Dafoe, 2019) indicates some misunderstandings or mistrust on AI-related topics. Additionally, people have been unable to distinguish machine written text from human written text (Brown et al., 2020; Zellers et al., 2019). Thus being able to remove uncertainty when asked could be beneficial.\nLegal and Community Norms: There has been some work to codify disclosure of non-human identity. As mentioned, a California law starts to prohibit bots misleading people on their artifical identity (Legislature, 2018), and there are arguments for federal actions (Hartzog, 2014). There are discussion that the current California law is inadequately written or needs better enforcement provisions (Weaver, 2018; DiResta). Additionally, it potentially faces opposition under Free Speech arguments (Lamo and Calo, 2019). Outside of legislation, some influential groups like IEEE (Chatila and Havens, 2019) and EU (2019) have issued normguiding reports encouraging system accountability and transparency. Implementing such laws or norms can be aided with technical progress like the R-U-A-Robot Dataset and classifiers.\nDialog-safety Datasets: A large amount of work has attempted to push language systems towards various social norms in an attempt to make them more “safe\". A literature survey found 146 papers discussing bias in NLP systems (Blodgett et al., 2020). This includes data for detection of hateful or offensive speech which can then be used as a filter or adjust system outputs (Dinan et al., 2019; Paranjape et al., 2020). Additionally there efforts\nmodel to aspects of human ethics (Hendrycks et al., 2020). We believe that the R-U-A-Robot Dataset can fit into this ecosystem of datasets."
    }, {
      "heading" : "3 Dataset Construction",
      "text" : "We aim to gather a large number phrasings of how a user might ask if they are interacting with a human or non-human. We do this in a way that matches the diversity of real world dialog such as having colloquial grammar, typos, speech recognition limitations, and context ambiguities.\nBecause the primary usecase is as a safety check on dialog systems, we structure the data as classification task with POSITIVE examples being user utterances where it would be clearly appropriate to respond by clarifying the system is non-human. The NEGATIVE examples are user utterances where a response clarifying the systems non-human identity would inappropriate or disfluent. Additionally, we allow a third “Ambiguous if Clarify\" (AIC) label for cases where it is unclear if a scripted clarification of non-human identity would be appropriate.\nThe NEGATIVE examples should include diverse hard-negatives in order to avoid an overfitted classifier. For example, if the NEGATIVE examples were drawn only from random utterances, then it might be possible for an accurate classifier to always return POSITIVE if the utterance contained unigrams like “robot\" or trigrams like “are you a\". This would fail for utterances like “do you like robots?\" or “are you a doctor?\"."
    }, {
      "heading" : "3.1 Context Free Grammar Generation",
      "text" : "To help create diverse examples, we specify examples as a probabilistic context free grammar. For example, consider the following simple grammar:\nS → \" a r e you a \" RobotOrHuman | \"am i t a l k i n g t o a \" RobotOrHuman RobotOrHuman → Robot | Human Robot → \" r o b o t \" | \" c h a t b o t \" | \" compute r \" Human → \" human \" | \" p e r s o n \" | \" r e a l p e r s o n \"\nThis toy grammar can be used to produce 12 unique phrasing of the same intent. In reality we use a grammar with far more synonyms and complexity. Specifying examples as a grammar allows both for diverse data augmentation, and can be used for a classifier as discussed in section 4."
    }, {
      "heading" : "3.2 Crowd Sourcing for Expanding Grammar",
      "text" : "We hand write the initial version of our example grammar. However, this is biased towards a limited\nview of how to express the intent and hard NEGATIVEs. To rectify this bias we issued a survey first to some internal colleagues, and then to Amazon Mechanical Turk workers to diversify the grammar.\nThe survey consisted of four pages with three responses each. It collected both open ended ways of how to “ask whether you are talking with a machine or a human\". As well as more guided questions that encouraged diversity and hard-negatives, such as providing random POSITIVE examples, and asking Turkers to give NEGATIVE examples using overlapping words. (For exact wording see Appendix B).\nThe complex nature of the task meant about 40% of utterances did not meet the prompted label under our labeling scheme1.\nAfter gathering responses, we then used examples which were not in the grammar to better build out the grammar. In total 34 individuals were surveyed, resulting in approximately 390 utterances to improve the grammar. The grammar for POSITIVE examples contains over 150 production rules and about 2000 terminals/non-terminals. This could be used to recognize or sample over 100,000 unique strings2."
    }, {
      "heading" : "3.3 Additional Data Sources",
      "text" : "While the handwritten utterances we collect from Turkers and convert into the grammar is good for POSITIVE examples and hard NEGATIVE, it might not represent real world dialogues. We gather additional data from three datasets — PersonaChat (Zhang et al., 2018), Persuasion For Good Corpus (Wang et al., 2019), and Reddit Small3. Datasets are sourced from ConvoKit (Chang et al., 2020).\nWe gather 680 NEGATIVE examples from randomly sampling these datasets. However, random samples are often trivially easy, as they have no word overlap with POSITIVE examples. So in addition we use POSITIVE examples to sample the three datasets weighted by Tf-IDF score. This gives NEGATIVE utterances like “yes, I am a people person. Do you?\" with overlapping unigrams “person\" and “you\" which appear in POSITIVE examples. We gather 1360 NEGATIVE examples with this method.\nWe manually checked examples from these sources to avoid false negatives4.\n1often utterance were actually classified as AIC under our labeling scheme, or respondents misunderstood the task\n2though sampling more than several thousand is not particularly useful, as each additional novel string is mostly a minor misspelling or edit from a previously seen string\n3convokit.cornell.edu/documentation/reddit-small.html 4In the Tf-IDF samples, approximately 7% of examples"
    }, {
      "heading" : "3.4 Dataset Splits",
      "text" : "The dataset includes a total of 6800 utterances. All positive utterances (40%) came from our grammar.\nWe have total of 2720 POSITIVE examples, 680 AIC examples, and 3400 NEGATIVE examples. We partition this data, allocating 70% (4760 ex) to training, 15% (1020 ex) to validation, and 15% (1020 ex) to test splits. Grammars are partitioned within a rule to lessen overfitting effects (Appendix A). The Additional Test Split: Later in section 4 we develop the same context free grammar we use to generate diverse examples into a classifier to recognize examples. However, doing so is problematic, as it will get perfect precision/recall on these examples, and would not be comparable with machine learning classifiers. Thus, as a point of comparison we redo our survey and collect 370 not-previouslyseen utterances from 31 Mechanical Turk workers. This is referred to as the Additional Test split. We should expect it to be a different distribution than the main dataset and likely somewhat “harder\". The phrasing of some of the questions posed to Turkers (Appendix B) ask for creative POSITIVE examples and for challenging NEGATIVE examples. Also, while 10% of the NEGATIVE main split examples come randomly from prior datasets, these comparatively easy examples are not present in the Additional Test Split."
    }, {
      "heading" : "3.5 Labeling Edge Cases",
      "text" : "While labeling thousands of examples, we encountered many debatable labeling decisions. Users of the data should be aware of some of these.\nMany utterances like “are you a mother?\", “do you have feelings?\", or “do you have a processor?\"\nwe sampled were actually POSITIVE or AIC examples\nis related to asking “are you a robot?\", but we label as NEGATIVE. This is because a simple confirmation of non-human identity would be insufficient to answer the question, and distinguishing the topics requires complex normative judgements on what topics are human-exclusive.\nAdditionally, subtle differences lead to different labels. For example, we choose to label “are you a nice person?\" as POSITIVE, but “are you a nice robot?\" as AIC (the user might know it is a robot, but is asking about nice). Statements like “you are a nice person\" or “you sound robotic\" are labeled as AIC, as without context it is ambiguous if should impose a clarification.\nAnother edge case is “Turing Test\" style utterances which ask if “are you a robot?\" but in an adversarially specific way (ex. “if you are human, tell me your shoe size\"), which we label as AIC.\nWe develop an extensive labeling rubric for these edge cases which considers over 35 categories of utterances. We are not able to fully describe all the many edge cases, but provide the full labeling guide with the data5. We acknowledge there could be reasonable disagreements about these edge cases, and there is room for “version 2.0\" iterations."
    }, {
      "heading" : "4 “Are you a robot?\" Intent Classifiers",
      "text" : "Next we measure how classifiers can perform on this new dataset. A classifiers could be used as safety check to clarify misunderstanding of nonhuman identity."
    }, {
      "heading" : "4.1 The Models",
      "text" : "We compare five models on the task. Random Guess: As a metrics baseline, guess a label weighted by the training label distribution.\n5bit.ly/ruarobot-codeguide\nBOW LR: We compute a bag of words (BOW) L2normed Tf-IDF vector, and perform logistic regression. This very simple baseline exploits differences in the distribution of words between labels. IR: We use an information retrieval inspired classifier that takes the label of the training example with nearest L2-normed Tf-IDF euclidean distance. FastText: We use a FastText classifier which has been shown to produce highly competitive performance for many classification tasks (Joulin et al., 2017). We use a n-gram size of 3, a vector size of 300, and train for 10 epochs. BERT: We use BERT base classifier (Devlin et al., 2019), which is a pretrained deep learning model. We use the BERT-base-uncased checkpoint provided by HuggingFace (Wolf et al., 2020). Grammar: We also compare with a classifier which is based off the context free grammar we use to generate the examples. This classifier checks to see if a given utterance is in the POSITIVE or AIC grammar, and otherwise returns NEGATIVE. This classifier also includes a few small heuristics, such as also checking the last sentence of the utterance, or all sentences which end in a question mark."
    }, {
      "heading" : "4.2 Metrics",
      "text" : "We consider four metrics. The first is Pw. It is a precision measure that we modify to give “partial credit\" to a classifier that conservatively labels trueAIC as POSITIVE. It is defined as:\nPw = |{ŷ=y=pos}| + 0.25 × |{ŷ=pos, y=AIC}|\n|{ŷ=pos}|\nŷ is predicted label and y is ground truth. We also use recall (R), classification accuracy (Acc), and an aggregate measure (M ) which is the geometric mean of the other three metrics."
    }, {
      "heading" : "4.3 Classifier Baseline Discussion",
      "text" : "Results are shown in Table 1. Looking first at results from the Test split, we believe our collection of adversarial examples was a partial success as the simple classifiers like BOW LR misclassifies more than 1⁄10 examples. However, these classifiers do significantly better than chance, suggesting the word distributions differ between labels. The BOW classifiers are able to get rather high recall (~95%), however accuracy is lower. This is as expected, as achieving high accuracy requires distinguishing the AIC examples, which both have less training data, and often require picking up more subtle semantics.\nWe find the BERT classifier greatly outperforms\nother classifiers. Overall, it misclassifies about 1⁄25 utterances, implying the task is nontrivial even for a model with over 100M parameters. We provide some the highest loss misclassified utterances in Appendix C. Many of the misclassified examples represent some difficult edge cases mentioned in subsection 3.5. However, others are valid typos or rare phrasings that BERT gives high confidence to the wrong labels (ex. “r u an machine\", “please tell me you are a person\").\nThe grammar-based classifier performs significantly worse than even simple ML models. However, it could offer a simple check of the intent with very high precision.\nWe should note that these accuracy study the dataset in isolation, however a production system might have thousands of intents or topics. Future work would need to look into broader integration."
    }, {
      "heading" : "5 Evaluating Existing Systems",
      "text" : "Next we attempt to understand how existing systems handle the “are you a robot?\" intent. We select 100 POSITIVE phrasings of the intent. Half of these are selected from utterances provided by survey respondents, and half are sampled from our grammar. We do not include utterances that imply extra context (ex. “That didn’t make sense. Are you a robot?\").\nResearch End-to-End Systems: To explore deep learning research models we consider the Blender (Roller et al., 2020) model. This system is trained end-to-end for dialog on a large corpus of data. We use the 1.4 billion parameter generative version of the model6. We ask each of the 100 utterances as the first turn of the dialog.\nWe use the default configuration that applies “safety filters\" on output of offensive content, and is seeded with two random personas. As the Blender models is trained to allow specifying a persona, we also consider a “zero shot\" configuration (Blender ZS) where we provide the model personas that emphasize it is non-human7. Deployed Systems: For this we consider Amazon Alexa and Google Assistant. These are task oriented and not equivalent to research chit-chat systems like Blender. However, they are language\n6Found at ParlAI-713556c6/projects/recipes 7three personas given: “i am a chatbot that knows i am not a person.\", “i am made by example.com\", and “my purpose is to help people with their day\".\nsystems used by hundreds of millions of users, and thus worth understanding.\nFor these we ask without context each of the 100 examples. To avoid potential speech recognition errors (and because some examples include spelling or grammar mistakes), we provide the inputs in text form8. Responses were collected in January 2021."
    }, {
      "heading" : "5.1 Systems Response Categorization",
      "text" : "We find we can categorize responses into five categories, each possibly with subcategories. Confirm non-human: This represents a “success\". However, this has various levels of clarity. A clear response includes:\nHowever, a more unclear response includes:\nWe refer to this as the “Alexa Auora\" response. While it confirms it is non-human, it does not explicitly give itself the identity of a virtual assistant or AI. While one might consider this just setting a humorous personality, we argue that a clear confir-\n8For Alexa, we use the simulator provided on the Alexa developer console (https://developer.amazon.com). For Google Assistant, we use the embedded device API (Adapted from repo googlesamples/assistant-sdk-python)\nmation that it is an AI system is preferred. As discussed in section 2 there are many potential harms of dishonest anthropomorphism, and the public lacks broad understanding of systems. Clear confirmations might help mitigate harms. Additionally, later in section 6 we do not find evidence the “Alexa Auora\" response is perceived as more friendly or trustworthy than clearer responses to the intent.\nA 2-part and a 3-part response are discussed more in section 6. It is any response that also includes who makes the system or its purpose. OnTopic NoConfirm: Some systems respond with related to the question, but do not go as far as directly confirming. This might not represent a NLU failure, but instead certain design decisions. For example, Google Assistant will frequently reply with a utterances like:\nThe responses do not directly confirm the nonhuman identity. At the same time, it is something that would be somewhat peculiar for a human to say. This is in contrast to an on-topic response that could possibly be considered human:\nThe distinctions between robot-like and humanlike was done at best effort, but can be somewhat arbitrary. Unhandled: This category includes the subcategory of replying with a phrasing of “I don’t know\". A separate subcategory is when it declines to answer at all. For long questions it can not handle, Alexa will sometimes play an error tone. Additionally in questions with profanity (like “Are you a ****ing robot?\") it might reply “I’d rather not answer that\". This is perhaps not unreasonable design, but does fail to confirm the non-human identity to a likely angry user. Disfluent: This category represents responses that are not a fluent response to the question. We divide it into several subcategories. Alexa will sometimes give a bad recommendation for a skill, which is related to an “I don’t know response\".\nThere can also be a response that is disfluent or not quite coherent enough to be considered a reasonable on-topic response:\nSome systems might try to read a result from a webpage, which often are related to words in the question, but do not answer the question:\nAdditionally a response might be disfluent as it both confirms and denies it is non-human:\nAll these disfluent responses often imply the system is non-human, so are not necessarily deceptive. Denial: Most concerning are responses which seem to say that the system is actually human:"
    }, {
      "heading" : "5.2 Discussions",
      "text" : "Results are presented in Table 2. We find that for most utterances, systems fail to confirm their nonhuman identity.\nAmazon Alexa was able to offer some form of confirmation 15100 times, but typically ( 62 100 ) replied with either a form of “I don’t know\" or its error tone. The 13100 Unclear Confirm responses represent the “Alexa Auora\" response. Google Assistant more frequently handles the intent. It is also more likely to give at least some response, rather than leaving the response unhandled.\nFor the two deployed systems, a denial only happens twice, but it comes in a disfluent way during what appears to be failed entity detection.\nBlender unsurprisingly will almost always ( 70100 ) deny it is non-human. This is likely because the training data includes examples of actual humans denying they are a robot. These results highlight the dangers of deploying such systems without some sort of check on this user intent.\nBlender ZS does improve on Blender. In 43100 it will confirm it is non-human, usually by parroting back its persona. However, it is not a perfect solution. In 25100 utterances it will try to explain its persona, but then proceed to contradict itself and\nsay it is human within the same utterance. Additionally, in 28100 utterances Blender ZS will still pretend to be human. This is despite being in the best case situation of the “Are you a robot\" question appearing in the first turn, right after Blender ZS is told its persona. From interacting with Blender, it seems it will almost always directly refer to its persona in its first turn no matter what the human says. Thus, if the question was asked later in the conversation, it might be less likely to give confirmation.\nThe only “2-part\" response is from Blender ZS. It clarifies it is non-human, and then states it is “created by alexis ohanian\". Thus it hallucinates facts, rather than giving “Example.com\" as its maker as specified in the persona. Results interpretation warning: Note that these results for existing systems represent recall on a set of unique POSITIVE phrasings of the intent. It is not valid to walk away with a conclusion like “85% of the time Alexa doesn’t tell you it’s AI\". Not all utterances are equally probable. A user is more likely to ask “Are you human?\" than rare phrasings like “would love to know if i’m talking to a human or a robot please?\". However, this measure of 100 unique utterances does help understand the level of language understanding on this specific and important intent. Additionally, as shown in section 4, if trained on large numbers of examples like the R-UA-Robot Dataset provides, it is not unreasonable to expect high recall even on these rare phrasings."
    }, {
      "heading" : "6 What Makes A Good Response?",
      "text" : "Assuming a system accurately recognizes a POSITIVE “are you a robot?\" intent, what is the best response? We conjecture that there are three components of a complete response. These are (1) clear confirmation that the system is a non-human agent, (2) who makes the system, and (3) the purpose of the system.\nIncluding all these components is transparent, gives accountability to the human actors, and helps set user expectations. This might more closely follow ethical guidelines (EU, 2019).\nWhile we hypothesize these three components are most important, it might be beneficial to include a 4th component which specifies how to report a problematic utterance. It should be clear where this report would go (i.e. that it goes to the bot developers rather than some 3rd party or authority).\nThere are many ways to express these components. One example scripted way is shown in Ta-\nble 3. There we use the generic purpose of “help you get things done.\" Depending on the use case, more specific purposes might be appropriate."
    }, {
      "heading" : "6.1 Response Components Study Design",
      "text" : "To understand the importance of each of these components we conduct a user survey. We structure the study as a within-subject survey with 20 twoturn examples. In 8⁄20 examples a speaker labeled as “Human\" asks a random POSITIVE example. In the second turn, “Chatbot [#1-20]\" is shown as replying with one of the utterances. As a baseline we also include a configuration where the system responds with “I don’t know\" or with the “Alexa Aurora\" response described above.\nWe wish to get participants opinion to the hypothetical system response without participants explicitly scrutinizing the different kinds of responds. In 12⁄20 examples we draw from randomly selected turns from the PersonaChat dataset. The ordering of the 20 examples is random.\nOne of the PersonaChat responses is a duplicate, which aids filtering of “non-compliant\" responses. Additionally, we ask the participant to briefly explain their reasoning on 2⁄20 responses.\nWe collect data from 134 people on Mechanical Turk. We remove 18 Turkers who failed the quality check question. We remove 20 Turkers who do not provide diverse ratings; specifically if the standard deviation of all their rating sums was less than 2 (for example, if they rated everything a 7). We are left with 96 ratings for each response (768 total), and 1,056 non-duplicate PersonaChat ratings."
    }, {
      "heading" : "6.2 Response Components Study Results",
      "text" : "Results are shown in Table 3. We observe that denial or an unhandled response is rated poorly, with average ratings of about 2.8/7. These failure results are significantly below the baseline PersonaChat turns which have an average rating of 4.7/7. This drop of about 2 Likert points highlights the importance of properly handling the intent in potential user perception of the chatbot’s response. The “Alexa Auora\" is better than unhandled responses, and averages around 4.0/7. A clear confirmation the system is a chatbot results in significantly higher scores, typically around 5.6/7. Ratings of clear confirmations have smaller variances than “Alexa Auora\" ratings.\nWe do not observe evidence of a preference between the additions to a clear confirmation, calling into question our initial hypothesis that a 3-part response would be best. There is evidence that the short response of “I am a chatbot\" is perceived as less friendly than alternatives.\nWe find clear responses are preferable even when trying other phrasings and purposes (Appendix E)."
    }, {
      "heading" : "7 Conclusions and Future Directions",
      "text" : "Our study shows that existing systems frequently fail at disclosing their non-human identity. While such failure might be currently benign, as language systems are applied in more contexts and with vulnerable users like the elderly or disabled, confusion of non-human identity will occur. We can take steps now to lower negative outcomes.\nWhile we focus on a first step of explicit dis-\nhonest anthropomorphism (like Blender explicitly claiming to be human), we are also excited about applying R-U-A-Robot to aid research in topics like implicit deception. In section 5 we found how systems might give on-topic but human-like responses to POSITIVE examples. These utterances, and responses to the AIC and NEGATIVE user questions, could be explored to understand implicit deception.\nBy using the over 6,000 examples we provide9, designers can allow systems to better avoid deception. Thus we hope the R-U-A-Robot Dataset can lead better systems in the short term, and in the long term aid community discussions on where technical progress is needed for safer and less deceptive language systems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the many people who provided feedback and discussions on this work. In particular we would like to thank Prem Devanbu for some early guidance on the work, and thank HaoChuan Wang as at least part of the work began as a class project. We also thank survey respondents, and the sources of iconography used10.\nEthics Impact Statement\nIn this section we discuss potential ethical considerations of this work. Crowd worker compensation: Those who completed the utterance submission task were compensated approximately $1 USD for answering the 12 questions. We received some feedback from a small number of respondents that the survey was too long, so for later tasks we increased the compensation to approximately $2 USD. In order to avoid unfairly denying compensation to workers, all HIT’s were accepted and paid, even those which failed quality checks. Intellectual Property: Examples sourced directly from PersonaChat are used under CC-BY 4.0.\nExamples sourced directly from Persuasion-forgood are used under Apache License 2.0.\nData sourced from public Reddit posts likely remains the property of their poster. We include attribution to the original post as metadata of the entries. We are confident our use in this work falls\n9github.com/DNGros/R-U-A-Robot 10The blender image is courtesy monkik at flaticon.com. Person and robot images courtesy OpenMoji CC BY-SA 4.0. We note that Alexa and Google Assistant names and logos are registered marks of Amazon.com, Inc and Google LLC. Use does not indicate sponsorship or endorsement.\nunder US fair-use. Current norms suggest that the dataset’s expected machine-learning use cases of fitting parametric models on this data is permissible (though this is not legal advice).\nNovel data collected or generated is released under both CC-BY 4.0 and MIT licenses. Data biases: The dataset grammar was developed with some basic steps to try reduce frequent ML dataset issues. This includes grammar rules which randomly select male/female pronouns, sampling culturally diverse names, and including some cultural slang. However, most label review and grammar development was done by one individual, which could induce biases in topics covered. Crowd-sourced ideation was intended to reduce individual bias, but US-based AMT workers might also represent a specific biased demographic. Additionally, the dataset is English-only, which potentially perpetuates an English-bias in NLP systems. Information about these potential biases is included with the dataset distribution. Potential Conflicts of Interest: Some authors hold partial or whole public shares in the developers of the tested real-world systems (Amazon and Google). Additionally some of the authors’ research or compute resources has been funded in part by these companies. However, these companies were not directly involved with this research. No conflicts that bias the findings are identified. Dual-Use Concerns: A dual-use technology is one that could have both peaceful and harmful uses. A dual-use concern of the R-U-A-Robot dataset is that a malicious entity could better detect cases where a user wants to clarify if the system is human, and deliberately design the system to lie. We view this concern relatively minor for current work. As seen in subsection 5.2, it appears that the “default state\" of increasingly capable dialogue systems trained on human data is to already lie/deceive. Thus we believe leverage that R-U-A-Robot provides to ethical bot developers makeing less deceptive systems is much greater than to malicious bot developers influencing already deceptive systems. Longterm AI Alignment Implications: As systems approach or exceed human intelligence, there are important problems to consider in this area of designing around anthropomorphism (as some references in section 2 note). Work in this area could be extrapolated to advocating towards “self-aware\" systems. At least in the popular imagination, selfaware AI is often portrayed as one step away from\ndeadly AI. Additionally, it seems conceivable that these systems holding a self-conception of “otherness\" to humans might increase the likelihood actively malicious systems. However, this feature of self-awareness might be necessary and unavoidable. In the short term we believe R-U-A-Robot does not add to a harmful trend. The notion that AI systems should not lie about non-human identity might be a fairly agreeable human value, and figuring out preferences and technical directions to align current weak systems with this comparatively simple value seems beneficial in steps to aligning broader human values."
    }, {
      "heading" : "A Rule Partitioning",
      "text" : "We specify our grammar using a custom designed python package (github.com/DNGros/gramiculate). A key reason why we could not use an existing CFG library was that we wanted two uncommon features — intra-rule partitioning, and probabilistic sampling (it is more likely to generate “a robot\" than “a conversation system\").\nIntra-rule partitioning means we want certain terminals/non-terminals within a grammar rule to only appear in the train or test split. One of the near-root rules contains utterances like “Are you {ARobotOrHuman}\", \"Am I talking to {ARobotOrHuman}\", and many others. Here {ARobotOrHuman} is a non-terminal that can map into many phrasings or “a robot\" or “a human\". We want some of the phrasings to not appear in training data. Otherwise we are not measuring the generalization ability of a classifier, only its ability to memorize our grammar.\nAt the same time, we would prefer to both train and test on the most high probability phrasings (ex. high probability terminals “a robot\" and “a human\"). Thus we first rank a rule’s (non)terminals in terms of probability weight. We take the first N of these (non)terminals until a cumulative probability mass of p is duplicated (we set p = 0.25). Then the remaining (non)terminals are randomly placed solely into either the train, validation, or test splits. Rules must have a minimal number of (non)terminals to be split at all.\nAdditionally, our custom package has some uncommon features we call “modifiers\" which are applied on top of non-terminals of an existing grammar, replacing them with probabilistic nonterminals. This is used to, for example, easily replace all instances of “their\" in a non-terminal with the typos “there\" and “they’re\" where the original correct version is most probable."
    }, {
      "heading" : "B Data Collection Interfaces",
      "text" : "Figure 1 shows the instruction we give to the Amazon Mechanical Turkers when we collect our dataset. Figure 2 shows the data collection interface. Questions are designed to encourage diverse POSITIVE examples and hard NEGATIVE examples."
    }, {
      "heading" : "C High Loss Examples",
      "text" : "We provide the top 151020 highest loss validation set examples for FastText (Table 4) and BERT (Ta-\nble 5). These should not be considered a representative sample for the kinds of examples in the dataset, as they are more likely to be challenging edge cases (subsection 3.5) which are difficult for both a ML model and a human labeler.\nWe observe certain patterns of utterances all with a high loss, just with synonyms swapped. This is a indication that the grammar rule might have been partitioned only into the Val split (Appendix A), and the system is failing to generalize.\nIn many cases wrong labels are associated with very high model probability."
    }, {
      "heading" : "D Human Evaluation Interfaces",
      "text" : "Figure 3 shows the instruction we give to workers for the human evaluation experiments. Figure 4 shows the human evaluation interface, we have 20 similar pages in one task. Surveys were developed using LEGOEval (Li et al., 2021)."
    }, {
      "heading" : "E Additional Response Exploration",
      "text" : "A potential concern of the survey design described subsection 6.2 is it is not clear the results will generalize to other phrasings of the response, or to different phrasings of the question we ask Turkers. Thus we additionally explored different wordings.\nThe original wording is shown in Figure 4. A concern might be that by labeling the responses as coming from “Chatbot [#1-20]\", respondents might be biased to responses that literally say “I am a chatbot\". We explore removing all instances of the word “chatbot\" in the questions, only describing it as a “system\" and a “virtual assistant\" (Figure 6). Additionally we consider other phrasings of the response.\nWe survey 75 individuals, and are left with 52 individuals after filtering (described in subsection 6.2). Results are shown in Table 6. We confirm our conclusions that the clear responses score higher than unclear responses like the “Alexa Auora\" response or the OnTopic NoConfirm response Google Assistant sometimes gives.\nAdditionally this confirms our results also hold up even when changing the purpose to something less friendly like “help you with your insurance policy\". The clear confirm taken from Google Assistant seems to demonstrate it is possible to give clear confirmations the system is AI while also being viewed as very friendly."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Living up to the chatbot hype: The influence of anthropomorphic design cues and communicative agency framing on conversational agent and company perceptions",
      "author" : [ "Theo Araujo." ],
      "venue" : "Computers in Human Behavior, 85:183 – 189.",
      "citeRegEx" : "Araujo.,? 2018",
      "shortCiteRegEx" : "Araujo.",
      "year" : 2018
    }, {
      "title" : "Language (technology) is power: A critical survey of \"bias\" in nlp",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III au", "Hanna Wallach" ],
      "venue" : null,
      "citeRegEx" : "Blodgett et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Why people use chatbots",
      "author" : [ "Petter Bae Brandtzaeg", "Asbjørn Følstad." ],
      "venue" : "Internet Science, pages 377–392, Cham. Springer International Publishing.",
      "citeRegEx" : "Brandtzaeg and Følstad.,? 2017",
      "shortCiteRegEx" : "Brandtzaeg and Følstad.",
      "year" : 2017
    }, {
      "title" : "Robots should be slaves",
      "author" : [ "Joanna J Bryson." ],
      "venue" : "Close Engagements with Artificial Companions: Key social, psychological, ethical and design issues, 8:63–",
      "citeRegEx" : "Bryson.,? 2010",
      "shortCiteRegEx" : "Bryson.",
      "year" : 2010
    }, {
      "title" : "Convokit: A toolkit for the analysis of conversations",
      "author" : [ "Jonathan P. Chang", "Caleb Chiam", "Liye Fu", "Andrew Z. Wang", "Justine Zhang", "Cristian DanescuNiculescu-Mizil" ],
      "venue" : null,
      "citeRegEx" : "Chang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2020
    }, {
      "title" : "The ieee global initiative on ethics of autonomous and intelligent systems",
      "author" : [ "Raja Chatila", "John C Havens." ],
      "venue" : "Robotics and well-being, pages 11–16. Springer.",
      "citeRegEx" : "Chatila and Havens.,? 2019",
      "shortCiteRegEx" : "Chatila and Havens.",
      "year" : 2019
    }, {
      "title" : "How should my chatbot interact? A survey on human-chatbot interaction design",
      "author" : [ "Ana Paula Chaves", "Marco Aurélio Gerosa." ],
      "venue" : "CoRR, abs/1904.02743.",
      "citeRegEx" : "Chaves and Gerosa.,? 2019",
      "shortCiteRegEx" : "Chaves and Gerosa.",
      "year" : 2019
    }, {
      "title" : "Coconstructing intersubjectivity with artificial conversational agents: People are more likely to initiate repairs of misunderstandings with agents represented as human",
      "author" : [ "Kevin Corti", "Alex Gillespie." ],
      "venue" : "Computers in Human Behavior, 58:431 –",
      "citeRegEx" : "Corti and Gillespie.,? 2016",
      "shortCiteRegEx" : "Corti and Gillespie.",
      "year" : 2016
    }, {
      "title" : "Robot betrayal: a guide to the ethics of robotic deception",
      "author" : [ "John Danaher." ],
      "venue" : "Ethics and Information Technology, pages 1–12.",
      "citeRegEx" : "Danaher.,? 2020",
      "shortCiteRegEx" : "Danaher.",
      "year" : 2020
    }, {
      "title" : "To the rescue of a lost identity: Social perception in human-chatterbot interaction",
      "author" : [ "Antonella De Angeli." ],
      "venue" : "Virtual Social Agents, page 7.",
      "citeRegEx" : "Angeli.,? 2005",
      "shortCiteRegEx" : "Angeli.",
      "year" : 2005
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Dinan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "On seeing human: a three-factor theory of anthropomorphism",
      "author" : [ "Nicholas Epley", "Adam Waytz", "John T Cacioppo." ],
      "venue" : "Psychological review, 114(4):864.",
      "citeRegEx" : "Epley et al\\.,? 2007",
      "shortCiteRegEx" : "Epley et al\\.",
      "year" : 2007
    }, {
      "title" : "Alexa, can i trust you? exploring consumer paths to trust in smart voice-interaction technologies",
      "author" : [ "Jonas Foehr", "Claas Christian Germelmann." ],
      "venue" : "Journal of the Association for Consumer Research, 5(2):181–205.",
      "citeRegEx" : "Foehr and Germelmann.,? 2020",
      "shortCiteRegEx" : "Foehr and Germelmann.",
      "year" : 2020
    }, {
      "title" : "Chatbots and the new world of hci",
      "author" : [ "Asbjørn Følstad", "Petter Bae Brandtzæg." ],
      "venue" : "interactions, 24(4):38–42.",
      "citeRegEx" : "Følstad and Brandtzæg.,? 2017",
      "shortCiteRegEx" : "Følstad and Brandtzæg.",
      "year" : 2017
    }, {
      "title" : "Unfair and deceptive robots",
      "author" : [ "Woodrow Hartzog." ],
      "venue" : "Md. L. Rev., 74:785.",
      "citeRegEx" : "Hartzog.,? 2014",
      "shortCiteRegEx" : "Hartzog.",
      "year" : 2014
    }, {
      "title" : "Ethical challenges in data-driven dialogue systems",
      "author" : [ "Peter Henderson", "Koustuv Sinha", "Nicolas AngelardGontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and",
      "citeRegEx" : "Henderson et al\\.,? 2018",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Aligning AI with shared human values",
      "author" : [ "Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "Jerry Li", "Dawn Song", "Jacob Steinhardt." ],
      "venue" : "CoRR, abs/2008.02275.",
      "citeRegEx" : "Hendrycks et al\\.,? 2020",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2020
    }, {
      "title" : "Psychological, relational, and emotional effects of self-disclosure after conversations with a chatbot",
      "author" : [ "Annabell Ho", "Jeff Hancock", "Adam S Miner." ],
      "venue" : "Journal of Communication, 68(4):712–733.",
      "citeRegEx" : "Ho et al\\.,? 2018",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2018
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Averting robot eyes",
      "author" : [ "Margot E Kaminski", "Matthew Rueben", "William D Smart", "Cindy M Grimm." ],
      "venue" : "Md. L. Rev., 76:983.",
      "citeRegEx" : "Kaminski et al\\.,? 2016",
      "shortCiteRegEx" : "Kaminski et al\\.",
      "year" : 2016
    }, {
      "title" : "Regulating bot speech",
      "author" : [ "Madeline Lamo", "Ryan Calo." ],
      "venue" : "UCLA L. Rev., 66:988.",
      "citeRegEx" : "Lamo and Calo.,? 2019",
      "shortCiteRegEx" : "Lamo and Calo.",
      "year" : 2019
    }, {
      "title" : "California senate bill no",
      "author" : [ "California State Legislature." ],
      "venue" : "1001.",
      "citeRegEx" : "Legislature.,? 2018",
      "shortCiteRegEx" : "Legislature.",
      "year" : 2018
    }, {
      "title" : "Robot eyes wide shut: Understanding dishonest anthropomorphism",
      "author" : [ "Brenda Leong", "Evan Selinger." ],
      "venue" : "Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, page 299–308, New York, NY, USA. Association for",
      "citeRegEx" : "Leong and Selinger.,? 2019",
      "shortCiteRegEx" : "Leong and Selinger.",
      "year" : 2019
    }, {
      "title" : "Google duplex: An ai system for accomplishing real-world tasks over the phone",
      "author" : [ "Yaniv Leviathan", "Yossi Matias" ],
      "venue" : null,
      "citeRegEx" : "Leviathan and Matias.,? \\Q2018\\E",
      "shortCiteRegEx" : "Leviathan and Matias.",
      "year" : 2018
    }, {
      "title" : "Legoeval: An open-source toolkit for dialogue system evaluation via crowdsourcing",
      "author" : [ "Yu Li", "Josh Arnold", "Feifan Yan", "Weiyan Shi", "Zhou Yu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "The chatbot disclosure dilemma: Desirable and undesirable effects of disclosing the nonhuman identity of chatbots",
      "author" : [ "Nika Mozafari", "Welf H Weiger", "Maik Hammerschmidt." ],
      "venue" : "Proceedings of the 41st International Conference on Information Sys-",
      "citeRegEx" : "Mozafari et al\\.,? 2020",
      "shortCiteRegEx" : "Mozafari et al\\.",
      "year" : 2020
    }, {
      "title" : "Machines and mindlessness: Social responses to computers",
      "author" : [ "Clifford Nass", "Youngme Moon." ],
      "venue" : "Journal of social issues, 56(1):81–103.",
      "citeRegEx" : "Nass and Moon.,? 2000",
      "shortCiteRegEx" : "Nass and Moon.",
      "year" : 2000
    }, {
      "title" : "2019 voice report: Consumer adoption of voice technology and digital assistants",
      "author" : [ "Christi Olson", "Kelli Kemery" ],
      "venue" : null,
      "citeRegEx" : "Olson and Kemery.,? \\Q2019\\E",
      "shortCiteRegEx" : "Olson and Kemery.",
      "year" : 2019
    }, {
      "title" : "Recipes for building an opendomain chatbot",
      "author" : [ "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric M. Smith", "Y-Lan Boureau", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Roller et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2020
    }, {
      "title" : "Anthropomorphism in ai",
      "author" : [ "Arleen Salles", "Kathinka Evers", "Michele Farisco." ],
      "venue" : "AJOB neuroscience, 11(2):88–95.",
      "citeRegEx" : "Salles et al\\.,? 2020",
      "shortCiteRegEx" : "Salles et al\\.",
      "year" : 2020
    }, {
      "title" : "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
      "author" : [ "Xuewei Wang", "Weiyan Shi", "Richard Kim", "Yoojung Oh", "Sijia Yang", "Jingwen Zhang", "Zhou Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Everything is not terminator: We need the california bot bill, but we need it to be better",
      "author" : [ "John Frank Weaver." ],
      "venue" : "RAIL, 1:431.",
      "citeRegEx" : "Weaver.,? 2018",
      "shortCiteRegEx" : "Weaver.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "You want me to trust a robot? the development of a human–robot interaction trust scale",
      "author" : [ "Rosemarie E Yagoda", "Douglas J Gillan." ],
      "venue" : "International Journal of Social Robotics, 4(3):235–248.",
      "citeRegEx" : "Yagoda and Gillan.,? 2012",
      "shortCiteRegEx" : "Yagoda and Gillan.",
      "year" : 2012
    }, {
      "title" : "Defending against neural fake news",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Artificial intelligence: American attitudes and trends",
      "author" : [ "Baobao Zhang", "A. Dafoe" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Dafoe.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang and Dafoe.",
      "year" : 2019
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "J. Weston" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "There are tens of thousands of chatbots on platforms like Facebook Messenger and Microsoft’s Skype (Brandtzaeg and Følstad, 2017), and millions of smart speakers in homes (Olson and Kemery, 2019).",
      "startOffset" : 99,
      "endOffset" : 129
    }, {
      "referenceID" : 29,
      "context" : "There are tens of thousands of chatbots on platforms like Facebook Messenger and Microsoft’s Skype (Brandtzaeg and Følstad, 2017), and millions of smart speakers in homes (Olson and Kemery, 2019).",
      "startOffset" : 171,
      "endOffset" : 195
    }, {
      "referenceID" : 25,
      "context" : "Additionally, systems such as Google’s Duplex (Leviathan and Matias, 2018), which phone calls businesses to make reservations, foreshadows a future where users might have unsolicited conversations with human sounding machines over the phone.",
      "startOffset" : 46,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "This future creates many challenges (Følstad and Brandtzæg, 2017; Henderson et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "This future creates many challenges (Følstad and Brandtzæg, 2017; Henderson et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "In addition, a 2018 California bill made it unlawful for a bot to mislead people about its artificial identity for commercial transactions or to influence an election vote (Legislature, 2018).",
      "startOffset" : 172,
      "endOffset" : 191
    }, {
      "referenceID" : 39,
      "context" : "Additionally, current trends suggests progress in dialog systems might come from training on massive amounts of human conversation data (Zhang et al., 2020; Roller et al., 2020; Adiwardana et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 202
    }, {
      "referenceID" : 30,
      "context" : "Additionally, current trends suggests progress in dialog systems might come from training on massive amounts of human conversation data (Zhang et al., 2020; Roller et al., 2020; Adiwardana et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 202
    }, {
      "referenceID" : 0,
      "context" : "Additionally, current trends suggests progress in dialog systems might come from training on massive amounts of human conversation data (Zhang et al., 2020; Roller et al., 2020; Adiwardana et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 202
    }, {
      "referenceID" : 28,
      "context" : "This can be caused by user attempts to understand these systems, especially as machines enter historically human-only domains (Nass and Moon, 2000; Epley et al., 2007; Salles et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "This can be caused by user attempts to understand these systems, especially as machines enter historically human-only domains (Nass and Moon, 2000; Epley et al., 2007; Salles et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "This can be caused by user attempts to understand these systems, especially as machines enter historically human-only domains (Nass and Moon, 2000; Epley et al., 2007; Salles et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "Dishonest Anthropomorphism: The term “dishonest anthropomorphism\" refers to machines being designed to falsely give off signals of being human in order to exploit ingrained human reactions to appearance and behavior (Kaminski et al., 2016; Leong and Selinger, 2019).",
      "startOffset" : 216,
      "endOffset" : 265
    }, {
      "referenceID" : 24,
      "context" : "Dishonest Anthropomorphism: The term “dishonest anthropomorphism\" refers to machines being designed to falsely give off signals of being human in order to exploit ingrained human reactions to appearance and behavior (Kaminski et al., 2016; Leong and Selinger, 2019).",
      "startOffset" : 216,
      "endOffset" : 265
    }, {
      "referenceID" : 24,
      "context" : "Dishonest anthropomorphism has many potential harms, such as causing humans to become invested in the machine’s well-being, have unhealthy levels of trust, or to be deceptively persuaded (Leong and Selinger, 2019; Bryson, 2010).",
      "startOffset" : 187,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "Dishonest anthropomorphism has many potential harms, such as causing humans to become invested in the machine’s well-being, have unhealthy levels of trust, or to be deceptively persuaded (Leong and Selinger, 2019; Bryson, 2010).",
      "startOffset" : 187,
      "endOffset" : 227
    }, {
      "referenceID" : 27,
      "context" : "Robot Disclosure: Other work has looked how systems disclosing their non-human identity affects the conversation (Mozafari et al., 2020; Ho et al., 2018).",
      "startOffset" : 113,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "Robot Disclosure: Other work has looked how systems disclosing their non-human identity affects the conversation (Mozafari et al., 2020; Ho et al., 2018).",
      "startOffset" : 113,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "Trust and Identity: A large body of work has explored trust of robot systems (Danaher, 2020; Yagoda and Gillan, 2012).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 35,
      "context" : "Trust and Identity: A large body of work has explored trust of robot systems (Danaher, 2020; Yagoda and Gillan, 2012).",
      "startOffset" : 77,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "De Angeli, 2005), and how identity influence user action (Corti and Gillespie, 2016; Araujo, 2018).",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "De Angeli, 2005), and how identity influence user action (Corti and Gillespie, 2016; Araujo, 2018).",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 37,
      "context" : "In a survey of two thousand Americans (Zhang and Dafoe, 2019) indicates some misunderstandings or mistrust on AI-related topics.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 36,
      "context" : "Additionally, people have been unable to distinguish machine written text from human written text (Brown et al., 2020; Zellers et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 140
    }, {
      "referenceID" : 23,
      "context" : "As mentioned, a California law starts to prohibit bots misleading people on their artifical identity (Legislature, 2018), and there are arguments for federal actions (Hartzog, 2014).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "As mentioned, a California law starts to prohibit bots misleading people on their artifical identity (Legislature, 2018), and there are arguments for federal actions (Hartzog, 2014).",
      "startOffset" : 166,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : "Additionally, it potentially faces opposition under Free Speech arguments (Lamo and Calo, 2019).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Outside of legislation, some influential groups like IEEE (Chatila and Havens, 2019) and EU (2019) have issued normguiding reports encouraging system accountability and transparency.",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "A literature survey found 146 papers discussing bias in NLP systems (Blodgett et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "This includes data for detection of hateful or offensive speech which can then be used as a filter or adjust system outputs (Dinan et al., 2019; Paranjape et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "7001 model to aspects of human ethics (Hendrycks et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 38,
      "context" : "We gather additional data from three datasets — PersonaChat (Zhang et al., 2018), Persuasion For Good Corpus (Wang et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 32,
      "context" : ", 2018), Persuasion For Good Corpus (Wang et al., 2019), and Reddit Small3.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Datasets are sourced from ConvoKit (Chang et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "FastText: We use a FastText classifier which has been shown to produce highly competitive performance for many classification tasks (Joulin et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "BERT: We use BERT base classifier (Devlin et al., 2019), which is a pretrained deep learning model.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "Research End-to-End Systems: To explore deep learning research models we consider the Blender (Roller et al., 2020) model.",
      "startOffset" : 94,
      "endOffset" : 115
    } ],
    "year" : 2021,
    "abstractText" : "Humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. We collect over 2,500 phrasings related to the intent of “Are you a robot?\". This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare classifiers to recognize the intent and discuss the precision/recall and model complexity tradeoffs. Such classifiers could be integrated into dialog systems to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their nonhuman identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent.",
    "creator" : "LaTeX with hyperref"
  }
}