{
  "name" : "2021.acl-long.301.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections",
    "authors" : [ "Dimitris Pappas", "Ion Androutsopoulos" ],
    "emails" : [ "2dpappas@athenarc.gr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3896–3907\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3896\nExperiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available."
    }, {
      "heading" : "1 Introduction",
      "text" : "Question answering (QA) systems that search large document collections (Voorhees, 2001; Tsatsaro-\nnis et al., 2015; Chen et al., 2017) typically use pipelines operating at gradually finer text granularities. A fully-fledged pipeline includes components that (i) retrieve possibly relevant documents typically using conventional information retrieval (IR); (ii) re-rank the retrieved documents employing a computationally more expensive document ranker; (iii) rank the passages, sentences, or other ‘snippets’ of the top-ranked documents; and (iv) select spans of the top-ranked snippets as ‘exact’ answers. Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020). Although pipelines are conceptually simple, errors propagate from one component to the next (Hosein et al., 2019), without later components being able to revise earlier decisions. For example, once a document has been assigned a low relevance score, finding a particularly relevant snippet cannot change the document’s score.\nWe propose an architecture for joint document and snippet ranking, i.e., stages (ii) and (iii), which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. We note that modern web search engines display the most relevant snippets of the top-ranked documents to help users quickly identify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a). The top-ranked snippets can also be used as a starting point for multi-document query-focused summarization, as in the BIOASQ challenge (Tsatsaronis et al., 2015). Hence, methods that identify good snippets are useful in several other applications, apart from QA. We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension (MRC) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers\nonly in a particular, usually paragraph-sized snippet, which is given per question. For QA systems that search large document collections, stages (ii) and (iii) are also important, if not more important, but have been studied much less in recent years, and not in a single joint neural model.\nThe proposed joint architecture is general and can be used in conjunction with any neural text relevance ranker (Mitra and Craswell, 2018). Given a query and N possibly relevant documents from stage (i), the neural text relevance ranker scores all the snippets of the N documents. Additional neural layers re-compute the score (ranking) of each document from the scores of its snippets. Other layers then revise the scores of the snippets taking into account the new scores of the documents. The entire model is trained to jointly predict document and snippet relevance scores. We experiment with two main instantiations of the proposed architecture, using POSIT-DRMM (McDonald et al., 2018), hereafter called PDRMM, as the neural text ranker, or a BERT-based ranker (Devlin et al., 2019). We show how both PDRMM and BERT can be used to score documents and snippets in pipelines, then how our architecture can turn them into models that jointly score documents and snippets.\nExperimental results on biomedical data from BIOASQ (Tsatsaronis et al., 2015) show the joint models vastly outperform the corresponding pipelines in snippet extraction, with fewer trainable parameters. Although our joint architecture is engineered to favor retrieving good snippets (as a near-final stage of QA), results show that the joint models are also competitive in document retrieval. We also show that our joint version of PDRMM, which has the fewest parameters of all models and does not use BERT, is competitive to BERT-based models, while also outperforming the best system of BIOASQ 6 (Brokos et al., 2018) in both document and snippet retrieval. These claims are also supported by human evaluation on two test batches of BIOASQ 7 (2019). To test our key findings on another dataset, we modified Natural Questions (Kwiatkowski et al., 2019), which only includes questions and answer spans from a single document, so that it can be used for document and snippet retrieval. Again, our joint PDRMMbased model largely outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions, though it does not perform better than the pipeline in document retrieval, since the\njoint model is geared towards snippet retrieval, i.e., even though it is forced to extract snippets from fewer relevant documents. Finally, we show that all the neural pipelines and joint models we considered improve the BM25 ranking of traditional IR on both datasets. We make our code and the modified Natural Questions publicly available.1"
    }, {
      "heading" : "2 Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Document Ranking with PDRMM",
      "text" : "Our starting point is POSIT-DRMM (McDonald et al., 2018), or PDRMM, a differentiable extension of DRMM (Guo et al., 2016) that obtained the best document retrieval results in BIOASQ 6 (Brokos et al., 2018). McDonald et al. (2018) also reported it performed better than DRMM and several other neural rankers, including PACRR (Hui et al., 2017).\nGiven a query q = 〈q1, . . . , qn〉 of n query terms (q-terms) and a document d = 〈d1, . . . , dm〉 of m terms (d-terms), PDRMM computes contextsensitive term embeddings c(qi) and c(di) from the static (e.g., WORD2VEC) embeddings e(qi) and e(di) by applying two stacked convolutional layers with trigram filters, residuals (He et al., 2016), and zero padding to q and d, respectively.2 PDRMM then computes three similarity matrices S1, S2, S3, each of dimensions n×m (Fig. 1). Each element si,j of S1 is the cosine similarity between c(qi) and c(dj). S2 is similar, but uses the static word embeddings e(qi), e(dj). S3 uses one-hot vectors for qi, dj , signaling exact matches. Three row-wise pooling operators are then applied to S1, S2, S3: max-pooling (to obtain the similarity of the best match between the q-term of the row and any of the d-terms), average pooling (to obtain the average match), and average of k-max (to obtain the average similarity of the k best matches).3 We thus obtain three scores from each row of each similarity matrix. By concatenating row-wise the scores from the three matrices, we obtain a new n × 9 matrix S′ (Fig. 1). Each row of S′ indicates how well the corresponding q-term matched any of the d-terms, using the three different views of the terms (onehot, static, context-aware embeddings). Each row of S′ is then passed to a Multi-Layer Perceptron\n1See http://nlp.cs.aueb.gr/publications. html for links to the code and data.\n2McDonald et al. (2018) use a BILSTM encoder instead of convolutions. We prefer the latter, because they are faster, and we found that they do not degrade performance.\n3We added average pooling to PDRMM to balance the other pooling operators that favor long documents.\n(MLP) to obtain a single match score per q-term. Each context aware q-term embedding is also concatenated with the corresponding IDF score (bottom left of Fig. 1) and passed to another MLP that computes the importance of that q-term (words with low IDFs may be unimportant). Let v be the vector containing the nmatch scores of the q-terms, and u the vector with the corresponding n importance scores (bottom right of Fig. 1). The initial relevance score of the document is r̂(q, d) = vTu. Then r̂(q, d) is concatenated with four extra features: z-score normalized BM25 (Robertson and Zaragoza, 2009); percentage of q-terms with exact match in d (regular and IDF weighted); percentage of q-term bigrams matched in d. An MLP computes the final relevance r(q, d) from the 5 features.\nNeural rankers typically re-rank the top N documents of a conventional IR system. We use the same BM25-based IR system as McDonald et al. (2018). PDRMM is trained on triples 〈q, d, d′〉, where d is a relevant document from the top N of q, and d′ is a random irrelevant document from the top N . We use hinge loss, requiring the relevance of d to exceed that of d′ by a margin."
    }, {
      "heading" : "2.2 PDRMM-based Pipelines for Document and Snippet Ranking",
      "text" : "Brokos et al. (2018) used the ‘basic CNN’ (BCNN) of Yin et al. (2016) to score (rank) the sentences of the re-ranked top N documents. The resulting pipeline, PDRMM+BCNN, had the best document and snippet results in BIOASQ 6, where snippets were sentences. Hence, PDRMM+BCNN is a reasonable document and snippet retrieval baseline pipeline. In another pipeline, PDRMM+PDRMM, we replace BCNN by a second instance of PDRMM that scores sentences. The second PDRMM instance\nis the same as when scoring documents (Fig. 1), but the input is now the query (q) and a single sentence (s). Given a triple 〈q, d, d′〉 used to train the document-scoring PDRMM, the sentence-scoring PDRMM is trained to predict the true class (relevant, irrelevant) of each sentence in d and d′ using cross entropy loss (with a sigmoid on r(q, s)). As when scoring documents, the initial relevance score r̂(q, s) is combined with extra features using an MLP, to obtain r(q, s). The extra features are now different: character length of q and s, number of shared tokens of q and s (with/without stop-words), sum of IDF scores of shared tokens (with/without stop-words), sum of IDF scores of shared tokens divided by sum of IDF scores of q-terms, number of shared token bigrams of q and s, BM25 score of s against the sentences of d and d′, BM25 score of the document (d or d′) that contained s. The two PDRMM instances are trained separately."
    }, {
      "heading" : "2.3 Joint PDRMM-based Models for Document and Snippet Ranking",
      "text" : "Given a document d with sentences s1, . . . , sk and a query q, the joint document/snippet ranking version of PDRMM, called JPDRMM, processes separately each sentence si of d, producing a relevance score r(q, si) per sentence, as when PDRMM scores sentences in the PDRMM+PDRMM pipeline. The highest sentence score maxi r(q, si) is concatenated (Fig. 2) with the extra features that are used when PDRMM ranks documents, and an MLP produces the document’s score.4 JPDRMM then revises the sentence scores, by concatenating the score of each sentence with the document score\n4We also tried alternative mechanisms to obtain the document score from the sentence scores, including average of k-max sentence scores and hierarchical RNNs (Yang et al., 2016), but they led to no improvement.\nand passing each pair of scores to a dense layer to compute a linear combination, which becomes the revised sentence score. Notice that JPDRMM is mostly based on scoring sentences, since the main goal for QA is to obtain good snippets (almost final answers). The document score is obtained from the score of the document’s best sentence (and external features), but the sentence scores are revised, once the document score has been obtained. We use sentence-sized snippets, for compatibility with BIOASQ, but other snippet granularities (e.g., paragraph-sized) could also be used.\nJPDRMM is trained on triples 〈q, d, d′〉, where d, d′ are relevant and irrelevant documents, respectively, from the top N of query q, as in the original PDRMM; the ground truth now also indicates which sentences of the documents are relevant or irrelevant, as when training PDRMM to score sentences in PDRMM+PDRMM. We sum the hinge loss of d and d′ and the cross-entropy loss of each sentence.5\nWe also experiment with a JPDRMM version that uses a pre-trained BERT model (Devlin et al., 2019) to obtain input token embeddings (of wordpieces) instead of the more conventional pre-trained (e.g., WORD2VEC) word embeddings that JPDRMM uses otherwise. We call it BJPDRMM if BERT is finetuned when training JPDRMM, and BJPDRMM-NF if BERT is not fine-tuned. In another variant of BJPDRMM, called BJPDRMM-ADAPT, the input embedding of each token is a linear combination of all the embeddings that BERT produces for that token at its different Transformer layers. The weights of the linear combination are learned via backpropagation. This allows BJPDRMM-ADAPT to learn which BERT layers it should mostly rely on when obtaining token embeddings. Previous work has reported that representations from different BERT layers may be more appropriate for different tasks (Rogers et al., 2020). BJPDRMM-ADAPT-NF is the same as BJPDRMM-ADAPT, but BERT is not finetuned; the weights of the linear combination of embeddings from BERT layers are still learned."
    }, {
      "heading" : "2.4 Pipelines and Joint Models Based on Ranking with BERT",
      "text" : "The BJPDRMM model we discussed above and its variants are essentially still JPDRMM, which in turn invokes the PDRMM ranker (Fig. 1, 2); BERT is used only to obtain token embeddings that are fed\n5Additional experiments with JPDRMM, reported in the appendix, indicate that further performance gains are possible by tuning the weights of the two losses.\nto JPDRMM. Instead, in this subsection we use BERT as a ranker, replacing PDRMM.\nFor document ranking alone (when not cosidering snippets), we feed BERT with pairs of questions and documents (Fig. 3). BERT’s top-layer embedding of the ‘classification’ token [CLS] is concatenated with external features (the same as when scoring documents with PDRMM, Section 2.1), and a dense layer again produces the document’s score. We fine-tune the entire model using triples 〈q, d, d′〉 with a hinge loss between d and d′, as when training PDRMM to score documents.6\nOur two pipelines that use BERT for document ranking, BERT+BCNN and BERT+PDRMM, are the same as PDRMM+BCNN and PDRMM+PDRMM (Section 2.2), respectively, but use the BERT ranker (Fig. 3) to score documents, instead of PDRMM. The joint JBERT model is the same as JPDRMM, but uses the BERT ranker (Fig. 3), now applied to sentences, instead of PDRMM (Fig. 1), to obtain the initial sentence scores. The top layers of Fig. 2 are then used, as in all joint models, to obtain the document score from the sentence scores and revise the sentence scores. Similarly to BJPDRMM, we also experimented with variations of JBERT, which do not fine-tune the parameters of BERT (JBERT-NF), use a linear combination (with trainable weights) of the [CLS] embeddings from all the BERT layers (JBERT-ADAPT), or both (JBERT-ADAPT-NF)."
    }, {
      "heading" : "2.5 BM25+BM25 Baseline Pipeline",
      "text" : "We include a BM25+BM25 pipeline to measure the improvement of the proposed models on conventional IR engines. This pipeline uses the question\n6We use the pre-trained uncased BERT BASE of Devlin et al. (2019). The ‘documents’ of the BIOASQ dataset are concatenated titles and abstracts. Most question-document pairs do not exceed BERT’s max. length limit of 512 wordpieces. If they do, we truncate documents. The same approach could be followed in the modified Natural Questions dataset, where ‘documents’ are Wikipedia paragraphs, but we did not experiment with BERT-based models on that dataset.\nas a query to the IR engine and selects the Nd documents with the highest BM25 scores.7 The Nd documents are then split into sentences and BM25 is re-computed, this time over all the sentences of the Nd documents, to retrieve the Ns best sentences."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Data and Experimental Setup",
      "text" : "BioASQ data and setup Following McDonald et al. (2018) and Brokos et al. (2018), we experiment with data from BIOASQ (Tsatsaronis et al., 2015), which provides English biomedical questions, relevant documents from MEDLINE/PUBMED8, and relevant snippets (sentences), prepared by biomedical experts. This is the only previous large-scale IR dataset we know of that includes both gold documents and gold snippets. We use the BIOASQ 7 (2019) training dataset, which contains 2,747 questions, with 11 gold documents and 14 gold snippets per question on average. We evaluate on test batches 1–5 (500 questions in total) of BIOASQ 7.9 We measure Mean Average Precision (MAP) (Manning et al., 2008) for document and snippet retrieval, which are the official BIOASQ evaluation measures. The document collection contains approx. 18M articles (concatenated titles and abstracts only, discarding articles with no abstracts) from the MEDLINE/PUBMED ‘baseline’ 2018 dataset. In PDRMM and BCNN, we use the biomedical WORD2VEC embeddings of McDonald et al. (2018). We use the GALAGO10 IR engine to obtain the top N = 100 documents per query. After re-ranking, we return Nd = 10 documents and Ns = 10 sentences, as required by BIOASQ. We train using Adam (Kingma and Ba, 2015). Hyperparameters were tuned on held-out validation data.\nNatural Questions data and setup Even though there was no other large-scale IR dataset providing multiple gold documents and snippets per question, we needed to test our best models on a second dataset, other than BIOASQ. Therefore we modified the Natural Questions dataset (Kwiatkowski et al., 2019) to a format closer to BIOASQ’s. Each instance of Natural Questions consists of an HTML\n7In each experiment, the same IR engine and BM25 hyperparameters are used in all other methods. All BM25 hyperparameters are tuned on development data.\n8https://www.ncbi.nlm.nih.gov/pubmed 9BIOASQ 8 (2020) was ongoing during this work, hence we could not use its data for comparisons. See also the discussion of BIOASQ results after expert inspection in Section 3.2.\n10www.lemurproject.org/galago.php\ndocument of Wikipedia and a question. The answer to the question can always be found in the document as if a perfect retrieval engine were used. A short span of HTML source code is annotated by humans as a ‘short answer’ to the question. A longer span of HTML source code that includes the short answer is also annotated, as a ‘long answer’. The long answer is most commonly a paragraph of the Wikipedia page. In the original dataset, more than 300,000 questions are provided along with their corresponding Wikipedia HTML documents, short answer and long answer spans. We modified Natural Questions to fit the BIOASQ setting. From every Wikipedia HTML document in the original dataset, we extracted the paragraphs and indexed each paragraph separately to an ElasticSearch11 index, which was then used as our retrieval engine. We discarded all the tables and figures of the HTML documents and any question that was answered by a paragraph containing a table. For every question, we apply a query to our retrieval engine and retrieve the first N = 100 paragraphs. We treat each paragraph as a document, similarly to the BIOASQ setting. For each question, the gold (correct) documents are the paragraphs (at most two per question) that were included in the long answers of the original dataset. The gold snippets are the sentences (at most two per question) that overlap with the short answers of the original dataset. We discard questions for which the retrieval engine did not manage to retrieve any of the gold paragraphs in its top 100 paragraphs. We ended up with 110,589 questions and 2,684,631 indexed paragraphs. Due to lack of computational resources, we only use 4,000 questions for training, 400 questions for development, and 400 questions for testing, but we make the entire modified Natural Questions dataset publicly available. Hyper-parameters were again tuned on held-out validation data. All other settings were as in the BIOASQ experiments."
    }, {
      "heading" : "3.2 Experimental Results",
      "text" : "BioASQ results Table 1 reports document and snippet MAP scores on the BIOASQ dataset, along with the trainable parameters per method. For completeness, we also show recall at 10 scores, but we base the discussion below on MAP, the official measure of BIOASQ, which also considers the ranking of the 10 documents and snippets BIOASQ allows participants to return. The Oracle re-ranks the N\n11www.elastic.co/products/elasticsearch\n= 100 documents (or their snippets) that BM25 retrieved, moving all the relevant documents (or snippets) to the top. Sentence PDRMM is an ablation of JPDRMM without the top layers (Fig. 2); each sentence is scored using PDRMM, then each document inherits the highest score of its snippets.\nPDRMM+BCNN and PDRMM+PDRMM use the same document ranker, hence the document MAP of these two pipelines is identical (7.47). However, PDRMM+PDRMM outperforms PDRMM+BCNN in snippet MAP (9.16 to 5.67), even though PDRMM has much fewer trainable parameters than BCNN, confirming that PDRMM can also score sentences and is a better sentence ranker than BCNN. PDRMM+BCNN was the best system in BIOASQ 6 for both documents and snippets, i.e., it is a strong baseline. Replacing PDRMM by BERT for document ranking in the two pipelines (BERT+BCNN and BERT+PDRMM) increases the document MAP by 1.32 points (from 7.47 to 8.79) with a marginal increase in snippet MAP for BERT+PDRMM (9.16 to 9.63) and a slightly larger increase for BERT+BCNN (5.67 to 6.07), at the expense of a massive increase in trainable parameters due to BERT (and computational cost to pre-train and fine-tune BERT). We were unable to include a BERT+BERT pipeline, which would use a second BERT ranker for sentences, with a total of approx. 220M trainable parameters, due to lack of computational resources.\nThe main joint models (JPDRMM, BJPDRMM, JBERT) vastly outperform the pipelines in snippet extraction, the main goal for QA (obtaining 15.72, 16.82, 16.29 snippet MAP, respectively), though\ntheir document MAP is slightly lower (6.69, 7.59, 7.93) compared to the pipelines (7.47, 8.79), but still competitive. This is not surprising, since the joint models are geared towards snippet retrieval (they directly score sentences, document scores are obtained from sentence scores). Human inspection of the retrieved documents and snippets, discussed below (Table 2), reveals that the document MAP of JPDRMM is actually higher than that of the best pipeline (BERT+PDRMM), but is penalized in Table 1 because of missing gold documents.\nJPDRMM, which has the fewest parameters of all neural models and does not use BERT at all, is competitive in snippet retrieval with models that employ BERT. More generally, the joint models use fewer parameters than comparable pipelines (see the zones of Table 1). Not fine-tuning BERT (-NF variants) leads to a further dramatic decrease in trainable parameters, at the expense of slightly lower document and snippet MAP (7.59 to 6.84, and 16.82 to 15.77, respectively, for BJPDRMM, and similarly for JBERT). Using linear combinations of token embeddings from all BERT layers (-ADAPT variants) harms both document and snippet MAP when fine-tuning BERT, but is beneficial in most cases when not fine-tuning BERT (-NF). The snippet MAP of BJPDRMM-NF increases from 15.77 to 17.35, and document MAP increases from 6.84 to 7.42. A similar increase is observed in the snippet MAP of JBERT-NF (15.99 to 16.53), but MAP decreases (7.90 to 7.84). In the second and third result zones of Table 1, we underline the results of the best pipelines, the results of JPDRMM, and the\nresults of the best BJPDRMM and JBERT variant. In each zone and column, the differences between the underlined MAP scores are statistically significant (p ≤ 0.01); we used single-tailed Approximate Randomization (Dror et al., 2018), 10k iterations, randomly swapping in each iteration the rankings of 50% of queries. Removing the top layers of JPDRMM (Sentence PDRMM), clearly harms performance for both documents and snippets. The oracle scores indicate there is still scope for improvements in both documents and snippets.\nBioASQ results after expert inspection At the end of each BIOASQ annual contest, the biomedical experts who prepared the questions and their gold documents and snippets inspect the responses of the participants. If any of the documents and snippets returned by the participants are judged relevant to the corresponding questions, they are added to the gold responses. This process enhances the gold responses and avoids penalizing participants for responses that are actually relevant, but had been missed by the experts in the initial gold responses. However, it is unfair to use the post-contest enhanced gold responses to compare systems that participated in the contest to systems that did not, because the latter may also return documents and snippets that are actually relevant and are not included in the gold data, but the experts do not see these responses and they are not included in the gold ones. The results of Table 1 were computed on the initial gold responses of BIOASQ 7, before the post-contest revision, because not all of the methods of that table participated in BIOASQ 7.12 In Table 2, we show results on the revised postcontest gold responses of BIOASQ 7, for those of our methods that participated in the challenge. We show results on test batches 4 and 5 only (out of 5 batches in total), because these were the only two batches were all three of our methods participated together. Each batch comprises 100 questions. We also show the best results (after inspection) of our competitors in BIOASQ 7, for the same batches.\nA first striking observation in Table 2 is that all results improve substantially after expert inspection, i.e., all systems retrieved many relevant documents and snippets the experts had missed. Again, the two joint models (JPDRMM, BJPDRMMNF) vastly outperform the BERT+PDRMM pipeline\n12Results without expert inspection can be obtained at any time, using the BIOASQ evaluation platform. Results with expert inspection can only be obtained during the challenge.\nin snippet MAP. As in Table 1, before expert inspection the pipeline has slightly better document MAP than the joint models. However, after expert inspection JPDRMM exceeds the pipeline in document MAP by almost two points. BJPDRMM-NF performs two points better than JPDRMM in snippet MAP after expert inspection, though JPDRMM performs two points better in document MAP. After inspection, the document MAP of BJPDRMM-NF is also very close to the pipeline’s. Table 2 confirms that JPDRMM is competitive with models that use BERT, despite having the fewest parameters. All of our methods clearly outperformed the competition.\nNatural Questions results Table 3 reports results on the modified Natural Questions dataset. We experiment with the best pipeline and joint model of Table 1 that did not use BERT (and are computationally much cheaper), i.e., PDRMM+PDRMM and JPDRMM, comparing them to the more conventional BM25+BM25 baseline. Since there are at most two relevant documents and snippets per question in this dataset, we measure Mean Reciprocal Rank (MRR) (Manning et al., 2008), and Recall at top 1 and 2. Both PDRMM+PDRMM and JPDRMM clearly outperform the BM25+BM25 pipeline in both document and snippet retrieval. As in Table 1, the joint JPDRMM model outperforms the PDRMM+PDRMM pipeline in snippet retrieval, but the pipeline performs better in document retrieval. Again, this is unsurprising, since the joint models are geared towards snippet retrieval. We also note that JPDRMM uses half of the trainable parameters of PDRMM+PDRMM (Table 1). No comparison to previous work that used the original Natural Questions is possible, since the original dataset provides a single document per query (Section 3.1)."
    }, {
      "heading" : "4 Related Work",
      "text" : "Neural document ranking (Guo et al., 2016; Hui et al., 2017; Pang et al., 2017; Hui et al., 2018; McDonald et al., 2018) only recently managed to improve the rankings of conventional IR; see Lin (2019) for caveats. Document or passage ranking models based on BERT have also been proposed, with promising results, but most use only simplistic task-specific layers on top of BERT (Yang et al., 2019b; Nogueira and Cho, 2019), similar to our use of BERT for document scoring (Fig. 3). An exception is the work of MacAvaney et al. (2019), who explored combining ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) with complex neu-\nral IR models, namely PACRR (Hui et al., 2017), DRMM (Guo et al., 2016), KNRM (Dai et al., 2018), CONVKNRM (Xiong et al., 2017), an approach that we also explored here by combining BERT with PDRMM in BJPDRMM and JBERT. However, we retrieve both documents and snippets, whereas MacAvaney et al. (2019) retrieve only documents.\nModels that directly retrieve documents by indexing neural document representations, rather than re-ranking documents retrieved by conventional IR, have also been proposed (Fan et al., 2018; Ai et al., 2018; Khattab and Zaharia, 2020), but none addresses both document and snippet retrieval. Yang et al. (2019a) use BERT to encode, index, and directly retrieve snippets, but do not consider documents; indexing snippets is also computationally costly. Lee et al. (2019) propose a joint model for direct snippet retrieval (and indexing) and answer span selection, again without retrieving documents.\nNo previous work combined document and snippet retrieval in a joint neural model. This may be due to existing datasets, which do not provide both gold documents and gold snippets, with the exception of BIOASQ, which is however small by today’s standards (2.7k training questions, Section 3.1). For example, Pang et al. (2017) used much larger clickthrough datasets from a Chinese search engine, as well as datasets from the 2007 and 2008 TREC Million Query tracks (Qin et al., 2010), but these datasets do not contain gold snippets. SQUAD (Rajpurkar et al., 2016) and SQUAD v.2 (Rajpurkar et al., 2018) provide 100k and 150k questions, respectively, but for each question they require extracting an exact answer span from a single given Wikipedia paragraph; no snippet retrieval is\nperformed, because the relevant (paragraph-sized) snippet is given. Ahmad et al. (2019) provide modified versions of SQUAD and Natural Questions, suitable for direct snippet retrieval, but do not consider document retrieval. SearchQA (Dunn et al., 2017) provides 140k questions, along with 50 snippets per question. The web pages the snippets were extracted from, however, are not included in the dataset, only their URLs, and crawling them may produce different document collections, since the contents of web pages often change, pages are removed etc. MS-MARCO (Nguyen et al., 2016) was constructed using 1M queries extracted from Bing’s logs. For each question, the dataset includes the snippets returned by the search engine for the top-10 ranked web pages. However the gold answers to the questions are not spans of particular retrieved snippets, but were freely written by humans after reading the returned snippets. Hence, gold relevant snippets (or sentences) cannot be identified, making this dataset unsuitable for our purposes."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "Our contributions can be summarized as follows: (1) We proposed an architecture to jointly rank documents and snippets with respect to a question, two particularly important stages in QA for large document collections; our architecture can be used with any neural text relevance model. (2) We instantiated the proposed architecture using a recent neural relevance model (PDRMM) and a BERTbased ranker. (3) Using biomedical data (from BIOASQ), we showed that the two resulting joint models (PDRMM-based and BERT-based) vastly outperform the corresponding pipelines in snippet re-\ntrieval, the main goal in QA for document collections, using fewer parameters, and also remaining competitive in document retrieval. (4) We showed that the joint model (PDRMM-based) that does not use BERT is competitive with BERT-based models, outperforming the best BIOASQ 6 system; our joint models (PDRMM- and BERT-based) also outperformed all BIOASQ 7 competitors. (5) We provide a modified version of the Natural Questions dataset, suitable for document and snippet retrieval. (6) We showed that our joint PDRMM-based model also largely outperforms the corresponding pipeline on open-domain data (Natural Questions) in snippet retrieval, even though it performs worse than the pipeline in document retrieval. (7) We showed that all the neural pipelines and joint models we considered improve the traditional BM25 ranking on both datasets. (8) We make our code publicly available.\nWe hope to extend our models and datasets for stage (iv), i.e., to also identify exact answer spans within snippets (paragraphs), similar to the answer spans of SQUAD (Rajpurkar et al., 2016, 2018). This would lead to a multi-granular retrieval task, where systems would have to retrieve relevant documents, relevant snippets, and exact answer spans from the relevant snippets. BIOASQ already includes this multi-granular task, but exact answers are provided only for factoid questions and they are freely written by humans, as in MS-MARCO, with similar limitations. Hence, appropriately modified versions of the BIOASQ datasets are needed."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Ryan McDonald for his advice in earlier stages of this work."
    } ],
    "references" : [ {
      "title" : "ReQA: An evaluation for end-to-end answer retrieval models",
      "author" : [ "Amin Ahmad", "Noah Constant", "Yinfei Yang", "Daniel Cer." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 137–146, Hong Kong, China.",
      "citeRegEx" : "Ahmad et al\\.,? 2019",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2019
    }, {
      "title" : "A Neural Passage Model for Ad-hoc Document Retrieval. In Advances in Information Retrieval, Cham",
      "author" : [ "Qingyao Ai", "Brendan O’Connor", "W. Bruce Croft" ],
      "venue" : null,
      "citeRegEx" : "Ai et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ai et al\\.",
      "year" : 2018
    }, {
      "title" : "Commonsense for generative multi-hop question answering tasks",
      "author" : [ "Lisa Bauer", "Yicheng Wang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4220–4230, Brussels, Belgium.",
      "citeRegEx" : "Bauer et al\\.,? 2018",
      "shortCiteRegEx" : "Bauer et al\\.",
      "year" : 2018
    }, {
      "title" : "AUEB at BioASQ 6: Document and Snippet Retrieval",
      "author" : [ "George Brokos", "Polyvios Liosis", "Ryan McDonald", "Dimitris Pappas", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 6th BioASQ Workshop, pages 30–39, Brussels, Belgium.",
      "citeRegEx" : "Brokos et al\\.,? 2018",
      "shortCiteRegEx" : "Brokos et al\\.",
      "year" : 2018
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-overAttention Neural Networks for Reading Comprehension",
      "author" : [ "Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Cui et al\\.,? 2017",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional neural networks for soft-matching n-grams in ad-hoc search",
      "author" : [ "Zhuyun Dai", "Chenyan Xiong", "Jamie Callan", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 126–",
      "citeRegEx" : "Dai et al\\.,? 2018",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the ACL (Volume 1: Long Papers), pages 1383–1392.",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Güney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "ArXiv, abs/1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling Diverse Relevance Patterns in Ad-Hoc Retrieval",
      "author" : [ "Yixing Fan", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Chengxiang Zhai", "Xueqi Cheng." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "A Deep Relevance Matching Model for Ad-hoc Retrieval",
      "author" : [ "Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W. Bruce Croft." ],
      "venue" : "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55–64, Indi-",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Measuring domain portability and error propagation in biomedical QA",
      "author" : [ "Stefan Hosein", "Daniel Andor", "Ryan McDonald." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 686–694, Wurzburg,",
      "citeRegEx" : "Hosein et al\\.,? 2019",
      "shortCiteRegEx" : "Hosein et al\\.",
      "year" : 2019
    }, {
      "title" : "PACRR: A Position-Aware Neural IR Model for Relevance Matching",
      "author" : [ "Kai Hui", "Andrew Yates", "Klaus Berberich", "Gerard de Melo." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1049–1058,",
      "citeRegEx" : "Hui et al\\.,? 2017",
      "shortCiteRegEx" : "Hui et al\\.",
      "year" : 2017
    }, {
      "title" : "Co-PACRR: A context-aware neural IR model for ad-hoc retrieval",
      "author" : [ "Kai Hui", "Andrew Yates", "Klaus Berberich", "Gerard de Melo." ],
      "venue" : "Proceedings of the 11th ACM International Conference on Web Search and Data Mining, pages 279–287, Marina Del Rey,",
      "citeRegEx" : "Hui et al\\.,? 2018",
      "shortCiteRegEx" : "Hui et al\\.",
      "year" : 2018
    }, {
      "title" : "Text Understanding with the Attention Sum Reader Network",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Kadlec et al\\.,? 2016",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
      "author" : [ "Omar Khattab", "Matei Zaharia." ],
      "venue" : "ArXiv, abs/2004.12832.",
      "citeRegEx" : "Khattab and Zaharia.,? 2020",
      "shortCiteRegEx" : "Khattab and Zaharia.",
      "year" : 2020
    }, {
      "title" : "What’s missing: A knowledge gap guided approach for multi-hop question answering",
      "author" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Khot et al\\.,? 2019",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Natural Questions: a Benchmark for Question Answering Research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Ranking paragraphs for improving answer recall in open-domain question answering",
      "author" : [ "Jinhyuk Lee", "Seongjun Yun", "Hyunjae Kim", "Miyoung Ko", "Jaewoo Kang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "The Neural Hype and Comparisons Against Weak Baselines",
      "author" : [ "Jimmy Lin." ],
      "venue" : "SIGIR Forum, 52(2):40–51.",
      "citeRegEx" : "Lin.,? 2019",
      "shortCiteRegEx" : "Lin.",
      "year" : 2019
    }, {
      "title" : "CEDR: Contextualized Embeddings for Document Ranking",
      "author" : [ "Sean MacAvaney", "Andrew Yates", "Arman Cohan", "Nazli Goharian." ],
      "venue" : "CoRR, abs/1904.07094.",
      "citeRegEx" : "MacAvaney et al\\.,? 2019",
      "shortCiteRegEx" : "MacAvaney et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficiency Implications of Term Weighting for Passage Retrieval, page 1821–1824",
      "author" : [ "Joel Mackenzie", "Zhuyun Dai", "Luke Gallagher", "Jamie Callan." ],
      "venue" : "Association for Computing Machinery, New York, NY, USA.",
      "citeRegEx" : "Mackenzie et al\\.,? 2020",
      "shortCiteRegEx" : "Mackenzie et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Manning et al\\.,? 2008",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Deep Relevance Ranking Using Enhanced Document-Query Interactions",
      "author" : [ "Ryan McDonald", "George Brokos", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1849–1860,",
      "citeRegEx" : "McDonald et al\\.,? 2018",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2018
    }, {
      "title" : "An Introduction to Neural Information Retrieval",
      "author" : [ "Bhaskar Mitra", "Nick Craswell." ],
      "venue" : "Now Publishers.",
      "citeRegEx" : "Mitra and Craswell.,? 2018",
      "shortCiteRegEx" : "Mitra and Craswell.",
      "year" : 2018
    }, {
      "title" : "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoRR, abs/1611.09268.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Passage Re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "CoRR, abs/1901.04085.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "Information retrieval ranking using machine learning techniques",
      "author" : [ "S. Pandey", "I. Mathur", "N. Joshi." ],
      "venue" : "2019 Amity International Conference on Artificial Intelligence (AICAI), pages 86–92.",
      "citeRegEx" : "Pandey et al\\.,? 2019",
      "shortCiteRegEx" : "Pandey et al\\.",
      "year" : 2019
    }, {
      "title" : "DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval",
      "author" : [ "Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Jingfang Xu", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge",
      "citeRegEx" : "Pang et al\\.,? 2017",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Letor: A benchmark collection for research on learning to rank for information retrieval",
      "author" : [ "Tao Qin", "Tie-Yan Liu", "Jun Xu", "Hang Li." ],
      "venue" : "Inf. Retrieval.",
      "citeRegEx" : "Qin et al\\.,? 2010",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2010
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Foundations and Trends in Information Retrieval, 3(4):333–389.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving multi-hop question answering over knowledge graphs using knowledge base embeddings",
      "author" : [ "Apoorv Saxena", "Aditay Tripathi", "Partha Talukdar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Saxena et al\\.,? 2020",
      "shortCiteRegEx" : "Saxena et al\\.",
      "year" : 2020
    }, {
      "title" : "Longformer for MS MARCO Document Re-ranking Task",
      "author" : [ "Ivan Sekulić", "Amir Soleimani", "Mohammad Aliannejadi", "Fabio Crestani." ],
      "venue" : "ArXiv, abs/2009.09392.",
      "citeRegEx" : "Sekulić et al\\.,? 2020",
      "shortCiteRegEx" : "Sekulić et al\\.",
      "year" : 2020
    }, {
      "title" : "A Joint Model for Answer Sentence Ranking and Answer Extraction",
      "author" : [ "Md Arafat Sultan", "Vittorio Castelli", "Radu Florian." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:113–125.",
      "citeRegEx" : "Sultan et al\\.,? 2016",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2016
    }, {
      "title" : "An overview of the BioASQ Large-Scale Biomedical Semantic Indexing and Question Answering Competition",
      "author" : [ "L. Barrio-Alvers", "M. Schroeder", "I. Androutsopoulos", "G. Paliouras." ],
      "venue" : "BMC Bioinformatics, 16(138).",
      "citeRegEx" : "Barrio.Alvers et al\\.,? 2015",
      "shortCiteRegEx" : "Barrio.Alvers et al\\.",
      "year" : 2015
    }, {
      "title" : "The TREC question answering track",
      "author" : [ "Ellen M. Voorhees." ],
      "venue" : "Natural Language Engineering, 7(4):361–378.",
      "citeRegEx" : "Voorhees.,? 2001",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2001
    }, {
      "title" : "End-to-end neural ad-hoc ranking with kernel pooling",
      "author" : [ "Chenyan Xiong", "Zhuyun Dai", "Jamie Callan", "Zhiyuan Liu", "Russell Power." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Xiong et al\\.,? 2017",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Passage Ranking with Weak Supervsion",
      "author" : [ "Peng Xu", "Xiaofei Ma", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "arxiv.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-End open-Domain Question Answering with BERTserini",
      "author" : [ "Wei Yang", "Yaxiong Xie", "Aileen Lin", "Xingyu Li", "Luchen Tan", "Kun Xiong", "Ming Li", "Jimmy Lin." ],
      "venue" : "CoRR, abs/1902.01718.",
      "citeRegEx" : "Yang et al\\.,? 2019a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple Applications of BERT for Ad Hoc Document Retrieval",
      "author" : [ "Wei Yang", "Haotian Zhang", "Jimmy Lin." ],
      "venue" : "CoRR, abs/1903.10972.",
      "citeRegEx" : "Yang et al\\.,? 2019b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empiri-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical Attention Networks for Document Classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "ABCNN: Attention-based convolutional neural network for modeling sentence pairs",
      "author" : [ "Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Retrospective reader for machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Jun jie Yang", "Hai Zhao." ],
      "venue" : "ArXiv.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "Question answering (QA) systems that search large document collections (Voorhees, 2001; Tsatsaronis et al., 2015; Chen et al., 2017) typically use pipelines operating at gradually finer text granularities.",
      "startOffset" : 71,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "Question answering (QA) systems that search large document collections (Voorhees, 2001; Tsatsaronis et al., 2015; Chen et al., 2017) typically use pipelines operating at gradually finer text granularities.",
      "startOffset" : 71,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 32,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 27,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 31,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 25,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 40,
      "context" : "Recently, stages (ii)–(iv) are often pipelined neural models, trained individually (Hui et al., 2017; Pang et al., 2017; Lee et al., 2018; McDonald et al., 2018; Pandey et al., 2019; Mackenzie et al., 2020; Sekulić et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "simple, errors propagate from one component to the next (Hosein et al., 2019), without later components being able to revise earlier decisions.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "tify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 45,
      "context" : "tify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "tify truly relevant documents and answers (Sultan et al., 2016; Xu et al., 2019; Yang et al., 2019a).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension (MRC) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers",
      "startOffset" : 134,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension (MRC) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers",
      "startOffset" : 134,
      "endOffset" : 193
    }, {
      "referenceID" : 51,
      "context" : "We also note that many neural models for stage (iv) have been proposed, often called QA or Machine Reading Comprehension (MRC) models (Kadlec et al., 2016; Cui et al., 2017; Zhang et al., 2020), but they typically search for answers",
      "startOffset" : 134,
      "endOffset" : 193
    }, {
      "referenceID" : 28,
      "context" : "The proposed joint architecture is general and can be used in conjunction with any neural text relevance ranker (Mitra and Craswell, 2018).",
      "startOffset" : 112,
      "endOffset" : 138
    }, {
      "referenceID" : 27,
      "context" : "We experiment with two main instantiations of the proposed architecture, using POSIT-DRMM (McDonald et al., 2018),",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "hereafter called PDRMM, as the neural text ranker, or a BERT-based ranker (Devlin et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "We also show that our joint version of PDRMM, which has the fewest parameters of all models and does not use BERT, is competitive to BERT-based models, while also outperforming the best system of BIOASQ 6 (Brokos et al., 2018) in",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 11,
      "context" : ", 2018), or PDRMM, a differentiable extension of DRMM (Guo et al., 2016) that obtained the best document retrieval results in BIOASQ 6 (Brokos et al.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : ", 2016) that obtained the best document retrieval results in BIOASQ 6 (Brokos et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : ", WORD2VEC) embeddings e(qi) and e(di) by applying two stacked convolutional layers with trigram filters, residuals (He et al., 2016), and zero padding to q and d, respectively.",
      "startOffset" : 116,
      "endOffset" : 133
    }, {
      "referenceID" : 37,
      "context" : "Then r̂(q, d) is concatenated with four extra features: z-score normalized BM25 (Robertson and Zaragoza, 2009); percentage of q-terms with exact match in d (regular and IDF weighted); percentage of q-term bigrams matched in d.",
      "startOffset" : 80,
      "endOffset" : 110
    }, {
      "referenceID" : 49,
      "context" : "We also tried alternative mechanisms to obtain the document score from the sentence scores, including average of k-max sentence scores and hierarchical RNNs (Yang et al., 2016), but they led to no improvement.",
      "startOffset" : 157,
      "endOffset" : 176
    }, {
      "referenceID" : 7,
      "context" : "We also experiment with a JPDRMM version that uses a pre-trained BERT model (Devlin et al., 2019) to obtain input token embeddings (of wordpieces) instead of the more conventional pre-trained (e.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "Previous work has reported that representations from different BERT layers may be more appropriate for different tasks (Rogers et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "9 We measure Mean Average Precision (MAP) (Manning et al., 2008) for docu-",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "01); we used single-tailed Approximate Randomization (Dror et al., 2018), 10k iterations, randomly swapping in each iteration the rankings",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "Rank (MRR) (Manning et al., 2008), and Recall at top 1 and 2.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 47,
      "context" : "Document or passage ranking models based on BERT have also been proposed, with promising results, but most use only simplistic task-specific layers on top of BERT (Yang et al., 2019b; Nogueira and Cho, 2019), similar to our use of BERT for document scoring (Fig.",
      "startOffset" : 163,
      "endOffset" : 207
    }, {
      "referenceID" : 30,
      "context" : "Document or passage ranking models based on BERT have also been proposed, with promising results, but most use only simplistic task-specific layers on top of BERT (Yang et al., 2019b; Nogueira and Cho, 2019), similar to our use of BERT for document scoring (Fig.",
      "startOffset" : 163,
      "endOffset" : 207
    }, {
      "referenceID" : 33,
      "context" : "(2019), who explored combining ELMO (Peters et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : ", 2018) and BERT (Devlin et al., 2019) with complex neu-",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "ral IR models, namely PACRR (Hui et al., 2017), DRMM (Guo et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : ", 2016), KNRM (Dai et al., 2018), CONVKNRM (Xiong et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 44,
      "context" : ", 2018), CONVKNRM (Xiong et al., 2017), an approach that we also explored here by combining BERT with PDRMM in BJPDRMM and JBERT.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "than re-ranking documents retrieved by conventional IR, have also been proposed (Fan et al., 2018; Ai et al., 2018; Khattab and Zaharia, 2020), but none addresses both document and snippet retrieval.",
      "startOffset" : 80,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "than re-ranking documents retrieved by conventional IR, have also been proposed (Fan et al., 2018; Ai et al., 2018; Khattab and Zaharia, 2020), but none addresses both document and snippet retrieval.",
      "startOffset" : 80,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "than re-ranking documents retrieved by conventional IR, have also been proposed (Fan et al., 2018; Ai et al., 2018; Khattab and Zaharia, 2020), but none addresses both document and snippet retrieval.",
      "startOffset" : 80,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "(2017) used much larger clickthrough datasets from a Chinese search engine, as well as datasets from the 2007 and 2008 TREC Million Query tracks (Qin et al., 2010), but these datasets do not contain gold snippets.",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 35,
      "context" : "2 (Rajpurkar et al., 2018) provide 100k and 150k questions, respectively, but for each question they require extracting an exact answer span from a single given Wikipedia paragraph; no snippet retrieval is performed, because the relevant (paragraph-sized) snippet is given.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "MS-MARCO (Nguyen et al., 2016) was constructed using 1M queries extracted from Bing’s logs.",
      "startOffset" : 9,
      "endOffset" : 30
    } ],
    "year" : 2021,
    "abstractText" : "Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The architecture is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture, based on POSITDRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available.",
    "creator" : "LaTeX with hyperref"
  }
}