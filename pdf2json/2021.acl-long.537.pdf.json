{
  "name" : "2021.acl-long.537.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "EMAILSUM: Abstractive Email Thread Summarization",
    "authors" : [ "Shiyue Zhang", "Asli Celikyilmaz", "Jianfeng Gao", "Mohit Bansal" ],
    "emails" : [ "mbansal}@cs.unc.edu", "aslic@fb.com", "jfgao@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6895–6909\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6895"
    }, {
      "heading" : "1 Introduction",
      "text" : "As one of the major natural language generation tasks, automatic summarization has been studied for decades. Most research efforts were focused on single-document summarization tasks, e.g., news document summarization (Hermann et al., 2015; Narayan et al., 2018). However, living in an information era, we are facing with diverse content\n1Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum\nin different structures. The summarization need is varied along with different application scenarios. Recently, there is an increasing research interest in diverse summarization tasks (Gao et al., 2020), e.g., timeline (Allan et al., 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al., 2018), meeting (Carletta et al., 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc. Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (EMAILSUM) dataset.\nEmail threads are widely used at work. An email thread is a special type of dialogue that usually has a specific structure (sender, receiver, greeting line, main body, and the signature), contains technical information, and involves multiple speakers. Unlike a conversational dialog turn, an email in a\nthread is much longer with longer sentences, multiple action items or requests, and stylistically similar to written text. Studies have shown that on average a worker sends/receives 122 business emails (Radicati, 2015) and spends more than 3 hours on those emails (Adobe, 2019) per day. One possible reason is that sometimes people have to read through the entire conversation before replying to the latest email. This happens when you forget the main points of previous discussions or you are newly included in a discussion thread. Therefore, automatically summarizing email threads can improve our work efficiency and provides practical benefits. Email Thread Summarization is not a new task. Carenini et al. (2007) collected extractive summaries of 39 email threads from Enron email corpus (Klimt and Yang, 2004) and proposed to use a fragment quotation graph and clue words to conduct summarization. Ulrich et al. (2008) collected both extractive and abstractive summaries of 40 threads from W3C email corpus (Craswell et al., 2006) plus speech acts, meta sentences, etc. However, this task has been much less studied compared to other summarization tasks, partially due to the lack of large labeled email thread datasets.\nIn this paper, we collect human-written short (< 30 words) and long (< 100 words) abstractive summaries of 2,549 email threads constructed from Avocado Research Email Collection (Oard et al., 2015), which is 64× the size of previously labeled email thread datasets (Carenini et al., 2007; Craswell et al., 2006). We limit each thread to a minimum of 3 and a maximum of 10 emails, an example is given in Table 1. We also extract 8,594 unlabeled email threads from both Avocado and W3C to facilitate semi-supervised learning.2 See Section 2 for details of data collection.\nNext, we present comprehensive baselines from different learning paradigms as a benchmark for our new email summarization dataset. Specifically, we explore different summarization techniques, including extractive and abstractive summarization methods, single-document and hierarchical models, transfer learning, and semi-supervised learning for both short and long summary generation. Experiments demonstrate that utilizing pretrained language model (e.g., T5 (Raffel et al., 2020)) is critical due to the small size of our data; taking the email thread as a single document sets up a\n2We apply strict criteria for thread extraction (see Section 2). More threads can be extracted by relaxing those constraints.\ngood baseline; transferring from news or dialogue datasets barely improve the performance; using hierarchical encoders only marginally improves it; while semi-supervised learning by using unlabelled email threads significantly (p < 0.01) improves ROUGE (Lin, 2004) scores in some cases.\nLastly, to better understand how well the email thread summarization models perform and investigate the correlation between automatic metrics and human judgment, we ask humans to rate the “salience” (how well the model summarizes salient points) and “faithfulness” (how well the model stays true to the email thread) of model-generated summaries, as well as to perform a pairwise comparison between our best and base models. We find that even though semi-supervised learning improves ROUGE scores, human judges still favor the summary generated by the baseline model (T5base). Two frequent errors made by the model are (1) failing to understand the sender’s intent and (2) failing to identify the roles of the sender and receiver. Relatedly, human correlation analysis reveals that automatic metrics (ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019)) are poorly correlated with human judgment, which stresses the importance of human evaluation in this task and the requirement for better metrics to be proposed. Overall, in this work, we propose the new EMAILSUM dataset that provides a larger resource for studying the email thread summarization task. We conduct a comprehensive empirical model study and human evaluation analysis, which will serve as an important starting point for future studies."
    }, {
      "heading" : "2 EMAILSUM Dataset",
      "text" : "To collect email thread summarization data, we first need to obtain unlabelled email threads. We resort to existing email collections: Enron (Klimt and Yang, 2004), W3C (Craswell et al., 2006), and Avocado (Oard et al., 2015). However, none of them provides explicit thread structure. Therefore, in this section, we will introduce our email thread preprocessing and summary collection procedures."
    }, {
      "heading" : "2.1 Email Thread Preprocessing",
      "text" : "We extract email threads from the flat email collections in the following steps: (1) we give every email a “normalized subject” by removing the reply or forward tags (e.g., “Re:”, “Fwd:”, etc.) from its original subject; (2) we group emails by the normalized subjects and sort emails in the same group (i.e.,\nthread) by timestamp; (3) we de-duplicate emails in every thread by sender’s email plus timestamp; (4) we traverse emails in every thread in temporal order and cut off the thread when none of the senders plus receivers of the current email appears in previous emails; (5) we filter out threads that only contain single repeated content.\nTo obtain a cleaner dataset, we remove threads that do not comply with the following constraints: (1) 3 ≤ the number of emails ≤ 10; (2) 5 < the number of words in each email < 200; (3) 30 < the total number of words < 1000; (4) does not contain non-English (e.g., German) tokens; (5) does not contain reply or forward tags in the subject of the first email.\nEmails often contain personal information such as full name, email/physical address, phone number, etc. To protect privacy, we anonymize all email threads before annotation: (1) only keep first names; (2) remove threads that have “password”, “pwd”, “confidential”, etc.; (3) replace email address, physical address, phone number, URL, IP address, local path, and other sensitive numbers with USERNAME@DOMAIN.COM, ADDRESS, PHONENUMBER, HTTP://LINK, IPADDRESS, PATH, and NUMBER, respectively.\nWe conduct an extensive manual quality scan to make sure that the extracted threads are truly threads (instead of random emails grouped) and properly anonymized. Finally, we obtain 8,116 threads from Avocado and 3,478 threads from W3C.3 We randomly sample 3K Avocado threads for summary annotation, and the remaining threads are used as unlabelled data."
    }, {
      "heading" : "2.2 Thread Summary Collection",
      "text" : "We collect summary annotations on Amazon Mechanical Turk. Since summarizing text is not an easy task, to get acceptable English summaries we\n3We find that the extracted threads from Enron are usually short (fewer than 3 emails) and noisy.\nuse several quality control strategies: (1) We select annotators that are located in the US, have an approval rate greater than 97%, and have at least 10,000 approved HITs; (2) During annotation, we periodically sample summaries, manually check their quality, and reject or block poor-quality annotators; (3) After annotation, we randomly sample 2 examples per annotator and manually categorize annotators into “good”, “fair”, and “bad” groups, then filter examples written by bad annotators.\nEmail threads oftentimes contain technical information, we instruct annotators not to get stuck on technical details, instead, focus on the major concerns, decisions, and consensus. We collect both short (< 30 words) and long (< 100 words) abstractive summaries per thread. For the short summary, we instruct annotators to write a concise description of what the thread is mainly talking about; while for the long summary, we instruct them to write a a narrative of what happens. We are intent to provide summaries with two different levels of abstractiveness, length, and concreteness. We show annotators an example written by an expert (a CS graduate student). More summary collection details can be found in Appendix A."
    }, {
      "heading" : "2.3 Final Dataset Description",
      "text" : "The summary collection and filtering process yield 2,549 email threads each with a long and a short summary. We randomly sample 500 examples from the “good” annotator group as our testing set and split the remaining examples into training (1,800 threads) and development (249 threads) sets. Table 2 shows the statistics of EMAILSUM.4. For ease of benchmarking, we also include statistics on other\n4Since comparing the model-generated summary to only one human-written reference may not be fully informative, recently we have also collected one more reference for each email thread in our test set, i.e., each test example will have two gold references now in our final dataset. The results in the paper are all still based on the original one-reference setup but we will release the updated two-reference results for our best baselines on Github.\ncommonly used summarization datasets: CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) are about news summarization; SAMSum (Gliwa et al., 2019) is about chit-chat summarization; CRD3 (Rameshkumar and Bailey, 2020) is a role-play dialogue summarization dataset; BC3 (Ulrich et al., 2008) is another email thread summarization with 40 threads from W3C. Compared to the other datasets, the average document length in the EMAILSUM dataset is not very long, containing 233 words; long summaries are more than twice as longer than short summaries. “Ext-Oracle-R1” in Table 2 indicates how abstractive the summaries are. It computes the ROUGE-1 scores of an oracle extractive method (see Section 3.1 for details of the oracle extractive method). The lower it is, the more abstractive the dataset is. According to this score, the abstractiveness of the EMAILSUM summaries is lower than the XSum summaries, while higher than the CNNDM summaries. Furthermore, the short summaries of EMAILSUM dataset are more abstractive than its long summaries."
    }, {
      "heading" : "3 Models",
      "text" : "The summarization models we explore in this work take the email thread as input and generate the summary as output. We experiment on EMAILSUMshort and EMAILSUMlong tasks separately."
    }, {
      "heading" : "3.1 Extractive",
      "text" : "Oracle. This method maximize an evaluation metric w.r.t. the gold summary. “Ext-Oracle-R1” in Table 2 is computed from an oracle summary that maximizes ROUGE-1 (Lin, 2004).\nLead. This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset (Narayan et al., 2018). We test two variants by selecting: (1) the first sentence of the email thread, which is usually the subject (see the example in Table 1), referred as Lead-1; (2) the first sentence of the email thread (the subject) plus the first sentences of every email, named Lead-1-Email.5\nTextRank. This is a graph-based method (Mihalcea and Tarau, 2004). It first builds a graph between sentences by their embedding similarities; then the PageRank algorithm is applied to obtain the rank\n5We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email.\nscores for each sentence, and top-rank sentences are selected as the summary.\nBertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM."
    }, {
      "heading" : "3.2 Abstractive",
      "text" : "Fast Abs RL. As the simple non-pretrained abstractive baseline, we use Chen and Bansal (2018), which is a hybrid model that first extracts sentences from the source document, then rewrites the extracted sentences by an abstractive rewriter. They pair summary sentences with the extracted sentences to train the abstractive rewriter. Adapting their model to our email thread summarization task, we make two adjustments: (1) We extract emails instead of sentences, which is a natural unit for email thread; (2) Since summary sentences usually follow the temporal order of the emails, we enhance this pairing procedure by using the Neeleman-Wunsch algorithm (Needleman and Wunsch, 1970; Rameshkumar and Bailey, 2020) to impose the order constraint to the alignment (see description and comparison in Appendix B).\nT5. T5 (Raffel et al., 2020) is a Transformer (Vaswani et al., 2017) based seq-to-seq model pretrained with large-scale English data. It achieves state-of-the-art performances on a lot of NLP tasks including the CNN/DM summarization task. As our main baseline, we take the email thread as a single document and finetune a T5 base to generate the summary (T5base). A similar setup is also used in transfer and semi-supervised learning. Since our training dataset is small, we find that using the pretrained knowledge transfer is crucial. Training a T5 model from scratch performs poorly (see the results in Appendix Table 7).\nTransfer Learning. To analyze how information from other summarization datasets (listed in Table 2) can be transferred to this new task and its impact on the performance, we investigate two simple transfer learning methods: (1) Pre-finetuning, in which we first finetune T5 on a bigger summarization dataset (e.g., CNN/DM) then continue the finetuning on our dataset, referred as Xpre (X is the bigger dataset’s name, e.g., CNNDMpre) in our result tables. This is analogous to the continual training method proposed for multilingual transfer learning of machine translation (Kocmi and Bojar,\n2018). (2) Joint-training, in which we upsample EMAILSUM data and mix it with another dataset, then use the combined data to finetune T5, similarly denoted as Xjoint. This is analogous to the multilingual joint training method used in machine translation (Johnson et al., 2017).\nSemi-supervised learning. Since we only have 2.5K labeled email threads, another important technique to improve the performance is to utilize unlabelled data (i.e., email threads without labeled summaries). As introduced in Section 2.1, in addition to the 3K email threads used for summary collection, we have 8,594 unlabelled email threads (5,116 from Avocado; 3,478 from W3C). We explore semi-supervised learning via the simple self-training technique (Scudder, 1965). We use a trained model (a finetuned T5) to generate summaries for unlabelled threads, then mix the model-labeled and human-labeled data to finetune T5 again, referred as SemiSupx (x stands for the unlabelled data source we use, i.e., W3C, Avocado, or together).\nHierarchical T5. Hierarchical summarization models have been shown to improve the performance of multi-document summarization task (Liu and Lapata, 2019a). Although an email thread can be treated as a single document due to the temporal dependency between consecutive emails, it also has a clear turn structure that encourages using of the hierarchical encoders. Recently, Zhu et al. (2020) proposed a hierarchical model (HMNet) for meeting summarization. Inspired by their work, we propose a hierarchical model that is similar to HMNet in structure but uses T5 as the backbone, therefore, it can take advantage of both the hierarchical structure and the pre-trained knowledge. As shown in Figure 1, this model contains two encoders: the token-level encodes the whole email\nthread (e.g., e1, e2, e3, e4) while the email-level receives mean-pooled email-level representations as input. The decoder has two cross attentions that attend to the outputs of the email-level and the token-level encoders respectively. Both token-level and email-level encoders are sharing the weights of the T5 encoder. We add a small number of new parameters by adding new cross attention between the decoder and the email-level encoder."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Evaluation Metrics",
      "text" : "ROUGE (Lin, 2004) is a commonly used automatic metric for summarization tasks. It has several variants: (1) ROUGE-1 (R1) measures the unigram overlap between the generated and reference summaries; (2) ROUGE-2 (R2) measures the bi-gram overlap; (2) ROUGE-L (RL) computes the longest common subsequence (LCS); (4) summary-level ROUGE-L (RLsum) computes LCS between each pair of reference and candidate sentences and returns the union-LCS. We use the rouge score package7 and report F1 scores.\nBERTScore (Zhang et al., 2019) goes beyond n-gram overlap to provide contextualized semantic similarity. Specifically, it uses BERT (Devlin et al., 2019) (or RoBERTa (Liu et al., 2019)) representations to “softly” align the words in candidate and reference summaries and then computes a “soft” uni-gram F1 score. We use the bert score package8 and report rescaled numbers with a baseline."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 3 shows the evaluation results on the testing set of different models (the corresponding results on the development set can be found in Appendix Table 7). It can be observed that the Oracle extractive model sets up a high upper bound on all metrics except for BERTScore (BertS). Among non-oracle extractive methods, the Lead-1-Email heuristic works best and even better than the deep extractive method, BertSumExt. The hybrid Fast Abs RL model outperforms purely extractive methods but works worse than purely abstractive methods with large-scale pretraining (e.g., T5).\n6The significance test is following the bootstrap test setup (Efron and Tibshirani, 1994) and sample for 100k times.\n7https://github.com/google-research/ google-research/tree/master/rouge\n8https://github.com/Tiiiger/bert_score\nTaking the email thread as one single document and finetuning T5 (i.e., T5base in Table 3) sets up a strong baseline. Upon this baseline model, we test the transfer learning from four different summarization datasets (CNN/DM, XSum, SAMSum, and CRD3). However, as shown in Table 3, transfer learning barely improves over baseline, and transferring by pre-finetuning always works better than joint-training. Since our EMAILSUM has a quite different domain as existing news or dialogue datasets, we conjecture that it is hard to transfer knowledge between them or better transferring techniques need to be applied. Similarly, we test the semi-supervised learning with unlabelled data from W3C, Avocado, and both of them (together). This method can mostly (or significantly in some cases) outperform the baseline’s performance for both EMAILSUMshort and EMAILSUMlong. Lastly, the hierarchical T5base model only marginally outperforms the non-hierarchical\nbaseline for EMAILSUMlong task. It is notable that overall EMAILSUMlong has higher ROUGE scores but lower BERTScore than EMAILSUMshort. Since we focus on generating abstractive summaries for email threads and the human-written summaries are fairly abstractive (as shown in Table 2), we further investigate the abstractiveness of model-generated summaries. We take summaries generated by the baseline (T5base) and the best ROUGE-1 models (SemiSuptogether for EMAILSUMshort, SemiSupw3c for EMAILSUMlong) as the pseudo ground-truth, respectively. Then, we evaluate the ROUGE-1 of extractive Oracle and Lead1-Email models; higher scores means more extractive summaries. As shown in Table 4, compared\nto humans, models generate much more extractive summaries. Moreover, the semi-supervised models (R1-best) are even more extractive than the baseline, which is probably because the self-training procedure amplifies the extraction tendency. Lastly, for both base and best models as well as for both short and long summaries, the model performance (ROUGE-1) decreases as the number of emails in the thread increases (shown in Figure 2)."
    }, {
      "heading" : "5 Human Evaluation",
      "text" : ""
    }, {
      "heading" : "5.1 Human Rating Collection",
      "text" : "To better understand where the model still falls short and investigate if the automatic metrics correlate well with human judgments, we conduct a human evaluation on Amazon Mechanical Turk. Initially, by manually checking the quality of modelgenerated summaries, we find that models can mostly generate grammatical, relevant, and fluent summaries; however, they often fail to be salient and faithful, i.e., models tend to be overdetailed or do not stay true to the source thread. Therefore, we ask human annotators to rate the “salience” and “faithfulness” of model-generated summaries. We choose the best ROUGE-1 models, SemiSuptogether for EMAILSUMshort, SemiSupw3c for EMAILSUMlong, to evaluate, then we sample 100 examples, and collect 3 responses for each example. Human judges are asked to rate on a 5-point Likert scale for salience and faithfulness respectively and annotate which summary sentences are not salient or unfaithful. We explain the meaning of “salience” and “faithfulness” to annotators and instruct them how to rate from 1 to 5. Meanwhile, to verify the improvement obtained by best R1 models over T5base, we ask them to compare the summaries generated by these models and those from T5base, and judge which one is more salient, more faithful, and has overall higher quality. More collection details can be found in the Appendix D.\nWe check the average inter-rater agreement (Krippendorff’s alpha (Krippendorff, 2011)) of “salience” and “faithfulness” ratings. It is around\n0.09 to 0.23, i.e., slight to fair agreement (Fleiss and Cohen, 1973). However, when we convert the ratings to 3-point by taking {3}, {4 and 5}, {1 and 2} as 3 classes, the agreement increases to 0.36 to 0.63, i.e., fair to substantial agreement. This indicates that humans’ subjectivity affects the ratings and people have a hard time distinguishing ‘bad’ from ‘very bad’ as well as ‘good’ from ‘very good’. Meanwhile, the ratings for short summaries are always less agreed across raters (0.36-0.38) than that for long summaries (0.58-0.63). This indicates that there might be multiple different ways of summarizing an email thread into a short summary. The agreement of pairwise comparison is around 0.20 to 0.24 (fair agreement), which is because the baseline and the best models have non-distinguishable performance (shown in Table 5). Finally, we take the 3-rater average as the final human rating for each example.\nIn addition, we evaluate the correlations (Pearson Correlation (Benesty et al., 2009)) among different human ratings. The correlation between salience and faithfulness ratings is 0.36/0.45 for short/long summarization. And the correlations among salience, faithfulness, and overall quality pairwise preferences are around 0.53 to 0.79. Overall, moderate to large (Cohen, 2013) correlations are observed."
    }, {
      "heading" : "5.2 Generated Summary’s Quality Analysis",
      "text" : "Surprisingly, human evaluators are mostly satisfied with the salience and faithfulness of modelgenerated summaries, ratings are around 4 out of 5. On average, humans rate 3.89 and 4.04 for the salience and faithfulness of SemiSuptogether generated short summaries, respectively; and they rate 4.22 and 4.29 for the salience and faithfulness of SemiSupw3c generated long summaries, respectively. Examples with low or high ratings are shown in Table 6 or Appendix Table 8. Humans rate higher for model-generated long summaries, which is correlated to the trend of ROUGE, and they are more satisfied with faithfulness than salience.\nTable 5 presents the human pairwise compari-\nson between the best ROUGE-1 models and T5base. Except for the faithfulness of EMAILSUMlong, the best ROUGE-1 models mostly lose to the baseline (though the loss and win are mostly marginal). Together with Table 4, we conjecture that the improvement obtained by semi-supervised learning exploits n-gram matching accuracy by making the summary more extractive, while humans prefer more abstractive summaries.\nLastly, we analyze the non-salient and unfaithful sentences labeled by the human evaluators. We find that two errors are frequently made by the summarization model: (1) Failing to understand the sender’s intent. Usually, when we send an email, there is a high-level intention behind the detailed content we write, e.g., start up a discussion, bring up a concern, broadcast a decision, etc. However, models are oftentimes unable to capture the intention and thus overly focus on details. As shown in the first example of Table 6, Om intends to summarize the important points from a meeting, while the model only picks the first piece of detail in that email as the summary. This problem is also related to the over-extractive issue (shown in Table 4). The model tends to extract details from the source thread and the extraction is biased to the first sentence of each email. (2) Failing to identify the roles of the sender and receiver. An email thread is a special type of conversation with multiple speakers involved. One important task\nC oe\nff ic\nie nt\nfor the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020)."
    }, {
      "heading" : "5.3 Correlation with Human Judgement",
      "text" : "ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and human judgments.\nSpecifically, we evaluate the Pearson Correlation between human ratings and automatic metric scores on the 100 examples used in the human evaluation. Besides, as described above, we conduct a pairwise model comparison between the best ROUGE-1 models and T5base for “salience”, “faithfulness”, and “overall quality”. We convert them to a pairwise ranking score, i.e., -1 if T5base is better; 1 if T5base is worse; 0 if two models are non-distinguishable. In the same way, we convert different metric scores to ranking scores. Then, we also evaluate the Pearson Correlation between human and metric ranking scores. Figure 3 illustrates the results. Overall, the correlations are fairly poor. The best correlation is between ROUGE-1 and human overall quality ranking for short summary generation (coefficient=0.14, p=0.16). There is little or negative correlation between metrics and human judgment for the long summary generation. Therefore, we emphasize the importance of human evaluation and better automatic proxies need to be proposed in the future."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries. We explore different summarization paradigms and find that taking the email thread as a single document and finetuning T5 (Raffel et al., 2020) sets up a good baseline. Transferring from other summarization datasets barely improves it. Using hierarchical structure also only marginally improves the performance. Semi-supervised learning by using unlabelled email threads improves automatic metrics (ROUGE) but still loses to the baseline in human evaluation. Finally, our human evaluation reveals that the model fails to understand the sender’s main\nintention and the roles of different speakers. Automatic metrics are poorly correlated with human judgment, which emphasizes the importance of human evaluation and designing new metrics for this task in the future."
    }, {
      "heading" : "7 Broader Impact Statement",
      "text" : "We use two email collections in this work: Avocado (Oard et al., 2015) and W3C (Craswell et al., 2006). W3C is derived from W3C Public Mailing List that is open-source available online. Avocado consists of emails and attachments taken from 279 accounts of a defunct information technology company referred to as “Avocado”. Its copyright is protected by Linguistic Data Consortium. Based on the license agreement, we will only open-source our collected summaries and provide scripts to obtain email threads from the original Avocado email collection. To further protect copyright and the privacy of the persons involved in the emails, as introduced in Section 2, we carefully anonymize all the email threads we construct from both email collections. We fairly pay crowd-source workers $1.37 (for threads with 5 or fewer emails) or $2 (for threads with more than 5 emails) for writing the short and long summaries and $0.6 for human rating such that the pay rate is higher than the federal minimum wage requirement."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their helpful comments and Xiang Zhou for useful discussions. We thank Saadia Gabriel, Yichen Jiang, Tom McCoy, and Yixin Nie for helping write summary examples (to show as initial examples to MTurk annotators) and estimate the workload for deciding the fair payment. This work was partially done while SZ was interning at MSR and later extended at UNC, where it was supported by NSF-CAREER Award 1846185, ONR Grant N00014-18-1-2871, and a Microsoft Investigator Fellowship. The views contained in this article are those of the authors and not of the funding agency."
    }, {
      "heading" : "A Summary Collection",
      "text" : "Figure 4 illustrates the questions we asked human annotators on Amazon Mechanical Turk during summary collection. Before these questions, here are some important instructions we listed on the webpage: (1) Long summary MUST be longer than short summary; (2) Summary length can be dynamically decided based on the content of the thread; (3) Short summary should be a concise and abstractive description of what the thread is mainly talking about; (4) Long summary can be a narrative of what happens. But do NOT simply summarize each email separately. The summary should be coherent; (5) It is NOT necessary to summarize every email in the long summary, i.e., it is OK to skip\nunimportant ones and merge similar ones if needed; (6) You are encouraged to include important sender and/or receiver names in long summary; (7) You are disencouraged to copy a lot from emails for both short and long summaries; You are supposed to write in your own words as much as you can; (8) You may find some content are technical. We do NOT expect any background knowledge. Just focus on the major concerns, decisions, and consensus. (9) In the thread, emails are ordered by time. However, one email does NOT necessarily reply to the previous one. It can reply to an earlier email OR forward to new receivers. In other words, the structure is NOT always continuous, so please be careful when you read."
    }, {
      "heading" : "B Fast Abs RL",
      "text" : "The original Fast Abs RL method (Chen and Bansal, 2018) uses ROUGE-Lrecall to align extracted source sentences and target summary sentences. In our case, we extract emails and align them with summary sentences. Since the emails and summary sentences usually follow the same temporal order, we enhance the alignment procedure by the Neeleman-Wunsch algorithm (Needleman and Wunsch, 1970; Rameshkumar and Bailey, 2020) to imposing strict order constraints, e.g., there should not be “emaili is aligned to sentencej while emaili+1 is aligned to sentencej−1” cases.\nMeanwhile, we modify it to allow one email to be aligned with multiple summary sentences but avoid one summary sentence aligning with multiple emails. Specifically, we first obtain the similarity matrix M of size ne × ns between each email and summary sentence by ROUGE-Lrecall (ne is the number of emails, ns is the number of summary sentences); then the alignment score matrix H of size (ne+1)×(ns+1) is initialized as all-zero then computed as follows for 1 ≤ x ≤ ne, 1 ≤ y ≤ ns:\nHx,y = max  Hx−1,y−1 +Mx−1,y−1 Hx,y−1 +Mx−1,y−1 Hx−1,y\nThen we traceback from Hne,ns to H0,0 to obtain the final alignment. As shown in Table 7, the “Fast Abs RL (default)” model refers to this method with the default setting which works mostly worse than our enhanced Fast Abs RL."
    }, {
      "heading" : "C Experimental Details & Additional Results",
      "text" : "We implement the TextRank (Mihalcea and Tarau, 2004) model via the summa python package9 and set the summarization ratio as the average summary length thread length ratio in the training set, which is 0.22 for short summary and 0.38 for long summary. 9https://github.com/summanlp/textrank\nWe test Fast Abs RL (Chen and Bansal, 2018) via the author’s open-source code.10 Most of our models are built on T5 (Raffel et al., 2020) and we use the base version that has 220 million parameters. Our hierarchical T5 shares the same T5 encoder parameters between the token-level and email-level encoders. The only new parameters added are from the first cross attention between decoder and emaillevel encoder. We use Transformers (Wolf et al., 2020)11 to run all the T5 based models. We run experiments on a single Tesla V100 GPU. We set the max input sequence length as 512 tokens and max output length as 56 tokens during training (200 tokens during evaluation). The total batch size (with gradient accumulation) is 128. The learning rate is 5e-4, except for training the T5base from scratch, we use 1e-4 instead. Since our training set only contains 1.8K examples, it only takes 2-4 minutes per epoch. We train models for 70 epochs.\nOur model selection is based on each of the five evaluation metrics, ROUGE-1/ROUGE-2/ROUGEL/summary-level ROUGE-L/BERTScore. We select the best checkpoints for each of the five metrics on our development set, then test those checkpoints on the testing set to report the final numbers for each metric. Table 7 shows all the results on our development set. Table 8 shows two examples that have high-rating model-generated summaries."
    }, {
      "heading" : "D Human Evaluation",
      "text" : "Figure 5 shows the questions we asked to human judges to evaluate the quality of model-generated summaries. Before these questions, we instruct annotators how to rate on a 5-point Likert scale for “salience” and “faithfulness”: (1) Rate salience from 1 to 5: 1 is the worst, none of the points in the summary is important enough to be summarized; 5 is the best, all of the points mentioned in the summary are important and worth to be summarized; (2) Rate faithfulness from 1 to 5: 1 is the worst, all of the sentences in the summary are either wrong or not existing in the email thread; 5 is the best, all of the points mentioned in the summary are true to the thread. Plus, we also prompt examples of “non-salient” and “unfaithful” summaries on the webpage. We pay annotators $0.60 per HIT.\n10https://github.com/ChenRocks/fast_ abs_rl\n11https://github.com/huggingface/ transformers"
    } ],
    "references" : [ {
      "title" : "Temporal summaries of new topics",
      "author" : [ "James Allan", "Rahul Gupta", "Vikas Khandelwal." ],
      "venue" : "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 10–18.",
      "citeRegEx" : "Allan et al\\.,? 2001",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 2001
    }, {
      "title" : "Pearson correlation coefficient",
      "author" : [ "Jacob Benesty", "Jingdong Chen", "Yiteng Huang", "Israel Cohen." ],
      "venue" : "Noise reduction in speech processing, pages 1–4. Springer.",
      "citeRegEx" : "Benesty et al\\.,? 2009",
      "shortCiteRegEx" : "Benesty et al\\.",
      "year" : 2009
    }, {
      "title" : "Reevaluating evaluation in text summarization",
      "author" : [ "Manik Bhandari", "Pranav Narayan Gour", "Atabak Ashfaq", "Pengfei Liu", "Graham Neubig." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Bhandari et al\\.,? 2020",
      "shortCiteRegEx" : "Bhandari et al\\.",
      "year" : 2020
    }, {
      "title" : "Summarizing email conversations with clue words",
      "author" : [ "Giuseppe Carenini", "Raymond T Ng", "Xiaodong Zhou." ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web, pages 91–100.",
      "citeRegEx" : "Carenini et al\\.,? 2007",
      "shortCiteRegEx" : "Carenini et al\\.",
      "year" : 2007
    }, {
      "title" : "The ami meeting corpus: A pre-announcement",
      "author" : [ "Dennis Reidsma", "Pierre Wellner." ],
      "venue" : "Machine Learning for Multimodal Interaction, pages 28–39, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Reidsma and Wellner.,? 2006",
      "shortCiteRegEx" : "Reidsma and Wellner.",
      "year" : 2006
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–686.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Statistical power analysis for the behavioral sciences",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Academic press.",
      "citeRegEx" : "Cohen.,? 2013",
      "shortCiteRegEx" : "Cohen.",
      "year" : 2013
    }, {
      "title" : "Overview of the trec-2005 enterprise track",
      "author" : [ "Nick Craswell", "Arjen P Vries", "Ian M Soboroff." ],
      "venue" : "Text Retrieval Conference (TREC).",
      "citeRegEx" : "Craswell et al\\.,? 2006",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2006
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "author" : [ "Esin Durmus", "He He", "Mona Diab." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–",
      "citeRegEx" : "Durmus et al\\.,? 2020",
      "shortCiteRegEx" : "Durmus et al\\.",
      "year" : 2020
    }, {
      "title" : "An introduction to the bootstrap",
      "author" : [ "Bradley Efron", "Robert J Tibshirani." ],
      "venue" : "CRC press.",
      "citeRegEx" : "Efron and Tibshirani.,? 1994",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1994
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 33(3):613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "From standard summarization to new tasks and beyond: Summarization with manifold information",
      "author" : [ "Shen Gao", "Xiuying Chen", "Zhaochun Ren", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Ar-",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Samsum corpus: A human-annotated dialogue dataset for abstractive summarization",
      "author" : [ "Bogdan Gliwa", "Iwona Mochol", "Maciej Biesek", "Aleksander Wawer." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79.",
      "citeRegEx" : "Gliwa et al\\.,? 2019",
      "shortCiteRegEx" : "Gliwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in neural information processing systems, pages 1693–1701.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Google’s multilingual neural machine translation system: Enabling zero-shot translation",
      "author" : [ "Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Viégas", "Martin Wattenberg", "Greg Corrado" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "The enron corpus: A new dataset for email classification research",
      "author" : [ "Bryan Klimt", "Yiming Yang." ],
      "venue" : "European Conference on Machine Learning, pages 217–226. Springer.",
      "citeRegEx" : "Klimt and Yang.,? 2004",
      "shortCiteRegEx" : "Klimt and Yang.",
      "year" : 2004
    }, {
      "title" : "Trivial transfer learning for low-resource neural machine translation",
      "author" : [ "Tom Kocmi", "Ondrej Bojar." ],
      "venue" : "WMT 2018, page 244.",
      "citeRegEx" : "Kocmi and Bojar.,? 2018",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2018
    }, {
      "title" : "Computing krippendorff’s alpha-reliability",
      "author" : [ "Klaus Krippendorff" ],
      "venue" : null,
      "citeRegEx" : "Krippendorff.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "Query-focused multidocument summarization: Combining a topic model with graph-based semi-supervised learning",
      "author" : [ "Yanran Li", "Sujian Li." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Li and Li.,? 2014",
      "shortCiteRegEx" : "Li and Li.",
      "year" : 2014
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070– 5081.",
      "citeRegEx" : "Liu and Lapata.,? 2019a",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019b",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919.",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Using summarization to discover argument facets in online idealogical dialog",
      "author" : [ "Amita Misra", "Pranav Anand", "Jean E Fox Tree", "Marilyn Walker." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Misra et al\\.,? 2015",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2015
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
      "author" : [ "Saul B Needleman", "Christian D Wunsch." ],
      "venue" : "Journal of molecular biology, 48(3):443–453.",
      "citeRegEx" : "Needleman and Wunsch.,? 1970",
      "shortCiteRegEx" : "Needleman and Wunsch.",
      "year" : 1970
    }, {
      "title" : "Avocado Research Email Collection LDC2015T03",
      "author" : [ "Douglas Oard", "William Webber", "David Kirsch", "Sergey Golitsynskiy." ],
      "venue" : "DVD. Philadelphia: Linguistic Data Consortium.",
      "citeRegEx" : "Oard et al\\.,? 2015",
      "shortCiteRegEx" : "Oard et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Storytelling with dialogue: A critical role dungeons and dragons dataset",
      "author" : [ "Revanth Rameshkumar", "Peter Bailey." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5121–5134.",
      "citeRegEx" : "Rameshkumar and Bailey.,? 2020",
      "shortCiteRegEx" : "Rameshkumar and Bailey.",
      "year" : 2020
    }, {
      "title" : "Probability of error of some adaptive pattern-recognition machines",
      "author" : [ "H Scudder." ],
      "venue" : "IEEE Transactions on Information Theory, 11(3):363–371.",
      "citeRegEx" : "Scudder.,? 1965",
      "shortCiteRegEx" : "Scudder.",
      "year" : 1965
    }, {
      "title" : "A publicly available annotated corpus for supervised email summarization",
      "author" : [ "Jan Ulrich", "Gabriel Murray", "Giuseppe Carenini." ],
      "venue" : "Proc. of aaai email-2008 workshop, chicago, usa.",
      "citeRegEx" : "Ulrich et al\\.,? 2008",
      "shortCiteRegEx" : "Ulrich et al\\.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Asking and answering questions to evaluate the factual consistency of summaries",
      "author" : [ "Alex Wang", "Kyunghyun Cho", "Mike Lewis." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "A hierarchical network for abstractive meeting summarization with cross-domain pretraining",
      "author" : [ "Chenguang Zhu", "Ruochen Xu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Msmo: Multimodal summarization with multimodal output",
      "author" : [ "Junnan Zhu", "Haoran Li", "Tianshang Liu", "Yu Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2018 conference on empirical methods in natural language processing, pages",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Recently, there is an increasing research interest in diverse summarization tasks (Gao et al., 2020), e.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : ", timeline (Allan et al., 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : ", 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 39,
      "context" : ", 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al., 2018), meeting (Carletta et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : ", 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc.",
      "startOffset" : 39,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : ", 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc.",
      "startOffset" : 39,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : ", 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc.",
      "startOffset" : 39,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "(2007) collected extractive summaries of 39 email threads from Enron email corpus (Klimt and Yang, 2004) and proposed to",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "(2008) collected both extractive and abstractive summaries of 40 threads from W3C email corpus (Craswell et al., 2006) plus speech acts, meta sentences, etc.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 29,
      "context" : "tive summaries of 2,549 email threads constructed from Avocado Research Email Collection (Oard et al., 2015), which is 64× the size of previously labeled email thread datasets (Carenini et al.",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : ", 2015), which is 64× the size of previously labeled email thread datasets (Carenini et al., 2007; Craswell et al., 2006).",
      "startOffset" : 75,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : ", 2015), which is 64× the size of previously labeled email thread datasets (Carenini et al., 2007; Craswell et al., 2006).",
      "startOffset" : 75,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : ", T5 (Raffel et al., 2020)) is critical due to the small size of our data; taking the email thread as a single document sets up a",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : "01) improves ROUGE (Lin, 2004) scores in some cases.",
      "startOffset" : 19,
      "endOffset" : 30
    }, {
      "referenceID" : 20,
      "context" : "Relatedly, human correlation analysis reveals that automatic metrics (ROUGE (Lin, 2004), BERTScore (Zhang et al.",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 37,
      "context" : "Relatedly, human correlation analysis reveals that automatic metrics (ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019)) are poorly correlated with human judgment, which stresses the",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "We resort to existing email collections: Enron (Klimt and Yang, 2004), W3C (Craswell et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "We resort to existing email collections: Enron (Klimt and Yang, 2004), W3C (Craswell et al., 2006), and Avocado (Oard et al.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "6898 commonly used summarization datasets: CNN/DM (Hermann et al., 2015) and XSum (Narayan et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 27,
      "context" : ", 2015) and XSum (Narayan et al., 2018) are about news summarization; SAMSum (Gliwa et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : ", 2018) are about news summarization; SAMSum (Gliwa et al., 2019) is about chit-chat summarization; CRD3 (Rameshkumar and Bailey, 2020) is a role-play dialogue summarization dataset; BC3 (Ulrich et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : ", 2019) is about chit-chat summarization; CRD3 (Rameshkumar and Bailey, 2020) is a role-play dialogue summarization dataset; BC3 (Ulrich et al.",
      "startOffset" : 47,
      "endOffset" : 77
    }, {
      "referenceID" : 33,
      "context" : ", 2019) is about chit-chat summarization; CRD3 (Rameshkumar and Bailey, 2020) is a role-play dialogue summarization dataset; BC3 (Ulrich et al., 2008) is another email thread summarization with 40 threads from W3C.",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : "“Ext-Oracle-R1” in Table 2 is computed from an oracle summary that maximizes ROUGE-1 (Lin, 2004).",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset (Narayan et al., 2018).",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 30,
      "context" : "T5 (Raffel et al., 2020) is a Transformer (Vaswani et al.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 34,
      "context" : ", 2020) is a Transformer (Vaswani et al., 2017) based seq-to-seq model pretrained with large-scale English data.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 32,
      "context" : "We explore semi-supervised learning via the simple self-training technique (Scudder, 1965).",
      "startOffset" : 75,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "Hierarchical summarization models have been shown to improve the performance of multi-document summarization task (Liu and Lapata, 2019a).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 37,
      "context" : "BERTScore (Zhang et al., 2019) goes beyond n-gram overlap to provide contextualized semantic similarity.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "Specifically, it uses BERT (Devlin et al., 2019) (or RoBERTa (Liu et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "The significance test is following the bootstrap test setup (Efron and Tibshirani, 1994) and sample for 100k times.",
      "startOffset" : 60,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "We check the average inter-rater agreement (Krippendorff’s alpha (Krippendorff, 2011)) of “salience” and “faithfulness” ratings.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "son Correlation (Benesty et al., 2009)) among different human ratings.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "Overall, moderate to large (Cohen, 2013) correlations are observed.",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 35,
      "context" : "This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 196
    }, {
      "referenceID" : 24,
      "context" : "This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al.",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 37,
      "context" : "ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "We explore different summarization paradigms and find that taking the email thread as a single document and finetuning T5 (Raffel et al., 2020) sets up a good baseline.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "We use two email collections in this work: Avocado (Oard et al., 2015) and W3C (Craswell et al.",
      "startOffset" : 51,
      "endOffset" : 70
    } ],
    "year" : 2021,
    "abstractText" : "Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EMAILSUM) dataset, which contains humanannotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender’s intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community.1",
    "creator" : "LaTeX with hyperref"
  }
}