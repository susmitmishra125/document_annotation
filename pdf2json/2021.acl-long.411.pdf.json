{
  "name" : "2021.acl-long.411.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units",
    "authors" : [ "Wei-Ning Hsu", "David Harwath", "Tyler Miller", "Christopher Song", "James Glass" ],
    "emails" : [ "wnhsu@csail.mit.edu,", "harwath@utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5284–5300\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5284"
    }, {
      "heading" : "1 Introduction",
      "text" : "Although there are over 7,000 languages spoken worldwide (Lewis et al., 2016), only several dozen have enough data available to support supervised speech recognition, and many languages do not even employ a writing system (Adda et al., 2016). In contrast, most people learn to use spoken language long before they learn to read and write, suggesting that linguistic annotation is not a prerequisite for speech processing systems. This line of reasoning motivates research that aims to discover meaningful linguistic abstractions (phones, words, etc.) directly from the speech signal, with the intention that they could reduce the reliance of spoken language systems on text transcripts.\nA rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016;\nKamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as how word-like and subword-like linguistic units can be made to emerge within these models (Harwath and Glass, 2017; Harwath et al., 2019; Drexler and Glass, 2017; Alishahi et al., 2017; Harwath et al., 2019; Harwath and Glass, 2019; Havard et al., 2019b; Harwath et al., 2020). So far, these efforts have predominantly focused on inference, where the goal is to learn a mapping from speech waveforms to a semantic embedding space. Generation of speech conditioned on a point in a semantic space has been less explored, and is what we focus on in this work. We hypothesize that generative approaches offer interesting advantages over relying solely on inference. For example, prior works have demonstrated the capability of recognizing visually descriptive words, but have not been shown to learn non-visual words or grammar. Our experiments show that these aspects of spoken language are learned to some degree by a visually-grounded generative model of speech.\nSpecifically, we introduce a model capable of directly generating fluent spoken audio captions of images without the need for natural language text, either as an intermediate representation or a form of supervision during training (Figure 1). Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al., 2017; Taigman et al., 2017; Wang et al., 2017; Shen et al., 2018; Oord et al., 2016). Combining these models provides a means for generating spoken image descrip-\ntions, but existing approaches for training these models are reliant on text during training. Instead, we leverage sub-word speech units discovered using a self-supervised learning objective as a drop-in replacement for the text. We hypothesize that by using such techniques, an even wider variety of traditionally text-based NLP models could be applied to speech data without the need for transcription or automatic speech recognition (ASR) systems. Because all human languages utilize small, discrete phonetic inventories (International Phonetic Association, 1999), we posit that our framework should be applicable for any language in the world. In our experiments, we demonstrate that not just any set of discovered speech units can function in this role. We find the greatest success with units that are discrete, exhibit a low frame-rate, and highly robust to speaker and environmental variability. The main contributions of our paper are as follows:"
    }, {
      "heading" : "1. The first methodology for fluent image-tospeech synthesis that does not rely on text. A",
      "text" : "critical aspect of our approach is factorizing the model into an Image-to-Unit (I2U) module and a Unit-to-Speech (U2S) module, where the speech units are discovered in a self-supervised fashion. This approach enables disentanglement of linguistic variability and acoustic/speaker variability.\n2. Extensive analysis on the properties required for learned units to replace text. While the idea may seem simple and straightforward, obtaining proper units is not a trivial task. In fact, most of the units experimented in this paper fail to serve as drop-in replacements. Moreover, we demonstrate that what are deemed good units vary significantly for inference and generation."
    }, {
      "heading" : "3. Demonstrating insufficiency of beam",
      "text" : "search-based evaluation. We show that even when an I2U model fails to generate sensible caption through beam search decoding, it can still pro-\nduce reasonable captions by sampling from the posterior, hinting that posterior mode-based evaluation can only inspect limited aspects of a model.\n4. Proposing a semantic diversity-aware metric. We identify issues of an existing metric (Vijayakumar et al., 2018) and propose M-SPICE for sampling-based evaluation to address the problems.\n5. Over 600,000 spoken audio captions for the MSCOCO dataset. We collect 742 hours of speech from 2,352 people tasked with reading each caption out loud. This dataset will be made publicly available to support work at the intersection of speech, language, and vision."
    }, {
      "heading" : "2 Related Work",
      "text" : "Image-to-Text and Image-to-Speech Captioning. Significant progress towards generating realistic (text) captions that describe the content of visual images was made with the advent of deep neural networks (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Anderson et al., 2018). Far less work has focused on generating spoken audio captions from natural images. Training an image-to-speech system using separate (image, text) and (text, speech) datasets was explored in (Ma et al., 2019). Hasegawa-Johnson et al. (2017) is the only prior work that has explored image-to-speech synthesis without using text, but with limited results. In that work, BLEU scores were only computed in terms of unsupervised acoustic units, not an estimate of the actual words produced by the synthesizer, which can be problematic as discussed in Section 4. The resulting captions were not evaluated for fluency, naturalness, or intelligibility, and the BLEU scores in terms of the unsupervised units were very low (0.014 on the MSCOCO test set) compared to ours (0.274). Wang et al. (2020b) is a concurrent work that proposes a text-free end-to-end image-to-\nspeech model, which simplifies the task by using pairs of image and synthesized speech generated from a single-speaker TTS model to reduce the acoustic variation. In contrast, by leveraging robust learned units, our I2U module can be trained on real speech with abundant variation, and the U2S module serves as a vocoder that requires a small amount of clean speech (transcripts not needed). Hence, our system imposes less data constraints yet still outperforms Wang et al. (2020b).\nVoice Conversion without Text aims to convert the speaker identity in a recording while preserving the textual content (Abe et al., 1990; Stylianou et al., 1998; Toda et al., 2007). It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to our own is the ZeroSpeech 2019 challenge (Dunbar et al., 2019; Tjandra et al., 2019; Cho et al., 2019), which addresses unsupervised learning of discrete speech units that can replace text and be used as input to TTS models. Unlike image-to-speech synthesis, these tasks only infer phonetic units from given audio recordings instead of generating ones.\nSpeech Pre-Training and Its Applications. Interest in this area has recently surged. Various learning objectives have been proposed, including autoencoding with structured latent spaces (van den Oord et al., 2017; Eloff et al., 2019; Chorowski et al., 2019; Hsu et al., 2017b; Hsu and Glass, 2018b; Khurana et al., 2019), predictive coding (Chung et al., 2019; Wang et al., 2020a), contrastive learning (Oord et al., 2018; Schneider et al., 2019), and more. Prior work addresses inferring linguistic content such as phones from the learned representations (Baevski et al., 2020; Kharitonov et al., 2020; Hsu et al., 2021). In contrast, this work focuses on generating the learned representation from a different modality, which evaluates representations from a different perspective."
    }, {
      "heading" : "3 Method",
      "text" : ""
    }, {
      "heading" : "3.1 Framework Overview",
      "text" : "A depiction of our modeling approach is shown in Figure 2. Caption generation for an image involves a cascade of two components: given an input image I , we first generate a linguistic unit sequence U according to the I2U module P (U | I). Given the linguistic symbol sequence U , we generate a speech waveform S according to the U2S module\nP (S | U). If the linguistic unit sequence U were to take the form of natural language text, the model would be equivalent to the cascade of a conventional image captioning system followed by a TTS module. Note that we assume S ⊥ I | U because prosody variation is not dependent on the image for the datasets considered.\nThe key idea in this paper is to instead define U to be a sequence of learned speech units that are as robust and compact as possible like text, but discovered without text supervision. We define inference with this S2U model as U = f(S), enabling us to “transcribe” any given speech audio waveform S into a sequence of units U . The addition of this third component enables us to train P (U | I) from a dataset of images paired with spoken captions {(I1, S1), . . . , (IN , SN )}. The conditional independence assumption between S and I given the U enables us to choose any arbitrary speech dataset for training P (S | U), therefore enabling the speaker characteristics and other acoustic properties to be independently controllable from the I2U system (Wang et al., 2018; Hsu et al., 2019; Henter et al., 2018; Akuzawa et al., 2018)."
    }, {
      "heading" : "3.2 Datasets",
      "text" : "Table 1 summarizes the five datasets used for training S2U, I2U, and U2S models. Note that we deliberately choose different datasets for training each module, which aims to examine the robustness of the units when transferring across domains, including shift in speaker demography, speaking style (scripted/spontaneous), and linguistic content (book/newspaper/image description). Among the three datasets with image and speech pairs: Places, Flickr8k, MSCOCO, we chose the latter two for training I2U models, because they include five captions per image, which is more suitable for caption metrics such as SPICE (Anderson et al., 2016); moreover, they are commonly used image captioning datasets with many text-based baselines in the literature. Places only contains one spoken caption per image and has not been used for captioning.\nSpecifically, as part of this work we collect SpokenCOCO, a spoken version of the MSCOCO captioning dataset (Lin et al., 2014) with 742 hours from 2532 speakers, via Amazon Mechanical Turk by displaying the text to a person and having them read it aloud. Additional details regarding the dataset can be found in appendix Section A. Note that although there exists a speech version\nof MSCOCO named Speech-COCO (Havard et al., 2017), it is comprised of only synthesized speech using a concatenative TTS model in eight speakers’ voice. Disfluencies (e.g. “uh”) are randomly inserted in between words to imitate real speech. Compared to SpokenCOCO, Speech-COCO offers limited diversity and naturalness."
    }, {
      "heading" : "3.3 Learning Robust Linguistic Units from Visually-Grounded Speech",
      "text" : "We propose to build the S2U model upon ResDAVEnet-VQ, an audio-visual grounding model introduced in Harwath et al. (2020) that has shown to learn discrete phone- and wordlike units in the intermediate vector quantizing (VQ) layers. This model is trained to associate speech with contextually relevant visual inputs using a triplet loss (Weinberger and Saul, 2009), which can be interpreted as maximizing a mutual information lower bound between image and speech (Tschannen et al., 2020). Since visual semantics are described with words, which in turn are composed of phones, the representations learned by ResDAVEnet-VQ are forced to be predictive of words and phones rather than speaker, noise, etc.\nIn contrast, many of the speech representations are trained by reconstructing (Chorowski et al., 2019; Hsu et al., 2017b) or predicting unseen speech signals (Chung et al., 2019), which would inevitable capture factors unrelated to the linguistic\ncontent. To demonstrate the advantage of representation learning with grounding, we will compare ResDAVEnet-VQ with a reconstruction based model, WaveNet-VQ, trained on the PlacesAudio dataset. We denote the units extracted from this model with WVQ. We use the implementation of Harwath et al. (2020) for ResDAVEnet-VQ, and Cho et al. (2019) for WaveNet-VQ which achieves the best ZeroSpeech 2019 challenge performance."
    }, {
      "heading" : "3.4 Unit Selection and Run Length Encoding",
      "text" : "Although the ResDAVEnet-VQ model has been shown to be capable of learning both phone-like and word-like units, the experiments in (Harwath et al., 2020) show that only several hundred words are explicitly learned, which tend to be “visual words.” Conversely, the phone-like units learned by the lower VQ layers of the model were shown to cover all of the phones in American English (as there are only several dozens). For this reason, we choose to use phone-like units learned by the lower VQ layers to represent U .\nNominally, the VQ layers will output one-hot vectors at a uniform temporal rate, downsampled with respect to the framerate of the acoustic input depending upon which VQ layer is used. Given an input computed with a 10ms frame shift, the two VQ layers investigated in this paper (VQ2 and VQ3) respectively output vectors every 20ms and 40ms. In general, the VQ units are repeated\nfor several consecutive frames. We can decrease the average length of the symbol sequence U by employing a lossy form of run-length encoding (RLE) (see Figure 2) which retains the sequence of symbol identities but discards duration information. Each unit then represents a variable-length segment. This removes the burden of unit duration modeling from the I2U model and shifts it onto the U2S model, which we will show to be crucial."
    }, {
      "heading" : "3.5 Image-to-Unit and Unit-to-Speech",
      "text" : "Both the I2U model and the U2S model are based upon recurrent seq2seq with attention networks (Bahdanau et al., 2015). Specifically, we adopt Show-Attend-and-Tell (SAT) (Xu et al., 2015) for the I2U model. It has an image encoder pre-trained for classification, which is language agnostic and hence should work in any language within our proposed framework. The decoder on the other hand is randomly initialized. We train the SAT model for two stages, where the encoder parameters are only updated in the second stage. We distinguish the models from the two stages with SAT and SAT-FT (finetuned) respectively when presenting the results. For the U2S model, we adopt Tacotron2 (Shen et al., 2018) and WaveGlow (Prenger et al., 2019) for unit-to-spectrogram and spectrogram-to-waveform generation, respectively. In particular, a pre-trained WaveGlow is used without fine-tuning.\nThe I2U model is trained on (I, f(S)) pairs, which requires pairs of image and speech, while the U2S model is trained on (f(S), S) pairs, which can be obtained from arbitrary set of speech. Both models are trained with the maximum likelihood objective (EI,U [logP (U | I)] for I2U and ES,U [logP (S | U)] for U2S)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We design experiments to address three questions: First, how can we measure the performance of an image-to-speech system? Our system can fail to produce a good caption if the I2U model fails to encode linguistic/semantic information into the unit sequence, or if the U2S model fails to synthesize an intelligible waveform given a unit sequence. To better localize these failure modes, we evaluate the full I2S system as well as the U2S system in isolation. We evaluate the U2S system by using it as a vocoder to synthesize unit sequences inferred from real speech and soliciting human judgements\nin the form of Mean Opinion Score (MOS) and Side-By-Side (SXS) preference tests (Table 2).\nTo evaluate the I2S system, we can use any method that measures the semantic information contained in the generated speech. We consider two sets of end-to-end metrics: word-based and retrieval-based, and one set of proxy unit-based metrics. Word-based metrics transcribe a generated spoken caption into text (manually or with an ASR system) and then measure word-based captioning metrics against a set of reference captions, such as BLEU-4 (Papineni et al., 2002) (adjusted ngram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al., 2015) (TF-IDF weighted n-gram cosine similarity), and SPICE (Anderson et al., 2016) (F-score of semantic propositions in scene graphs). This enables comparison between image-to-speech systems with a text “upperbound”, but is not applicable to unwritten languages.\nRetrieval-based metrics include image-to-speech and speech-to-image retrieval (Harwath et al., 2020), which require a separately trained crossmodal retrieval model for evaluation. Such metrics are text-free, but they cannot measure other aspects of language generation such as syntactic correctness (partially captured by BLEU-4) and scope of the learned vocabulary. Lastly, unit-based metrics are similar to text-based, but replace words with units when computing n-gram statistics. However, systems built on different units are not directly comparable, and second, can be inflated if duration is modeled using unit repetition.\nSecond, what properties must learned units have to be a drop-in replacement for text? The most essential differences between text and speech are the amount of information encoded and the sequence lengths. Beyond text, speech also encodes prosody, speaker, environment information and the duration for each phone, all of which are minimally correlated with the conditioned images. We hypothesize that learned speech units should discard such information in order to seamlessly connect the I2U and U2S modules. To verify it, we pay particular attention to the variations of the learned units in frame rate (VQ2/VQ3), encoding of duration information (RLE or not), and robustness to domain shift (WVQ/VQ3). Units are run-length encoded by default. Table 2a shows the properties of the units before run-length encoding.\nThird, how should language generation models be evaluated more generally? We examine evaluation of the I2S model using beam searchbased decoding as well as sampling-based decoding. We find that because evaluation metrics that are reliant on beam search-based decoding only evaluate the mode of a model’s posterior, they do not reflect the ability of a model to generate diverse linguistic content. Furthermore, we show that it is possible for a model’s posterior mode to be linguistically meaningless, and yet meaningful language can still be generated with sampling-based decoding. Towards this end, we introduce a novel multihypothesis evaluation metric (M-SPICE), which uses sampling-based decoding (instead of beam search) to generate a set of captions. We can then compute the overall coverage of this caption set against a reference; see Section 4.4 for details."
    }, {
      "heading" : "4.1 Evaluating the U2S Model",
      "text" : "We construct a Tacotron-2 model for each of the three unit types on the LJSpeech audio data by transcribing each LJSpeech utterances into an unit sequence, then train the U2S model from the RLEed unit sequence and spectrogram pairs. We evaluate the naturalness of the speech produced by each model on held-out data, both in-domain using LJSpeech and out-of-domain (OOD) using SpokenCOCO.1 Amazon Mechanical Turk (AMT) workers performed Side-by-Side preference tests (SXS) and naturalness evaluation based on mean opinion scores (MOS) on a scale from 1 to 5 for each U2S model, which we display in Table 2. Although VQ2 was preferred for in-domain synthesis on LJSpeech, VQ3 achieved the highest scores and least degradation (-0.387) on the out-of-domain SpokenCOCO, indicating that out of the three units VQ3 has the strongest robustness to domain shift."
    }, {
      "heading" : "4.2 Incorporating the I2U Model",
      "text" : "We trained an SAT model on SpokenCOCO for each of the three RLE-ed units, as well as VQ3 units without RLE. We also compare to text characters and words; the full hyperparameter and training details for all models are provided in Section B in the appendix, but in general we kept these as constant as possible when comparing different linguistic representations.\nBefore connecting the U2S model, we noticed that all RLE speech unit models except the one\n1In-domainess is defined with respect to the U2S training data (LJSpeech) not the S2U training data (PlacesAudio).\nUnit ABX Frame MOS Error Rate LJSpeech SpokenCOCO\nVQ3 14.52% 40ms 3.723 ± 0.039 3.336 ± 0.044 VQ2 12.51% 20ms 3.932 ± 0.036 2.961 ± 0.045 WVQ 24.87% 40ms 3.658 ± 0.040 2.896 ± 0.053\n(a) Properties of the units and MOS of the U2S models trained on these units with 95% confidence interval. ABX errors are computed on the ZeroSpeech 2020 English test set.\ntrained on VQ3 units failed during beam search decoding on the test images (WVQ consistently failed, while VQ2 sometimes succeeded); rather than producing a diverse sequence of output units, the decoder would generally get stuck in a loop until the maximum decoding length was reached. This also happened using VQ3 units without RLE, indicating that the decoder could not model unit duration. Example outputs are provided in Table 3. We hypothesize that the reason the VQ2 and WVQ units failed is due to their lack of invariance to domain shift, as evidenced by their decay in naturalness when used for OOD synthesis as shown in Table 2. This may cause the entropy of the unit distribution conditioned on an image to be higher as each phoneme may be represented by multiple units, and therefore the I2U model suffers from the same looping issues as the unconditional language model of text, as observed in (Holtzman et al., 2018; Fan et al., 2018; Holtzman et al., 2020;\nKulikov et al., 2019; Welleck et al., 2020). To evaluate the full Image-to-Speech model, we first train an ASR system on the re-synthesized SpokenCOCO captions using the VQ3 Tacotron-2 model. This enables us to estimate a word-level transcription of the spoken captions produced by our system. In order to verify that the synthesized captions are intelligible to humans and the ASR system did not simply learn to recognize artifacts of the synthesized speech, we asked AMT workers to transcribe into words a set of 500 captions generated by our I2U→U2S system and also evaluated their naturalness. Three workers transcribed and three workers rated each caption, allowing us to compute an MOS score (3.615±0.038), a word error rate (WER) between the 3 human transcriptions (9.40%), as well as an average WER between the human and ASR-produced transcriptions (13.97%). This confirms that our system produces reasonably natural speech and ASR is sufficiently accurate for transcribing synthesized speech.\nTable 4 summarizes our results on MSCOCO and Flickr8k using beam search. We compare with the literature for bottom-up text captioning (row 1-2) and text-free end-to-end image-to-speech synthesis (row 3). We train the decoder of an SAT model while keeping the image encoder fixed (row\n4-6), in addition to fine-tuning the encoder (row 7-9). Despite having no access to text, the SATFT speech captioning model trained on VQ3 units achieves a BLEU-4 score of .233 with beam search decoding on MSCOCO. This is very close to the .243 achieved by the original SAT word-based captioning model. Figure 1 shows that the generated captions are fluent and reflect the implicit learning of some syntactic rules. It is evident that the proposed model is capable of generating fluent and meaningful image captions.\nResults comparing four unit representations on all three sets of metrics are shown in Table 5. First of all, by comparing word-based and unit-based evaluations, we do note that the relative ranking among VQ3, VQ2, and WVQ is consistent across BLEU-4, METEOR, and ROUGE for SAT models, however, VQ3 \\ RLE achieves abnormally high scores on these metrics despite producing trivial captions for all images as shown in Table 3. This is because unit “32” has learned to represent nonspeech frames such as silence, which frequently occurs at both the beginning and end of utterances. Without RLE, consecutive strings of “32” units are extremely common in both the candidate and reference captions, which inflates the scores of this model. The exception here is the CIDEr metric,\nwhich incorporates TF-IDF weighting that tends to de-emphasize these kinds of uninformative patterns. Nonetheless, when comparing SAT and SAT-FT with VQ3 units, CIDEr does not rank them the same as word-based metrics.\nRegarding retrieval-based evaluation, despite the fact that the ResDAVEnet model was only trained on the original, human-spoken captions for the MSCOCO images, it works very well for the fully synthetic captions. The speech and image retrieval scores for 1k human-spoken validation captions are 0.867 and 0.828 R@10, respectively, while the SAT-FT VQ3 model achieves 0.766 and 0.765 R@10. This indicates that this image-to-speech model is able to infer the salient semantic content of an input image, generate a unit sequence that captures that content, and generate speech that is sufficiently natural sounding for the ResDAVEnet model to recover that semantic information. Several of the other image-to-speech models also achieve respectable retrieval performance, and the overall ranking of the models mirrors that which we found when using word-based evaluation metrics."
    }, {
      "heading" : "4.3 From Mode to Distribution: Evaluating Captions Generated via Sampling",
      "text" : "The results in the previous section only evaluate beam search decoding with the I2U model, and do not fully reveal the posterior over captions for an input image, or whether the unit representations that failed with beam search would work well with other methods. To probe this, we evaluate the models using sampling-based caption generation. Figure 3 shows the SPICE scores on SpokenCOCO using beam search and two sampling-based methods. VQ3 still performs the best of all unit types with both beam search and sampled decoding. VQ2 can sometimes generate captions with beam search when the beam is kept small, but as the beam grows it begins to loop and the scores become very low.\nWe see that all unit types can generate reasonable captions when decoding via sampling. Moreover, we discovered that 1) ResDAVEnet-VQ units consistently outperform the WaveNet-VQ units, suggesting that they better capture sub-word structure, and 2) VQ3 \\ RLE achieves better scores than VQ2 when using a larger temperature or k for top-k.\nWe estimated the vocabulary size of the SAT-FT model with VQ3 by counting the number of unique recognized words produced at least 3 times when captioning the SpokenCOCO test images. These numbers are shown for the model under the various decoding methods in Figure 4. The number of captions per image is denoted by n, where top candidates are used for beam search and i.i.d. samples are drawn for sampling. Sampling-based decoding reveals a larger vocabulary size than beam search, and the number of words learned by our models (≥ 212) is far greater than the number of words learned by the ResDAVEnet-VQ model (approx."
    }, {
      "heading" : "U2S trained on LJSpeech",
      "text" : "279) in (Harwath et al., 2020). We hypothesize that training a model to generate spoken captions encourages it to learn many more words than only being trained to retrieve images from captions. We also hypothesize that because beam search attempts to find the mode of the posterior over captions, it tends to produce a smaller set of words and does not reveal the breadth of the model distribution."
    }, {
      "heading" : "4.4 New Diversity-Aware Metric: M-SPICE",
      "text" : "The previous section showed that even when the SPICE scores were comparable, sampling-based decoding revealed a much larger model vocabulary than beam search, especially when multiple captions are generated for each image. This highlights a limitation of SPICE in measuring the diversity. Formally speaking, SPICE computes an F-score between two bags of semantic propositions T (S) and T (c) parsed from a set of references S = {si}i and a hypothesis c, where T (c) denotes a bag of propositions extracted from a scene graph parsed c, and we can compute that for multiple sentences with T (S) = ∪i(T (si)).\nTo extend SPICE for scoring multiple hypotheses C = {cj}Jj=1, one can compute an average SPICE: 1J ∑ j F1(T (S), T (cj)), or use the ora-\ncle SPICE proposed in Vijayakumar et al. (2018): maxjF1(T (S), T (cj)). However, these metrics fail to capture the diversity among hypotheses. Consider two hypothesis set, C1 = {c11, c12} and C2 = {c21, c22}, where T (c11) = T (c12) = T (c21) = {(girl), (table), (girl, sit-at, table)}, T (c22) = {(girl), (girl, young)}, and T (S) = {(girl), (table), (girl, young), (girl, sit-at, table)}.\nTo address the deficiencies of the existing metrics, we propose a new metric named multicandidate SPICE (M-SPICE), which takes the union of the candidate propositions and computes\nthe F-score against the reference propositions: F1(T (S),∪jT (cj)). M-SPICE assigns a higher score if the set captures diverse and correct propositions, and it is obvious that the score ofC2 is higher than C1 as desired.Figure 5 shows the M-SPICE scores of our SAT-FT model using VQ3 units on SpokenCOCO. When evaluating over multiple captions (n > 1), using the beam search hypotheses increases the score less than sampling."
    }, {
      "heading" : "4.5 Disentangled Voice Control for Image-to-Speech Synthesis",
      "text" : "We examine to what extent the VQ3 units are portable across different speakers by training a U2S model on the VCTK dataset that additionally takes a speaker ID as input. The resulting model is able to generate speech with the voice of any VCTK speaker. We evaluate the captions produced by this system on SpokenCOCO for 5 speakers in Table 6. To compute these scores we transcribe the captions generated by each model into text using the ASR system we describe in Section 4.2, which was solely trained on re-synthesized SpokenCOCO captions using the LJSpeech U2S model. The scores in Table 6 indicate not only that the I2U model can be easily integrated with U2S models representing a diverse set of speakers, but also that the LJSpeech ASR system works very well on the speech synthesized from the VCTK models."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we presented the first model capable of generating fluent spoken captions of images without relying on text, which almost matches the performance of early text-based image captioning models. Our comprehensive experiments demonstrated that learned units need to be robust, of low framerate, and encoding little or none duration information to be a drop-in replacement for text. We also identified the caveats of mode-based evaluation and proposed a new metric to address semantic diversity. As part of this work, a novel dataset of over 600k spoken captions for the MSCOCO dataset is introduced, which we will make publicly available to the research community.\nFuture work should investigate applying the proposed method to additional languages, devising improved speech unit representations, and jointly training the speech unit model with the I2S model. This would offer the opportunity to explore new analysis-by-synthesis training objectives."
    }, {
      "heading" : "B Detailed Experimental Setups",
      "text" : "In this section, we provide details about data preprocessing, model architecture, and training hyperparameters for each module used in this paper. The same setups are used for all unit types unless otherwise stated.\nB.1 Image-to-Unit Model\nData Images are reshaped to 256×256×3 matrices and are per-channel normalized with µ = [0.485, 0.456, 0.406] and σ =[0.229, 0.224, 0.225]. During training, unit sequences are truncated or padded to the target length shown in Table A2. The target lengths are determined such that there are less than 10% sequences truncated while still allowing a reasonable batch size to be used. Units that occurred less than five times are excluded. Sequences are not truncated during evaluation. We follow the data splits used in (Harwath et al., 2020) for Places, and (Karpathy and Fei-Fei, 2015) for Flickr8k and SpokenCOCO (the “Karpathy split”).\nWord Char VQ3 VQ2 WVQ VQ3 \\ RLE\nTarget Length 18 70 100 200 110 160 Sequence Truncated (%) 1.12 1.74 6.90 9.37 7.80 6.35 Batch Size (SAT) 80 60 40 40 40 40 Batch Size (SAT-FT) 32 32 20 - - -\nTable A2: Configuration for each type of units used in the Image-to-Unit model.\nModel We adopt an open-source reimplementation2 of Show, Attend, and Tell (Xu et al., 2015) (SAT) with soft attention, which replaces the original CNN encoder with a ResNet-101 pre-trained on ImageNet for image classification. The last two layers of the ResNet are removed (a pooling and a fully-connected layer) such that the encoder produces a 14×14×2048 feature map for each image.\nTraining Adam (Kingma and Ba, 2015) with a learning rate of 10−4 is used for optimizing both stages (SAT and SAT-FT). The training objective is maximum likelihood combined with a doubly stochastic attention regularization introduced in (Xu et al., 2015) with a weight of 1. Dropout is applied to the input of decoder softmax layer with a probability of 0.5 during training. Gradients are clipped at 5 for each dimension. The first stage is trained for at most 30 epochs, and the best checkpoint from which is used to initialize the second\n2Link to the SAT implementation on Github\nFigure A1: Utterance duration histograms for the three visually-grounded speech datasets.\nFigure A2: M-SPICE F-score (same as Figure 5) and recall on the SpokenCOCO test set with different candidate proposal methods.\nstage trained for at most another 20 epochs. Models are selected based on the unit BLEU-4 score on the validation set. Using two NVIDIA TITAN X Pascal GPUs with data parallel training, each epoch takes about 2.8 hours for VQ3 units and 5.3 hours for VQ2 units.\nB.2 Unit-to-Speech Model Data RLE-ed unit sequences are used as input for all systems (VQ3 and VQ3 \\ RLE systems share the same U2S model). The native audio sample rates in LJSpeech and VCTK are 22050Hz and 48kHz, respectively. For consistency and compatibility with the spectrogram-to-waveform model, we down-sample those in VCTK to 22050Hz. Following Tacotron2, we compute a 80 dimensional Mel spectrogram for each audio file with a 256-sample (11.6ms) frame hop, a 1024-sample (46.4ms) frame size, and a Hann window function. Utterances longer than 8 seconds are discarded during training to accommodate for the GPU memory constraints. We follow the data splits provided at https://github.com/NVIDIA/tacotron2 for\nLJSpeech. For the multi-speaker VCTK dataset, we randomly sample 2.5% of the utterances from each speaker for validation.\nModel We use an re-implementation3 of Tacotron2 (Shen et al., 2018) for U2S models. For single-speaker models trained on LJSpeech, the exact same hyperparameters and model architecture are used as (Shen et al., 2018). For multi-speaker models trained on VCTK, we create an additional speaker embedding table of 256 dimensions for all speakers and control the speaker identity through these speaker embeddings. Speaker embeddings are injected at two places in the decoder: first in concatenation with the original input to the decoder LSTM, and second in concatenation with the output of the decoder LSTM, right before predicting the stop token and the spectra of a frame. A pre-trained4 WaveGlow (Prenger et al., 2019) vocoder is used for all U2S models, which demonstrates the universality of vocoder models\n3https://github.com/NVIDIA/tacotron2 4https://github.com/NVIDIA/waveglow\nMetric symbol Sampling with Temperature Top-K Sampling (t = 1.0) Top-K Sampling (t = 0.7) t = 1.0 t = 0.7 t = 0.4 t = 0.1 k = 10 k = 5 k = 3 k = 10 k = 5 k = 3\nBLEU-4\nVQ3 0.052 0.097 0.132 0.137 0.084 0.108 0.120 0.109 0.119 0.124 VQ2 0.039 0.058 0.068 0.066 0.059 0.068 0.069 0.064 0.070 0.071 WVQ 0.033 0.047 0.025 0.012 0.056 0.050 0.037 0.052 0.042 0.025 VQ3 \\ RLE 0.049 0.075 0.035 0.000 0.070 0.087 0.092 0.082 0.094 0.093\nMETEOR\nVQ3 0.124 0.151 0.168 0.165 0.147 0.160 0.166 0.159 0.165 0.168 VQ2 0.115 0.134 0.146 0.140 0.134 0.142 0.147 0.140 0.144 0.147 WVQ 0.096 0.106 0.078 0.069 0.112 0.104 0.088 0.105 0.094 0.080 VQ3 \\ RLE 0.119 0.135 0.055 0.002 0.136 0.146 0.148 0.141 0.144 0.141\nROUGE-L\nVQ3 0.303 0.358 0.403 0.416 0.346 0.371 0.386 0.373 0.386 0.397 VQ2 0.293 0.330 0.351 0.345 0.325 0.345 0.351 0.340 0.348 0.355 WVQ 0.270 0.297 0.287 0.287 0.312 0.309 0.292 0.309 0.295 0.276 VQ3 \\ RLE 0.295 0.330 0.152 0.001 0.328 0.349 0.355 0.340 0.348 0.350\nCIDEr\nVQ3 0.195 0.345 0.461 0.451 0.312 0.383 0.424 0.395 0.431 0.444 VQ2 0.143 0.231 0.272 0.267 0.220 0.260 0.277 0.251 0.270 0.278 WVQ 0.095 0.150 0.044 0.009 0.180 0.145 0.082 0.154 0.116 0.055 VQ3 \\ RLE 0.182 0.277 0.130 0.000 0.260 0.316 0.340 0.304 0.328 0.332\nSPICE\nVQ3 0.063 0.093 0.111 0.114 0.086 0.100 0.108 0.100 0.106 0.109 VQ2 0.052 0.074 0.086 0.087 0.073 0.082 0.085 0.079 0.084 0.087 WVQ 0.035 0.046 0.019 0.011 0.051 0.042 0.026 0.043 0.034 0.020 VQ3 \\ RLE 0.060 0.078 0.034 0.001 0.077 0.087 0.091 0.083 0.088 0.086\nTable A3: Results of SAT models trained on MSCOCO and decoded with various sampling methods.\nn Beam Search Sampling (t: temperature; k: top-k) beam size=? (t, k) = (?, All) (t, k) = (1.0, ?) (t, k) = (0.7, ?)\n1 3 5 8 10 1.0 0.7 0.4 0.1 10 5 3 10 5 3"
    }, {
      "heading" : "1 551 479 447 421 411 1447 978 689 561 1058 908 770 694 663 670",
      "text" : ""
    }, {
      "heading" : "2 - 572 523 502 474 2100 1367 917 696 1522 1289 1025 907 867 851",
      "text" : ""
    }, {
      "heading" : "3 - 693 620 585 562 2550 1644 1075 803 1855 1515 1222 1069 1003 973",
      "text" : ""
    }, {
      "heading" : "5 - - 681 625 617 3239 2111 1305 938 2367 1861 1511 1266 1209 1155",
      "text" : "10 - - - - 700 4311 2876 1664 1155 3176 2512 1954 1618 1552 1437\nTable A4: The vocabulary size of the VQ3 SAT-FT model as estimated by various decoding approaches. The numbers in this table display the specific values of the curves depicted in Figure 4.\nand how little acoustic properties of interest are affected by them.\nTraining A batch size of 64 are used for all systems. Adam (Kingma and Ba, 2015) with an initial learning rate of 10−3 is used to minimize the mean square error from spectrogram prediction and the binary cross entropy from stop token prediction combined. L2 regularization for the parameters with a weight of 10−6 is applied, and the L2 norm of the gradients are clipped at 1. Models are trained for 500 epochs on LJSpeech and 250 epochs on VCTK, and selected based on the validation loss. Empirically, each training epoch on LJSpeech takes about 12 minutes using two NVIDIA Titan X Pascal GPUs for both VQ2 and VQ3 models."
    }, {
      "heading" : "C Full Results of Decoding via Sampling",
      "text" : "Table A3 presents the word-based evaluation results of decoding via sampling for all 5 metrics, supplementing Figure 3 in the main paper that only presents the SPICE results. We see that ranking between symbols are generally consistent among\nall those metrics, except the ranking between WVQ and VQ3 \\ RLE when sampling with a temperature of 0.4. This is a relatively low-score regime when both model are transiting from generating trivial caption (t = 0.1) to non-trivial captions (t = 0.7)."
    }, {
      "heading" : "D Full Results of Learned Vocabulary Size",
      "text" : "In Table A4, we display the numerical results depicted graphically in Figure 4."
    }, {
      "heading" : "E More Image-to-Speech Samples",
      "text" : "Table A5 shows captions sampled from the VQ3 model trained on MSCOCO. Here, we note that the sampled captions exhibit diversity both their content and linguistic style. We observe that the captioning model has learned to produce captions that correctly use quantifiers and conjugate verbs (“a couple of cows walking” vs. “a cow is standing”). The model also disentangles object identity from attributes such as color “red fire hydrant” vs. “yellow fire hydrant” vs. “green fire hydrant”).\nImage Generated Spoken Captions / Transcripts (SAT-FT, VQ3, Sampling (t, k) = (0.4, 3))trial 1 trial 2 trial 3\nthe airplane is parked on the field\na plane is parked in the grass near a white and\nwhite airplane\na small airplane that is standing in a field\na surfer riding a wave in the water\nthe man is riding the wave in the water\na surfer is riding a wave on a wave\nthe bus parked on the side of the road a large red bus is stopped in the road a bus is parked on the road\na couple of cows walking in a field\na couple of cows in a grassy field\na couple of cows walking in a grassy field\na cow is standing in a store\na brown cow walking down the side of a street\na brown and white cow standing in a line\na red fire hydrant is sitting on the side of the street\na red fire hydrant sitting on a sidewalk in a\nconcrete\na red fire hydrant sitting on the side of a road\na yellow fire hydrant in the middle of the side of a\nroad\na yellow fire hydrant is sitting in the park\na yellow fire hydrant in a line on the side of a street\na fire hydrant on a sidewalk in the middle\na green fire hydrant on the side of the road a fire hydrant with a curb on the side of the street\nTable A5: Samples. More at https://wnhsu.github.io/image-to-speech-demo/2_vq3_sample_ diversity_sat-ft_model"
    } ],
    "references" : [ {
      "title" : "Voice conversion through vector quantization",
      "author" : [ "Masanobu Abe", "Satoshi Nakamura", "Kiyohiro Shikano", "Hisao Kuwabara." ],
      "venue" : "Journal of the Acoustical Society of Japan, 11(2):71–76.",
      "citeRegEx" : "Abe et al\\.,? 1990",
      "shortCiteRegEx" : "Abe et al\\.",
      "year" : 1990
    }, {
      "title" : "Breaking the unwritten language barrier: The BULB",
      "author" : [ "Gilles Adda", "Sebastian Stüker", "Martine Adda-Decker", "Odette Ambouroue", "Laurent Besacier", "David Blachon", "Hélene Bonneau-Maynard", "Pierre Godard", "Fatima Hamlaoui", "Dmitry Idiatov" ],
      "venue" : null,
      "citeRegEx" : "Adda et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Adda et al\\.",
      "year" : 2016
    }, {
      "title" : "Expressive speech synthesis via modeling expressions with variational autoencoder",
      "author" : [ "Kei Akuzawa", "Yusuke Iwasawa", "Yutaka Matsuo." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Akuzawa et al\\.,? 2018",
      "shortCiteRegEx" : "Akuzawa et al\\.",
      "year" : 2018
    }, {
      "title" : "Encoding of phonology in a recurrent neural model of grounded speech",
      "author" : [ "Afra Alishahi", "Marie Barking", "Grzegorz Chrupała." ],
      "venue" : "Proc. ACL Conference on Natural Language Learning (CoNLL).",
      "citeRegEx" : "Alishahi et al\\.,? 2017",
      "shortCiteRegEx" : "Alishahi et al\\.",
      "year" : 2017
    }, {
      "title" : "SPICE: Semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "Proc. IEEE European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pat-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "author" : [ "Alexei Baevski", "Steffen Schneider", "Michael Auli." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Baevski et al\\.,? 2020",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "VQVAE with speaker adversarial training",
      "author" : [ "Suhee Cho", "Yeonjung Hong", "Yookyunk Shin", "Youngsun Cho" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised speech representation learning using wavenet autoencoders",
      "author" : [ "Jan Chorowski", "Ron J. Weiss", "Samy Bengio", "Aäron van den Oord." ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing.",
      "citeRegEx" : "Chorowski et al\\.,? 2019",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2019
    }, {
      "title" : "On using backpropagation for speech texture generation and voice conversion",
      "author" : [ "Jan Chorowski", "Ron J Weiss", "Rif A Saurous", "Samy Bengio." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Chorowski et al\\.,? 2018",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-target voice conversion",
      "author" : [ "Ju-chieh Chou", "Cheng-chieh Yeh", "Hung-yi Lee", "Lin-shan Lee" ],
      "venue" : null,
      "citeRegEx" : "Chou et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chou et al\\.",
      "year" : 2018
    }, {
      "title" : "Representations of language in a model of visually grounded speech signal",
      "author" : [ "Grzegorz Chrupała", "Lieke Gelderloos", "Afra Alishahi." ],
      "venue" : "Proc. Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chrupała et al\\.,? 2017",
      "shortCiteRegEx" : "Chrupała et al\\.",
      "year" : 2017
    }, {
      "title" : "An unsupervised autoregressive model for speech representation learning",
      "author" : [ "Yu-An Chung", "Wei-Ning Hsu", "Hao Tang", "James R. Glass." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Chung et al\\.,? 2019",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive learning for image captioning",
      "author" : [ "Bo Dai", "Dahua Lin." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Dai and Lin.,? 2017",
      "shortCiteRegEx" : "Dai and Lin.",
      "year" : 2017
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "In Proceedings of the Ninth Workshop on Statistical Machine Translation.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Analysis of audio-visual features for unsupervised speech recognition",
      "author" : [ "Jennifer Drexler", "James Glass." ],
      "venue" : "Proc. Grounded Language Understanding Workshop.",
      "citeRegEx" : "Drexler and Glass.,? 2017",
      "shortCiteRegEx" : "Drexler and Glass.",
      "year" : 2017
    }, {
      "title" : "The zero resource speech",
      "author" : [ "Ewan Dunbar", "Robin Algayres", "Julien Karadayi", "Mathieu Bernard", "Juan Benjumea", "Xuan-Nga Cao", "Lucie Miskic", "Charlotte Dugrain", "Lucas Ondel", "Alan W. Black", "Laurent Besacier", "Sakriani Sakti", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Dunbar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dunbar et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal one-shot learning of speech and images",
      "author" : [ "Ryan Eloff", "Herman Engelbrecht", "Herman Kamper." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Eloff et al\\.,? 2019",
      "shortCiteRegEx" : "Eloff et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proc. Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "High-quality nonparallel voice conversion based on cycle-consistent adversarial network",
      "author" : [ "Fuming Fang", "Junichi Yamagishi", "Isao Echizen", "Jaime Lorenzo-Trueba." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Fang et al\\.,? 2018",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep multimodal semantic embeddings for speech and images",
      "author" : [ "David Harwath", "James Glass." ],
      "venue" : "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).",
      "citeRegEx" : "Harwath and Glass.,? 2015",
      "shortCiteRegEx" : "Harwath and Glass.",
      "year" : 2015
    }, {
      "title" : "Learning wordlike units from joint audio-visual analysis",
      "author" : [ "David Harwath", "James Glass." ],
      "venue" : "Proc. Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Harwath and Glass.,? 2017",
      "shortCiteRegEx" : "Harwath and Glass.",
      "year" : 2017
    }, {
      "title" : "Towards visually grounded sub-word speech unit discovery",
      "author" : [ "David Harwath", "James Glass." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Harwath and Glass.,? 2019",
      "shortCiteRegEx" : "Harwath and Glass.",
      "year" : 2019
    }, {
      "title" : "Learning hierarchical discrete linguistic units from visually-grounded speech",
      "author" : [ "David Harwath", "Wei-Ning Hsu", "James Glass." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Harwath et al\\.,? 2020",
      "shortCiteRegEx" : "Harwath et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly discovering visual objects and spoken words from raw sensory input",
      "author" : [ "David Harwath", "Adrià Recasens", "Dídac Surís", "Galen Chuang", "Antonio Torralba", "James Glass." ],
      "venue" : "International Journal of Computer Vision.",
      "citeRegEx" : "Harwath et al\\.,? 2019",
      "shortCiteRegEx" : "Harwath et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised learning of spoken language with visual context",
      "author" : [ "David Harwath", "Antonio Torralba", "James R. Glass." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Harwath et al\\.,? 2016",
      "shortCiteRegEx" : "Harwath et al\\.",
      "year" : 2016
    }, {
      "title" : "Image2speech: Automatically generating audio descriptions of images",
      "author" : [ "Mark Hasegawa-Johnson", "Alan Black", "Lucas Ondel", "Odette Scharenborg", "Francesco Ciannella." ],
      "venue" : "International Conference on Natural Language, Signal and Speech Process-",
      "citeRegEx" : "Hasegawa.Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Hasegawa.Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Speech-coco: 600k visually grounded spoken captions aligned to mscoco data set",
      "author" : [ "William Havard", "Laurent Besacier", "Olivier Rosec." ],
      "venue" : "arXiv preprint arXiv:1707.08435.",
      "citeRegEx" : "Havard et al\\.,? 2017",
      "shortCiteRegEx" : "Havard et al\\.",
      "year" : 2017
    }, {
      "title" : "Models of visually grounded speech signal pay attention to nouns: a bilingual experiment on English and Japanese",
      "author" : [ "William Havard", "Jean-Pierre Chevrot", "Laurent Besacier." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Havard et al\\.,? 2019a",
      "shortCiteRegEx" : "Havard et al\\.",
      "year" : 2019
    }, {
      "title" : "Word recognition, competition, and activation in a model of visually grounded speech",
      "author" : [ "William Havard", "Jean-Pierre Chevrot", "Laurent Besacier." ],
      "venue" : "Proc. ACL Conference on Natural Language Learning (CoNLL).",
      "citeRegEx" : "Havard et al\\.,? 2019b",
      "shortCiteRegEx" : "Havard et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep encoder-decoder models for unsupervised learning of controllable speech synthesis",
      "author" : [ "Gustav Eje Henter", "Jaime Lorenzo-Trueba", "Xin Wang", "Junichi Yamagishi." ],
      "venue" : "arXiv preprint arXiv:1807.11470.",
      "citeRegEx" : "Henter et al\\.,? 2018",
      "shortCiteRegEx" : "Henter et al\\.",
      "year" : 2018
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to write with cooperative discriminators",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Antoine Bosselut", "David Golub", "Yejin Choi." ],
      "venue" : "Proc. Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Holtzman et al\\.,? 2018",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2018
    }, {
      "title" : "Voice conversion from non-parallel corpora using variational",
      "author" : [ "Chin-Cheng Hsu", "Hsin-Te Hwang", "Yi-Chiao Wu", "Yu Tsao", "Hsin-Min Wang" ],
      "venue" : null,
      "citeRegEx" : "Hsu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2016
    }, {
      "title" : "Disentangling by partitioning: A representation learning framework for multimodal sensory data",
      "author" : [ "Wei-Ning Hsu", "James Glass." ],
      "venue" : "arXiv preprint arXiv:1805.11264.",
      "citeRegEx" : "Hsu and Glass.,? 2018a",
      "shortCiteRegEx" : "Hsu and Glass.",
      "year" : 2018
    }, {
      "title" : "Scalable factorized hierarchical variational autoencoder training",
      "author" : [ "Wei-Ning Hsu", "James Glass." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Hsu and Glass.,? 2018b",
      "shortCiteRegEx" : "Hsu and Glass.",
      "year" : 2018
    }, {
      "title" : "Hubert: How much can a bad teacher benefit asr pre-training? In ICASSP",
      "author" : [ "Wei-Ning Hsu", "Yao-Hung Hubert Tsai", "Benjamin Bolte", "Ruslan Salakhutdinov", "Abdelrahman Mohamed." ],
      "venue" : "IEEE.",
      "citeRegEx" : "Hsu et al\\.,? 2021",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning latent representations for speech generation and transformation",
      "author" : [ "Wei-Ning Hsu", "Yu Zhang", "James Glass." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Hsu et al\\.,? 2017a",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised learning of disentangled and interpretable representations from sequential data",
      "author" : [ "Wei-Ning Hsu", "Yu Zhang", "James Glass." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Hsu et al\\.,? 2017b",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2017
    }, {
      "title" : "Hierarchical generative modeling for controllable speech synthesis",
      "author" : [ "Wei-Ning Hsu", "Yu Zhang", "Ron Weiss", "Heiga Zen", "Yonghui Wu", "Yuan Cao", "Yuxuan Wang." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Hsu et al\\.,? 2019",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale representation learning from visually grounded untranscribed speech",
      "author" : [ "Gabriel Ilharco", "Yuan Zhang", "Jason Baldridge." ],
      "venue" : "Proc. ACL Conference on Natural Language Learning (CoNLL).",
      "citeRegEx" : "Ilharco et al\\.,? 2019",
      "shortCiteRegEx" : "Ilharco et al\\.",
      "year" : 2019
    }, {
      "title" : "Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet",
      "author" : [ "International Phonetic Association." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Association.,? 1999",
      "shortCiteRegEx" : "Association.",
      "year" : 1999
    }, {
      "title" : "The LJ speech dataset",
      "author" : [ "Keith Ito." ],
      "venue" : "https:// keithito.com/LJ-Speech-Dataset/.",
      "citeRegEx" : "Ito.,? 2017",
      "shortCiteRegEx" : "Ito.",
      "year" : 2017
    }, {
      "title" : "Visually grounded learning of keyword prediction from untranscribed speech",
      "author" : [ "Herman Kamper", "Shane Settle", "Gregory Shakhnarovich", "Karen Livescu." ],
      "venue" : "Proc. Annual Conference of International Speech Communication",
      "citeRegEx" : "Kamper et al\\.,? 2017",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2017
    }, {
      "title" : "Semantic speech retrieval with a visually grounded model of untranscribed speech",
      "author" : [ "Herman Kamper", "Gregory Shakhnarovich", "Karen Livescu." ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing, PP:1–1.",
      "citeRegEx" : "Kamper et al\\.,? 2018",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Data augmenting contrastive learning of speech representations in the time domain",
      "author" : [ "Eugene Kharitonov", "Morgane Rivière", "Gabriel Synnaeve", "Lior Wolf", "Pierre-Emmanuel Mazaré", "Matthijs Douze", "Emmanuel Dupoux." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Kharitonov et al\\.,? 2020",
      "shortCiteRegEx" : "Kharitonov et al\\.",
      "year" : 2020
    }, {
      "title" : "A factorial deep markov model for unsupervised disentangled representation learning from speech",
      "author" : [ "Sameer Khurana", "Shafiq Rayhan Joty", "Ahmed Ali", "James Glass." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Khurana et al\\.,? 2019",
      "shortCiteRegEx" : "Khurana et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel." ],
      "venue" : "Proc. International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Kiros et al\\.,? 2014",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Importance of search and evaluation strategies in neural dialogue modeling",
      "author" : [ "Ilia Kulikov", "Alexander Miller", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation.",
      "citeRegEx" : "Kulikov et al\\.,? 2019",
      "shortCiteRegEx" : "Kulikov et al\\.",
      "year" : 2019
    }, {
      "title" : "Ethnologue: Languages of the World, Nineteenth edition",
      "author" : [ "M. Paul Lewis", "Gary F. Simon", "Charles D. Fennig." ],
      "venue" : "SIL International. Online version: http://www.ethnologue.com.",
      "citeRegEx" : "Lewis et al\\.,? 2016",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ArXiv, abs/1405.0312.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods",
      "author" : [ "Jaime Lorenzo-Trueba", "Junichi Yamagishi", "Tomoki Toda", "Daisuke Saito", "Fernando Villavicencio", "Tomi Kinnunen", "Zhenhua Ling." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Lorenzo.Trueba et al\\.,? 2018",
      "shortCiteRegEx" : "Lorenzo.Trueba et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "author" : [ "Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural baby talk",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Lu et al\\.,? 2018",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2018
    }, {
      "title" : "Unpaired image-to-speech synthesis with multimodal information bottleneck",
      "author" : [ "Shuang Ma", "Daniel McDuff", "Yale Song." ],
      "venue" : "Proc. IEEE International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Mao et al\\.,? 2015",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "Language learning using speech to image retrieval",
      "author" : [ "Danny Merkx", "Stefan L. Frank", "Mirjam Ernestus." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Merkx et al\\.,? 2019",
      "shortCiteRegEx" : "Merkx et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural discrete representation learning",
      "author" : [ "Aaron van den Oord", "Oriol Vinyals", "Koray Kavukcuoglu." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Oord et al\\.,? 2017",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2017
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu." ],
      "venue" : "arXiv preprint arXiv:1609.03499.",
      "citeRegEx" : "Oord et al\\.,? 2016",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Deep voice 3: Scaling text-to-speech with convolutional sequence learning",
      "author" : [ "Wei Ping", "Kainan Peng", "Andrew Gibiansky", "Sercan O Arik", "Ajay Kannan", "Sharan Narang", "Jonathan Raiman", "John Miller." ],
      "venue" : "arXiv preprint arXiv:1710.07654.",
      "citeRegEx" : "Ping et al\\.,? 2017",
      "shortCiteRegEx" : "Ping et al\\.",
      "year" : 2017
    }, {
      "title" : "Waveglow: A flow-based generative network for speech synthesis",
      "author" : [ "Ryan Prenger", "Rafael Valle", "Bryan Catanzaro." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Prenger et al\\.,? 2019",
      "shortCiteRegEx" : "Prenger et al\\.",
      "year" : 2019
    }, {
      "title" : "Collecting image annotations using Amazon’s Mechanical Turk",
      "author" : [ "Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proc. NAACL Conference on Human Language Technologies (NAACL-HLT).",
      "citeRegEx" : "Rashtchian et al\\.,? 2010",
      "shortCiteRegEx" : "Rashtchian et al\\.",
      "year" : 2010
    }, {
      "title" : "Self-critical sequence training for image captioning",
      "author" : [ "Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jerret Ross", "Vaibhava Goel." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Rennie et al\\.,? 2017",
      "shortCiteRegEx" : "Rennie et al\\.",
      "year" : 2017
    }, {
      "title" : "Linguistic unit discovery from multi-modal inputs in unwritten languages: Summary of the \"Speaking Rosetta",
      "author" : [ "Francesco Ciannella", "Mingxing Du", "Elin Larsen", "Danny Merkx", "Rachid Riad", "Liming Wang", "Emmanuel Dupoux" ],
      "venue" : "JSALT",
      "citeRegEx" : "Ciannella et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ciannella et al\\.",
      "year" : 2018
    }, {
      "title" : "wav2vec: Unsupervised pre-training for speech recognition",
      "author" : [ "Steffen Schneider", "Alexei Baevski", "Ronan Collobert", "Michael Auli." ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH).",
      "citeRegEx" : "Schneider et al\\.,? 2019",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2019
    }, {
      "title" : "Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion",
      "author" : [ "Joan Serrà", "Santiago Pascual", "Carlos Segura Perales." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Serrà et al\\.,? 2019",
      "shortCiteRegEx" : "Serrà et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
      "author" : [ "Jonathan Shen", "Ruoming Pang", "Ron J Weiss", "Mike Schuster", "Navdeep Jaitly", "Zongheng Yang", "Zhifeng Chen", "Yu Zhang", "Yuxuan Wang", "Rj Skerrv-Ryan" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Continuous probabilistic transform for voice conversion",
      "author" : [ "Yannis Stylianou", "Olivier Cappé", "Eric Moulines." ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, 6(2):131–142.",
      "citeRegEx" : "Stylianou et al\\.,? 1998",
      "shortCiteRegEx" : "Stylianou et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning words by drawing images",
      "author" : [ "Dídac Surís", "Adrià Recasens", "David Bau", "David Harwath", "James Glass", "Antonio Torralba." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Surís et al\\.,? 2019",
      "shortCiteRegEx" : "Surís et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning words from images and speech",
      "author" : [ "Gabriel Synnaeve", "Maarten Versteegh", "Emmanuel Dupoux." ],
      "venue" : "Proc. Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Synnaeve et al\\.,? 2014",
      "shortCiteRegEx" : "Synnaeve et al\\.",
      "year" : 2014
    }, {
      "title" : "Voiceloop: Voice fitting and synthesis via a phonological loop",
      "author" : [ "Yaniv Taigman", "Lior Wolf", "Adam Polyak", "Eliya Nachmani." ],
      "venue" : "arXiv preprint arXiv:1707.06588.",
      "citeRegEx" : "Taigman et al\\.,? 2017",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2017
    }, {
      "title" : "VQVAE unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019",
      "author" : [ "Andros Tjandra", "Berrak Sisman", "Mingyang Zhang", "Sakriani Sakti", "Haizhou Li", "Satoshi Nakamura." ],
      "venue" : "arXiv preprint arXiv:1905.11449.",
      "citeRegEx" : "Tjandra et al\\.,? 2019",
      "shortCiteRegEx" : "Tjandra et al\\.",
      "year" : 2019
    }, {
      "title" : "Voice conversion based on maximumlikelihood estimation of spectral parameter trajectory",
      "author" : [ "Tomoki Toda", "Alan W Black", "Keiichi Tokuda." ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing, 15(8):2222–2235.",
      "citeRegEx" : "Toda et al\\.,? 2007",
      "shortCiteRegEx" : "Toda et al\\.",
      "year" : 2007
    }, {
      "title" : "On mutual information maximization for representation learning",
      "author" : [ "Michael Tschannen", "Josip Djolonga", "Paul K. Rubenstein", "Sylvain Gelly", "Mario Lucic." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Tschannen et al\\.,? 2020",
      "shortCiteRegEx" : "Tschannen et al\\.",
      "year" : 2020
    }, {
      "title" : "2017. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit",
      "author" : [ "Christophe Veaux", "Junichi Yamagishi", "Kirsten Macdonald" ],
      "venue" : null,
      "citeRegEx" : "Veaux et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Veaux et al\\.",
      "year" : 2017
    }, {
      "title" : "CIDEr: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Diverse beam search for improved description of complex scenes",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasaath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra." ],
      "venue" : "Proc. AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Vijayakumar et al\\.,? 2018",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised pre-training of bidirectional speech encoders via masked reconstruction",
      "author" : [ "Weiran Wang", "Qingming Tang", "Karen Livescu." ],
      "venue" : "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Show and speak: Directly synthesize spoken description of images",
      "author" : [ "Xinsheng Wang", "Siyuan Feng", "Jihua Zhu", "Mark Hasegawa-Johnson", "Odette Scharenborg." ],
      "venue" : "arXiv preprint arXiv:2010.12267.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "author" : [ "Yuxuan Wang", "Daisy Stanton", "Yu Zhang", "RJ SkerryRyan", "Eric Battenberg", "Joel Shor", "Ying Xiao", "Fei Ren", "Ye Jia", "Rif A Saurous." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "Kilian Q. Weinberger", "Lawrence K. Saul." ],
      "venue" : "Journal of Machine Learning Research (JMLR).",
      "citeRegEx" : "Weinberger and Saul.,? 2009",
      "shortCiteRegEx" : "Weinberger and Saul.",
      "year" : 2009
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Welleck et al\\.,? 2020",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2020
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "Proc. International Conference on Machine",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 52,
      "context" : "Although there are over 7,000 languages spoken worldwide (Lewis et al., 2016), only several dozen have enough data available to support supervised speech recognition, and many languages do not even employ a writing system (Adda et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : ", 2016), only several dozen have enough data available to support supervised speech recognition, and many languages do not even employ a writing system (Adda et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 75,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 21,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 26,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 44,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 29,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 60,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 12,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 3,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 35,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 45,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 74,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 41,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 18,
      "context" : "A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as",
      "startOffset" : 124,
      "endOffset" : 434
    }, {
      "referenceID" : 50,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 59,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 83,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 46,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 89,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 68,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 14,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 56,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 57,
      "context" : "Tremendous progress has been made recently in natural language image caption generation (Kiros et al., 2014; Mao et al., 2015; Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Rennie et al., 2017; Dai and Lin, 2017; Lu et al., 2017; Anderson et al., 2018; Lu et al., 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al.",
      "startOffset" : 88,
      "endOffset" : 290
    }, {
      "referenceID" : 65,
      "context" : ", 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al., 2017; Taigman et al., 2017; Wang et al., 2017; Shen et al., 2018; Oord et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 153
    }, {
      "referenceID" : 76,
      "context" : ", 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al., 2017; Taigman et al., 2017; Wang et al., 2017; Shen et al., 2018; Oord et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 153
    }, {
      "referenceID" : 72,
      "context" : ", 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al., 2017; Taigman et al., 2017; Wang et al., 2017; Shen et al., 2018; Oord et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 153
    }, {
      "referenceID" : 62,
      "context" : ", 2018) and naturalistic text-tospeech synthesis (TTS) (Ping et al., 2017; Taigman et al., 2017; Wang et al., 2017; Shen et al., 2018; Oord et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 153
    }, {
      "referenceID" : 82,
      "context" : "We identify issues of an existing metric (Vijayakumar et al., 2018) and propose M-SPICE for sampling-based evaluation to address the problems.",
      "startOffset" : 41,
      "endOffset" : 67
    }, {
      "referenceID" : 83,
      "context" : "alistic (text) captions that describe the content of visual images was made with the advent of deep neural networks (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Anderson et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 206
    }, {
      "referenceID" : 46,
      "context" : "alistic (text) captions that describe the content of visual images was made with the advent of deep neural networks (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Anderson et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 206
    }, {
      "referenceID" : 89,
      "context" : "alistic (text) captions that describe the content of visual images was made with the advent of deep neural networks (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Anderson et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "alistic (text) captions that describe the content of visual images was made with the advent of deep neural networks (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Anderson et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 206
    }, {
      "referenceID" : 58,
      "context" : "Training an image-to-speech system using separate (image, text) and (text, speech) datasets was explored in (Ma et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to",
      "startOffset" : 54,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to",
      "startOffset" : 54,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to",
      "startOffset" : 54,
      "endOffset" : 192
    }, {
      "referenceID" : 55,
      "context" : "It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to",
      "startOffset" : 54,
      "endOffset" : 192
    }, {
      "referenceID" : 71,
      "context" : "It has recently seen progress using neural approaches (Hsu et al., 2016, 2017a,b; Fang et al., 2018; Chorowski et al., 2018; Chou et al., 2018; Lorenzo-Trueba et al., 2018; Serrà et al., 2019), but the most relevant work to",
      "startOffset" : 54,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "our own is the ZeroSpeech 2019 challenge (Dunbar et al., 2019; Tjandra et al., 2019; Cho et al., 2019), which addresses unsupervised learning of discrete speech units that can replace text and be used as input to TTS models.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 77,
      "context" : "our own is the ZeroSpeech 2019 challenge (Dunbar et al., 2019; Tjandra et al., 2019; Cho et al., 2019), which addresses unsupervised learning of discrete speech units that can replace text and be used as input to TTS models.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "our own is the ZeroSpeech 2019 challenge (Dunbar et al., 2019; Tjandra et al., 2019; Cho et al., 2019), which addresses unsupervised learning of discrete speech units that can replace text and be used as input to TTS models.",
      "startOffset" : 41,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : ", 2019), predictive coding (Chung et al., 2019; Wang et al., 2020a), contrastive learning (Oord et al.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 84,
      "context" : ", 2019), predictive coding (Chung et al., 2019; Wang et al., 2020a), contrastive learning (Oord et al.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Prior work addresses inferring linguistic content such as phones from the learned representations (Baevski et al., 2020; Kharitonov et al., 2020; Hsu et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 163
    }, {
      "referenceID" : 47,
      "context" : "Prior work addresses inferring linguistic content such as phones from the learned representations (Baevski et al., 2020; Kharitonov et al., 2020; Hsu et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 163
    }, {
      "referenceID" : 37,
      "context" : "Prior work addresses inferring linguistic content such as phones from the learned representations (Baevski et al., 2020; Kharitonov et al., 2020; Hsu et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 163
    }, {
      "referenceID" : 86,
      "context" : "the speaker characteristics and other acoustic properties to be independently controllable from the I2U system (Wang et al., 2018; Hsu et al., 2019; Henter et al., 2018; Akuzawa et al., 2018).",
      "startOffset" : 111,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "the speaker characteristics and other acoustic properties to be independently controllable from the I2U system (Wang et al., 2018; Hsu et al., 2019; Henter et al., 2018; Akuzawa et al., 2018).",
      "startOffset" : 111,
      "endOffset" : 191
    }, {
      "referenceID" : 31,
      "context" : "the speaker characteristics and other acoustic properties to be independently controllable from the I2U system (Wang et al., 2018; Hsu et al., 2019; Henter et al., 2018; Akuzawa et al., 2018).",
      "startOffset" : 111,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "the speaker characteristics and other acoustic properties to be independently controllable from the I2U system (Wang et al., 2018; Hsu et al., 2019; Henter et al., 2018; Akuzawa et al., 2018).",
      "startOffset" : 111,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "Among the three datasets with image and speech pairs: Places, Flickr8k, MSCOCO, we chose the latter two for training I2U models, because they include five captions per image, which is more suitable for caption metrics such as SPICE (Anderson et al., 2016); moreover, they are commonly used image captioning datasets with many text-based baselines in the literature.",
      "startOffset" : 232,
      "endOffset" : 255
    }, {
      "referenceID" : 54,
      "context" : "Specifically, as part of this work we collect SpokenCOCO, a spoken version of the MSCOCO captioning dataset (Lin et al., 2014) with 742 hours from 2532 speakers, via Amazon Mechanical Turk by displaying the text to a person and having them read it aloud.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : "S2U PlacesAudio (Harwath et al., 2016) 936 400K 2683 American spontaneous image caption",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "I2U Flickr8kAudio (Harwath and Glass, 2015) 46 40K 183 American scripted image caption SpokenCOCO (this work) 742 605K 2353",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 43,
      "context" : "U2S LJSpeech (Ito, 2017) 24 13K 1 American read non-fiction books VCTK (Veaux et al.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 80,
      "context" : "U2S LJSpeech (Ito, 2017) 24 13K 1 American read non-fiction books VCTK (Veaux et al., 2017) 44 40K 109 British read newspaper",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 54,
      "context" : "For training S2U and I2U models, their corresponding image datasets, MSCOCO (Lin et al., 2014), Flickr8k (Rashtchian et al.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 67,
      "context" : ", 2014), Flickr8k (Rashtchian et al., 2010), and Places (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 28,
      "context" : "of MSCOCO named Speech-COCO (Havard et al., 2017), it is comprised of only synthesized speech using a concatenative TTS model in eight speak-",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 87,
      "context" : "This model is trained to associate speech with contextually relevant visual inputs using a triplet loss (Weinberger and Saul, 2009), which can be interpreted as maximizing a mutual information lower bound between image and speech (Tschannen et al.",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 79,
      "context" : "This model is trained to associate speech with contextually relevant visual inputs using a triplet loss (Weinberger and Saul, 2009), which can be interpreted as maximizing a mutual information lower bound between image and speech (Tschannen et al., 2020).",
      "startOffset" : 230,
      "endOffset" : 254
    }, {
      "referenceID" : 9,
      "context" : "In contrast, many of the speech representations are trained by reconstructing (Chorowski et al., 2019; Hsu et al., 2017b) or predicting unseen speech signals (Chung et al.",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 39,
      "context" : "In contrast, many of the speech representations are trained by reconstructing (Chorowski et al., 2019; Hsu et al., 2017b) or predicting unseen speech signals (Chung et al.",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : ", 2017b) or predicting unseen speech signals (Chung et al., 2019), which would inevitable capture factors unrelated to the linguistic content.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "Although the ResDAVEnet-VQ model has been shown to be capable of learning both phone-like and word-like units, the experiments in (Harwath et al., 2020) show that only several hundred words are explicitly learned, which tend to be “visual words.",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Both the I2U model and the U2S model are based upon recurrent seq2seq with attention networks (Bahdanau et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 89,
      "context" : "adopt Show-Attend-and-Tell (SAT) (Xu et al., 2015) for the I2U model.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 72,
      "context" : "For the U2S model, we adopt Tacotron2 (Shen et al., 2018) and WaveGlow (Prenger et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 66,
      "context" : ", 2018) and WaveGlow (Prenger et al., 2019) for unit-to-spectrogram and spectrogram-to-waveform generation, respectively.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 64,
      "context" : "Word-based metrics transcribe a generated spoken caption into text (manually or with an ASR system) and then measure word-based captioning metrics against a set of reference captions, such as BLEU-4 (Papineni et al., 2002) (adjusted n-",
      "startOffset" : 199,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "gram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 53,
      "context" : "gram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al.",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 81,
      "context" : "gram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al., 2015) (TF-IDF weighted n-gram cosine similarity), and SPICE (Anderson",
      "startOffset" : 148,
      "endOffset" : 171
    }, {
      "referenceID" : 89,
      "context" : "Our word-based SAT models outperform (Xu et al., 2015) because we use a stronger image encoder (ResNet-101).",
      "startOffset" : 37,
      "endOffset" : 54
    } ],
    "year" : 2021,
    "abstractText" : "In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.",
    "creator" : "LaTeX with hyperref"
  }
}