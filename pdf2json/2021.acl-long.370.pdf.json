{
  "name" : "2021.acl-long.370.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation",
    "authors" : [ "Huayang Li", "Lemao Liu", "Guoping Huang", "Shuming Shi" ],
    "emails" : [ "alanili@tencent.com", "redmondliu@tencent.com", "donkeyhuang@tencent.com", "shumingshi@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4792–4802\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4792\nComputer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark1 to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine translation (MT) has witnessed great advancements with the emergence of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn,\n1The information of benchmark datasets is in https: //github.com/ghrua/gwlan\n2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators.\nWe note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b\nand c) has not been systematically studied. Second, almost no public benchmarks are available for the autocompletion task of CAT. Although some achievements have been made, research progress in CAT is more sluggish than that in automatic MT. The lack of benchmarks has hindered researchers from making continuous progress in this area.\nIn this work, we propose a General Word-Level AutocompletioN (GWLAN) task, and construct a benchmark with automatic evaluation to facilitate further research progress in CAT. Specifically, the GWLAN task aims to complete the target word for human translators based on a source sentence, translation context as well as human typed characters. Compared with previous work, GWLAN considers four most general types of translation context: prefix, suffix, zero context, and bidirectional context. Besides, as in most real world scenarios, we only know the relative position between input words and the spans of translation context in the GWLAN task. We construct a benchmark for the task, with the goal of supporting automatic evaluation and ensuring a convenient and fair comparison among different methods. The benchmark is built by extracting triples of source sentences, translation contexts, and human typed characters from standard parallel datasets. Accuracy is adopted as the evaluation metric in the benchmark.\nTo address the variety of context types and weak position information issue, we propose a neural model to complete a word in different types of context as well as a joint training strategy to optimize its parameters. Our model can learn the representation of potential target words in translation and then choose the most possible word based on the human input.\nOur contributions are two-fold:\n• We propose the task of general word-level autocompletion for CAT, and construct the first public benchmark to facilitate research in this topic.\n• We propose a joint training strategy to optimize the model parameters on different types of contexts together. 2"
    }, {
      "heading" : "2 Related Work",
      "text" : "Computer-aided translation (CAT) is a widely used practice when using MT technology in the industry.\n2This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.transmart.qq.com.\nAs the the MT systems advanced and improved, various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios.\nSentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016).\nAnother sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD.\nWord-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be the next word of the translation prefix, which limits its application. In contrast, in our work the proposed word-level autocompletion is more general and can be applied to real-world scenarios such as post-editing (Vasconcellos and León, 1985;\nGreen et al., 2013) and LCD, where human translators need to input some words (corrections or constraints). Huang et al. (2015) propose a method to predict a target word based on human typed characters, however, this method only uses the source side information and does not consider translation context, leading to limited performance compared with our work.\nOthers Our work may also be related to previous works in input method editors (IME) (Huang et al., 2018; Lee et al., 2007). However, they are in the monolingual setting and not capable of using the useful multilingual information."
    }, {
      "heading" : "3 Task and Benchmark",
      "text" : "In this section, we first describe why we need wordlevel autocompletion in real-world CAT scenarios. We then present the details of the GWLAN task and the construction of benchmark.\nWhy GWLAN? Word level autocompletion is beneficial for improving input efficiency (Langlais et al., 2000). Previous works assume that the translation context should be a prefix and the desired word is next to the prefix as shown in Figure 1 (b), where the context is “We asked two” and the desired word is “specialists”. However, in some real-world CAT scenarios such as post-editing and lexically constrained decoding, translation context may be discontinuous and the input words (corrections or lexical constraints) are not necessarily conjunct to the translation context. As shown in Figure 1 (c), the context is “We · · · their opinion” and the human typed characters “sp” is conjunct to neither “We” nor “their” in the context. Therefore, existing methods can not perform well on such a general scenario. This motivates us to propose a general word-level autocompletion task for CAT."
    }, {
      "heading" : "3.1 Task Definition",
      "text" : "Suppose x = (x1, x2, . . . , xm) is a source sequence, s = (s1, s2, . . . , sk) is a sequence of human typed characters, and a translation context is denoted by c = (cl, cr), where cl = (cl,1, cl,2, . . . , cl,i), cr = (cr,1, cr,2, . . . , cr,j). The translation pieces cl and cr are on the left and right hand side of s, respectively. Formally, given a source sequence x, typed character sequence s and a context c, the general word-level autocompletion (GWLAN) task aims to predict a target word w which is to be placed in the middle be-\ntween cl and cr to constitute a partial translation. Note that in the partial translation consisting of cl, w and cr, w is not necessary to be consecutive to cl,i or cr,1. For example, in Figure 1 (c), cl = (“We”, ), cr = (“their”, “option”, “.”), s = (“sp”, ), the GWLAN task is expected to predictw = “specialists” to constitute a partial translation “We · · · specialists · · · their opinion.”, where “· · · ” represents zero, one, or more words (i.e., the two words before and after it are not necessarily consecutive).\nTo make our task more general in real-world scenarios, we assume that the left context cl and right context cr can be empty, which leads to the following four types of context: • Zero-context: both cl and cr are empty; • Suffix: cl is empty; • Prefix: cr is empty; • Bi-context: neither cl nor cr is empty. With the tuple (x, s, c), the GWLAN task is to predict the human desired word w.\nRelation to most similar tasks Some similar techniques have been explored in CAT. Green et al. (2014) and Knowles and Koehn (2016) studied a autocompletion scenario called translation prediction (TP), which provides suggestions of the next word (or phrase) given a prefix. Besides the strict assumption of translation context (i.e., prefix here), compared with GWLAN, another difference is that the information of human typed characters is ignored in their setting. There also exist some works that consider the human typed sequences (Huang et al., 2015; Santy et al., 2019), but they only consider a specific type of translation contexts. Huang et al. (2015) propose to complete a target word based on the zero-context assumption. Despite its flexibility, this method is unable to explore translation contexts to improve the autocompletion performance. The word-level autocompletion methods in Langlais et al. (2000); Santy et al. (2019) have the same assumption as TP, which impedes the use of their methods under the scenarios like post editing and lexically constrained decoding, where human inputs are not necessarily conjunct to the variety of translation contexts."
    }, {
      "heading" : "3.2 Benchmark Construction",
      "text" : "To set up a benchmark, firstly we should create a large scale dataset including tuples of (x, s, c, w) for training and evaluating GWLAN models. Ideally, we may hire professional translators to man-\nually annotate such a dataset, but it is too costly in practice. Therefore, in this work, we propose to automatically construct the dataset from parallel datasets which is originally used in automatic machine translation tasks. The procedure for constructing our data is the same for train, validation, and test sets. And we construct a dataset for each type of translation context.\nAssume we are given a parallel dataset {(xi,yi)}, where yi is the reference translation of xi. Then, we can automatically construct the data ci and si by randomly sampling from yi. We first sample a word w = yik and then demonstrate how to extract ci for different translation contexts: • Zero-context: both cl and cr are empty; • Suffix: randomly sample a translation piece cr = ypr,1:pr,2 from y, where k < pr,1 < pr,2. The cl is empty here; • Prefix: randomly sample a translation piece cl = ypl,1:pl,2 from y, where pl,1 < pl,2 < k. The cr is empty here; • Bi-context: sample cl as in prefix, and sample cr as in suffix.\nThen we have to simulate the human typed characters s based onw. For languages like English and German, we sample a position p from the character sequence and the human input s = w1:p, where 1 ≤ p < Lw. For languages like Chinese, the human input is the phonetic symbols of the word, since the word cannot be directly typed into the computer. Therefore, we have to convert w to phonetic symbols that are characters in alphabet and sample s from phonetic symbols like we did on English.\nEvaluation Metric To evaluate the performance of the well-trained models, we choose accuracy as the evaluation metric:\nAcc = Nmatch Nall , (1)\nwhere Nmatch is the number of words that are correctly predicted and Nall is the number of testing examples."
    }, {
      "heading" : "4 Proposed Approach",
      "text" : "Given a tuple (x, c, s), our approach decomposes the whole word autocompletion process into two parts: model the distribution of the target word w based on the source sequence x and the translation context c, and find the most possible word w based on the distribution and human typed sequence s.\nTherefore, in the following subsections, we firstly propose a word prediction model (WPM) to define the distribution p(w|x, c) of the target word w (§4.1). Then we can treat the human input sequence s as soft constraints or hard constraints to complete s and obtain the target word w (§4.2). Finally, we present two strategies for training and inference (§4.3)."
    }, {
      "heading" : "4.1 Word Prediction Model",
      "text" : "The purpose of WPM is to model the distribution p(w|x, c). More concretely, we will use a single placeholder [MASK] to represent the unknown target word w, and use the representation of [MASK] learned from WPM to predict it. Formally, given the source sequence x, and the translation context c = (cl, cr), the possibility of the target word w is:\nP (w|x, cl, cr; θ) = softmax (φ(h)) [w] (2)\nwhere h is the representation of [MASK], φ is a linear network that projects the hidden representation h to a vector with dimension of target vocabulary size V , and softmax(d)[w] takes the component regarding to w after the softmax operation over a vector d ∈ RV .\nInspired by the attention-based architectures (Vaswani et al., 2017; Devlin et al., 2019)3, we\n3Because the use of attention-based models has become ubiquitous recently, we omit an exhaustive background de-\nuse a dual-encoder architecture to learn the representation h based on source sequence x and translation context c. Our model has a source encoder and a cross-lingual encoder. The source encoder of WPM is the same as the Transformer encoder, which is used to encode the source sequence x. As shown in Figure 2, the output of source encoder will be passed to the cross-lingual encoder later. The cross-lingual encoder is similar to the Transformer decoder, while the only difference is that we replace the auto-regressive attention (ARA) layer by a bidirectional masked attention (BMA) module, due to that the ARA layer cannot use the leftward information flow (i.e., cr).\nSpecifically, the BMA module is built by a multiple-layer self attention network. As shown in Figure 3, in each layer of BMA, each token in the attention query can attend to all words in translation context cl and cr. In addition, the input consists of three parts, the [MASK] token, and translation contexts cl and cr, as illustrated in Figure 3. Note that its position embeddings E are only used to represent the relative position relationship between tokens. Taking the sentence in Figure 3 as an example, E3 does not precisely specify the position of the target word w but roughly indicates that w is on the right-hand-side of cl and on the left-hand-side of cr. Finally, the representation of [MASK] as learnt by BMA will be passed to Add & Norm layer as shown in Figure 2.\nscription of the model and refer readers to Vaswani et al. (2017) and Devlin et al. (2019)."
    }, {
      "heading" : "4.2 Human Input Autocompletion",
      "text" : "After learning the representation h of the [MASK] token, there are two ways to use the human input sequence s to determinate the human desired word. Firstly, we can learn the representation of s and use it as a soft constraint while predicting word w. Taking the sentence in Figure 3 as an example, supposing the human typed sequence is s = “des”, we can use an RNN network to learn the representation of des and concatenate it with h to predict the word descending. Alternatively, we can use des as a hard constraint:\nPs[w] =\n{ P (w|x,c;θ)\nZ , if w starts with s 0, otherwise.\nwhere P (·|·) is the probability distribution defined in Eq. (2) and Z is the normalization term independent onw. Then we pickw∗ = argmaxw Ps[w] as the most possible word. In our preliminary experiments, the performances of these two methods are comparable, and there is no significant gain when we use them together. One main reason is that the model can already learn the starts-with action precisely in the soft constraint method. Therefore, we propose to use the human inputs as hard constraints in our later experiments, because of the method’s efficiency and simplicity."
    }, {
      "heading" : "4.3 Training and Inference Strategy",
      "text" : "Suppose D denotes the training data for GWLAN, i.e., a set of tuples (x, c, s, w). Since there are four different types of context in D as presented in\n§3, we can split D into four subsets Dzero, Dprefix, Dsuffix and Dbi. To yield good performances on those four types of translation context, we also propose two training strategies. The inference strategy differs accordingly.\nStrategy 1: One Context Type One Model For this strategy, we will train a model for each translation context, respectively. Specifically, for each type of context t ∈ {zero, prefix, suffix, bi}, we independently train one model θt by minimizing the following loss L(Dt, θ):\nL(Dt; θ) = 1 |Dt| ∑\n(x,c,s,w)∈Dt\nlogP (w|x, c; θ),\n(3) where P (w|x, c; θ) is the WPM model defined in Eq. 2, |Dt| is the size of training dataset Dt, and t can be any type of translation context. In this way, we actually obtain four models in total after training. In the inference process, for each testing instance (x, cl, cr, s), we decide its context type t in terms of cl and cr and then use θ̂t to predict the word w.\nStrategy 2: Joint Model The separate training strategy is straightforward. However, it may also make the models struck in the local optimal. To address these issues, we also propose a joint training strategy, which has the ability to stretch the model out of the local optimal once the parameters is over-fitting on one particular translation context. Therefore, using the joint training strategy, we train a single model for all types of translation context by minimizing the following objective:\nL(D; θ̂) = L(Dzero; θ̂) + L(Dprefix; θ̂)+ L(Dsuffix; θ̂) + L(Dbi; θ̂)\nwhere each L(Dt; θ) is as defined in Eq. 3. In this way, we actually obtain a single model θ̂ after training. In the inference process, for each testing instance (x, cl, cr, s) we always use θ̂ to predict the target word w."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We carry out experiments on four GWLAN tasks including bidirectional Chinese–English tasks and German–English tasks. The benchmarks for our experiments are based on the public translation\ndatasets. The training set for two directional Chinese–English tasks consists of 1.25M bilingual sentence pairs from LDC corpora. The toolkit we used to convert Chinese word w to phonetic symbols is pypinyin4. As discussed in (§3.2), the training data for GWLAN is extracted from 1.25M sentence pairs. The validation data for GWLAN is extracted from NIST02 and the test datasets for GWLAN are constructed from NIST05 and NIST06. For two directional German–English tasks, we use the WMT14 dataset preprocessed by Stanford5. The validation and test sets for our tasks are based on newstest13 and newstest14 respectively. For each dataset, the models are tuned and selected based on the validation set.\nThe main strategies we used to prepare our benchmarks are shown in §3.2. However, lots of trivial instances may be included if we directly use the uniform distribution for sampling, e.g., predicting word “the” given “th”. Therefore, we apply some intuitive rules to reduce the probability of trivial instances. For example, we assign higher probability for words with more than 4 characters in English and 2 characters in Chinese, and we require that the lengths of input character sequence s and translation contexts c should not be too long."
    }, {
      "heading" : "5.2 Systems for Comparison",
      "text" : "In the experiments, we evaluate and compare the performance of our methods (WPM-Sep and WPMJoint) and a few baselines. They are illustrated below,\nWPM-SEP is our approach with the “one context one model” training and inference strategy in Section §4.3. In other words, we train our model for each translation context separately.\nWPM-JOINT is our approach with the “joint model” strategy in Section §4.3.\nTRANSTABLE: We train an alignment model6 on the training set and build a word-level translation table. While testing, we can find the translations of all source words based on this table, and select out valid translations based on the human input. The word with highest frequency among all candidates is regarded as the prediction. This baseline is inspired by Huang et al. (2015).\n4https://github.com/mozillazg/ python-pinyin\n5https://nlp.stanford.edu/projects/ nmt/\n6https://github.com/clab/fast_align\nTRANS-PE: We train a vanilla NMT model using the Transformer-base model. During the inference process, we use the context on the left hand side of human input as the model input, and return the most possible words based on the probability of valid words selected out by the human input. This baseline is inspired by Langlais et al. (2000); Santy et al. (2019).\nTRANS-NPE: As another baseline, we also train an NMT model based on Transformer, but without position encoding on the target side. While testing, we use the averaged hidden vectors of all the target words outputted by the last decoder layer to predict the potential candidates."
    }, {
      "heading" : "5.3 Main Results",
      "text" : "Table 1 shows the main results of our methods and three baselines on the test sets of Chinese-English and German-English datasets. It is clear from the results that our methods WPM-SEP and WPMJOINT significantly outperform the three baseline methods. Results on Row 4 and Row 5 of Table 1 also show that the WPM-JOINT method, which uses a joint training strategy to optimize a single model, achieves better overall performance than WPM-SEP, which trains four models for different translation contexts respectively. In-depth analysis about the two training strategies is presented in the\nnext section. The method TRANS-PE, which assumes the human input is the next word of the given context, behaves poorly under the more general setting. As the results of TRANS-NPE show, when we use the same model as TRANS-PE and relax the constraint of position by removing the position encoding, the accuracy of the model improves. One interesting finding is that the TRANSTABLE method, which is only capable of leveraging the zero-context, achieves good results on the ChineseEnglish task when the target language is English. However, when the target language is Chinese, the performance of TRANSTABLE drops significantly."
    }, {
      "heading" : "6 Experimental Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Effects on Different Translation Context",
      "text" : "In this section, we presents more detailed results on the four translation contexts and analyze the features of GWLAN. These analyses can help us to better understand the task and propose effective approaches in the future.\nSeparate Training VS. Joint Training Compared with WPM-SEP, WPM-JOINT shows two advantages. On one hand, even there is only one model, WPM-JOINT yields better performances than WPM-SEP, enabling simpler deployment. This may be caused by that training on multiple\nrelated tasks can force the model learn more expressive representations, avoiding over-fitting. On the other hand, the variance of results on different translation contexts of WPM-JOINT is smaller, which can provide an more steady autocompletion service. From the viewpoint of joint training, the lower variance may be caused by that WPM-JOINT spends more efforts to minimize the one with maximal risk (i.e., zero-context), although sometimes it may slightly sacrifice the task with minimal risk (i.e., bi-context).\nThe results of WPM-SEP and WPM-JOINT also have some shared patterns. Firstly, the performances of the two methods on prefix and suffix translation contexts are nearly the same. Although the prefix and suffix may play different roles in the SVO language structure, they have little impact on the the autocompletion accuracy using our method. Moreover, among the results on four translation contexts, the performances on bi-context are better than prefix and suffix, and prefix and suffix are better than zero-context. This finding shows that more context information can help to reduce the uncertainty of human desired words.\nComparison with baselines The TRANS-PE method in previous works is more sensitive to the position of human input. The statistical results shows that the averaged distances in the original sentence between the prediction words and translation contexts are various for different translation contexts, which are 7.4, 6.5, 14.1, and 3.2 for prefix, suffix, zero-context, and bi-context, respectively. When the desired words are much closer to the context, TRANS-PE can achieve better performances. Moreover, TRANS-PE can achieve more than 80 accuracy scores when the prediction word is the next word of the given prefix, however, its performance drops significantly when the word is not necessarily conjunct to the prefix. We can also find that TRANS-NPE, which removes the position information of target words, achieves better overall performances compared with TRANS-PE.\nIn contrast, the performance of TRANSTABLE is less affected by the position of the prediction words, which is demonstrated by the low variances on both tasks in Table 2. The results of TRANSTABLE have also surprised us, which achieves more than 41 accuracy scores on the Zh⇒En task. This observation shows the importance of alignment and the potential of statistical models. Compared with the results on the Zh⇒En task, the overall accu-\nracy on En⇒Zh task is much lower, likely due to that the number of valid words after filtered by the human input on Chinese is much more than that on English. Therefore, it is easier for TRANSTABLE to determine the human desired words in English."
    }, {
      "heading" : "6.2 Robustness on Noisy Contexts",
      "text" : "In this work, the translation contexts are simulated using the references. However, in real-world scenarios, translation contexts may not be perfect, i.e., some words in the translation contexts may be incorrect. In this section, we evaluate the robustness of our model on noisy contexts. We first use the translation table constructed by TRANSTABLE to find some target words that share the same source words with the original target words, and then use those found words as noise tokens.\nThe robustness results are shown in Figure 4. For all the translation context types except for zerocontext, the performance drops slowly when the percentage of noise tokens increases. However, even with 80% words in the context, the performance of WPM-JOINT outperforms the case of zero-context, which shows that our WPM-JOINT method is noise tolerant."
    }, {
      "heading" : "6.3 Discussion",
      "text" : "In this work, we formalize the task as a classification problem. However, the generation formalization also deserves to be explored in the future. For example, the generation may happen in two circumstances: word-level completion based on\nsubwords, and phrase-level completion. In the first case, although the autocompletion service provided for human translators is word-level, in the internal system we can generate a sequence of subwords (Sennrich et al., 2015) that satisfy the human typed characters, and provide human translators with the merged subwords. This subword sequence generation can significantly alleviate the OOV issue in the word-level autocompletion. In the phrase-level autocompletion case, if we can predict more than one desired words, the translation efficiency and experience may be improved further. We would like to leave it as future work.\nIt is also worth noting that we did not conduct human studies in this work. We think evidences in previous work can already prove the effectiveness of word-level autocompletion when assisting human translators. For example, TransType (Langlais et al., 2000) is a simple rule-based tool that only considers the prefix context, but the majority of translators said that TransType improved their typing speed a lot. Huang et al. (2015) hired 12 professional translators and systematically evaluate their word autocompletion tool based on zero-context. Experiments show that the more keystrokes are reduced, the more time can be saved for translators. Since the prediction accuracy is highly correlated with the keystrokes, we think higher accuracy will make translators more productive. That is the main reason that we use accuracy to automatically evaluate the model performance. Besides, the automatic evaluation metric also makes the GWLAN task easier to follow."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose a General Word-Level AutocompletioN (GWLAN) task for computer-aided translation (CAT). In our setting, we relax the strict constraints on the translation contexts in previous work, and abstract four most general translation contexts used in real-world CAT scenarios. We propose two approaches to address the variety of context types and weak position information issues in GWLAN. To support automatic evaluation and to ensure a convenient and fair comparison among different methods, we construct a benchmark for the task. Experiments on this benchmark show that our method outperforms baseline methods by a large margin on four datasets. We believe that this benchmark to be released will push forward future research in CAT."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank three anonymous reviewers for their invaluable discussions on this work. The corresponding is Lemao Liu."
    } ],
    "references" : [ {
      "title" : "Casmacat: A computer-assisted translation workbench",
      "author" : [ "Vicent Alabau", "Christian Buck", "Michael Carl", "Francisco Casacuberta", "Mercedes Garcı́a-Martı́nez", "Ulrich Germann", "Jesús González-Rubio", "Robin Hill", "Philipp Koehn", "Luis A Leiva" ],
      "venue" : null,
      "citeRegEx" : "Alabau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alabau et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical approaches to computer-assisted translation",
      "author" : [ "Sergio Barrachina", "Oliver Bender", "Francisco Casacuberta", "Jorge Civera", "Elsa Cubel", "Shahram Khadivi", "Antonio Lagarda", "Hermann Ney", "Jesús Tomás", "Enrique Vidal" ],
      "venue" : null,
      "citeRegEx" : "Barrachina et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Barrachina et al\\.",
      "year" : 2009
    }, {
      "title" : "Primt: A pickrevise framework for interactive machine translation",
      "author" : [ "Shanbo Cheng", "Shujian Huang", "Huadong Chen", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical phrase-based model for statistical machine translation",
      "author" : [ "David Chiang." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 263– 270.",
      "citeRegEx" : "Chiang.,? 2005",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2005
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Training neural machine translation to apply terminology constraints",
      "author" : [ "Georgiana Dinu", "Prashant Mathur", "Marcello Federico", "Yaser Al-Onaizan." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages",
      "citeRegEx" : "Dinu et al\\.,? 2019",
      "shortCiteRegEx" : "Dinu et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML), pages 1243–1252.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "The efficacy of human post-editing for language translation",
      "author" : [ "Spence Green", "Jeffrey Heer", "Christopher D Manning." ],
      "venue" : "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI), pages 439–448.",
      "citeRegEx" : "Green et al\\.,? 2013",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2013
    }, {
      "title" : "Human effort and machine learnability in computer aided translation",
      "author" : [ "Spence Green", "Sida I Wang", "Jason Chuang", "Jeffrey Heer", "Sebastian Schuster", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Green et al\\.,? 2014",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural machine translation decoding with terminology constraints",
      "author" : [ "Eva Hasler", "Adrià de Gispert", "Gonzalo Iglesias", "Bill Byrne." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Hasler et al\\.,? 2018",
      "shortCiteRegEx" : "Hasler et al\\.",
      "year" : 2018
    }, {
      "title" : "Lexically constrained decoding for sequence generation using grid beam search",
      "author" : [ "Chris Hokamp", "Qun Liu." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1535–1546.",
      "citeRegEx" : "Hokamp and Liu.,? 2017",
      "shortCiteRegEx" : "Hokamp and Liu.",
      "year" : 2017
    }, {
      "title" : "Improved lexically constrained decoding for translation and monolingual rewriting",
      "author" : [ "J. Edward Hu", "Huda Khayrallah", "Ryan Culkin", "Patrick Xia", "Tongfei Chen", "Matt Post", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Transmart: a practical interactive machine translation system",
      "author" : [ "Guoping Huang", "Lemao Liu", "Xing Wang", "Longyue Wang", "Huayang Li", "Zhaopeng Tu", "Chengyan Huang", "Shuming Shi." ],
      "venue" : "arXiv preprint arXiv.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "A new input method for human translators: integrating machine translation effectively and imperceptibly",
      "author" : [ "Guoping Huang", "Jiajun Zhang", "Yu Zhou", "Chengqing Zong." ],
      "venue" : "Proceedings of the International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Moon ime: neural-based chinese pinyin aided input method with customizable association",
      "author" : [ "Yafang Huang", "Zuchao Li", "Zhuosheng Zhang", "Hai Zhao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL): Sys-",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Negative lexically constrained decoding for paraphrase generation",
      "author" : [ "Tomoyuki Kajiwara." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 6047– 6052.",
      "citeRegEx" : "Kajiwara.,? 2019",
      "shortCiteRegEx" : "Kajiwara.",
      "year" : 2019
    }, {
      "title" : "Neural interactive translation prediction",
      "author" : [ "Rebecca Knowles", "Philipp Koehn." ],
      "venue" : "Proceedings of the Association for Machine Translation in the Americas (AMTA), pages 107–120.",
      "citeRegEx" : "Knowles and Koehn.,? 2016",
      "shortCiteRegEx" : "Knowles and Koehn.",
      "year" : 2016
    }, {
      "title" : "Statistical machine translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Koehn.,? 2009",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2009
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz J. Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Transtype: a computer-aided translation typing system",
      "author" : [ "Philippe Langlais", "George Foster", "Guy Lapalme." ],
      "venue" : "ANLP-NAACL 2000 Workshop: Embedded Machine Translation Systems.",
      "citeRegEx" : "Langlais et al\\.,? 2000",
      "shortCiteRegEx" : "Langlais et al\\.",
      "year" : 2000
    }, {
      "title" : "Language input architecture for converting one text form to another text form with modeless entry",
      "author" : [ "Kai-fu Lee", "Zheng Chen", "Jian Han." ],
      "venue" : "US Patent 7,165,019.",
      "citeRegEx" : "Lee et al\\.,? 2007",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation with noisy lexical constraints",
      "author" : [ "Huayang Li", "Guoping Huang", "Deng Cai", "Lemao Liu." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), 28:1864–1874.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Interactive neural machine translation",
      "author" : [ "Álvaro Peris", "Miguel Domingo", "Francisco Casacuberta." ],
      "venue" : "Computer Speech & Language, 45:201–220.",
      "citeRegEx" : "Peris et al\\.,? 2017",
      "shortCiteRegEx" : "Peris et al\\.",
      "year" : 2017
    }, {
      "title" : "Fast lexically constrained decoding with dynamic beam allocation for neural machine translation",
      "author" : [ "Matt Post", "David Vilar." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Post and Vilar.,? 2018",
      "shortCiteRegEx" : "Post and Vilar.",
      "year" : 2018
    }, {
      "title" : "INMT: Interactive neural machine translation prediction",
      "author" : [ "Sebastin Santy", "Sandipan Dandapat", "Monojit Choudhury", "Kalika Bali." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International",
      "citeRegEx" : "Santy et al\\.,? 2019",
      "shortCiteRegEx" : "Santy et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Code-switching for enhancing NMT with pre-specified translation",
      "author" : [ "Kai Song", "Yue Zhang", "Heng Yu", "Weihua Luo", "Kun Wang", "Min Zhang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Lexically constrained neural machine translation with levenshtein transformer",
      "author" : [ "Raymond Hendy Susanto", "Shamil Chollampatt", "Liling Tan." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages",
      "citeRegEx" : "Susanto et al\\.,? 2020",
      "shortCiteRegEx" : "Susanto et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Spanam and engspan: machine translation at the pan american health organization",
      "author" : [ "Muriel Vasconcellos", "Marjorie León." ],
      "venue" : "Computational Linguistics (CL), 11(2-3):122–136.",
      "citeRegEx" : "Vasconcellos and León.,? 1985",
      "shortCiteRegEx" : "Vasconcellos and León.",
      "year" : 1985
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Touch editing: A flexible one-time interaction approach for translation",
      "author" : [ "Qian Wang", "Jiajun Zhang", "Lemao Liu", "Guoping Huang", "Chengqing Zong." ],
      "venue" : "Proceedings of the Conference of the AsiaPacific Chapter of the Association for Computa-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Correct-andmemorize: Learning to translate from interactive revisions",
      "author" : [ "Rongxiang Weng", "Hao Zhou", "Shujian Huang", "Lei Li", "Yifan Xia", "Jiajun Chen." ],
      "venue" : "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages",
      "citeRegEx" : "Weng et al\\.,? 2019",
      "shortCiteRegEx" : "Weng et al\\.",
      "year" : 2019
    }, {
      "title" : "Models and inference for prefix-constrained machine translation",
      "author" : [ "Joern Wuebker", "Spence Green", "John DeNero", "Saša Hasan", "Minh-Thang Luong." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages",
      "citeRegEx" : "Wuebker et al\\.,? 2016",
      "shortCiteRegEx" : "Wuebker et al\\.",
      "year" : 2016
    }, {
      "title" : "Balancing quality and human involvement: An effective approach to interactive neural machine translation",
      "author" : [ "Tianxiang Zhao", "Lemao Liu", "Guoping Huang", "Zhaopeng Tu", "Huayang Li", "Yingling Liu", "Guiquan Liu", "Shuming Shi." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al.",
      "startOffset" : 18,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al.",
      "startOffset" : 18,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al.",
      "startOffset" : 18,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al.",
      "startOffset" : 18,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 247
    }, {
      "referenceID" : 9,
      "context" : "Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 247
    }, {
      "referenceID" : 17,
      "context" : "Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 247
    }, {
      "referenceID" : 25,
      "context" : "Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 247
    }, {
      "referenceID" : 13,
      "context" : "This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 30,
      "context" : "various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 164
    }, {
      "referenceID" : 33,
      "context" : "various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 164
    }, {
      "referenceID" : 32,
      "context" : "various efficient interaction ways of CAT have emerged (Vasconcellos and León, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 177
    }, {
      "referenceID" : 25,
      "context" : "A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "plete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 125
    }, {
      "referenceID" : 35,
      "context" : "plete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 125
    }, {
      "referenceID" : 34,
      "context" : "For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al.",
      "startOffset" : 83,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al.",
      "startOffset" : 83,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 28,
      "context" : "Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 16,
      "context" : "Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 11,
      "context" : ", lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : ", lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : ", lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : ", lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD.",
      "startOffset" : 49,
      "endOffset" : 85
    }, {
      "referenceID" : 27,
      "context" : "Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD.",
      "startOffset" : 49,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Others Our work may also be related to previous works in input method editors (IME) (Huang et al., 2018; Lee et al., 2007).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Others Our work may also be related to previous works in input method editors (IME) (Huang et al., 2018; Lee et al., 2007).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "beneficial for improving input efficiency (Langlais et al., 2000).",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "There also exist some works that consider the human typed sequences (Huang et al., 2015; Santy et al., 2019), but they only consider a specific type of translation contexts.",
      "startOffset" : 68,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "There also exist some works that consider the human typed sequences (Huang et al., 2015; Santy et al., 2019), but they only consider a specific type of translation contexts.",
      "startOffset" : 68,
      "endOffset" : 108
    }, {
      "referenceID" : 31,
      "context" : "Inspired by the attention-based architectures (Vaswani et al., 2017; Devlin et al., 2019)3, we",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the attention-based architectures (Vaswani et al., 2017; Devlin et al., 2019)3, we",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "case, although the autocompletion service provided for human translators is word-level, in the internal system we can generate a sequence of subwords (Sennrich et al., 2015) that satisfy the human typed characters, and provide human translators with the merged subwords.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "For example, TransType (Langlais et al., 2000) is a simple rule-based tool that only considers the prefix context, but the majority of translators said that TransType improved their typ-",
      "startOffset" : 23,
      "endOffset" : 46
    } ],
    "year" : 2021,
    "abstractText" : "Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark1 to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.",
    "creator" : "LaTeX with hyperref"
  }
}