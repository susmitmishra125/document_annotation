{
  "name" : "2021.acl-long.465.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting",
    "authors" : [ "Yi Cheng", "Siyao Li", "Bang Liu", "Ruihui Zhao", "Sujian Li", "Chenghua Lin", "Yefeng Zheng" ],
    "emails" : [ "yefengzheng}@tencent.com,", "siyaol@andrew.cmu.edu,", "bang.liu@umontreal.ca,", "lisujian@pku.edu.cn,", "c.lin@shef.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5968–5978\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5968"
    }, {
      "heading" : "1 Introduction",
      "text" : "The task of Difficulty-Controllable Question Generation (DCQG) aims at generating questions with required difficulty levels and has recently attracted researchers’ attention due to its wide application, such as facilitating certain curriculum-learningbased methods for QA systems (Sachan and Xing, 2016) and designing exams of various difficulty levels for educational purpose (Kurdi et al., 2020).\nCompared to previous QG works which control the interrogative word (Zi et al., 2019; Kang et al., 2019) or the context of a question (Liu et al., 2020, 2019a), few works have been conducted on difficulty control, as it is hard to formally define the difficulty of a question. To the best of our knowledge, Gao et al. (2019) is the only previous work of DCQG for free text, and defines question difficulty as whether a QA model can correctly answer it.\n∗Corresponding author.\nThis definition gives only two difficulty levels and is mainly empirically driven, lacking interpretability for what difficulty is and how difficulty varies.\nIn this work, we redefine the difficulty level of a question as the number of inference steps required to answer it, which reflects the requirements on reasoning and cognitive abilities (Pan et al., 2019). Existing QA systems perform substantially worse in answering multi-hop questions than single-hop ones (Yang et al., 2018), also supporting the soundness of using reasoning hops to define difficulty.\nTo achieve DCQG with the above definition, a QG model should have strong control over the logic and reasoning complexity of generated questions. Graph-based methods are well suited for such logic modelling (Pearl and Paz, 1986; Zhang et al., 2020). In previous QG researches, Yu et al. (2020) and Pan et al. (2020) implemented graph-to-sequence\nframeworks to distill the inner structure of the context, but they mainly used graphs to enhance document representations, rather than to control the reasoning complexity of questions.\nIn this paper, we propose a highly-controllable QG framework that progressively increases difficulties of the generated questions through step-bystep rewriting. Specifically, we first transform a given raw text into a context graph, from which we sample the answer and the reasoning chain for the generated question. Then, we design a question generator and a question rewriter to generate an initial simple question and step-by-step rewrite it into more complex ones. As shown in Fig. 1, “Tom Cruise” is the selected answer, and Q1 is the initial question, which is then adapted into Q2 by adding one more inference step (i.e. N1←N2) in the reasoning chain. That is, it requires to infer “Top Gun” is “the film directed by Tony Scott” before answering Q1. Similarly, we can further increase its difficulty level and step-by-step extend it into more difficult questions (i.e., Q3, Q4 and Q5).\nTo train our DCQG framework, we design effective strategies to automatically construct the training data from existing QA datasets instead of building one from scratch with intensive human efforts. Specifically, we utilize HotpotQA (Yang et al., 2018), a QA dataset where most questions require two inference steps to answer and can be decomposed into two 1-hop questions. Thus, we get the dataset that contains 2-hop questions and their corresponding 1-hop reasoning steps. Having learned how to rewrite 1-hop questions into 2-hop ones with this dataset, our framework can easily extend to the generation of (n+1)-hop questions from n-hop ones only with a small amount of corresponding data, because the rewriting operation follows rather certain patterns regardless of the exact value of n, as shown in Fig. 1.\nExtensive evaluations show that our method can controllably generate questions with required difficulty, and keep competitive question quality at the same time, compared with a set of strong baselines.\nIn summary, our contributions are as follows: • To the best of our knowledge, this is the first\nwork of difficulty-controllable question generation, with question difficulty defined as the inference steps to answer it;\n• We propose a novel framework that achieves DCQG through step-by-step rewriting under the guidance of an extracted reasoning chain;\n• We build a dataset that can facilitate training of rewriting questions into more complex ones, paired with constructed context graphs and the underlying reasoning chain of the question."
    }, {
      "heading" : "2 Related Work",
      "text" : "Deep Question Generation Most of the previous QG researches (Zhou et al., 2017; Pan et al., 2019; Liu et al., 2020) mainly focused on generating single-hop questions like the ones in SQuAD (Rajpurkar et al., 2016). In the hope that AI systems could provoke more in-depth interaction with humans, deep question generation aims at generating questions that require deep reasoning. Many recent works attempted to conquer this task with graph-based neural architectures. Talmor and Berant (2018) and Kumar et al. (2019) generated complex questions based on knowledge graphs, but their methods could not be directly applied to QG for free text, which lacks clear logical structures. In sequential question generation, Chai and Wan (2020) used a dual-graph interaction to better capture context dependency. However, they considered all the tokens as nodes, which led to a very complex graph. Yu et al. (2020) tried to generate multi-hop questions from free text with the help of entity graphs constructed by external tools. Our work shares a similar setting with Yu et al. (2020), and we further explore the problem of how to generate deep questions in a more controllable paradigm.\nDifficulty-Controllable Question Generation DCQG is a relatively new task. Gao et al. (2019) classified questions as easy or hard according to whether they could be correctly answered by a BERT-based QA model, and controlled the question difficulty by modifying the hidden states before decoding. Another research on QG for knowledge graphs (Kumar et al., 2019) estimated the question difficulty based on popularity of the named entity. They manipulated the generation process by incorporating the difficulty level into the input embedding of the Transformer-based decoder. In our work, we control the question difficulty based on the number of its reasoning hops, which is more explainable.\nQuestion Rewriting It is another emerging trend in the recent researches, demonstrating benefits to both QG and QA tasks. With rewriting, QG models produced more complex questions by incorporating more context information into simple questions\n(Elgohary et al., 2019; Vakulenko et al., 2020), and QA pipelines could also decompose the original complex question into multiple shorter questions to improve model performance (Min et al., 2019; Khot et al., 2020)."
    }, {
      "heading" : "3 Method",
      "text" : "Given input context text C and a specific difficulty level d, our objective is to generate a (question, answer) pair (Q,A), where A is a sub-span of C and Q requires d-hop reasoning to answer. Fig. 2 and Algorithm 1 give an overview of our proposed framework. First, we construct a context graph GCG corresponding to the given context, from which a subgraph GT is selected to serve as the reasoning chain of the generated question. Next, with the reasoning chain and other contextual information as input, a question generator (QGInitial) produces an initial simple question Q1. Then, Q1 is fed to a question rewriting module (QGRewrite), which iteratively rewrites it into a more complex question Qi (i = 2, 3, . . . , d). In what follows, we will introduce the whole generation process in more details.\nContext Graph Construction We follow the method proposed by Fan et al. (2019) to build the context graph GCG. Specifically, we first apply open information extraction (Stanovsky et al., 2018) to extract 〈subject, relation, object〉 triples from context sentences. Each triple is then trans-\nformed into two nodes connected with a directed edge, like A Perfect Murder is−→ a 1998 American crime film in Fig. 2. The two nodes respectively represent the subject and object, and the edge describes their relation. Coreference resolution (Lee et al., 2017) is applied to merge nodes referring to the same entity. For instance, A Perfect Murder is merged with It in Fig. 2.\nReasoning Chain Selection With the context graph constructed, we sample a connected subgraph GT consisting of d+ 1 nodes from it to serve as the reasoning chain of the generated question. A node N0 is first sampled as the answer of the question, if it is, or linked with, a named entity that has more than one node degree. Next, we extract from GCG a maximum spanning tree GL, with N0 as its root node, e.g., the tree structure shown in Fig. 1. GCG is temporarily considered as an undirected graph at this step. We then prune GL into GT to keep only d + 1 nodes. During pruning, we consider the sentence position where each node is extracted in order to make the reasoning chain relevant to more context. In the following, we will denote a node in GT asNi (i = 0, 1, . . . , d), where each node is subscripted by preorder traversal of GT , and NP (i) as the parent of Ni.\nStep-by-step Question Generation Our stepby-step QG process is described at lines 5-11 in Algorithm 1. The following notations are defined for clearer illustration:\nAlgorithm 1 Procedure of Our DCQG Framework Input: context C, difficulty level d Output: (Q,A) 1: GCG ← BuildCG(C) 2: N0 ← SampleAnswerNode(GCG) 3: GL ←MaxTree(GCG,N0) 4: GT ← Prune(GL, d) 5: forNi in PreorderTraversal(GT ) do 6: if i = 0 then continue 7: NP (i) = Parent(Ni) 8: Si = ContextSentence(C,Ni,NP (i))\n9: Ri ← { Bridge ifNi=FirstChild(NP (i)) Intersection else\n10: Qi ← { QGInitial(Ni,NP (i),Si) if i = 1 QGRewrite(Qi−1,Ni,NP (i),Si,Ri) else 11: end for 12: return (Qd,N0)\n• Qi (i = 1, 2, . . . , d) represents the question generated at each step, whereQd is the final question Q, and Qi+1 is rewritten from Qi by adding one more hop of reasoning. • Si represents the context sentence from which we extract the triple Ni → NP (i). • Ri is the rewriting type of Qi (i = 2, 3, . . . , d). Specifically, we consider two types of rewriting patterns in this work: Bridge and Intersection. As shown in Fig. 1, Bridge-style rewriting replaces an entity with a modified clause, while Intersection adds another restriction to an existing entity in the question. These two types can be distinguished by whether Ni is the first child of its parent node, i.e., whether its parent node has already been rewritten once in Bridge style. To generate the final question with the required difficulty level d, we first use a question generator QGInitial to generate an initial simple question based on N1, N0, and the corresponding context sentence S1. Then, we repeatedly (for d− 1 times) use QGRewrite to rewrite questionQi−1 into a more complex one Qi, based on node Ni and its parent node NP (i), context sentence Si, and the rewriting typeRi (i = 2, 3, . . . , d). Formally, the generation process of QGInitial and the rewriting process of QGRewrite can be defined as:\nQ1 = arg max Q̄1 P (Q̄1|N1,N0,S1)\nQi = arg max Q̄i P (Q̄i|Qi−1,Ni,NP (i),Si,Ri)\nwhere i = 2, 3, . . . , d.\nAlgorithm 2 Procedure of Data Construction Input: context C = {P1,P2}, QA pair (Q2,A2), support-\ning facts F Output: R1, (Q1,A1),S1,S2, {N0, E1,N1, E2,N2} 1: R1 ← TypeClassify(Q2) 2: ifR1 /∈ {Bridge, Intersection} then return 3: subq1, subq2 ← DecompQ(Q2) 4: suba1, suba2 ← QA(subq1),QA(subq2)\n5: Q1,A1 ← { subq2, suba2 if A2 = suba2 subq1, suba1 else\n6: S1,S2 ← { F ∩ P1,F ∩ P2 ifQ1 concerns P1 F ∩ P2,F ∩ P1 else 7: N2 ← FindNode(A2) 8: N0, E1,N1, E2 ←Match(subq1, subq2)\nIn our implementation, both QGInitial and QGRewrite are initialized with the pre-trained GPT2-small model (Radford et al., 2019), and then fine-tuned on our constructed dataset (see Sec. 4). The encoder of QGRewrite, as illustrated in Fig. 2, is similar to Liu et al. (2020). IfNi points toNP (i), then the input sequence is organized in the form of “〈bos〉 Si 〈nodeC〉 Ni 〈edge〉 Ei 〈nodeP〉 NP (i) 〈type〉 Ri 〈subq〉 Qi−1 〈eos〉”, where Ei is the edge from Ni to NP (i). The positions of “〈nodeC〉 Ni” and “〈nodeP〉 NP (i)” will be exchanged if NP (i) points to Ni. As for QGInitial, its input is organized in the same way except without “〈type〉 Ri 〈subq〉 Qi−1”.\nThe segment embedding layer is utilized to identify different segments. For those parts in Si and Qi−1 that are the same as, or refer to the same entity as NP (i), we replace their segment embeddings with the one of NP (i), considering that the parent node of Ni plays an important role in denoting what to ask about, or which part to rewrite, as shown in Fig. 1."
    }, {
      "heading" : "4 Automatic Dataset Construction",
      "text" : "Manually constructing a new dataset for our task is difficult and costly. Instead, we propose to automatically build a dataset from existing QA datasets without extra human annotation. In our work, the training data is constructed from HotpotQA (Yang et al., 2018), in which every context C consists of two paragraphs {P1,P2}, and most of the questions require two hops of reasoning, each concerning one paragraph. HotpotQA also annotates supporting facts F , which are the part of the context most relevant to the question. In addition to the information already available in HotpotQA, we also need the following information to train\nQGInitial and QGRewrite: i) (Q1,A1), the simple initial question and its answer, which are used to train QGInitial; ii) R2, the type of rewriting from Q1 to Q2; iii) {N0,N1,N2}, the reasoning chain of Q2; and iv) Si (i = 1, 2), the context sentences where we extract N0, N1 and N2.\nAlgorithm 2 describes our procedure to obtain the above information. The construction process is facilitated with the help of a reasoning type classifier (TypeClassify) and a question decomposer (DecompQ), referring to Min et al. (2019). For each question in HotpotQA (i.e. Q2), we first distinguish its reasoning type, and filter out those that are not Bridge and Intersection. The reasoning type here corresponds to the rewriting type Ri. Then, DecompQ decomposes Q2 into two subquestions, subq1 and subq2, based on span prediction and linguistic rules. For example, the Q2 in Fig. 2 will be decomposed into subq1=“To which film A Perfect Murder was a modern remake?”, and subq2=“Who directed Dial M for Murder?”. After that, an off-the-shelf single-hop QA model (Min et al., 2019) is utilized to acquire the answer of the two sub-questions, which should be “Dial M for Murder” and “Alfred Hitchcock” in the example.\nAs for Q1, it is one of the sub-questions. When Q2 is of the Intersection type, Q1 can be either subq1 or subq2. For the Bridge type, it is the subquestion that shares the same answer asA2. For the example above, Q1 is subq2 because suba2 = A2. The context sentence Si is supposed to provide supporting facts contained in the paragraph F that concerns Qi (i = 1, 2). For the reasoning chain, it is selected from the local context graph by first locating N2 and then finding N0,N1 through text matching with the two sub-questions."
    }, {
      "heading" : "5 Experiments",
      "text" : "In the following experiments, we mainly evaluate the generation results of our proposed method when required to produce 1-hop and 2-hop questions, denoted as Ours1-hop and Ours2-hop. In Sec. 5.2, we compare our method with a set of strong baselines using both automatic and human evaluations on question quality. In Sec. 5.3, we provide controllability analysis by manually evaluating their difficulty levels and testing the performance of QA systems in answering questions generated by different methods. In Sec. 5.4, we test the effect of our generated QA pairs on the performance of a multi-hop QA model in a data augmentation setting.\nIn Sec. 5.5, we further analyze the extensibility of our method, i.e., its potential in generating questions that require reasoning of more than two hops. Our code and constructed dataset have been made publicly available to facilitate future research.1"
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Datasets The constructed dataset described in Sec. 4 consists of 57,397/6,072/6,072 samples for training/validation/test. For context graph construction, we use the coreference resolution toolkit from AllenNLP 1.0.0 (Lee et al., 2017) and the open information extraction toolkit provided by the Plasticity developer API.2 The question decomposer and the reasoning type classifier follow the implementations of Min et al. (2019).\nBaselines The following baselines are trained to generate the 2-hop questions in the datasets: • NQG++ (Zhou et al., 2017) is a seq2seq model\nbased on bi-directional Gate Recurrent Unit (GRU), with features enriched by answer position and lexical information. • ASs2s (Kim et al., 2019) is a seq2seq model based on Long Short-term Memory (LSTM), which separately encodes answer and context. • SRL-Graph and DP-Graph (Pan et al., 2020) are two state-of-the-art QG systems. They encode graph-level and document-level information with an attention-based Graph Neural Network (GNN) and a bi-directional GRU, respectively. SRL-Graph constructs the semantic graph by semantic role labelling, and DP-Graph by dependency parsing. • GPT2 is a vanilla GPT2-based QG model. Its input is the concatenation of context and sampled answer. The position where the answer appears in the context segment is denoted in the segment embedding layer.\nImplementation Details The baseline models are trained to directly produce the 2-hop questions, while QGInitial and QGRewrite are respectively trained to generate 1-hop questions and rewrite 1-hop ones into 2-hop. QGInitial, QGRewrite, and GPT2 are initialized with the GPT2-small model from the HuggingFace Transformer library (Wolf et al., 2019), and fine-tuned for 8, 10, and 7 epochs, respectively, with batch size of 16. We apply top-p nucleus sampling with p = 0.9 during decoding.\n1 https://tinyurl.com/19esunzz 2 https://www.plasticity.ai/\nAdamW (Loshchilov and Hutter, 2017) is used as optimizer, with the initial learning rate set to be 6.25×10−5 and adaptively decays during training. For DP-Graph, we use their released model and code to perform the experiment. For the other three baselines, we directly refer to the experiment results reported in Pan et al. (2020). The performances of these baselines are compared under the same setting as in Pan et al. (2020), where each context is abbreviated to only include the supporting facts and the part that overlaps with the question. More implementation details can be found in our code and the supplementary materials."
    }, {
      "heading" : "5.2 Evaluation of Question Quality",
      "text" : "Automatic Evaluation The automatic evaluation metrics are BLEU3, BLEU4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015), which measure the similarity between the generation results and the reference questions in terms of n-grams. As the four baselines are trained to generate 2-hop questions only, we only compare them with Ours2-hop. As shown in Table 1, we can see that Ours2-hop and GPT2 perform consistently better than the others. Though the performances of Ours2-hop and GPT2 are close in terms of automatic metrics, we observe that the questions generated by Ours2-hop are usually more well-formed, concise and answerable, as illustrated in Table 2. These advantages cannot be reflected through automatic evaluation.\nHuman Evaluation We randomly sample 200 questions respectively from DP-Graph, GPT2, Ours1-hop, Ours2-hop, as well as the reference 1- hop and 2-hop questions in the constructed dataset (Gold1-hop, Gold2-hop). The questions are manually evaluated by eight human annotators, who are graduate students, majoring in English Literature, Computer Science, or Electronic Engineering. They voluntarily offer to help without being com-\npensated in any form. Before annotation, they are informed of the detailed annotation instruction with clear scoring examples. The generated questions are evaluated in the following four dimensions: • Well-formed: It checks whether a question is se-\nmantically correct. Annotators are asked to mark a question as yes, acceptable, or no. Acceptable is selected if the question is not grammatically correct, but its meaning is still inferrable. • Concise: It checks whether the QG models are overfitted, generating questions with redundant modifiers. The question is marked as yes if no single word can be deleted, acceptable if it is a little lengthy but still in a natural way, and no if it is abnormally verbose. • Answerable: It checks whether a question is answerable according to the given context. The anonnotion is either yes or no. • Answer Matching: It checks whether the given answer is the correct answer to the question. The anonnotion is either yes or no. The results are shown in Table 3. Overall, we can see that Ours2-hop performs consistently better than DP-Graph and GPT2 across all metrics and comparable to the hand-crafted reference questions. Our method performs especially well in terms of concise, even better than the reference questions. For reference, the average word number of the questions generated by DP-Graph, GPT2, Ours2-hop, and Gold2-hop are 19.32, 19.26, 17.18, 17.44, respectively. It demonstrates that the enriched graph information and our multi-stage rewriting mechanism indeed enhance the question structure and content. In comparison, we find that the questions generated by the two baselines tend to unreasonably pile too many modifiers and subordinate clauses.\nAs for the 1-hop questions, Ours1-hop performs well in terms of answerable and answer matching, but not so competitive in terms of well-formed, mainly due to the limitation of its training data. As the 1-hop reference questions (Gold1-hop) are automatically decomposed from the hand-crafted 2-hop questions, a significant portion (44%) of them have some grammatical errors, but most of them are still understandable despite that."
    }, {
      "heading" : "5.3 Controllability Analysis",
      "text" : "Human Evaluation of Controllability For controllability analysis, we manually evaluate the numbers of inference steps involved in generated questions. DP-Graph and GPT2 are also evaluated for comparison. The results are shown in Table 4. 70.65% of Ours1-hop require one step of inference and 67.74% of Ours2-hop require two steps, proving that our framework can successfully control the number of inference steps of most generated questions. In comparison, DP-Graph and GPT2 are not difficulty-aware and their generated questions are more scattered in difficulty levels.\nDifficulty Assessment with QA Systems For further assessment of question difficulty, we test the performance of QA models in answering questions generated by different models. Specifically, we utilize two off-the-shelf QA models provided by the HuggingFace Transformer library (Wolf et al., 2019), which are respectively initialized with\nBERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b), and then fine-tuned on SQuAD (Rajpurkar et al., 2016). We select those generated questions that are ensured to be paired with correct answers by the human evaluation described in Sec. 5.2, and test the performance of two QA models in answering them. The evaluation metrics include Exact Match (EM) and F1.\nThe results are shown in Table 5. We can see that questions generated by Ours2-hop are more difficult than Ours1-hop not only to humans (requiring more hops of reasoning), but also to the state-ofthe-art QA models. In comparison, with a more scattered mix of 1-hop and 2-hop questions, the performances on DP-Graph and GPT2 are between Ours1-hop and Ours2-hop. This result demonstrates that our method can controllably generate questions of different difficulty levels for QA systems and that inference steps can effectively model the question difficulty."
    }, {
      "heading" : "5.4 Boosting Multi-hop QA Performance",
      "text" : "We further evaluate whether the generated QA pairs can boost QA performance through data augmentation. Specifically, we heuristically sample the answers and reasoning chains from the context graphs in our constructed dataset to generate 150,305 twohop questions. As a comparison, we utilize GPT2 to generate the same amount of data with the same sampled answers and contextual sentences. Some low-quality questions are filtered out if their word\n0.0 2.5 5.0 7.5 10 Number of Augmented Samples\n(in thousands)\n0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 E M\n100%HotpotQA + Ours 100%HotpotQA + GPT2 25%HotpotQA + Ours 25%HotpotQA + GPT2\n0.0 2.5 5.0 7.5 10 Number of Augmented Samples\n(in thousands)\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nF1\n100%HotpotQA + Ours 100%HotpotQA + GPT2 25%HotpotQA + Ours 25%HotpotQA + GPT2\nFigure 3: Performance of the DistilBERT-based QA system on HotpotQA, augmented with different quantities of generated data.\ncounts are not between 6∼30 (4.7% for ours and 9.2% for GPT2), or the answers directly appear in the questions (2.7% for ours and 2.4% for GPT2). Finally, we randomly sample 100,000 QA pairs and augment the HotpotQA dataset with them.\nA DistilBERT-based (Sanh et al., 2019) QA model is implemented. It takes as input the concatenation of context and question to predict the answer span. To speed up the experiment, we only consider those necessary supporting facts as the question answering context. During training, the original samples from HotpotQA are oversampled to ensure that they are at least 4 times as the generated data. We use Adam (Kingma and Ba, 2015) as the optimizer, with the mini-batch size of 32. The learning rate is initially set to 3×10−5 and adaptively decays during training. The configurations are the same in all the QA experiments, except that the training datasets are different combinations of HotpotQA and the generated data. The validation and test sets are the same as those in HotpotQA.\nWe test the impact of the generated data under both high-resource (using the whole training set of HotpotQA) and low-resource settings (using only 25% of the data randomly sampled from HotpotQA). Fig. 3 compares the QA performance, augmented with different quantities of the data generated by our method and by GPT2, respectively. We can see that under both settings, our method achieves better performance than GPT2. Under the low-resource setting, performance boost achieved by our generated data is more significant and obviously better than that of GPT2. The performance of the QA model steadily improves when the training dataset is augmented with more data. EM and F1 of the QA model are improved by 2.56% and 1.69%, respectively, when 100,000 samples of our generated data are utilized.\nContext Reasoning QG Process\nHollywood Arms is a play by Carrie Hamilton and Carol Burnett. It ran at the Goodman Theatre and on Broadway in 2002…\nQ : What was run at the Goodman Theatre in 2002?\nQ : What play by Carrie Hamilton was run at the Goodman Theatre in 2002?\nReasoning Chain QG Process\nQ : Which actor who starred in Top Gun?\nQ : Which star of Top Gun was also in the movie Rain Man?\nQ : Which star of Top Gun was also in the movie directed by Barry Levinson?\nQ : Which actor who starred in Top Gun?\nQ : What actor starred in the film that was directed by Tony Scott?\nQ : What actor starred in the film that was directed by Tony Scott and was released in 1986?\n1\n1\n2\n3\n2\n3\nTop Gun\nTom Cruise\nRain Man\nstarredstarred\nBarry Levinson\ndirected\nTop Gun\nTom Cruise\nTony Scott\ndirected\nstarred\na 1986 action film\nis\nFigure 4: Two examples of generating three-hop questions based on the extracted reasoning chains."
    }, {
      "heading" : "5.5 More-hop Question Generation",
      "text" : "To analyze the extensibility of our method, we experiment with the generation of questions that are more than 2-hop, by repeatedly using QGRewrite to increase question difficulty. Fig. 4 shows two examples of 3-hop question generation process. The two intermediate questions and the corresponding reasoning chains are also listed for reference.\nWe can see that the intermediate questions, serving as springboards, are effectively used by QGRewrite to generate more complex questions. With the training data that only contains 1-hop and 2-hop questions, our framework is able to generate some high-quality 3-hop questions, demonstrating the extensibility of our framework. It can be expected that the performance of our model can be further strengthened if a small training set of 3-hop question data is available.\nBesides, it can also be observed that though the contexts and answers of these two questions are the same, two different questions with different underlying logic are generated, illustrating that the extracted reasoning chain effectively controls the question content.\nHowever, when generating questions with more than 3 hops, we find that the question quality drastically declines. The semantic errors become more popular, and some content tend to be unreasonably repeated. It is probably because the input of QGRewrite has become too long to be precisely encoded by the GPT2-small model due to the growing length of the question. It will be our future work to explore how to effectively extend our method to more-hop question generation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it. A step-by-step generation framework was proposed to accomplish this objective, with an input sampler to extract the reasoning chain, a question generator to produce a simple question, and a question rewriter to further adapt it into a more complex one. A dataset was automatically constructed based on HotpotQA to facilitate the research. Extensive evaluations demonstrated that our method can effectively control difficulty of the generated questions, and keep high question quality at the same time."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to Zijing Ou, Yafei Liu and Suyuchen Wang for their helpful comments on this paper."
    } ],
    "references" : [ {
      "title" : "Learning to ask more: Semi-autoregressive sequential question generation under dual-graph interaction",
      "author" : [ "Zi Chai", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 225–237. Association for",
      "citeRegEx" : "Chai and Wan.,? 2020",
      "shortCiteRegEx" : "Chai and Wan.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you unpack that? Learning to rewrite questions-in-context",
      "author" : [ "Ahmed Elgohary", "Denis Peskov", "Jordan L. BoydGraber." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint",
      "citeRegEx" : "Elgohary et al\\.,? 2019",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2019
    }, {
      "title" : "Using local knowledge graph construction to scale seq2seq models to multi-document inputs",
      "author" : [ "Angela Fan", "Claire Gardent", "Chloé Braud", "Antoine Bordes." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Let me know what to ask: Interrogative-word-aware question generation",
      "author" : [ "Junmo Kang", "Haritz Puerto San Roman", "SungHyon Myaeng." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 163–171. Associ-",
      "citeRegEx" : "Kang et al\\.,? 2019",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2019
    }, {
      "title" : "Text modular networks: Learning to decompose tasks in the language of existing models",
      "author" : [ "Tushar Khot", "Daniel Khashabi", "Kyle Richardson", "Peter Clark", "Ashish Sabharwal." ],
      "venue" : "CoRR, abs/2009.00751.",
      "citeRegEx" : "Khot et al\\.,? 2020",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural question generation using answer separation",
      "author" : [ "Yanghoon Kim", "Hwanhee Lee", "Joongbo Shin", "Kyomin Jung." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, pages 6602–6609. AAAI Press.",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Difficulty-controllable multi-hop question generation from knowledge graphs",
      "author" : [ "Vishwajeet Kumar", "Yuncheng Hua", "Ganesh Ramakrishnan", "Guilin Qi", "Lianli Gao", "Yuan-Fang Li." ],
      "venue" : "The 18th International Semantic Web Conference, volume 11778 of",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "A systematic review of automatic question generation for educational purposes",
      "author" : [ "Ghader Kurdi", "Jared Leo", "Bijan Parsia", "Uli Sattler", "Salam Al-Emari." ],
      "venue" : "Int. J. Artif. Intell. Educ., 30(1):121–204.",
      "citeRegEx" : "Kurdi et al\\.,? 2020",
      "shortCiteRegEx" : "Kurdi et al\\.",
      "year" : 2020
    }, {
      "title" : "METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231. Association for Compu-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197. Association for Computational Lin-",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Asking questions the human way: Scalable question-answer generation from text corpus",
      "author" : [ "Bang Liu", "Haojie Wei", "Di Niu", "Haolan Chen", "Yancheng He." ],
      "venue" : "The Web Conference, pages 2032–2043. ACM / IW3C2.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to generate questions by learningwhat not to generate",
      "author" : [ "Bang Liu", "Mingjun Zhao", "Di Niu", "Kunfeng Lai", "Yancheng He", "Haojie Wei", "Yu Xu." ],
      "venue" : "The World Wide Web Conference, pages 1106–1118.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fixing weight decay regularization in Adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "CoRR, abs/1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Multi-hop reading comprehension through question decomposition and rescoring",
      "author" : [ "Sewon Min", "Victor Zhong", "Luke Zettlemoyer", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, Volume",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Recent advances in neural question generation",
      "author" : [ "Liangming Pan", "Wenqiang Lei", "Tat-Seng Chua", "Min-Yen Kan." ],
      "venue" : "CoRR, abs/1905.08949.",
      "citeRegEx" : "Pan et al\\.,? 2019",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic graphs for generating deep questions",
      "author" : [ "Liangming Pan", "Yuxi Xie", "Yansong Feng", "Tat-Seng Chua", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1463–1475. Association",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318. ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Graphoids: Graphbased logic for reasoning about relevance relations or when would x tell you more about y if you already know z",
      "author" : [ "Judea Pearl", "Azaria Paz" ],
      "venue" : "In Advances in Artificial Intelligence II,",
      "citeRegEx" : "Pearl and Paz.,? \\Q1986\\E",
      "shortCiteRegEx" : "Pearl and Paz.",
      "year" : 1986
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Easy questions first? A case study on curriculum learning for question answering",
      "author" : [ "Mrinmaya Sachan", "Eric Xing." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sachan and Xing.,? 2016",
      "shortCiteRegEx" : "Sachan and Xing.",
      "year" : 2016
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Supervised open information extraction",
      "author" : [ "Gabriel Stanovsky", "Julian Michael", "Luke Zettlemoyer", "Ido Dagan." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Stanovsky et al\\.,? 2018",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2018
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "Question rewriting for conversational question answering",
      "author" : [ "Svitlana Vakulenko", "Shayne Longpre", "Zhucheng Tu", "Raviteja Anantha." ],
      "venue" : "arXiv preprint arXiv:2004.14652.",
      "citeRegEx" : "Vakulenko et al\\.,? 2020",
      "shortCiteRegEx" : "Vakulenko et al\\.",
      "year" : 2020
    }, {
      "title" : "CIDEr: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, pages 4566– 4575. IEEE Computer Society.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "HuggingFace’s Transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating multi-hop reasoning questions to improve machine reading comprehension",
      "author" : [ "Jianxing Yu", "Xiaojun Quan", "Qinliang Su", "Jian Yin." ],
      "venue" : "The Web Conference, pages 281–291. ACM / IW3C2.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient probabilistic logic reasoning with graph neural networks",
      "author" : [ "Yuyu Zhang", "Xinshi Chen", "Yuan Yang", "Arun Ramamurthy", "Bo Li", "Yuan Qi", "Le Song." ],
      "venue" : "8th International Conference on Learning Representations. OpenReview.net.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural question generation from text: A preliminary study",
      "author" : [ "Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou." ],
      "venue" : "In Proceedings of 6th CCF International Conference on Natural Language, volume 10619 of Lec-",
      "citeRegEx" : "Zhou et al\\.,? 2017",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "Answer-focused and position-aware neural network",
      "author" : [ "Kangli Zi", "Xingwu Sun", "Yanan Cao", "Shi Wang", "Xiaoming Feng", "Zhaobo Ma", "Cungen Cao" ],
      "venue" : null,
      "citeRegEx" : "Zi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zi et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The task of Difficulty-Controllable Question Generation (DCQG) aims at generating questions with required difficulty levels and has recently attracted researchers’ attention due to its wide application, such as facilitating certain curriculum-learningbased methods for QA systems (Sachan and Xing, 2016) and designing exams of various difficulty levels for educational purpose (Kurdi et al.",
      "startOffset" : 280,
      "endOffset" : 303
    }, {
      "referenceID" : 9,
      "context" : "The task of Difficulty-Controllable Question Generation (DCQG) aims at generating questions with required difficulty levels and has recently attracted researchers’ attention due to its wide application, such as facilitating certain curriculum-learningbased methods for QA systems (Sachan and Xing, 2016) and designing exams of various difficulty levels for educational purpose (Kurdi et al., 2020).",
      "startOffset" : 377,
      "endOffset" : 397
    }, {
      "referenceID" : 34,
      "context" : "Compared to previous QG works which control the interrogative word (Zi et al., 2019; Kang et al., 2019) or the context of a question (Liu et al.",
      "startOffset" : 67,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "Compared to previous QG works which control the interrogative word (Zi et al., 2019; Kang et al., 2019) or the context of a question (Liu et al.",
      "startOffset" : 67,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "In this work, we redefine the difficulty level of a question as the number of inference steps required to answer it, which reflects the requirements on reasoning and cognitive abilities (Pan et al., 2019).",
      "startOffset" : 186,
      "endOffset" : 204
    }, {
      "referenceID" : 30,
      "context" : "Existing QA systems perform substantially worse in answering multi-hop questions than single-hop ones (Yang et al., 2018), also supporting the soundness of using reasoning hops to define difficulty.",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "Graph-based methods are well suited for such logic modelling (Pearl and Paz, 1986; Zhang et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 32,
      "context" : "Graph-based methods are well suited for such logic modelling (Pearl and Paz, 1986; Zhang et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "Specifically, we utilize HotpotQA (Yang et al., 2018), a QA dataset where most questions require two inference steps to answer and can be decomposed into two 1-hop questions.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "Deep Question Generation Most of the previous QG researches (Zhou et al., 2017; Pan et al., 2019; Liu et al., 2020) mainly focused on generating single-hop questions like the ones in SQuAD (Rajpurkar et al.",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "Deep Question Generation Most of the previous QG researches (Zhou et al., 2017; Pan et al., 2019; Liu et al., 2020) mainly focused on generating single-hop questions like the ones in SQuAD (Rajpurkar et al.",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Deep Question Generation Most of the previous QG researches (Zhou et al., 2017; Pan et al., 2019; Liu et al., 2020) mainly focused on generating single-hop questions like the ones in SQuAD (Rajpurkar et al.",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : ", 2020) mainly focused on generating single-hop questions like the ones in SQuAD (Rajpurkar et al., 2016).",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "Another research on QG for knowledge graphs (Kumar et al., 2019) estimated the question difficulty based on popularity of the named entity.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "(Elgohary et al., 2019; Vakulenko et al., 2020), and QA pipelines could also decompose the original complex question into multiple shorter questions to improve model performance (Min et al.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "(Elgohary et al., 2019; Vakulenko et al., 2020), and QA pipelines could also decompose the original complex question into multiple shorter questions to improve model performance (Min et al.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : ", 2020), and QA pipelines could also decompose the original complex question into multiple shorter questions to improve model performance (Min et al., 2019; Khot et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : ", 2020), and QA pipelines could also decompose the original complex question into multiple shorter questions to improve model performance (Min et al., 2019; Khot et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "Specifically, we first apply open information extraction (Stanovsky et al., 2018) to extract 〈subject, relation, object〉 triples from context sentences.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Coreference resolution (Lee et al., 2017) is applied to merge nodes referring to the same entity.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "In our implementation, both QGInitial and QGRewrite are initialized with the pre-trained GPT2-small model (Radford et al., 2019), and then fine-tuned on our constructed dataset (see Sec.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 30,
      "context" : "In our work, the training data is constructed from HotpotQA (Yang et al., 2018), in which every context C consists of two paragraphs {P1,P2}, and most of the questions require two hops of reasoning, each concerning one paragraph.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "After that, an off-the-shelf single-hop QA model (Min et al., 2019) is utilized to acquire the answer of the two sub-questions, which should be “Dial M for Murder” and “Alfred Hitchcock” in the example.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "0 (Lee et al., 2017) and the open information extraction toolkit provided by the Plasticity developer API.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 33,
      "context" : "Baselines The following baselines are trained to generate the 2-hop questions in the datasets: • NQG++ (Zhou et al., 2017) is a seq2seq model based on bi-directional Gate Recurrent Unit (GRU), with features enriched by answer position and lexical information.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "• ASs2s (Kim et al., 2019) is a seq2seq model based on Long Short-term Memory (LSTM), which separately encodes answer and context.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "• SRL-Graph and DP-Graph (Pan et al., 2020) are two state-of-the-art QG systems.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : "QGInitial, QGRewrite, and GPT2 are initialized with the GPT2-small model from the HuggingFace Transformer library (Wolf et al., 2019), and fine-tuned for 8, 10, and 7 epochs, respectively, with batch size of 16.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "AdamW (Loshchilov and Hutter, 2017) is used as optimizer, with the initial learning rate set to be 6.",
      "startOffset" : 6,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "Automatic Evaluation The automatic evaluation metrics are BLEU3, BLEU4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 28,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015), which measure the",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "Specifically, we utilize two off-the-shelf QA models provided by the HuggingFace Transformer library (Wolf et al., 2019), which are respectively initialized with Test Set BERT RoBERTa EM F1 EM F1",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019b), and then fine-tuned on SQuAD (Rajpurkar et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : ", 2019b), and then fine-tuned on SQuAD (Rajpurkar et al., 2016).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "A DistilBERT-based (Sanh et al., 2019) QA model is implemented.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "We use Adam (Kingma and Ba, 2015) as the optimizer, with the mini-batch size of 32.",
      "startOffset" : 12,
      "endOffset" : 33
    } ],
    "year" : 2021,
    "abstractText" : "This paper explores the task of DifficultyControllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-bystep rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method.",
    "creator" : "LaTeX with hyperref"
  }
}