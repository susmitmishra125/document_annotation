{
  "name" : "2021.acl-long.468.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation",
    "authors" : [ "Xin Liu", "Baosong Yang", "Dayiheng Liu", "Haibo Zhang", "Weihua Luo", "Min Zhang", "Haiying Zhang", "Jinsong Su" ],
    "emails" : [ "liuxin@stu.xmu.edu.cn", "yangbaosong.ybs@alibaba-inc.com", "liudayiheng.ldyh@alibaba-inc.com", "zhanhui.zhb@alibaba-inc.com", "weihua.luowh@alibaba-inc.com", "minzhang@suda.edu.cn", "zhang2002@xmu.edu.cn", "jssu@xmu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6001"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal\n1We release the code at https://github.com/ DeepLearnXMU/embedding-transfer\n*Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group.\nmodel using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b).\nIn comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it?\nIn this paper, we provide the first answer from the subword discrepancy aspect, namely, the subword vocabulary extracted according to the pretraining data distribution is insufficient to cope with the downstream NLG tasks. Such inflexibility stems from the fact that downstream NLG models have to inherit the vocabulary from their pre-trained counterparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo\nand Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020).\nWe argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation to maintain the coverage of a large amount of diverse vocabulary. Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths. As one example, M-BERT exploits 100 thousand fine-grained subwords to encode hundreds of languages, while most of downstream NLG tasks, in fact, require only one language and its associate tokens. Secondly, words that are rare in upstream task but frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020). Considering the English sequence “Cenozoic palaeohydrodynamic” shown in Table 1, all the words are frequent in a thesis domain translation task and can be well preserved in its vocabulary. Nevertheless, they are segmented into under-represented tokens by pre-trained models, preventing the finetuning stage from better learning their compositionality for generation. An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020). However, retraining the upstream model from scratch for each task is time-consuming and unavailable for largescale models like M-BERT, GPT, etc.\nTo this end, we propose a simple yet generalized pretrain-finetune strategy, where an embedding transfer stage is inserted between pre-training and finetuning to eliminate their token granularity gaps. Unlike the prior strategy using a fixed vocabulary, our vocabulary is changeable and its items including mismatched ones can be easily initialized by the pre-trained embeddings. Concretely, we equip the pre-trained model with a plug-and-play embedding generator, which is able to produce the embedding of any token by feeding its subwords and hyperwords that appeared in pre-trained vocabulary. To train this generator, we randomly split or merge some tokens to replace their original embeddings with those produced by the generator. The parameters of the generator are optimized under the vanilla pre-training framework to minimize the\ndivergence before and after replacing the embeddings. Accordingly, we can use a task-specific vocabulary for the downstream task, where common tokens are immediately initialized with pre-trained embeddings while mismatched ones are initialized by our generator.\nWe conduct experiments on various tasks under NLG context, in a range from domain adaptation to knowledge transferring, and from machine translation to answer-aware question generation. Empirical results demonstrate the universal-effectiveness of the proposed strategy comparing with strong baselines and related approaches. Quantitative and qualitative analyses verify that tackling subword discrepancy can exactly alleviate the problem of exposure bias, large computational cost, and the under-represented tokens in vanilla pretrainfinetune paradigm. To summarize, the contributions of our work are as follows:\n• Through in-depth analyses, we point out and formally analyze subword discrepancy, affecting the conventional pretrain-finetune strategy in NLG tasks. • We propose a simple, flexible, and generalized pretrain-finetune training strategy, where an embedding generator is introduced to leverage the knowledge of the pre-trained model to initialize embeddings of any required tokens. • Extensive experiments show that our strategy is able to efficiently decrease the vocabulary gaps in pretrain-finetune paradigm and significantly boost the performance of NLG models."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020). This problem has been attributed to many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks.\nTask-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding\nlayer with an independent encoder, of which vocabulary and parameters are learned from the downstream corpus. Along this line, Sato et al. (2020) exploit external monolingual data to construct a new embedding layer and achieve improvements in domain adaptation. This series of studies empirically confirm the necessity of the suitable vocabulary for the finetuning stage. However, these methods have to learn the task-specific embeddings separately before each adaptation, which brings in additional computational cost thus limiting their applicability. Besides, they completely discard the pre-trained embeddings, which have been proved to be useful by Aji et al. (2020). Extra encoder or embedding layer may fail to be well optimized with insufficient downstream resources. Accordingly, Rothe et al. (2020) employ a task-specific vocabulary to retrain M-BERT, which is then used to initialize neural machine translation (NMT) model. Considering more robust approaches, Kudo (2018) and Provilkov et al. (2020) randomly sample segmentations for each sentence at the training time. Unlike the above methods, our goal is to build a plug-andplay component, that involves neither retraining the pre-trained model nor learning task-specific embeddings separately.\nEmbedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator is able to produce embeddings of any tokens, since these embeddings are directly embedded into the pre-trained model with an objective in terms of minimizing the divergence. Moreover, previous studies mainly focus on handling the problem of OOV, while our work, to our best of knowledge, is the first study that exploits embedding generator to transfer granularity over subwords for pretrain-\nfinetune paradigm."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce our proposed pretrainfinetune strategy in detail."
    }, {
      "heading" : "3.1 Main Steps in Our Strategy",
      "text" : "Initialized Downstream Models\nAs shown in Figure 1, we extend the prior pretrain-finetune paradigm with an embedding transfer stage. Specifically, we revise the conventional pretrain-finetune pipeline as follows: Pretrain. As usual, we first construct a pre-trained model using an existing large-scale corpus. In addition, we further pretrain an embedding generator regardless of downstream tasks. It’s expected to produce the embedding of any required token, by feeding pre-trained embeddings of its subwords and hyperwords. Hence, it can be employed into any downstream tasks for embedding transferring. Finetune. We differently initialize the word embeddings and the other parameters (inner layer) for the downstream model, respectively. For the former, we use the downstream-task training corpus to learn a task-specific subword segmentation and corresponding vocabulary. For an unseen token, we apply the generator to produce its initial representation. Otherwise, we directly initialize it with the corresponding pre-trained embeddings. Considering the latter, we directly adapt inner-layer parameters of the pre-trained model to the downstream model. Finally, we continue to train the\ndownstream model using the finetuning data following the common fashion.\nAs seen, our strategy is lightweight and also able to avoid the issue of subword discrepancy, since it does not require retraining for the pre-trained model and can be quickly applied to various downstream NLG models."
    }, {
      "heading" : "3.2 Constructing the Embedding Generator",
      "text" : "To make the word embedding generator applicable to all downstream NLG models, we design the generator so that it can generate the embedding of any input token according to those of its morphologically similar tokens from the learned pre-training vocabulary. The basic intuition behind our design stems from this fact: if the input token is a complete word, like motorcycle, its semantic meaning is related to those of its subwords, motor and ##cycle. On the contrary, if the input token is a subword, such as ##er, the words that contain the input token, which we call them hyperwords, e.g., worker, writer and singer, can be exploited to learn its semantic meaning.\nConcretely, given a mismatch token w, we borrow the segmentation principle from pre-trained model to split w into subwords based on the pretraining vocabulary, and traverse the pre-training vocabulary to select all longer tokens containing w. Then, we combine the generated subwords and the selected hyperwords to form the morphologically similar token set of w, denoted by Sm(w). Afterwards, we explore three kinds of generators to produce the embedding G(w) of w:\nAVG-EG: Averaging-Based Embedding Generator Intuitively, we can simply define G(w) as the average embedding of the words from Sm(w):\nG(w) = 1 |Sm(w)| ∑\nw′∈Sm(w)\nE(w′), (1)\nwhere E(w′) is the pre-trained embedding of the token w′. In this way, our generator can be directly used, without increasing the cost of training time.\nATT-EG: Attention-Based Embedding Generator Another natural solution is to softly fuse information from different morphologically similar words using an attention mechanism (Bahdanau\net al., 2015). The G(w) is formally expressed as:\nG(w) = 1 |Sm(w)| ∑\nw′∈Sm(w)\nα(w′) · E(w′),\nα(w′) = exp(W>E(w′))∑\nw′′∈Sm(w) exp(W >E(w′′))\n,\n(2)\nwhere W ∈ R1×d indicates a learnable vector, d denotes the dimensionality of word embedding. Compared with the first generator, this generator can be jointly trained with the pre-trained model, therefore it is capable of better quantifying the effects of morphologically similar words in Sm(w).\nPATT-EG: Position-Aware Attention-Based Embedding Generator From the linguistic perspective, different locations of morphemes in a word reflect distinct semantic meaning. Consequently, we refine the above attention-based generator by considering six kinds of morphology relationships between w and w′ ∈ Sm(w): if w′ is a subword of w, w′ can be the prefix/infix/suffix subword of w. In turn, if w′ is a hyperword of w, w can be the prefix/infix/suffix subword of w′. Formally, G(w) is produced in the following way:\nG(w) = 1 |Sm(w)| ∑\nw′∈Sm(w)\nα(w′)E(w′),\nα(w′) = exp(IWrE(w′))∑\nw′′∈Sm(w) exp(IWrE(w ′′))\n,\n(3)\nwhere Wr ∈ R6×d is a learnable parameter matrix, and I ∈ R1×6 is the one-hot vector indicating the relationship between w and w′.\nNote that, all the trainable generators are designed to lightweight architectures with a few of parameters. We believe this can achieve a more generalizable model and speed up their convergence. We will compare and investigate these generators in the subsequent experiment section."
    }, {
      "heading" : "3.3 Training the Embedding Generator",
      "text" : "One principle of our strategy is plug-and-play, which can be directly applied to initialize any unseen tokens in all downstream NLG tasks, avoiding the time cost of retraining the model. To this end, we borrow the pre-trained model and its associated corpus to train our generator before finetuning.\nIn the specific implementation, we first preprocess the sentences of pre-training corpus, where two kinds of preprocessing operations are applied\nto simulate unseen tokens: 1) randomly selecting some consecutive subwords and combining them into an unseen token; and 2) randomly choosing a token and splitting it into several consecutive unseen tokens. Figure 2 provides an example of sentence preprocessing, where the word nothing is randomly split into two unseen subwords noth and ##ing, while the subwords ima and ##gine are concatenated into an unseen token imagine. Through this data preprocessing, we can obtain large amounts of samples with unseen tokens involving various granularities, which facilitates the robustness of our generator.\nThen, we embed our generator into the pretrained model to encode unseen words, and fix parameters of the pre-trained model to train the generator according to the following objectives:\nReusing Pre-training Loss The generated embeddings should share the same latent space with the existing embeddings, in the meanwhile, representing appropriate semantic meaning. Accordingly, we serve to minimize the vanilla loss of pretrained model as the basic training objective of our generator. The loss function can be diverse according to the upstream tasks, which is denoted as Lp(s′) with s′ being the preprocessed training sentence.\nKnowledge Distillation We further exploit knowledge distillation (Hinton et al., 2015) to narrow the divergence between hidden states in the pre-trained model before and after applying the generated embeddings. Given a training example s, the vanilla pre-trained model and our generator preprocess it to sp and s′, respectively. As shown\nin Figure 2, we transfer the knowledge of the output layer in terms of sp to that of s′. Euclidean Distance is adopted to measure the divergence between representations output by vanilla pretrained model hp(w) and that of our model h′(w) with respect to the same word w. Since each word may be split into different sequences of tokens, we regard the average hidden states of the corresponding token sequence as its representation. Thus, the loss function can be defined as:\nLd(sp, s′) = 1 |s| ∑ w∈s ||hp(w)− h′(w)||2, (4)\nFinally, we assign a hyper-parameter λ to quantify the effect of L(·) and Ld(·), which is empirically set to 0.5 as default:\nL(sp, s′) = Lp(s′) + λLd(sp, s′). (5)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we examine the effectiveness of the proposed strategy in a variety of NLG tasks. We first run a set of experiments to compare the variants of our approach and the related methods on domain adaptation translation tasks. Then, we assess the superiority of our approach on transferring the knowledge from M-BERT (Devlin et al., 2019) and M-BART (Liu et al., 2020c) to two downstream NLG tasks: machine translation (MT) and answer-aware question generation (QG)."
    }, {
      "heading" : "4.1 Domain Adaptation",
      "text" : "We conduct experiments on English-to-Chinese (En⇒Zh) domain adaptation translation tasks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an\nout-of-domain dataset LDC†, in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl.\nImplementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up strategy where the linear warm-up phase takes 4K steps, reaching its maximum learning rate to 5× 10−4. The training of each model is early-stopped to maximize BLEU score on the development set. Other hyperparameters are set following Base setting in Vaswani et al. (2017). We investigate the following methods: §\n• Baseline: We design baselines under two basic settings: Single-Run denotes that the translation model only trained on in-domain corpus with the domain-specific vocabulary. Pretrain-Finetune represents the wellknown pipeline, i.e., pre-training using upstream corpus, then finetuning on in-domain dataset via inheriting pre-training vocabulary. • Task-Specific Vocabulary: This group of methods retrain the upstream model using a taskspecific vocabulary, involving: the vocabulary collected from in-domain data (Downstream Vocab, Rothe et al., 2020), the joint vocabulary extracted from all corpus (Joint Vocab, Nguyen and Chiang, 2017), as well as the pre-trained vocabulary with a subword regularization process on upstream corpus for robustness (BPE-Drop, Provilkov et al., 2020). • Embedding Generator: We also examine several representatives of existing embedding generators on pretrain-finetune paradigm. We\n†Including LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, LDC2004T08 and LDC2005T06.\n‡https://github.com/pytorch/fairseq §Hyperparameters that are not mentioned in our paper are\nset to the default according to the corresponding literatures.\nassign the domain-specific vocabulary for each downstream model, in which embeddings of the seen tokens are reused, while the mismatched ones are: 1) randomly initialized (Random Init, Aji et al., 2020); 2) learned by Word2Vec (Mikolov et al., 2013) using in-domain data; and 3) produced by a generator trained via reconstructing embeddings using Bag-of-Subwords (Embedding Recon, Zhao et al., 2018). • New Embedding Layer: These methods assigned the domain-specific vocabulary for each downstream model, but completely discard the embeddings of upstream models. The new embeddings are produced from: 1) randomly initialized Independent Encoder (Lewis et al., 2020); and 2) CBOW model trained under the downstream corpus (Sato et al., 2020). • Our Strategy: Our embedding generators are trained using the setting of pre-trained model with one epoch, as described in § 3.\nNote that, to eliminate the influence of control variables, all the vocabulary transfers in above models are conducted on the decoder-side only.\nResults Table 2 lists our results on domain adaptation tasks. Considering baseline models, imme-\ndiately finetuning a downstream model with outof-domain vocabulary performs worse than merely training each model using in-domain data and taskspecific vocabulary. This is consistent with findings in Edunov et al. (2019) and Zhu et al. (2020). We observe that there are over 13K and 11K tokens in the vocabulary in terms of Out-of-Domain are mismatched with that of Thesis and Laws respectively, indicating that subword discrepancy indeed harms the performance of downstream NLG models. When adapting task-specific vocabulary to retrain upstream models, all the translation qualities are improved, confirming the necessity of bridging subword gaps between upstream and downstream models. In addition, we also appraise several existing embedding transfer strategies into pretrainfinetune pipeline. Interestingly, randomly initializing embeddings of unseen tokens yields even slightly better results than utilizing “Word2Vec” and “Embedding Recon”. We attribute this to the fact that the training of the latter two generators is individual regardless of the pre-trained model, resulting in unshared latent space between the generated and pre-trained embeddings.\nOur models surpass all baselines and related methods on translation qualities. Most importantly, in contrast to existing approaches that have to either retrain the pre-trained model from scratch or learn a separate embedding generator for each domain, our strategy can be immediately adopted to any downstream tasks once ready. Specifically, PATT-EG achieves the best performance, confirming our hypothesis that softly summarizing information from morphologically similar tokens and considering positions of morphemes facilitate the embedding transferring. Besides, using knowledge distillation to narrow the divergence before and after applying our generator can progressively improve the performance. Accordingly, we use PATT-EG + Knowledge Distillation as the default setting in\nsubsequent experiments."
    }, {
      "heading" : "4.2 Knowledge Transferring",
      "text" : "We test our method on transferring the knowledge from two advanced large-scale language models: non-autoregressive M-BERT and autoregressive M-BART. For computational efficiency, we randomly extract 4M samples from the conventional pre-training corpus|| to train our embedding generator using the configurations of pre-trained models with one epoch and 4,096 batch size. Comparisons are conducted on machine translation and question generation task. The pre-trained model is employed on both of encoder and decoder. Same as configurations in domain adaptation, we merely perform the embedding transferring in decoder. Since the two language models exploit different segmentation tools, i.e., WordPiece (Wu et al., 2016) and SentencePiece (Kudo, 2018), we set 32K and 10K as the number of word and sentence pieces for downstream tasks, respectively.\nMachine Translation Considering machine translation, we examine our method on the widely used English-to-German (En⇒De) benchmarks: WMT14. We follow Rothe et al. (2020) and Liu et al. (2020c) to deal this task.\nQuestion Generation We use the SQuAD v1.1 (Rajpurkar et al., 2016) dataset for question generation. We follow the common setting to preprocess dataset and train our models (Liu et al., 2020a). The answer and the passage are taken as the model input, while the question is the target output. ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics.\nResults As illustrated in Table 3, the randomly initialized NMT model yields comparable results\n¶Single NVIDIA v100 GPU with batch size being 32. ||https://dumps.wikimedia.org\nwith the reported system with the same architecture (26.1 vs. 26.0, Rothe et al., 2020), making our subsequent experiments convincing. Our methods significantly boost NLG performances across different pre-trained models, downstream tasks, linguistic resources, as well as segmentation tools, demonstrating its universal-effectiveness. Moreover, the embedding generator is able to decrease the vocabulary size and the generated sentence length, leading to less computational costs."
    }, {
      "heading" : "5 Analysis",
      "text" : "To better understand subword discrepancy and our method, we make in-depth analyses on WMT En⇒De task to investigate three problems: Q1: How subword granularity affects NLG models? (§ 5.1) Q2: How embedding transfer benefits to downstream models? (§ 5.2) Q3: Dose our strategy acquire large computational costs? (§ 5.3) Q4: Can our strategy exactly handle under-represented tokens? (§ 5.4)"
    }, {
      "heading" : "5.1 Impact of Subword Granularity",
      "text" : "Figure 3 visualizes the inference speed and exposure bias (Inference Expected Calibration Error (ECE), Wang et al., 2020) of translation models with different token granularities in their vocabulary. Obviously, for a translation model, neither too small nor too large granularity regarding to subwords can reach a satisfactory performance on inference speed. At the same time, the granularity indeed affects the problem of exposure bias in translation task. The experiments confirm the suitable segmentation strategy can effectively alleviate the problem of exposure bias."
    }, {
      "heading" : "5.2 Impact of Embedding Transfer",
      "text" : "We further investigate how the embedding transfer impacts the initialization of downstream models. We draw Figure 4 to plot the BLEU scores of downstream models using the embedding generators trained with different steps. The X-axis indicates the training steps of the generator. Both “+Ours” and “w/ M-BERT” are fully finetuned, but the latter doesn’t employ our embedding generator, resulting in an unchanged line. It is encouraging to see that the BLEU scores of downstream model converges very fast, indicating that our generator can be used with only a few of training steps. We argue that the commonalities in word compositionality lead to the fast transfer learning on generating different embeddings, and the simple architecture of our generator further speeds up such procedure."
    }, {
      "heading" : "5.3 Computational Costs",
      "text" : "As shown in Figure 4, our generator converges very fast (around 20K steps). The training process of our generator takes about 2 hours under our experimental setting. As a reference, the vanilla WMT finetuning process takes approximately 40 hours. In\naddition, our generator only takes about 3 minutes for producing 13K embeddings in Thesis, which is also insignificant compare to the finetuning time. Most importantly, once the embedding generator is well-trained, it’s available for any downstream tasks. Thus, we argue that the computational costs are not the obstacle to the extensibility of our approach."
    }, {
      "heading" : "5.4 Qualitative Analysis",
      "text" : "Table 4 gives an example to show the effectiveness of our model on handling under-represented tokens. The German word dankbar (gratifying) is over segmented by M-BERT, and fail to be generated by the model trained under conventional pipeline. On the contrary, our approach offers an opportunity for the downstream model to preserve the word into vocabulary, thus better learning its semantic meaning and correctly predicting it during inference."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we point out that the one-size-fits-all subword vocabulary, despite its all-encompassing superiority, is not the preferred solution for the popular pretrain-finetune paradigm. It causes the subword discrepancy among upstream and downstream models, which is given concrete form to the unsuitable granularity and under-represented words. Consequently, we propose a novel embedding transfer strategy with a plug-and-play embedding generator. Empirical results suggest that: 1) our approach is universally effective on overcoming subword discrepancy; 2) embedding transfer can bring benefits to computational efficiency; and 3) embedding generator can be achieved via either directly averaging the input embeddings or applying trainable components, the latter performs better but depends on few of training. As our approach is transparent to model architectures and tasks, we believe it can be widely applied and further raise the flexibility and applicability of pre-trained models.\nIn the future, we plan to investigate its effectiveness on other generation tasks, such as code generation (Jiang et al., 2021; Xie et al., 2021), summarization (Shi et al., 2021) and so on."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The project was supported by National Natural Science Foundation of China (No. 62036004, No. 61672440), National Key Research and Development Program of China (No. 2018YFB1403202),\nNatural Science Foundation of Fujian Province of China (No. 2020J06001), Youth Innovation Fund of Xiamen (No. 3502Z20206059), and the Fundamental Research Funds for the Central Universities (No. ZK20720200077). We also thank the reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "In neural machine translation, what does transfer learning transfer",
      "author" : [ "References Alham Fikri Aji", "Nikolay Bogoychev", "Kenneth Heafield", "Rico Sennrich" ],
      "venue" : null,
      "citeRegEx" : "Aji et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Aji et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR 2015.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "ACL 2005.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer." ],
      "venue" : "NIPS 2015.",
      "citeRegEx" : "Bengio et al\\.,? 2015",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting character-based neural machine translation with capacity and compression",
      "author" : [ "Colin Cherry", "George Foster", "Ankur Bapna", "Orhan Firat", "Wolfgang Macherey." ],
      "venue" : "EMNLP 2018.",
      "citeRegEx" : "Cherry et al\\.,? 2018",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 2018
    }, {
      "title" : "An embarrassingly simple approach for transfer learning from pretrained language models",
      "author" : [ "Alexandra Chronopoulou", "Christos Baziotis", "Alexandros Potamianos." ],
      "venue" : "NAACL 2019.",
      "citeRegEx" : "Chronopoulou et al\\.,? 2019",
      "shortCiteRegEx" : "Chronopoulou et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "ACL 2019.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-trained language model representations for language generation",
      "author" : [ "Sergey Edunov", "Alexei Baevski", "Michael Auli." ],
      "venue" : "ACL 2019.",
      "citeRegEx" : "Edunov et al\\.,? 2019",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust Backed-off Estimation of Out-of-Vocabulary Embeddings",
      "author" : [ "Nobukazu Fukuda", "Naoki Yoshinaga", "Masaru Kitsuregawa." ],
      "venue" : "EMNLP Findings 2020.",
      "citeRegEx" : "Fukuda et al\\.,? 2020",
      "shortCiteRegEx" : "Fukuda et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "CoRR 2015, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring dynamic selection of branch expansion orders for code generation",
      "author" : [ "Hui Jiang", "Chulun Zhou", "Fandong Meng", "Biao Zhang", "Jie Zhou", "Degen Huang", "Qingqiang Wu", "Jinsong Su." ],
      "venue" : "ACL 2021.",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Regularized training objective",
      "author" : [ "Huda Khayrallah", "Brian Thompson", "Kevin Duh", "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Khayrallah et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Khayrallah et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Trivial transfer learning for low-resource neural machine translation",
      "author" : [ "Tom Kocmi", "Ondřej Bojar." ],
      "venue" : "Machine Translation: Research Papers 2018.",
      "citeRegEx" : "Kocmi and Bojar.,? 2018",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2018
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "ACL 2018.",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "EMNLP 2018.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic evaluation of summaries using n-gram cooccurrence statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "NAACL 2003.",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "2020a. GLGE: A",
      "author" : [ "Dayiheng Liu", "Yu Yan", "Yeyun Gong", "Weizhen Qi", "Hang Zhang", "Jian Jiao", "Weizhu Chen", "Jie Fu", "Linjun Shou", "Ming Gong", "Pengcheng Wang", "Jiusheng Chen", "Daxin Jiang", "Jiancheng Lv", "Ruofei Zhang", "Winnie Wu", "Ming Zhou", "Nan Duan" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "An iterative multi-source mutual knowledge transfer framework for machine reading comprehension",
      "author" : [ "Xin Liu", "Kai Liu", "Xiang Li", "Jinsong Su", "Yubin Ge", "Bin Wang", "Jiebo Luo." ],
      "venue" : "IJCAI 2020.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TACL 2020.",
      "citeRegEx" : "Liu et al\\.,? 2020c",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "ICLR 2013.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Transfer learning across low-resource, related languages for neural machine translation",
      "author" : [ "Toan Q. Nguyen", "David Chiang." ],
      "venue" : "IJCNLP 2017.",
      "citeRegEx" : "Nguyen and Chiang.,? 2017",
      "shortCiteRegEx" : "Nguyen and Chiang.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL 2002.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Mimicking word embeddings using subword RNNs",
      "author" : [ "Yuval Pinter", "Robert Guthrie", "Jacob Eisenstein." ],
      "venue" : "EMNLP 2017.",
      "citeRegEx" : "Pinter et al\\.,? 2017",
      "shortCiteRegEx" : "Pinter et al\\.",
      "year" : 2017
    }, {
      "title" : "BPE-dropout: Simple and effective subword regularization",
      "author" : [ "Ivan Provilkov", "Dmitrii Emelianenko", "Elena Voita." ],
      "venue" : "ACL 2020.",
      "citeRegEx" : "Provilkov et al\\.,? 2020",
      "shortCiteRegEx" : "Provilkov et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog 2019, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP 2016.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation tasks",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn." ],
      "venue" : "TACL 2020.",
      "citeRegEx" : "Rothe et al\\.,? 2020",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "Subword-based Compact Reconstruction of Word Embeddings",
      "author" : [ "Shota Sasaki", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "NAACL 2019.",
      "citeRegEx" : "Sasaki et al\\.,? 2019",
      "shortCiteRegEx" : "Sasaki et al\\.",
      "year" : 2019
    }, {
      "title" : "Vocabulary adaptation for domain adaptation in neural machine translation",
      "author" : [ "Shoetsu Sato", "Jin Sakuma", "Naoki Yoshinaga", "Masashi Toyoda", "Masaru Kitsuregawa." ],
      "venue" : "EMNLP 2020.",
      "citeRegEx" : "Sato et al\\.,? 2020",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL 2016.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural abstractive text summarization with sequence-to-sequence models",
      "author" : [ "Tian Shi", "Yaser Keneshloo", "Naren Ramakrishnan", "Chandan K. Reddy." ],
      "venue" : "Trans. Data Sci., 2(1):1:1–1:37.",
      "citeRegEx" : "Shi et al\\.,? 2021",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring discriminative word-level domain contexts for multidomain neural machine translation",
      "author" : [ "Jinsong Su", "Jiali Zeng", "Jun Xie", "Huating Wen", "Yongjing Yin", "Yang Liu." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "UM-corpus: A large English-Chinese parallel corpus for statistical machine translation",
      "author" : [ "Liang Tian", "Derek F. Wong", "Lidia S. Chao", "Paulo Quaresma", "Francisco Oliveira", "Yi Lu", "Shuo Li", "Yiming Wang", "Longyue Wang." ],
      "venue" : "LREC 2014.",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS 2017.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Self-paced learning for neural machine translation",
      "author" : [ "Yu Wan", "Baosong Yang", "Derek F. Wong", "Yikai Zhou", "Lidia S. Chao", "Haibo Zhang", "Boxing Chen." ],
      "venue" : "EMNLP 2020, pages 1074–1080.",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "On the inference calibration of neural machine translation",
      "author" : [ "Shuo Wang", "Zhaopeng Tu", "Shuming Shi", "Yang Liu." ],
      "venue" : "ACL 2020.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "nick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean" ],
      "venue" : "CoRR 2016,",
      "citeRegEx" : "nick et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "nick et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving tree-structured decoder training for code generation via mutual learning",
      "author" : [ "Binbin Xie", "Jinsong Su", "Xiang Li", "Yubin Ge", "Jianwei Cui", "Junfeng Yao", "Bin Wang", "." ],
      "venue" : "AAAI 2021.",
      "citeRegEx" : "Xie et al\\.,? 2021",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip Yu." ],
      "venue" : "ACL 2019.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension",
      "author" : [ "An Yang", "Quan Wang", "Jing Liu", "Kai Liu", "Yajuan Lyu", "Hua Wu", "Qiaoqiao She", "Sujian Li." ],
      "venue" : "ACL 2019.",
      "citeRegEx" : "Yang et al\\.,? 2019a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "NeurIPS 2019.",
      "citeRegEx" : "Yang et al\\.,? 2019b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain transfer based data augmentation for neural query translation",
      "author" : [ "Liang Yao", "Baosong Yang", "Haibo Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "COLING 2020.",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "Multidomain neural machine translation with word-level domain context discrimination",
      "author" : [ "Jiali Zeng", "Jinsong Su", "Huating Wen", "Yang Liu", "Jun Xie", "Yongjing Yin", "Jianqiang Zhao." ],
      "venue" : "EMNLP 2018.",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Generalizing word embeddings using bag of subwords",
      "author" : [ "Jinman Zhao", "Sidharth Mudgal", "Yingyu Liang." ],
      "venue" : "EMNLP 2018.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Incorporating BERT into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tie-Yan Liu." ],
      "venue" : "ICLR 2020.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : ", domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al.",
      "startOffset" : 20,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : ", domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al.",
      "startOffset" : 20,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : ", 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al.",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 36,
      "context" : ", 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al.",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : ", 2020), as well as knowledge transferring (Liu et al., 2020b).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : ", 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al.",
      "startOffset" : 60,
      "endOffset" : 99
    }, {
      "referenceID" : 46,
      "context" : ", 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al.",
      "startOffset" : 60,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "(2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 163
    }, {
      "referenceID" : 25,
      "context" : "However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 25,
      "context" : "Secondly, words that are rare in upstream task but frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : "An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al.",
      "startOffset" : 111,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al.",
      "startOffset" : 111,
      "endOffset" : 159
    }, {
      "referenceID" : 25,
      "context" : "An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020).",
      "startOffset" : 197,
      "endOffset" : 221
    }, {
      "referenceID" : 7,
      "context" : "Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 161
    }, {
      "referenceID" : 46,
      "context" : "Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 161
    }, {
      "referenceID" : 28,
      "context" : "Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 185
    }, {
      "referenceID" : 45,
      "context" : "In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 185
    }, {
      "referenceID" : 29,
      "context" : "In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "ATT-EG: Attention-Based Embedding Generator Another natural solution is to softly fuse information from different morphologically similar words using an attention mechanism (Bahdanau et al., 2015).",
      "startOffset" : 173,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Knowledge Distillation We further exploit knowledge distillation (Hinton et al., 2015) to narrow the divergence between hidden states in the pre-trained model before and after applying the generated embeddings.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "Then, we assess the superiority of our approach on transferring the knowledge from M-BERT (Devlin et al., 2019) and M-BART (Liu et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and M-BART (Liu et al., 2020c) to two downstream NLG tasks: machine translation (MT) and answer-aware question generation (QG).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 34,
      "context" : "We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014).",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : ", 2020); 2) learned by Word2Vec (Mikolov et al., 2013) using in-domain data; and 3) produced by a generator trained via reconstructing embeddings using Bag-of-Subwords (Embedding Recon, Zhao et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "The new embeddings are produced from: 1) randomly initialized Independent Encoder (Lewis et al., 2020); and 2) CBOW model trained under the downstream corpus (Sato et al.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : ", 2020); and 2) CBOW model trained under the downstream corpus (Sato et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : ", 2016) and SentencePiece (Kudo, 2018), we set 32K and 10K as the number of word and sentence pieces for downstream tasks, respectively.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "1 (Rajpurkar et al., 2016) dataset for question generation.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics.",
      "startOffset" : 47,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "In the future, we plan to investigate its effectiveness on other generation tasks, such as code generation (Jiang et al., 2021; Xie et al., 2021), summarization (Shi et al.",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 39,
      "context" : "In the future, we plan to investigate its effectiveness on other generation tasks, such as code generation (Jiang et al., 2021; Xie et al., 2021), summarization (Shi et al.",
      "startOffset" : 107,
      "endOffset" : 145
    } ],
    "year" : 2021,
    "abstractText" : "A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones. Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized. We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1",
    "creator" : "LaTeX with hyperref"
  }
}