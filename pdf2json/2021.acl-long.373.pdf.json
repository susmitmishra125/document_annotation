{
  "name" : "2021.acl-long.373.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MLBiNet: A Cross-Sentence Collective Event Detection Network",
    "authors" : [ "Dongfang Lou", "Zhilin Liao", "Shumin Deng", "Ningyu Zhang", "Huajun Chen" ],
    "emails" : [ "loudongfang2015@163.com,", "zhilinliao@yeah.net", "231sm@zju.edu.cn", "zhangningyu@zju.edu.cn", "huajunsir@zju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4829–4839\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4829"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event detection (ED) is a crucial sub-task of event extraction, which aims to identify and classify event triggers. For instance, the document shown in Table 1, which contains six sentences {s1, . . . , s6}, the ED system is required to identify four events: an Injure event triggered by “injuries”, two Attack events triggered by “firing” and “fight”, and a Die event triggered by “death”.\nDetecting event triggers from natural language text is a challenge task because of the following problems: a). Sentence-level contextual representation and document-level information aggregation (Chen et al., 2018; Zhao et al., 2018;\n∗ Equal contribution and shared co-first authorship. † Corresponding author.\n1The code is available in https://github.com/ zjunlp/DocED.\nShen et al., 2020). In ACE 2005 corpus, the arguments of a single event instance may be scattered in multiple sentences (Zheng et al., 2019; Ebner et al., 2019), which indicates that document-level information aggregation is critical for ED task. What’s more, a word in different contexts would express different meanings and trigger different events. For example, in Table 1, “firing” in s3 means the action of firing guns (Attack event) or forcing somebody to leave their job (End Position event). To specify its event type, cross-sentence information should be considered. b). Intra-sentence and inter-sentence event inter-dependency modeling (Liao and Grishman, 2010; Chen et al., 2018; Liu et al., 2018). For s4 in Table 1, an Attack event is triggered by “fight”, and a Die event is triggered by “death”. This kind of event co-occurrence is common in ACE 2005 corpus, we investigated the dataset and found that about 44.4% of the triggers appeared in this way. The cross-sentence event co-occurrence shown in s4 and s3 is also very common. Therefore, modeling the sentence-level and document-level event inter-dependency is crucial for jointly detecting multiple events.\nTo address those issues, previous approaches (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yan et al., 2019; Liu et al., 2019; Zhang et al., 2019) mainly focused on sentence-level event de-\ntection, neglecting the document-level event interdependency and semantic information. Some studies (Chen et al., 2018; Zhao et al., 2018) tried to integrate semantic information across sentences via the attention mechanism. For the documentlevel event inter-dependency modeling, Liao and Grishman (2010) extended the features with event types to capture dependencies between different events in a document. Although great progress has been made in ED task due to recent advances in deep learning, there is still no unified framework to model the document-level semantic information and event inter-dependency.\nWe try to analyze the ACE 2005 data to reunderstand the challenges encountered in ED task. Firstly, we find that event detection is essentially a special Seq2Seq task, in which the source sequence is a given document or sentence, and the event tag sequence is target of task. Seq2Seq tasks can be effectively modeled via the RNN-based encoder-decoder framework, in which the encoder captures rich semantic information, while the decoder generates a sequence of target symbols with inter-dependency been captured. This separate encoder and decoder framework can correspondingly deal with the semantic aggregation and event interdependency modeling challenges in ED task. Secondly, for the propagation of cross-sentence information, we find that the relevant information is mainly stored in several neighboring sentences, while little is stored in distant sentences. For example, as shown in Table 1, it seems that s2 and s4 contribute more to s3 than s1 and s5.\nIn this paper, we propose a novel Multi-Layer Bidirectional Network (MLBiNet) for ED task. A bidirectional decoder layer is firstly devised to decode the event tag vector corresponding to each token with forward and backward event interdependency been captured. Then, the event-related information in the sentence is summarized through a sentence information aggregation module. Finally, the multiple bidirectional tagging layers stacking mechanism is proposed to propagate crosssentence information between adjacent sentences, and capture long-range information as the increasing of layers. We conducted experimental studies on ACE 2005 corpus to demonstrate its benefits in cross-sentence joint event detection. Our contributions are summarized as follows:\n• We propose a novel bidirectional decoder model to explicitly capture bidirectional event\ninter-dependency within a sentence, alleviating long-range forgetting problem of traditional tagging structure;\n• We propose a model called MLBiNet to propagate semantic and event inter-dependency information across sentences and detect multiple events collectively;\n• We achieve the best performance (F1 value) on ACE 2005 corpus, surpassing the state-ofthe-art by 1.9 points."
    }, {
      "heading" : "2 Approach",
      "text" : "Generally, event detection on ACE 2005 corpus is treated as a classification problem, which is to determine whether it forms a part of an event trigger. Specifically, for a given document d = {s1, . . . , sn}, where si = {wi,1, . . . , wi,ni} denotes the i-th sentence containing ni tokens. We are required to predict the triggered event type sequence yi = {yi,1, . . . , yi,ni} based on contextual information of d. Without ambiguity, we omit the subscript i.\nFor a given sentence, the event tags corresponding to tokens are associated, which is important for collectively detecting multiple events (Chen et al., 2018; Liu et al., 2018). The way tokens are classified independently will miss the association. In order to capture the event inter-dependency, the sequential information of event tag should be retained. Intuitively, the ED task can be regarded as event tag sequence generation problem, which is essentially a Seq2Seq task. Specifically, the source sequence is a given document or sentence, and the event tag sequence to be generated is the target sequence. For instance, for sentence “did you hear about the injuries she sustained”, the decoder model is required to generate a tag sequence [O,O,O,O,O,B Injure, O,O], where “O” denotes that the corresponding token is not part of event trigger and “B Injure” indicates an Injure event is triggered.\nWe introduce the RNN-based encoder-decoder framework for ED task, considering that it is an efficient solution for Seq2Seq tasks. And we propose a multi-layer bidirectional network called MLBiNet shown in Figure 1 to deal with the challenges in detecting multiple events collectively. The model framework consists of four components: the semantic encoder, the bidirectional decoder, the information aggregation module and stacking of mul-\ni+1 ]\nintegrated in the previous layer, and the blue arrow represents the input of forward event tag vector.\ntiple bidirectional tagging layers. We firstly introduce the encoder-decoder framework and discuss its compatibility with the ED task."
    }, {
      "heading" : "2.1 Encoder–Decoder",
      "text" : "The RNN-based encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gu et al., 2016) consists of two components: a) an encoder which converts the source sentence into a fixed length vector c and b) a decoder is to unfold the context vector c into the target sentence. As is formalized in (Gu et al., 2016), the source sentence si is converted into a fixed length vector c by the encoder RNN,\nht = f(ht−1, wt), c = φ({h1, . . . ,hni})\nwhere f is the RNN function, {ht} are the RNN states, wt is the t-th token of source sentence, c is the so-called context vector, and φ summarizes the hidden states, e.g. choosing the last state hni . And the decoder RNN translates c into the target sentence according to:\nst = f(yt−1, st−1, c) p(yt|y<t, si) = g(yt−1, st, c)\n(1)\nwhere st is the state at time t, yt is the predicted symbol at time t, g is a classifier over the vocabulary, and y<t denotes the history {y1, . . . , yt−1}.\nStudies (Bahdanau et al., 2015; Luong et al., 2015) have shown that summarizing the entire source sentence into a fixed length vector will limit the performance of the decoder. They introduced\nthe attention mechanism to dynamically changing context vector ct in the decoding process, where ct can be uniformly expressed as\nct = ni∑ τ=1 αtτhτ (2)\nwhere αtτ is the contribution weight of τ -th source token’s state to context vector at time t, hτ denotes the representation of τ -th token.\nWe introduce the encoder-decoder framework to model ED task, mainly considering the following advantages: a) the separate encoder module is flexible in fusing sentence-level and documentlevel semantic information and b) the RNN decoder model (1) can capture sequential event tag dependency as the predicted tag vectors before t will be used as input for predicting t-th symbol.\nThe encoder-decoder framework for ED task is slightly different from the general Seq2Seq task as follows: a) For ED task, the length of event tag sequence (target sequence) is known because its elements correspond one-to-one with tokens in the source sequence. However, the length of target sequence in the general Seq2Seq task is unknown. b) The vocabulary of decoder for ED task is a collection of event types, instead of words."
    }, {
      "heading" : "2.2 Semantic Encoder",
      "text" : "In this module, we encode the sentence-level contextual information for each token with Bidirectional LSTM (BiLSTM) and self-attention mechanism. Firstly, each token is transformed into\ncomprehensive representation by concatenating its word embedding and NER type embedding. The word embedding matrix is pretrained by Skip-gram model (Mikolov et al., 2013), and the NER type embedding matrix is randomly initialized and updated in the training process. For a given token wt, its embedded vector is denoted as et.\nWe apply the BiLSTM (Zaremba and Sutskever, 2014) model for sentence-level semantic encoding, which can effectively capture sequential and contextual information for each token. The BiLSTM architecture is composed of a forward LSTM and a backward LSTM, i.e.,\n−→ h t =−−−−→\nLSTM( −→ h t−1, et), ←− h t = ←−−−− LSTM( ←− h t+1, et). After encoding, the contextual representation of each token is ht = [ −→ h t; ←− h t].\nAttention mechanism between tokens within a sentence has been proven to further integrate long-range contextual semantic information. For each token wt, its contextual representation is the weighted average of the semantic information of all tokens in the sentence. We apply the attention mechanism proposed by (Luong et al., 2015) with the weights derived by\nαt,j = exp(zt,j)∑ni\nm=1 exp(zt,m)\nzt,m = tanh(h > t Wsahm + bsa)\n(3)\nAnd the contextual representation of wt is hat =∑ni j=1 αt,jhj . By concatenating its lexical embedding and contextual representation, we get the final comprehensive semantic representation of wt as xt = [h a t ; et]."
    }, {
      "heading" : "2.3 Bidirectional Decoder",
      "text" : "The decoder layer for ED task is to generate a sequence of event tags corresponding to tokens. As is noted, the tag sequence (target sequence) elements and tokens (source sequence) are in one-toone correspondence. Therefore, the context vector c shown in (1) and (2) can be personalized directly by ct = xt, which is equivalent to attention with degenerate weights. That is, αtt = 1 and αtτ = 0, ∀τ 6= t.\nIn traditional Seq2Seq tasks, the target sequence length is unknown during the inference process, so only the forward decoder is feasible. However, for the ED task, the length of the target sequence is known when given source sequence. Thus, we devise a bidirectional decoder to model event interdependency within a sentence.\nForward Decoder In addition to the semantic context vector ct = xt, the event information previously involved can help determine the event type triggered by t-th token. This kind of association can be captured by the forward decoder model:\n→ s t = ffw( → y t−1, → s t−1,xt)\n→ y t = f̃(Wy → s t + by)\n(4)\nwhere ffw is the forward RNN, { → s t} are the states of forward RNN, {→y t} are the forward event tag vectors. Compared with general decoder (1), the classifier g(·) over vocabulary is replaced with a transformation f̃(·) (identity function, tanh, sigmoid, etc.) to obtain the event tag vector.\nBackward Decoder Considering the associated events may also be mentioned later, we devise a backward decoder to capture this kind of dependency as follows:\n← s t = fbw( ← y t+1, ← s t+1,xt)\n← y t = f̃(Wy ← s t + by)\n(5)\nwhere fbw is the backward RNN, { ← s t} are the states of backward RNN, {←y t} are the backward event tag vectors.\nBidirectional Decoder By concatenating→y t and← y t, we get the event tag vector yt = [ → y t; ← y t] with bidirectional event inter-dependency been captured. The semantic and event-related entity information is also carried by yt as xt is an indirect input.\nAn alternative method modeling the sentencelevel event inter-dependency called hierarchical tagging layer is proposed by (Chen et al., 2018). The bidirectional decoder is quite different from the hierarchical tagging layer as follows:\n• The bidirectional decoder models event interdependency immediately by combining a forward and a backward decoder. The hierarchical tagging layer utilizes two forward decoders and the tag attention mechanism to capture bidirectional event inter-dependency.\n• In the bidirectional decoder, the ED task is formalized as a special Seq2Seq task, which can simplify the event inter-dependency modeling problem and cross-sentence information propagation problem discussed below.\nThe bidirectional RNN decoder unfolds the event tag vector corresponding to each token, and captures the bidirectional event inter-dependency within the sentence. To propagate information across sentences, we need to firstly aggregate useful information of each sentence."
    }, {
      "heading" : "2.4 Information Aggregation",
      "text" : "For current sentence si, the information we are concerned about can be summarized as recording which entities and tokens trigger which events. Thus, to summarize the information, we devise another LSTM layer (information aggregation module shown in Figure 1) with the event tag vector yt as input. The information at t-th token is computed by\nĨt = −−−−→ LSTM(Ĩt−1,yt) (6)\nWe choose the last state Ĩni as the summary information, which is Ii = Ĩni .\nThe sentence-level information aggregation module bridges the information across sentences, as the well-formalized information can be easily integrated into the decoding process of other sentences, enhancing the event-related signal."
    }, {
      "heading" : "2.5 Multi-Layer Bidirectional Network",
      "text" : "In this module, we introduce a multiple bidirectional tagging layers stacking mechanism to aggregate information of adjacent sentences into the bidirectional decoder, and propagate information across sentences. The information ({yt}, Ii) obtained by the bidirectional decoder layer and information aggregation module has captured the event relevant information within a sentence. However, the cross-sentence information has not yet interacted. For a given sentence, as we can see in Table 1, its relevant information is mainly stored in several neighboring sentences, while distant sentences are rarely relevant. Thus, we propose to transmit the summarized sentence information Ii among adjacent sentences.\nFor the decoder framework shown in (4) and (5), the cross-sentence information can be integrated by extending the input with Ii−1 and Ii+1. Further, we introduce a multiple bidirectional tagging layers stacking mechanism shown in Figure 1 to iteratively aggregate information of adjacent sentences. The overall framework is named Multi-Layer Bidirectional Network (MLBiNet). As shown in Figure 1, a bidirectional tagging layer\nis composed of a bidirectional decoder and an information aggregation module. For sentence si, the outputs of k-th layer can be computed by\n→ s t = ffw( → y k t−1, → s t−1,xt, I k−1 i−1 , I k−1 i+1 ) ← s t = fbw( ← y k t+1, ← s t+1,xt, I k−1 i−1 , I k−1 i+1 )\n→ y k t = f̃(Wy → s t + by) ← y k t = f̃(Wy ← s t + by)\nykt = [ → y k t ; ← y k t ]\n(7)\nwhere Ik−1i−1 is the sentence information of si−1 aggregated in (k-1)-th layer, and {ykt } are event tag vectors obtained in k-th layer. The equation suggests that for each token of source sentence si, the input of cross-sentence information is identical [Ik−1i−1 , I k−1 i+1 ]. It is reasonable as their crosssentence information available is the same for each token of current sentence.\nThe iteration process shown in equation (7) is actually an evolutionary diffusion of the crosssentence semantic and event information in the document. Specifically, in the first tagging layer, information of current sentence is effectively modeled by the bidirectional decoder and information aggregation module. In the second layer, information of adjacent sentences is propagated to current sentence by plugging in I1i−1 and I 1 i+1 to the decoder. In general, in the k-th (k ≥ 3) layer, since si−1 has captured the information of sentence si−k+1 in the (k-1)-th layer, then si can obtain information in si−k+1 by acquiring the information in si−1. Thus, as the number of decoder layers increases, the model will capture information from distant sentences. For K-layer bidirectional tagging model, the sentence information with the longest distance of K-1 can be captured.\nWe define the final event tag vector of wt as the weighted sum of {ykt }k in different layers, i.e., ydt = ∑K k=1 α\nk−1ykt , where α ∈ (0, 1] is a weight decay parameter. It means that cross-sentence information can supplement to the current sentence, and the contribution gradually decreases as the distance increases when α < 1.\nWe note that the parameters of bidirectional decoder and information aggregation module at different layers can be shared, because they encode and propagate the same structured information. In this paper, we set the parameters of different layers to be the same."
    }, {
      "heading" : "2.6 Loss Function",
      "text" : "In order to train the networks, we minimize the negative log-likelihood loss function J(θ),\nJ(θ) = − ∑ d∈D ∑ s∈d ∑ wt∈s log p(Oytt |d; θ) (8)\nwhere D denotes training documents set. The tag probability for token wt is computed by\nOt =Woy d t + bo\np(Ojt |d; θ) = exp(O j t )/ M∑ m=1 exp(Omt ) (9)\nwhereM is the number of event classes, p(Ojt |d; θ) is the probability that assigning event type j to token wt in document d when parameter is θ."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset and Settings",
      "text" : "We performed extensive experimental studies on the ACE 2005 corpus to demonstrate the effectiveness of our method on ED task. It defines 33 types of events and an extra NONE type for the nontrigger tokens. We formalize it as a task to generate a sequence of 67-class event tag (with BIO tagging schema). The data splitting for training, validation and testing follows (Ji and Grishman, 2008; Chen et al., 2015; Liu et al., 2018; Chen et al., 2018; Huang and Ji, 2020), where the training set contains 529 documents, the validation set contains 30 documents and the remaining 40 documents are used as testing set.\nWe evaluated the performance of three multilayer settings with 1-, 2- and 3-layer MLBiNet, respectively. We use the Adam (Kingma and Ba, 2017) for optimization. In all three settings, we cut every 8 consecutive sentences into a new document and padding when needed. Each sentence is truncated or padded to make it 50 in length. We set the dimension of word embedding as 100, the dimension of golden NER type and subtype embedding as 20. We set the dropout rate as 0.5 and penalty coefficient as 2 ∗ 10−5 to avoid overfitting. The hidden size of semantic encoder layer and decoder layer is set to 100 and 200, respectively. The size of forward and backward event tag vectors is set to 100. And we set the batch size as 64, the learning rate as 5 ∗ 10−4 with decay rate 0.99, the weight decay parameter α as 1.0. The results we report are the average of 10 trials."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "For comparison, we investigated the performance of the following state-of-the-art methods: 1) DMCNN (Chen et al., 2015), which extracts multiple events from one sentence with dynamic multipooling CNN; 2) HBTNGMA (Chen et al., 2018), which models sentence event inter-dependency via a hierarchical tagging model; 3) JMEE (Liu et al., 2018), which models the sentence-level event interdependency via a graph model of the sentence syntactic parsing graph; 4) DMBERT-Boot (Wang et al., 2019), which augments the training data with external unlabeled data by adversarial mechanism; 5) MOGANED (Yan et al., 2019), which uses graph convolution network with aggregative attention to explicitly model and aggregate multiorder syntactic representations; 6) SS-VQ-VAE (Huang and Ji, 2020), which learns to induct new event type by a semi-supervised vector quantized variational autoencoder framework, and fine-tunes with the pre-trained BERT-large model."
    }, {
      "heading" : "3.3 Overall Performance",
      "text" : "Table 2 presents the overall performance comparison between different methods with gold-standard entities. As shown, under 2-layer and 3-layer settings, our proposed model MLBiNet achieves better performance, surpassing the current state-ofthe-art by 1.9 points. More specifically, our models achieve higher recalls by at least 0.7, 5.9 and 5.2 points, respectively.\nThe powerful encoder of BERT pre-trained model (Devlin et al., 2018) has been proven to improve the performance of downstream NLP tasks. The 2-layer MLBiNet outperforms BERT-Boot (BERT-base) and SS-VQ-VAE (BERT-large) by 3.5 and 1.9 points, respectively. It proves the im-\nportance of event inter-dependency modeling and cross-sentence information integration for ED task.\nWhen only information of current sentence is available, the 1-layer MLBiNet outperforms HBTNGMA by 2.9 points. It proves that the hierarchical tagging mechanism adopted by HBTNGMA is not as effective as the bidirectional decoding mechanism we proposed. Intuitively, the bidirectional decoder models event inter-dependency explicitly by a forward decoder and a backward decoder, which is more efficient than hierarchies."
    }, {
      "heading" : "3.4 Effect on Extracting Multiple Events",
      "text" : "The existing event inter-dependency modeling methods (Chen et al., 2015, 2018; Liu et al., 2018) aim to extract multiple events jointly within a sentence. To demonstrate that sentence-level event inter-dependency modeling benefits from crosssentence information propagation, we evaluated the performance of our model in single event extraction (1/1) and multiple events joint extraction (1/n). 1/1 means one sentence that has one event; otherwise, 1/n is used.\nThe experimental results are presented in Table 3. As shown, we can verify the importance of cross-sentence information propagation mechanism and bidirectional decoder in sentence-level multiple events joint extraction based on the following results: a) When only the current sentence information is available, the 1-layer MLBiNet outperforms existing methods at least by 2.4 points in 1/n case, which proves the effectiveness of bidirectional decoder we proposed; b) For ours 2-layer and 3-layer models, their performance in both 1/1 and 1/n cases surpasses the current methods by a large margin, which proves the importance of propagating information across sentences for single event and multiple events extraction. We conclude that it\nis the propagating information across sentences and bidirectional decoder which make cross-sentence joint event detection successful."
    }, {
      "heading" : "3.5 Analysis of Decoder Layer",
      "text" : "Table 4 presents the performance of the model in three decoder mechanisms: forward, backward and bidirectional decoder, as well as three multilayer settings. We can reach the following conclusions: a) Under three decoder mechanisms, the performance of the proposed model will be significantly improved as the number of decoder layers increases; b) The bidirectional decoder dominates both forward decoder and backward decoder, and forward decoder dominates backward decoder; c) The information propagation across sentences will enhance event relevant signal regardless of the decoder mechanism applied. Among the three decoder models, the bidirectional decoder performs best because of its ability in capturing bidirectional event inter-dependency, which proves both the forward and backward decoders are critical for event inter-dependency modeling."
    }, {
      "heading" : "3.6 Analysis of Aggregation Model",
      "text" : "In information aggregation module, we introduce a LSTM shown in (6) to aggregate sentence information, and then propagate to other sentences via the bidirectional decoder. We compare other aggregation methods: a) concat means the sentence information is aggregated by simply concatenating the first and last event tag vector of the sentence, and b) average means the sentence information is aggregated by averaging the event tag vectors of tokens in the sentence. The experimental results\nare presented in Table 5. Compared with the baseline 1-layer model, other three 2-layer settings equipped with information aggregation and cross-sentence propagation performs better. It proves that sentence information aggregation module can integrate some useful information and propagate it to other sentences through the decoder. On the other hand, the performance of LSTM and concat are comparable and stronger than average. Considering that the input of the information aggregation module is the event tag vector obtained by the bidirectional decoder, which has captured the sequential event information. Therefore, it is not surprising that LSTM does not have that great advantage over concat and average."
    }, {
      "heading" : "4 Related Work",
      "text" : "Event detection is a well-studied task with research effort in the last decade. The existing methods (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2017; Nguyen and Grishman, 2018; Deng et al., 2020; Tong et al., 2020; Lai et al., 2020; Liu et al., 2020; Li et al., 2020; Cui et al., 2020; Deng et al., 2021; Shen et al., 2021) mainly focus on sentence-level event trigger extraction, neglecting the document information. Or the document-level semantic and event inter-dependency information are modeled separately.\nFor the problem of event inter-dependency modeling, some methods were proposed to jointly extract triggers within a sentence. Among them, Chen et al. (2015) used dynamic multi-pooling CNN to preserve information of multiple events; Nguyen et al. (2016) utilized the bidirectional recurrent neural networks to extract events; Liu et al. (2018) introduced syntactic shortcut arcs to enhance information flow and used graph neural networks to model graph information; Chen et al. (2018) proposed a hierarchical tagging LSTM layer and tagging attention mechanism to model the event interdependency within a sentence. Considering that adjacent sentences also store some relevant event information, which would enhance the event signals of other sentences. These methods would miss the event inter-dependency information across sentences. For document-level event inter-dependency modeling, Lin et al. (2020) proposed to incorporate global features to capture the cross-subtask and cross-instance interactions.\nThe deep learning methods on document-level semantic information aggregation are primarily\nbased on multi-level attention mechanism. Chen et al. (2018) integrated document information by introducing a multi-level attention. Zhao et al. (2018) used trigger and sentence supervised attention to aggregate information and enhance the sentence-level event detection. Zheng et al. (2019) utilized the memory network to store document level contextual information and entities. Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012). And Zhang et al. (2020) proposed to aggregate the document-level information by latent topic modeling. The attention-based documentlevel information aggregation mechanisms treat all sentences in the document equally, which may introduce some noises from distant sentences. And the feature-based methods require extensive human engineering, which also greatly affects the portability of the model."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper presents a novel Multi-Layer Bidirectional Network (MLBiNet) to propagate documentlevel semantic and event inter-dependency information for event detection task. To the best of our knowledge, this is the first work to unify them in one model. Firstly, a bidirectional decoder is proposed to explicitly model the sentence-level event inter-dependency, and event relevant information within a sentence is aggregated by an information aggregation module. Then the multiple bidirectional tagging layers stacking mechanism is devised to iteratively propagate semantic and event-related information across sentence. We conducted extensive experiments on the widely-used ACE 2005 corpus, the results demonstrate the effectiveness of our model, as well as all modules we proposed.\nIn the future, we will extend the model to the event argument extraction task and other information extraction tasks, where the document-level semantic aggregation and object inter-dependency are critical. For example, the recently concerned document-level relation extraction (Quirk and Poon, 2017; Yao et al., 2019), which requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. For other sequence labeling tasks, such as the named entity recognition, we can also utilize the proposed architecture\nto model the entity label dependency."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We want to express gratitude to the anonymous reviewers for their hard work and kind comments. This work is funded by NSFCU19B2027/91846204, National Key R&D Program of China (Funding No.SQ2018YFC000004)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR, abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Collective event detection via a hierarchical and bias tagging networks with gated multilevel attention mechanisms",
      "author" : [ "Yubo Chen", "Hang Yang", "Kang Liu", "Jun Zhao", "Yantao Jia." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Edgeenhanced graph convolution networks for event detection with syntactic relation",
      "author" : [ "Shiyao Cui", "Bowen Yu", "Tingwen Liu", "Zhenyu Zhang", "Xuebin Wang", "Jinqiao Shi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Metalearning with dynamic-memory-based prototypical network for few-shot event detection",
      "author" : [ "Shumin Deng", "Ningyu Zhang", "Jiaojian Kang", "Yichi Zhang", "Wei Zhang", "Huajun Chen." ],
      "venue" : "Proceedings of the 13th International Conference on Web",
      "citeRegEx" : "Deng et al\\.,? 2020",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "Ontoed: Low-resource event detection with ontology embedding",
      "author" : [ "Shumin Deng", "Ningyu Zhang", "Luoqiu Li", "Hui Chen", "Huaixiao Tou", "Mosha Chen", "Fei Huang", "Huajun Chen." ],
      "venue" : "ACL. Association for Computational Linguistics.",
      "citeRegEx" : "Deng et al\\.,? 2021",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert: Pre-training of deep",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-sentence argument linking",
      "author" : [ "Seth Ebner", "Patrick Xia", "Ryan Culkin", "Kyle Rawlins", "Benjamin Van Durme." ],
      "venue" : "arXiv preprint arXiv:1911.03766.",
      "citeRegEx" : "Ebner et al\\.,? 2019",
      "shortCiteRegEx" : "Ebner et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Using cross-entity inference to improve event extraction",
      "author" : [ "Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Hong et al\\.,? 2011",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2011
    }, {
      "title" : "Semi-supervised new event type induction and event detection",
      "author" : [ "Lifu Huang", "Heng Ji." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 718–724.",
      "citeRegEx" : "Huang and Ji.,? 2020",
      "shortCiteRegEx" : "Huang and Ji.",
      "year" : 2020
    }, {
      "title" : "Modeling textual cohesion for event extraction",
      "author" : [ "Ruihong Huang", "Ellen Riloff." ],
      "venue" : "Proceedings of the 26th Conference on Artificial Intelligence.",
      "citeRegEx" : "Huang and Riloff.,? 2012",
      "shortCiteRegEx" : "Huang and Riloff.",
      "year" : 2012
    }, {
      "title" : "Refining event extraction through cross-document inference",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 254–262.",
      "citeRegEx" : "Ji and Grishman.,? 2008",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2017
    }, {
      "title" : "Event detection: Gate diversity and syntactic importance scoresfor graph convolution neural networks",
      "author" : [ "Viet Dac Lai", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "arXiv preprint arXiv:2010.14123.",
      "citeRegEx" : "Lai et al\\.,? 2020",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2020
    }, {
      "title" : "Event extraction as multi-turn question answering",
      "author" : [ "Fayuan Li", "Weihua Peng", "Yuguang Chen", "Quan Wang", "Lu Pan", "Yajuan Lyu", "Yong Zhu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Using document level cross-event inference to improve event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789–797.",
      "citeRegEx" : "Liao and Grishman.,? 2010",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "A joint neural model for information extraction with global features",
      "author" : [ "Ying Lin", "Heng Ji", "Fei Huang", "Lingfei Wu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7999–8009.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6754–6761.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Event extraction as machine reading comprehension",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Wei Bi", "Xiaojiang Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1641–1651.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting argument information to improve event detection via supervised attention mechanisms",
      "author" : [ "Shulin Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Meeting of the Association for Computational Linguistics, pages 1789–1798.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly multiple events extraction via attentionbased graph information aggregation",
      "author" : [ "Xiao Liu", "Zhunchen Luo", "Heyan Huang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic event extraction with structured preference modeling",
      "author" : [ "Wei Lu", "Dan Roth." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 835–844.",
      "citeRegEx" : "Lu and Roth.,? 2012",
      "shortCiteRegEx" : "Lu and Roth.",
      "year" : 2012
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "International Conference on Learning Representations, abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Event detection and domain adaptation with convolutional neural networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Graph convolutional networks with argument-aware pooling for event detection",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, pages 5900–5907.",
      "citeRegEx" : "Nguyen and Grishman.,? 2018",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2018
    }, {
      "title" : "Distant supervision for relation extraction beyond the sentence boundary",
      "author" : [ "Chris Quirk", "Hoifung Poon." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Quirk and Poon.,? 2017",
      "shortCiteRegEx" : "Quirk and Poon.",
      "year" : 2017
    }, {
      "title" : "Hierarchical chinese legal",
      "author" : [ "Lusheng Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2020
    }, {
      "title" : "Adversarial training for weakly",
      "author" : [ "Peng Li" ],
      "venue" : null,
      "citeRegEx" : "Li.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2019
    }, {
      "title" : "Event detection with",
      "author" : [ "Guo", "Xueqi Cheng" ],
      "venue" : null,
      "citeRegEx" : "Guo and Cheng.,? \\Q2019\\E",
      "shortCiteRegEx" : "Guo and Cheng.",
      "year" : 2019
    }, {
      "title" : "Document embedding enhanced event detection with hierarchical and supervised attention",
      "author" : [ "Yue Zhao", "Xiaolong Jin", "Yuanzhuo Wang", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 414–",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Doc2EDAG: An end-to-end document-level framework for Chinese financial event extraction",
      "author" : [ "Shun Zheng", "Wei Cao", "Wei Xu", "Jiang Bian." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "In ACE 2005 corpus, the arguments of a single event instance may be scattered in multiple sentences (Zheng et al., 2019; Ebner et al., 2019), which indicates that document-level information aggregation is critical for ED task.",
      "startOffset" : 100,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "In ACE 2005 corpus, the arguments of a single event instance may be scattered in multiple sentences (Zheng et al., 2019; Ebner et al., 2019), which indicates that document-level information aggregation is critical for ED task.",
      "startOffset" : 100,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Intra-sentence and inter-sentence event inter-dependency modeling (Liao and Grishman, 2010; Chen et al., 2018; Liu et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Intra-sentence and inter-sentence event inter-dependency modeling (Liao and Grishman, 2010; Chen et al., 2018; Liu et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "Intra-sentence and inter-sentence event inter-dependency modeling (Liao and Grishman, 2010; Chen et al., 2018; Liu et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "To address those issues, previous approaches (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yan et al., 2019; Liu et al., 2019; Zhang et al., 2019) mainly focused on sentence-level event de-",
      "startOffset" : 45,
      "endOffset" : 159
    }, {
      "referenceID" : 26,
      "context" : "To address those issues, previous approaches (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yan et al., 2019; Liu et al., 2019; Zhang et al., 2019) mainly focused on sentence-level event de-",
      "startOffset" : 45,
      "endOffset" : 159
    }, {
      "referenceID" : 22,
      "context" : "To address those issues, previous approaches (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yan et al., 2019; Liu et al., 2019; Zhang et al., 2019) mainly focused on sentence-level event de-",
      "startOffset" : 45,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "To address those issues, previous approaches (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yan et al., 2019; Liu et al., 2019; Zhang et al., 2019) mainly focused on sentence-level event de-",
      "startOffset" : 45,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Some studies (Chen et al., 2018; Zhao et al., 2018) tried to integrate semantic information across sentences via the attention mechanism.",
      "startOffset" : 13,
      "endOffset" : 51
    }, {
      "referenceID" : 33,
      "context" : "Some studies (Chen et al., 2018; Zhao et al., 2018) tried to integrate semantic information across sentences via the attention mechanism.",
      "startOffset" : 13,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "collectively detecting multiple events (Chen et al., 2018; Liu et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "collectively detecting multiple events (Chen et al., 2018; Liu et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "The RNN-based encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gu et al., 2016) consists of two components: a) an encoder which converts the source sentence into a fixed length vector c and",
      "startOffset" : 40,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "The RNN-based encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gu et al., 2016) consists of two components: a) an encoder which converts the source sentence into a fixed length vector c and",
      "startOffset" : 40,
      "endOffset" : 142
    }, {
      "referenceID" : 24,
      "context" : "The RNN-based encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gu et al., 2016) consists of two components: a) an encoder which converts the source sentence into a fixed length vector c and",
      "startOffset" : 40,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "The RNN-based encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gu et al., 2016) consists of two components: a) an encoder which converts the source sentence into a fixed length vector c and",
      "startOffset" : 40,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "As is formalized in (Gu et al., 2016), the source sentence si is converted into a fixed length vector c by the encoder RNN,",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Studies (Bahdanau et al., 2015; Luong et al., 2015) have shown that summarizing the entire source sentence into a fixed length vector will limit the performance of the decoder.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : "Studies (Bahdanau et al., 2015; Luong et al., 2015) have shown that summarizing the entire source sentence into a fixed length vector will limit the performance of the decoder.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 25,
      "context" : "The word embedding matrix is pretrained by Skip-gram model (Mikolov et al., 2013), and the NER type embedding matrix is randomly initialized and updated in the training process.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "We apply the attention mechanism proposed by (Luong et al., 2015) with the weights derived by",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "An alternative method modeling the sentencelevel event inter-dependency called hierarchical tagging layer is proposed by (Chen et al., 2018).",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "For comparison, we investigated the performance of the following state-of-the-art methods: 1) DMCNN (Chen et al., 2015), which extracts multiple events from one sentence with dynamic multipooling CNN; 2) HBTNGMA (Chen et al.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : ", 2015), which extracts multiple events from one sentence with dynamic multipooling CNN; 2) HBTNGMA (Chen et al., 2018), which models sentence event inter-dependency via a hierarchical tagging model; 3) JMEE (Liu et al.",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : ", 2018), which models sentence event inter-dependency via a hierarchical tagging model; 3) JMEE (Liu et al., 2018), which models the sentence-level event interdependency via a graph model of the sentence syntactic parsing graph; 4) DMBERT-Boot (Wang et al.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "(Huang and Ji, 2020), which learns to induct new event type by a semi-supervised vector quantized variational autoencoder framework, and fine-tunes with the pre-trained BERT-large model.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "The powerful encoder of BERT pre-trained model (Devlin et al., 2018) has been proven to improve the performance of downstream NLP tasks.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "methods (Chen et al., 2015, 2018; Liu et al., 2018) aim to extract multiple events jointly within a sentence.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012).",
      "startOffset" : 83,
      "endOffset" : 222
    }, {
      "referenceID" : 17,
      "context" : "Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012).",
      "startOffset" : 83,
      "endOffset" : 222
    }, {
      "referenceID" : 10,
      "context" : "Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012).",
      "startOffset" : 83,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012).",
      "startOffset" : 83,
      "endOffset" : 222
    }, {
      "referenceID" : 23,
      "context" : "Some feature-based document level information aggregation methods were proposed by (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012; Reichart and Barzilay, 2012; Lu and Roth, 2012).",
      "startOffset" : 83,
      "endOffset" : 222
    }, {
      "referenceID" : 29,
      "context" : "For example, the recently concerned document-level relation extraction (Quirk and Poon, 2017; Yao et al., 2019), which requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document.",
      "startOffset" : 71,
      "endOffset" : 111
    } ],
    "year" : 2021,
    "abstractText" : "We consider the problem of collectively detecting multiple events, particularly in crosssentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a documentlevel. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentencelevel semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results1.",
    "creator" : "LaTeX with hyperref"
  }
}