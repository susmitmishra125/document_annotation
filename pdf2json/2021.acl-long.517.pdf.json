{
  "name" : "2021.acl-long.517.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study",
    "authors" : [ "Divyansh Kaushik", "Douwe Kiela", "Zachary C. Lipton", "Wen-tau Yih" ],
    "emails" : [ "dkaushik@cmu.edu,", "zlipton@cmu.edu,", "dkiela@fb.com", "scottyih@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6618–6633\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6618"
    }, {
      "heading" : "1 Introduction",
      "text" : "Across such diverse natural language processing (NLP) tasks as natural language inference (NLI; Poliak et al., 2018; Gururangan et al., 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al., 2020), researchers have discovered that models can succeed on popular benchmarks by exploiting spurious associations that characterize a particular dataset but do not hold more widely. Despite performing well on independent and identically distributed (i.i.d.) data, these models are liable under plausible domain shifts. With the goal of providing more challenging benchmarks that require this stronger form of generalization, an emerging line of research has\n1Data collected during this study is publicly available at https://github.com/facebookresearch/aqa-study.\ninvestigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019).\nWhile adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy?\nIn this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to\ngenerate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount.\nWe fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-à-vis robustness under distribution shift.\nTo study the differences between adversarial and standard data, we perform a qualitative analysis, categorizing questions based on a taxonomy (Hovy et al., 2000). We notice that more questions in the ADC dataset require numerical reasoning compared to the SDC sample. These qualitative insights may offer additional guidance to future researchers."
    }, {
      "heading" : "2 Related Work",
      "text" : "In an early example of model-in-the-loop data collection, Zweig and Burges (2012) use n-gram lan-\nguage models to suggest candidate incorrect answers for a fill-in-the-blank task. Richardson et al. (2013) suggested ADC for QA as proposed future work, speculating that it might challenge state-ofthe-art models. In the Build It Break It, The Language Edition shared task (Ettinger et al., 2017), teams worked as builders (training models) and breakers (creating challenging examples for subsequent training) for sentiment analysis and QA-SRL.\nResearch on ADC has picked up recently, with Chen et al. (2019) tasking crowdworkers to construct multiple-choice questions to fool a BERT model and Wallace et al. (2019) employing Quizbowl community members to write Jeopardystyle questions to compete against QA models. Zhang et al. (2018) automatically generated questions from news articles, keeping only those questions that were incorrectly answered by a QA model. Dua et al. (2019) and Dasigi et al. (2019) required crowdworkers to submit only questions that QA models answered incorrectly. To construct FEVER 2.0 (Thorne et al., 2019), crowdworkers were required to fool a fact-verification system trained on the FEVER (Thorne et al., 2018) dataset. Some works explore ADC over multiple rounds, with adversarial data from one round used to train models in the subsequent round. Yang et al. (2018b) ask workers to generate challenging datasets working first as adversaries and later as collaborators. Dinan et al. (2019) build on their work, employing ADC to address offensive lan-\nguage identification. They find that over successive rounds of training, models trained on ADC data are harder for humans to fool than those trained on standard data. Nie et al. (2020) applied ADC for an NLI task over three rounds, finding that training for more rounds improves model performance on adversarial data, and observing improvements on the original evaluations set when training on a mixture of original and adversarial training data. Williams et al. (2020) conducted an error analysis of model predictions on the datasets collected by Nie et al. (2020). Bartolo et al. (2020) studied the empirical efficacy of ADC for SQuAD (Rajpurkar et al., 2016), observing improved performance on adversarial test sets but noting that trends vary depending on the models used to collect data and to train. Previously, Lowell et al. (2019) observed similar issues in active learning, when the models used to acquire data and for subsequent training differ. Yang et al. (2018a); Zellers et al. (2018, 2019) first collect datasets and then filter examples based on predictions from a model. Paperno et al. (2016) apply a similar procedure to generate a language modeling dataset (LAMBADA). Kaushik et al. (2020, 2021) collect counterfactually augmented data (CAD) by asking crowdworkers to edit existing documents to make counterfactual labels applicable, showing that models trained on CAD generalize better out-of-domain.\nAbsent further assumptions, learning classifiers robust to distribution shift is impossible (BenDavid et al., 2010). While few NLP papers on the matter make their assumptions explicit, they typically proceed under the implicit assumptions that the labeling function is deterministic (there is one right answer), and that covariate shift (Shimodaira, 2000) applies (the labeling function p(y|x) is invariant across domains). Note that neither condition is generally true of prediction problems. For example, faced with label shift (Schölkopf et al., 2012; Lipton et al., 2018) p(y|x) can change across distributions, requiring one to adapt the predictor to each environment."
    }, {
      "heading" : "3 Study Design",
      "text" : "In our study of ADC for QA, each crowdworker is shown a short passage and asked to create 5 questions and highlight answers (spans in the passage, see Fig. 1). We provide all workers with the same base pay and for those assigned to ADC, pay out an additional bonus for each question that fools\nthe QA model. Finally, we field a different set of workers to validate the generated examples.\nContext passages For context passages, we use the first 100 words of Wikipedia articles. Truncating the articles keeps the task of generating questions from growing unwieldy. These segments typically contain an overview, providing ample material for factoid questions. We restrict the pool of candidate contexts by leveraging a variant of the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019). We first keep only a subset of 23.1k question/answer pairs for which the context passages are the first 100 words of Wikipedia articles2. From these passages, we sample 10k at random for our study.\nModels in the loop We use BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020) models as our adversarial models in the loop, using the implementations provided by Wolf et al. (2020). We fine-tune these models for span-based question-answering, using the 23.1k training examples (subsampled previously) for 20 epochs, with early-stopping based on word-overlap F13 over the validation set. Our BERT model achieves an EM score of 73.1 and an F1 score of 80.5 on an i.i.d. validation set. The ELECTRA model performs slightly better, obtaining an 74.2 EM and 81.2 F1 on the same set.\nCrowdsourcing protocol We build our crowdsourcing platform on the Dynabench interface (Kiela et al., 2021) and use Amazon’s Mechanical Turk to recruit workers to write questions. To ensure high quality, we restricted the pool to U.S. residents who had already completed at least 1000 HITs and had over 98% HIT approval rate. For each task, we conducted several pilot studies to gather feedback from crowdworkers on the task and interface. We identified median time taken by workers to complete the task in our pilot studies and used that to design the incentive structure for the main task. We also conducted multiple studies with different variants of instructions to observe trends in the quality of questions and refined our instructions based on feedback from crowdworkers. Feedback from the pilots also guided improvements to\n2We used the data prepared by Karpukhin et al. (2020), available at https://www.github.com/facebookresearch/DPR.\n3Word-overlap F1 and Exact Match (EM) metrics introduced in Rajpurkar et al. (2016) are commonly used to evaluate performance of passage-based QA systems, where the correct answer is a span in the given passage.\nour crowdsourcing interface. In total, 984 workers took part in the study, with 741 creating questions. In our final study, we randomly assigned workers to generate questions in the following ways: (i) to fool the BERT baseline; (ii) to fool the ELECTRA baseline; or (iii) without a model in the loop. Before beginning the task, each worker completes an onboarding process to familiarize them with the platform. We present the same set of passages to workers regardless of which group they are assigned to, tasking them with generating 5 questions for each passage.\nIncentive structure During our pilot studies, we found that workers spend ≈ 2–3 minutes to generate 5 questions. We provide workers with the same base pay—$0.75 per HIT—(to ensure compensation at a $15/hour rate). For tasks involving a model in the loop, we define a model prediction to be incorrect if its F1 score is less than 40%, following the threshold set by Bartolo et al. (2020). Workers tasked with fooling the model receive bonus pay of $0.15 for every question that leads to an incorrect model prediction. This way, a worker can double their pay if all 5 of their generated questions induce incorrect model predictions.\nQuality control Upon completion of each batch of our data collection process, we presented≈ 20% of the collected questions to a fourth group of crowdworkers who were tasked with validating whether the questions were answerable and the answers were correctly labeled. In addition, we manually verified a small fraction of the collected question-answer pairs. If validations of at least 20% of the examples generated by a particular worker were incorrect, their work was discarded in its entirety. The entire process, including the pilot studies cost ≈ $50k and spanned a period of seven months. Through this process, we collected over 150k question-answer pairs corresponding to the 10k contexts (50k from each group) but the final datasets are much smaller, as we explain below."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "Our study allows us to answer three questions: (i) how well do models fine-tuned on ADC data generalize to unseen distributions compared to finetuning on SDC? (ii) Among the differences between ADC and SDC, how many are due to workers trying to fool the model regardless of whether they are successful? and (iii) what is the impact of training on adversarial data only versus using it as a data augmentation strategy?\nDatasets For both BERT and ELECTRA, we first identify contexts for which at least one question elicited an incorrect model prediction. Note that this set of contexts is different for BERT and ELECTRA. For each such context c, we identify the number of questions kc (out of 5) that successfully fooled the model. We then create 3 datasets per model by, for each context, (i) choosing precisely those kc questions that fooled the model (BERTfooled and ELECTRAfooled); (ii) randomly choosing kc questions (out of 5) from ADC data without replacement (BERTrandom and ELECTRArandom)—regardless of whether they fooled the model; and (iii) randomly choosing kc questions (out of 5) from the SDC data without replacement. Thus, we create 6 datasets, where all 3 BERT datasets have the same number of questions per context (and 11.3k total training examples), while all 3 ELECTRA datasets likewise share the same number of questions per context (and 14.7k total training examples). See Table 1 for details on the number of passages and question-answer pairs used in the different splits.\nModels For our empirical analysis, we fine-tune BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al., 2020) models on all six datasets generated as part of our study (four datasets via ADC: BERTfooled, BERTrandom, ELECTRAfooled, ELECTRArandom, and the two datasets via SDC). We also fine-tune these models after augmenting the original data to collected datasets. We report the means and standard deviations (in subscript) of EM and F1 scores following 10 runs of each experiment. Models fine-tuned on all ADC datasets typically perform better on their held-out test sets than those trained on SDC data and vice-versa (Table 2 and Appendix Table 5). RoBERTa fine-tuned on the BERTfooled training set obtains EM and F1 scores of 49.2 and 71.2, respectively, on the BERTfooled test set, outperforming\nRoBERTa models fine-tuned on BERTrandom (EM: 48.0, F1: 69.8) and SDC (EM: 42.0, F1: 65.3). Performance on the original dev set (Karpukhin et al., 2020) is generally comparable across all models.\nOut-of-domain generalization to adversarial data We evaluate these models on adversarial test sets constructed with BiDAF (DBiDAF), BERT (DBERT) and RoBERTa (DRoBERTa) in the loop (Bartolo et al., 2020). Prior work suggests that training on ADC data leads to models that perform better on similarly constructed adversarial evaluation sets. Both BERT and RoBERTa models fine-tuned on adversarial data generally outperform models fine-tuned on SDC data (or when either datasets are augmented to the original data) on all three evaluation sets (Table 3 and Appendix Table 6). A RoBERTa model fine-tuned on BERTfooled outperforms a RoBERTa model fine-tuned on SDC by 9.1, 9.3, and 6.2 EM points on DRoBERTa, DBERT, and DBiDAF, respectively. We observe similar trends on ELECTRA models fine-tuned on ADC data versus SDC data, but these gains disappear when the same models are finetuned on augmented data. For instance, while ELECTRA fine-tuned on BERTrandom obtains an EM score of 14.8 on DRoBERTa, outperforming an ELECTRA fine-tuned on SDC data by≈ 3 pts, the difference is no longer significant when respective models are fine-tuned\nafter original data is augmented to these datasets. ELECTRA models fine-tuned on ADC data with ELECTRA in the loop perform no better than those trained on SDC. Fine-tuning ELECTRA on SDC augmented to original data leads to an ≈ 1 pt improvement on both metrics compared to augmenting ADC. Overall, we find that models fine-tuned on ADC data typically generalize better to out-ofdomain adversarial test sets than models fine-tuned on SDC data, confirming the findings by Dinan et al. (2019).\nOut-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on\n4The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017).\n5Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020).\nADC data collected with BERT) on 9 out of 12 MRQA datasets, with gains of more than 10 EM pts on 6 of them. On BioASQ, BERT fine-tuned on BERTfooled obtains EM and F1 scores of 23.5 and 30.3, respectively. By comparison, fine-tuning on SDC data yields markedly higher EM and F1 scores of 35.1 and 55.7, respectively. Similar trends hold across models and datasets. Interestingly, ADC fine-tuning often improves performance on DROP compared to SDC. For instance, RoBERTa finetuned on ELECTRArandom outperforms RoBERTa fine-tuned on SDC by ≈ 7 pts. Note that DROP itself was adversarially constructed. On Natural Questions, models fine-tuned on ADC data generally perform comparably to those fine-tuned on SDC data. RoBERTa fine-tuned on BERTrandom obtains EM and F1 scores of 48.1 and 62.6, respectively, whereas RoBERTa fine-tuned on SDC data obtains scores of 47.9 and 61.7, respectively. It is worth noting that passages sourced to construct both ADC and SDC datasets come from the Natural Questions dataset, which could be one reason why models fine-tuned on ADC datasets perform similar to those fine-tuned on SDC datasets when evaluated on Natural Questions.\nOn the the adversarial process versus adversarial success We notice that models fine-tuned on\nBERTrandom and ELECTRArandom typically outperform models fine-tuned on BERTfooled and ELECTRAfooled, respectively, on adversarial test data collected in prior work (Bartolo et al., 2020), as well as on MRQA. Similar observation can be made when the ADC data is augmented with the original training data. These trends suggest that the ADC process (regardless of the outcome) explains our results more than successfully fooling a model. Furthermore, models fine-tuned only on SDC data tend to outperform ADC-only fine-tuned models; however, following augmentation, ADC fine-tuning achieves comparable performance on more datasets than before, showcasing generalization following augmentation. Notice that augmenting ADC data to original data may not always help. BERT fine-tuned on original 23.1k examples achieves an EM 11.3 on SearchQA. When fine-tuned on BERTfooled augmented to the original data, this drops to 8.7, and when fine-tuned on BERTrandom augmented to the original data, it drops to 11.2. Fine-tuning on SDC augmented to the original data, however, results in EM of 13.6."
    }, {
      "heading" : "5 Qualitative Analysis",
      "text" : "Finally, we perform a qualitative analysis over the collected data, revealing profound differences with models in (versus out of) the loop. Recall that be-\ncause these datasets were constructed in a randomized study, any observed differences are attributable to the model-in-the loop collection scheme.\nTo begin, we analyze 100 questions from each\ndataset and categorize them using the taxonomy introduced by Hovy et al. (2000).6 We also look at\n6This taxonomy can be accessed at https://www.isi.edu/nat ural-language/projects/webclopedia/Taxonomy/taxonomy\nthe first word of the wh-type questions in each dev set (Fig. 3) and observe key qualitative differences between data via ADC and SDC for both models.\nIn case of ADC with BERT (and associated SDC), while we observe that most questions in the dev sets start with what, ADC has a higher proportion compared to SDC (587 in BERTfooled and 492 in BERTrandom versus 416 in SDC). Furthermore, we notice that compared to BERTfooled dev set, SDC has more when- (148) and who-type (220) questions, the answers to which typically refer to dates, places and people (or organizations), respectively. This is also reflected in the taxonomy categorization. Interestingly, the BERTrandom dev set has more when- and who-type questions than BERTfooled (103 and 182 versus 50 and 159, respectively). This indicates that the BERT model could have been better at answering questions related to dates and people (or organizations), which could have further incentivized workers not to generate\ntoplevel.html\nsuch questions upon observing these patterns. Similarly, in the 100-question samples, we find that a larger proportion of questions in ADC are categorized as requiring numerical reasoning (11 and 18 in BERTfooled and BERTrandom, respectively) compared to SDC (7). It is possible that the model’s performance on numerical reasoning (as also demonstrated by its lower performance on DROP compared to fine-tuning on ADC or SDC) would have incentivized workers to generate more questions requiring numerical reasoning and as a result, skewed the distribution towards such questions.\nSimilarly, with ELECTRA, we observe that what-type questions constitute most of the questions in the development sets for both ADC and SDC, although data collected via ADC has a higher proportion of these (641 in ELECTRAfooled and 619 in ELECTRArandom versus 542 in SDC). We also notice more how-type questions in ADC (126 in ELECTRArandom) vs 101 in SDC, and that the SDC sample has more questions that relate\nto dates (223) but the number is lower in the ADC samples (157 and 86 in ELECTRArandom and ELECTRAfooled, respectively). As with BERT, the ELECTRA model was likely better at identifying answers about dates or years which could have further incentivized workers to generate less questions of such types. However, unlike with BERT, we observe that the ELECTRA ADC and SDC 100-question samples contain similar numbers of questions involving numerical answers (8, 9 and 10 in ELECTRAfooled, ELECTRArandom and SDC respectively).\nLastly, despite explicit instructions not to generate questions about passage structure (Fig. 1), a small number of workers nevertheless created such questions. For instance, one worker wrote, “What is the number in the passage that is one digit less than the largest number in the passage?” While most such questions were discarded during validation, some of these are present in the final data. Overall, we notice considerable differences between ADC and SDC data, particularly vis-avis what kind of questions workers generate. Our qualitative analysis offers additional insights that suggest that ADC would skew the distribution of questions workers create, as the incentives align with quickly creating more questions that can fool the model. This is reflected in all our ADC datasets. One remedy could be to provide workers with initial questions, asking them to edit those questions to elicit incorrect model predictions. Similar strategies were employed in (Ettinger et al., 2017), where breakers minimally edited original data to elicit incorrect predictions from the models built by builders, as well as in recently introduced adversarial benchmarks for sentiment analysis (Potts et al., 2020)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially. Additionally, our results suggest that the ADC process (regardless of the outcome) might matter more than successfully fooling a model. We also identify key qualitative differences between data generated via ADC and SDC, particularly the kinds of questions created.\nOverall, our work investigates ADC in a con-\ntrolled setting, offering insights that can guide future research in this direction. These findings are particularly important given that ADC is more timeconsuming and expensive than SDC, with workers requiring additional financial incentives. We believe that a remedy to these issues could be to ask workers to edit questions rather than to generate them. In the future, we would like to extend this study and investigate the efficacy of various constraints on question creation, and the role of other factors such as domain complexity, passage length, and incentive structure, among others."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors thank Max Bartolo, Robin Jia, Tanya Marwah, Sanket Vaibhav Mehta, Sina Fazelpour, Kundan Krishna, Shantanu Gupta, Simran Kaur, and Aishwarya Kamath for their valuable feedback on the crowdsourcing platform and the paper.\nEthical Considerations\nThe passages in our datasets are sourced from the datasets released by Karpukhin et al. (2020) under a Creative Commons License. As described in main text, we designed our incentive structure to ensure that crowdworkers were paid $15/hour, which is twice the US federal minimum wage. Our datasets focus on the English language, and are not collected for the purpose of designing NLP applications but to conduct a human study. We share our dataset to allow the community to replicate our findings and do not foresee any risks associated with the use of this data."
    } ],
    "references" : [ {
      "title" : "Cloze-driven pretraining of self-attention networks",
      "author" : [ "Alexei Baevski", "Sergey Edunov", "Yinhan Liu", "Luke Zettlemoyer", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Baevski et al\\.,? 2019",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "Beat the AI: Investigating adversarial human annotation for reading comprehension",
      "author" : [ "Max Bartolo", "Alastair Roberts", "Johannes Welbl", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:662–678.",
      "citeRegEx" : "Bartolo et al\\.,? 2020",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2020
    }, {
      "title" : "Impossibility theorems for domain adaptation",
      "author" : [ "Shai Ben-David", "Tyler Lu", "Teresa Luu", "Dávid Pál." ],
      "venue" : "Artificial Intelligence and Statistics (AISTATS).",
      "citeRegEx" : "Ben.David et al\\.,? 2010",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "CODAH: An adversarially-authored question answering dataset for common sense",
      "author" : [ "Michael Chen", "Mike D’Arcy", "Alisa Liu", "Jared Fernandez", "Doug Downey" ],
      "venue" : "In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations",
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "author" : [ "Pradeep Dasigi", "Nelson F. Liu", "Ana Marasović", "Noah A. Smith", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "SearchQA: A new Q&A dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V Ugur Guney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards linguistically generalizable NLP systems: A workshop",
      "author" : [ "Allyson Ettinger", "Sudha Rao", "Hal Daumé III", "Emily M. Bender" ],
      "venue" : null,
      "citeRegEx" : "Ettinger et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2017
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering,",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretrained transformers improve out-of-distribution robustness",
      "author" : [ "Dan Hendrycks", "Xiaoyuan Liu", "Eric Wallace", "Adam Dziedzic", "Rishabh Krishnan", "Dawn Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hendrycks et al\\.,? 2020",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2020
    }, {
      "title" : "Question answering in webclopedia",
      "author" : [ "Eduard Hovy", "Laurie Gerber", "Ulf Hermjakob", "Michael Junk", "Chin-Yew Lin." ],
      "venue" : "TREC, volume 52.",
      "citeRegEx" : "Hovy et al\\.,? 2000",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2000
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning the difference that makes A difference with counterfactuallyaugmented data",
      "author" : [ "Divyansh Kaushik", "Eduard H. Hovy", "Zachary Chase Lipton." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis",
      "citeRegEx" : "Kaushik et al\\.,? 2020",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2020
    }, {
      "title" : "How much reading does reading comprehension require? a critical investigation of popular benchmarks",
      "author" : [ "Divyansh Kaushik", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Kaushik and Lipton.,? 2018",
      "shortCiteRegEx" : "Kaushik and Lipton.",
      "year" : 2018
    }, {
      "title" : "Explaining the efficacy of counterfactually-augmented data",
      "author" : [ "Divyansh Kaushik", "Amrith Setlur", "Eduard Hovy", "Zachary C Lipton." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kaushik et al\\.,? 2021",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2021
    }, {
      "title" : "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
      "author" : [ "Aniruddha Kembhavi", "Min Joon Seo", "Dustin Schwenk", "Jonghyun Choi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "2017 IEEE Conference",
      "citeRegEx" : "Kembhavi et al\\.,? 2017",
      "shortCiteRegEx" : "Kembhavi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dynabench: Rethinking benchmarking in NLP",
      "author" : [ "hit Bansal", "Christopher Potts", "Adina Williams" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Bansal et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2021
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "RACE: Large-scale ReAding comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot relation extraction via reading comprehension",
      "author" : [ "Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342.",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Detecting and correcting for label shift with black box predictors",
      "author" : [ "Zachary C. Lipton", "Yu-Xiang Wang", "Alexander J. Smola." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stock-",
      "citeRegEx" : "Lipton et al\\.,? 2018",
      "shortCiteRegEx" : "Lipton et al\\.",
      "year" : 2018
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Practical obstacles to deploying active learning",
      "author" : [ "David Lowell", "Zachary C. Lipton", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Lowell et al\\.,? 2019",
      "shortCiteRegEx" : "Lowell et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "The LAMBADA dataset: Word prediction requiring a broad discourse context",
      "author" : [ "Denis Paperno", "Germán Kruszewski", "Angeliki Lazaridou", "Ngoc Quan Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fernández" ],
      "venue" : null,
      "citeRegEx" : "Paperno et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Paperno et al\\.",
      "year" : 2016
    }, {
      "title" : "Hypothesis only baselines in natural language inference",
      "author" : [ "Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,",
      "citeRegEx" : "Poliak et al\\.,? 2018",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2018
    }, {
      "title" : "DynaSent: A Dynamic Benchmark for Sentiment Analysis",
      "author" : [ "Christopher Potts", "Zhengxuan Wu", "Atticus Geiger", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2012.15349.",
      "citeRegEx" : "Potts et al\\.,? 2020",
      "shortCiteRegEx" : "Potts et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "MCTest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Richardson et al\\.,? 2013",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "On causal and anticausal learning",
      "author" : [ "Bernhard Schölkopf", "Dominik Janzing", "Jonas Peters", "Eleni Sgouritsa", "Kun Zhang", "Joris M. Mooij." ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning, ICML.",
      "citeRegEx" : "Schölkopf et al\\.,? 2012",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2012
    }, {
      "title" : "Improving predictive inference under covariate shift by weighting the loglikelihood function",
      "author" : [ "Hidetoshi Shimodaira." ],
      "venue" : "Journal of statistical planning and inference, 90(2):227–244.",
      "citeRegEx" : "Shimodaira.,? 2000",
      "shortCiteRegEx" : "Shimodaira.",
      "year" : 2000
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "The FEVER2.0 shared task",
      "author" : [ "James Thorne", "Andreas Vlachos", "Oana Cocarascu", "Christos Christodoulopoulos", "Arpit Mittal" ],
      "venue" : "In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER),",
      "citeRegEx" : "Thorne et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2019
    }, {
      "title" : "An overview of the bioasq",
      "author" : [ "George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos" ],
      "venue" : null,
      "citeRegEx" : "Tsatsaronis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsatsaronis et al\\.",
      "year" : 2015
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:621–633.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering",
      "author" : [ "Eric Wallace", "Pedro Rodriguez", "Shi Feng", "Ikuya Yamada", "Jordan Boyd-Graber." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Anlizing the adversarial natural language inference dataset",
      "author" : [ "Adina Williams", "Tristan Thrush", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2010.12729.",
      "citeRegEx" : "Williams et al\\.,? 2020",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Em-",
      "citeRegEx" : "Yang et al\\.,? 2018a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Mastering the dungeon: Grounded language learning by mechanical turker descent",
      "author" : [ "Zhilin Yang", "Saizheng Zhang", "Jack Urbanek", "Will Feng", "Alexander H. Miller", "Arthur Szlam", "Douwe Kiela", "Jason Weston." ],
      "venue" : "6th International Conference on",
      "citeRegEx" : "Yang et al\\.,? 2018b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "HellaSwag: Can a machine really finish your sentence",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Record: Bridging the gap between human and machine commonsense reading comprehension",
      "author" : [ "Sheng Zhang", "Xiaodong Liu", "Jingjing Liu", "Jianfeng Gao", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "arXiv preprint arXiv:1810.12885.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "A challenge set for advancing language modeling",
      "author" : [ "Geoffrey Zweig", "Chris J.C. Burges." ],
      "venue" : "Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages",
      "citeRegEx" : "Zweig and Burges.,? 2012",
      "shortCiteRegEx" : "Zweig and Burges.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Across such diverse natural language processing (NLP) tasks as natural language inference (NLI; Poliak et al., 2018; Gururangan et al., 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al.",
      "startOffset" : 90,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "Across such diverse natural language processing (NLP) tasks as natural language inference (NLI; Poliak et al., 2018; Gururangan et al., 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al.",
      "startOffset" : 90,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : ", 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al.",
      "startOffset" : 28,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", 2018), question answering (QA; Kaushik and Lipton, 2018), and sentiment analysis (Kaushik et al., 2020), researchers have discovered that models can succeed on popular benchmarks by exploiting spurious associations that characterize a particular dataset but do not hold more widely.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 221
    }, {
      "referenceID" : 7,
      "context" : "Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : ", 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : ", 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al.",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "To study the differences between adversarial and standard data, we perform a qualitative analysis, categorizing questions based on a taxonomy (Hovy et al., 2000).",
      "startOffset" : 142,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "In the Build It Break It, The Language Edition shared task (Ettinger et al., 2017), teams worked as builders (training models) and breakers (creating challenging examples for subsequent training) for sentiment analysis and QA-SRL.",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 38,
      "context" : "0 (Thorne et al., 2019), crowdworkers were required to fool a fact-verification system trained on the FEVER (Thorne et al.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 37,
      "context" : ", 2019), crowdworkers were required to fool a fact-verification system trained on the FEVER (Thorne et al., 2018) dataset.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "(2020) studied the empirical efficacy of ADC for SQuAD (Rajpurkar et al., 2016), observing improved performance on adversarial test sets but noting that trends vary depending on the models used to collect data and to train.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : "While few NLP papers on the matter make their assumptions explicit, they typically proceed under the implicit assumptions that the labeling function is deterministic (there is one right answer), and that covariate shift (Shimodaira, 2000) applies (the labeling function p(y|x) is invariant across domains).",
      "startOffset" : 220,
      "endOffset" : 238
    }, {
      "referenceID" : 35,
      "context" : "For example, faced with label shift (Schölkopf et al., 2012; Lipton et al., 2018) p(y|x) can change across distributions, requiring one to adapt the predictor to each environment.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "For example, faced with label shift (Schölkopf et al., 2012; Lipton et al., 2018) p(y|x) can change across distributions, requiring one to adapt the predictor to each environment.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "We restrict the pool of candidate contexts by leveraging a variant of the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019).",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Models in the loop We use BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : ", 2019) and ELECTRAlarge (Clark et al., 2020) models as our adversarial models in the loop, using the implementations provided by Wolf et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Models For our empirical analysis, we fine-tune BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : ", 2019), and ELECTRA (Clark et al., 2020) models on all six datasets generated as part of our study (four datasets via ADC: BERTfooled, BERTrandom, ELECTRAfooled, ELECTRArandom, and the two datasets via SDC).",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Performance on the original dev set (Karpukhin et al., 2020) is generally comparable across all models.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "Out-of-domain generalization to adversarial data We evaluate these models on adversarial test sets constructed with BiDAF (DBiDAF), BERT (DBERT) and RoBERTa (DRoBERTa) in the loop (Bartolo et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 202
    }, {
      "referenceID" : 44,
      "context" : "5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on (4)The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al.",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : ", 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 33,
      "context" : ", 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : ", 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : ", 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : ", 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 13,
      "context" : "Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 40,
      "context" : "Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 1,
      "context" : "On the the adversarial process versus adversarial success We notice that models fine-tuned on BERTrandom and ELECTRArandom typically outperform models fine-tuned on BERTfooled and ELECTRAfooled, respectively, on adversarial test data collected in prior work (Bartolo et al., 2020), as well as on MRQA.",
      "startOffset" : 258,
      "endOffset" : 280
    }, {
      "referenceID" : 10,
      "context" : "Similar strategies were employed in (Ettinger et al., 2017), where breakers minimally edited original data to elicit incorrect predictions from the models built by builders, as well as in recently introduced adversarial benchmarks for sentiment analysis (Potts et al.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : ", 2017), where breakers minimally edited original data to elicit incorrect predictions from the models built by builders, as well as in recently introduced adversarial benchmarks for sentiment analysis (Potts et al., 2020).",
      "startOffset" : 202,
      "endOffset" : 222
    } ],
    "year" : 2021,
    "abstractText" : "In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC’s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.1",
    "creator" : "LaTeX with hyperref"
  }
}