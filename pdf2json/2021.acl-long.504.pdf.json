{
  "name" : "2021.acl-long.504.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Selective Knowledge Distillation for Neural Machine Translation",
    "authors" : [ "Fusheng Wang", "Jianhao Yan", "Fandong Meng", "Jie Zhou" ],
    "emails" : [ "wfs0315@pku.edu.com", "fandongmeng}@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6456–6466\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6456"
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu\n∗Equal contribution. †This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1We release our code on https://github.com/Les lieOverfitting/selective distillation.\net al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches, the model learns from teacher models by minimizing gaps between their outputs on every training word/sentence (i.e., corresponding training sample) without distinction.\nDespite their promising results, previous studies mainly focus on finding what to teach and rarely investigate how these words/sentences (i.e., samples), which serve as the medium or carrier for transferring teacher knowledge, participate in the knowledge distillation. Several questions remain unsolved for these samples: Which part of all samples shows more impact in knowledge distillation? Intuitively, we may regard that longer sentences are hard to translate and might carry more teacher knowledge. But are there more of these criteria that can identify these more important/suitable samples for distillation? Further, what are the connections among these samples? Are they all guiding the student model to the same direction? By investigating the carrier of teacher knowledge, we can shed light on finding the most effective KD method.\nHence, in this paper, we aim to investigate the impacts and differences among all samples. However, it is non-trivial to analyze each of them. Therefore, we propose a novel analytical protocol by partitioning the samples into two halves with a specific criterion (e.g., sentence length or word cross-entropy) and study the gap between performance. Extensive empirical experiments are conducted to analyze the most suitable sample for transferring knowledge. We find that different samples differ in transferring knowledge for a substantial margin. More interestingly, with some partitions, especially the student\nmodel’s word cross-entropy, the model with half of the knowledge even shows better performance than the model using all distill knowledge. The benefit of the distillation of two halves cannot collaborate. This phenomenon reveals that the distillation of two halves cannot collaborate, even hurt the whole performance. Hence, a more sophisticated selective strategy is necessary for KD methods.\nNext, we propose two simple yet effective methods to address the observed phenomenon according to word cross-entropy (Word CE), which we find is the most distinguishable criterion. We first propose a batch-level selection strategy that chooses words with higher Word CE within the current batch’s distribution. Further, to step forward from local (batch) distribution to global distribution, we use a global-level FIFO queue to approximate the optimal global selection strategy, which caches the Word CE distributions across several steps. We evaluate our proposed method on two large-scale machine translation datasets: WMT’14 EnglishGerman and WMT’19 Chinese-English. Experimental results show that our approach yields an improvement of +1.28 and + 0.89 BLEU points over the Transformer baseline.\nIn summary, our contributions are as follows:\n• We propose a novel protocol for analyzing the property for the suitable medium samples for transferring teacher’s knowledge.\n• We conduct extensive analyses and find that some of the teacher’s knowledge will hurt the whole effect of knowledge distillation.\n• We propose two selective strategies: batchlevel selection and global-level selection. The experimental results validate our methods are effective."
    }, {
      "heading" : "2 Related Work",
      "text" : "Knowledge distillation approach (Hinton et al., 2015) aims to transfer knowledge from teacher model to student model. Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.\nAs for neural machine translation (NMT), knowledge distillation methods commonly focus on bet-\nter improving the student model and learning from the teacher model. Kim and Rush (2016) first applied knowledge distillation to NMT and proposed the sequence-level knowledge distillation that lets student model mimic the sequence distribution generated by the teacher model. It was explained as a kind of data augmentation and regularization by Gordon and Duh (2019). Further, Freitag et al. (2017) improved the quality of distillation information by using an ensemble model as the teacher model. Gu et al. (2017) improved non-autoregressive model performance by learning distillation information from the autoregressive model. Wu et al. (2020) proposed a layer-wise distillation method to be suitable for the deep neural network. Chen et al. (2020b) let translation model learn from language model to help the generation of machine translation.\nTo the best of our knowledge, there is no previous work in NMT concerning the selection of suitable samples for distillation. The few related ones mainly focus on selecting appropriate teachers for the student model to learn. For instance, Tan et al. (2019) let the student model only learn from the individual teacher model whose performance surpasses it. Wei et al. (2019) proposed an online knowledge distillation method that let the model selectively learn from history checkpoints. Unlike the above approaches, we explore the effective selective distillation strategy from sample perspective and let each sample determine learning content and degree."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Neural Machine Translation",
      "text" : "Given a source sentence x = (x1, ..., xn), and its corresponding ground-truth translation sentence y = (y∗1, ..., y ∗ m), an NMT model minimizes the word negative log-likelihood loss at each position by computing cross-entropy. For the j-th word in the target sentence, the loss can be formulated as:\nLce = − |V |∑ k=1 1{y∗j = k} log p(yj = k|y<j ,x; θ), (1) where |V | is the size of target vocabulary, 1 is the indicator function, and p(·|·) denotes conditional probability with model parameterized by θ."
    }, {
      "heading" : "3.2 Word-level Knowledge Distillation",
      "text" : "In knowledge distillation, student model S gets extra supervision signal by matching its own outputs to the probability outputs of teacher model T . Specifically, word-level knowledge distillation defines the Kullback–Leibler distance between the output distributions of student and teacher (Hu et al., 2018). After removing constants, the objective is formulated as:\nLkd = − |V |∑ k=1 q(yj = k|y<j ,x; θT )\n× log p(yj = k|y<j ,x; θS), (2) where q(·|·) is the conditional probability of teacher model. θS and θT is the parameter set of student model and teacher model, respectively.\nAnd then, the overall training procedure is minimizing the summation of two objectives:\nL = Lce + αLkd, (3)\nwhere α is a weight to balance two losses."
    }, {
      "heading" : "4 Are All Words Equally Suitable for KD?",
      "text" : "As discussed before, as a carrier of the teacher’s knowledge, ground-truth words might greatly influence the performance of knowledge distillation. Therefore, in this section, we first do some preliminary empirical studies to evaluate the importance\nof different words/sentences in knowledge distillation."
    }, {
      "heading" : "4.1 Partition of Different Parts",
      "text" : "The optimal way to analyze samples’ different impacts on distillation is to do ablation studies over each of them. However, it is clearly timeconsuming and intractable. Hence, we propose an analytical protocol by using the partition and comparison as an approximation, which we believe could shed light on future analyses. Particularly, we leverage a specific criterion f to partition samples into two complementary parts:\nSHigh := { yi | f(yi) > Median(f(y)), yi ∈ y }, SLow := { yi | f(yi) ≤ Median(f(y)), yi ∈ y },\nand analyze different effects between SHigh and SLow. Each part consists of 50% words/sentences precisely. The criteria come from three different perspectives: data property, student model, and teacher model. The detailed descriptions are as follows:\n• Data Property. As longer sentences and rare words are more challenging to translate (Kocmi and Bojar, 2017; Platanios et al., 2019), its corresponding teacher knowledge may benefit the student model more. Hence, we choose sentence length and word frequency as criteria.\n• Student Model. As for the student model, we care if the student model thinks these words/sentences are too complicated. Therefore, we use Word CE (cross-entropy of words), Sentence CE (mean of the crossentropy of all words in sentences), and each word’s embedding norm (Liu et al., 2020).\n• Teacher Model. For the teacher model, we guess that the teacher’s prediction confidence may be crucial for transferring knowledge. Hence, we use the prediction probability of ground-truth label (Pgolden) and entropy of prediction distribution as our criteria."
    }, {
      "heading" : "4.2 Analytic Results",
      "text" : "Table 1 presents our results on different criteria. We also add the performance of Transformer baseline, Distill-All (distillation with all words) and Distill-Half(distillation with 50% words chosen by random) for comparison.\nImpact of Different Parts. Through most of the rows, we observe noticeable gaps between the BLEU scores of the SHigh and SLow, indicating there exists a clear difference of impact on medium of teacher knowledge. Specifically, for most of the criteria like cross-entropies or word frequency, the gap between two halves surpasses 0.35. In contrast, teacher Pgolden seems not useful for partitioning KD knowledge. We conjecture this is because no matter whether the teacher is convinced with the golden label or not, other soft labels could contain useful information (Gou et al., 2020). Besides, we find teacher entropy is a good-enough criterion for partitioning KD data, which inlines with previous studies of dark knowledge (Dong et al., 2019). Finally, we find that the KD is most sensitive (+0.64) with the Word CE criterion, which enjoys the adaptivity during the training phase and is a good representative for whether the student thinks the sample is difficult.\nIn conclusion, we regard the most suitable samples should have the following properties: higher Word CE, higher Sentence CE, higher Word Frequency, which probably benefits future studies of effective KD methods.\nImpact of All and Halves. More interestingly, compared with ‘Distill-All’, which is the combination of the SHigh and SLow, the SHigh halves’ BLEU score even surpass the ‘Distill-All’, for Word CE, Sentence CE and Word Frequency criteria. This leads to two conclusions:\n(1) Within some partitions, the SHigh contributes most to the KD improvements.\n(2) The amount of teacher knowledge is not the more, the better. The distillation knowledge of the SLow does not directly combine with the SHigh, even hurts SHigh’s performance.\nImpact of the Amount of Knowledge. Given that distillation knowledge is most sensitive to Word CE, we conduct extra analysis on the Word CE. Figure 1 presents the results of varying the amount of knowledge for SHigh and SLow. The consistent phenomenon is that the SHigh perform significantly better than the SLow when using the same amount of teacher’s knowledge. These results suggest that we should focus more on the SHigh than on SLow. Besides, we notice that the model performance increases when we increase the knowledge in SHigh, but not the case for SLow. We conclude that the Word CE is distinguishable and a\nbetter indicator of teachers’ useful knowledge only for SHigh.\nAt the end of this section, we can summary the following points:\n• To find out the most suitable medium for transferring medium, we adopt a novel method of partition and comparison, which can easily be adopted to future studies.\n• The benefit of distillation knowledge drastically changes when applying to different mediums of knowledge.\n• Among all criteria, knowledge distillation is the most sensitive to Word CE. Distilling words with higher Word CE is more reliable than words with lower CE.\n• In some partitions, the distillation benefit of SLow can not add to the SHigh, even hurts SHigh’s performance."
    }, {
      "heading" : "5 Selective Knowledge Distillation for NMT",
      "text" : "As mentioned above, there exist un-suitable mediums/samples that hurt the performance of knowledge distillation. In this section, we address this problem by using two simple yet effective strategy of selecting useful samples.\nIn Section 4, we find that Word CE is the most distinguishable criterion. Hence, we continue to\nuse the Word CE as the measure in our methods. As the word cross-entropy is a direct measure of how the student model agrees with the golden label, we refer to words with relatively large cross-entropy as difficult words, and words with relatively small cross-entropy as easy words, in the following parts. This is to keep the notation different from previous analysis.\nThen, we only need to define what is “relatively large”. Here, we introduce two CE-based selective strategies: Batch-level Selection (BLS). Given a minibatch B of sentence pairs with M target words, we sort all words in the current batch with their Word CE in descending order and select the top r percent of all words to distill teacher knowledge. More formally, letA denote the Word CE set, which contains the Word CE of each word in batch B. We define SHard = top r%(A) as the set of the r% largest cross-entropy words among the batch, and SEasy is its complementary part.\nFor those words in SHard, we let them get extra supervision signal from teacher model’s distillation information. Therefore, the knowledge distillation objective in Equation 3 can be be re-formulated as:\nLkd =\n{ − ∑|V |\nk=1 q(yk) · log p(yk), y ∈ SHard 0 , y ∈ SEasy\nwhere we simplify the notation of p and q for clarity.\nGlobal-level Selection (GLS). Limited by the number of words in a mini-batch, batch-level selection only reflects the current batch’s CE distribution and can not represent the real global CE distribution of the model very well. In addition, the batch-level method makes our relative difficulty measure easily affected by each local batch’s composition. The optimal approach to get the global CE distribution is to traverse all training set words and calculate their CE to get the real-time distribution after each model update. However, this brings a formidable computational cost and is not realistic in training.\nTherefore, as a proxy to optimal way, we extend batch-level selection to global-level selection by dexterously using a First-In-First-Out (FIFO) global queue Q. At each training step, we push batch words’ CE into FIFO global queue Q and pop out the ‘Oldest’ words’ CE in the queue to retain the queue’s size. Then, we sort all CE values in the queue and calculate the ranking position\nAlgorithm 1 Global-level Selection Input: B: mini-batch, Q: FIFO global queue, T : teacher model, S: student model 1: for each wordi in B do 2: Compute Lce of wordi by Equation 1 3: Compute Lkd of wordi by Equation 2 4: Push Lce to Q 5: if Lce in top r%(Q) then 6: Lossi ← Lce + α · Lkd 7: else 8: Lossi ← Lce 9: Loss← Loss+ Lossi\n10: Update S with respect to Loss\nof each word. The storage of queue is much bigger than a mini-batch so that we can evaluate the current batch’s CEs with more words, which reduces the fluctuation of CE distribution caused by the batch-level one. Algorithm 1 details the entire procedure."
    }, {
      "heading" : "6 Experiments",
      "text" : "We carry out experiments on two large-scale machine translation tasks: WMT’14 English-German (En-De) and WMT’19 Chinese-English (Zh-En)."
    }, {
      "heading" : "6.1 Setup",
      "text" : "Datasets. For WMT’14 En-De task, we use 4.5M preprocessed data, which is tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016) with 32K merge operations and a shared vocabulary for English and German. We use newstest2013 as the validation set and newstest2014 as the test set, which contain 3000 and 3003 sentences, respectively.\nFor the WMT’19 Zh-En task, we use 20.4M preprocessed data, which is tokenized and split using 47K/32K BPE merge operations for source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 3981 and 2000 sentences, respectively.\nEvaluation. For evaluation, we train all the models with a maximum of 300K steps for WMT EnDe’14 and WMT’19 Zh-En. We choose the model which performs the best on the validation set and report its performance on test set. We measure case sensitive BLEU calculated by multi-bleu.perl2\n2https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/multibleu.perl\nand mteval-v13a.pl3 with significance test (Koehn, 2004) for WMT’14 En-De and WMT’19 Zh-En, respectively.\nModel and Hyper-parameters. Following the setting in Vaswani et al. (2017), we carry out our experiments on standard Transformer (Vaswani et al., 2017) with the fairseq toolkit (Ott et al., 2019). By default, we use Transformer (Base), which contains six stacked encoder layers and six stacked decoder layers as both teacher model and student model. To verify our approaches can be applied to a stronger teacher and student models, we further use deep Transformers with twelve encoder layers and six decoder layers. In training processing, we use Adam optimizer with β1 = 0.9, β2 = 0.98, learning rate is 7e-4 and dropout is 0.1. All experiments are conducted using 4 NVIDIA P40 GPUs, where the batch size of each GPUs is set to 4096 tokens. And we accumulate the gradient of parameters and update every two steps. The average runtimes are 3 GPU days for all experiments.\nThere are two hyper-parameters in our experiment, i.e., distil rate r% and global queue size Qsize. For distil rate r%, the search space is [10%, 30%, 50%, 70%, 90%]. The search result of r% is shown in Figure 2, we can find that the performance is sensitive to the value of r%. When the ratio is smaller than 50%, the increase of ratio is consistent with the BLEU score increases, and the best performance peaks at 50%. We directly apply the distil rate r% to the WMT’19 Zh-En task without extra searching. Besides, We set the Qsize = 30K for WMT’14 En-De. For larger dataset WMT’19 ZhEn, we enlarge the Qsize to from 30K to 50K and keep word rate unchanged. The hyper-parameter search of Qsize can be found in Section 6.4.\n3https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/mteval -v13a.pl\nCompared Methods. We compare our method with several existing NMT systems (KD and others):\n• Word-KD (Kim and Rush, 2016). WordKD is a standard method that distills knowledge equally for each word. The detailed description is in Section 3.2.\n• Seq-KD (Kim and Rush, 2016). SequenceKD uses teacher generated outputs on training corpus as an extra source. The training loss can be formulated as:\nLseq kd = − J∑\nj=1 |V |∑ k=1 1{ŷj = k}\n× log p(yj = k|ŷ<j ,x; θ), (4)\nwhere ŷ denotes the sequence predicted by teacher model from running beam search, J is the length of target sentence.\n• Bert-KD (Chen et al., 2020b). This method leverages the pre-trained Bert as teacher model to help NMT model improve machine translation quality.\n• Other Systems. We also include some existing methods based on Transformer(Base) for comparison, i.e., Zheng et al. (2019); So et al. (2019); Tay et al. (2020)."
    }, {
      "heading" : "6.2 Main Results",
      "text" : "Results on WMT’14 English-German. The results on WMT’14 En-De are shown in Table 2. In\nthis experiment, both the teacher model and student model are Transformer (Base). We also list our implementation of word-level distillation and sequence level distillation (Kim and Rush, 2016) method.\nFirstly, compared with the Transformer (Base), our re-implemented word-level and the sequencelevel distillation show similar improvements with the BLEU scores up from 27.29 to 28.14 and 28.15, respectively. Secondly, compared with these already strong baseline methods, our batch-level selective approach further extends the improvement to 28.42, proving the selective strategy’s effectiveness. Thirdly, our global-level distillation achieves a 28.57 BLEU score and outperforms all previous methods, showing that the better evaluation of words’ CE distribution with FIFO global queue helps selection. It is worth noting that our strategy also significantly improves translation quality over all others methods including Word-KD. Finally, our methods show comparable/better performance than other existing NMT systems and even surpass the Transformer (Big), with much fewer parameters."
    }, {
      "heading" : "6.3 Analysis",
      "text" : "Even though we find some interesting phenomena and achieve great improvement by selective distillation, the reason behind it is still unclear. Hence, in this section, we conduct some experiments to analyze and explain the remaining question.\nNote that we follow the previous partition and comparison method in this section and divide the samples with/without KD loss defined in our selection strategy as SHard/SEasy.\nConflict on Different Parts. The first question is that why our methods surpass the Word-KD with more knowledge. To answer this question, we collect the statistics on the gradient difference between\nknowledge distillation loss and cross-entropy loss on the ground-truth label for SHard and SEasy.\nHere, we study gradients over the output distributions, which are directly related to the model’s performance. Particularly, decoder maps target sentences y = (y∗1, ..., y ∗ m) to their corresponding hidden representation h = (h1, ..., hm). For words in target sequence, the prediction logits l ∈ Rdmodel×|V | is given by:\nl = hTW (5) p = Softmax(l) (6)\nwhere h ∈ Rdmodel is the layer output of transformer decoder, W ∈ Rdmodel×|V | is projection matrix. Then, the gradient respect to l from golden cross-entropy loss can be denotes as ∇lLce. The gradient from distillation loss can be denotes as ∇lLkd. Next, we calculate the probability that ∇lLce and ∇lLkd share the same direction.\nFigure 3 presents the results with the probability that gradients agree with each other during training. We observe that SEasy (green line) is consistently lower than distillation with all words (blue line) and SHard (red line), which means SEasy has more inconsistency with ground-truth. Combining with the BLEU performances, we argue this consistency leads to the risk of introducing noise and disturbs the direction of parameter updating.\nBesides, the agreement of Distill-All (blue line in Fig) lies in the middle of two halves. It proves that SEasy and SHard compromise with each other on some conflicts. It also proves that there exist some conflicts between the knowledge in SEasy and SHard.\nKnowledge on Different Parts. In our approaches, we select the transferring samples from\nthe student model’s point of view. However, in previous literature, they commonly consider knowledge from the teacher’s perspective. Hence, in this section, we study the correlation between these two perspectives.\nBecause previous studies commonly regard teacher’s soft-labels contain dark knowledge (Dong et al., 2019), we take the entropy of teacher’s prediction as a proxy. Concretely, we randomly select 100K tokens in the training set and calculate the entropy of distribution predicted by the teacher model for both SHard and SEasy. As shown in Figure 4, we notice that the SEasy’s entropy distribution is more concentrated in range (0, 4) and peaks around 1.2. In contrast, the SHard’s entropy distribution is more spread out. The overall distribution shifts to higher entropy, which indicates SHard tends to provide a smoother supervision signal. Consequently, we conclude that even though our selective strategy comes from the student’s perspective, it also favors samples with abundant dark knowledge in teacher’s perspective. To some extent, this explains why the SHard’ knowledge benefits distillation performance more."
    }, {
      "heading" : "6.4 Generalizability",
      "text" : "Results on WMT’19 Chinese-English. We also conduct experiments on the larger WMT’19 Zh-en dataset (20.4M sentence pairs) to ensure our methods can provide consistent improvements across different language pairs.\nAs shown in Table 3, our method still significantly outperforms the Transformer (Base) with +0.89. Compared with the Word-KD, our approach consistently improves with +0.41 BLEU points. Besides, we also find that Seq-KD with our methods extends the improvement of BLEU score from 27.27 to 27.61. This indicates that our selective strategy is partially orthogonal to the improvement\nof Seq-KD and maintains generalizability. In summary, these results suggest that our methods can achieve consistent improvement on different sized datasets across different language pairs.\nResults with Larger Model Size. Here, we investigate how our method is well-generalized to larger models. We use a deep transformer model with twelve encoder layers and six decoder layers for our larger model experiments. As shown in Table 4, Deep Transformer (12 + 6) and Word-KD have already achieved strong performance with up to 28.90 BLEU points, and our method still outperforms these baselines (29.12 BLEU). It proves our methods’ generalizability to larger models."
    }, {
      "heading" : "6.5 Effect of the Global Queue",
      "text" : "This section analyzes how Qsize affects our model’s performance. As mentioned before, Qsize denotes the size of the global FIFO queue, which affects simulating the word cross-entropy distribution of the current model.\nFigure 5 shows the search results of Qsize. We can find that smaller and larger queue size both hurts the BLEU scores. Besides, 30K and 50K of queue size are the best for WMT’14 En-De and WMT’19 Zh-En, respectively. This also accords with our intuition that smaller Qsize degrades the global-level queue to batch level, and larger Qsize slows down the update of CE distribution.\nFigure 6 plots the partition Word CE of SHard and SEasy for batch-level and global-level selection. We can see that, as the training progresses, batch-level selection starts to suffer from the high variance because of each batch’s randomness. Selections with FIFO queue drastically reduce the variance and make a reasonable estimation of global CE distribution. These findings prove the effectiveness of our proposed FIFO queue."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we conduct an extensive study to analyze the impact of different words/sentences as the carrier in knowledge distillation. Analytic results show that distillation benefits have a substantial margin, and these benefits may not collaborate with their complementary parts and even hurt the performance. To address this problem, we propose two simple yet effective strategies, namely the batch-level selection and global-level selection.\nThe experiment results show that our approaches can achieve consistent improvements on different sized datasets across different language pairs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper."
    } ],
    "references" : [ {
      "title" : "Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation",
      "author" : [ "Xiuyi Chen", "Fandong Meng", "Peng Li", "Feilong Chen", "Shuang Xu", "Bo Xu", "Jie Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling knowledge learned in bert for text generation",
      "author" : [ "Yen-Chun Chen", "Zhe Gan", "Yu Cheng", "Jingzhou Liu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7893–7905.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Distillation ≈ early stopping? harvesting dark knowledge utilizing anisotropic information retrieval for overparameterized neural network",
      "author" : [ "Bin Dong", "Jikai Hou", "Yiping Lu", "Zhihua Zhang." ],
      "venue" : "arXiv preprint arXiv:1910.01255.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Ensemble distillation for neural machine translation",
      "author" : [ "Markus Freitag", "Yaser Al-Onaizan", "Baskaran Sankaran." ],
      "venue" : "arXiv preprint arXiv:1702.01802.",
      "citeRegEx" : "Freitag et al\\.,? 2017",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2017
    }, {
      "title" : "Explaining sequence-level knowledge distillation as dataaugmentation for neural machine translation",
      "author" : [ "Mitchell A Gordon", "Kevin Duh." ],
      "venue" : "arXiv preprint arXiv:1912.03334.",
      "citeRegEx" : "Gordon and Duh.,? 2019",
      "shortCiteRegEx" : "Gordon and Duh.",
      "year" : 2019
    }, {
      "title" : "Knowledge distillation: A survey",
      "author" : [ "Jianping Gou", "Baosheng Yu", "Stephen John Maybank", "Dacheng Tao." ],
      "venue" : "arXiv preprint arXiv:2006.05525.",
      "citeRegEx" : "Gou et al\\.,? 2020",
      "shortCiteRegEx" : "Gou et al\\.",
      "year" : 2020
    }, {
      "title" : "Nonautoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor OK Li", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1711.02281.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention-guided answer distillation for machine reading comprehension",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Furu Wei", "Zhen Huang", "Dongsheng Li", "Nan Yang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1808.07644.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:1909.10351.",
      "citeRegEx" : "Jiao et al\\.,? 2019",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M Rush." ],
      "venue" : "arXiv preprint arXiv:1606.07947.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Curriculum learning and minibatch bucketing in neural machine translation",
      "author" : [ "Tom Kocmi", "Ondrej Bojar." ],
      "venue" : "arXiv preprint arXiv:1707.09533.",
      "citeRegEx" : "Kocmi and Bojar.,? 2017",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2017
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 388–395.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Learning light-weight translation models from deep transformer",
      "author" : [ "Bei Li", "Ziyang Wang", "Hui Liu", "Quan Du", "Tong Xiao", "Chunliang Zhang", "Jingbo Zhu." ],
      "venue" : "arXiv preprint arXiv:2012.13866.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Norm-based curriculum learning for neural machine translation",
      "author" : [ "Xuebo Liu", "Houtim Lai", "Derek F Wong", "Lidia S Chao." ],
      "venue" : "arXiv preprint arXiv:2006.02014.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "WeChat neural machine translation systems for WMT20",
      "author" : [ "Fandong Meng", "Jianhao Yan", "Yijin Liu", "Yuan Gao", "Xianfeng Zeng", "Qinsong Zeng", "Peng Li", "Ming Chen", "Jie Zhou", "Sifan Liu", "Hao Zhou." ],
      "venue" : "Proceedings of the Fifth Conference on Machine",
      "citeRegEx" : "Meng et al\\.,? 2020",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2020
    }, {
      "title" : "DTMT: A novel deep transition architecture for neural machine translation",
      "author" : [ "Fandong Meng", "Jinchao Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 224– 231.",
      "citeRegEx" : "Meng and Zhang.,? 2019",
      "shortCiteRegEx" : "Meng and Zhang.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Competence-based curriculum learning for neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Otilia Stretcu", "Graham Neubig", "Barnabas Poczos", "Tom M Mitchell." ],
      "venue" : "arXiv preprint arXiv:1903.09848.",
      "citeRegEx" : "Platanios et al\\.,? 2019",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "The evolved transformer",
      "author" : [ "David So", "Quoc Le", "Chen Liang." ],
      "venue" : "International Conference on Machine Learning, pages 5877–5886. PMLR.",
      "citeRegEx" : "So et al\\.,? 2019",
      "shortCiteRegEx" : "So et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1908.09355.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, 27:3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual neural machine translation with knowledge distillation",
      "author" : [ "Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Zhou Zhao", "TieYan Liu." ],
      "venue" : "arXiv preprint arXiv:1902.10461.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling taskspecific knowledge from bert into simple neural networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1903.12136.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Synthesizer: Rethinking self-attention in transformer models",
      "author" : [ "Yi Tay", "Dara Bahri", "Donald Metzler", "Da-Cheng Juan", "Zhe Zhao", "Che Zheng." ],
      "venue" : "arXiv preprint arXiv:2005.00743.",
      "citeRegEx" : "Tay et al\\.,? 2020",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Online distilling from checkpoints for neural machine translation",
      "author" : [ "Hao-Ran Wei", "Shujian Huang", "R. Wang", "Xin-Yu Dai", "Jiajun Chen." ],
      "venue" : "NAACLHLT.",
      "citeRegEx" : "Wei et al\\.,? 2019",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    }, {
      "title" : "Why skip if you can combine: A simple knowledge distillation technique for intermediate layers",
      "author" : [ "Yimeng Wu", "Peyman Passban", "Mehdi Rezagholizade", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:2010.03034.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-unit transformers for neural machine translation",
      "author" : [ "Jianhao Yan", "Fandong Meng", "Jie Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1047–1059, Online.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Future-aware knowledge distillation for neural machine translation",
      "author" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Jiebo Luo." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(12):2278–2287.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Ternarybert: Distillation-aware ultra-low bit bert",
      "author" : [ "Wei Zhang", "Lu Hou", "Yichun Yin", "Lifeng Shang", "Xiao Chen", "Xin Jiang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:2009.12812.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4334–",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Dynamic past and future for neural machine translation",
      "author" : [ "Zaixiang Zheng", "Shujian Huang", "Zhaopeng Tu", "Xin-Yu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1904.09646.",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 26,
      "context" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 32,
      "context" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 29,
      "context" : "Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "Knowledge distillation approach (Hinton et al., 2015) aims to transfer knowledge from teacher model to student model.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 21,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 24,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 0,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 15,
      "context" : "Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge.",
      "startOffset" : 46,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "output distributions of student and teacher (Hu et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Therefore, we use Word CE (cross-entropy of words), Sentence CE (mean of the crossentropy of all words in sentences), and each word’s embedding norm (Liu et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "We conjecture this is because no matter whether the teacher is convinced with the golden label or not, other soft labels could contain useful information (Gou et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Besides, we find teacher entropy is a good-enough criterion for partitioning KD data, which inlines with previous studies of dark knowledge (Dong et al., 2019).",
      "startOffset" : 140,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "5M preprocessed data, which is tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016) with 32K merge operations and a shared vocabulary",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "pl3 with significance test (Koehn, 2004) for WMT’14 En-De and WMT’19 Zh-En, respectively.",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "We also list our implementation of word-level distillation and sequence level distillation (Kim and Rush, 2016) method.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Because previous studies commonly regard teacher’s soft-labels contain dark knowledge (Dong et al., 2019), we take the entropy of teacher’s prediction as a proxy.",
      "startOffset" : 86,
      "endOffset" : 105
    } ],
    "year" : 2021,
    "abstractText" : "Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples’ partitions. Based on above protocol, we conduct extensive experiments and find that the teacher’s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively. 1",
    "creator" : "LaTeX with hyperref"
  }
}