{
  "name" : "2021.acl-long.533.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The statistical advantage of automatic NLG metrics at the system level",
    "authors" : [ "Johnny Tian-Zheng", "Wei Robin Jia" ],
    "emails" : [ "robinjia}@usc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6840–6854\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6840"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic metrics are involved in many developmental settings for natural language generation (NLG) systems. In machine translation (MT), metrics like BLEU (Papineni et al., 2002) enable settings where the amount of human effort required would be infeasible, such as architecture or hyperparameter search (Britz et al., 2017). As objective, reproducible quantities, BLEU scores facilitate cross-paper comparisons (Post, 2018). Historically, progress in MT has been attributed to its\n1The data and code to reproduce our analyses are available at https://github.com/johntzwei/ metric-statistical-advantage.\nuse (Callison-Burch et al., 2006). Metrics are an active research area in many NLG subfields, including summarization (Lin, 2004), dialogue (Tao et al., 2018), and image captioning (Anderson et al., 2016), which seek to realize the goal of quick and reliable automatic evaluation.\nIn all these subfields, the primary goal when conducting evaluation is typically to compare NLG systems. Both human annotators and automatic metrics produce segment-level scores, i.e., scores for individual examples, so comparing systems requires aggregating segment-level scores into an overall system-level score for each system. Ideally, we would compare systems by their expected human annotator score (an average over infinite human judgments), which we term the true system quality. In practice, we can only estimate this expectation with a sample mean over a finite number of human judgments. Metrics offer a cheaper alternative: we can instead compare systems by their aggregate metric scores on a number of system out-\nputs. When comparing systems, we care primarily about how well we estimate the difference of their true system qualities, and in particular the sign of this difference (i.e., which system is better), which we term the true pairwise label.\nThere is a gap in our understanding of systemlevel metrics. To recount a perplexing anecdote, in the most recent edition of the WMT metrics shared task (Mathur et al., 2020b), initial human evaluation disagreed with most metrics on a pairwise prediction of two translation systems. In a manual re-evaluation, the second round results favored the metrics. Our paper offers a statistical explanation for how humans could go “wrong”: even if human estimation for the difference in system quality is unbiased, it has high variance. On the other hand, while estimators based on metrics are biased, they have low variance. It is therefore possible for metrics to give a more accurate pairwise prediction than humans when the bias is small (see illustration in Figure 1). Our paper explores this distinction in the following three questions:\n(1) How can we evaluate system-level metrics? When observing estimator error in terms of pairwise predictions, predictions are evaluated against noisy, human predicted labels rather than the ground truth. In addition, metric predictions fluctuate based on the sample of outputs from the generation system. To disentangle these properties, we examine observed estimator error under a biasvariance-noise decomposition. Under simulation, we find that the label noise and metric variance account for a small fraction of observed error in both MT and summarization.\n(2) How good are these metrics? We compare the errors of metric estimators computed on an infinite number of system outputs, against human estimators with varying amounts of human judgment. We also derive the error of a perfect segment-level annotator (i.e. they provide noiseless/expected human scores for each output), which is also unbiased and judgment dependent. Empirically, some MT metrics exceed the performance of unbiased estimators with a small number of judgments.\n(3) What are the limits of system-level evaluation? The perfect segment-level annotator, as the noiseless human, provides an optimistic estimate for the number of human judgments necessary to achieve a fixed performance. With a power analysis, we can analytically calculate the number of judgments necessary to detect differences between\nsystems of varying sizes. When differences in system quality are small, a prohibitively large number of perfect annotator judgments are required to give a correct pairwise prediction."
    }, {
      "heading" : "2 Formalization",
      "text" : ""
    }, {
      "heading" : "2.1 System-level scores",
      "text" : "We will now formalize scoring at the system level, adopting notation from Chaganty et al. (2018). Let X be a distribution over inputs (e.g. source sentences), and S be a set of systems (e.g. all translation systems in WMT). Each system S ∈ S takes input x ∼ X and returns output z = S(x) (e.g. z is a translation). Let H(z) be a random variable representing a human judgment according to some evaluation prompt (e.g. translation adequacy, from 0-100). A central quantity of interest is the quality of system S, defined as\nµHS = Ex∼X [H(S(x))] (1)\nand is not directly observable as it requires infinite human judgment. We can estimate (1) with a finite test set of n examples. Let x(1), . . . , x(n) i.i.d.∼ X be a sampled test set and z(1), . . . , z(n) be the set of outputs where each z(i) = S(x(i)). Human judgments are sampled independently as y(i) ∼ H(z(i)). The sample mean\nµ̂HS = 1\nn n∑ i=1 y(i) (2)\nis an unbiased estimator of (1). Only (2) is observable, which is a noisy approximation of (1).\nA cheaper alternative to estimating the true quality scores is with an estimator based on an automatic metric. Let M (e.g. BERTSCORE) be an automatic metric that takes as input any number of outputs from a system S and produces score\nµ̂MS = M(z (1), . . . , z(n)) (3)\nwhere µ̂MS is a biased estimator of µ H S . As the test set is sampled, the metric score has non-zero variance. Note that while we use the greek letter µ, only some system-level metrics (e.g. ROUGE) are averages of their segment-level counterparts (their score decomposes to µ̂MS = 1 n ∑n i=1M(z\n(i))). Empirically, we find that metrics using other aggregation strategies have convergent properties similar to an average (see Appendix B). We sidestep this by defining the “true” metric score as\nµMS = M(z (1), . . . , z(m)) (4)\nfor test sets of size m sufficiently large so that this true score is nearly constant."
    }, {
      "heading" : "2.2 Problems in evaluating with correlation",
      "text" : "Research in system-level metrics have a tradition of evaluating metric correlation to human judgment with the Pearson correlation coefficient (Reiter, 2018). Formally, these evaluations compare r̂M = CorrS(µ̂HS , µ̂ M S ) for different metrics M .\nRecently, Mathur et al. (2020a) highlights two issues with the use of correlation: First, Pearson’s r is neither interpretable nor reflective of systemlevel metric use in practice. Second, outlier systems (systems with very high/low human/metric scores) can arbitrarily inflate Pearson’s r, and outlier systems often exist. Mathur et al. (2020a) propose evaluating metric accuracy in pairwise prediction (can the metric differentiate which generation system is better?) as an alternative that mitigates the issues mentioned above.\nWe add two points that apply to any measure of metric performance, correlation or pairwise predictions: First, metrics cannot be perfect due to noise in human labels. For instance, while r ranges from [−1, 1], even for the metric that predicts µHS it has CorrS(µ̂HS , µ H S ) < 1 due to noise in µ̂ H S . It is unclear what is the true upper bound of performance we can expect to achieve. Second, direct measurement of any performance measure on our datasets introduces sample bias (Engstrom et al., 2020). For correlation, r̂M could be high because µ̂HS and µ̂ M S happened to align for this data collection, but a repeat experiment could yield different results. A more holistic view is to give an estimate of average case performance.2\nThe evaluation methodology we derive in §4 addresses the latter points we raise for pairwise predictions and mean squared error (which has direct relationship to the correlation). However, we also believe that pairwise predictions is a step in the right direction, and our discussion continues with pairwise predictions."
    }, {
      "heading" : "2.3 Pairwise predictions",
      "text" : "We will now formalize pairwise predictions. For systems S, S′ ∈ S, define the true difference in their system scores as\nδHS,S′ = µ H S − µHS′ (5)\n2Pearson’s r was not formulated for individual distributions µ̂HS and µ̂ M S for each datapoint, so applying the William’s test (Graham and Baldwin, 2014) also falls short here.\nand the observed difference as\nδ̂HS,S′ = µ̂ H S − µ̂HS′ (6)\nand likewise for the differences δMS,S′ and δ̂ M S,S′ w.r.t. to a metric M . In practice, we are interested in the pairwise prediction of S and S′ i.e. whether δHS,S′ ? > 0, given that we have collected human\njudgments (we observe δ̂HS,S′ ≶ 0), or computed\nmetric scores (we observe δ̂MS,S′ ≶ 0). Refer to Figure 1 for an illustration.\nTo operationalize the pairwise prediction of S and S′, let the true pairwise label\n∆HS,S′ = sign(δ H S,S′) (7)\nbe defined as the central quantity of interest. Define the human predicted pairwise label as\n∆̂HS,S′ = sign(δ̂ H S,S′) (8)\nand likewise for the true and estimated predictions\n∆MS,S′ and ∆̂ M S,S′ w.r.t. to a metric M. The 0-1 classification loss for metric M on this example is\nL(∆HS,S′ , ∆̂ M S,S′) = I[∆ H S,S′ 6= ∆̂MS,S′ ] (9)\nand the pairwise error of an estimator is the loss incurred averaged over all pairwise examples. Ideally, we could calculate the true error of M\nErrtrue(M) = ES [L(∆HS,S′ ,∆ M S,S′)] (10)\nbut we can only compute an error of M with noisy human labels and metric scores estimated from finite sized test sets\nErrobs(M) = EX ,S [L(∆̂HS,S′ , ∆̂ M S,S′)] (11)\nwhich is typically estimated when we calculate metric pairwise accuracy from our datasets."
    }, {
      "heading" : "3 Datasets",
      "text" : ""
    }, {
      "heading" : "3.1 WMT16-19 metrics shared task",
      "text" : "Data. We use the past 4 years of to-English translation data from the WMT metrics shared task (Bojar et al., 2016b, 2017; Ma et al., 2018, 2019).3 Across all years and language pairs, there are 261 MT systems. Pairs of MT systems are extracted within each year, within each language pair, resulting in 1324 pairwise examples. For each output of an MT system, there are one or more humans judgements and one reference for metric scoring. 1306-5117 outputs were collected for each MT system totaling\n3The WMT20 metrics shared task data was not publicly available at the time of submission.\nabout 1312-5612 judgments, depending on the year and language pair. For ease of interpretation, we always use raw direct assessment judgments which range from 0-100.\nMetrics. We evaluate the performance of the three metrics included in SacreBleu (BLEU, TER, chrF; Post, 2018; Koehn et al., 2007). These three have also participated in every year of the metrics task as baselines. In addition, we include two recently developed metrics: BERTSCORE (Zhang et al., 2020) and BLEURT (Sellam et al., 2020). Both metrics are found to effectively utilize contextual embeddings (Devlin et al., 2019), and BLEURT is a learned metric (tuned on data outside of WMT2019). For all metrics, we use the default settings for scoring. Since BLEURT is trained on WMT15-18, we test it only on WMT2019 pairs."
    }, {
      "heading" : "3.2 SummEval",
      "text" : "Data. The SummEval dataset (Fabbri et al., 2020) contains 100 outputs from 17 summarization systems. This results in 136 pairwise examples. For each system output, 3 expert judgments, and 11 references for metric scoring. Each summarization is judged in four categories from 0-5: coherence, consistency, fluency, and relevance. To compute system-level human scores for a system, we first average over categories for an aggregate expert score, and then average the aggregated expert scores per system. Metric scores for system outputs were computed against as many references as possible.\nMetrics. We evaluate the performance of several metrics that were found to be effective at the system-level in Fabbri et al. (2020). This includes the traditional ROUGE-4 (Lin, 2004) summarization metric, its extension ROUGE-WE (Ng and Abrecht, 2015), and METEOR (Lavie and Agarwal, 2007). In addition, we include two metrics\nbased on BERT (Devlin et al., 2019). BertScore (Zhang et al., 2020), also present in the WMT analysis, and SUPERT (Gao et al., 2020), which is a reference-less metric for summarization."
    }, {
      "heading" : "4 Decomposing observed metric error",
      "text" : "Two sources of variation distinguish the observed pairwise error (11) from the true error in (10) — the noise in the human predicted labels due to finite judgements, and the variance in the metric due to finite test sets. Approximating (11) is straightforward with the bootstrap, but disentangling the error from these two sources of variation requires more care. With the bias-variance-noise decomposition, we can adjust our observed error estimates to the noise-free, infinite test set setting of the true error."
    }, {
      "heading" : "4.1 The bias-variance-noise decomposition",
      "text" : "The bias-variance-noise decomposition due to Domingos (2000) decomposes the observed pairwise error in (11) w.r.t. two constant labels for any pairwise example on systems S, S′ ∈ S: • The true pairwise label for this example is\n∆H∗S,S′ := arg min y∈{−1,1} EX [L(∆̂HS,S′ , y)] (12)\nand the estimator that produces these true labels has, by definition, the lowest observed error. In the decomposition, the human predicted label noise and metric bias is defined relative to the true labels. Assuming the central limit theorem (proof in Appendix A), we actually have ∆H∗S,S′ = ∆ H S,S′ as defined in eq. (5).\n• The main prediction of a metric for this ex. is\n∆M∗S,S′ = arg min y∈{−1,1} EX [L(∆̂MS,S′ , y)] (13)\nand we assume that the metric prediction converges onto the main prediction as the test data\nincreases for S and S′ (empirically validated in Appendix B). In the decomposition, the metric variance is defined relative to the main prediction.\nStarting from the loss incurred by M on this pairwise example, the decomposition gives us\nEX [L(∆̂HS,S′ , ∆̂ M S,S′)] = c0Noise(∆̂ H S,S′) (14)\n+ Bias(∆̂MS,S′) + c1Var(∆̂ M S,S′)\nwhere\n• Noise(∆̂HS,S′) = E[L(∆̂ H S,S′ ,∆ H∗ S,S′)] where the\nnoise is an irreducible loss incurred by computing pairwise accuracy to the human predicted labels instead of the true labels. Note that this noise term also exactly corresponds to the lowest achievable observable error (see §4.2).\n• Bias(∆̂MS,S′) = L(∆ H∗ S,S′ ,∆ M∗ S,S′) where the bias\nis 0 if the main prediction is correct (w.r.t. to the true label), and 1 otherwise. Note that this term is also the true error of a metric estimator in a noise-free, infinite test set setting. For unbiased estimators this term is zero, as their main prediction matches the true label.\n• Var(∆̂MS,S′) = E[L(∆̂ M S,S′ ,∆ M∗ S,S′)] where the\nvariance is a likelihood that the estimator deviates from its main prediction under random sampling.\n• c0 = 2PX (∆̂MS,S′ = ∆ H∗ S,S′)−1 which means that\nthe influence of label noise on the error becomes small if the estimator prediction are close to random chance. When the estimator gives constant predictions, the sign of c0 is dependent on the estimator’s correctness.\n• c1 = 1 if ∆̂MS,S′ = ∆ H∗ S,S′ and c1 = −1 other-\nwise. Variance can both increase and decrease the observed error. If the estimator is unbiased, the variance causes the prediction to from the correct main prediction. On the other hand, for a biased estimator, deviation from its incorrect main prediction occasionally decreases the error.\nUnlike the decomposition for mean squared error, the interaction between the c0 and Var terms only allows the error of two hypothetical settings to be read off directly from the table: when Noise −→ 0, corresponding to estimator error when computed against the ground truth; or when Noise+Var −→ 0, when the ground truth is used and metrics have access to an infinite test set for scoring."
    }, {
      "heading" : "4.2 A lower bound for the observed error",
      "text" : "By definition the constant estimator that produces the true pairwise labels ∆H∗S,S′ (defined in (12)) for each pairwise example has the lowest possible observable error. The observable error of this optimal estimator is exactly E[L(∆̂HS,S′ ,∆ H∗ S,S′)] =\nNoise(∆̂HS,S′). Since this estimator is constant it has no variance, and since it is instantiated by definition it has no bias. Analytically, the observed error of any estimator is lower bounded by\nNoise(∆̂HS,S′) and is the agreement of our human predicted labels with the ground truth."
    }, {
      "heading" : "4.3 Best-faith estimation with the bootstrap",
      "text" : "Assuming the bootstrap (Efron and Tibshirani, 1993) which is a common procedure in NLP (Dror et al., 2018), we can estimate the expectation quantities in the decomposition. By assuming that sampling with replacement from our datasets approximates real sampling, we can repeatedly simulate the quantity in an expectation. Taking the mean over trials gives the bootstrap estimate of the expectation. We emphasize that this is a regular application of a widely accepted technique—the bootstrap assumption allows us to study problems that would be impossible due to the cost of repeat experiments."
    }, {
      "heading" : "4.4 Results",
      "text" : "The following analyses refer to the error components (averaged over all examples) from the simulated decomposition presented in Table 1.\nThe noise component almost always accounts for a small fraction of the total error. We found this to be counterintuitive—while the lowest observable error (optimal predictions, see §4.2) incur about 5% error on both datasets, the influence of the noise is much smaller than those errors suggest. For the constant c0 scaling the noise, c0 = 0 if the metric prediction is near random. Since the c0Noise term on average is small two cases hold true: when humans are uncertain about the example (noise term large) metrics are as well (c0 term small), and when metrics are certain about the examples (c0 term large) humans are as well (noise term small). The second case empirically shows studying the sampling distribution of metrics (Koehn, 2004; Berg-Kirkpatrick et al., 2012) is effective, as metric certainty in the difference of system quality often implies human certainty.\nMetric variance introduces little to the pair-\nwise error, because it is low. Alternatively, metrics stand to gain little from using more test set examples. In MT, dropping both the noise and variance components for the error results in at most a 1 or 2 percent reduction in the observed error (see §9 for the implications in metrics research). Metrics generally have low variance, so at the test set sizes of WMT and SummEval, they are likely to converge to their main predictions."
    }, {
      "heading" : "5 Comparing to the human estimator",
      "text" : "In §4, several MT metrics approach the error of the WMT human evaluation. The WMT human evaluation is expensive, using thousands of judgments per translation system. While each human judgment has associated monetary cost, once a large test set is collected, running metrics only incurs computational cost. This section explores this asymmetry, and seeks to understand how much metric predictions are worth, in terms of human judgments."
    }, {
      "heading" : "5.1 Noise-free, variance-free error estimates",
      "text" : "We wish to give our best comparison between metrics and unbiased estimators (humans or the perfect annotator). Ideally, metrics would be given their best chance to perform, by using an infinite test set. With the decomposition, we can adjust metric errors estimates to a noise-free and infinite test set setting by taking only their bias component. For human and perfect annotator estimators, we can adjust their errors to a noise-free setting by taking only the variance component. The following\nsections compare these adjusted errors."
    }, {
      "heading" : "5.2 Simulating the perfect annotator",
      "text" : "While we can estimate the lower bound to the pairwise error for a given dataset (in §4.2), it is achieved by a constant estimator using system-level ground truth. Comparing segment-level metrics against the unbiased “perfect annotator”, or the best scorer at the segment-level, is more informative. At the high-level, we can simulate scoring with the perfect annotator at n judgments using the human estimator at n′ > n judgments to match the variance of the perfect annotator estimator.\nLet’s start from the unbiased human estimator µ̂HS (2). Recall that the estimator is a sample mean, so its variance is Var(µ̂HS ) = Var(H(x))/n. An insight from Chaganty et al. (2018) gives us the decomposition of the variance of H(x)\nVar(H(x)) = Var(E[H(x)|x]) (15) + E[Var(H(x)|x)]\nwith the law of total variance. In words, the variance term can be thought of as the variance of each output sentence’s true quality score (some translations produced by S are better than others) and the expectation term is the noise introduced by the humans when estimating the quality of a sentence (human scores have mean 0 noise around an output’s true quality score).\nOne intuition is that even if a perfect annotator gives the correct score for each sentence, every time, there is still some unavoidable variance in the estimator due to the variance of the hypothetical quality scores for each output. To formalize this notion, let P (x) = E[H(x)|x] be the human scoring function of a “perfect annotator”, and the estimator µ̂PS be an empirical mean of n independent samples from P (x) similar to eq. (1). As a sample mean, Var(µ̂PS ) = Var(P (x))/n. Relating this to (15)\nVar(H(x)) = E[Var(H(x)|x)]+Var(P (x)) (16) and while Var(P (x)) is not directly observable, we can calculate Var(H(x)) with the sample variance on all the human judgments, and E[Var(H(x)|x)] with a pooled variance over variances from repeat human judgments on the same output sentence.\nOur final step considers the efficiency ratio r = Var(H(x))/Var(P (x)). If we are interested in the perfect annotator estimator at n judgments, the hu-\nman estimator at n′ = rn judgments has variance\nVar(µ̂HS ) = Var(H(x))\nrn (17)\n= Var(P (x))\nn = Var(µ̂PS ) (18)\nand we invoke the central limit theorem to claim both µ̂PS and µ̂ H S are normal. This completes our reasoning that for scoring on the system-level, sampling n′ = nr human judgments is nearly equivalent to sampling n perfect annotator judgments. See Appendix C for step-by-step derivations for the perfect annotator variance in our datasets."
    }, {
      "heading" : "5.3 Results",
      "text" : "The following analyses refer to the comparison of metric estimators to unbiased estimators at varying number of judgments for WMT in Figure 2.\nJudgments from the perfect annotator have low variance, like those of professional linguists. While we do not have data from professional linguists, we can qualitatively compare them to the perfect annotator. A growing body of MT literature focuses on professional linguists (Freitag et al., 2020; Mathur et al., 2020b), and there are at least two known properties of their judgments: their judgments have better interannotator agreement (contain less noise), and they are more sensitive to linguistic phenomena. The perfect annotator has no noise, as they assign a constant score to each segment. However, the perfect annotator in WMT is better described as a noiseless crowdworker. With the biases of crowdworkers, the perfect annotator\nmay not share the sensitivity property, and our use of crowdworkers may be biased w.r.t. professional linguists.\nIn terms of average pairwise error, MT metrics have an equivalence to a high number of human judgments. Since the error of the human estimator monotonically increases as the number of judgments decrease, each MT metric has a breakeven point. Metrics outperform human estimators using judgments below this threshold. BERTSCORE is as accurate as using a human estimator with 600 judgments per system, or the perfect annotator estimator with 300 judgments, across the WMT dataset. We highlight the statistical advantage in variance many metrics share, and that this advantage offers a possibility that metrics can outperform humans, determined by which human estimator the metric is compared against. This is a consequence of the general fact that humans are unbiased, high-variance estimators, and metrics are biased, low-variance estimators, as depicted in Figure 1. For metrics such as BERTSCORE or CHRF, the bias is low as well, which gives it remarkably good error properties."
    }, {
      "heading" : "6 The limits of human evaluation",
      "text" : "The perfect annotator provides optimistic figures for human annotation, providing the best performance for a fixed number of judgments, and requiring the least judgments for a fixed performance. In §5, we saw that the perfect annotator is weak at low number of judgments, due to its non-zero variance. In this section we identify another consequence of the perfect annotator’s variance, where estimating small differences in system quality is hard."
    }, {
      "heading" : "6.1 Power analysis of the perfect annotator",
      "text" : "The performance of an unbiased estimator is dependent on their variance and the effect size it is trying to detect. This section performs a power analysis to determine how much annotator effort is needed to reliably detect the correct pairwise judgment between two systems (Card et al., 2020). To make an optimistic estimate, we assume our annotator variance is close to that of a perfect annotator. We make two assumptions to apply a basic power analysis for the estimation of the difference of system quality between two systems: normality and equal variance across groups. For parameters α = 0.05 (false positive rate) and β = 0.95 (false negative rate), we can analytically compute the number of\njudgments needed to ensure our pairwise judgment is at least β(1− α) ≈ 90% accurate. Table 2 contains power analyses for different instantiations of annotator variance and effect size."
    }, {
      "heading" : "In WMT, detecting a difference of 1 point",
      "text" : "requires at least 10K perfect annotator judgments, for different instantiations of its variance. To put this in perspective, the top 5 zh-en translation systems in WMT19 differed by less than 3 points (Barrault et al., 2019). Depending on how much is paid per judgment, this cost can quickly become infeasible. Here, the merit of such a task may be argued, as knowing a small difference exists between two systems may not always be productive. From a scientific perspective, many NLG techniques will yield small improvements, and not being able to detect small differences means we will not know whether these techniques are useful."
    }, {
      "heading" : "6.2 Metrics more easily achieve significance",
      "text" : "Since metrics tend to have lower variance, metrics often achieve significance in estimating the difference of system qualities, when humans cannot. For instance, BERTSCORE achieves significance in estimating quality differences over half of the pairwise examples where humans do not (see Appendix §E). In extreme cases, human evaluation is nearly as bad as flipping a coin, but the metric can still offer a consistent prediction between two systems. When comparing systems similar in quality, practitioners must accept that the number of possible analyses are limited. In ablation studies where similar systems are often compared, metrics may be our only insight into system performance. With white-box metrics such as BLEU, value can be derived from qualitative insight (e.g. systems with high BLEU score have high n-gram overlap with the reference set). In addition, we may qualitatively analyze output statistics not intended to correlate with humans judgment at all (Neubig et al., 2019)."
    }, {
      "heading" : "7 Caveats to the analysis",
      "text" : "Our analysis assumes that the human judgments are unbiased. In WMT16-19, direct assessment (Graham et al., 2013) was used to elicit judgments from a combination of crowdworkers and researchers. Direct assessment (DA) uses an adequacy evaluation prompt (“Rate how much you agree that the output translation adequately expresses the meaning of the reference translation”) and asks contribu-\ntors to rate on a 0-100 scale. The unbiased ground truth is not a fixed goalpost. A number of factors are known to change the eventual ranking of translation systems with human scoring. Employing a different collection methodology, such as human translation edit rate (HTER) of instead of DA, can result in divergent system rankings (Graham et al., 2016). In an earlier edition of WMT, DA judgments were collected with both a grammaticality prompt and an adequacy prompt, corresponding to different system rankings by the respective attribute (Bojar et al., 2016a). Several studies have shown scoring differences between professional linguists and crowdworkers which are due in part to the fact that linguists are more sensitive to linguistic phenomena (Fabbri et al., 2019; Freitag et al., 2019).\nThe goals of an evaluation should be decided by the practitioner. We do not give suggestions on any particular goals, and practitioners should understand what their application is, and which evaluation is the best approximation (refer to Gatt and Krahmer, 2018). Unfortunately, since the existing data in this domain is limited, our analyses are limited as well. However, the statistical techniques apply to any empirical method. We hope that our analysis inspires others to think about statistical limits in this domain."
    }, {
      "heading" : "8 Pushing the limits of evaluation",
      "text" : "To push the limits of what can be evaluated, we need to improve on fundamental aspects of human evaluation. On the human side, we may focus on creating larger effect sizes or reducing noise by adopting new annotation schemes (Läubli et al., 2018; Shapira et al., 2019) or employing professional linguists (Fabbri et al., 2020; Toral et al., 2018). To make the human estimator more efficient, we may consider adaptive data collection techniques to stop data collection early when significance is achieved, in a statistically sound manner (Johari et al., 2017).\nStrategies combining human and metric evaluation are also shown to have potential. Variance reduction techniques can be applied to the human estimator by taking advantage of strong metrics (Chaganty et al., 2018). Another bottleneck in human evaluation is in the random sampling of the test set. Metrics could form the basis of an importance sampling procedure to choose test sets that would best differentiate two systems, as a form of\nrobust evaluation (Chaganty et al., 2017). On the metric side, if we can reliably estimate metric bias, we can skip human evaluation altogether when the metric is known to be good. Probabilistic reinterpretations of current metrics could be a useful technique for confidence estimation (Keith and O’Connor, 2018). Optimistically, metrics could have provable guarantees, ensuring the correctness of metric decisions (Jia et al., 2019)."
    }, {
      "heading" : "9 Best practices for metrics research",
      "text" : "We reinterpret problems in evaluating metrics with correlation (§2.2) as a set of guidelines for metrics research. To next year’s organizers of the WMT metrics shared task and the broader metrics community we suggest the following: (1) Pairwise accuracy has desirable properties as an evaluation measure for metrics. Our bias-variance-noise decomposition shows that the observed pairwise accuracy is very close to the true pairwise accuracy from a noise-free, infinite test set setting (§4.4). We suggest the use of pairwise accuracy as it reflects metric performance well (which may be verified using this analysis). As a normalized form of pairwise accuracy, Kendall’s τ is also a suitable measure. (2) Since pairwise accuracy is computed against noisy human predictions, on average, it should be impossible for metrics to achieve a perfect accuracy. We suggest providing an upper bound of metric performance (§4.2) to clarify how much improvement is possible for metrics on the dataset."
    }, {
      "heading" : "10 Related work",
      "text" : "The fact that a manual evaluation can be weak, and an automatic one can be better is gaining attention in the metrics community. Mathur et al. (2020b) studied a disagreement between crowdworkers and metrics, and a reevaluation favored the metrics over the human prediction. Recently, Freitag et al. (2021) shows that metrics can achieve higher agreement with professional linguists than crowdworkers in judging translation systems. Their results fit into our formalization: if we assume professional linguists are unbiased, the bias and variance properties of metrics combined are superior to those of crowdworkers. Our analysis assumes that crowdworkers are unbiased, where they assume professional linguists are instead.\nWe wish to highlight several works which inspired the elements of ours: Chaganty et al. (2018) and Hashimoto et al. (2019) formalize metrics as\nstatistical estimators and provide understanding of their statistical properties and limits. In the replication of ImageNet, Engstrom et al. (2020) found that dataset bias accounted for classifier performance differences between the original and the replicated dataset, and provide a decomposition for the sources of error. In automated essay scoring, scorers are often evaluated against noisy human judgment, and Loukina et al. (2020) developed the PRMSE to calculate the MSE between scorer prediction and the true judgment, rather than noisy judgment. Finally, in bioinformatics, Li et al. (2020) derive an upper bound of the R2 coefficient due to experimental noise when regressing on experiment-derived results."
    }, {
      "heading" : "11 Conclusion",
      "text" : "Through rigorous comparison between metrics, humans, and the perfect segment-level annotator, we identify the settings where metrics outperform humans due to a statistical advantage in variance. These results challenge the notion that metrics are always secondary to human evaluation. Instead, we encourage practitioners to understand when human evaluation is weak, and when metrics are necessary. Finally, we hope to provide tools for analysis and future directions for evaluation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Discussions with Nitika Mathur, Markus Freitag, and Thibault Sellam led to several insights. Nelson Liu and Tianyi Zhang provided feedback on our first draft, and anonymous reviewers provided feedback on the submitted draft. Nanyun Peng advised the first author, and on this work. Alex Fabbri provided a scored version of the SummEval dataset. We thank all who have made our work possible."
    }, {
      "heading" : "A Equivalence between optimal prediction and true system differences",
      "text" : "There is a slight difference between the definition of the true difference in (5) which we can alternatively define as\n∆HS,S′ = sign(E[µ̂HS − µ̂HS′ ]) (19)\nand the definition of the optimal prediction ∆H∗S,S′ in (12), which is positive when\nPX (µ̂HS − µ̂HS′ > 0) > 1\n2 (20)\nand the two are not immediately equivalent. However, if we assume that the central limit theorem applies (which can be reasonable as our sample means always have n > 100) and X = µ̂HS − µ̂HS′ is normal, the CDF of X is\nF (x) = φ((x− E[X])/Var(X)) (21) where φ is the CDF of the standard normal distribution. Since the standard normal is centered and symmetric, φ(x) > 1/2 ⇐⇒ x > 0. Together we have\nF (x) > 1\n2 ⇐⇒ E[X] > x (22)\nwhere for x = 0 the left and right hand sides are equivalent to (20) and (19), respectively."
    }, {
      "heading" : "B Convergence of metric predictions to the main prediction",
      "text" : "A key assumption in interpreting the results from the bias-variance-noise decomposition in §4 is that as system-level metrics have access to more outputs for evaluation, metric predictions converges onto the main prediction.\nFor many metrics, their system-level score is the mean of their segment-level scores (e.g. BLEURT, BERTSCORE, ROUGE etc.). This is true for all summarization metrics. For these metrics, assuming the central limit theorem allows us to prove that metrics converge to the main prediction, similar to the proof in Appendix A. However, some MT metrics (BLEU, TER, and CHRF) are not simple averages of their segment-level scores, making them harder to analyze.\nFor system-level metrics that are not simple averages, we analytically observe that their aggregation method is similar to a mean (e.g. BLEU is a macro-average). We empirically verify that as the system-level metric evaluates on more test set outputs, their pairwise predictions converge to the\nmain predictions. Refer to Figures 3 and 4."
    }, {
      "heading" : "C Efficiency ratios for the perfect annotator",
      "text" : "With repeat human judgments for a given output example, we can estimate the variance of the perfect annotator (or true segment-level score variance) in WMT and SummEval. For WMT, we use only valid judgments (SYSTEM and REPEAT judgments), and discard all attention check judgments (BAD REF judgments). For SummEval, we use the dataset as is.\nIn WMT, we analyze all the to-English data grouped by year. We believe this grouping is appropriate because the to-English evaluation is batched together every year. Direct assessment, which WMT uses to collect human judgments (Graham et al., 2013), is a score assigned by crowdworkers to an English translation while referring to an English reference, requiring only monolingual knowledge."
    }, {
      "heading" : "D SummEval analysis results",
      "text" : "The main analyses in §5 and §6 are presented for SummEval here. When comparing expert humans to metrics, no metric comes close to expert performance at any number of expert judgments. For the power analysis, small differences are also hard to detect, similar to the findings in WMT. Note that while the perfect expert requires relatively less judgments compared to the perfect crowdworker in WMT, judgments from experts are likely to be much more expensive."
    }, {
      "heading" : "E Metric and human significance breakdown",
      "text" : "For the pairwise examples in WMT, we provide the co-occurrence of significance for metric and human estimators. Refer to Figures 6 and 7 for analyses on BERTSCORE and BLEURT, respectively."
    } ],
    "references" : [ {
      "title" : "SPICE: semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Pro-",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "An empirical investigation of statistical significance in NLP",
      "author" : [ "Taylor Berg-Kirkpatrick", "David Burkett", "Dan Klein." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-",
      "citeRegEx" : "Berg.Kirkpatrick et al\\.,? 2012",
      "shortCiteRegEx" : "Berg.Kirkpatrick et al\\.",
      "year" : 2012
    }, {
      "title" : "Ten years of wmt evaluation campaigns: Lessons learnt",
      "author" : [ "Ondrej Bojar", "C. Federmann", "B. Haddow", "Philipp Koehn", "Matt Post", "Lucia Specia." ],
      "venue" : "Proceedings of the LREC 2016 Workshop “Translation Evaluation – From Fragmented Tools and",
      "citeRegEx" : "Bojar et al\\.,? 2016a",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Results of the WMT17 metrics shared task",
      "author" : [ "Ondřej Bojar", "Yvette Graham", "Amir Kamran." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 489–513, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Bojar et al\\.,? 2017",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2017
    }, {
      "title" : "Results of the WMT16 metrics shared task",
      "author" : [ "Ondřej Bojar", "Yvette Graham", "Amir Kamran", "Miloš Stanojević." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 199–231, Berlin, Germany. As-",
      "citeRegEx" : "Bojar et al\\.,? 2016b",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Massive exploration of neural machine translation architectures",
      "author" : [ "Denny Britz", "Anna Goldie", "Minh-Thang Luong", "Quoc Le." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1442–1451, Copen-",
      "citeRegEx" : "Britz et al\\.,? 2017",
      "shortCiteRegEx" : "Britz et al\\.",
      "year" : 2017
    }, {
      "title" : "Re-evaluating the role of Bleu in machine translation research",
      "author" : [ "Chris Callison-Burch", "Miles Osborne", "Philipp Koehn." ],
      "venue" : "11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "With little power comes great responsibility",
      "author" : [ "Dallas Card", "Peter Henderson", "Urvashi Khandelwal", "Robin Jia", "Kyle Mahowald", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Card et al\\.,? 2020",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2020
    }, {
      "title" : "The price of debiasing automatic metrics in natural language evalaution",
      "author" : [ "Arun Chaganty", "Stephen Mussmann", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chaganty et al\\.,? 2018",
      "shortCiteRegEx" : "Chaganty et al\\.",
      "year" : 2018
    }, {
      "title" : "Importance sampling for unbiased on-demand evaluation of knowledge base population",
      "author" : [ "Arun Chaganty", "Ashwin Paranjape", "Percy Liang", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Chaganty et al\\.,? 2017",
      "shortCiteRegEx" : "Chaganty et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified bias-variance decomposition and its applications",
      "author" : [ "Pedro M. Domingos." ],
      "venue" : "Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pages",
      "citeRegEx" : "Domingos.,? 2000",
      "shortCiteRegEx" : "Domingos.",
      "year" : 2000
    }, {
      "title" : "The hitchhiker’s guide to testing statistical significance in natural language processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "An Introduction to the Bootstrap",
      "author" : [ "Bradley Efron", "Robert Tibshirani." ],
      "venue" : "Springer.",
      "citeRegEx" : "Efron and Tibshirani.,? 1993",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1993
    }, {
      "title" : "Identifying statistical bias in dataset replication",
      "author" : [ "Logan Engstrom", "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Jacob Steinhardt", "Aleksander Madry." ],
      "venue" : "CoRR, abs/2005.09619.",
      "citeRegEx" : "Engstrom et al\\.,? 2020",
      "shortCiteRegEx" : "Engstrom et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Summeval: Reevaluating summarization evaluation",
      "author" : [ "Alexander R Fabbri", "Wojciech Kryściński", "Bryan McCann", "Caiming Xiong", "Richard Socher", "Dragomir Radev." ],
      "venue" : "arXiv preprint arXiv:2007.12626.",
      "citeRegEx" : "Fabbri et al\\.,? 2020",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2020
    }, {
      "title" : "APE at scale and its implications on MT evaluation biases",
      "author" : [ "Markus Freitag", "Isaac Caswell", "Scott Roy." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 34–44, Florence, Italy. Association for Com-",
      "citeRegEx" : "Freitag et al\\.,? 2019",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEU might be guilty but references are not innocent",
      "author" : [ "Markus Freitag", "David Grangier", "Isaac Caswell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 61–71, Online. Association for",
      "citeRegEx" : "Freitag et al\\.,? 2020",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2020
    }, {
      "title" : "SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization",
      "author" : [ "Yang Gao", "Wei Zhao", "Steffen Eger." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1347–",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
      "author" : [ "Albert Gatt", "Emiel Krahmer." ],
      "venue" : "J. Artif. Intell. Res., 61:65–170.",
      "citeRegEx" : "Gatt and Krahmer.,? 2018",
      "shortCiteRegEx" : "Gatt and Krahmer.",
      "year" : 2018
    }, {
      "title" : "Testing for significance of increased correlation with human judgment",
      "author" : [ "Yvette Graham", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, Doha, Qatar. Associ-",
      "citeRegEx" : "Graham and Baldwin.,? 2014",
      "shortCiteRegEx" : "Graham and Baldwin.",
      "year" : 2014
    }, {
      "title" : "Is all that glitters in machine translation quality estimation really gold",
      "author" : [ "Yvette Graham", "Timothy Baldwin", "Meghan Dowling", "Maria Eskevich", "Teresa Lynn", "Lamia Tounsi" ],
      "venue" : "In Proceedings of COLING 2016,",
      "citeRegEx" : "Graham et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous measurement scales in human evaluation of machine translation",
      "author" : [ "Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33–41,",
      "citeRegEx" : "Graham et al\\.,? 2013",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2013
    }, {
      "title" : "Unifying human and statistical evaluation for natural language generation",
      "author" : [ "Tatsunori Hashimoto", "Hugh Zhang", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Hashimoto et al\\.,? 2019",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Certified robustness to adversarial word substitutions",
      "author" : [ "Robin Jia", "Aditi Raghunathan", "Kerem Göksel", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Peeking at A/B tests: Why it matters, and what to do about it",
      "author" : [ "Ramesh Johari", "Pete Koomen", "Leonid Pekelis", "David Walsh." ],
      "venue" : "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax,",
      "citeRegEx" : "Johari et al\\.,? 2017",
      "shortCiteRegEx" : "Johari et al\\.",
      "year" : 2017
    }, {
      "title" : "Uncertainty-aware generative models for inferring document class prevalence",
      "author" : [ "Katherine Keith", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Keith and O.Connor.,? \\Q2018\\E",
      "shortCiteRegEx" : "Keith and O.Connor.",
      "year" : 2018
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796,",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Repub-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Performance of regression models as a function of experiment noise",
      "author" : [ "Gang Li", "Jan Zrimec", "Boyang Ji", "Jun Geng", "Johan Larsbrink", "Aleksej Zelezniak", "Jens Nielsen", "Martin KM Engqvist" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Using PRMSE to evaluate automated scoring systems in the presence of label noise",
      "author" : [ "Anastassia Loukina", "Nitin Madnani", "Aoife Cahill", "Lili Yao", "Matthew S. Johnson", "Brian Riordan", "Daniel F. McCaffrey." ],
      "venue" : "Proceedings of the Fifteenth Workshop",
      "citeRegEx" : "Loukina et al\\.,? 2020",
      "shortCiteRegEx" : "Loukina et al\\.",
      "year" : 2020
    }, {
      "title" : "Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance",
      "author" : [ "Qingsong Ma", "Ondřej Bojar", "Yvette Graham." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges",
      "author" : [ "Qingsong Ma", "Johnny Wei", "Ondřej Bojar", "Yvette Graham." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics",
      "author" : [ "Nitika Mathur", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Mathur et al\\.,? 2020a",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2020
    }, {
      "title" : "Results of the wmt20 metrics shared task",
      "author" : [ "Nitika Mathur", "Johnny Wei", "Markus Freitag", "Qingsong Ma", "OndÅTMej Bojar." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 688–725, Online. Association for Computa-",
      "citeRegEx" : "Mathur et al\\.,? 2020b",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2020
    }, {
      "title" : "compare-mt: A tool for holistic comparison of language generation systems",
      "author" : [ "Graham Neubig", "Zi-Yi Dou", "Junjie Hu", "Paul Michel", "Danish Pruthi", "Xinyi Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Neubig et al\\.,? 2019",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2019
    }, {
      "title" : "Better summarization evaluation with word embeddings for ROUGE",
      "author" : [ "Jun-Ping Ng", "Viktoria Abrecht." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925–1930, Lisbon, Portugal. Association for",
      "citeRegEx" : "Ng and Abrecht.,? 2015",
      "shortCiteRegEx" : "Ng and Abrecht.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "A structured review of the validity of BLEU",
      "author" : [ "Ehud Reiter." ],
      "venue" : "Computational Linguistics, 44(3):393– 401.",
      "citeRegEx" : "Reiter.,? 2018",
      "shortCiteRegEx" : "Reiter.",
      "year" : 2018
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Crowdsourcing lightweight pyramids for manual summary evaluation",
      "author" : [ "Ori Shapira", "David Gabay", "Yang Gao", "Hadar Ronen", "Ramakanth Pasunuru", "Mohit Bansal", "Yael Amsterdamer", "Ido Dagan." ],
      "venue" : "Proceedings of the 2019 Conference of the",
      "citeRegEx" : "Shapira et al\\.,? 2019",
      "shortCiteRegEx" : "Shapira et al\\.",
      "year" : 2019
    }, {
      "title" : "RUBER: an unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Attaining the unattainable? reassessing claims of human parity in neural machine translation",
      "author" : [ "Antonio Toral", "Sheila Castilho", "Ke Hu", "Andy Way." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation, Volume 1: Research Papers,",
      "citeRegEx" : "Toral et al\\.,? 2018",
      "shortCiteRegEx" : "Toral et al\\.",
      "year" : 2018
    }, {
      "title" : "Bertscore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 40,
      "context" : "In machine translation (MT), metrics like BLEU (Papineni et al., 2002) enable settings where the amount of human effort required would be infeasible, such as architecture or hyperparameter search (Britz et al.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : ", 2002) enable settings where the amount of human effort required would be infeasible, such as architecture or hyperparameter search (Britz et al., 2017).",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 41,
      "context" : "As objective, reproducible quantities, BLEU scores facilitate cross-paper comparisons (Post, 2018).",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 32,
      "context" : "Metrics are an active research area in many NLG subfields, including summarization (Lin, 2004), dialogue (Tao et al.",
      "startOffset" : 83,
      "endOffset" : 94
    }, {
      "referenceID" : 45,
      "context" : "Metrics are an active research area in many NLG subfields, including summarization (Lin, 2004), dialogue (Tao et al., 2018), and image captioning (Anderson et al.",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : ", 2018), and image captioning (Anderson et al., 2016), which seek to realize the goal of quick and reliable automatic evaluation.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 37,
      "context" : "To recount a perplexing anecdote, in the most recent edition of the WMT metrics shared task (Mathur et al., 2020b), initial human evaluation disagreed with most metrics on a pairwise prediction of two translation systems.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 42,
      "context" : "Research in system-level metrics have a tradition of evaluating metric correlation to human judgment with the Pearson correlation coefficient (Reiter, 2018).",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "Second, direct measurement of any performance measure on our datasets introduces sample bias (Engstrom et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "tions μ̂S and μ̂ M S for each datapoint, so applying the William’s test (Graham and Baldwin, 2014) also falls short here.",
      "startOffset" : 72,
      "endOffset" : 98
    }, {
      "referenceID" : 41,
      "context" : "We evaluate the performance of the three metrics included in SacreBleu (BLEU, TER, chrF; Post, 2018; Koehn et al., 2007).",
      "startOffset" : 71,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Both metrics are found to effectively utilize contextual embeddings (Devlin et al., 2019), and BLEURT is a learned metric (tuned on data outside of WMT2019).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "The SummEval dataset (Fabbri et al., 2020) contains 100 outputs from 17 summarization systems.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 32,
      "context" : "This includes the traditional ROUGE-4 (Lin, 2004) summarization metric, its extension ROUGE-WE (Ng and Abrecht, 2015), and METEOR (Lavie and Agarwal, 2007).",
      "startOffset" : 38,
      "endOffset" : 49
    }, {
      "referenceID" : 39,
      "context" : "This includes the traditional ROUGE-4 (Lin, 2004) summarization metric, its extension ROUGE-WE (Ng and Abrecht, 2015), and METEOR (Lavie and Agarwal, 2007).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "This includes the traditional ROUGE-4 (Lin, 2004) summarization metric, its extension ROUGE-WE (Ng and Abrecht, 2015), and METEOR (Lavie and Agarwal, 2007).",
      "startOffset" : 130,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "In addition, we include two metrics based on BERT (Devlin et al., 2019).",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 47,
      "context" : "BertScore (Zhang et al., 2020), also present in the WMT analysis, and SUPERT (Gao et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : ", 2020), also present in the WMT analysis, and SUPERT (Gao et al., 2020), which is a reference-less metric for summarization.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Assuming the bootstrap (Efron and Tibshirani, 1993) which is a common procedure in NLP (Dror et al.",
      "startOffset" : 23,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "Assuming the bootstrap (Efron and Tibshirani, 1993) which is a common procedure in NLP (Dror et al., 2018), we can estimate the expectation quantities in the decomposition.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "The second case empirically shows studying the sampling distribution of metrics (Koehn, 2004; Berg-Kirkpatrick et al., 2012) is effective, as metric certainty in the difference of system quality often implies human certainty.",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "The second case empirically shows studying the sampling distribution of metrics (Koehn, 2004; Berg-Kirkpatrick et al., 2012) is effective, as metric certainty in the difference of system quality often implies human certainty.",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "A growing body of MT literature focuses on professional linguists (Freitag et al., 2020; Mathur et al., 2020b), and there are at least two known properties of their judgments: their judgments have better interannotator agreement (contain less noise), and they are more sensitive to linguistic phenomena.",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 37,
      "context" : "A growing body of MT literature focuses on professional linguists (Freitag et al., 2020; Mathur et al., 2020b), and there are at least two known properties of their judgments: their judgments have better interannotator agreement (contain less noise), and they are more sensitive to linguistic phenomena.",
      "startOffset" : 66,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "This section performs a power analysis to determine how much annotator effort is needed to reliably detect the correct pairwise judgment between two systems (Card et al., 2020).",
      "startOffset" : 157,
      "endOffset" : 176
    }, {
      "referenceID" : 38,
      "context" : "In addition, we may qualitatively analyze output statistics not intended to correlate with humans judgment at all (Neubig et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "In WMT16-19, direct assessment (Graham et al., 2013) was used to elicit judgments from a combination of crowdworkers and researchers.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "Employing a different collection methodology, such as human translation edit rate (HTER) of instead of DA, can result in divergent system rankings (Graham et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "In an earlier edition of WMT, DA judgments were collected with both a grammaticality prompt and an adequacy prompt, corresponding to different system rankings by the respective attribute (Bojar et al., 2016a).",
      "startOffset" : 187,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "Several studies have shown scoring differences between professional linguists and crowdworkers which are due in part to the fact that linguists are more sensitive to linguistic phenomena (Fabbri et al., 2019; Freitag et al., 2019).",
      "startOffset" : 187,
      "endOffset" : 230
    }, {
      "referenceID" : 17,
      "context" : "Several studies have shown scoring differences between professional linguists and crowdworkers which are due in part to the fact that linguists are more sensitive to linguistic phenomena (Fabbri et al., 2019; Freitag et al., 2019).",
      "startOffset" : 187,
      "endOffset" : 230
    }, {
      "referenceID" : 29,
      "context" : "On the human side, we may focus on creating larger effect sizes or reducing noise by adopting new annotation schemes (Läubli et al., 2018; Shapira et al., 2019) or employing professional linguists (Fabbri et al.",
      "startOffset" : 117,
      "endOffset" : 160
    }, {
      "referenceID" : 44,
      "context" : "On the human side, we may focus on creating larger effect sizes or reducing noise by adopting new annotation schemes (Läubli et al., 2018; Shapira et al., 2019) or employing professional linguists (Fabbri et al.",
      "startOffset" : 117,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : ", 2019) or employing professional linguists (Fabbri et al., 2020; Toral et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : ", 2019) or employing professional linguists (Fabbri et al., 2020; Toral et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "To make the human estimator more efficient, we may consider adaptive data collection techniques to stop data collection early when significance is achieved, in a statistically sound manner (Johari et al., 2017).",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "Variance reduction techniques can be applied to the human estimator by taking advantage of strong metrics (Chaganty et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "Probabilistic reinterpretations of current metrics could be a useful technique for confidence estimation (Keith and O’Connor, 2018).",
      "startOffset" : 105,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "Optimistically, metrics could have provable guarantees, ensuring the correctness of metric decisions (Jia et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 119
    } ],
    "year" : 2021,
    "abstractText" : "Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating systemlevel quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. By applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. Our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. In MT, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small.1",
    "creator" : "LaTeX with hyperref"
  }
}