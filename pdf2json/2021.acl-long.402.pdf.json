{
  "name" : "2021.acl-long.402.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bootstrapped Unsupervised Sentence Representation Learning",
    "authors" : [ "Yan Zhang", "Ruidan He", "Zuozhu Liu", "Lidong Bing", "Haizhou Li" ],
    "emails" : [ "eleyanz@nus.edu.sg,", "ruidan.he@alibaba-inc.com", "zuozhuliu@intl.zju.edu.cn,", "l.bing@alibaba-inc.com", "haizhou.li@nus.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5168–5180\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5168\nAs high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https: //github.com/yanzhangnlp/BSL."
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentence representation learning aims to map sentences into vectors that capture rich semantic information. Among previous approaches, supervised methods achieve state-of-the-art performance by leveraging quality sentence labels. For example, the recently proposed model Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) fine-tunes a Siamese BERT network on natural language inference (NLI) tasks with labeled sentence pairs. It achieves state-of-the-art results on multiple semantic textual similarity (STS) tasks. However, such performance is mostly induced by high-quality supervision, while labeled data are difficult and ex-\n∗ Equally Contributed. † Corresponding author.\npensive to obtain in practice. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks that differ significantly from NLI on which SBERT is fine-tuned.\nMany unsupervised methods learn sentence representations by optimizing over various selfsupervised learning (SSL) objectives on a largescale unlabeled corpus. Early works often use auto-encoders (Socher et al., 2011; Hill et al., 2016) or next-sentence prediction (Kiros et al., 2015) for sentence representation learning. Recently, more efforts have been devoted to representation learning with transformer-based networks using masked language modeling (MLM). However, transformer-based methods do not directly produce meaningful sentence representations. Instead, significant supervised fine-tuning steps with labeled data are commonly required to form good representations (Reimers and Gurevych, 2019). Recently, Giorgi et al. (2020) and Zhang et al. (2020) proposed novel transformer-based frameworks to directly learn sentence representations from an unlabeled corpus, which even exhibited competitive performance to the supervised counterparts on some tasks. However, Giorgi et al. (2020) required long text during training while the contrastive learning strategy employed by Zhang et al. (2020) need a careful treatment of negative pairs. More important, there is still great room for improvement in terms of the quality of learned sentence representations.\nIn this paper, we introduce Bootstrapped Sentence Representation Learning (BSL), a simple and lightweight framework that directly learns sentence representations without supervised finetuning. Our work is inspired by the recent success of Siamese networks (Bromley et al., 1994) for unsupervised visual representation learning (Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), especially the BYOL framework (Grill et al., 2020). These models employed\nvarious kinds of unsupervised learning objectives to maximize the similarity between two augmented views of each image, yielding performance on par with supervised methods. Unlike contrastive learning-based methods, which demand a carefully negative sampling process and large batch sizes, BYOL could achieve great performance without negative pairs.\nThe proposed BSL works as follows. Given an input sentence, we first construct two augmented views through back-translation. These two views are simultaneously fed into the two branches of the Siamese network, i.e., an online network and a target network following the terminology in (Grill et al., 2020). In particular, the online and target networks use two pre-trained transformer networks with the same structure, e.g., BERT, to encode the two views separately. During learning, the online network is trained to predict the representation of the other augmented view generated by the target network, and its parameters are updated by minimizing a predefined prediction loss. As for the target network, we apply a stop-gradient strategy (Chen and He, 2020) and update it with a weighted moving average of the online network. Hence, the outputs of the target network are iteratively bootstrapped to serve as targets, enabling enhanced representation learning of the online network while avoiding trivial solutions.\nOur method is evaluated through extensive experiments. Empirical results show that BSL significantly outperforms strong unsupervised baselines on a standard suite of STS and classification tasks from the SentEval benchmark (Conneau and Kiela, 2018). We also demonstrate that BSL can serve as an effective post-training approach to boost the performance of the state-of-the-art supervised SBERT model. We further extend our method for learning multilingual sentence representations and demonstrate that it is able to outperform strong multilingual baselines on cross-lingual STS tasks under both unsupervised and supervised settings. Detailed analysis of a few factors that could affect the model performance is provided as well to motivate future research."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Sentence Representation Learning",
      "text" : "Prior approaches for sentence representation learning include two main categories – supervised and unsupervised methods, while a few works\nmight leverage on both of them. Most of the supervised methods are trained on labeled natural language inference (NLI) datasets including Stanford NLI (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018). Recently, SBERT (Reimers and Gurevych, 2019) fine-tuned a pre-trained Siamese BERT network on NLI and demonstrated the state-of-the-art performance. Though effective, those methods highly rely on labeled data and could be problematic to port to new domains. Zhang et al. (2020) showed that SBERT generalizes poorly on target tasks with a data distribution significantly different from the NLI data.\nThere are also fruitful outcomes for unsupervised methods. Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with generative (Kiros et al., 2015; Hill et al., 2016) or discriminative objectives (Jernite et al., 2017; Logeswaran and Lee, 2018). Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield meaningful sentence representations directly without supervised fine-tuning. Reimers and Gurevych (2019) show that sentence embeddings obtained from BERT without fine-tuning even underperform the GloVe embeddings (Pennington et al., 2014) in terms of semantic textual similarity.\nMore recently, a few unsupervised methods were proposed to learn sentence representations from transformer-based networks without supervised fine-tuning. Li et al. (2020) proposes to transform the representation obtained by a pre-trained language model to an isotropic Gaussian distribution. Giorgi et al. (2020) minimizes the distance between different spans sampled from the same document. However, it requires an extremely long document of 2,048 tokens as input, which limits its applications to domains with only short documents. Zhang et al. (2020) proposed IS-BERT to maximize the mutual information between the global embedding and local n-gram embeddings of a given sentence. However, IS-BERT requires careful negative sampling and the n-gram embeddings may be suboptimal in capturing sentence-level semantics."
    }, {
      "heading" : "2.2 Unsupervised Representation Learning with Siamese Networks",
      "text" : "Siamese networks have been increasingly used in various models (Chen and He, 2020; Grill et al., 2020; Caron et al., 2020) for unsupervised visual representation learning. These models typically maximize the similarity between two augmented views of an image encoded by the Siamese network. The main difference among these models is how they prevent undesired trivial solutions. Most works rely on contrastive learning with negative sampling (Chen et al., 2020; Tian et al., 2020) to avoid collapsing. Our method BSL is mainly inspired by BYOL (Grill et al., 2020), which shows that one can learn transferable visual representations via bootstrapping representations without negative sampling. We transfer this learning strategy from images to texts with different network architectures and augmenting methods."
    }, {
      "heading" : "3 BSL",
      "text" : ""
    }, {
      "heading" : "3.1 Model Description",
      "text" : "Given a sentence x sampled from the dataset D without label information, our goal is to learn a meaningful representation h , f(x). In our framework, we adopt the idea from BYOL for unsupervised sentence representation learning with a\nSiamese network. The architecture of the proposed BSL is illustrated in Figure 1. Given a sentence x, we first obtain two augmented views x1 , T (x) and x2 , T ′(x), where T and T ′ are augmentation transformations.\nThe two views are fed into the Siamese network separately. The online network contains an encoder network fθ(·) and a predictor network pθ(·). The target network contains an encoder network fξ(·) without a predictor, leading to an asymmetric framework. For the first augmented view x1, the online network outputs a representation z1 , pθ(fθ(x1)). For the second augmented view, the target network outputs a representation h2 , fξ(x2). Afterwards, we define a mean squared loss between the two normalized representations from the online and target networks, which can be simplified as minimizing their negative cosine similarity:\nDθ,ξ(z1, h2) = − < z1 ‖z1‖ , h2 ‖h2‖ >, (1)\nwhere ‖ · ‖ denotes the l2-norm and <,> denotes the dot product between two vectors. As the loss is asymmetric over the two views, we also feed x2 to the online network and x1 to the target network to get z̃2 , pθ(fθ(x2)) and h̃1 , fξ(x1), leading to the final objective:\nLθ,ξ = 1\n2 Dθ,ξ(z1, h2) +\n1 2 Dθ,ξ(z̃2, h̃1). (2)\nThough we define the loss with parameters {θ, ξ}, we only update θ during training, as shown in the stop-gradient operation Fig 1. This stopgradient operation is empirically demonstrated effective for Siamese network (Grill et al., 2020; Chen and He, 2020). fξ is detached from the optimization graph of Lθ,ξ and will be updated with a weighted moving average of fθ. The updating dynamics becomes:\nθt ← θt−1 +5θLθ,ξ, (3) ξt ← δξt−1 + (1− δ)θt. (4)\nHere δ is the momentum. When it is set to 1, the target network is never updated. When it is set to 0, the target network is instantaneously synchronized to the online network at each training step. At the inference stage, we obtain the representation of a sentence with the online encoder fθ."
    }, {
      "heading" : "3.2 Architecture Details",
      "text" : "Augmentation We use back-translation to obtain two augmented views x1 and x2. In this work, we only consider input sentence x in English. We use an English-to-German machine translation (MT) system to translate x to y1, and subsequently use a German-to-English MT system to translate y1 back to x1 to obtain one augmented view. Similarly, we use English-to-French and French-to-English MT systems to obtain another augmented view x2.1 Besides back-translation, we also discuss other text augmentation approaches in § 4.4.\nArchitecture The online network fθ and the target network fξ take x1 and x2 as inputs and output h1 and h2. We use pre-trained language models to initialize the weights in fθ and fξ such that they benefit from the knowledge obtained at the pretraining stage. We apply average-pooling over outputs from the pre-trained language models to obtain h1 and h2. A multi-layer perceptron (MLP) pθ is stacked on top of fθ as the predictor to transform h1 to predictions z1 such as z1 matches the target representation h2."
    }, {
      "heading" : "4 Experiment",
      "text" : "Design We conduct various experiments to evaluate the effectiveness of the proposed method. Following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020), our major evaluations are conducted on the Semantic Textual Similarity (STS) tasks and the classification tasks with the SentEval toolkit (Conneau and Kiela, 2018). To demonstrate the flexibility of the proposed method, we further extend it for learning multilingual sentence representations and evaluate it on cross-lingual STS tasks.\nImplementation The MLP contains three linear layers. Given an input vector of dimension d, the output dimensions of the three layers are kd → kd→ d, where k is a hyperparameter controlling the hidden size. Batch normalization and rectified linear units (ReLU) are applied to the intermediate linear layers. We use BERT-base or RoBERTabase to initialize the online and target networks in monolingual settings.\nHyperparameter We tune learning rate, batch size, momentum δ, and the hyperparameter k on\n1We use Google translation engine. The datasets are released.\nthe development set of STS-B (Cer et al., 2017). For all unsupervised experiments, we set learning rate to 5e-4, momentum to 0.999, and k to 8. Adam (Kingma and Ba, 2015) is used as the optimizer. 2\nBaselines Under a unsupervised learning setting, we compare to the unigram-TFIDF model, the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016), the Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016). Those models are all trained on the Toronto book corpus with 70M sentences (Zhu et al., 2015). We also compare with sentence representations obtained with the average of GloVe embeddings (GloVe avg.), the average of BERT embeddings (BERT avg.), and the [CLS] representation of BERT (BERT [CLS]), as those are common ways to get sentence-level representations. We compare with BERT-flow (Li et al., 2020), a recent method that transforms the representation obtained by BERT to an isotropic Gaussian distribution. In addition, we compare with two unsupervised BERT fine-tuning methods. The first is to finetune BERT with masked language modeling (MLM) objective (BERT-mlm) (Gururangan et al., 2020). The second is IS-BERT (Zhang et al., 2020) which employs a mutual information maximization objective for fine-tuning BERT. We denote our model initialized by BERT-base (RoBERTa-base) as BSL-BERT (BSL-RoBERTa).\nUnder a supervised learning setting, we compared to InferSent (Conneau et al., 2017), Universal Sentence Encoder (USE) (Cer et al., 2018), and sentence BERT/RoBERTa (SBERT/SRoBERTa) (Reimers and Gurevych, 2019), which are all trained on the SNLI and MultiNLI datasets. To adapt BSL to a supervised learning setting, we first train a SBERT (SRoBERTa) model and then use the learned weights to initialize the online and target networks of BSL and perform BSL training. We denote this model variant as BSL-SBERT (BSL-SRoBERTa)."
    }, {
      "heading" : "4.1 Semantic Textual Similarity (STS)",
      "text" : "SentEval contains a suite of STS datasets including the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSB) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets con-\n2Hyperparameters and implementation details are attached in Appendix A\nsist of sentence pairs with scores from 0 to 5, where a larger score indicates higher semantic relatedness of the two sentences. We use Spearman’s rank correlation between the cosine-similarities of the sentence pairs and the gold scores as an evaluation metric, following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020).\nMost of the prior unsupervised methods were trained on the Toronto book corpus (Zhu et al., 2015), while the most recent and the best performed unsupervised method IS-BERT was trained on unlabeled texts from SNLI and Multi-Genre NLI (MultiNLI) datasets. To have a fair comparison with IS-BERT, we follow its setting to train BSL on unlabeled texts from the SNLI and MultiNLI datasets. The BERT-mlm baseline is also trained with the same setting for a fair comparison. We illustrate the effect of corpus choice in § 4.4. SNLI contains 570k sentence pairs and MultiNLI contains 430k sentence pairs from a wider range of genres of spoken and written texts. In both datasets, each sentence pair is labeled with contradiction, entailment, and neutral. Note that the labels are\nexcluded when training BSL in unsupervised settings.\nTable 1 presents the comparison results. Models are divided into two sets: trained on unlabeled data, or trained on labeled data. For unsupervised models, Unigram-TFIDF, SDAE, SkipThought and FastSent are trained on the Toronto book corpus while BERT-mlm, IS-BERT, BERT-flow and our proposed method are trained on NLI. In the supervised setting, BSL-SBERT and BSL-SRoBERTa only take labeled entailment pairs as the inputs to the online and target networks.\nWe make the following observations. First, BSL outperforms all prior unsupervised methods by large margins. On average, it outperforms IS-BERT and BERT-flow trained with the same encoder and training corpus by 5.45%, and 6.65%, respectively. It even outperforms supervised baselines InferSent and USE. Second, unsupervised BSL still underperforms SBERT since the latter was fine-tuned on labeled NLI data. We show that by using BSL as a post-training approach, BSL-SBERT ( BSLSRoBERTa) can further increase the average result\nby 2.6% (4.7%) from SBERT. This suggests that BSL can also be used as an effective post-training approach after supervised fine-tuning."
    }, {
      "heading" : "4.2 SentEval Classification Tasks",
      "text" : "Following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020), we evaluate sentence representations on a set of classification tasks from SentEval. The evaluation is done by the SentEval toolkit. It takes sentence representations as fixed input features to a logistic regression classifier, which is trained in a 10-fold cross-validation setup and the prediction results is computed on the test-fold. The sentence encoder is not fine-tuned in the training process. This set of tasks is the common bechmark used to evaluate the transferability of sentence representations on downstream tasks.\nTable 2 presents the comparison results. On average, BSL outperforms all prior unsupervised baselines. It also outperforms supervised baselines InferSent and USE, and only slightly underperforms SBERT. BSL-SBERT can marginally improve the results of SBERT. BSL-SRoBERTa achieves the best performance."
    }, {
      "heading" : "4.3 Multilingual STS",
      "text" : "In this subsection, we show that BSL can be easily extended for learning multilingual sentence representations. Following (Reimers and Gurevych, 2020), we conduct evaluation on the multilingual STS 2017 dataset (Cer et al., 2017) which contains annotated pairs for EN-EN, AR-AR, ES-ES, ENAR, EN-ES, EN-TR, EN-DE, and EN-FR.\nTo learn multilingual representations under the unsupervised setting, we process the NLI data as follows. We translate the English NLI sentences to AR, ES, TR, DE and FR using Google translation engine and pair the original English sentence to each of its translations. We obtain 5 pairs (ENAR/ES/TR/DE/FR) from one sentence and treat the English sentence as one view and its translation as the other view. We concatenate all pairs as the training data. We use multilingual BERT (mBERT) to initialize fθ and fξ, such that the token-level representations between the different languages are aligned. The remaining training procedure is the same as described in § 3. We denote our unsupervised model as BSL-uns. We compare with sentence representations obtained with mean pooling of mBERT and XLM-R (Conneau et al., 2020) embeddings under the unsupervised setting.\nFor supervised learning, we compare with meth-\nods from (Reimers and Gurevych, 2020): mBERT/ XLM-R-nli-stsb denotes the setting where we fine-tune XLM-R and mBERT on the English NLI and the English training set of the STS benchmark (STS-B); mBERT- /XLM-R ← SBERT-nli-stsb is the knowledge-distillation method proposed in their paper where we learn mBERT and XLM-R to imitate the output of the English SBERT trained on NLI and STS-B with multilingual parallel sentence pairs. We also compared to results of mUSE (Chidambaram et al., 2019) and LaBSE (Feng et al., 2020), which use dual encoder transformer architectures. mUSE was trained on question-answer pairs, SNLI, translated SNLI data, and parallel corpora over 16 languages. LaBSE was trained on 6 billion translation pairs for 109 languages. For BSL, we initialize our online and target networks with the learned weights from XLM-R← SBERTnli-stsb3 and then perform BSL training in a same way as described above. We denote our model in this setting as BSL-sup.\nTable 3 presents the results. Under the unsupervised setting, averaging the multilingual token representations yields poor results. BSL-uns achieves promising results with scores higher than 70. For the supervised methods, we observe that directly fine-tuning multilingual pre-trained models on English NLI and STS-B datasets does not generalize well in a cross-lingual setting. Knowledge distillation-based models are strong baselines. Applying BSL as a post-training approach can boost the results of the distilled models by large margins. These observations demonstrate that BSL has the\n3Downloaded from https://www.sbert.net/ docs/pretrained_models.html\nflexibility to be applied to learning multilingual sentence representations."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "In this subsection, we discuss a few factors that could affect the model performance. We use BERTbase as the encoder for analysis.\nChoice of Corpus Previous works (Hill et al., 2016; Cer et al., 2018) indicated that the dataset used for learning sentence representations in a supervised setting significantly impacts their performance on STS tasks. They found learning with NLI datasets is particularly useful and yields good results on common STS benchmarks. We have similar observations with the proposed unsupervised method. In Table 4, we show the results of training our model with a subset of 5 million sentences from the Toronto book corpus. This setting achieves an average result of 69.65 on STS tasks, still outperforming prior best unsupervised model IS-BERT by 3.07%, which again demonstrates the effectiveness of the proposed framework.\nHowever, we observe that the average result obtained from training with the book corpus is 2.38% lower than the result of training with the NLI datasets even the number of training pairs of the latter is only 1 million. Training on both of them still underperforms training on NLI alone. This finding indicates that the choice of training corpus is a key factor that affects model performance. When evaluating the common STS benchmarks as used in our experiments, the NLI datasets are better choices as they are semantically related to the STS data. We also conduct an evaluation on an Argument Facet Similarity task, which is more domain-specific and\ndissimilar to the NLI tasks. The results are provided in Appendix B. We find that in this scenario, training with NLI data yields poor generalization results on the target test set while training on the target raw text yields a much better performance. The results indicate that semantically related corpus to the target task should be adopted as the training set.\nAugmentation Techniques It has been shown that data augmentation plays a crucial role in unsupervised visual representation learning (He et al., 2020; Chen et al., 2020; Grill et al., 2020). The images can be augmented easily by rotating, resizing, or cropping (Chen et al., 2020). However, less work has been done on augmentation techniques for texts (Fang et al., 2020; Giorgi et al., 2020). Here, we study how different augmentation techniques would affect the model performance. We present the results of another two augmentation approaches besides back-translation in Table 4. Synonym denotes the setting where we randomly replace a few words with their synonyms. MLM denotes the setting where we first randomly mask a few tokens and then use a pre-trained masked language model to generate the masked tokens. Specifically, for both methods, given a sentence x, we make x1 = x and obtain x2 with the respective augmentation technique. We found that\nusing one augmented view performs slightly better than using two augmented views for synonymand MLM-based methods. One possible reason is that these methods may generate augmented sentences with semantics totally different from the original sentences as we will show in this subsection. Such kind of augmentation may bring in too much randomness and noise. Therefore using two augmented views might instead harm the model performance.\nFor Synonym, we select 30% of words and substitute them with similar words according to WordNet (Miller, 1995). For MLM, we mask 20% of tokens and use RoBERTa-base for token generation. In addition, we show results of a setting where we treat the sentence pairs labeled with entailment from the NLI datasets as the two views (NLIentail) for our model, as well as a setting using the combination of NLI unlabeled text with backtranslations and the entailment pairs as the training corpus(Back-translation+NLIentail). The purpose is to illustrate how our model would perform with high quality augmented data.\nThe results in Table 4 show that our proposed framework can work with both Synonym and MLM, as they still outperform IS-BERT on the average result by 1.63% and 2.81%, respectively. However, they are less effective compared to Backt-\ntranslation. We observe that training with entailment pairs yields good results, with only 300k training pairs, NLIentail is comparable to the model trained on all data from the NLI datasets augmented with back-translation (1 million training pairs). In addition, when training on both (Back-translation + NLIentail), a 2.91% improvement on the average result over Back-translation is observed. The results indicate that the quality of the augmented pairs directly affects the performance of the proposed framework.\nTable 5 presents an example of augmentations generated to the same sentence.4 We observe that Synonym substitutes words without considering the context while MLM generates words based on the context but losing the original word semantics. Back-translation yields a relatively better sentence, however, the drawback of which is that it relies on external machine translation systems. The Entailment refers to the sentence in the NLI datasets to which the original sentence has an entailment relation. It can be regarded as an ideal augmentation of the original sentence. How to automatically generate such augmentations remains an open question, and we leave it to future research.\nMomentum The momentum δ in Equation (4) is an important hyperparameter. When it is set to 1, the target network is never updated and remains the same to its initialization. When it is set to 0, the target network is updated to the online network at each training step. Table 6 shows the results of our method with different values of momentum. We observe that our proposed method works better with larger momentum near but not equals to 1. A similar phenomenon has also been observed in BYOL (Grill et al., 2020). In addition, we find that\n4More examples are provided in Appendix C\nalthough directly averaging the token embeddings from BERT yields poor sentence representations as shown in Table 1, initializing the target network using BERT and keeping it unchanged (set momentum to 1) during the learning procedure helps the online network learn much better representations, yielding a 21.84% improvement on STS-B.\nBatch Size & Contrastive Learning Lastly, we analyze the effect of batch size. Table 7 shows how the proposed model performs with batch sizes in {16, 32, 64, 128}. We also compare to a setting where contrastive learning is used as the selfsupervised learning objective since it is more commonly used in visual representation learning (Chen et al., 2020). Specifically, in this setting, given a batch of n augmented sentence pairs (2n sentences), each of them is treated as a positive pair. For each positive pair, we treat the other 2(n− 1) augmented examples within the minibatch as negative examples.\nThe results in Table 7 show that for BSL, setting the batch size to 64 yields the best result. Overall BSL is less sensitive to changes in batch size while contrastive learning tends to perform better with a larger batch size such that sufficient negative samples can be obtained. Contrastive learning may achieve better performance with a larger batch size while we leave it for future investigation due to its large memory consumption."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose BSL for unsupervised sentence representation learning. The experimental results demonstrate that our method could significantly outperform the state-of-the-art unsupervised methods and it can be further extended for learning multilingual sentence representations. In future work, we expect both theoretically advance of Siamese networks for representation learning, e.g., why stop-gradient works so well and how to further improve the updating dynamics, as well as specifically designated ideas for NLP, e.g., augmentation or learning objectives."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is partly supported by Human-Robot Interaction Phase 1 (Grant No. 19225 00054), National Research Foundation (NRF) Singapore under the National Robotics Programme; Human Robot Collaborative AI for AME (Grant No. A18A2b0046), NRF Singapore."
    }, {
      "heading" : "B Argument Facet Similarity",
      "text" : "We have demonstrated that the proposed method significantly outperforms other unsupervised baselines on a suite of STS and classification tasks that are commonly used in previous works. However, those tasks are less domain or task specific. Here, we further investigate the effectiveness of BSL in a domain-specific scenario. Following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020), we conduct evaluations on an Argument Facet Similarity (AFS) (Misra et al., 2016) dataset.\nThe dataset consists of 6k argument pairs on three controversial topics: gun control, gay marriage, and death penalty. Each pair was annotated on a scale from 0 (different) to 5 (equivalent). This dataset is more challenging compared to the STS benchmarks: the lexical gap between the sentences in AFS is larger and to be consider similar, a pair of arguments must not only make similar claims, but also provide a similar reasoning.\nWe compare models in a setting where task- or domain-specific labeled data is not available. In this setting, supervised method such as SBERT and InferSent need to be trained on NLI data and perform cross-domain predictions on the AFS sentence pairs. Unsupervised methods such as BERTmlm, IS-BERT and our proposed BSL can be directly trained on the task-specific raw texts.\nTable 10 shows the comparison results. We present both Pearson correlation and Spearman’s rank correlation. The results show that the proposed method still outperforms other methods. It is interesting to find that the two supervised methods InferSent and SBERT perform the worst in this setting. This is due to the fact that AFS data differes significantly from NLI data. This suggests that the domain-relatedness between the training set and the target test set has a huge impact on the model performance, and the models learned with supervised methods are problematic to port to other distant domains."
    }, {
      "heading" : "C More Examples",
      "text" : "More examples of augmentations generated by different approaches are provided in the Table 11."
    } ],
    "references" : [ {
      "title" : "Semeval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proc. of Se-",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proc. of Se-",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre." ],
      "venue" : "Proc. of SemEval@ACL.",
      "citeRegEx" : "Agirre et al\\.,? 2012",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : "sem 2013 shared task: Semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo." ],
      "venue" : "The Second Joint Conference on Lexical and Computational Semantics.",
      "citeRegEx" : "Agirre et al\\.,? 2013",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2013
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Signature verification using a ”siamese” time delay neural network",
      "author" : [ "J. Bromley", "J. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "Eduard Säckinger", "R. Shah." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Bromley et al\\.,? 1994",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1994
    }, {
      "title" : "Unsupervised learning of visual features by contrasting cluster assignments",
      "author" : [ "M. Caron", "I. Misra", "J. Mairal", "Priya Goyal", "P. Bojanowski", "Armand Joulin." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Caron et al\\.,? 2020",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity multilingual and cross-lingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proc. of SemEval@ACL.",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal sentence encoder for English",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He" ],
      "venue" : null,
      "citeRegEx" : "Chen and He.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2020
    }, {
      "title" : "Learning cross-lingual sentence representations via a multi-task dual-encoder model",
      "author" : [ "Muthu Chidambaram", "Yinfei Yang", "Daniel Cer", "Steve Yuan", "Yunhsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of the 4th Workshop on Representa-",
      "citeRegEx" : "Chidambaram et al\\.,? 2019",
      "shortCiteRegEx" : "Chidambaram et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "SentEval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proc. of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Cert: Contrastive self-supervised learning for language understanding",
      "author" : [ "Hongchao Fang", "Sicheng Wang", "Meng Zhou", "Jiayuan Ding", "Pengtao Xie." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Fang et al\\.,? 2020",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "Languageagnostic BERT sentence embedding",
      "author" : [ "Fangxiaoyu Feng", "Yinfei Yang", "Daniel Cer", "Naveen Arivazhagan", "Wei Wang." ],
      "venue" : "CoRR, abs/2007.01852.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M. Giorgi", "Osvald Nitski", "Gary D. Bader", "Bo Wang." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Giorgi et al\\.,? 2020",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2020
    }, {
      "title" : "Bootstrap your own latent: A new approach to self-supervised learning",
      "author" : [ "Michal Valko." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Valko.,? 2020",
      "shortCiteRegEx" : "Valko.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross B. Girshick" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning distributed representations of sentences from unlabelled data",
      "author" : [ "Felix Hill", "Kyunghyun Cho", "Anna Korhonen." ],
      "venue" : "Proc. of NAACL-HLT.",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Discourse-based objectives for fast unsupervised sentence representation learning",
      "author" : [ "Yacine Jernite", "Samuel R. Bowman", "David A. Sontag." ],
      "venue" : "arXiv preprint arXiv:1705.00557.",
      "citeRegEx" : "Jernite et al\\.,? 2017",
      "shortCiteRegEx" : "Jernite et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Russ R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V. Le", "Tomas Mikolov." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "An efficient framework for learning sentence representations",
      "author" : [ "Lajanugen Logeswaran", "Honglak Lee." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Logeswaran and Lee.,? 2018",
      "shortCiteRegEx" : "Logeswaran and Lee.",
      "year" : 2018
    }, {
      "title" : "A SICK cure for the evaluation of compositional distributional semantic models",
      "author" : [ "Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Marelli et al\\.,? 2014",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "G. Miller." ],
      "venue" : "Commun. ACM, 38:39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Measuring the similarity of sentential arguments in dialogue",
      "author" : [ "Amita Misra", "Brian Ecker", "Marilyn Walker." ],
      "venue" : "Proc. of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue.",
      "citeRegEx" : "Misra et al\\.,? 2016",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proc. of EMNLP-IJCNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H. Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y. Ng." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "What makes for good views for contrastive learning? In Proc",
      "author" : [ "Yonglong Tian", "Chen Sun", "Ben Poole", "Dilip Krishnan", "Cordelia Schmid", "Phillip Isola." ],
      "venue" : "of NeurIPS.",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proc. of NAACL-HLT.",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "An unsupervised sentence embedding method by mutual information maximization",
      "author" : [ "Yan Zhang", "Ruidan He", "Zuozhu Liu", "Kwan Hui Lim", "Lidong Bing." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "S. Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proc. of ICCV.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "For example, the recently proposed model Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) fine-tunes a Siamese BERT network on natural language inference (NLI) tasks with labeled sentence pairs.",
      "startOffset" : 63,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "Early works often use auto-encoders (Socher et al., 2011; Hill et al., 2016) or next-sentence prediction (Kiros et al.",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "Early works often use auto-encoders (Socher et al., 2011; Hill et al., 2016) or next-sentence prediction (Kiros et al.",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : ", 2016) or next-sentence prediction (Kiros et al., 2015) for sentence representation learning.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 34,
      "context" : "nificant supervised fine-tuning steps with labeled data are commonly required to form good representations (Reimers and Gurevych, 2019).",
      "startOffset" : 107,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "Our work is inspired by the recent success of Siamese networks (Bromley et al., 1994) for unsupervised visual representation learning (Chen et al.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : ", 1994) for unsupervised visual representation learning (Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), especially the BYOL framework (Grill et al.",
      "startOffset" : 56,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : ", 1994) for unsupervised visual representation learning (Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), especially the BYOL framework (Grill et al.",
      "startOffset" : 56,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : ", 1994) for unsupervised visual representation learning (Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), especially the BYOL framework (Grill et al.",
      "startOffset" : 56,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "(Chen and He, 2020) and update it with a weighted moving average of the online network.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Empirical results show that BSL significantly outperforms strong unsupervised baselines on a standard suite of STS and classification tasks from the SentEval benchmark (Conneau and Kiela, 2018).",
      "startOffset" : 168,
      "endOffset" : 193
    }, {
      "referenceID" : 4,
      "context" : "Most of the supervised methods are trained on labeled natural language inference (NLI) datasets including Stanford NLI (SNLI) (Bowman et al., 2015) and MultiNLI (Williams et al.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Early methods demonstrate good performance on a wide range of tasks (Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 108
    }, {
      "referenceID" : 34,
      "context" : "Recently, SBERT (Reimers and Gurevych, 2019) fine-tuned a pre-trained Siamese BERT network on NLI and demonstrated the state-of-the-art performance.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 36,
      "context" : "Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with genera-",
      "startOffset" : 86,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with genera-",
      "startOffset" : 86,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : "Some early studies attempt to learn from the internal structures within each sentence (Socher et al., 2011; Hill et al., 2016; Le and Mikolov, 2014) or utilize a distributional hypothesis to encode contextual information with genera-",
      "startOffset" : 86,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "tive (Kiros et al., 2015; Hill et al., 2016) or discriminative objectives (Jernite et al.",
      "startOffset" : 5,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "tive (Kiros et al., 2015; Hill et al., 2016) or discriminative objectives (Jernite et al.",
      "startOffset" : 5,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : ", 2016) or discriminative objectives (Jernite et al., 2017; Logeswaran and Lee, 2018).",
      "startOffset" : 37,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : ", 2016) or discriminative objectives (Jernite et al., 2017; Logeswaran and Lee, 2018).",
      "startOffset" : 37,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield mean-",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 28,
      "context" : "Recently, transformer-based networks attract more attentions (Devlin et al., 2019; Liu et al., 2019), however, they do not yield mean-",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "Reimers and Gurevych (2019) show that sentence embeddings obtained from BERT without fine-tuning even underperform the GloVe embeddings (Pennington et al., 2014) in",
      "startOffset" : 136,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "various models (Chen and He, 2020; Grill et al., 2020; Caron et al., 2020) for unsupervised visual representation learning.",
      "startOffset" : 15,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "various models (Chen and He, 2020; Grill et al., 2020; Caron et al., 2020) for unsupervised visual representation learning.",
      "startOffset" : 15,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Most works rely on contrastive learning with negative sampling (Chen et al., 2020; Tian et al., 2020) to avoid collapsing.",
      "startOffset" : 63,
      "endOffset" : 101
    }, {
      "referenceID" : 37,
      "context" : "Most works rely on contrastive learning with negative sampling (Chen et al., 2020; Tian et al., 2020) to avoid collapsing.",
      "startOffset" : 63,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "This stopgradient operation is empirically demonstrated effective for Siamese network (Grill et al., 2020; Chen and He, 2020).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 34,
      "context" : "Following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020), our major evaluations are con-",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 39,
      "context" : "Following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020), our major evaluations are con-",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "ducted on the Semantic Textual Similarity (STS) tasks and the classification tasks with the SentEval toolkit (Conneau and Kiela, 2018).",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : "Baselines Under a unsupervised learning setting, we compare to the unigram-TFIDF model, the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016), the Skipthought (Kiros et al.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 25,
      "context" : ", 2016), the Skipthought (Kiros et al., 2015) and FastSent (Hill et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 40,
      "context" : "Those models are all trained on the Toronto book corpus with 70M sentences (Zhu et al., 2015).",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "We compare with BERT-flow (Li et al., 2020), a recent method that transforms the representation obtained by BERT to an isotropic Gaussian distribution.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "to finetune BERT with masked language modeling (MLM) objective (BERT-mlm) (Gururangan et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 39,
      "context" : "The second is IS-BERT (Zhang et al., 2020) which employs a mutual information maximization objective for fine-tuning BERT.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "Under a supervised learning setting, we compared to InferSent (Conneau et al., 2017), Universal Sentence Encoder (USE) (Cer et al.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : ", 2017), Universal Sentence Encoder (USE) (Cer et al., 2018), and sentence BERT/RoBERTa (SBERT/SRoBERTa) (Reimers and Gurevych, 2019), which are all trained on the SNLI and MultiNLI datasets.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 34,
      "context" : ", 2018), and sentence BERT/RoBERTa (SBERT/SRoBERTa) (Reimers and Gurevych, 2019), which are all trained on the SNLI and MultiNLI datasets.",
      "startOffset" : 52,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2012, 2013, 2014, 2015, 2016), the STS benchmark (STSB) (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : ", 2017), and the SICK-Relatedness dataset (Marelli et al., 2014).",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Results of baselines marked with † are obtained from (Hill et al., 2016) (with a different number of decimal places).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "Results of baselines marked with ‡, ∗ and ◦ are obtained from (Reimers and Gurevych, 2019), (Zhang et al.",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 39,
      "context" : "Results of baselines marked with ‡, ∗ and ◦ are obtained from (Reimers and Gurevych, 2019), (Zhang et al., 2020) and (Li et al.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : "We use Spearman’s rank correlation between the cosine-similarities of the sentence pairs and the gold scores as an evaluation metric, following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 204
    }, {
      "referenceID" : 39,
      "context" : "We use Spearman’s rank correlation between the cosine-similarities of the sentence pairs and the gold scores as an evaluation metric, following prior works (Reimers and Gurevych, 2019; Zhang et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : "Most of the prior unsupervised methods were trained on the Toronto book corpus (Zhu et al., 2015), while the most recent and the best performed unsupervised method IS-BERT was trained on unlabeled texts from SNLI and Multi-Genre NLI (MultiNLI) datasets.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "Results of baselines marked with † are obtained from (Hill et al., 2016) (with a different number of decimal places).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "Results of baselines marked with ‡ and ∗ are obtained from (Reimers and Gurevych, 2019) and (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 39,
      "context" : "Results of baselines marked with ‡ and ∗ are obtained from (Reimers and Gurevych, 2019) and (Zhang et al., 2020), respectively.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 35,
      "context" : "Following (Reimers and Gurevych, 2020), we conduct evaluation on the multilingual STS 2017 dataset (Cer et al.",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "Following (Reimers and Gurevych, 2020), we conduct evaluation on the multilingual STS 2017 dataset (Cer et al., 2017) which contains annotated pairs for EN-EN, AR-AR, ES-ES, EN-",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "We compare with sentence representations obtained with mean pooling of mBERT and XLM-R (Conneau et al., 2020) embeddings under the unsupervised setting.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 35,
      "context" : "Results of baselines are obtained from (Reimers and Gurevych, 2020).",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 35,
      "context" : "ods from (Reimers and Gurevych, 2020): mBERT/ XLM-R-nli-stsb denotes the setting where we fine-tune XLM-R and mBERT on the English NLI and the English training set of the STS benchmark (STS-B); mBERT- /XLM-R ← SBERT-nli-stsb is the knowledge-distillation method proposed in",
      "startOffset" : 9,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "We also compared to results of mUSE (Chidambaram et al., 2019) and LaBSE (Feng et al.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Choice of Corpus Previous works (Hill et al., 2016; Cer et al., 2018) indicated that the dataset used for learning sentence representations in a supervised setting significantly impacts their performance on STS tasks.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Choice of Corpus Previous works (Hill et al., 2016; Cer et al., 2018) indicated that the dataset used for learning sentence representations in a supervised setting significantly impacts their performance on STS tasks.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "Augmentation Techniques It has been shown that data augmentation plays a crucial role in unsupervised visual representation learning (He et al., 2020; Chen et al., 2020; Grill et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Augmentation Techniques It has been shown that data augmentation plays a crucial role in unsupervised visual representation learning (He et al., 2020; Chen et al., 2020; Grill et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "The images can be augmented easily by rotating, resizing, or cropping (Chen et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "However, less work has been done on augmentation techniques for texts (Fang et al., 2020; Giorgi et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "However, less work has been done on augmentation techniques for texts (Fang et al., 2020; Giorgi et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 31,
      "context" : "For Synonym, we select 30% of words and substitute them with similar words according to WordNet (Miller, 1995).",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "We also compare to a setting where contrastive learning is used as the selfsupervised learning objective since it is more commonly used in visual representation learning (Chen et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 189
    } ],
    "year" : 2021,
    "abstractText" : "As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https: //github.com/yanzhangnlp/BSL.",
    "creator" : "LaTeX with hyperref"
  }
}