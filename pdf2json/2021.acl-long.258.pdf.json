{
  "name" : "2021.acl-long.258.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Meta-Learning to Compositionally Generalize",
    "authors" : [ "Henry Conklin", "Bailin Wang", "Kenny Smith", "Ivan Titov" ],
    "emails" : [ "kenny.smith}@ed.ac.uk,", "ititov@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3322–3335\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3322"
    }, {
      "heading" : "1 Introduction",
      "text" : "Compositionality is the property of human language that allows for the meaning of a sentence to be constructed from the meaning of its parts and the way in which they are combined (Cann, 1993). By decomposing phrases into known parts we can generalize to novel sentences despite never having encountered them before. In practice this allows us to produce and interpret a functionally limitless number of sentences given finite means (Chomsky, 1965).\n∗Equal contribution.\nWhether or not neural networks can generalize in this way remains unanswered. Prior work asserts that there exist fundamental differences between cognitive and connectionist architectures that makes compositional generalization by the latter unlikely (Fodor and Pylyshyn, 1988). However, recent work has shown these models’ capacity for learning some syntactic properties. Hupkes et al. (2018) show how some architectures can handle hierarchy in an algebraic context and generalize in a limited way to unseen depths and lengths. Work looking at the latent representations learned by deep machine translation systems show how these models seem to extract constituency and syntactic class information from data (Blevins et al., 2018; Belinkov et al., 2018). These results, and the more general fact that neural models perform a variety of NLP tasks with high fidelity (eg. Vaswani et al., 2017; Dong and Lapata, 2016), suggest these models have some sensitivity to syntactic structure and by extension may be able to learn to generalize compositionally.\nRecently there have been a number of datasets designed to more formally assess connectionist models’ aptitude for compositional generalization (Kim and Linzen, 2020; Lake and Baroni, 2018; Hupkes et al., 2019). These datasets frame the problem of compositional generalization as one of outof-distribution generalization: the model is trained on one distribution and tested on another which differs in ways that would be trivial for a compositional strategy to resolve. A variety of neural network architectures have shown mixed performance across these tasks, failing to show conclusively that connectionist models are reliably capable of generalizing compositionally (Keysers et al., 2020; Lake and Baroni, 2018). Natural language requires a mixture of memorization and generalization (Jiang et al., 2020), memorizing exceptions and atomic concepts with which to generalize. Previous work\nlooking at compositional generalization has suggested that models may memorize large spans of sentences multiple words in length (Hupkes et al., 2019; Keysers et al., 2020). This practice may not harm in-domain performance, but if at test time the model encounters a sequence of words it has not encountered before it will be unable to interpret it having not learned the atoms (words) that comprise it. Griffiths (2020) looks at the role of limitations in the development of human cognitive mechanisms. Humans’ finite computational ability and limited memory may be central to the emergence of robust generalization strategies like compositionality. A hard upper-bound on the amount we can memorize may be in part what forces us to generalize as we do. Without the same restriction models may prefer a strategy that memorizes large sections of the input potentially inhibiting their ability to compositionally generalize.\nIn a way the difficulty of these models to generalize out of distribution is unsurprising: supervised learning assumes that training and testing data are drawn from the same distribution, and therefore does not necessarily favour strategies that are robust out of distribution. Data necessarily underspecifies for the generalizations that produced it. Accordingly for a given dataset there may be a large number of generalization strategies that are compatible with the data, only some of which will perform well outside of training (D’Amour et al., 2020). It seems connectionist models do not reliably extract the strategies from their training data that generalize well outside of the training distribution. Here we focus on an approach that tries to to introduce a bias during training such that the model arrives at a more robust strategy.\nTo do this we implement a variant of the model agnostic meta-learning algorithm (MAML, Finn et al., 2017a). The approach used here follows Wang et al. (2020a) which implements an objective function that explicitly optimizes for out-ofdistribution generalization in line with Li et al. (2018). Wang et al. (2020a) creates pairs of tasks for each batch (which here we call meta-train and meta-test) by sub-sampling the existing training data. Each meta-train, meta-test task pair is designed to simulate the divergence between training and testing: meta-train is designed to resemble the training distribution, and meta-test to resemble the test distribution. The training objective then requires that update steps taken on meta-train are\nalso beneficial for meta-test. This serves as a kind of regularizer, inhibiting the model from taking update steps that only benefit meta-train. By manipulating the composition of meta-test we can control the nature of the regularization applied. Unlike other meta-learning methods this is not used for few or zero-shot performance. Instead it acts as a kind of meta-augmented supervised learning, that helps the model to generalize robustly outside of its training distribution.\nThe approach taken by Wang et al. (2020a) relies on the knowledge of the test setting. While it does not assume access to the test distribution, it assumes access to the family of test distributions, from which the actual test distribution will be drawn. While substantially less restrictive than the standard iid setting, it still poses a problem if we do not know the test distribution, or if the model is evaluated in a way that does not lend itself to being represented by discrete pairs of tasks (i.e. if test and train differ in a variety of distinct ways). Here we propose a more general approach that aims to generate meta-train, meta-test pairs which are populated with similar (rather than divergent) examples in an effort to inhibit the model from memorizing its input. Similarity is determined by a string or tree kernel so that for each meta-train task a corresponding meta-test task is created from examples deemed similar.\nBy selecting for similar examples we design the meta-test task to include examples with many of the same words as meta-train, but in novel combinations. As our training objective encourages gradient steps that are beneficial for both tasks we expect the model to be less likely to memorize large chunks which are unlikely to occur in both tasks, and therefore generalize more compositionally. This generalizes the approach from Wang et al. (2020a), by using the meta-test task to apply a bias not-strictly related to the test distribution: the design of the meta-test task allows us to design the bias which it applies. It is worth noting that other recent approaches to this problem have leveraged data augmentation to make the training distribution more representative of the test distribution (Andreas, 2020). We believe this line of work is orthogonal to ours as it does not focus on getting a model to generalize compositionally, but rather making the task simple enough that compositional generalization is not needed. Our method is model agnostic, and does not require prior knowledge of\nthe target distribution. We summarise our contributions as follows: • We approach the problem of compositional\ngeneralization with a meta-learning objective that tries to explicitly reduce input memorization using similarity-driven virtual tasks. • We perform experiments on two text-tosemantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional supervised learning. 1"
    }, {
      "heading" : "2 Methods",
      "text" : "We introduce the meta-learning augmented approach to supervised learning from Li et al. (2018); Wang et al. (2020a) that explicitly optimizes for outof-distribution generalization. Central to this approach is the generation of tasks for meta-learning by sub-sampling training data. We introduce three kinds of similarity metrics used to guide the construction of these tasks."
    }, {
      "heading" : "2.1 Problem Definition",
      "text" : "Compositional Generalization Lake and Baroni (eg. 2018); Kim and Linzen (eg. 2020) introduce datasets designed to assess compositional generalization. These datasets are created by generating synthetic data with different distributions for testing and training. The differences between the distributions are trivially resolved by a compositional strategy. At their core these tasks tend to assess three key components of compositional ability: systematicity, productivity, and primitive application. Systematicity allows for the use of known parts in novel combinations as in (a). Productivity enables generalization to longer sequences than those seen in training as in (b). Primitive application allows for a word only seen in isolation during training to be applied compositionally at test time as in (c).\n(a) The cat gives the dog a gift→ The dog gives the cat a gift\n(b) The cat gives the dog a gift→ The cat gives the dog a gift and the bird a gift\n(c) made→ The cat made the dog a gift 1Our implementations are available at https://\ngithub.com/berlino/tensor2struct-public.\nAlgorithm 1 MAML Training Algorithm Require: Original training set T Require: Learning rate α, Batch size N\n1: for step← 1 to T do 2: Sample a random batch from T as a virtual training set Bt 3: Initialize an empty generalization set Bg 4: for i← 1 to N do 5: Sample an example from p̃(· | Bt[i]) 6: Add it to Bg 7: end for 8: Construct a virtual task τ := (Bt,Bg) 9: Meta-train update:\nθ′ ← θ − α∇θLBt(θ) 10: Compute meta-test objective: Lτ (θ) = LBt(θ) + LBg(θ′) 11: Final Update: θ ← Update(θ,∇θLτ (θ)) 12: end for\nA compositional grammar like the one that generated the data would be able to resolve these three kinds of generalization easily, and therefore performance on these tasks is taken as an indication of a model’s compositional ability.\nConventional Supervised Learning The compositional generalization datasets we look at are semantic parsing tasks, mapping between natural language and a formal representation. A usual supervised learning objective for semantic parsing is to minimize the negative log-likelihood of the correct formal representation given a natural language input sentence, i.e. minimising\nLB(θ) = − 1\nN N∑ i=1 log pθ(y|x) (1)\nwhere N is the size of batch B, y is a formal representation and x is a natural language sentence. This approach assumes that the training and testing data are independent and identically distributed.\nTask Distributions Following from Wang et al. (2020a), we utilize a learning algorithm that can enable a parser to benefit from a distribution of virtual tasks, denoted by p(τ), where τ refers to an instance of a virtual compositional generalization task that has its own training and test examples."
    }, {
      "heading" : "2.2 MAML Training",
      "text" : "Once we have constructed our pairs of virtual tasks we need a training algorithm that encourages\ncompositional generalization in each. Like Wang et al. (2020a), we turn to optimization-based metalearning algorithms (Finn et al., 2017b; Li et al., 2018) and apply DG-MAML (Domain Generalization with Model-Agnostic Meta-Learning), a variant of MAML (Finn et al., 2017b). Intuitively, DGMAML encourages optimization on meta-training examples to have a positive effect on the meta-test examples as well.\nDuring each learning episode of MAML training we randomly sample a task τ which consists of a training batch Bt and a generalization batch Bg and conduct optimization in two steps, namely metatrain and meta-test.\nMeta-Train The meta-train task is sampled at random from the training data. The model performs one stochastic gradient descent step on this batch\nθ′ ← θ − α∇θLBt(θ) (2)\nwhere α is the meta-train learning rate.\nMeta-Test The fine-tuned parameters θ′ are evaluated on the accompanying generalization task, meta-test, by computing their loss on it denoted as LBg(θ′). The final objective for a task τ is then to jointly optimize the following:\nLτ (θ) = LBt(θ) + LBg(θ′) = LBt(θ) + LBg(θ − α∇θLβ(θ)) (3)\nThe objective now becomes to reduce the joint loss of both the meta-train and meta-test tasks. Optimizing in this way ensures that updates on metatrain are also beneficial to meta-test. The loss on meta-test acts as a constraint on the loss from metatrain. This is unlike traditional supervised learning (Lτ (θ) = LBt(θ) +LBg(θ)) where the loss on one batch does not constrain the loss on another.\nWith a random Bt and Bg, the joint loss function can be seen as a kind of generic regularizer, ensuring that update steps are not overly beneficial to meta-train alone. By constructing Bt and Bg in ways which we expect to be relevant to compositionality, we aim to allow the MAML algorithm to apply specialized regularization during training. Here we design meta-test to be similar to the metatrain task because we believe this highlights the systematicity generalization that is key to compositional ability: selecting for examples comprised of the same atoms but in different arrangements. In constraining each update step with respect to meta-train by performance on similar examples\nin meta-test we expect the model to dis-prefer a strategy that does not also work for meta-test like memorization of whole phrases or large sections of the input."
    }, {
      "heading" : "2.3 Similarity Metrics",
      "text" : "Ideally, the design of virtual tasks should reflect specific generalization cases for each dataset. However, in practice this requires some prior knowledge of the distribution to which the model will be expected to generalize, which is not always available. Instead we aim to naively structure the virtual tasks to resemble each other. To do this we use a number of similarity measures intended to help select examples which highlight the systematicity of natural language.\nInspired by kernel density estimation (Parzen, 1962), we define a relevance distribution for each example:\np̃(x′, y′|x, y) ∝ exp ( k([x, y], [x′, y′]/η ) (4)\nwhere k is the similarity function, [x, y] is a training example, η is a temperature that controls the sharpness of the distribution. Based on our extended interpretation of relevance, a high p̃ implies that [x, y] is systematically relevant to [x′, y′] - containing many of the same atoms but in a novel combination. We look at three similarity metrics to guide subsampling existing training data into meta-test tasks proportional to each example’s p̃.\nLevenshtein Distance First, we consider Levenshtein distance, a kind of edit distance widely used to measure the dissimilarity between strings. We compute the negative Levenshtein distance at the word-level between natural language sentences of two examples:\nk([x, y], [x′, y′]) = −1 ∗ LevDistance(x, x′) (5)\nwhere LevDistance returns the number of edit operations required to transform x into x′. See Table 1 for examples.\nAnother family of similarity metrics for discrete structures are convolution kernels (Haussler, 1999).\nString-Kernel Similarity We use the string subsequence kernel (Lodhi et al., 2002):\nk([x, y], [x′, y′]) = SSK(x, x′) (6)\nwhere SSK computes the number of common subsequences between natural language sentences at the word-level. See Table 1 for examples. 2\n2We use the normalized convolution kernels in this work, i.e., k′(x1, x2) = k(x1, x2)/ √ k(x1, x1)k(x2, x2)\nTree-Kernel Similarity In semantic parsing, the formal representation y usually has a known grammar which can be used to represent it as a tree structure. In light of this we use tree convolution kernels to compute similarity between examples: 3\nk([x, y], [x′, y′]) = TreeKernel(y, y′) (7)\nwhere the TreeKernel function is a convolution kernel (Collins and Duffy, 2001) applied to trees. Here we consider a particular case where y is represented as a dependency structure, as shown in Figure 1. We use the partial tree kernel (Moschitti, 2006) which is designed for application to dependency trees. For a given dependency tree partial tree kernels generate a series of all possible partial trees: any set of one or more connected nodes. Given two trees the kernel returns the number of partial trees they have in common, interpreted as a similarity score. Compared with string-based similarity, this kernel prefers sentences that share common syntactic sub-structures, some of which are not assigned high scores in string-based similarity metrics, as shown in Table 1.\nThough tree-structured formal representations are more informative in obtaining relevance, not all logical forms can be represented as tree structures. In SCAN (Lake and Baroni, 2018) y are action sequences without given grammars. As we will show in the experiments, string-based similarity metrics have a broader scope of applications but are less effective than tree kernels in cases where y can be tree-structured.\nSampling for Meta-Test Using our kernels we compute the relevance distribution in Eq 4 to construct virtual tasks for MAML training. We show the resulting procedure in Algorithm 1. In order to construct a virtual task τ , a meta-train batch is first sampled at random from the training data (line 2), then the accompanying meta-test batch is created by sampling examples similar to those in meta-train (line 5).\nWe use Lev-MAML, Str-MAML and Tree-MAML to denote the meta-training using Levenshtein distance, string-kernel and tree-kernel similarity, respectively.\n3Alternatively, we can use tree edit-distance (Zhang and Shasha, 1989)."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets and Splits",
      "text" : "We evaluate our methods on the following semantic parsing benchmarks that target compositional generalization.\nSCAN contains a set of natural language commands and their corresponding action sequences (Lake and Baroni, 2018). We use the Maximum Compound Divergence (MCD) splits (Keysers et al., 2020), which are created based on the principle of maximizing the divergence between the compound (e.g., patterns of 2 or more action sequences) distributions of the training and test tests. We apply Lev-MAML and Str-MAML to SCAN where similarity measures are applied to the natural language commands. Tree-MAML (which uses a tree kernel) is not applied as the action sequences do not have an underlying dependency tree-structure.\nCOGS contains a diverse set of natural language sentences paired with logical forms based on lambda calculus (Kim and Linzen, 2020). Compared with SCAN, it covers various systematic linguistic abstractions (e.g., passive to active) including examples of lexical and structural generalization, and thus better reflects the compositionality of natural language. In addition to the standard splits of Train/Dev/Test, COGS provides a generalization (Gen) set drawn from a different distribution that specifically assesses compositional generalization. We apply Lev-MAML, Str-MAML and Tree-MAML to COGS; Lev-MAML and StrMAML make use of the natural language sentences while Tree-MAML uses the dependency structures reconstructed from the logical forms."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "In general, our method is model-agnostic and can be coupled with any semantic parser to improve its compositional generalization. Additionally LevMAML, and Str-MAML are dataset agnostic provided the dataset has a natural language input. In this work, we apply our methods on two widely used sequence-to-sequences models. 4\nLSTM-based Seq2Seq has been the backbone of many neural semantic parsers (Dong and Lapata, 2016; Jia and Liang, 2016). It utilizes\n4Details of implementations and hyperparameters can be found in the Appendix.\nLSTM (Hochreiter and Schmidhuber, 1997) and attention (Bahdanau et al., 2014) under an encoderdecoder (Sutskever et al., 2014) framework.\nTransformer-based Seq2Seq also follows the encoder-decoder framework, but it uses Transformers (Vaswani et al., 2017) to replace the LSTM for encoding and decoding. It has proved successful in many NLP tasks e.g., machine translation. Recently, it has been adapted for semantic parsing (Wang et al., 2020b) with superior performance.\nWe try to see whether our MAML training can improve the compositional generalization of contemporary semantic parsers, compared with standard supervised learning. Moreover, we include a meta-baseline, referred to as Uni-MAML, that constructs meta-train and meta-test splits by uniformly sampling training examples. By comparing with this meta-baseline, we show the effect of similarity-driven construction of meta-learning splits. Note that we do not focus on making comparisons with other methods that feature specialized architectures for SCAN datasets (see Section 5), as these methods do not generalize well to more complex datasets (Furrer et al., 2020).\nGECA We additionally apply the good enough compositional augmentation (GECA) method laid out in Andreas (2020) to the SCAN MCD splits. Data augmentation of this kind tries to make the training distribution more representative of the test distribution. This approach is distinct from ours which focuses on the training objective, but the two can be combined with better overall performance as we will show. Specifically, we show the results of GECA applied to the MCD splits as well as GECA combined with our Lev-MAML variant. Note that we elect not to apply GECA to COGS, as the time and space complexity 5 of GECA proves very costly for COGS in our preliminary experiments."
    }, {
      "heading" : "3.3 Construction of Virtual Tasks",
      "text" : "The similarity-driven sampling distribution p̃ in Eq 4 requires computing the similarity between every pair of training examples, which can be very expensive depending on the size of of the dataset. As the sampling distributions are fixed during training, we compute and cache them beforehand. However, they take an excess of disk space to store as essentially we need to store an N ×N matrix where N\n5See the original paper for details.\nis the number of training examples. To allow efficient storage and sampling, we use the following approximation. First, we found that usually each example only has a small set of neighbours that are relevant to it. 6 Motivated by this observation, we only store the top 1000 relevant neighbours for each example sorted by similarity, and use it to construct the sampling distribution denoted as p̃top1000. To allow examples out of top 1000 being sampled, we use a linear interpolation between p̃top1000 and a uniform distribution. Specifically, we end up using the following sampling distribution:\np̃(x′, y′|x, y) = λ p̃top1000(x′, y′|x, y)+(1−λ) 1\nN\nwhere p̃top1000 assigns 0 probability to out-of top 1000 examples, N is the number of training examples, and λ is a hyperparameter for interpolation. In practice, we set λ to 0.5 in all experiments. To sample from this distribution, we first decide whether the sample is in the top 1000 by sampling from a Bernoulli distribution parameterized by λ. If it is, we use p̃top1000 to do the sampling; otherwise, we uniformly sample an example from the training set."
    }, {
      "heading" : "3.4 Development Set",
      "text" : "Many tasks that assess out-of-distribution (O.O.D.) generalization (e.g. COGS) do not have an O.O.D.\n6For example, in COGS, each example only retrieves 3.6% of the whole training set as its neighbours (i.e., have non-zero tree-kernel similarity) on average.\nDev set that is representative of the generalization distribution. This is desirable as a parser in principle should never have knowledge of the Gen set during training. In practice though the lack of an O.O.D. Dev set makes model selection extremely difficult and not reproducible. 7 In this work, we propose the following strategy to alleviate this issue: 1) we sample a small subset from the Gen set, denoted as ‘Gen Dev’ for tuning meta-learning hyperparmeters, 2) we use two disjoint sets of random seeds for development and testing respectively, i.e., retraining the selected models from scratch before applying them to the final test set. In this way, we make sure that our tuning is not exploiting the models resulting from specific random seeds: we do not perform random seed tuning. At no point are any of our models trained on the Gen Dev set."
    }, {
      "heading" : "3.5 Main Results",
      "text" : "On SCAN, as shown in Table 2, Lev-MAML substantially helps both base parsers achieve better performance across three different splits constructed according to the MCD principle. 8 Though our models do not utilize pre-training such as T5 (Raffel et al., 2019), our best model (Lev-MAML + LSTM) still outperforms T5 based models significantly in MCD1 and MCD2. We show that GECA is also effective for MCD splits (especially\n7We elaborate on this issue in the Appendix. 8Our base parsers also perform much better than previous\nmethods, likely due to the choice of hyperparameters.\nin MCD1). More importantly, augmenting GECA with Lev-MAML further boosts the performance substantially in MCD1 and MCD2, signifying that our MAML training is complementary to GECA to some degree.\nTable 3 shows our results on COGS. TreeMAML boosts the performance of both LSTM and Transformer base parsers by a large margin: 6.5% and 8.1% respectively in average accuracy. Moreover, Tree-MAML is consistently better than other MAML variants, showing the effectiveness of exploiting tree structures of formal representation to construct virtual tasks. 9"
    }, {
      "heading" : "4 Discussion",
      "text" : ""
    }, {
      "heading" : "4.1 SCAN Discussion",
      "text" : "The application of our string-similarity driven metalearning approaches to the SCAN dataset improved the performance of the LSTM baseline parser. Our results are reported on three splits of the dataset generated according to the maximum compound divergence (MCD) principle. We report results on the only MCD tasks for SCAN as these tasks explicitly focus on the systematicity of language. As such they assess a model’s ability to extract sufficiently atomic concepts from its input, such that it can still recognize those concepts in a new context (i.e. as part of a different compound). To succeed here a model must learn atoms from the training data and apply them compositionally at test time. The improvement in performance our approach achieves on this task suggests that it does disincentivise the model from memorizing large sections - or entire compounds - from its input.\nGECA applied to the SCAN MCD splits does improve performance of the baseline, however not to the same extent as when applied to other SCAN tasks in Andreas (2020). GECA’s improvement is comparable to our meta-learning method, despite the fact that our method does not leverage any data augmentation. This means that our method achieves high performance by generalizing robustly outside of its training distribution, rather than by making its training data more representative of the test distribution. The application of our LevMAML approach to GECA-augmented data results in further improvements in performance, suggest-\n9The improvement of all of our MAML variants applied to the Transformer are significant (p < 0.03) compared to the baseline, of our methods applied to LSTMs, Tree-MAML is significant (p < 0.01) compared to the baseline.\ning that these approaches aid the model in distinct yet complementary ways."
    }, {
      "heading" : "4.2 COGS Discussion",
      "text" : "All variants of our meta-learning approach improved both the LSTM and Transformer baseline parsers’ performance on the COGS dataset. The Tree-MAML method outperforms the Lev-MAML, Str-MAML, and Uni-MAML versions. The only difference between these methods is the similarity metric used, and so differences in performance must be driven by what each metric selects for. For further analysis of the metrics refer to the appendix.\nThe strong performance of the Uni-MAML variant highlights the usefulness of our approach generally in improving models’ generalization performance. Even without a specially designed metatest task this approach substantially improves on the baseline Transformer model. We see this as evidence that this kind of meta-augmented supervised learning acts as a robust regularizer particularly for tasks requiring out of distribution generalization.\nAlthough the Uni-MAML, Lev-MAML, and StrMAML versions perform similarly overall on the COGS dataset they may select for different generalization strategies. The COGS generalization set is comprised of 21 sub-tasks which can be used to better understand the ways in which a model is generalizing (refer to Table 4 for examples of subtask performance). Despite having very similar overall performance Uni-MAML and Str-MAML perform distinctly on individual COGS tasks - with their performance appearing to diverge on a number of of them. This would suggest that the design of the meta-test task may have a substantive impact on the kind of generalization strategy that emerges in the model. For further analysis of COGS sub-task performance see the appendix.\nOur approaches’ strong results on both of these datasets suggest that it aids compositional generalization generally. However it is worth nothing that both datasets shown here are synthetic, and although COGS endeavours to be similar to natural data, the application of our methods outside of synthetic datasets is important future work."
    }, {
      "heading" : "5 Related Work",
      "text" : "Compositional Generalization A large body of work on compositional generalization provide models with strong compositional bias, such as specialized neural architectures (Li et al., 2019; Russin\net al., 2019; Gordon et al., 2019), or grammar-based models that accommodate alignments between natural language utterances and programs (Shaw et al., 2020; Herzig and Berant, 2020). Another line of work utilizes data augmentation via fixed rules (Andreas, 2020) or a learned network (Akyürek et al., 2020) in an effort to transform the out-of-distribution compositional generalization task into an in-distribution one. Our work follows an orthogonal direction, injecting compositional bias using a specialized training algorithm. A related area of research looks at the emergence of compositional languages, often showing that languages which seem to lack natural-language like compositional structure may still be able to generalize to novel concepts (Kottur et al., 2017; Chaabouni et al., 2020). This may help to explain the ways in which models can generalize robustly on in-distribution data unseen during training while still struggling on tasks specifically targeting compositionality.\nMeta-Learning for NLP Meta-learning methods (Vinyals et al., 2016; Ravi and Larochelle, 2016; Finn et al., 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In this work, we extend the conventional MAML (Finn et al., 2017b) algorithm, which was initially proposed for few-shot learning, as a tool to inject inductive bias, inspired by Li et al. (2018); Wang et al. (2020a). For compositional generalization, Lake (2019) proposes a meta-learning procedure to train a memory-augmented neural model. However, its meta-learning algorithm is specialized for the SCAN dataset (Lake and Baroni, 2018) and not suitable to more realistic datasets."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Our work highlights the importance of training objectives that select for robust generalization strategies. The meta-learning augmented approach to supervised learning used here allows for the specification of different constraints on learning through the design of the meta-tasks. Our similarity-driven task design improved on baseline performance on two different compositional generalization datasets, by inhibiting the model’s ability to memorize large sections of its input. Importantly though the overall approach used here is model agnostic, with portions of it (Str-MAML, Lev-MAML, and Uni-MAML) proving dataset agnostic as well requiring only that the input be a natural language sentence. Our methods are simple to implement compared with other approaches to improving compositional generalization, and we look forward to their use in combination with other techniques to further improve models’ compositional ability.\nAcknowledgements\nThis work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh, School of Informatics and School of Philosophy, Psychology & Language Sciences. We also acknowledge the financial support of the European Research Council (Titov, ERC StG BroadSem 678254) and the Dutch National Science Foundation (Titov, NWO VIDI 639.022.518)."
    }, {
      "heading" : "A Experiments",
      "text" : "A.1 Details of Base Parsers\nWe implemented all models with Pytorch (Paszke et al., 2019). For the LSTM parsers, we use a twolayer encoder and one-layer decoder with attention (Bahdanau et al., 2014) and input-feeding (Luong et al., 2015). We only test bidirectional LSTM encoders, as unidirectional LSTM models do not perform very well in our preliminary experiments. For Transformer parsers, we use 2 encoder and decoder layers, 4 attention heads, and a feed-forward dimension of 1024. The hidden size for both LSTM and Transformer models are 256. The hyparameters of base parsers are mostly borrowed from related work and not tuned, as the primary goal of this work is the MAML training algorithm. To experiment with a wide variety of possible Seq2Seq models, we also try a Transformer encoder + LSTM decoder and find that this variant actually performs slightly better than both vanilla Transformer and LSTM models. Further exploration of this combination in pursuit of a better neural architecture for compositional generalization might be interesting for future work.\nA.2 Model Selection Protocol\nIn our preliminary experiments on COGS, we find almost all the Seq2Seq models achieve > 99% in accuracy on the original Dev set. However, their performance on the Gen set diverge dramatically, ranging from 10% to 70%. The lack of an informative Dev set makes model selection extremely difficult and difficult to reproduce. This issue might also be one of the factors that results in the large variance of performance reported in previous work. Meanwhile, we found that some random seeds 10 yield consistently better performance than others across different conditions. For example, among\n10Random seeds control the initialization of parameters and the order of training batches.\nthe ten random seeds used for Lev-MAML + Transformer on COGS, the best performing seed obtains 73% whereas the lowest performing seed obtains 54%. Thus, it is important to compare different models using the same set of random seeds, and not to tune the random seeds in any model. To alleviate these two concerns, we choose the protocol that is mentioned in the main paper. This protocol helps to make the results reported in our paper reproducible.\nA.3 Details of Training and Evaluation Following Kim and Linzen (2020), we train all models from scratch using randomly initialized embeddings. For SCAN, models are trained for 1,000 steps with batch size 128. We choose model checkpoints based on their performance on the Dev set. For COGS, models are trained for 6,000 steps with batch size of 128. We choose the meta-train learning rate α in Equation 2, temperature η in Equation 4 based on the performance on the Gen Dev set. Finally we use the chosen α, η to train models with new random seeds, and only the last checkpoints (at step 6,000) are used for evaluation on the Test and Gen set.\nA.4 Other Splits of SCAN The SCAN dataset contains many splits, such as Add-Jump, Around Right, and Length split, each assessing a particular case of compositional generalization. We think that MCD splits are more representative of compositional generalization due to the nature of the principle of maximum compound divergence. Moreover, it is more challenging than other splits (except the Length split) according to Furrer et al. (2020). That GECA, which obtains 82% in accuracy on JUMP and Around Right splits, only obtains < 52% in accuracy on MCD splits in our experiments confirms that MCD splits are more challenging.\nA.5 Kernel Analysis The primary difference between the tree-kernel and string-kernel methods is in the diversity of the examples they select for the meta-test task. The tree kernel selects a broader range of lengths, often including atomic examples, a single word in length, matching a word in the original example from metatrain (see table 5). By design the partial tree kernel will always assign a non-zero value to an example that is an atom contained in the original sentence. We believe the diversity of the sentences selected\nby the tree kernel accounts for the superior performance of Tree-MAML compared with the other MAML conditions. The selection of a variety of lengths for meta-test constrains model updates on the meta-train task such that they must also accommodate the diverse and often atomic examples selected for meta-test. This constraint would seem to better inhibit memorizing large spans of the input unlikely to be present in meta-test.\nA.6 Meta-Test Examples\nIn Table 6, we show top scoring examples retrieved by the similarity metrics for two sentences. We found that in some cases (e.g., the right part of Table 6), the tree-kernel can retrieve examples that diverge in length but are still semantically relevant. In contrast, string-based similarity metrics, especially LevDistance, tends to choose examples with similar lengths.\nA.7 COGS Subtask Analysis\nWe notice distinct performance for different conditions on the different subtasks from the COGS dataset. In Figure 2 we show the performance of the Uni-MAML and Str-MAML conditions compared with the mean of those conditions. Where the bars are equal to zero the models’ performance on that task is roughly equal.\nFull task names for figure 2: (1) prim→subj proper, (2) active→passive, (3) only seen as unacc subj→ unerg subj, (4) subj→obj proper, (5) only seen as unacc subj→ obj omitted transitive subj, (6) pp recursion, (7) cp recursion, (8) obj pp→subj pp, (9) obj→subj common, (10) do dative→pp dative, (11) passive→active,\n(12) only seen as transitive subj→ unacc subj, (13) obj omitted transitive→transitive, (14) subj→obj common, (15) prim→obj proper, (16) obj→subj proper, (17) pp dative→do dative, (18) unacc→transitive, (19) prim→subj common, (20) prim→obj common, (21) prim→inf arg."
    } ],
    "references" : [ {
      "title" : "Learning to recombine and resam",
      "author" : [ "Ekin Akyürek", "Afra Feyza Akyürek", "Jacob Andreas" ],
      "venue" : null,
      "citeRegEx" : "Akyürek et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Akyürek et al\\.",
      "year" : 2020
    }, {
      "title" : "Good-enough compositional data augmentation",
      "author" : [ "Jacob Andreas." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556–7566, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Andreas.,? 2020",
      "shortCiteRegEx" : "Andreas.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
      "author" : [ "Yonatan Belinkov", "Lluís Màrquez", "Hassan Sajjad", "Nadir Durrani", "Fahim Dalvi", "James Glass." ],
      "venue" : "arXiv:1801.07772 [cs]. ArXiv:",
      "citeRegEx" : "Belinkov et al\\.,? 2018",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep RNNs Encode Soft Hierarchical Syntax",
      "author" : [ "Terra Blevins", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "arXiv:1805.04218 [cs]. ArXiv: 1805.04218.",
      "citeRegEx" : "Blevins et al\\.,? 2018",
      "shortCiteRegEx" : "Blevins et al\\.",
      "year" : 2018
    }, {
      "title" : "Formal semantics an introduction",
      "author" : [ "Ronnie Cann." ],
      "venue" : "Cambridge University Press, Cambridge [etc. OCLC: 1120437841.",
      "citeRegEx" : "Cann.,? 1993",
      "shortCiteRegEx" : "Cann.",
      "year" : 1993
    }, {
      "title" : "Compositionality and generalization in emergent languages",
      "author" : [ "Rahma Chaabouni", "Eugene Kharitonov", "Diane Bouchacourt", "Emmanuel Dupoux", "Marco Baroni." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Chaabouni et al\\.,? 2020",
      "shortCiteRegEx" : "Chaabouni et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspects of the theory of syntax, 50th anniversary edition edition",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Number no. 11 in Massachusetts Institute of Technology. Research Laboratory of Electronics. Special technical report. The MIT Press, Cambridge, Massachusetts.",
      "citeRegEx" : "Chomsky.,? 1965",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1965
    }, {
      "title" : "Convolution kernels for natural language",
      "author" : [ "Michael Collins", "Nigel Duffy." ],
      "venue" : "Advances in neural information processing systems, pages 625–632.",
      "citeRegEx" : "Collins and Duffy.,? 2001",
      "shortCiteRegEx" : "Collins and Duffy.",
      "year" : 2001
    }, {
      "title" : "Underspecification Presents Challenges for Credibility",
      "author" : [ "tin Seneviratne", "Shannon Sequeira", "Harini Suresh", "Victor Veitch", "Max Vladymyrov", "Xuezhi Wang", "Kellie Webster", "Steve Yadlowsky", "Taedong Yun", "Xiaohua Zhai", "D. Sculley" ],
      "venue" : null,
      "citeRegEx" : "Seneviratne et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Seneviratne et al\\.",
      "year" : 2020
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33–43, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "arXiv:1703.03400 [cs]. ArXiv: 1703.03400.",
      "citeRegEx" : "Finn et al\\.,? 2017a",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org.",
      "citeRegEx" : "Finn et al\\.,? 2017b",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Connectionism and cognitive architecture: A critical analysis",
      "author" : [ "Jerry A. Fodor", "Zenon W. Pylyshyn." ],
      "venue" : "Cognition, 28(1-2):3–71.",
      "citeRegEx" : "Fodor and Pylyshyn.,? 1988",
      "shortCiteRegEx" : "Fodor and Pylyshyn.",
      "year" : 1988
    }, {
      "title" : "Compositional generalization in semantic parsing: Pre-training vs",
      "author" : [ "Daniel Furrer", "Marc van Zee", "Nathan Scales", "Nathanael Schärli." ],
      "venue" : "specialized architectures. arXiv preprint arXiv:2007.08970.",
      "citeRegEx" : "Furrer et al\\.,? 2020",
      "shortCiteRegEx" : "Furrer et al\\.",
      "year" : 2020
    }, {
      "title" : "Permutation equivariant models for compositional generalization in language",
      "author" : [ "Jonathan Gordon", "David Lopez-Paz", "Marco Baroni", "Diane Bouchacourt." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gordon et al\\.,? 2019",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding human intelligence through human limitations",
      "author" : [ "Thomas L Griffiths." ],
      "venue" : "Trends in Cognitive Sciences.",
      "citeRegEx" : "Griffiths.,? 2020",
      "shortCiteRegEx" : "Griffiths.",
      "year" : 2020
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor O.K. Li", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631,",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolution kernels on discrete structures",
      "author" : [ "David Haussler." ],
      "venue" : "Technical report, Technical report, Department of Computer Science, University of California . . . .",
      "citeRegEx" : "Haussler.,? 1999",
      "shortCiteRegEx" : "Haussler.",
      "year" : 1999
    }, {
      "title" : "Spanbased semantic parsing for compositional generalization",
      "author" : [ "Jonathan Herzig", "Jonathan Berant." ],
      "venue" : "arXiv preprint arXiv:2009.06040.",
      "citeRegEx" : "Herzig and Berant.,? 2020",
      "shortCiteRegEx" : "Herzig and Berant.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "The compositionality of neural networks: integrating symbolism and connectionism",
      "author" : [ "Dieuwke Hupkes", "Verna Dankers", "Mathijs Mul", "Elia Bruni." ],
      "venue" : "arXiv:1908.08351 [cs, stat]. ArXiv: 1908.08351.",
      "citeRegEx" : "Hupkes et al\\.,? 2019",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualisation and’diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure",
      "author" : [ "Dieuwke Hupkes", "Sara Veldhoen", "Willem Zuidema." ],
      "venue" : "Journal of Artificial Intelligence Research, 61:907–926.",
      "citeRegEx" : "Hupkes et al\\.,? 2018",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2018
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12–22, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "Characterizing Structural Regularities of Labeled Data in Overparameterized Models",
      "author" : [ "Ziheng Jiang", "Chiyuan Zhang", "Kunal Talwar", "Michael C. Mozer." ],
      "venue" : "arXiv:2002.03206 [cs, stat]. ArXiv: 2002.03206.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
      "author" : [ "Najoung Kim", "Tal Linzen." ],
      "venue" : "arXiv:2010.05465 [cs]. ArXiv: 2010.05465.",
      "citeRegEx" : "Kim and Linzen.,? 2020",
      "shortCiteRegEx" : "Kim and Linzen.",
      "year" : 2020
    }, {
      "title" : "Natural language does not emerge’naturally’in multi-agent dialog",
      "author" : [ "Satwik Kottur", "José MF Moura", "Stefan Lee", "Dhruv Batra." ],
      "venue" : "arXiv preprint arXiv:1706.08502.",
      "citeRegEx" : "Kottur et al\\.,? 2017",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2017
    }, {
      "title" : "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
      "author" : [ "Brenden Lake", "Marco Baroni." ],
      "venue" : "International Conference on Machine Learning, pages 2873–2882. PMLR.",
      "citeRegEx" : "Lake and Baroni.,? 2018",
      "shortCiteRegEx" : "Lake and Baroni.",
      "year" : 2018
    }, {
      "title" : "Compositional generalization through meta sequence-to-sequence learning",
      "author" : [ "Brenden M Lake." ],
      "venue" : "arXiv preprint arXiv:1906.05381.",
      "citeRegEx" : "Lake.,? 2019",
      "shortCiteRegEx" : "Lake.",
      "year" : 2019
    }, {
      "title" : "Learning to generalize: Metalearning for domain generalization",
      "author" : [ "Da Li", "Yongxin Yang", "Yi-Zhe Song", "Timothy M Hospedales." ],
      "venue" : "ThirtySecond AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Compositional generalization for primitive substitutions",
      "author" : [ "Yuanpeng Li", "Liang Zhao", "Jianyu Wang", "Joel Hestness." ],
      "venue" : "arXiv preprint arXiv:1910.02612.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Text classification using string kernels",
      "author" : [ "Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins." ],
      "venue" : "Journal of Machine Learning Research, 2(Feb):419–444.",
      "citeRegEx" : "Lodhi et al\\.,? 2002",
      "shortCiteRegEx" : "Lodhi et al\\.",
      "year" : 2002
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient convolution kernels for dependency and constituent syntactic trees",
      "author" : [ "Alessandro Moschitti." ],
      "venue" : "European Conference on Machine Learning, pages 318–329. Springer.",
      "citeRegEx" : "Moschitti.,? 2006",
      "shortCiteRegEx" : "Moschitti.",
      "year" : 2006
    }, {
      "title" : "Model-agnostic meta-learning for relation classification with limited supervision",
      "author" : [ "Abiola Obamuyide", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5873–5879, Florence,",
      "citeRegEx" : "Obamuyide and Vlachos.,? 2019",
      "shortCiteRegEx" : "Obamuyide and Vlachos.",
      "year" : 2019
    }, {
      "title" : "On estimation of a probability density function and mode",
      "author" : [ "Emanuel Parzen." ],
      "venue" : "The annals of mathematical statistics, 33(3):1065–1076.",
      "citeRegEx" : "Parzen.,? 1962",
      "shortCiteRegEx" : "Parzen.",
      "year" : 1962
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Optimization as a model for few-shot learning",
      "author" : [ "Sachin Ravi", "Hugo Larochelle" ],
      "venue" : null,
      "citeRegEx" : "Ravi and Larochelle.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ravi and Larochelle.",
      "year" : 2016
    }, {
      "title" : "Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708",
      "author" : [ "Jake Russin", "Jason Jo", "Randall C O’Reilly", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Russin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Russin et al\\.",
      "year" : 2019
    }, {
      "title" : "Compositional generalization and natural language variation: Can a semantic parsing approach handle both? arXiv preprint arXiv:2010.12725",
      "author" : [ "Peter Shaw", "Ming-Wei Chang", "Panupong Pasupat", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Shaw et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Daan Wierstra" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Meta-learning for domain generalization in semantic parsing",
      "author" : [ "Bailin Wang", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:2010.11988.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "RATSQL: Relation-aware schema encoding and linking for text-to-SQL parsers",
      "author" : [ "Bailin Wang", "Richard Shin", "Xiaodong Liu", "Oleksandr Polozov", "Matthew Richardson." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple fast algorithms for the editing distance between trees and related problems",
      "author" : [ "Kaizhong Zhang", "Dennis Shasha." ],
      "venue" : "SIAM journal on computing, 18(6):1245–1262.",
      "citeRegEx" : "Zhang and Shasha.,? 1989",
      "shortCiteRegEx" : "Zhang and Shasha.",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Compositionality is the property of human language that allows for the meaning of a sentence to be constructed from the meaning of its parts and the way in which they are combined (Cann, 1993).",
      "startOffset" : 180,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "In practice this allows us to produce and interpret a functionally limitless number of sentences given finite means (Chomsky, 1965).",
      "startOffset" : 116,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Prior work asserts that there exist fundamental differences between cognitive and connectionist architectures that makes compositional generalization by the latter unlikely (Fodor and Pylyshyn, 1988).",
      "startOffset" : 173,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "Work looking at the latent representations learned by deep machine translation systems show how these models seem to extract constituency and syntactic class information from data (Blevins et al., 2018; Belinkov et al., 2018).",
      "startOffset" : 180,
      "endOffset" : 225
    }, {
      "referenceID" : 3,
      "context" : "Work looking at the latent representations learned by deep machine translation systems show how these models seem to extract constituency and syntactic class information from data (Blevins et al., 2018; Belinkov et al., 2018).",
      "startOffset" : 180,
      "endOffset" : 225
    }, {
      "referenceID" : 10,
      "context" : "These results, and the more general fact that neural models perform a variety of NLP tasks with high fidelity (eg. Vaswani et al., 2017; Dong and Lapata, 2016), suggest these models have some sensitivity to syntactic structure and by extension may be able to learn to generalize compositionally.",
      "startOffset" : 110,
      "endOffset" : 159
    }, {
      "referenceID" : 25,
      "context" : "Recently there have been a number of datasets designed to more formally assess connectionist models’ aptitude for compositional generalization (Kim and Linzen, 2020; Lake and Baroni, 2018; Hupkes et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 209
    }, {
      "referenceID" : 27,
      "context" : "Recently there have been a number of datasets designed to more formally assess connectionist models’ aptitude for compositional generalization (Kim and Linzen, 2020; Lake and Baroni, 2018; Hupkes et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 209
    }, {
      "referenceID" : 21,
      "context" : "Recently there have been a number of datasets designed to more formally assess connectionist models’ aptitude for compositional generalization (Kim and Linzen, 2020; Lake and Baroni, 2018; Hupkes et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 209
    }, {
      "referenceID" : 27,
      "context" : "A variety of neural network architectures have shown mixed performance across these tasks, failing to show conclusively that connectionist models are reliably capable of generalizing compositionally (Keysers et al., 2020; Lake and Baroni, 2018).",
      "startOffset" : 199,
      "endOffset" : 244
    }, {
      "referenceID" : 24,
      "context" : "Natural language requires a mixture of memorization and generalization (Jiang et al., 2020), memorizing exceptions and atomic concepts with which to generalize.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "3323 looking at compositional generalization has suggested that models may memorize large spans of sentences multiple words in length (Hupkes et al., 2019; Keysers et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "It is worth noting that other recent approaches to this problem have leveraged data augmentation to make the training distribution more representative of the test distribution (Andreas, 2020).",
      "startOffset" : 176,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "(2020a), we turn to optimization-based metalearning algorithms (Finn et al., 2017b; Li et al., 2018) and apply DG-MAML (Domain Generalization with Model-Agnostic Meta-Learning), a variant of MAML (Finn et al.",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "(2020a), we turn to optimization-based metalearning algorithms (Finn et al., 2017b; Li et al., 2018) and apply DG-MAML (Domain Generalization with Model-Agnostic Meta-Learning), a variant of MAML (Finn et al.",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : ", 2018) and apply DG-MAML (Domain Generalization with Model-Agnostic Meta-Learning), a variant of MAML (Finn et al., 2017b).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 35,
      "context" : "Inspired by kernel density estimation (Parzen, 1962), we define a relevance distribution for each example:",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "Another family of similarity metrics for discrete structures are convolution kernels (Haussler, 1999).",
      "startOffset" : 85,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "String-Kernel Similarity We use the string subsequence kernel (Lodhi et al., 2002):",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "where the TreeKernel function is a convolution kernel (Collins and Duffy, 2001) applied to trees.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 33,
      "context" : "We use the partial tree kernel (Moschitti, 2006) which is designed for application to dependency trees.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "In SCAN (Lake and Baroni, 2018) y are action sequences without given grammars.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 46,
      "context" : "Alternatively, we can use tree edit-distance (Zhang and Shasha, 1989).",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "SCAN contains a set of natural language commands and their corresponding action sequences (Lake and Baroni, 2018).",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "COGS contains a diverse set of natural language sentences paired with logical forms based on lambda calculus (Kim and Linzen, 2020).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "LSTM-based Seq2Seq has been the backbone of many neural semantic parsers (Dong and Lapata, 2016; Jia and Liang, 2016).",
      "startOffset" : 73,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "LSTM-based Seq2Seq has been the backbone of many neural semantic parsers (Dong and Lapata, 2016; Jia and Liang, 2016).",
      "startOffset" : 73,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "LSTM (Hochreiter and Schmidhuber, 1997) and attention (Bahdanau et al.",
      "startOffset" : 5,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "LSTM (Hochreiter and Schmidhuber, 1997) and attention (Bahdanau et al., 2014) under an encoderdecoder (Sutskever et al.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : ", 2014) under an encoderdecoder (Sutskever et al., 2014) framework.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : "Transformer-based Seq2Seq also follows the encoder-decoder framework, but it uses Transformers (Vaswani et al., 2017) to replace the LSTM for encoding and decoding.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 45,
      "context" : "Recently, it has been adapted for semantic parsing (Wang et al., 2020b) with superior performance.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "isons with other methods that feature specialized architectures for SCAN datasets (see Section 5), as these methods do not generalize well to more complex datasets (Furrer et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 37,
      "context" : "8 Though our models do not utilize pre-training such as T5 (Raffel et al., 2019), our best model (Lev-MAML + LSTM) still outperforms T5 based models significantly in MCD1 and MCD2.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Another line of work utilizes data augmentation via fixed rules (Andreas, 2020) or a learned network (Akyürek et al.",
      "startOffset" : 64,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Another line of work utilizes data augmentation via fixed rules (Andreas, 2020) or a learned network (Akyürek et al., 2020) in an effort to transform the out-of-distribution compositional generalization task into an in-distribution one.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "A related area of research looks at the emergence of compositional languages, often showing that languages which seem to lack natural-language like compositional structure may still be able to generalize to novel concepts (Kottur et al., 2017; Chaabouni et al., 2020).",
      "startOffset" : 222,
      "endOffset" : 267
    }, {
      "referenceID" : 6,
      "context" : "A related area of research looks at the emergence of compositional languages, often showing that languages which seem to lack natural-language like compositional structure may still be able to generalize to novel concepts (Kottur et al., 2017; Chaabouni et al., 2020).",
      "startOffset" : 222,
      "endOffset" : 267
    }, {
      "referenceID" : 43,
      "context" : "Meta-Learning for NLP Meta-learning methods (Vinyals et al., 2016; Ravi and Larochelle, 2016; Finn et al., 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al.",
      "startOffset" : 44,
      "endOffset" : 113
    }, {
      "referenceID" : 38,
      "context" : "Meta-Learning for NLP Meta-learning methods (Vinyals et al., 2016; Ravi and Larochelle, 2016; Finn et al., 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al.",
      "startOffset" : 44,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "Meta-Learning for NLP Meta-learning methods (Vinyals et al., 2016; Ravi and Larochelle, 2016; Finn et al., 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al.",
      "startOffset" : 44,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : ", 2017b) that are widely used for few-shot learning, have been adapted for NLP applications like machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019).",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : ", 2018) and relation classification (Obamuyide and Vlachos, 2019).",
      "startOffset" : 36,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "In this work, we extend the conventional MAML (Finn et al., 2017b) algorithm, which was initially proposed for few-shot learning, as a tool to inject inductive bias, inspired by Li et al.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "However, its meta-learning algorithm is specialized for the SCAN dataset (Lake and Baroni, 2018) and not suitable to more realistic datasets.",
      "startOffset" : 73,
      "endOffset" : 96
    } ],
    "year" : 2021,
    "abstractText" : "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similaritydriven meta-learning can improve generalization performance.",
    "creator" : "LaTeX with hyperref"
  }
}