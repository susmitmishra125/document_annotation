{
  "name" : "2021.acl-long.271.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GTM: A Generative Triple-Wise Model for Conversational Question Generation",
    "authors" : [ "Lei Shen", "Fandong Meng", "Jinchao Zhang", "Yang Feng", "Jie Zhou" ],
    "emails" : [ "shenlei17z@ict.ac.cn,", "fandongmeng@tencent.com", "dayerzhang@tencent.com", "fengyang@ict.ac.cn,", "withtomzhou@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3495–3506\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3495"
    }, {
      "heading" : "1 Introduction",
      "text" : "Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018).\nJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗Yang Feng is the corresponding author.\nCQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019).\nAt first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on the observation that an answer has strong relevance to its question and post, Wang et al. (2019) tried to integrate answer into the question generation process. They applied a reinforcement learning framework that firstly generated a question given the post, and then used a pre-trained matching model to estimate the relevance score (reward) between\nanswer and generated question. This method separates a post-question-answer (PQA) triple into post-question (PQ) and question-answer (QA) pairs rather than considering the triple as a whole and modeling the overall coherence. Furthermore, the training process of the matching model only utilizes one-to-one relation of each QA pair and neglects the one-to-many mapping feature.\nAn open-domain PQA often takes place under a background that can be inferred from all utterances in the triple and help enhance the overall coherence. When it comes to the semantic relationship in each triple, the content of a specific question is under the control of its post and answer (Lee et al., 2020). Meanwhile, either a post or an answer could correspond to several meaningful questions. As shown in Table 1, the triple is about a person’s eating activity (the background of the entire conversation). There are one-to-many mappings in both PQ and QA pairs that construct different meaningful combinations, such as P-Q1.1-A1, P-Q1.2-A1, P-Q2.1-A2 and P-Q2.2-A2. An answer connects tightly to both its post and question, and in turn helps decide the expression of a question.\nOn these grounds, we propose a generative triplewise model (GTM) for CQG. Specifically, we firstly introduce a triple-level variable to capture the shared background among PQA. Then, two separate variables conditioned on the triple-level variable are used to represent the latent space for question and answer, and the question variable is also dependent on the answer one. During training, the latent variables are constrained to reconstruct both the original question and answer according to the hierarchical structure we define, making sure the triple-wise relationship flows through the latent variables without any loss. For the question generation process, we sample the triple-level and answer variable given a post, then obtain the question variable conditioned on them, and finally generate a question based on the post, triple-level and question variables. Experimental results on a largescale CQG dataset show that GTM can generate more fluent, coherent, and intriguing questions for open-domain conversations.\nThe main contribution is threefold:\n• To generate coherent and informative questions in the CQG task, we propose a generative triple-wise model that models the semantic relationship of a triple in three levels: PQA, PQ, and QA.\n• Our variational hierarchical structure can not only utilize the “future” information (answer), but also capture one-to-many mappings in PQ and QA, which matches the open-domain scenario well.\n• Experimental results on a large-scale CQG corpus show that our method significantly outperforms the state-of-the-art baselines in both automatic and human evaluations."
    }, {
      "heading" : "2 Proposed Model",
      "text" : "Given a post as the input, the goal of CQG is to generate the corresponding question. Following the work of Zhao et al. (2017) and Wang et al. (2019), we leverage the question type qt to control the generated question, and take advantage of the answer information a to improve coherence. In training set, each conversation is represented as {p, q, qt,a}, consisting of post p = {pi}|p|i=1, question q = {qi}|q|i=1 with its question type qt, and answer a = {ai}|a|i=1."
    }, {
      "heading" : "2.1 Overview",
      "text" : "The graphical model of GTM for training process is shown in Figure 1. θ, ϕ, and φ are used to denote parameters of generation, prior, and recognition network, respectively. We integrate answer generation to assist question generation with hierarchical latent variables. Firstly, a triple-level variable zt is imported to capture the shared background and\nis inferred from PQA utterances. Then answer latent variable za and question latent variable zq are sampled from Gaussian distributions conditioned on both post and zt. To ensure that the question is controlled by answer, zq is also dependent on za."
    }, {
      "heading" : "2.2 Input Representation",
      "text" : "We use a bidirectional GRU (Cho et al., 2014) as encoder to capture the semantic representation of each utterance. Take post p as an example. Each word in p is firstly encoded into its embedding vector. The GRU then computes forward hidden states { −→ h i}|p|i=1 and backward hidden states { ←− h i}|p|i=1:\n−→ h i = −−→ GRU(epi , −→ h i−1), ←− h i = ←−− GRU(epi , ←− h i+1),\nwhere epi is employed to represent the embedding vector of word pi. We finally get the post representation by concatenating the last hidden states of two directions hencp = [ −→ h |p|; ←− h 1]. Similarly, we can obtain representations of question q and answer a, denoted as hencq and h enc a , respectively.\nThe question type qt is represented by a realvalued, low dimensional vector vqt which is updated during training and is regarded as a linguistic feature that benefits the training of latent variables (Zhao et al., 2017). We use the actual question type qt during training to provide the information of interrogative words that is the most important feature to distinguish question types."
    }, {
      "heading" : "2.3 Triple-level Latent Variable",
      "text" : "To capture the shared background of entire triple, we introduce a triple-level latent variable zt that\nis inferred from PQA utterances and is in turn responsible for generating the whole triple. Inspired by Park et al. (2018), we use a standard Gaussian distribution as the prior distribution of zt:\npϕ(z t) = N (z|0, I),\nwhere I represents the identity matrix. For the inference of zt in training set, we consider three utterance representations hencp , h enc q and henca as a sequence, and use a bidirectional GRU to take individual representation as the input of each time step. The triple representation ht is obtained by concatenating the last hidden states of both directions. Then, zt is sampled from:\nqφ(z t|p, q,a) = N (z|µt,σtI), µt = MLPtφ(h t), σt = softplus(MLPtφ(h t)),\nwhere MLP(·) is a feed-forward network, and softplus function is a smooth approximation to ReLU and can be used to ensure positiveness (Park et al., 2018; Serban et al., 2017)."
    }, {
      "heading" : "2.4 One-to-many Mappings",
      "text" : "After obtaining zt, we use a GRU f to get a vector hctxp for connecting p and q/a. h ctx p is then transformed to hctxq and h ctx a that are used in prior and recognition networks for zq and za:\nhctxp = f(z t,hencp ), hctxq = MLP tr1 θ (h ctx p ), hctxa = MLP tr2 θ (h ctx p ).\nTo model one-to-many mappings in PQ and QA pairs under the control of zt, we design two utterance-level variables, zq and za, to represent latent spaces of question and answer. We define the prior and posterior distributions of za as follows:\npϕ(z a|p, zt) = N (z|µa,σaI), qφ(z a|p, zt,a) = N (z|µ′a,σ ′ aI),\nwhere µa, σa, µ ′ a, and σ ′ a, the parameters of two Gaussian distributions, are calculated as:\nµa = MLPaϕ([h ctx a ; z t]), σa = softplus(MLPaϕ([h ctx a ; z t])),\nµ ′ a = MLP a φ([h ctx a ; z t;henca ]), σ ′ a = softplus(MLP a φ([h ctx a ; z t;henca ])).\nTo make sure the content of question is also decided by answer and improve their relatedness, we import za into zq space. The prior and posterior distributions of zq are computed as follows:\npϕ(z q|p, zt, za) = N (z|µq,σqI), qφ(z q|p, zt, q, qt, za) = N (z|µ′q,σ ′ qI),\nwhere µq, σq, µ ′ q, and σ ′ q are calculated as:\nµq = MLPqϕ([h ctx q ; z t; za]), σq = softplus(MLPqϕ([h ctx q ; z t; za])), µ ′ q = MLP q φ([h ctx q ; z t;hencq ;vqt; z a]), σ ′ q = softplus(MLP q φ([h ctx q ; z t;hencq ;vqt, z a]))."
    }, {
      "heading" : "2.5 Question Generation Network",
      "text" : "Following the work of Zhao et al. (2017) and Wang et al. (2019), a question type prediction network MLPqt is introduced to approximate pθ(qt|zq, zt,p) in training process and produces question type qt′ during inference.\nAs shown in Figure 2, there are two decoders in our model, one is for answer generation that is an auxiliary task and only exists in the training process, and the other is for desired question generation. The question decoder employs a variant of GRU that takes the concatenation result of zq, zt, hctxq , and qt as initial state s0, i.e., s0 = [z\nq; zt,hctxq , qt]. For each time step j, it calculates the context vector cj following Bahdanau\net al. (2015), and computes the probability distribution pθ(q|zq, zt,p, qt) over all words in the vocabulary:\nsj = GRU(ej−1, sj−1, cj)\ns̃j = MLP([ej−1; cj ; sj ]), pθ(qj |q<j , zq, zt,p, qt) = softmax(Wos̃j),\nwhere ej−1 represents the embedding vector of the (j − 1)-th question word. Similarly, the answer decoder receives the concatenation result of za, zt, and hctxa as initial state to approximate the probability pθ(a|za, zt,p)."
    }, {
      "heading" : "2.6 Training and Inference",
      "text" : "Importantly, our model GTM is trained to maximize the log-likelihood of the joint probability p(p, q,a, qt):\nlogp(p, q,a, qt) = log ∫ zt p(p, q,a, qt, zt).\nHowever, the optimization function is not directly tractable. Inspired by Serban et al. (2017) and Park et al. (2018), we convert it to the following objective that is based on the evidence lower bound and needs to be maximized in training process:\nLGTM = −KL(qφ(zt|p, q,a)||pϕ(zt)) −KL(qφ(za|p, zt,a)||pϕ(za|p, zt)) −KL(qφ(zq|p, zt, q, qt, za)||pϕ(zq|p, zt, za)) + Eza,zt∼qφ [log pθ(a|z a, zt,p)]\n+ Ezq ,zt∼qφ [log pθ(q|z q, zt,p, qt)] + Ezq ,zt∼qφ [log pθ(qt|z q, zt,p)].\nThe objective consists of two parts: the variational lower bound (the first five lines) and question type prediction accuracy (the last line). Meanwhile, the variational lower bound includes the reconstruction terms and KL divergence terms based on three hierarchical latent variables. The gradients to the prior and recognition networks can be estimated using the reparameterization trick (Kingma and Welling, 2014).\nDuring inference, latent variables obtained via prior networks and predicted question type qt′ are fed to the question decoder, which corresponds to red dashed arrows in Figure 2. The inference process is as follows:\n(1) Sample triple-level LV: zt ∼ qφ(zt|p)1. (2) Sample answer LV: za ∼ pϕ(za|p, zt). (3) Sample question LV: zq ∼ pϕ(zq|p, zt, za). (4) Predict question type: qt ∼ pθ(qt|zq, zt,p). (5) Generate question: q ∼ pθ(zq, zt,p, qt)."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we conduct experiments to evaluate our proposed method. We first introduce some empirical settings, including dataset, hyperparameters, baselines, and evaluation measures. Then we illustrate our results under both automatic and human evaluations. Finally, we give out some cases generated by different models and do further analyses over our method."
    }, {
      "heading" : "3.1 Dataset",
      "text" : "We apply our model on a large-scale CQG corpus2 extracted from Reddit3 by Wang et al. (2019). There are over 1.2 million PQA triples which have been divided into training/validation/test set with the number of 1,164,345/30,000/30,000. The dataset has been tokenized into words using the NLTK tokenizer (Bird et al., 2009). The average number of words in post/question/answer is 18.84/19.03/19.30, respectively. Following Fan et al. (2018) and Wang et al. (2019), we categorize questions in training and validation set into 9 types based on interrogative words, i.e., “what”, “when”, “where”, “who”, “why”, “how”, “can (could)”, “do (did, does)”, “is (am, are, was, were)”"
    }, {
      "heading" : "3.2 Hyper-parameter Settings",
      "text" : "We keep the top 40,000 frequent words as the vocabulary and the sentence padding length is set to 30. The dimension of GRU layer, word embedding and latent variables is 300, 300, and 100. The prior networks and MLPs have one hidden layer with size 300 and tanh non-linearity, while the number of hidden layers in recognition networks for both triple-level and utterance-level variables is 2. We apply dropout ratio of 0.2 during training. The mini-batch size is 64. For optimization, we use Adam (Kingma and Ba, 2015) with a learning rate of 1e-4. In order to alleviate degeneration problem of variational framework (Park et al., 2018), we\n1Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution.\n2https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG\n3http://www.reddit.com\napply KL annealing, word drop (Bowman et al., 2016) and bag-of-word (BOW) loss (Zhao et al., 2017)4. The KL multiplier λ gradually increases from 0 to 1, and the word drop probability is 0.25. We use Pytorch to implement our model, and the model is trained on Titan Xp GPUs."
    }, {
      "heading" : "3.3 Baselines",
      "text" : "We compare our methods with four groups of representative models: (1) S2S-Attn: A simple Seq2Seq model with attention mechanism (Shang et al., 2015). (2) CVAE&kgCVAE: The CVAE model integrates an extra BOW loss to generate diverse questions. The kgCVAE is a knowledge-guided CVAE that utilizes some linguistic cues (question types in our experiments) to learn meaningful latent variables (Zhao et al., 2017). (3) STD&HTD: The STD uses soft typed decoder that estimates a type distribution over word types, and the HTD uses hard typed decoder that specifies the type of each word explicitly with Gumbel-softmax (Wang et al., 2018). (4) RL-CVAE: A reinforcement learning method that regards the coherence score (computed by a one-to-one matching network) of a pair of generated question and answer as the reward function (Wang et al., 2019). RL-CVAE is the first work to utilize the future information, i.e., answer, and is also the state-of-the-art model for CQG5.\nAdditionally, we also conduct ablation study to better analyze our method as follows: (5) GTMzt: GTM without the triple-level latent variable, which means zt is not included in the prior and posterior distributions of both zp and za. (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq. Besides, zt here does not capture the semantics from answer. (7) GTMzq/za: GTM variant in which distributions of zq are not conditioned on za, i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables.\nIn our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020)\n4The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details.\n5For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the corresponding paper.\nthat provide the controllable feature, i.e., question types, in advance for inference. Therefore, we do not consider CT-based models as comparable ones."
    }, {
      "heading" : "3.4 Evaluation Measures",
      "text" : "To better evaluate our results, we use both quantitative metrics and human judgements in our experiments.\nAutomatic Metrics For automatic evaluation, we mainly choose four kinds of metrics: (1) BLEU Scores: BLEU (Papineni et al., 2002) calculates the n-gram overlap score of generated questions against ground-truth questions. We use BLEU-1 and BLEU-2 here and normalize them to 0 to 1 scale. (2) Embedding Metrics: Average, Greedy and Extrema metrics are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus6 in this part. Please refer to Serban et al. (2017) for more details. (3) Dist-1& Dist-2: Following the work of Li et al. (2016a), we apply Distinct to report the degree of diversity. Dist-1/2 is defined as the ratio of unique uni/bi-grams over all uni/bi-grams in generated questions. (4) RUBER Scores: Referenced metric and Unreferenced metric Blended Evaluation Routine (Tao et al., 2018) has shown a high correlation with human annotation in open-domain conversation evaluation. There are two versions, one is RubG based on geometric averaging and the other is RubA based on arithmetic averaging.\nEmbedding metrics and BLEU scores are used to measure the similarity between generated and ground-truth questions. RubG/A reflects the se-\n6https://code.google.com/archive/p/ word2vec/\nmantic coherence of PQ pairs (Wang et al., 2019), while Dist-1/2 evaluates the diversity of questions.\nHuman Evaluation Settings Inspired by Wang et al. (2019), Shen et al. (2019), and Wang et al. (2018), we use following three criteria for human evaluation: (1) Fluency measures whether the generated question is reasonable in logic and grammatically correct. (2) Coherence denotes whether the generated question is semantically consistent with the given post. Incoherent questions include dull cases. (3) Willingness measures whether a user is willing to answer the question. This criterion is to justify how likely the generated questions can elicit further interactions.\nWe randomly sample 500 examples from test set, and generate questions using models mentioned above. Then, we send each post and corresponding 10 generated responses to three human annotators without order, and require them to evaluate whether each question satisfies criteria defined above. All annotators are postgraduate students and not involved in other parts of our experiments."
    }, {
      "heading" : "3.5 Experimental Results",
      "text" : "Now we demonstrate our experimental results on both automatic evaluation and human evaluation.\nAutomatic Evaluation Results Now we demonstrate our experimental results on both automatic evaluation and human evaluation. The automatic results are shown in Table 2. The top part is the results of all baseline models, and we can see that GTM outperforms other methods on all metrics (significance tests (Koehn, 2004), p-value < 0.05), which indicates that our proposed model can improve the overall quality of generated questions. Specifically, Dist-2 and RubA have been improved by 2.43% and 1.90%, respectively, compared to the state-of-the-art RL-CVAE model.\nFirst, higher embedding metrics and BLEU scores show that questions generated by our model are similar to ground truths in both topics and contents. Second, taking answer into account and using it to decide the expression of question can improve the consistency of PQ pairs evaluated by RUBER scores. Third, higher distinct values illustrate that one-to-many mappings in PQ and QA pairs make the generated responses more diverse.\nThe bottom part of Table 2 shows the results of our ablation study, which demonstrates that taking advantage of answer information, modeling the shared background in entire triple, and considering one-to-many mappings in both PQ and QA pairs can help enhance the performance of our hierarchical variational model in terms of relevance, coherence and diversity.\nHuman Evaluation Results As shown in Table 3, GTM can alleviate the problem of generating dull and deviated questions compared with other models (significance tests (Koehn, 2004), p-value < 0.05). Both our proposed model and the state-of-the-art model RL-CVAE utilize the answer information and the results of them could prove that answers assist the question generation process. Besides, GTM can produce more relevant and intriguing questions, which indicates the effectiveness of modeling the shared background and one-to-many mappings in CQG task. The interannotator agreement is calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973). Fleiss’ kappa for Fluency, Coherence and Willingness is 0.493, 0.446 and 0.512, respectively, indicating “Moderate Agreement” for all three criteria."
    }, {
      "heading" : "3.6 Question-Answer Coherence Evaluation",
      "text" : "Automatic metrics in Section “Automatic Metrics” are designed to compare generated questions with ground-truth ones (RUBER also takes the post information into consideration), but ignore answers in the evaluation process. To measure the semantic coherence between generated questions and answers, we apply two methods (Wang et al., 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al., 2017) to obtain sentence embeddings and calculate cosine similarity between the embeddings of generated responses\n7The Infersent model is trained to predict the meaning of sentences based on natural language inference, and the cosine similarity computed with it is more consistent with human’s judgements, which performs better than the pre-trained Transformer/BERT model in our experiments.\nand answers. (2) Matching Score: We use the GRUMatchPyramid (Wang et al., 2019) model that adds the MatchPyramid network (Pang et al., 2016) on top of a bidirectional GRU to calculate the semantic coherence. As shown in Table 4, questions generated by GTM are more coherent to answers. Attributing to the design of triple-level latent variable that captures the shared background, one-to-many\nmappings in PQ and QA pairs, and relationship modeling for zq and za, GTM can improve the relevance in QA pairs."
    }, {
      "heading" : "3.7 Case Study",
      "text" : "In Table 5, we list the generated results of two posts from the test set to compare the performance of different models.\nIn the first case, both the post and answer mention two topics, “donation” and “song”, so the question is better to consider their relations. Besides, the answer here begins with “because”, then “why” and “what (reason)” questions are reasonable. For the second case, the post only talks about “pen”, while the answer refers to “ink”, which means there is a topic transition the question needs to cover. The second case shows the effectiveness of an answer that not only decides the expression of question but also improves the entire coherence of a tripe. Questions generated by GTM are more relevant to\nboth posts and answers, and could attract people to give an answer to them. However, other baselines may generate dull or deviated responses, even the RL-CVAE model that considers the answer information would only contain the topic words in answers (e.g., the question in case two), but fail to ensure the PQA coherence."
    }, {
      "heading" : "3.8 Further Analysis of GTM",
      "text" : "Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park et al., 2018; Wang et al., 2019). Generally, KL divergence measures the amount of information encoded in a latent variable. In the extreme case where the KL divergence of latent variable z equals to zero, the model completely ignores z, i.e., it degenerates. Figure 3 shows that the total KL divergence of GTM model maintains around 2 after 18 epochs indicating that the degen-\neration problem does not exist in our model and latent variables can play their corresponding roles."
    }, {
      "heading" : "4 Related Work",
      "text" : "The researches on open-domain dialogue systems have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems. We introduce these two fields as follows and point out the main differences between our method and previous ones."
    }, {
      "heading" : "4.1 CQG",
      "text" : "Traditional question generation (TQG) has been widely studied and can be seen in reading comprehension (Zhou et al., 2019; Kim et al., 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al., 2019; Nema et al., 2019), visual question generation (Fan et al., 2018) and task-oriented dialogues (Li et al., 2017). In such tasks, finding information via a generated question is the major goal and the answer is usually part of the input. Different from TQG, CQG aims to enhance the interactiveness and persistence of conversations (Wang et al., 2018). Meanwhile, the answer is the “future” information which means it is unavailable in the inference process. Wang et al. (2018) first studied on CQG, and they used soft and hard typed decoders to capture the distribution of different word types in a question. Hu et al. (2018) added a target aspect in the input and proposed an extended Seq2Seq model to generate aspect-specific questions. Wang et al. (2019) devised two methods based on either reinforcement learning or generative adversarial network (GAN)\nto further enhance semantic coherence between posts and questions under the guidance of answers."
    }, {
      "heading" : "4.2 Context Modeling in Dialogue Systems",
      "text" : "Existing methods mainly focus on the historical context in multi-turn conversations, and hierarchical models occupy a vital position in this field. Serban et al. (2016) proposed the hierarchical recurrent encoder-decoder (HRED) model with a context RNN to integrate historical information from utterance RNNs. To capture utterance-level variations, Serban et al. (2017) raised a new model Variational HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019).\nThe differences between our method and aforementioned ones in Section 4.1 and 4.2 are: (1) Rather than dividing PQA triples into two parts, i.e., PQ (history and current utterances) and QA (current and future utterances) pairs, we model the entire coherence by utilizing a latent variable to capture the share background in a triple. (2) Instead of regarding the relationship between question and answer as a text matching task that lacks the consideration of diversity, we incorporate utterance-level latent variables to help model one-to-many mappings in both PQ and QA pairs."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a generative triple-wise model for generating appropriate questions in open-domain conversations, named GTM. GTM models the entire background in a triple and one-to-many mappings in PQ and QA pairs simultaneously with latent variables in three hierarchies. It is trained in a onestage end-to-end framework without pre-training\nlike the previous state-of-the-art model that also takes answer into consideration. Experimental results on a large-scale CQG dataset show that GTM can generate fluent, coherent, informative as well as intriguing questions."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank all the reviewers for their insightful and valuable comments and suggestions."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language processing with Python: analyzing text with the natural language toolkit",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "” O’Reilly Media, Inc.”.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating multiple diverse responses with multi-mapping and posterior mapping selection",
      "author" : [ "Chaotao Chen", "Jinhua Peng", "Fan Wang", "Jun Xu", "Hua Wu." ],
      "venue" : "Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "A question type driven framework to diversify visual question generation",
      "author" : [ "Zhihao Fan", "Zhongyu Wei", "Piji Li", "Yanyan Lan", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4048–4054.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Posterior-gan: Towards informative and coherent response generation with posterior generative adversarial network",
      "author" : [ "Shaoxiong Feng", "Hongshen Chen", "Kan Li", "Dawei Yin." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 33(3):613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "Jointly optimizing diversity and relevance in neural response generation",
      "author" : [ "Xiang Gao", "Sungjin Lee", "Yizhe Zhang", "Chris Brockett", "Michel Galley", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect-based question generation",
      "author" : [ "Wenpeng Hu", "Bing Liu", "Jinwen Ma", "Dongyan Zhao", "Rui Yan" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural question generation using answer separation",
      "author" : [ "Yanghoon Kim", "Hwanhee Lee", "Joongbo Shin", "Kyomin Jung." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6602–6609.",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederick P Kingma", "Jimmy Ba." ],
      "venue" : "the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "the 2nd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Generating diverse and consistent QA pairs from contexts with information-maximizing hierarchical conditional VAEs",
      "author" : [ "Dong Bok Lee", "Seanie Lee", "Woo Tae Jeong", "Donghwan Kim", "Sung Ju Hwang." ],
      "venue" : "Proceedings of the 58th Annual",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving question generation with to the point context",
      "author" : [ "Jingjing Li", "Yifan Gao", "Lidong Bing", "Irwin King", "Michael R. Lyu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning through dialogue interactions by asking questions. ICLR",
      "author" : [ "Jiwei Li", "Alexander H Miller", "Sumit Chopra", "Marc’Aurelio Ranzato", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Like hiking? you probably enjoy nature: Personagrounded dialog with commonsense expansions",
      "author" : [ "Bodhisattwa Prasad Majumder", "Harsh Jhamtani", "Taylor Berg-Kirkpatrick", "Julian McAuley." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Majumder et al\\.,? 2020",
      "shortCiteRegEx" : "Majumder et al\\.",
      "year" : 2020
    }, {
      "title" : "Let’s ask again: Refine network for automatic question generation",
      "author" : [ "Preksha Nema", "Akash Kumar Mohankumar", "Mitesh M. Khapra", "Balaji Vasan Srinivasan", "Balaraman Ravindran." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Nema et al\\.,? 2019",
      "shortCiteRegEx" : "Nema et al\\.",
      "year" : 2019
    }, {
      "title" : "Text matching as image recognition",
      "author" : [ "Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Pang et al\\.,? 2016",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A hierarchical latent structure for variational conversation modeling",
      "author" : [ "Yookoon Park", "Jaemin Cho", "Gunhee Kim." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "CDL: Curriculum dual learning for emotion-controllable response generation",
      "author" : [ "Lei Shen", "Yang Feng." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 556–566, Online. Association for Com-",
      "citeRegEx" : "Shen and Feng.,? 2020",
      "shortCiteRegEx" : "Shen and Feng.",
      "year" : 2020
    }, {
      "title" : "Modeling semantic relationship in multi-turn conversations with hierarchical latent variables",
      "author" : [ "Lei Shen", "Yang Feng", "Haolan Zhan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5497–5502, Florence,",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to select context in a hierarchical and global perspective for open-domain dialogue generation",
      "author" : [ "Lei Shen", "Haolan Zhan", "Xin Shen", "Yang Feng." ],
      "venue" : "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "NEXUS network: Connecting the preceding and the following in dialogue generation",
      "author" : [ "Xiaoyu Shen", "Hui Su", "Wenjie Li", "Dietrich Klakow." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4316–",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "How to make context more useful? an empirical study on contextaware neural conversational models",
      "author" : [ "Zhiliang Tian", "Rui Yan", "Lili Mou", "Yiping Song", "Yansong Feng", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association",
      "citeRegEx" : "Tian et al\\.,? 2017",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2017
    }, {
      "title" : "The importance of being important: Question generation",
      "author" : [ "Lucy Vanderwende." ],
      "venue" : "Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge, Arlington, VA.",
      "citeRegEx" : "Vanderwende.,? 2008",
      "shortCiteRegEx" : "Vanderwende.",
      "year" : 2008
    }, {
      "title" : "Answer-guided and semantic coherent question generation in open-domain conversation",
      "author" : [ "Weichao Wang", "Shi Feng", "Daling Wang", "Yifei Zhang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to ask questions in opendomain conversational systems with typed decoders",
      "author" : [ "Yansen Wang", "Chenyi Liu", "Minlie Huang", "Liqiang Nie." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical recurrent attention network for response generation",
      "author" : [ "Chen Xing", "Yu Wu", "Wei Wu", "Yalou Huang", "Ming Zhou." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 5610–5617.",
      "citeRegEx" : "Xing et al\\.,? 2018",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2018
    }, {
      "title" : "Strategy and policy learning for nontask-oriented conversational systems",
      "author" : [ "Zhou Yu", "Ziyu Xu", "Alan W Black", "Alexander Rudnicky." ],
      "venue" : "Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, pages 404–",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Augmenting knowledge-grounded conversations with sequential knowledge transition",
      "author" : [ "Haolan Zhan", "Hainan Zhang", "Hongshen Chen", "Zhuoye Ding", "Yongjun Bao", "Yanyan Lan." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Zhan et al\\.,? 2021",
      "shortCiteRegEx" : "Zhan et al\\.",
      "year" : 2021
    }, {
      "title" : "Context-sensitive generation of open-domain conversational responses",
      "author" : [ "Weinan Zhang", "Yiming Cui", "Yifa Wang", "Qingfu Zhu", "Lingzhi Li", "Lianqiang Zhou", "Ting Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguis-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
      "author" : [ "Tiancheng Zhao", "Ran Zhao", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    }, {
      "title" : "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "author" : [ "Hao Zhou", "Minlie Huang", "Tianyang Zhang", "Xiaoyan Zhu", "Bing Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Question-type driven question generation",
      "author" : [ "Wenjie Zhou", "Minghua Zhang", "Yunfang Wu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017).",
      "startOffset" : 172,
      "endOffset" : 189
    }, {
      "referenceID" : 39,
      "context" : "Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016).",
      "startOffset" : 105,
      "endOffset" : 122
    }, {
      "referenceID" : 37,
      "context" : "Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018).",
      "startOffset" : 164,
      "endOffset" : 183
    }, {
      "referenceID" : 44,
      "context" : "CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a ques-",
      "startOffset" : 69,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a ques-",
      "startOffset" : 69,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a ques-",
      "startOffset" : 69,
      "endOffset" : 123
    }, {
      "referenceID" : 36,
      "context" : "While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 37,
      "context" : "At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1).",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1).",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "When it comes to the semantic relationship in each triple, the content of a specific question is under the control of its post and answer (Lee et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "We use a bidirectional GRU (Cho et al., 2014) as encoder to capture the semantic representation of",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 42,
      "context" : "feature that benefits the training of latent variables (Zhao et al., 2017).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "where MLP(·) is a feed-forward network, and softplus function is a smooth approximation to ReLU and can be used to ensure positiveness (Park et al., 2018; Serban et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 175
    }, {
      "referenceID" : 27,
      "context" : "where MLP(·) is a feed-forward network, and softplus function is a smooth approximation to ReLU and can be used to ensure positiveness (Park et al., 2018; Serban et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "The gradients to the prior and recognition networks can be estimated using the reparameterization trick (Kingma and Welling, 2014).",
      "startOffset" : 104,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "For optimization, we use Adam (Kingma and Ba, 2015) with a learning rate of 1e-4.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 25,
      "context" : "In order to alleviate degeneration problem of variational framework (Park et al., 2018), we",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "com apply KL annealing, word drop (Bowman et al., 2016) and bag-of-word (BOW) loss (Zhao et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 42,
      "context" : ", 2016) and bag-of-word (BOW) loss (Zhao et al., 2017)4.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "We compare our methods with four groups of representative models: (1) S2S-Attn: A simple Seq2Seq model with attention mechanism (Shang et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 42,
      "context" : "The kgCVAE is a knowledge-guided CVAE that utilizes some linguistic cues (question types in our experiments) to learn meaningful latent variables (Zhao et al., 2017).",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 37,
      "context" : "distribution over word types, and the HTD uses hard typed decoder that specifies the type of each word explicitly with Gumbel-softmax (Wang et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 153
    }, {
      "referenceID" : 36,
      "context" : "by a one-to-one matching network) of a pair of generated question and answer as the reward function (Wang et al., 2019).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020)",
      "startOffset" : 134,
      "endOffset" : 192
    }, {
      "referenceID" : 43,
      "context" : "In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020)",
      "startOffset" : 134,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020)",
      "startOffset" : 134,
      "endOffset" : 192
    }, {
      "referenceID" : 27,
      "context" : "are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 163
    }, {
      "referenceID" : 33,
      "context" : "(4) RUBER Scores: Referenced metric and Unreferenced metric Blended Evaluation Routine (Tao et al., 2018) has shown a high correlation with human annotation in open-domain conversation evaluation.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : "word2vec/ mantic coherence of PQ pairs (Wang et al., 2019), while Dist-1/2 evaluates the diversity of questions.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "The top part is the results of all baseline models, and we can see that GTM outperforms other methods on all metrics (significance tests (Koehn, 2004), p-value < 0.",
      "startOffset" : 137,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : "lem of generating dull and deviated questions compared with other models (significance tests (Koehn, 2004), p-value < 0.",
      "startOffset" : 93,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "annotator agreement is calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973).",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 36,
      "context" : "To measure the semantic coherence between generated questions and answers, we apply two methods (Wang et al., 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : ", 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al., 2017) to obtain sentence embeddings and calculate cosine similarity between the embeddings of generated responses",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 36,
      "context" : "(2) Matching Score: We use the GRUMatchPyramid (Wang et al., 2019) model that adds the MatchPyramid network (Pang et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 23,
      "context" : ", 2019) model that adds the MatchPyramid network (Pang et al., 2016) on top of a bidirectional GRU to calculate the semantic coherence.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 42,
      "context" : "Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park et al., 2018; Wang et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 204
    }, {
      "referenceID" : 25,
      "context" : "Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park et al., 2018; Wang et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 204
    }, {
      "referenceID" : 36,
      "context" : "Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park et al., 2018; Wang et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems.",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 40,
      "context" : "have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems.",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 31,
      "context" : "have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems.",
      "startOffset" : 23,
      "endOffset" : 84
    }, {
      "referenceID" : 44,
      "context" : "Traditional question generation (TQG) has been widely studied and can be seen in reading comprehension (Zhou et al., 2019; Kim et al., 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al.",
      "startOffset" : 103,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "Traditional question generation (TQG) has been widely studied and can be seen in reading comprehension (Zhou et al., 2019; Kim et al., 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al.",
      "startOffset" : 103,
      "endOffset" : 140
    }, {
      "referenceID" : 35,
      "context" : ", 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : ", 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al., 2019; Nema et al., 2019), visual question generation (Fan et al.",
      "startOffset" : 73,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : ", 2019), sentence transformation (Vanderwende, 2008), question answering (Li et al., 2019; Nema et al., 2019), visual question generation (Fan et al.",
      "startOffset" : 73,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : ", 2019), visual question generation (Fan et al., 2018) and task-oriented dialogues (Li et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : ", 2018) and task-oriented dialogues (Li et al., 2017).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 37,
      "context" : "enhance the interactiveness and persistence of conversations (Wang et al., 2018).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level.",
      "startOffset" : 5,
      "endOffset" : 24
    } ],
    "year" : 2021,
    "abstractText" : "Generating some appealing questions in opendomain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation. However, they separate a post-questionanswer (PQA) triple into two parts: postquestion (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a largescale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.",
    "creator" : "LaTeX with hyperref"
  }
}