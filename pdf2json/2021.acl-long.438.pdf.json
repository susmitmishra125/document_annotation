{
  "name" : "2021.acl-long.438.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance",
    "authors" : [ "Zhongkun Liu", "Pengjie Ren", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke", "Ming Zhou", "Ahold Delhaize" ],
    "emails" : [ "zhaochun.ren}@sdu.edu.cn", "m.derijke@uva.nl;", "mingzhou926@hotmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5638–5650\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5638"
    }, {
      "heading" : "1 Introduction",
      "text" : "Conversational information seeking (CIS) (Zamani and Craswell, 2020; Ren et al., 2021b) has received extensive attention. It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020). One of the key features of CIS is mixed initiative behavior, where a system can improve user satisfaction by proactively asking clarification questions (Zhang et al., 2018; Aliannejadi et al., 2019; Xu et al., 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020).\nPrevious studies on asking clarification questions can be grouped into two categories: conversational question generation (Duan et al., 2017) and conversational question ranking (Aliannejadi et al., 2019).\n∗ Corresponding authors.\nThe former directly generates conversational questions based on the dialogue context. However, the generated questions may be irrelevant and meaningless (Rosset et al., 2020). A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan, 2020). Instead, the latter proposes to retrieve questions from a collection for the given dialogue context, which can usually guarantee that the questions are relevant and useful (Shen et al., 2018; Rosset et al., 2020). However, question ranking methods do not lead to a natural communication between human and machine (Pulman, 1995), as they neglect important characteristics in conversations, e.g., anaphora and ellipsis. As shown in Fig. 1, the self-contained question (SQ4) lacks these characteristics, which makes it look unnatural.\nIn this work, we study the task of Conversa-\ntional Question Simplification (CQS). Given a dialogue context and self-contained question as input, CQS aims to transform the self-contained question into a conversational one by simulating conversational characteristics, such as anaphora and ellipsis. For example, in Fig. 1, four simplification operations are applied to obtain the conversational question (CQ4), which is context-dependent and superior to its origin one (SQ4) in terms of naturalness and conveying. The reverse process, i.e., Conversational Question Rewriting (CQR) (Elgohary et al., 2019; Voskarides et al., 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al., 2020; Yu et al., 2020). Although the proposed methods for CQR can be easily adopted for CQS, they do not always generate satisfactory results as they are all trained to optimize a maximum likelihood estimation (MLE) objective, which gives equal attention to generate each question token. Therefore, they often get stuck in easily learned tokens, i.e., tokens appearing in input, ignoring conversational tokens, e.g., him, which is a small but important portion of output.\nTo address the above issue, we propose a new scheme for CQS, namely minimum Levenshtein distance (MLD). It minimizes the differences between input and output, forcing the model to pay attention to contributing tokens that are related to conversational tokens, e.g., “Ira Hay” and “him” in Fig. 1. Therefore, MLD is expected to outperform MLE for CQS. However, MLD cannot be minimized by direct optimization due to the discrete nature, i.e., minimizing the number of discrete edits. We present an alternative solution, a Reinforcement Iterative Sequence Editing (RISE) framework for the optimization of MLD.\nWe formulate RISE as a Hierarchical Combinatorial Markov Decision Process (HCMDP) consisting of an editing Markov Decision Process (MDP) to predict multiple edits for all tokens in the selfcontained question, e.g., ‘Keep (K)’ to keep a token, and a phrasing MDP to predict a phrase if the edit is ‘Insert (I)’ or ‘Substitute (S)’. We only have the self-contained and conversational question pairs in the dataset while the demonstrations of the editing iterations are lacked. Thus, we cannot train each editing iteration of RISE with teacher forcing. To this end, we devise an Iterative Reinforce Training (IRT) algorithm that allows RISE to do some exploration itself. The exploration can be rewarded\naccording to its Levenshtein distance (LD) with the demonstrated conversational question. Traditional exploration methods like -sampling (Sutton and Barto, 1998) neglect the interdependency between edits for all tokens, resulting in poor exploration. Thus, we further introduce a Dynamic Programming based Sampling (DPS) process that adopts a Dynamic Programming (DP) algorithm to track and model the interdependency in IRT. Experiments on the CANARD (Elgohary et al., 2019) and CAsT (Dalton et al., 2019) datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well to unseen data.\n2 Conversational Question Simplification: From maximum likelihood estimation to minimum Levenshtein distance"
    }, {
      "heading" : "2.1 CQS",
      "text" : "Given a dialogue context C representing the previous conversation utterances and the self-contained clarification question candidate x = {x1, . . . , x|x|} to be asked next (e.g., from a conversational question ranking model), the goal of Conversational Question Simplification (CQS) is to reformulate question x to a conversational question y = {y1, . . . , y|y|} by simulating conversational characteristics, e.g., anaphora and ellipsis. A target conversational question y∗ = {y∗1, . . . , y∗|y∗|} is provided during the training phase."
    }, {
      "heading" : "2.2 Maximum likelihood estimation for CQS",
      "text" : "A commonly adopted paradigm for tasks similar to CQS, e.g., CQR, is to model the task as a conditional sequence generation process parameterized by θ, which is usually optimized by MLE:\nLθ = − log pθ(y∗|x,C)\n= − |y∗|∑ t=1 log pθ(y ∗ t |y∗<t, x, C),\n(1)\nwhere y∗ is the target question and y∗<t denotes the prefix y∗1, y ∗ 2, . . . , y ∗ t−1. As we can see, MLE gives equal weight to each token and falls in easily learned tokens, the overwhelming duplicate tokens between x and y, while underestimating subtle differences of tokens related to conversational characteristics."
    }, {
      "heading" : "2.3 Minimum Levenshtein distance for CQS",
      "text" : "Inspired by Arjovsky et al. (2017), to minimize the distance between two distributions, we propose\nto minimize the LD between the target question y∗ and the model output y so as to leverage the high overlap between x and y and focus on subtle different tokens:\nLθ = LD(y, y∗). (2)\nUnfortunately, it is impossible to directly optimize Eq. 2 because the LD between y and y∗ is the minimum number of single-token edits (insertions, deletions or substitutions) required to change y into y∗, which is discrete and non-differentiable."
    }, {
      "heading" : "3 RISE",
      "text" : "To optimize MLD in Eq. 2, we devise the Reinforcement Iterative Sequence Editing (RISE) framework, which reformulates the optimization of MLD as a Hierarchical Combinatorial Markov Decision Process (HCMDP). Next, we first describe our HCMDP formulation of RISE. We then detail the modeling of each ingredient in RISE. Finally, we present the training process of RISE."
    }, {
      "heading" : "3.1 HCMDP formulation for RISE",
      "text" : "RISE produces its output y by iteratively editing x with four types of edit, i.e., ‘K’ to keep a token, ‘Delete (D)’ to delete a token, ‘I’ to insert a phrase (a sequence of tokens) after a token, and ‘S’ to substitute a phrase by a new one. If a token is predicted as ‘I’ or ‘S’, we need to further predict a corresponding phrase. Note that we only predict one phrase for successive ‘S’ edits. We formulate RISE as a Hierarchical Combinatorial Markov Decision Process (HCMDP) consisting of (1) an editing MDP to predict multiple edits for all tokens, and (2) a phrasing MDP to predict a phrase if the edit is ‘I’ or ‘S’.\nThe editing MDP can be formulated as a tuple 〈Se,Ae, T e,R, πe〉. Here, set ∈ Se denotes the question at t-th iteration yt together with the context C, i.e., set = (y\nt, C). Note that se0 = (x,C). aet = [a e t,1, a e t,2, . . . , a e t,|yt|] ∈ A\ne is a combinatorial action consisting of several interdependent edits. The number of edits corresponds to the length of yt. For example, in Fig. 2, aet = [‘K’, ‘K’, ‘K’, ‘K’, ‘S’, ‘S’, ‘K’, ‘K’]. In our case, the transition function T e is deterministic, which means that the next state set+1 is obtained by applying the predicted actions from both the editing MDP and phrasing MDP to the current state set . rt ∈ R is the reward function, which estimates the joint effect of taking the predicted actions from both the edit-\ning and phrasing MDPs. πe is the editing policy network.\nThe phrasing MDP can be formulated as a tuple 〈Sp,Ap, T p,R, πp〉. Here, spt ∈ Sp consists of the current question yt, the predicted action from the editing MDP aet , and the context C, i.e., s p t = (yt, aet , C). a p t = [a p t,1, a p t,2, . . .] ∈ Ap is also a combinatorial action, where apt,i denotes a phrase from a predefined vocabulary and i corresponds to the index of the ‘I’ or ‘S’ edits, e.g., in Fig. 2, ‘apt,1 = him’ is the predicted phrase for the first ‘S’ edit. The length of the action sequence corresponds to the number of ‘I’ or ‘S’ edits. The transition function T p returns the next state spt+1 by applying the predicted actions from the phrasing MDP to the current state spt . rt ∈ R is the shared reward function. πp is the phrasing policy network.\nRISE tries to maximize the expected reward:\nJ(θ) = Eaet∼πe,a p t∼πp [rt], (3)\nwhere θ is the model parameter which is optimized with the policy gradient:\n∇J(θ) = Eaet∼πe,apt∼πp [rt(∇ log π e(aet |set ) +\n∇ log πp(apt |s p t ))],\n(4)\nNext, we will show how to model πe(aet |set ), πp(apt |s p t ), and rt."
    }, {
      "heading" : "3.2 Policy networks",
      "text" : "We implement the editing and phrasing policy networks (πe and πp) based on BERT2BERT (Rothe et al., 2020) as shown in Fig. 2. The editing policy network is implemented by the encoder to predict combinatorial edits, and the phrasing policy network is implemented by the decoder to predict phrases."
    }, {
      "heading" : "3.2.1 Editing policy network",
      "text" : "We unfold all tokens of the utterances in the context into a sequence C = (w1, . . . , wc), where wi denotes a token and we add “[SEP]” to separate different utterances. Then the context and input question in t-th iteration are concatenated with “[SEP]” as the separator. Finally, we feed them into the encoder of BERT2BERT to obtain hidden representations for tokens in question Ht = (ht1, . . . , h t |yt|) and apply a linear layer with parameter W e to predict aet :\nπe(aet |set = (yt, C)) = softmax(W eHt). (5)"
    }, {
      "heading" : "3.2.2 Phrasing policy network",
      "text" : "We first extract the spans corresponding to the ‘I’ or ‘S’ edits from the question. If the edit is ‘I’, the question span spanti consists of tokens before and after this insertion, i.e., spanti = [y t j , y t j+1]; if the edit is ‘S’, the question span spanti consists of successive tokens corresponding to the ‘S’ edit, i.e., spanti = [y t j , . . . , y t k], where a e t,j:k =‘S’ and aet,k+1 6= ‘S’. We only predict once for successive ‘S’ edits, e.g., in Fig. 2, the phrase ‘him’ is predicted to substitute question span [“Ira”, “Hayes”].\nFor the i-th ‘I’ or ‘S’ edit with a question span spanti, we concatenate the span and “[CLS]” token as input tokens, and feed them into the decoder of BERT2BERT to obtain a hidden representation of “[CLS]” token sti. We obtain S\nt by concatenating each sti and predict the phrases for all ‘S’ and ‘I’ edits by a linear layer with parameter W p:\nπp(apt |s p t ) = softmax(W pSt). (6)"
    }, {
      "heading" : "3.3 Reward R",
      "text" : "We devise the reward rt to estimate the effect of taking the joint action (aet , a p t ) by encouraging actions that can result in low LD values between yt+1 and y∗, i.e., minimizing Eq. 2. Besides, we discourage those actions to achieve same yt+1 with extra non ‘K’ edits:\nrt = 1\n1 + LD(yt+1, y∗) ×(\nl − ∑ t (aet 6= ‘K’) + 1\n) ,\nl = LD(yt, y∗)− LD(yt+1, y∗),\n(7)\nwhere 1 1+LD(yt+1,y∗) will reward actions that result in low LD values between yt+1 and y∗ and (l − ∑ t(a e t 6= ‘K’)) will punish those actions with unnecessary non ‘K’ edits."
    }, {
      "heading" : "3.4 Training",
      "text" : "To train RISE, we need training samples in the form of a tuple (set , a e t , s p t , a p t , rt). However, we only have (y0 = x, y∗) in our dataset. Traditional exploration methods like -greedy sampling sample edits for all tokens independently, ignoring the interdependency between them. Instead, we devise an Iterative Reinforce Training (IRT) algorithm to sample an edit for each token by considering its future expectation, i.e., sampling aet,i based on expectation of aet,:i−1 from i = |yt| to 1. We maintain a matrix M t for this expectation based on both yt and y∗, which is computed by a Dynamic Programming based Sampling (DPS) process due to the exponential number of edit combinations of aet,:i. The details of IRT are provided in Alg. 1; it contains a DPS process that consists of two parts: computing the matrix M t (line 4–8) and sampling actions (aet , a p t ) (line 10) based on M t."
    }, {
      "heading" : "3.4.1 Computing the matrix M t",
      "text" : "Given (yt, y∗) with length m and n, we maintain a matrix M t ∈ R(m+1)×(n+1) (including ‘[SEP]’, see the upper right part in Fig. 3) where each element M ti,j tracks the expectation of a e t,:i to convert yt:i to y ∗ :j :\nM ti,j = Epi,j(aet,i)[Ep(aet,:i−1)πyt:i−>y∗:j (a e t,:i)]\n= Epi,j(aet,i) πe(aet,i|yt, C)×  M ti−1,j−1, if a e t,i = ‘K’ M ti−1,j , if a e t,i = ‘D’ M ti,j−1, if a e t,i = ‘I’\nM ti−1,j−1, if a e t,i = ‘S’\n, (8)\nwhere aet,:i is the combinational edits for tokens y t :i and πe(aet,i|yt, C) is calculated by Eq. 5 (see the upper left part in Fig. 3). M t0,0 is initialized to 1. We will first introduce pi,j(aet,i) and then introduce πyt:i−>y∗:j (a e t,:i) in Eq. 8.\nTraditional sampling methods sample each edit aet,i independently, based on model likelihood πe(aet,i|yt, C). Instead, we sample each edit with probability pi,j(aet,i) based on edits expectation M t, which is modeled as:\npi,j(a e t,i) =\n1\nZti,j π(aet,i|yt, C)× M ti−1,j−1, if a e t,i = ‘K’ M ti−1,j , if a e t,i = ‘D’ M ti,j−1, if a e t,i = ‘I’\nM ti−1,j−1, if a e t,i = ‘S’,\n(9)\nwhere Zti,j is the normalization term. We give an example on computing M t1,2 in the bottom part of Fig. 3. For edit ‘I’ in M t1,2, its probability is 1, and its value is πe(aet,i = ‘I’|yt, C) ×M t1,1 = 0.008. For the other edits, the probability is 0. Therefore, M t1,2 = 0.008. πyt:i−>y∗:j (a e t,:i) is the probability of conducting edits aet,:i to convert y t :i to y ∗ :j :\nπyt:i−>y∗:j (a e t,:i) = π e(aet,i|yt, C)× πyt:i−1−>y∗:j−1(a e t,:i−1), if a e t,i−1 = ‘K’ πyt:i−1−>y∗:j (a e t,:i−1), if a e t,i−1 = ‘D’ πyt:i−>y∗:j−1(a e t,:i), if a e t,i = ‘I’\nπyt:i−1−>y∗:j−1(a e t,:i−1), if a e t,i−1 = ‘S’,\n(10)\nTo convert yt:i to y ∗ :j , we need to make sure that yti can convert to y ∗ j and that y t :i−1 can convert to y∗:j−1, which can be calculated recursively. Note that we only allow ‘S’ and ‘D’ for yti when y t i 6= y∗j and ‘K’ and ‘I’ for y t i when y t i = y ∗ j . And M ti−1,j−1 = Ep(aet,:i−1)πyt:i−1−>y∗:j−1(a e t,:i−1).\n3.4.2 Sampling (aet , a p t ) We sample (aet , a p t ) based on matrix M\nt by backtracking from i = m, j = n. For example, as shown in the upper right in Fig. 3, we backtrack along the blue arrows. In this truncated sample, we start from M t7,6, sample an edit ‘K’ to keep ‘revealing’ based on p7,6(aet,7) in Eq. 9, and move toM t 6,5. Then, we sample ‘S’ to substitute ‘Ira Hayes’ to ‘him’ and move to M t4,4. Finally, we sample ‘K’\nAlgorithm 1: Training Process of RISE Input: The origin data D = {(x, y∗)}, the\nnumber of samples L; Output: The model parameters θ;\nin [M t4,4,M t 3,3,M t 2,2M t 1,1,M t 0,0] to keep [‘to’, ‘opposed’, ‘anyone’, ‘Was’, ‘[SEP]’]. Therefore, we can obtain aet = [K, K, K, K, K, S, S, K], a p t = [‘him’]. Note that we obtain apt by merging all corresponding tokens y∗j as the phrase for each ‘I’ edit and successive ‘S’ edits and we only substitute once. The backtracking rule can be formulated as:\nM ti,j →  M ti−1,j−1, if a e t,i ∈ [‘K’, ‘S’] M ti−1,j , if a e t,i = ‘D’\nM ti,j−1, if a e t,i = ‘I’.\n(11)"
    }, {
      "heading" : "3.5 Inference",
      "text" : "During inference, RISE iteratively edits x until it predicts ‘K’ edits for all tokens or it achieves the\nmaximum iteration limit. For example, for editing iteration t in Figure 2, it predicts ‘S’ for ‘Ira’ and ‘Hayes’ to substitute it to ‘him’ and ‘K’ for other tokens, which results in ‘Was anyone opposed to him revealing . . . ’ as output. The output in iteration t is the input of iteration t+ 1. The actual editing iteration times vary with different samples."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "As with previous studies (Elgohary et al., 2019; Yu et al., 2020; Vakulenko et al., 2020; Lin et al., 2020a), we conduct experiments on the CANARD1 (Elgohary et al., 2019) dataset, which is a large open-domain dataset for conversational question answering (with over 30k training samples). Each sample in the CANARD dataset includes a conversational context (historical questions and answers), an self-contained question, and its corresponding conversational question under the context. The questions always have clear answers, e.g., ‘Did he win the lawsuit?’ We follow the CANARD splits for training and evaluation.\nIn addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search. Different from CANARD, its context only contains questions without corresponding answers. Besides, most questions in the CAsT dataset are exploring questions to explore relevant information, e.g., ‘What about for great whites?’ Since the CAsT dataset only contains 479 samples from different domains compared to CANARD, we use it for testing."
    }, {
      "heading" : "4.2 Evaluation metrics",
      "text" : "Following Su et al. (2019); Xu et al. (2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation. BLEU-n and ROUGE-L measure the word overlap between the generated and golden questions. CIDEr measures the extent to which important information is missing. Elgohary et al. (2019); Lin et al. (2020a); Xu et al. (2020) have shown that automatic evaluation has a high correlation with human judgement on this task, so we do not conduct human evaluation in this paper.\n1http://canard.qanta.org 2http://www.treccast.ai"
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare with several recent state-of-the-art methods for this task or closely related tasks: • Origin uses the original self-contained question\nas output. • Rule (Yu et al., 2020) employs two simple\nrules to mimic two conversational characteristics: anaphora and ellipsis. • QGDiv (Sultan et al., 2020) uses RoBERTa (Liu\net al., 2019) with beam search (Wiseman and Rush, 2016) for generation. • Trans++ (Vakulenko et al., 2020) predicts sev-\neral word distributions, and combines them to obtain the final word distribution when generating each token. • QuerySim (Yu et al., 2020) adopts a GPT-\n2 (Radford et al., 2019) model to generate conversational question. We also found some methods from related tasks. But they do not work on this task for various reasons. For example, due to the lack of labels needed for training, we cannot compare with the methods proposed by Rosset et al. (2020) and Xu et al. (2020). Su et al. (2019) propose a model that can only copy tokens from input; it works well on the reverse task (i.e., CQR), but not on CQS."
    }, {
      "heading" : "4.4 Implementation details",
      "text" : "We use BERT2BERT for the modeling of the editing and phrasing parts (Rothe et al., 2020), as other pretrained models like GPT-2 (Radford et al., 2019) cannot work for both. The hidden size is 768 and phrase vocabulary is 3461 following (Malmi et al., 2019). We use the BERT vocabulary (30,522 tokens) for all BERT-based or BERT2BERT-based models. We use the Adam optimizer (learning rate 5e-5) (Kingma and Ba, 2015) to train all models. In particular, we train all models for 20,000 warm-up steps, 5 epochs with pretrained model parameters frozen, and 20 epochs for all parameters. For RISE, the maximum editing iteration times is set to 3. We use gradient clipping with a maximum gradient norm of 1.0. We select the best models based on the performance on the validation set. During inference, we use greedy decoding for all models."
    }, {
      "heading" : "4.5 Results",
      "text" : "We list the results of all methods on both CANARD and CAsT in Table 1. From the results, we have two main observations.\nFirst, RISE significantly outperforms all base-\nTable 1: Overall performance (%) on CANARD and CAsT. Bold face indicates the best results in terms of the corresponding metrics. Significant improvements over the best baseline results are marked with ∗ (t-test, p < 0.01). Note that we denote BLEU-n as B-n and ROUGE-L as R-L.\nCANARD (%) CAsT (%) (unseen)\nMethod B-1 B-2 B-3 B-4 R-L CIDEr B-1 B-2 B-3 B-4 R-L CIDEr\nOrigin 54.7 47.0 40.6 35.3 70.9 3.460 75.9 69.2 62.9 57.6 85.0 5.946 Rule 55.0 47.0 40.2 34.8 70.5 3.420 78.0 71.4 65.3 60.0 86.1 6.220\nTrans++ 84.3 77.5 72.1 67.5 84.6 6.348 76.0 64.3 54.8 47.2 76.5 4.258 QGDiv 85.2 78.6 73.3 68.9 85.2 6.469 75.9 65.3 56.7 59.6 78.0 4.694 QuerySim 83.1 78.5 74.5 71.0 82.7 6.585 80.6 75.3 70.2 65.5 83.3 6.345\nRISE 86.3∗ 80.5∗ 75.6 71.6∗ 86.2∗ 6.759 85.1∗ 78.4 72.2 66.8 87.8∗ 6.543\nlines on both datasets. Specifically, RISE outperforms the strongest baseline QuerySim by ˜4% in terms of ROUGE-L. The reason is that RISE enhanced by DPS has a better ability to emphasize conversational tokens, rather than treating all tokens equally.\nSecond, RISE is more robust, which generalizes better to unseen data of CAsT. The results of the neural methods on CANARD are much better than those on CAsT. But, RISE is more stable than the other neural models. For example, RISE outperforms QuerySim by 0.6% in BLEU-4 on CANARD, while 1.3% on CAsT. The reason is that RISE learns to cope with conversational tokens only, while other models need to generate each token from scratch."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Ablation study",
      "text" : "To analyze where the improvements of RISE come from, we conduct an ablation study on the CANARD and CAsT datasets (see Table 2). We consider two settings: • -DPS. Here, we replace DPS by -greedy sam-\npling ( = 0.2) (Sutton and Barto, 1998). • -MLD. Here, we replace MLD by MLE in\nRISE. The results show that both parts (DPS and MLD) are helpful to RISE as removing either of them leads to a decrease in performance. Without MLD, the performance drops a lot in terms of all metrics, e.g., 3% and 7% in BLEU-4 on CANARD and CAsT, respectively. This indicates that optimizing MLD is more effective than optimizing MLE. Besides, MLD generalizes better on unseen CAsT as it drops slightly in all metrics, while with MLE, we see a drop of 10% in BLEU-1.\nFigure 4: Average number of editing iteration of RISE conditioned on number of tokens in x - y and y - x.\nWithout DPS, the results drop dramatically, which indicates that DPS can do better exploration than -greedy and is of vital importance for RISE. For example, -DPS tends to sample more non ‘K’ edits (RISE vs -DPS: 10% vs 22% on CANARD), which is redundant and fragile. The performance of -DPS is even worse than Origin in CAsT in BLEU4. This may be because CAsT is unseen."
    }, {
      "heading" : "5.2 Editing iterations",
      "text" : "To analyze the relation between the number of editing iterations of RISE and the editing difficulty, we plot a heatmap in Fig. 4, where the deeper color represents a larger number of editing iterations. The x-axis denotes the number of tokens shown in input x but not shown in output y and the y-axis denotes the number of tokens shown in y but not in x.\nAs the number of different tokens between x and y increases, the number of editing iterations increases too. For example, when the y-axis is 1, as the x-axis ranges from 1 to 10, the number of\nediting iterations increases from 1.2 to 2.6 because more ‘D’ edits are needed. We also found that when the x-axis is between 3 and 7 and the y-axis is between 1 and 4, only 1–2 editing iterations are needed. Usually, this is because RISE only needs 1 or 2 successive ‘S’ edits for simulating anaphora."
    }, {
      "heading" : "5.3 Influence of the number of editing iterations",
      "text" : "The overall performance of RISE improves as the number of editing iterations increases. RISE achieves 70.5% in BLEU-4 in the first iteration (even worse than QuerySim in Table 1) but 71.5% and 71.6% in the second and third iterations. This shows that some samples are indeed more difficult to be directly edited into conversational ones, and thus need more editing iterations.\nEven though it will not hurt the performance a lot, more editing iterations are not always helpful. About 5% of the samples achieve worse BLEU-4 scores as the number of editing iterations increases. For example, RISE edits ‘where did humphrey lyttelton go to school at?’ into ‘where did he go to school at?’ in the first iteration, which is perfect. But RISE continues to edit it into ‘where did he go to school?’ in the second iteration, which is undesirable. This is because RISE fails to decide whether to stop or continue editing."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "In Table 3 we present two examples of the output of RISE. We present the context, the original self-contained question, the target conversational question, and the output of RISE in the n-th iteration, denoted as ‘Context’, ‘Question’, ‘Target’ and ‘Rewrite#n’, respectively. We have two main observations. First, it is helpful to edit iteratively. As shown in Example 1, RISE first replaces ‘Abu’ as ‘he’ in the first iteration and then deletes ‘bakr’ in the second iteration, which simulates anaphora by editing twice. In Example 2, RISE simulates el-\nlipsis by deleting multiple words and achieves poor grammar after the first iteration but corrects this by deleting some of the leftover words. RISE may have learned to check the grammar and remove redundant words.\nSecond, RISE can simulate more conversational characteristics than human, and sometimes it can achieve a better result, sometimes not. As we can see, RISE results a better conversational question by additionally simulating anaphora for ‘Abu Bakr’ in Example 1. However, RISE leaves out necessary information in Example 2. Here, RISE tries to simulate conversational characteristics as much as\npossible, where the result may be uncontrollable. In future work, we will add a discriminator to check the necessary information."
    }, {
      "heading" : "6 Related work",
      "text" : "Studies on asking conversational question can be divided into two categories: conversational question generation and conversational question ranking.\nConversational question generation aims to directly generate conversational questions conditioned on the dialogue context (Sultan et al., 2020; Ren et al., 2021a). Zamani et al. (2020) and Qi et al. (2020) define a question utility function to guide the generation of conversational questions. Nakanishi et al. (2019); Jia et al. (2020) incorporate knowledge with auxiliary tasks. These methods may generate irrelevant questions due to their pure generation nature.\nConversational question ranking (Aliannejadi et al., 2019) retrieves questions from a collection based on the given context, so the questions are mostly relevant to the context. Kundu et al. (2020) propose a pair-wise matching network between context and question to do question ranking. Some studies also use auxiliary tasks to improve ranking performance, such as Natural Language Inference (Kumar et al., 2020) and relevance classification (Rosset et al., 2020). The retrieved questions are often unnatural without considering the conversational characteristics, e.g., anaphora and ellipsis.\nCQS rewrites the retrieved self-contained questions into conversational ones by incorporating the conversational characteristics. Existing applicable methods for CQS are all MLE based (Xu et al., 2020; Yu et al., 2020; Lin et al., 2020b; Vakulenko et al., 2020), which often get stuck in easily learned tokens as each token is treated equally by MLE. Instead, we propose a MLD based RISE framework to formulate CQS as a HCMDP, which is able to discriminate different tokens through explicit editing actions, so that it can learn to emphasize the conversational tokens and generate more natural and appropriate questions."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we have proposed a minimum Levenshtein distance (MLD) based Reinforcement Iterative Sequence Editing (RISE) framework for Conversational Question Simplification (CQS). To train RISE, we have devised an Iterative Reinforce Training (IRT) algorithm with a novel Dynamic\nProgramming based Sampling (DPS) process. Extensive experiments show that RISE is more effective and robust than several state-of-the-art CQS methods. A limitation of RISE is that it may fail to decide whether to stop or continue editing and leave out necessary information. In future work, we plan to address this issue by learning a reward function that considers the whole editing process through adversarial learning (Goodfellow et al., 2014).\nCode\nTo facilitate the reproducibility of the results, we share the codes of all methods at https://github. com/LZKSKY/CaSE_RISE."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their valuable feedback. This research was partially supported by the National Key R&D Program of China with grant No. 2020YFB1406704, the Natural Science Foundation of China (61972234, 61902219, 62072279), the Key Scientific and Technological Innovation Program of Shandong Province (2019JZZY010129), the Tencent WeChat Rhino-Bird Focused Research Program (JR-WXG-2021411), the Fundamental Research Funds of Shandong University, and the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https: //hybrid-intelligence-centre.nl.\nAll content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."
    } ],
    "references" : [ {
      "title" : "Asking clarifying questions in open-domain information-seeking conversations",
      "author" : [ "Mohammad Aliannejadi", "Hamed Zamani", "Fabio Crestani", "W. Bruce Croft." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and",
      "citeRegEx" : "Aliannejadi et al\\.,? 2019",
      "shortCiteRegEx" : "Aliannejadi et al\\.",
      "year" : 2019
    }, {
      "title" : "Wasserstein gan",
      "author" : [ "Martin Arjovsky", "Soumith Chintala", "Léon Bottou." ],
      "venue" : "arXiv preprint arXiv:1701.07875.",
      "citeRegEx" : "Arjovsky et al\\.,? 2017",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to ask more: Semi-autoregressive sequential question generation under dual-graph interaction",
      "author" : [ "Zi Chai", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 225–237.",
      "citeRegEx" : "Chai and Wan.,? 2020",
      "shortCiteRegEx" : "Chai and Wan.",
      "year" : 2020
    }, {
      "title" : "Search engines: Information retrieval in practice, volume 520",
      "author" : [ "W Bruce Croft", "Donald Metzler", "Trevor Strohman." ],
      "venue" : "Addison-Wesley Reading.",
      "citeRegEx" : "Croft et al\\.,? 2010",
      "shortCiteRegEx" : "Croft et al\\.",
      "year" : 2010
    }, {
      "title" : "Cast 2019: The conversational assistance track overview",
      "author" : [ "Jeffrey Dalton", "Chenyan Xiong", "Jamie Callan." ],
      "venue" : "Proceedings of the 28th Text REtrieval Conference, TREC 2019, pages 13–15.",
      "citeRegEx" : "Dalton et al\\.,? 2019",
      "shortCiteRegEx" : "Dalton et al\\.",
      "year" : 2019
    }, {
      "title" : "Question generation for question answering",
      "author" : [ "Nan Duan", "Duyu Tang", "Peng Chen", "Ming Zhou." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, pages 866–874.",
      "citeRegEx" : "Duan et al\\.,? 2017",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2017
    }, {
      "title" : "Can you unpack that? Learning to rewrite questions-in-context",
      "author" : [ "Ahmed Elgohary", "Denis Peskov", "Jordan L. BoydGraber." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Elgohary et al\\.,? 2019",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2019
    }, {
      "title" : "Advances and challenges in conversational recommender systems: A survey",
      "author" : [ "Chongming Gao", "Wenqiang Lei", "Xiangnan He", "Maarten de Rijke", "Tat-Seng Chua." ],
      "venue" : "arXiv preprint arXiv:2101.09459.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Generative adversarial networks",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "arXiv arxiv arXiv:1406.2661.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "How to ask good questions? Try to leverage paraphrases",
      "author" : [ "Xin Jia", "Wenjie Zhou", "Xu Sun", "Yunfang Wu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 6130–6140.",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Ranking clarification questions via natural language inference",
      "author" : [ "Vaibhav Kumar", "Vikas Raunak", "Jamie Callan." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information and Knowledge Management, CIKM 2020, pages 2093–2096.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to identify follow-up questions in conversational question answering",
      "author" : [ "Souvik Kundu", "Qian Lin", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 58th Conference of the Association for Computational Linguistics, ACL 2020, pages 959–968.",
      "citeRegEx" : "Kundu et al\\.,? 2020",
      "shortCiteRegEx" : "Kundu et al\\.",
      "year" : 2020
    }, {
      "title" : "Estimation-action-reflection: Towards deep interaction between conversational and recommender systems",
      "author" : [ "Wenqiang Lei", "Xiangnan He", "Yisong Miao", "Qingyun Wu", "Richang Hong", "Min-Yen Kan", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 13th Interna-",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL 2002, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Conversational question reformulation via sequence-to-sequence architectures and pretrained language models",
      "author" : [ "Sheng-Chieh Lin", "Jheng-Hong Yang", "Rodrigo Nogueira", "Ming-Feng Tsai", "Chuan-Ju Wang", "Jimmy Lin." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Lin et al\\.,? 2020a",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Query reformulation using query history for passage retrieval in conversational search",
      "author" : [ "Sheng-Chieh Lin", "Jheng-Hong Yang", "Rodrigo Nogueira", "Ming-Feng Tsai", "Chuan-Ju Wang", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:2005.02230.",
      "citeRegEx" : "Lin et al\\.,? 2020b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards answer-unaware conversational question generation",
      "author" : [ "Mao Nakanishi", "Tetsunori Kobayashi", "Yoshihiko Hayashi." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, MRQA@EMNLP 2019, pages 63–71.",
      "citeRegEx" : "Nakanishi et al\\.,? 2019",
      "shortCiteRegEx" : "Nakanishi et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL 2002, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Anaphora and ellipsis in artificial languages",
      "author" : [ "Stephen G Pulman." ],
      "venue" : "Natural Language Engineering, 1(3):217–234.",
      "citeRegEx" : "Pulman.,? 1995",
      "shortCiteRegEx" : "Pulman.",
      "year" : 1995
    }, {
      "title" : "Stay hungry, stay focused: Generating informative and specific questions in information-seeking conversations",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Open-retrieval conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Cen Chen", "Minghui Qiu", "W. Bruce Croft", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Qu et al\\.,? 2020",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "A theoretical framework for conversational search",
      "author" : [ "Filip Radlinski", "Nick Craswell." ],
      "venue" : "Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, CHIIR 2017, pages 117–126.",
      "citeRegEx" : "Radlinski and Craswell.,? 2017",
      "shortCiteRegEx" : "Radlinski and Craswell.",
      "year" : 2017
    }, {
      "title" : "Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation",
      "author" : [ "Pengjie Ren", "Zhumin Chen", "Christof Monz", "Jun Ma", "Maarten de Rijke." ],
      "venue" : "The Thirty-Fourth AAAI Conference on",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "Conversations with search engines: Serpbased conversational response generation",
      "author" : [ "Pengjie Ren", "Zhumin Chen", "Zhaochun Ren", "Evangelos Kanoulas", "Christof Monz", "Maarten de Rijke." ],
      "venue" : "ACM Transactions on Information Systems (TOIS), 2021.",
      "citeRegEx" : "Ren et al\\.,? 2021a",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "Wizard of search engine: Access to information through conversations with search engines",
      "author" : [ "Pengjie Ren", "Zhongkun Liu", "Xiaomeng Song", "Hongtao Tian", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke." ],
      "venue" : "Proceedings of the 44rd International",
      "citeRegEx" : "Ren et al\\.,? 2021b",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "Leading conversational search by suggesting useful questions",
      "author" : [ "Corbin Rosset", "Chenyan Xiong", "Xia Song", "Daniel Campos", "Nick Craswell", "Saurabh Tiwary", "Paul N. Bennett." ],
      "venue" : "Proceedings of the Web Conference, WWW 2020, pages 1160–1170.",
      "citeRegEx" : "Rosset et al\\.,? 2020",
      "shortCiteRegEx" : "Rosset et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation tasks",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:264–280.",
      "citeRegEx" : "Rothe et al\\.,? 2020",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge-aware attentive neural network for ranking question answer pairs",
      "author" : [ "Ying Shen", "Yang Deng", "Min Yang", "Yaliang Li", "Nan Du", "Wei Fan", "Kai Lei." ],
      "venue" : "Proceedings of the 41st International ACM SIGIR Conference on Research and Development in",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving multi-turn dialogue modelling with utterance rewriter",
      "author" : [ "Hui Su", "Xiaoyu Shen", "Rongzhi Zhang", "Fei Sun", "Pengwei Hu", "Cheng Niu", "Jie Zhou." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "On the importance of diversity in question generation for QA",
      "author" : [ "Md. Arafat Sultan", "Shubham Chandel", "Ramón Fernandez Astudillo", "Vittorio Castelli." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Sultan et al\\.,? 2020",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2020
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto.,? 1998",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Question rewriting for conversational question answering",
      "author" : [ "Svitlana Vakulenko", "Shayne Longpre", "Zhucheng Tu", "Raviteja Anantha." ],
      "venue" : "arXiv preprint arXiv:2004.14652.",
      "citeRegEx" : "Vakulenko et al\\.,? 2020",
      "shortCiteRegEx" : "Vakulenko et al\\.",
      "year" : 2020
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, CVPR, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Query resolution for conversational search with limited supervision",
      "author" : [ "Nikos Voskarides", "Dan Li", "Pengjie Ren", "Evangelos Kanoulas", "Maarten de Rijke." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and development in",
      "citeRegEx" : "Voskarides et al\\.,? 2020",
      "shortCiteRegEx" : "Voskarides et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to ask questions in opendomain conversational systems with typed decoders",
      "author" : [ "Yansen Wang", "Chenyi Liu", "Minlie Huang", "Liqiang Nie." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018,",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence-to-sequence learning as beam-search optimization",
      "author" : [ "Sam Wiseman", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, pages 1296–1306.",
      "citeRegEx" : "Wiseman and Rush.,? 2016",
      "shortCiteRegEx" : "Wiseman and Rush.",
      "year" : 2016
    }, {
      "title" : "Asking clarification questions in knowledgebased question answering",
      "author" : [ "Jingjing Xu", "Yuechen Wang", "Duyu Tang", "Nan Duan", "Pengcheng Yang", "Qi Zeng", "Ming Zhou", "Xu Sun." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic role labeling guided multi-turn dialogue rewriter",
      "author" : [ "Kun Xu", "Haochen Tan", "Linfeng Song", "Han Wu", "Haisong Zhang", "Linqi Song", "Dong Yu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot generative conversational query rewriting",
      "author" : [ "Shi Yu", "Jiahua Liu", "Jingqin Yang", "Chenyan Xiong", "Paul N. Bennett", "Jianfeng Gao", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and develop-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Macaw: An extensible conversational information seeking platform",
      "author" : [ "Hamed Zamani", "Nick Craswell." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2020, pages 2193–",
      "citeRegEx" : "Zamani and Craswell.,? 2020",
      "shortCiteRegEx" : "Zamani and Craswell.",
      "year" : 2020
    }, {
      "title" : "Generating clarifying questions for information retrieval",
      "author" : [ "Hamed Zamani", "Susan T. Dumais", "Nick Craswell", "Paul N. Bennett", "Gord Lueck." ],
      "venue" : "Proceedings of the Web Conference 2020, WWW 2020, pages 418–428.",
      "citeRegEx" : "Zamani et al\\.,? 2020",
      "shortCiteRegEx" : "Zamani et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "Conversational information seeking (CIS) (Zamani and Craswell, 2020; Ren et al., 2021b) has received extensive attention.",
      "startOffset" : 41,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "Conversational information seeking (CIS) (Zamani and Craswell, 2020; Ren et al., 2021b) has received extensive attention.",
      "startOffset" : 41,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "It introduces a new way to connect people to information through conversations (Qu et al., 2020; Gao et al., 2021; Ren et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "One of the key features of CIS is mixed initiative behavior, where a system can improve user satisfaction by proactively asking clarification questions (Zhang et al., 2018; Aliannejadi et al., 2019; Xu et al., 2019), besides passively providing answers (Croft et al.",
      "startOffset" : 152,
      "endOffset" : 215
    }, {
      "referenceID" : 40,
      "context" : "One of the key features of CIS is mixed initiative behavior, where a system can improve user satisfaction by proactively asking clarification questions (Zhang et al., 2018; Aliannejadi et al., 2019; Xu et al., 2019), besides passively providing answers (Croft et al.",
      "startOffset" : 152,
      "endOffset" : 215
    }, {
      "referenceID" : 3,
      "context" : ", 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : ", 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : ", 2019), besides passively providing answers (Croft et al., 2010; Radlinski and Craswell, 2017; Lei et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : "Previous studies on asking clarification questions can be grouped into two categories: conversational question generation (Duan et al., 2017) and conversational question ranking (Aliannejadi et al.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : ", 2017) and conversational question ranking (Aliannejadi et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "However, the generated questions may be irrelevant and meaningless (Rosset et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 38,
      "context" : "A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan, 2020).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "A lack of explicit semantic guidance makes it difficult to produce each question token from scratch while preserving relevancy and usefulness at the same time (Wang et al., 2018; Chai and Wan, 2020).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 31,
      "context" : "Instead, the latter proposes to retrieve questions from a collection for the given dialogue context, which can usually guarantee that the questions are relevant and useful (Shen et al., 2018; Rosset et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 212
    }, {
      "referenceID" : 29,
      "context" : "Instead, the latter proposes to retrieve questions from a collection for the given dialogue context, which can usually guarantee that the questions are relevant and useful (Shen et al., 2018; Rosset et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 212
    }, {
      "referenceID" : 21,
      "context" : "However, question ranking methods do not lead to a natural communication between human and machine (Pulman, 1995), as they neglect important characteristics in conversations, e.",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : ", Conversational Question Rewriting (CQR) (Elgohary et al., 2019; Voskarides et al., 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al.",
      "startOffset" : 42,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : ", Conversational Question Rewriting (CQR) (Elgohary et al., 2019; Voskarides et al., 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al.",
      "startOffset" : 42,
      "endOffset" : 90
    }, {
      "referenceID" : 35,
      "context" : ", 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al., 2020; Yu et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : ", 2020) which rewrites CQ4 into SQ4, has been widely explored in the literature (Vakulenko et al., 2020; Yu et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 121
    }, {
      "referenceID" : 34,
      "context" : "Traditional exploration methods like -sampling (Sutton and Barto, 1998) neglect the interdependency between edits for all tokens, resulting in poor exploration.",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "Experiments on the CANARD (Elgohary et al., 2019) and CAsT (Dalton et al.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : ", 2019) and CAsT (Dalton et al., 2019) datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well to unseen data.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "We implement the editing and phrasing policy networks (πe and πp) based on BERT2BERT (Rothe et al., 2020) as shown in Fig.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "As with previous studies (Elgohary et al., 2019; Yu et al., 2020; Vakulenko et al., 2020; Lin et al., 2020a), we conduct experiments on the CANARD1 (Elgohary et al.",
      "startOffset" : 25,
      "endOffset" : 108
    }, {
      "referenceID" : 42,
      "context" : "As with previous studies (Elgohary et al., 2019; Yu et al., 2020; Vakulenko et al., 2020; Lin et al., 2020a), we conduct experiments on the CANARD1 (Elgohary et al.",
      "startOffset" : 25,
      "endOffset" : 108
    }, {
      "referenceID" : 35,
      "context" : "As with previous studies (Elgohary et al., 2019; Yu et al., 2020; Vakulenko et al., 2020; Lin et al., 2020a), we conduct experiments on the CANARD1 (Elgohary et al.",
      "startOffset" : 25,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "As with previous studies (Elgohary et al., 2019; Yu et al., 2020; Vakulenko et al., 2020; Lin et al., 2020a), we conduct experiments on the CANARD1 (Elgohary et al.",
      "startOffset" : 25,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : ", 2020a), we conduct experiments on the CANARD1 (Elgohary et al., 2019) dataset, which is a large open-domain dataset for conversational question answering (with over 30k training samples).",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "In addition, we evaluate the model performance on the CAsT2 dataset (Dalton et al., 2019), which is built for conversational search.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "(2020), we use BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : ", 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : ", 2002), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015) for automatic evaluation.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 42,
      "context" : "• Rule (Yu et al., 2020) employs two simple rules to mimic two conversational characteristics: anaphora and ellipsis.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : ", 2020) uses RoBERTa (Liu et al., 2019) with beam search (Wiseman and Rush, 2016) for generation.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 39,
      "context" : ", 2019) with beam search (Wiseman and Rush, 2016) for generation.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 35,
      "context" : "• Trans++ (Vakulenko et al., 2020) predicts several word distributions, and combines them to obtain the final word distribution when generating each token.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 42,
      "context" : "• QuerySim (Yu et al., 2020) adopts a GPT2 (Radford et al.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : ", 2020) adopts a GPT2 (Radford et al., 2019) model to generate conversational question.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "We use BERT2BERT for the modeling of the editing and phrasing parts (Rothe et al., 2020), as other pretrained models like GPT-2 (Radford et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : ", 2020), as other pretrained models like GPT-2 (Radford et al., 2019) cannot work for both.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "The hidden size is 768 and phrase vocabulary is 3461 following (Malmi et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "We use the Adam optimizer (learning rate 5e-5) (Kingma and Ba, 2015) to train all models.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "Conversational question generation aims to directly generate conversational questions conditioned on the dialogue context (Sultan et al., 2020; Ren et al., 2021a).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 27,
      "context" : "Conversational question generation aims to directly generate conversational questions conditioned on the dialogue context (Sultan et al., 2020; Ren et al., 2021a).",
      "startOffset" : 122,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "Conversational question ranking (Aliannejadi et al., 2019) retrieves questions from a collection based on the given context, so the questions are mostly relevant to the context.",
      "startOffset" : 32,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "Some studies also use auxiliary tasks to improve ranking performance, such as Natural Language Inference (Kumar et al., 2020) and relevance classification (Rosset et al.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : ", 2020) and relevance classification (Rosset et al., 2020).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "Existing applicable methods for CQS are all MLE based (Xu et al., 2020; Yu et al., 2020; Lin et al., 2020b; Vakulenko et al., 2020), which often get stuck in easily learned tokens as each token is treated equally by MLE.",
      "startOffset" : 54,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "Existing applicable methods for CQS are all MLE based (Xu et al., 2020; Yu et al., 2020; Lin et al., 2020b; Vakulenko et al., 2020), which often get stuck in easily learned tokens as each token is treated equally by MLE.",
      "startOffset" : 54,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Existing applicable methods for CQS are all MLE based (Xu et al., 2020; Yu et al., 2020; Lin et al., 2020b; Vakulenko et al., 2020), which often get stuck in easily learned tokens as each token is treated equally by MLE.",
      "startOffset" : 54,
      "endOffset" : 131
    }, {
      "referenceID" : 35,
      "context" : "Existing applicable methods for CQS are all MLE based (Xu et al., 2020; Yu et al., 2020; Lin et al., 2020b; Vakulenko et al., 2020), which often get stuck in easily learned tokens as each token is treated equally by MLE.",
      "startOffset" : 54,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "In future work, we plan to address this issue by learning a reward function that considers the whole editing process through adversarial learning (Goodfellow et al., 2014).",
      "startOffset" : 146,
      "endOffset" : 171
    } ],
    "year" : 2021,
    "abstractText" : "Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.",
    "creator" : "LaTeX with hyperref"
  }
}