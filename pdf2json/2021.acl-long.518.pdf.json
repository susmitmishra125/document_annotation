{
  "name" : "2021.acl-long.518.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Dense Representations of Phrases at Scale",
    "authors" : [ "Jinhyuk Lee", "Mujeen Sung", "Jaewoo Kang", "Danqi Chen" ],
    "emails" : [ "jinhyuk_lee@korea.ac.kr", "mujeensung@korea.ac.kr", "kangj@korea.ac.kr", "danqic@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6634–6647\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6634"
    }, {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus (Voorhees et al., 1999; Ferrucci et al., 2010; Chen and Yih, 2020). While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on\n∗Work partly done while visiting Princeton University. 1Our code is available at https://github.com/\nprinceton-nlp/DensePhrases.\na recent new paradigm solely based on phrase retrieval (Seo et al., 2019; Lee et al., 2020). Phrase retrieval highlights the use of phrase representations and finds answers purely based on the similarity search in the vector space of phrases.2 Without relying on an expensive reader model for processing text passages, it has demonstrated great runtime efficiency at inference time.\nDespite great promise, it remains a formidable challenge to build vector representations for every single phrase in a large corpus. Since phrase representations are decomposed from question representations, they are inherently less expressive than cross-attention models (Devlin et al., 2019). Moreover, the approach requires retrieving answers correctly out of billions of phrases (e.g., 6× 1010 phrases in English Wikipedia), making the scale of the learning problem difficult. Consequently, existing approaches heavily rely on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models (Seo et al., 2019; Lee et al., 2020).\nIn this work, we investigate whether we can build fully dense phrase representations at scale for opendomain QA. First, we aim to learn strong phrase representations from the supervision of reading comprehension tasks. We propose to use data augmentation and knowledge distillation to learn better phrase representations within a single passage. We then adopt negative sampling strategies such as inbatch negatives (Henderson et al., 2017; Karpukhin et al., 2020), to better discriminate the phrases at a larger scale. Here, we present a novel method called pre-batch negatives, which leverages preceding mini-batches as negative examples to compensate the need of large-batch training. Lastly, we present a query-side fine-tuning strategy that dras-\n2Following previous work (Seo et al., 2018), ‘phrase’ denotes any contiguous segment of text up to L words (including single words), which is not necessarily a linguistic phrase.\ntically improves phrase retrieval performance and allows for transfer learning to new domains, without re-building billions of phrase representations.\nAs a result, all these improvements lead to a much stronger phrase retrieval model, without the use of any sparse representations (Table 1). We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019; Lee et al., 2020), with 15%– 25% absolute improvement on most datasets. Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020; Karpukhin et al., 2020). Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.\nFinally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data."
    }, {
      "heading" : "2 Background",
      "text" : "We first formulate the task of open-domain question answering for a set of K documents D = {d1, . . . , dK}. We follow the recent work (Chen et al., 2017; Lee et al., 2019) and treat all of English Wikipedia as D, hence K ≈ 5 × 106. However,\nmost approaches—including ours—are generic and could be applied to other collections of documents.\nThe task aims to provide an answer â for the input question q based on D. In this work, we focus on the extractive QA setting, where each answer is a segment of text, or a phrase, that can be found in D. Denote the set of phrases inD as S(D) and each phrase sk ∈ S(D) consists of contiguous words wstart(k), . . . , wend(k) in its document ddoc(k). In practice, we consider all the phrases up to L = 20 words in D and S(D) comprises a large number of 6× 1010 phrases. An extractive QA system returns a phrase ŝ = argmaxs∈S(D) f(s|D, q) where f is a scoring function. The system finally maps ŝ to an answer string â: TEXT(ŝ) = â and the evaluation is typically done by comparing the predicted answer â with a gold answer a∗.\nAlthough we focus on the extractive QA setting, recent works propose to use a generative model as the reader (Lewis et al., 2020; Izacard and Grave, 2021), or learn a closed-book QA model (Roberts et al., 2020), which directly predicts answers without using an external knowledge source. The extractive setting provides two advantages: first, the model directly locates the source of the answer, which is more interpretable, and second, phraselevel knowledge retrieval can be uniquely adapted to other NLP tasks as we show in §7.3.\nRetriever-reader. A dominating paradigm in open-domain QA is the retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020), which leverages a firststage document retriever fretr and only reads top K ′ K documents with a reader model fread. The scoring function f(s | D, q) is decomposed as:\nf(s | D, q) = fretr({dj1 , . . . , djK′} | D, q) × fread(s | {dj1 , . . . , djK′}, q), (1)\nwhere {j1, . . . , jK′} ⊂ {1, . . . ,K} and if s /∈ S({dj1 , . . . , djK′}), the score will be 0. It can easily adapt to passages and sentences (Yang et al., 2019; Wang et al., 2019). However, this approach suffers from error propagation when incorrect documents are retrieved and can be slow as it usually requires running an expensive reader model on every retrieved document or passage at inference time.\nPhrase retrieval. Seo et al. (2019) introduce the phrase retrieval approach that encodes phrase and question representations independently and performs similarity search over the phrase representations to find an answer. Their scoring function f is computed as follows:\nf(s | D, q) = Es(s,D)>Eq(q), (2)\nwhere Es and Eq denote the phrase encoder and the question encoder respectively. As Es(·) and Eq(·) representations are decomposable, it can support maximum inner product search (MIPS) and improve the efficiency of open-domain QA models. Previous approaches (Seo et al., 2019; Lee et al., 2020) leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: Es(s,D) = [Esparse(s,D), Edense(s,D)].3 However, since the sparse vectors are difficult to parallelize with dense vectors, their method essentially conducts sparse and dense vector search separately. The goal of this work is to only use dense representations, i.e., Es(s,D) = Edense(s,D), which can model f(s | D, q) solely with MIPS, as well as close the gap in performance."
    }, {
      "heading" : "3 DensePhrases",
      "text" : ""
    }, {
      "heading" : "3.1 Overview",
      "text" : "We introduce DensePhrases, a phrase retrieval model that is built on fully dense representations. Our goal is to learn a phrase encoder as well as a question encoder, so we can pre-index all the possible phrases in D, and efficiently retrieve phrases for any question through MIPS at testing time. We outline our approach as follows:\n3Seo et al. (2019) use sparse representations of both paragraphs and documents and Lee et al. (2020) use contextualized sparse representations conditioned on the phrase.\n• We first learn a high-quality phrase encoder and an (initial) question encoder from the supervision of reading comprehension tasks (§4.1), as well as incorporating effective negative sampling to better discriminate phrases at scale (§4.2, §4.3). • Then, we fix the phrase encoder and encode all the phrases s ∈ S(D) and store the phrase indexing offline to enable efficient search (§5). • Finally, we introduce an additional strategy called query-side fine-tuning (§6) by further updating the question encoder.4 We find this step to be very effective, as it can reduce the discrepancy between training (the first step) and inference, as well as support transfer learning to new domains.\nBefore we present the approach in detail, we first describe our base architecture below."
    }, {
      "heading" : "3.2 Base Architecture",
      "text" : "Our base architecture consists of a phrase encoder Es and a question encoder Eq. Given a passage p = w1, . . . , wm, we denote all the phrases up to L tokens as S(p). Each phrase sk has start and end indicies start(k) and end(k) and the gold phrase is s∗ ∈ S(p). Following previous work on phrase or span representations (Lee et al., 2017; Seo et al., 2018), we first apply a pre-trained language model Mp to obtain contextualized word representations for each passage token: h1, . . . ,hm ∈ Rd. Then, we can represent each phrase sk ∈ S(p) as the concatenation of corresponding start and end vectors:\nEs(sk, p) = [hstart(k),hend(k)] ∈ R2d. (3)\nA great advantage of this representation is that we eventually only need to index and store all the word vectors (we useW(D) to denote all the words in D), instead of all the phrases S(D), which is at least one magnitude order smaller.\nSimilarly, we need to learn a question encoder Eq(·) that maps a question q = w̃1, . . . , w̃n to a vector of the same dimension as Es(·). Since the start and end representations of phrases are produced by the same language model, we use another two different pre-trained encoders Mq,start and Mq,end to differentiate the start and end positions. We apply Mq,start and Mq,end on q separately and obtain representations qstart and qend\n4In this paper, we use the term question and query interchangeably as our question encoder can be naturally extended to “unnatural” queries.\ntaken from the [CLS] token representations respectively. Finally, Eq(·) simply takes their concatenation:\nEq(q) = [q start,qend] ∈ R2d. (4)\nNote that we use pre-trained language models to initialize Mp, Mq,start and Mq,end and they are fine-tuned with the objectives that we will define later. In our pilot experiments, we found that SpanBERT (Joshi et al., 2020) leads to superior performance compared to BERT (Devlin et al., 2019). SpanBERT is designed to predict the information in the entire span from its two endpoints, therefore it is well suited for our phrase representations. In our final model, we use SpanBERT-base-cased as our base LMs for Es and Eq, and hence d = 768.5 See Table 5 for an ablation study."
    }, {
      "heading" : "4 Learning Phrase Representations",
      "text" : "In this section, we start by learning dense phrase representations from the supervision of reading comprehension tasks, i.e., a single passage p contains an answer a∗ to a question q. Our goal is to learn strong dense representations of phrases for s ∈ S(p), which can be retrieved by a dense representation of the question and serve as a direct\n5Our base model is largely inspired by DenSPI (Seo et al., 2019), although we deviate from theirs as follows. (1) We remove coherency scalars and don’t split any vectors. (2) DenSPI uses a shared encoder for phrases and questions while we use 3 separate language models initialized from the same pre-trained model. (3) We use SpanBERT instead of BERT.\nanswer (§4.1). Then, we introduce two different negative sampling methods (§4.2, §4.3), which encourage the phrase representations to be better discriminated at the full Wikipedia scale. See Figure 1 for an overview of DensePhrases."
    }, {
      "heading" : "4.1 Single-passage Training",
      "text" : "To learn phrase representations in a single passage along with question representations, we first maximize the log-likelihood of the start and end positions of the gold phrase s∗ where TEXT(s∗) = a∗. The training loss for predicting the start position of a phrase given a question is computed as:\nzstart1 , . . . , z start m = [h > 1 q start, . . . ,h>mq start],\nP start = softmax(zstart1 , . . . , z start m ),\nLstart = − logP startstart(s∗). (5)\nWe can define Lend in a similar way and the final loss for the single-passage training is\nLsingle = Lstart + Lend\n2 . (6)\nThis essentially learns reading comprehension without any cross-attention between the passage and the question tokens, which fully decomposes phrase and question representations.\nData augmentation Since the contextualized word representations h1, . . . ,hm are encoded in a query-agnostic way, they are always inferior to\nquery-dependent representations in cross-attention models (Devlin et al., 2019), where passages are fed along with the questions concatenated by a special token such as [SEP]. We hypothesize that one key reason for the performance gap is that reading comprehension datasets only provide a few annotated questions in each passage, compared to the set of possible answer phrases. Learning from this supervision is not easy to differentiate similar phrases in one passage (e.g., s∗ = Charles, Prince of Wales and another s = Prince George for a question q = Who is next in line to be the monarch of England?).\nFollowing this intuition, we propose to use a simple model to generate additional questions for data augmentation, based on a T5-large model (Raffel et al., 2020). To train the question generation model, we feed a passage p with the gold answer s∗ highlighted by inserting surrounding special tags. Then, the model is trained to maximize the log-likelihood of the question words of q. After training, we extract all the named entities in each training passage as candidate answers and feed the passage p with each candidate answer to generate questions. We keep the questionanswer pairs only when a cross-attention reading comprehension model6 makes a correct prediction on the generated pair. The remaining generated QA pairs {(q̄1, s̄1), (q̄2, s̄2), . . . , (q̄r, s̄r)} are directly augmented to the original training set.\nDistillation We also propose improving the phrase representations by distilling knowledge from a cross-attention model (Hinton et al., 2015). We minimize the Kullback–Leibler divergence between the probability distribution from our phrase encoder and that from a standard SpanBERT-base QA model. The loss is computed as follows:\nLdistill = KL(P start||P startc ) + KL(P end||P endc )\n2 ,\n(7) where P start (and P end) is defined in Eq. (5) and P startc and P end c denote the probability distributions used to predict the start and end positions of answers in the cross-attention model."
    }, {
      "heading" : "4.2 In-batch Negatives",
      "text" : "Eventually, we need to build phrase representations for billions of phrases. Therefore, a bigger challenge is to incorporate more phrases as negatives so the representations can be better discriminated\n6SpanBERT-large, 88.2 EM on SQuAD.\nat a larger scale. While Seo et al. (2019) simply sample two negative passages based on question similarity, we use in-batch negatives for our dense phrase representations, which has been shown to be effective in learning dense passage representations before (Karpukhin et al., 2020).\nAs shown in Figure 2 (a), for the i-th example in a mini-batch of size B, we denote the hidden representations of the gold start and end positions hstart(s∗) and hend(s∗) as gstarti and gendi , as well as the question representation as [qstarti ,q end i ]. Let G\nstart,Gend,Qstart,Qend be the B × d matrices and each row corresponds to gstarti ,g end i ,q start i ,q end i respectively. Basically, we can treat all the gold phrases from other passages in the same mini-batch as negative examples. We compute Sstart = QstartGstartᵀ and Send = QendGend\nᵀ and the i-th row of Sstart and Send return B scores each, including a positive score and B−1 negative scores: sstart1 , . . . , s start B and s end 1 , . . . , s end B . Similar to Eq. (5), we can compute the loss function for the i-th example as:\nP start_ibi = softmax(s start 1 , . . . , s start B ),\nP end_ibi = softmax(s end 1 , . . . , s end B ),\nLneg = − logP start_ibi + logP end_ib i\n2 ,\n(8)\nWe also attempted using non-gold phrases from other passages as negatives but did not find a meaningful improvement."
    }, {
      "heading" : "4.3 Pre-batch Negatives",
      "text" : "The in-batch negatives usually benefit from a large batch size (Karpukhin et al., 2020). However, it is challenging to further increase batch sizes, as they are bounded by the size of GPU memory. Next, we propose a novel negative sampling method\ncalled pre-batch negatives, which can effectively utilize the representations from the preceding C mini-batches (Figure 2 (b)). In each iteration, we maintain a FIFO queue of C mini-batches to cache phrase representations Gstart and Gend. The cached phrase representations are then used as negative samples for the next iteration, providing B × C additional negative samples in total.7\nThese pre-batch negatives are used together with in-batch negatives and the training loss is the same as Eq. (8), except that the gradients are not backpropagated to the cached pre-batch negatives. After warming up the model with in-batch negatives, we simply shift from in-batch negatives (B − 1 negatives) to in-batch and pre-batch negatives (hence a total number ofB×C+B−1 negatives). For simplicity, we use Lneg to denote the loss for both inbatch negatives and pre-batch negatives. Since we do not retain the computational graph for pre-batch negatives, the memory consumption of pre-batch negatives is much more manageable while allowing an increase in the number of negative samples."
    }, {
      "heading" : "4.4 Training Objective",
      "text" : "Finally, we optimize all the three losses together, on both annotated reading comprehension examples and generated questions from §4.1:\nL = λ1Lsingle + λ2Ldistill + λ3Lneg, (9)\nwhere λ1, λ2, λ3 determine the importance of each loss term. We found that λ1 = 1, λ2 = 2, and λ3 = 4 works well in practice. See Table 5 and Table 6 for an ablation study of different components."
    }, {
      "heading" : "5 Indexing and Search",
      "text" : "Indexing After training the phrase encoder Es, we need to encode all the phrases S(D) in the entire English Wikipedia D and store an index of the phrase dump. We segment each document di ∈ D into a set of natural paragraphs, from which we obtain token representations for each paragraph using Es(·). Then, we build a phrase dump H = [h1, . . . ,h|W(D)|] ∈ R|W(D)|×d by stacking the token representations from all the paragraphs in D. Note that this process is computationally expensive and takes about hundreds of GPU hours with a large disk footprint. To reduce the\n7This approach is inspired by the momentum contrast idea proposed in unsupervised visual representation learning (He et al., 2020). Contrary to their approach, we have separate encoders for phrases and questions and back-propagate to both during training without a momentum update.\nsize of phrase dump, we follow and modify several techniques introduced in Seo et al. (2019) (see Appendix E for details). After indexing, we can use two rows i and j of H to represent a dense phrase representation [hi,hj ]. We use faiss (Johnson et al., 2017) for building a MIPS index of H.8\nSearch For a given question q, we can find the answer ŝ as follows:\nŝ = argmax s(i,j)\nEs(s(i,j),D)>Eq(q),\n= argmax s(i,j)\n(Hqstart)i + (Hq end)j ,\n(10)\nwhere s(i,j) denotes a phrase with start and end indices as i and j in the index H. We can compute the argmax of Hqstart and Hqend efficiently by performing MIPS over H with qstart and qend. In practice, we search for the top-k start and top-k end positions separately and perform a constrained search over their end and start positions respectively such that 1 ≤ i ≤ j < i+ L ≤ |W(D)|."
    }, {
      "heading" : "6 Query-side Fine-tuning",
      "text" : "So far, we have created a phrase dump H that supports efficient MIPS search. In this section, we propose a novel method called query-side fine-tuning by only updating the question encoder Eq to correctly retrieve a desired answer a∗ for a question q given H. Formally speaking, we optimize the marginal log-likelihood of the gold answer a∗ for a question q, which resembles the weakly-supervised QA setting in previous work (Lee et al., 2019; Min et al., 2019). For every question q, we retrieve top k phrases and minimize the objective:\nLquery = − log ∑\ns∈S̃(q),TEXT(s)=a∗ exp ( f(s|D,q) ) ∑\ns∈S̃(q) exp ( f(s|D,q) ) , (11)\nwhere f(s|D, q) is the score of the phrase s (Eq. (2)) and S̃(q) denotes the top k phrases for q (Eq. (10)). In practice, we use k = 100 for all the experiments.\nThere are several advantages for doing this: (1) we find that query-side fine-tuning can reduce the discrepancy between training and inference, and hence improve the final performance substantially (§8). Even with effective negative sampling, the model only sees a small portion of passages compared to the full scale of D and this training objective can effectively fill in the gap. (2) This training strategy allows for transfer learning to unseen\n8We use IVFSQ4 with 1M clusters and set n-probe to 256.\ndomains, without rebuilding the entire phrase index. More specifically, the model is able to quickly adapt to new QA tasks (e.g., WebQuestions) when the phrase dump is built using SQuAD or Natural Questions. We also find that this can transfers to non-QA tasks when the query is written in a different format. In §7.3, we show the possibility of directly using DensePhrases for slot filling tasks by using a query such as (Michael Jackson, is a singer of, x). In this regard, we can view our model as a dense knowledge base that can be accessed by many different types of queries and it is able to return phrase-level knowledge efficiently."
    }, {
      "heading" : "7 Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 Setup",
      "text" : "Datasets. We use two reading comprehension datasets: SQuAD (Rajpurkar et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019) to learn phrase representations, in which a single gold passage is provided for each question. For the opendomain QA experiments, we evaluate our approach on five popular open-domain QA datasets: Natural Questions, WebQuestions (WQ) (Berant et al., 2013), CuratedTREC (TREC) (Baudiš and Šedivỳ, 2015), TriviaQA (TQA) (Joshi et al., 2017), and SQuAD. Note that we only use SQuAD and/or NQ to build the phrase index and perform query-side fine-tuning (§6) for other datasets.\nWe also evaluate our model on two slot filling tasks, to show how to adapt our DensePhrases for other knowledge-intensive NLP tasks. We focus on using two slot filling datasets from the KILT benchmark (Petroni et al., 2021): T-REx (Elsahar et al., 2018) and zero-shot relation extraction (Levy et al., 2017). Each query is provided in the form of “{subject entity} [SEP] {relation}\" and the answer is the object entity. Appendix C provides the statistics of all the datasets.\nImplementation details. We denote the training datasets used for reading comprehension (Eq. (9)) as Cphrase. For open-domain QA, we train two versions of phrase encoders, each of which are trained on Cphrase = {SQuAD} and {NQ,SQuAD}, respectively. We build the phrase dump H for the 2018-12-20 Wikipedia snapshot and perform queryside fine-tuning on each dataset using Eq. (11). For slot filling, we use the same phrase dump for opendomain QA, Cphrase = {NQ,SQuAD} and perform query-side fine-tuning on randomly sampled 5K\nor 10K training examples to see how rapidly our model adapts to the new query types. See Appendix D for details on the hyperparameters and Appendix A for an analysis of computational cost."
    }, {
      "heading" : "7.2 Experiments: Question Answering",
      "text" : "Reading comprehension. In order to show the effectiveness of our phrase representations, we first evaluate our model in the reading comprehension setting for SQuAD and NQ and report its performance with other query-agnostic models (Eq. (9) without query-side fine-tuning). This problem was originally formulated by Seo et al. (2018) as the phrase-indexed question answering (PIQA) task.\nCompared to previous query-agnostic models, our model achieves the best performance of 78.3 EM on SQuAD by improving the previous phrase retrieval model (DenSPI) by 4.7% (Table 2). Although it is still behind cross-attention models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.\nOpen-domain QA. Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%–25% absolute improvement on all datasets except SQuAD. Training the model of Lee et al. (2020) on Cphrase = {NQ,SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models (Karpukhin et al., 2020), while running much faster during inference (Table 1)."
    }, {
      "heading" : "7.3 Experiments: Slot Filling",
      "text" : "Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples."
    }, {
      "heading" : "8 Analysis",
      "text" : "Ablation of phrase representations. Table 5 shows the ablation result of our model on SQuAD. Upon our choice of architecture, augmenting training set with generated questions (QG = 3) and performing distillation from cross-attention models (Distill = 3) improve performance up to EM = 78.3. We attempted adding the generated questions to the training of the SpanBERT-QA model but find a 0.3% improvement, which validates that data sparsity is a bottleneck for query-agnostic models.\nEffect of batch negatives. We further evaluate the effectiveness of various negative sampling methods introduced in §4.2 and §4.3. Since it is computationally expensive to test each setting at the full Wikipedia scale, we use a smaller text corpus Dsmall of all the gold passages in the development sets of Natural Questions, for the ablation study. Empirically, we find that results are generally well correlated when we gradually increase the size of |D|. As shown in Table 6, both in-batch and pre-batch negatives bring substantial improvements. While using a larger batch size (B = 84) is beneficial for in-batch negatives, the number of preceding batches in pre-batch negatives is optimal when C = 2. Surprisingly, the pre-batch negatives also improve the performance when D = {p}.\nEffect of query-side fine-tuning. We summarize the effect of query-side fine-tuning in Table 7. For the datasets that were not used for training the phrase encoders (TQA, WQ, TREC), we observe a 15% to 20% improvement after query-side finetuning. Even for the datasets that have been used (NQ, SQuAD), it leads to significant improvements (e.g., 32.6%→40.9% on NQ for Cphrase = {NQ}) and it clearly demonstrates it can effectively reduce the discrepancy between training and inference."
    }, {
      "heading" : "9 Related Work",
      "text" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019). Beyond words, dense representations of many different granularities of text such as sentences (Le and Mikolov, 2014; Kiros et al., 2015) or documents (Yih et al., 2011) have been explored. While dense phrase representations have been also studied for statistical machine translation (Cho et al., 2014) or syntactic parsing (Socher et al., 2010), our work focuses on learning dense phrase representations for QA and any other knowledge-intensive tasks where phrases can be easily retrieved by performing MIPS.\nThis type of dense retrieval has been also studied for sentence and passage retrieval (Humeau et al., 2019; Karpukhin et al., 2020) (see Lin et al., 2020 for recent advances in dense retrieval). While DensePhrases is explicitly designed to retrieve phrases that can be used as an answer to given queries, retrieving phrases also naturally entails retrieving larger units of text, provided the datastore maintains the mapping between each phrase and the sentence and passage in which it occurs."
    }, {
      "heading" : "10 Conclusion",
      "text" : "In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks. We learn both phrase and question encoders from the supervision of reading comprehension tasks and introduce two batch-negative techniques to better discriminate phrases at scale. We also introduce query-side fine-tuning that adapts our model to different types of queries. We achieve strong performance on five popular open-domain QA datasets, while reducing the storage footprint and improving latency significantly. We also achieve strong performance on two slot filling datasets using only a small number of training examples, showing the possibility of utilizing our DensePhrases as a knowledge base."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Sewon Min, Hyunjae Kim, Gyuwan Kim, Jungsoo Park, Zexuan Zhong, Dan Friedman, Chris Sciavolino for providing valuable comments and feedback. This research was supported by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HR20C0021) and National Research Foundation of Korea (NRF-2020R1A2C3010638). It was also partly supported by the James Mi *91 Research Innovation Fund for Data Science and an Amazon Research Award.\nEthical Considerations\nOur work builds on standard reading comprehension datasets such as SQuAD to build phrase representations. SQuAD, in particular, is created from a small number of Wikipedia articles sampled from top-10,000 most popular articles (measured by PageRanks), hence some of our models trained only on SQuAD could be easily biased towards the small number of topics that SQuAD contains. We hope that excluding such datasets during training or inventing an alternative pre-training procedure for learning phrase representations could mitigate this problem. Although most of our efforts have been made to reduce the computational complexity of previous phrase retrieval models (further detailed in Appendices A and E), leveraging our phrase retrieval model as a knowledge base will inevitably increase the minimum requirement for the additional experiments. We plan to apply vector quantization techniques to reduce the additional cost of using our model as a KB."
    }, {
      "heading" : "A Computational Cost",
      "text" : "We describe the resources and time spent during inference (Table 1 and A.1) and indexing (Table A.1). With our limited GPU resources (24GB × 4), it takes about 20 hours for indexing the entire phrase representations. We also largely reduced the storage from 1,547GB to 320GB by (1) removing sparse representations and (2) using our sharing and split strategy. See Appendix E for the details on the reduction of storage footprint and Appendix B for the specification of our server for the benchmark."
    }, {
      "heading" : "B Server Specifications for Benchmark",
      "text" : "To compare the complexity of open-domain QA models, we install all models in Table 1 on the same server using their public open-source code. Our server has the following specifications:\nFor DPR, due to its large memory consumption, we use a similar server with a 24GB GPU (TITAN RTX). For all models, we use 1,000 randomly sampled questions from the Natural Questions development set for the speed benchmark and measure #Q/sec. We set the batch size to 64 for all models except BERTSerini, ORQA and REALM, which do not allow a batch size of more than 1 in their open-source implementations. #Q/sec for DPR includes retrieving passages and running a reader\nmodel and the batch size for the reader model is set to 8 to fit in the 24GB GPU (retriever batch size is still 64). For other hyperparameters, we use the default settings of each model. We also exclude the time and the number of questions in the first five iterations for warming up each model. Note that despite our effort to match the environment of each model, their latency can be affected by various different settings in their implementations such as the choice of library (PyTorch vs. Tensorflow)."
    }, {
      "heading" : "C Data Statistics and Pre-processing",
      "text" : "In Table C.3, we show the statistics of five opendomain QA datasets and two slot filling datasets. Pre-processed open-domain QA datasets are provided by Chen et al. (2017) except Natural Questions and TriviaQA. We use a version of Natural Questions and TriviaQA provided by Min et al. (2019); Lee et al. (2019), which are pre-processed for the open-domain QA setting. Slot filling datasets are provided by Petroni et al. (2021). We use two reading comprehension datasets (SQuAD and Natural Questions) for training our model on Eq. (9). For SQuAD, we use the original dataset provided by the authors (Rajpurkar et al., 2016). For Natural Questions (Kwiatkowski et al., 2019), we use the pre-processed version provided by Asai et al. (2020).9 We use the short answer as a ground truth answer a∗ and its long answer as a gold passage p. We also match the gold passages in Natural Questions to the paragraphs in Wikipedia whenever possible. Since we want to check the performance changes of our model with the growing number of tokens, we follow the same split (train/dev/test) used in Natural Questions-Open for the reading comprehension setting as well. During the valida-\n9https://github.com/AkariAsai/ learning_to_retrieve_reasoning_paths\ntion of our model and baseline models, we exclude samples whose answers lie in a list or a table from a Wikipedia article."
    }, {
      "heading" : "D Hyperparameters",
      "text" : "We use the Adam optimizer (Kingma and Ba, 2015) in all our experiments. For training our phrase and question encoders with Eq. (9), we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1. We use a batch size of B =84 and train each model for 4 epochs for all datasets, where the loss of pre-batch negatives is applied in the last two epochs. We use SQuAD to train our QG model10 and use spaCy11 for extracting named entities in each training passage, which are used to generate questions. The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively. The number of preceding batches C is set to 2.\nFor the query-side fine-tuning with Eq. (11), we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1. We use a batch size of 12 and train each model for 10 epochs for all datasets. The top k for the Eq. (11) is set to 100. While we use a single 24GB GPU (TITAN RTX) for training the phrase encoders with Eq. (9), query-side fine-tuning is relatively cheap and uses a single 12GB GPU (TITAN Xp). Using the development set, we select the best performing model (based on EM) for each dataset, which are then evaluated on each test set. Since SpanBERT only supports cased models, we also truecase the questions (Lita et al., 2003) that are originally provided in the lowercase (Natural Questions and WebQuestions)."
    }, {
      "heading" : "E Reducing Storage Footprint",
      "text" : "As shown in Table 1, we have reduced the storage footprint from 1,547GB (Lee et al., 2020) to 320GB. We detail how we can reduce the storage footprint in addition to the several techniques introduced by Seo et al. (2019).\nFirst, following Seo et al. (2019), we apply a linear transformation on the passage token representations to obtain a set of filter logits, which can be used to filter many token representations from W(D). This filter layer is supervised by applying the binary cross entropy with the gold start/end\n10The quality of generated questions from a QG model trained on Natural Questions is worse due to the ambiguity of information-seeking questions.\n11https://spacy.io/\npositions (trained together with Eq. (9)). We tune the threshold for the filter logits on the reading comprehension development set to the point where the performance does not drop significantly while maximally filtering tokens. In the full Wikipedia setting, we filter about 75% of tokens and store 770M token representations.\nSecond, in our architecture, we use a base model (SpanBERT-base) for a smaller dimension of token representations (d = 768) and does not use any sparse representations including tf-idf or contextualized sparse representations (Lee et al., 2020). We also use the scalar quantization for storing float32 vectors as int4 during indexing.\nLastly, since the inference in Eq. (10) is purely based on MIPS, we do not have to keep the original start and end vectors which takes about 500GB. However, when we perform query-side fine-tuning, we need the original start and end vectors for reconstructing them to compute Eq. (11) since (the on-disk version of) MIPS index only returns the top-k scores and their indices, but not the vectors."
    } ],
    "references" : [ {
      "title" : "Learning to retrieve reasoning paths over wikipedia graph for question answering",
      "author" : [ "Akari Asai", "Kazuma Hashimoto", "Hannaneh Hajishirzi", "Richard Socher", "Caiming Xiong." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Asai et al\\.,? 2020",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling of the question answering task in the YodaQA system",
      "author" : [ "Petr Baudiš", "Jan Šedivỳ." ],
      "venue" : "International Conference of the Cross-Language Evaluation Forum for European Languages (CLEF).",
      "citeRegEx" : "Baudiš and Šedivỳ.,? 2015",
      "shortCiteRegEx" : "Baudiš and Šedivỳ.",
      "year" : 2015
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "The Journal of Machine Learning Research (JMLR).",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Semantic parsing on Freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Deformer: Decomposing pre-trained Transformers for faster question answering",
      "author" : [ "Qingqing Cao", "Harsh Trivedi", "Aruna Balasubramanian", "Niranjan Balasubramanian." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Open-domain question answering",
      "author" : [ "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen and Yih.,? 2020",
      "shortCiteRegEx" : "Chen and Yih.",
      "year" : 2020
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Empirical",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "JMLR.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "T-REx: A large scale alignment of natural language with knowledge base triples",
      "author" : [ "Hady Elsahar", "Pavlos Vougiouklis", "Arslen Remaci", "Christophe Gravier", "Jonathon Hare", "Frederique Laforest", "Elena Simperl." ],
      "venue" : "International Conference on Lan-",
      "citeRegEx" : "Elsahar et al\\.,? 2018",
      "shortCiteRegEx" : "Elsahar et al\\.",
      "year" : 2018
    }, {
      "title" : "Building Watson: An overview of the deepqa project",
      "author" : [ "David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager" ],
      "venue" : "AI magazine,",
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "REALM: Retrieval-augmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient natural language response suggestion for smart reply",
      "author" : [ "Matthew Henderson", "Rami Al-Rfou", "Brian Strope", "YunHsuan Sung", "László Lukács", "Ruiqi Guo", "Sanjiv Kumar", "Balint Miklos", "Ray Kurzweil." ],
      "venue" : "arXiv preprint arXiv:1705.00652.",
      "citeRegEx" : "Henderson et al\\.,? 2017",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2017
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring",
      "author" : [ "Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Humeau et al\\.,? 2019",
      "shortCiteRegEx" : "Humeau et al\\.",
      "year" : 2019
    }, {
      "title" : "Leveraging passage retrieval with generative models for open domain question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave." ],
      "venue" : "European Chapter of the Association for Computational Linguistics (EACL).",
      "citeRegEx" : "Izacard and Grave.,? 2021",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2021
    }, {
      "title" : "Billion-scale similarity search with GPUs",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou." ],
      "venue" : "arXiv preprint arXiv:1702.08734.",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association of Computational Linguistics (TACL).",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oğuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Russ R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Tom Kwiatkowski", "Jennimaria Palomaki", "Olivia Redfield", "Michael Collins", "Ankur Parikh", "Chris Alberti", "Danielle Epstein", "Illia Polosukhin", "Jacob Devlin", "Kenton Lee" ],
      "venue" : null,
      "citeRegEx" : "Kwiatkowski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2019
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Contextualized sparse representations for real-time open-domain question answering",
      "author" : [ "Jinhyuk Lee", "Minjoon Seo", "Hannaneh Hajishirzi", "Jaewoo Kang." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning recurrent span representations for extractive question answering",
      "author" : [ "Kenton Lee", "Shimi Salant", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das", "Jonathan Berant." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Zero-shot relation extraction via reading comprehension",
      "author" : [ "Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer." ],
      "venue" : "Computational Natural Language Learning (CoNLL).",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretrained Transformers for text ranking: BERT and beyond",
      "author" : [ "Jimmy Lin", "Rodrigo Nogueira", "Andrew Yates." ],
      "venue" : "arXiv preprint arXiv:2010.06467.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "tRuEcasIng",
      "author" : [ "Lucian Vlad Lita", "Abe Ittycheriah", "Salim Roukos", "Nanda Kambhatla." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lita et al\\.,? 2003",
      "shortCiteRegEx" : "Lita et al\\.",
      "year" : 2003
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A discrete hard EM approach for weakly supervised question answering",
      "author" : [ "Sewon Min", "Danqi Chen", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "KILT: a benchmark for knowledge intensive language",
      "author" : [ "Fabio Petroni", "Aleksandra Piktus", "Angela Fan", "Patrick Lewis", "Majid Yazdani", "Nicola De Cao", "James Thorne", "Yacine Jernite", "Vassilis Plachouras", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "How much knowledge can you pack into the parameters of a language model? In Empirical Methods in Natural Language Processing (EMNLP)",
      "author" : [ "Adam Roberts", "Colin Raffel", "Noam Shazeer" ],
      "venue" : null,
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Phraseindexed question answering: A new challenge for scalable document comprehension",
      "author" : [ "Minjoon Seo", "Tom Kwiatkowski", "Ankur Parikh", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Seo et al\\.,? 2018",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2018
    }, {
      "title" : "Real-time open-domain question answering with dense-sparse phrase index",
      "author" : [ "Minjoon Seo", "Jinhyuk Lee", "Tom Kwiatkowski", "Ankur P Parikh", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Seo et al\\.,? 2019",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2019
    }, {
      "title" : "Delaying interaction layers in transformer-based encoders for efficient open domain question answering",
      "author" : [ "Wissam Siblini", "Mohamed Challal", "Charlotte Pasqual." ],
      "venue" : "arXiv preprint arXiv:2010.08422.",
      "citeRegEx" : "Siblini et al\\.,? 2020",
      "shortCiteRegEx" : "Siblini et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
      "author" : [ "Richard Socher", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Proceedings of the NIPS-2010 deep learning and unsupervised feature learning work-",
      "citeRegEx" : "Socher et al\\.,? 2010",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2010
    }, {
      "title" : "The TREC-8 question answering track report",
      "author" : [ "Ellen M Voorhees" ],
      "venue" : "Trec.",
      "citeRegEx" : "Voorhees,? 1999",
      "shortCiteRegEx" : "Voorhees",
      "year" : 1999
    }, {
      "title" : "Multi-passage BERT: A globally normalized BERT model for open-domain question answering",
      "author" : [ "Zhiguo Wang", "Patrick Ng", "Xiaofei Ma", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end open-domain question answering with bertserini",
      "author" : [ "Wei Yang", "Yuqing Xie", "Aileen Lin", "Xingyu Li", "Luchen Tan", "Kun Xiong", "Ming Li", "Jimmy Lin." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning discriminative projections for text similarity measures",
      "author" : [ "Wen-tau Yih", "Kristina Toutanova", "John C Platt", "Christopher Meek." ],
      "venue" : "Computational Natural Language Learning (CoNLL).",
      "citeRegEx" : "Yih et al\\.,? 2011",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2011
    }, {
      "title" : "TriviaQA. We use a version of Natural Questions and TriviaQA provided by Min et al",
      "author" : [ "Lee" ],
      "venue" : null,
      "citeRegEx" : "Lee,? \\Q2021\\E",
      "shortCiteRegEx" : "Lee",
      "year" : 2021
    }, {
      "title" : "2020).9 We use the short answer as a ground truth answer a∗ and its long answer as a gold passage p. We also match the gold passages in Natural Questions to the paragraphs in Wikipedia",
      "author" : [ "Asai" ],
      "venue" : null,
      "citeRegEx" : "Asai,? \\Q2020\\E",
      "shortCiteRegEx" : "Asai",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019).",
      "startOffset" : 151,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus (Voorhees et al., 1999; Ferrucci et al., 2010; Chen and Yih, 2020).",
      "startOffset" : 116,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus (Voorhees et al., 1999; Ferrucci et al., 2010; Chen and Yih, 2020).",
      "startOffset" : 116,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on",
      "startOffset" : 69,
      "endOffset" : 148
    }, {
      "referenceID" : 27,
      "context" : "While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on",
      "startOffset" : 69,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on",
      "startOffset" : 69,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : "While a dominating approach is a two-stage retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020), we focus on",
      "startOffset" : 69,
      "endOffset" : 148
    }, {
      "referenceID" : 41,
      "context" : "a recent new paradigm solely based on phrase retrieval (Seo et al., 2019; Lee et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "a recent new paradigm solely based on phrase retrieval (Seo et al., 2019; Lee et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "Since phrase representations are decomposed from question representations, they are inherently less expressive than cross-attention models (Devlin et al., 2019).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 41,
      "context" : "Consequently, existing approaches heavily rely on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models (Seo et al., 2019; Lee et al., 2020).",
      "startOffset" : 171,
      "endOffset" : 207
    }, {
      "referenceID" : 26,
      "context" : "Consequently, existing approaches heavily rely on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models (Seo et al., 2019; Lee et al., 2020).",
      "startOffset" : 171,
      "endOffset" : 207
    }, {
      "referenceID" : 14,
      "context" : "We then adopt negative sampling strategies such as inbatch negatives (Henderson et al., 2017; Karpukhin et al., 2020), to better discriminate the phrases at a larger scale.",
      "startOffset" : 69,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "We then adopt negative sampling strategies such as inbatch negatives (Henderson et al., 2017; Karpukhin et al., 2020), to better discriminate the phrases at a larger scale.",
      "startOffset" : 69,
      "endOffset" : 117
    }, {
      "referenceID" : 40,
      "context" : "Following previous work (Seo et al., 2018), ‘phrase’ denotes any contiguous segment of text up to L words (including single words), which is not necessarily a linguistic phrase.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "Phrase Retrieval DenSPI (Seo et al., 2019) 3 1,200 2.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019; Lee et al., 2020), with 15%– 25% absolute improvement on most datasets.",
      "startOffset" : 151,
      "endOffset" : 187
    }, {
      "referenceID" : 26,
      "context" : "We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019; Lee et al., 2020), with 15%– 25% absolute improvement on most datasets.",
      "startOffset" : 151,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020; Karpukhin et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020; Karpukhin et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 36,
      "context" : "With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.",
      "startOffset" : 162,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "We follow the recent work (Chen et al., 2017; Lee et al., 2019) and treat all of English Wikipedia as D, hence K ≈ 5 × 106.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "We follow the recent work (Chen et al., 2017; Lee et al., 2019) and treat all of English Wikipedia as D, hence K ≈ 5 × 106.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 30,
      "context" : "Although we focus on the extractive QA setting, recent works propose to use a generative model as the reader (Lewis et al., 2020; Izacard and Grave, 2021), or learn a closed-book QA model (Roberts et al.",
      "startOffset" : 109,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "Although we focus on the extractive QA setting, recent works propose to use a generative model as the reader (Lewis et al., 2020; Izacard and Grave, 2021), or learn a closed-book QA model (Roberts et al.",
      "startOffset" : 109,
      "endOffset" : 154
    }, {
      "referenceID" : 39,
      "context" : ", 2020; Izacard and Grave, 2021), or learn a closed-book QA model (Roberts et al., 2020), which directly predicts answers without using an external knowledge source.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "A dominating paradigm in open-domain QA is the retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020), which leverages a firststage document retriever fretr and only reads top K ′ K documents with a reader model fread.",
      "startOffset" : 73,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "A dominating paradigm in open-domain QA is the retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020), which leverages a firststage document retriever fretr and only reads top K ′ K documents with a reader model fread.",
      "startOffset" : 73,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "A dominating paradigm in open-domain QA is the retriever-reader approach (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020), which leverages a firststage document retriever fretr and only reads top K ′ K documents with a reader model fread.",
      "startOffset" : 73,
      "endOffset" : 134
    }, {
      "referenceID" : 46,
      "context" : "It can easily adapt to passages and sentences (Yang et al., 2019; Wang et al., 2019).",
      "startOffset" : 46,
      "endOffset" : 84
    }, {
      "referenceID" : 45,
      "context" : "It can easily adapt to passages and sentences (Yang et al., 2019; Wang et al., 2019).",
      "startOffset" : 46,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "Previous approaches (Seo et al., 2019; Lee et al., 2020) leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: Es(s,D) = [Esparse(s,D), Edense(s,D)].",
      "startOffset" : 20,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "Previous approaches (Seo et al., 2019; Lee et al., 2020) leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: Es(s,D) = [Esparse(s,D), Edense(s,D)].",
      "startOffset" : 20,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "Following previous work on phrase or span representations (Lee et al., 2017; Seo et al., 2018), we first apply a pre-trained language model Mp to obtain contextualized word representations for each passage token: h1, .",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 40,
      "context" : "Following previous work on phrase or span representations (Lee et al., 2017; Seo et al., 2018), we first apply a pre-trained language model Mp to obtain contextualized word representations for each passage token: h1, .",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "In our pilot experiments, we found that SpanBERT (Joshi et al., 2020) leads to superior performance compared to BERT (Devlin et al.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : ", 2020) leads to superior performance compared to BERT (Devlin et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 41,
      "context" : "Our base model is largely inspired by DenSPI (Seo et al., 2019), although we deviate from theirs as follows.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "6638 query-dependent representations in cross-attention models (Devlin et al., 2019), where passages are fed along with the questions concatenated by a special token such as [SEP].",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 37,
      "context" : "Following this intuition, we propose to use a simple model to generate additional questions for data augmentation, based on a T5-large model (Raffel et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 162
    }, {
      "referenceID" : 15,
      "context" : "Distillation We also propose improving the phrase representations by distilling knowledge from a cross-attention model (Hinton et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "(2019) simply sample two negative passages based on question similarity, we use in-batch negatives for our dense phrase representations, which has been shown to be effective in learning dense passage representations before (Karpukhin et al., 2020).",
      "startOffset" : 223,
      "endOffset" : 247
    }, {
      "referenceID" : 21,
      "context" : "The in-batch negatives usually benefit from a large batch size (Karpukhin et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "This approach is inspired by the momentum contrast idea proposed in unsupervised visual representation learning (He et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "We use faiss (Johnson et al., 2017) for building a MIPS index of H.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 27,
      "context" : "Formally speaking, we optimize the marginal log-likelihood of the gold answer a∗ for a question q, which resembles the weakly-supervised QA setting in previous work (Lee et al., 2019; Min et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 201
    }, {
      "referenceID" : 34,
      "context" : "Formally speaking, we optimize the marginal log-likelihood of the gold answer a∗ for a question q, which resembles the weakly-supervised QA setting in previous work (Lee et al., 2019; Min et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 201
    }, {
      "referenceID" : 38,
      "context" : "We use two reading comprehension datasets: SQuAD (Rajpurkar et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : ", 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019) to learn phrase representations, in which a single gold passage is provided for each question.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "For the opendomain QA experiments, we evaluate our approach on five popular open-domain QA datasets: Natural Questions, WebQuestions (WQ) (Berant et al., 2013), CuratedTREC (TREC) (Baudiš and Šedivỳ, 2015), TriviaQA (TQA) (Joshi et al.",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : ", 2013), CuratedTREC (TREC) (Baudiš and Šedivỳ, 2015), TriviaQA (TQA) (Joshi et al.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : ", 2013), CuratedTREC (TREC) (Baudiš and Šedivỳ, 2015), TriviaQA (TQA) (Joshi et al., 2017), and SQuAD.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 36,
      "context" : "We focus on using two slot filling datasets from the KILT benchmark (Petroni et al., 2021): T-REx (Elsahar et al.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : ", 2021): T-REx (Elsahar et al., 2018) and zero-shot relation extraction (Levy et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 29,
      "context" : ", 2018) and zero-shot relation extraction (Levy et al., 2017).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Our performance is also competitive with recent retriever-reader models (Karpukhin et al., 2020), while running much faster during inference (Table 1).",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "9 - DPR-multi (Karpukhin et al., 2020) {NQ, WQ, TREC, TQA} 41.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 41,
      "context" : "DenSPI (Seo et al., 2019) also included a coherency scalar and see their paper for more details.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 190
    }, {
      "referenceID" : 8,
      "context" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 190
    }, {
      "referenceID" : 33,
      "context" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 190
    }, {
      "referenceID" : 35,
      "context" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "Learning effective dense representations of words is a long-standing goal in NLP (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 190
    }, {
      "referenceID" : 25,
      "context" : "Beyond words, dense representations of many different granularities of text such as sentences (Le and Mikolov, 2014; Kiros et al., 2015) or documents (Yih et al.",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "Beyond words, dense representations of many different granularities of text such as sentences (Le and Mikolov, 2014; Kiros et al., 2015) or documents (Yih et al.",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 47,
      "context" : ", 2015) or documents (Yih et al., 2011) have been explored.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "While dense phrase representations have been also studied for statistical machine translation (Cho et al., 2014) or syntactic parsing (Socher et al.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : ", 2014) or syntactic parsing (Socher et al., 2010), our work focuses on learning dense phrase representations for QA and any other knowledge-intensive tasks where phrases can be easily retrieved by performing MIPS.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "This type of dense retrieval has been also studied for sentence and passage retrieval (Humeau et al., 2019; Karpukhin et al., 2020) (see Lin et al.",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "This type of dense retrieval has been also studied for sentence and passage retrieval (Humeau et al., 2019; Karpukhin et al., 2020) (see Lin et al.",
      "startOffset" : 86,
      "endOffset" : 131
    } ],
    "year" : 2021,
    "abstractText" : "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in opendomain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%– 25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.1",
    "creator" : "LaTeX with hyperref"
  }
}