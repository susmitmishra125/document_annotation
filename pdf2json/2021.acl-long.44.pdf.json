{
  "name" : "2021.acl-long.44.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation",
    "authors" : [ "Yiran Xing", "Zai Shi", "Zhao Meng", "Gerhard Lakemeyer", "Yunpu Ma", "Roger Wattenhofer" ],
    "emails" : [ "gerhard}@rwth-aachen.de", "cognitive.yunpu@gmail.com", "wattenhofer}@ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 525–535\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n525"
    }, {
      "heading" : "1 Introduction",
      "text" : "Early work on Vision-Language models has been largely focused on pure understanding tasks (Tan and Bansal, 2019; Lu et al., 2019). These models, although improving model performance on understanding tasks such as Visual Question Answering (Antol et al., 2015), are not capable of multimodal generation tasks (You et al., 2016). To ease this problem, researchers have proposed various models (Zhou et al., 2020; Li et al., 2020) for generating texts based on visual inputs.\nThese models are mainly pretrained on general visual and language understanding tasks such as masked language modeling and masked region modeling, which enable the models to build an\n∗The first three authors contribute equally to this work.\nalignment between visual and language features. However, only feature alignments are inadequate to enhance the model’s ability in conducting complex multimodal commonsense reasoning, which requires the model to understand the underlying relations and effects between objects.\nCommonsense reasoning was traditionally studied on natural language (Rajani et al., 2019; Trinh and Le, 2018), while recent works have paid attention to commonsense reasoning with joint visual and language inputs. For instance, Zellers et al. (2019) proposes the task of Visual Commonsense Reasoning (VCR). However, the task focuses on understanding instead of generating as it asks the model to answer multiple-choice questions. A newly introduced dataset, Visual Commonsense Generation (VCG) (Park et al., 2020), provides a more challenging task by requiring the model to generate commonsense inferences about what might happen before/after, and the present intents of characters (see Table 2 for an example). In this work, we propose to tackle the task of VCG by leveraging our Knowledge Enhanced Multimodal BART (Lewis et al., 2020), which we call KMBART. KM-BART is a Transformer-based model consisting of an encoder and a decoder and is pretrained on carefully designed tasks for VCG. Figure 1 presents our model architecture1.\nOur contributions in this work are three-folded:\n1. We extend the BART model to process multimodal data of images and texts, and enable multimodal reasoning by introducing taskrelevant tokens.\n2. To improve the model performance on Visual Commonsense Generation (VCG), we implicitly incorporate commonsense knowledge from external knowledge graphs to our\n1https://github.com/FomalhautB/ KM-BART-ACL\nKM-BART by designing a novel pretraining task, which we call Knowledge-based Commonsense Generation (KCG).\n3. Besides KCG, we further equip our KMBART with standard pretraining tasks including Masked Language Modeling (MLM), Masked Region Modeling (MRM), as well as Attribution Prediction (AP) and Relation Prediction (RP). Experimental results show that all pretraining tasks are effective, and combining these pretraining tasks enable our KMBART to achieve state-of-the-art performance on the VCG task."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Vision-Language Models",
      "text" : "Visual-Language (VL) tasks such as Visual Question Answering (VQA) (Antol et al., 2015) and Image-Text Matching (Li et al., 2019) require the models to process multimodal inputs and comprehend visual and textual information simultaneously. Inspired by successful pretrained language models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Yu et al., 2020) have been proposed. These multimodal pretrained models use Transformers as backbone and are denoising autoencoders trained to predict the alignment of imagetext pairs and the semantics of masked words and image regions.\nThe models mentioned above typically focus more on understanding tasks. To further bridge the gap between visual and textual clues in multimodal data, in addition to cross-modal understanding, a model should also acquire abilities to complete generation tasks, for example, the image-to-text task of Image Captioning (You et al., 2016). However, directly transferring a model pretrained on VL understanding tasks to generation tasks is infeasible, as these models are merely Transformer-based encoders and are thus not suitable for generation tasks.\nZhou et al. (2020) ease this problem by using a Transformer-based network as both an encoder and a decoder, making the model capable of generating texts based on visual and textual inputs. While Li et al. (2020) propose OSCAR, which improves the generation ability by introducing object tags as\nan additional clue during pretraining. These models achieve state-of-the-art performance in downstream multimodal generation tasks such as Image Captioning (You et al., 2016)."
    }, {
      "heading" : "2.2 Commonsense Knowledge",
      "text" : "Commonsense knowledge refers to the necessary level of practical knowledge and reasoning about everyday situations and events common among most people (Sap et al., 2020). For example, one should know that “water is for drinking” and “sunshine makes people warm”. Simple as it looks, enabling artificial intelligence to conduct commonsense reasoning has been difficult for learning-based models (Gunning, 2018). Researchers have resorted to knowledge graphs due to their exact graph-structured representation of knowledge to overcome this problem. For example, ConceptNet (Speer et al., 2017) is a knowledge graph with nodes representing general concepts and edges indicating relational knowledge between concepts. Another commonsense knowledge graph, ATOMIC (Sap et al., 2019), extends nodes to natural language phrases, and edges to relations such as intent, attribution, effect, etc.\nDespite improvements in modeling commonsense knowledge, graph-based methods require heavy human engineering, making it challenging to scale robustly. For instance, model performance usually deteriorates dramatically when retrieved contextual knowledge is noisy due to imperfect knowledge matching (Lin et al., 2019). Therefore, we implicitly leverage external knowledge using supervision signals inferred by COMET (Bosselut et al., 2019), which is a Transformer-based, generative model pretrained on commonsense knowledge graphs including ConceptNet and Atomic. Given a natural language phrase and a relation type, COMET generates natural language commonsense descriptions.\nIn summary, on the one hand, existing crossmodal architectures not focusing on commonsense interpretation as their pretraining tasks are designed for multimodal understanding, making them unsuitable for the downstream VCG task. On the other hand, Transformer-based generative models such as COMET (Bosselut et al., 2019) cannot generate commonsense inferences from cross-modal inputs. Therefore, in this work, we propose KM-BART to conduct the task of Visual Commonsense Generation (VCG). Our KM-BART is pretrained on a\ndedicated pretraining task for VCG as well as other standard pretraining tasks. Experimental results show that our KM-BART achieves state-of-the-art performance on the VCG task."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we describe our methodology for Visual Commonsense Generation. Section 3.1 gives our model architecture. Section 3.2 introduces our pretraining tasks as well as our self-training based data filtering technique."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "Figure 1 illustrates the architecture of our KMBART. The backbone of our model is BART (Lewis et al., 2020), which is a Transformer-based sequence-to-sequence autoencoder. We modify the original BART to adapt the model to crossmodality inputs of images and texts. We add special tokens to adapt the model to different pretraining/evaluation tasks. In the following subsections. We give the details of our visual feature extractor, the encoder, and the decoder."
    }, {
      "heading" : "3.1.1 Visual Feature Extractor",
      "text" : "Following previous work on Vision-Language models (Tan and Bansal, 2019; Lu et al., 2019), we use a convolution neural network pretrained on the COCO dataset to extract visual embeddings, which are subsequently fed to the Transformerbased cross-modal encoder. Specifically, we use the pretrained Masked R-CNN (He et al., 2017) from detectron22. For each image, the pretrained Masked R-CNN proposes the bounding boxes for detected objects. The area within a bounding box is a Region of Interest (RoI). We leverage the intermediate representations of the RoIs in the Masked R-CNN to obtain fixed-size embeddings for RoIs V = {v1, . . . , vi, . . . , vN}, where i is the index to RoIs, and N is the number of RoIs for an image. The visual embedding of the i-th RoI vi is vi ∈ Rd, where d is the embedding dimension. For each of the RoIs, the Masked R-CNN also outputs the class distribution p(vi), which is later used for Masked Region Modeling."
    }, {
      "heading" : "3.1.2 Cross-Modal Encoder",
      "text" : "Following Lewis et al. (2020), the encoder of our model is based on a multi-layer bidirectional Transformer. We introduce special tokens to adapt it to our pretraining and downstream evaluation tasks. Specifically, each example starts with a special token indicating the task type of the current example.\nFor our pretraining task of Knowledge-Based Commonsense Generation (see Section 3.2.1), we use <before>, <after>, or <intent> as the\n2https://github.com/facebookresearch/ detectron2\nstarting special token. For Attribution Prediction and Relation Prediction (Section 3.2.2), we use <region caption>. Finally, for Masked Language Modeling and Masked Region Modeling, we use <caption>.\nFurthermore, to inform the model of different modalities of inputs, we add three sets of different special tokens: For images, we use <img> and </img> to indicate the start and the end of visual embeddings, respectively. For texts, we introduce different special tokens to distinguish between two sets of textual inputs: events and captions. Events are image descriptions which the model uses for reasoning about future/past events or present intents of characters in the commonsense generation task, while captions are for Masked Language Modeling, where linguistic information plays a more important role. Hence, to inform the model of these two types of textual inputs, we use <event> and </event> for events, and <mlm> and </mlm> for captions. In the following sections, we denote textual inputs of words and specical tokens by W = {w1, .., wT }, where T is the length of textual inputs. For a token w, its embedding is e ∈ Rd, where d is the dimension of the embeddings."
    }, {
      "heading" : "3.1.3 Decoder",
      "text" : "The decoder of our model is also a multi-layer Transformer. Unlike the encoder, which is bidirectional, the decoder is unidirectional as it is supposed to be autoregressive when generating texts. The decoder does not take the visual embeddings as inputs. Instead, we use embeddings of the special token <img feat> to replace the actual visual embeddings. For Masked Region Modeling and Masked Language Modeling, we use <cls> to replace the masked regions or words (see Figure 1). The model should predict the masked words and the class distribution of the masked regions during pretraining."
    }, {
      "heading" : "3.2 Pretraining Tasks",
      "text" : "To pretrain our model, we use four image-text datasets: Conceptual Captions Dataset (Sharma et al., 2018), SBU Dataset (Ordonez et al., 2011), Microsoft COCO Dataset (Lin et al., 2014) and Visual Genome (Krishna et al., 2017). In the remaining of this section, we use D to denote the individual datasets for each of the pretraining tasks. Statistics of the datasets are given in Table 1. The above datasets consist of examples of parallel images and texts and are widely used in previous\nwork (Tan and Bansal, 2019; Lu et al., 2019; Zhou et al., 2020; Yu et al., 2020)."
    }, {
      "heading" : "3.2.1 Knowledge-Based Commonsense Generation",
      "text" : "The knowledge-based commonsense generation (KCG) task aims to improve the performance of KM-BART on the VCG task. We leverage knowledge induced from COMET (Bosselut et al., 2019), which is a large language model pretrained on external commonsense knowledge graphs. Given a natural language phrase and a relation as inputs, COMET generates natural language phrases as commonsense descriptions. Relations of COMET include xIntent, xWant, xNeed, xReact and xEffect.\nWe only use COMET to generate new commonsense descriptions on SBU and COCO datasets due to limits in computational power for pretraining. For each image-text pair, we use COMET to generate commonsense descriptions from the text using all five relations mentioned above. To adapt COMET generated commonsense knowledge to VCG, we consider relations xIntent and xWant from COMET as intent, xNeed as before, xReact and xEffect as after. In this way, we generate additional commonsense knowledge for SBU and COCO datasets. The newly generated dataset has more than 3.6 million examples (Table 3). However, the generated commonsense knowledge is not always reasonable as only textual information is used while the visual information is completely ignored. To ease this problem, we further filter the dataset by employing a self-training based data filtering strategy. Self-Training Based Data Filtering Our strategy aims to filter the generated commonsense knowledge dataset so that the examples in the filtered dataset closely resemble the examples in the VCG dataset. To achieve this goal, we first initialize our KM-BART with BART parameters and finetune KM-BART on the VCG dataset for 30 epochs. The finetuned KM-BART already has a good performance on the VCG dataset with a CIDER score of 39.13 (see Table 4).\nWe then leverage this finetuned model to evaluate the quality of commonsense descriptions generated by COMET. We feed the corresponding images, texts, and relations as inputs to the finetuned KM-BART and then compute the cross-entropy (CE) loss of COMET generated commonsense descriptions. We observe that commonsense descrip-\ntions with a lower CE loss make more sense than those with a higher CE loss. Notice that when computing the CE loss of the COMET generated commonsense descriptions, our KM-BART leverages both the textual inputs and the visual inputs. We provide examples of our data filtering strategy in Supplementary Material.\nWe compute CE loss for all the commonsense descriptions in the VCG dataset and the new dataset generated by COMET. Figure 2 shows the distributions of CE loss for the two datasets. We observe that commonsense descriptions generated by COMET result in higher CE losses, which are expected as images are completely ignored when using COMET to generate natural language commonsense descriptions. We only keep the examples of which CE loss is below 3.5. Table 3 shows the statistics of generated datasets before and after data filtering. By filtering, we keep only 1.46 million examples, roughly accounting for 40% of the original examples.\nFinally, we leverage the newly generated commonsense knowledge dataset by pretraining KMBART on it. We expect by pretraining, the model reaches higher performance on the VCG dataset.\nLet S = {w1, ..., wL} be a commonsense description of the newly generated dataset D, the loss function for KCG is:\nLKCG(θ) =\n− E(W,V )∼D L∑ l=1 log(Pθ(wl|w<l,W, V )) (1)\nwhere L is the length of the generated sequence, l is the index to individual tokens in the target commonsense description S, V andW are visual inputs and textual inputs, respectively. θ represents model parameters to be optimized."
    }, {
      "heading" : "3.2.2 Attribute Prediction and Relation Prediction",
      "text" : "The Visual Genome dataset consists of 2.3 million relationships and 2.8 million attributes. To utilize these data, we use the attribute prediction (AP) and\nthe relation prediction (RP) as pretraining tasks, which enable the model to learn intrinsic properties among different objects in an image.\nIn the AP task, we feed the output vectors of the decoder for each image feature into an MLP classifier. In the RP task, we concatenate two output vectors of the decoder for each image feature pair and feed it into another MLP classifier. We use the cross-entropy loss for both tasks.\nWe denote the indices for AP by 1 ≤ j ≤ A, the indices for RP by 1 ≤ k ≤ R, where A is the number of AP examples, and R is the number of RP examples. We denote the label for the j-th AP example by La(vj), and the label for the k-th RP example as Lr(vk1 , vk2), where vk1 and vk1 are the two RoIs of the current RP example. The loss function for the AP task is:\nLAP (θ) =\n− E(W,V )∼D A∑ j=1 log(Pθ(La(vj) |W,V )) (2)\nAnd the loss function for the RP task is:\nLRP (θ) =\n− E(W,V )∼D R∑ k=1 log(Pθ(Lr(vk1 , vk2)) |W,V ))\n(3)"
    }, {
      "heading" : "3.2.3 Masked Language Modeling",
      "text" : "Following previous works (Devlin et al., 2019; Liu et al., 2019), we randomly mask the input textual tokens with a probability of 15% in the Masked Language Modeling (MLM) task. Within this 15% of the tokens, we use <mask> to replace the masked\ntoken with a probability of 80%, use a random token to replace with a probability of 10%, and keep the masked token unchanged with a probability of 10%.\nWe denote the mask indices by 1 ≤ m ≤ M , where M is the number of masked tokens. We denote the masked token by wm, and the remaining tokens that are not masked by w\\m, the loss function for MLM is defined as:\nLMLM (θ) =\n− E(W,V )∼D M∑ m=1 log(Pθ(wm|w\\m,W, V )) (4)"
    }, {
      "heading" : "3.2.4 Masked Region Modeling",
      "text" : "In the Masked Region Modeling (MRM) task, we sample image regions and mask the corresponding feature vectors with a probability of 15%. The masked vector will be replaced by a vector filled with zeros. The model needs to predict the distribution over semantic classes for the masked regions. The loss function is to minimize the KL divergence of the output distribution and the distribution predicted by the Masked R-CNN used in visual features extraction.\nWe denote the mask indices by 1 ≤ n ≤ N , where N is the number of masked regions. We let p(vn) denote the class distribution of the masked region vn detected by Masked R-CNN, qθ(vn) denote the class distribution output by our model, the loss function for MRM is then:\nLMRM (θ) =\nE(W,V )∼D N∑ n=1 DKL(p(vn)||qθ(vn))) (5)"
    }, {
      "heading" : "3.2.5 Combining Losses",
      "text" : "To combine all the losses we described above, we weight each of the losses by WKCG,WAP ,WRP ,WMLM ,WMRM ∈ R. The weights are chosen to roughly balance every term during the training phase. The final loss is:\nL =WKCGLKCG +WAPLAP +WRPLRP+ WMLMLMLM +WMRMLMRM\n(6)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We describe our experiments in this section. Section 4.1 is the experimental settings of different pretraining and initialization strategies. Section 4.2 gives the evaluation task and metrics. We show our results in Section 4.3. In Section 4.4, we give example inferences generated by our model. We have the human evaluation results in Section 4.5."
    }, {
      "heading" : "4.1 Settings",
      "text" : "In our experiments, following the base model from Lewis et al. (2020), we fix the model architecture to a 6-layer encoder and a 6-layer decoder. To understand how each pretraining task helps model performance on the downstream task of VCG, we ablate on pretraining tasks. We use the following experimental settings: (1) Without any pretraining; (2) Only with Knowledge-based Commonsense Generation; (3) Only with Attribute Prediction and Relation Prediction; (4) Only with Masked Language Modeling and Masked Region Modeling; (4) With all the pretraining tasks combined. For only with Knowledge-based Commonsense Generation, we further compare the model performance before\nand after data filtering (see Section 3.2.1). For each of the above settings, we initialize the model from random or from BART weights, respectively. Besides, we are most interested in the model performance under two settings (see the second column of Table 4): (1) Only using images as inputs; (2) Using both images and event descriptions as inputs. Note that when only using images as inputs for evaluation, we also do not use textual inputs during pretraining/finetuning."
    }, {
      "heading" : "4.2 Evaluation Task and Metrics",
      "text" : "We evaluate our model on the recently proposed Visual Commonsense Generation (VCG) Dataset (Park et al., 2020). Given an image and a description of the event in the image, the task aims to predict events which might happen before/after, and the present intents of the characters in the given image. The dataset consists of 1174K training examples and 146K validation examples. Some examples in the dataset share the same images or events, but with different inferences for events before/after or intents at present. Table 2 gives an example of the dataset. We report our model performance on the validation set as the test set is not available yet.\nBesides event descriptions, the VCG dataset also provides Place and Person information for each image. Note that although Park et al. (2020) also leverages the Place and Person information for training and evaluation, we argue that such information is not generally available in normal settings, where only images and event descriptions are given. Hence, we do not use the Place and Person information in our KM-BART. As an additional reference, we nevertheless show in Table 5 the best performed models from Park et al. (2020), which also use Place and Person information.\nWe use three automatic evaluation metrics, including BLEU-2 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CIDER (Vedantam et al., 2015). Following Park et al. (2020), we also report Unique as the number of inference sentences unique in generated sentences divided by the total number of sentences, and Novel as the number of generated sentences not in the training data divided by the total number of sentences."
    }, {
      "heading" : "4.3 Results",
      "text" : "We first ablate on different pretraining tasks to understand the effect of each task. We then combine all the pretraining tasks together to train our full\nmodel. As a last step, we pick the best performed models to compare against previous state-of-the-art system (Park et al., 2020).\nTable 4 shows the effect of each pretraining task to our KM-BART on the VCG dataset. We can see that all our pretraining tasks help improve model performance. Most importantly, we observe that although filtering on the commonsense generation pretraining task reduces the dataset size by more than 60%, pretraining with KCG still reaches comparable or better performance than pretraining with KCG (before filtering). This demonstrates that our self-training based filtering technique is helpful, as it helps the model reach similar or even better performance with less training data. The advantage is most evident when we initialize from BART parameters and use both images and event descriptions as inputs. Under this setting, pretraining with KCG outperforms pretraining with KCG (before filtering) in terms of all the evaluation metrics.\nFor using both images and event descriptions as inputs, the model performs better when initialized from pretrained BART parameters. As pretrained BART can better leverage the information in the event descriptions. Hence, to obtain our full KMBART model for using images and events as inputs, we adopt the setting of initializing from BART parameters. Experimental results show that our full model† reaches high performance on BLEU-2, METEOR and CIDER, and that the full model† generates the most unique and novel inferences. For using only images as inputs, models initializing from random parameters outperforms those initialized from BART parameters. We argue that initializing from BART parameters results in optimization disadvantages where the model has to switch from pure textual inputs to pure visual inputs. This observation becomes evident as the\nmodel performs the worst when no pretraining is used, which indicates that the model has to entirely rely on finetuning on the VCG dataset to adapt to visual inputs. Therefore, for using only images as inputs, we obtain our full KM-BART model by initializing from random parameters. Our full model§ reaches best performance on BLEU-2, METEOR and CIDER, and is the second best in terms of Unique.\nIn Table 5, we compare our full model to previous state-of-the-art (Park et al., 2020).3 We observe that although our full model† taking as inputs images and event descriptions does not use Place and Person information, the model still outperforms previous state-of-the-art (Park et al. (2020)c). For using only images as inputs, our model§ also performs better than previous results (Park et al. (2020)b). Furthermore, our model§ reaches comparable performance to Park et al. (2020)a in terms of BLEU-2, METEOR and CIDER, with much higher performance on Uniqueness and Novelty, even though our model§ uses much less information during training compared to Park et al. (2020)a."
    }, {
      "heading" : "4.4 Case Study",
      "text" : "In Table 2, we show example inferences and compare the results of our model predictions to the ground truths. The generated sentences from the model without event descriptions as inputs can already capture the most important information of commonsense. We also observe that adding event descriptions to the inputs helps the model generate more details. We gives more examples of our model in the Appendix.\n3Note that model performance in Table 5 is not directly comparable to that of Table 4 as we use different decoding strategies to generate different number of inference sentences per example in these two tables."
    }, {
      "heading" : "4.5 Human Evaluation",
      "text" : "We conduct human evaluation to further understand how humans perceive the inferences generated by our KM-BART. We employ a comparison approach for a better assessment between our KM-BART and the model from Park et al. (2020). To be specific, we randomly sample 30 examples from the VCG validation set. For each example, we use our KMBART or the baseline model to generate 5 sets of inferences, each of which consist of the task type before, after, and intent.\nWe use two settings for our human evaluation: (1) With event: event descriptions are given as input during inference time; (2) Without event: event descriptions are not given during inference time. Under each of the settings we compare our KMBART model with the mode from Park et al. (2020). We use the same 30 examples for each model under the two settings. For each example in a task type (before, after, or intent), we generate 5 inferences for one model of each setting. In total, we generate 450 inferences for each model of each setting during the human evaluation.\nFor the same example, we use our KM-BART and the model from Park et al. (2020) to generate an inference under one of the three task types, then the workers choose the more reasonable inference from the two generated inferences. We hire three workers from Amazon Mechanical Turk4 to evaluate each inference. We take the majority of the three workers as the final evaluation for an inference. Among all the inferences, we use the percentage of one model better than another model as the score of that model. For example, in Table 6, the score of our model (Ours§) is 61.3 for the task type before when event descriptions are missing. This indicates that our model is better than the baseline model for the task type before in 61.3% of the cases. We also\n4https://www.mturk.com/\ntake the average over the three task types as the final score (see Total in Table 6).\nFrom Table 6, we can observe that our model outperforms Park et al. (2020) under both of the settings. To be specific, when event descriptions are not given, among all the inferences, our model is better than Park et al. (2020) in 66.7% of the cases. Furthermore, our model has a lead of at least 22.6% over Park et al. (2020) in each individual task. For example, our model generates better inferences in 68.7% of the cases in task type after, while the model from Park et al. (2020) is only better than our model in 31.3% of the cases. We can obtain similar results when looking at the task type before and intent.\nWhen event descriptions are given, our model is still better than Park et al. (2020) in 55.1% of all the cases. For each individual task, the advantage of our model is smaller when event descriptions are given than when event descriptions are not given, showing that our model can better capture information from the images."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we propose Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based model capable of reasoning about and generating commonsense descriptions from cross modality inputs of images and texts. We propose the pretraining task of Knowledge-Based Commonsense Generation, which improves the reasoning ability of KM-BART by leveraging a large language model pretrained on external commonsense knowledge graphs. We use the self-training technique to filter the automatically generated commonsense descriptions. Experimental results on the VCG task show that our KM-BART pretrained on the pretraining tasks reaches state-of-the-art performance. Further human evaluation demonstrates that our KM-BART can generate commonsense inferences of high quality.\nFor future work, we plan to further expand our pretraining dataset for Knowledge-Based Commonsense Generation by including the Conceptual Captions Dataset (Sharma et al., 2018). Furthermore, while we argue that Place and Person information is not generally available in practical scenarios, we still plan to add Place and Person information to our model in the future."
    } ],
    "references" : [ {
      "title" : "VQA: visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "COMET: commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Çelikyilmaz", "Yejin Choi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "UNITER: universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Machine common sense concept paper",
      "author" : [ "David Gunning." ],
      "venue" : "arXiv preprint arXiv:1810.07528.",
      "citeRegEx" : "Gunning.,? 2018",
      "shortCiteRegEx" : "Gunning.",
      "year" : 2018
    }, {
      "title" : "Mask R-CNN",
      "author" : [ "Kaiming He", "Georgia Gkioxari", "Piotr Dollár", "Ross B. Girshick." ],
      "venue" : "ICCV.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual semantic reasoning for imagetext matching",
      "author" : [ "Kunpeng Li", "Yulun Zhang", "Kai Li", "Yuanyuan Li", "Yun Fu." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Ordonez et al\\.,? 2011",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Visualcomet: Reasoning about the dynamic context of a still image",
      "author" : [ "Jae Sung Park", "Chandra Bhagavatula", "Roozbeh Mottaghi", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Park et al\\.,? 2020",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "ACL.",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for ifthen reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense reasoning for natural language processing",
      "author" : [ "Maarten Sap", "Vered Shwartz", "Antoine Bosselut", "Yejin Choi", "Dan Roth." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : null,
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "LXMERT: learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "A simple method for commonsense reasoning",
      "author" : [ "Trieu H Trinh", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1806.02847.",
      "citeRegEx" : "Trinh and Le.,? 2018",
      "shortCiteRegEx" : "Trinh and Le.",
      "year" : 2018
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "R. Vedantam", "C.L. Zitnick", "D. Parikh." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Image captioning with semantic attention",
      "author" : [ "Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo." ],
      "venue" : "CVPR.",
      "citeRegEx" : "You et al\\.,? 2016",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2016
    }, {
      "title" : "Ernievil: Knowledge enhanced vision-language representations through scene graph",
      "author" : [ "Fei Yu", "Jiji Tang", "Weichong Yin", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2006.16934.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified vision-language pre-training for image captioning and VQA",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason J. Corso", "Jianfeng Gao." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "Early work on Vision-Language models has been largely focused on pure understanding tasks (Tan and Bansal, 2019; Lu et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Early work on Vision-Language models has been largely focused on pure understanding tasks (Tan and Bansal, 2019; Lu et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "These models, although improving model performance on understanding tasks such as Visual Question Answering (Antol et al., 2015), are not capable of multimodal generation tasks (You et al.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : ", 2015), are not capable of multimodal generation tasks (You et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 30,
      "context" : "To ease this problem, researchers have proposed various models (Zhou et al., 2020; Li et al., 2020) for generating texts based on visual inputs.",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "To ease this problem, researchers have proposed various models (Zhou et al., 2020; Li et al., 2020) for generating texts based on visual inputs.",
      "startOffset" : 63,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Commonsense reasoning was traditionally studied on natural language (Rajani et al., 2019; Trinh and Le, 2018), while recent works have paid attention to commonsense reasoning with joint visual and language inputs.",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : "Commonsense reasoning was traditionally studied on natural language (Rajani et al., 2019; Trinh and Le, 2018), while recent works have paid attention to commonsense reasoning with joint visual and language inputs.",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "A newly introduced dataset, Visual Commonsense Generation (VCG) (Park et al., 2020), provides a more challenging task by requiring the model to generate commonsense inferences about what might happen before/after, and the present intents of characters (see Table 2 for an example).",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "In this work, we propose to tackle the task of VCG by leveraging our Knowledge Enhanced Multimodal BART (Lewis et al., 2020), which we call KMBART.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Visual-Language (VL) tasks such as Visual Question Answering (VQA) (Antol et al., 2015) and Image-Text Matching (Li et al.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and Image-Text Matching (Li et al., 2019) require the models to process multimodal inputs and comprehend visual and textual information simultaneously.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Inspired by successful pretrained language models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and GPT-2 (Radford et al., 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : ", 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Yu et al., 2020) have been proposed.",
      "startOffset" : 86,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : ", 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Yu et al., 2020) have been proposed.",
      "startOffset" : 86,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : ", 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Yu et al., 2020) have been proposed.",
      "startOffset" : 86,
      "endOffset" : 161
    }, {
      "referenceID" : 28,
      "context" : ", 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Yu et al., 2020) have been proposed.",
      "startOffset" : 86,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : "To further bridge the gap between visual and textual clues in multimodal data, in addition to cross-modal understanding, a model should also acquire abilities to complete generation tasks, for example, the image-to-text task of Image Captioning (You et al., 2016).",
      "startOffset" : 245,
      "endOffset" : 263
    }, {
      "referenceID" : 27,
      "context" : "These models achieve state-of-the-art performance in downstream multimodal generation tasks such as Image Captioning (You et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Commonsense knowledge refers to the necessary level of practical knowledge and reasoning about everyday situations and events common among most people (Sap et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "Simple as it looks, enabling artificial intelligence to conduct commonsense reasoning has been difficult for learning-based models (Gunning, 2018).",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "For example, ConceptNet (Speer et al., 2017) is a knowledge graph with nodes representing general concepts and edges indicating relational knowledge between concepts.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "Another commonsense knowledge graph, ATOMIC (Sap et al., 2019), extends nodes to natural language phrases, and edges to relations such as intent, attribution, effect, etc.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "contextual knowledge is noisy due to imperfect knowledge matching (Lin et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Therefore, we implicitly leverage external knowledge using supervision signals inferred by COMET (Bosselut et al., 2019), which is a Transformer-based, generative model pretrained on commonsense knowledge graphs including ConceptNet and Atomic.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, Transformer-based generative models such as COMET (Bosselut et al., 2019) cannot generate commonsense inferences from cross-modal inputs.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "Conceptual Captions (Sharma et al., 2018) 2,683,686 2,683,686",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "The backbone of our model is BART (Lewis et al., 2020), which is a Transformer-based sequence-to-sequence autoencoder.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "Following previous work on Vision-Language models (Tan and Bansal, 2019; Lu et al., 2019), we use a convolution neural network pretrained on",
      "startOffset" : 50,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "Following previous work on Vision-Language models (Tan and Bansal, 2019; Lu et al., 2019), we use a convolution neural network pretrained on",
      "startOffset" : 50,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Specifically, we use the pretrained Masked R-CNN (He et al., 2017) from detectron22.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "To pretrain our model, we use four image-text datasets: Conceptual Captions Dataset (Sharma et al., 2018), SBU Dataset (Ordonez et al.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : ", 2018), SBU Dataset (Ordonez et al., 2011), Microsoft COCO Dataset (Lin et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : ", 2011), Microsoft COCO Dataset (Lin et al., 2014) and Visual Genome (Krishna et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 24,
      "context" : "The above datasets consist of examples of parallel images and texts and are widely used in previous work (Tan and Bansal, 2019; Lu et al., 2019; Zhou et al., 2020; Yu et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "The above datasets consist of examples of parallel images and texts and are widely used in previous work (Tan and Bansal, 2019; Lu et al., 2019; Zhou et al., 2020; Yu et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 180
    }, {
      "referenceID" : 30,
      "context" : "The above datasets consist of examples of parallel images and texts and are widely used in previous work (Tan and Bansal, 2019; Lu et al., 2019; Zhou et al., 2020; Yu et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "The above datasets consist of examples of parallel images and texts and are widely used in previous work (Tan and Bansal, 2019; Lu et al., 2019; Zhou et al., 2020; Yu et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "We leverage knowledge induced from COMET (Bosselut et al., 2019), which is a large language model pretrained on external commonsense knowledge graphs.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Following previous works (Devlin et al., 2019; Liu et al., 2019), we randomly mask the input textual tokens with a probability of 15% in the Masked Language Modeling (MLM) task.",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "Following previous works (Devlin et al., 2019; Liu et al., 2019), we randomly mask the input textual tokens with a probability of 15% in the Masked Language Modeling (MLM) task.",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "We evaluate our model on the recently proposed Visual Commonsense Generation (VCG) Dataset (Park et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "We use three automatic evaluation metrics, including BLEU-2 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CIDER (Vedantam et al.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014), and CIDER (Vedantam et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014), and CIDER (Vedantam et al., 2015).",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "As a last step, we pick the best performed models to compare against previous state-of-the-art system (Park et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "In Table 5, we compare our full model to previous state-of-the-art (Park et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : "For future work, we plan to further expand our pretraining dataset for Knowledge-Based Commonsense Generation by including the Conceptual Captions Dataset (Sharma et al., 2018).",
      "startOffset" : 155,
      "endOffset" : 176
    } ],
    "year" : 2021,
    "abstractText" : "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformerbased sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.",
    "creator" : "LaTeX with hyperref"
  }
}