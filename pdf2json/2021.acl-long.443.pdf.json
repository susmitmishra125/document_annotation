{
  "name" : "2021.acl-long.443.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Rewriter-Evaluator Architecture for Neural Machine Translation",
    "authors" : [ "Yangming Li", "Kaisheng Yao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5701–5710\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5701"
    }, {
      "heading" : "1 Introduction",
      "text" : "Encoder-Decoder architecture (Sutskever et al., 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020). Given a source sentence, an encoder firstly converts it into hidden representations, which are then conditioned by a decoder to produce a target sentence. In analogy to the development of statistical machine translation (SMT) (Och and Ney, 2002; Shen et al., 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al., 2017; Zhang et al., 2018;\nGeng et al., 2018; Niehues et al., 2016). In these models, more than one translation is generated for a source sentence. Except for the first translation, each of the later translations is conditioned on the previous one. While these methods have achieved promising results, they lack a proper termination poqlicy for this multi-turn process. For instance, Xia et al. (2017); Zhang et al. (2018) adopt a fixed number of decoding passes, which is inflexible and can be sub-optimal. Geng et al. (2018) utilize reinforcement learning (RL) (Sutton et al., 2000) to automatically decide the number of decoding passes. However, RL is known to be unstable due to the high variance in gradient estimation (Boyan and Moore, 1995).\nTo address this problem, we introduce a novel architecture, Rewriter-Evaluator. This architecture contains a rewriter and an evaluator. The translation process involves multiple passes. Given a source sentence, at every turn, the rewriter generates a new target sequence to improve the translation from the prior pass, and the evaluator measures the translation quality to determine whether to end the iterative rewriting process. Hence, the translation process is continued until a certain condition is met, such as no significant improvement in the measured translation quality. In implementations, the rewriter is a conditional language model (Sutskever et al., 2014) and the evaluator is a text matching model (Wang et al., 2017).\nWe also propose prioritized gradient descent (PGD) that facilitates training the rewriter and the evaluator both jointly and efficiently. PGD uses a priority queue to store previous translation cases. The queue stores translations with descending order of their scores, computed from the evaluator. The capacity of the queue is limited to be a few times of batch size. Due to its limited size, the queue pops those translations with high scores and only keeps the translations with lower scores. The samples in\nthe queue are combined together with new cases from the training data to train the rewriter.\nRewriter-Evaluator has been applied to improve two mainstream NMT models, RNNSearch (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017). We have conducted extensive experiments on three translation tasks, NIST Chineseto-English, WMT’18 Chinese-to-English, and WMT’14 English-to-German. The results show that our architecture notably improves the performance of NMT models and significantly outperforms related approaches. We conduct oracle experiment to understand the source of improvements. The oracle can pick the best translation from all the rewrites. Results indicate that the evaluator helps our models achieve the performances close to the oracle, outperforming the methods of fixing the number of rewriting turns. Compared against averaged performances using a fixed number of rewriting iterations, performance gaps to the oracle can be reduced by 80.7% in the case of RNNSearch and 75.8% in the case of Transformer. Quantitatively, we find the evaluator trained with PGD is significantly more accurate in determining the optimal number of rewriting turns. For example, whereas the method in Geng et al. (2018) has 50.2% accuracy in WMT’14, the evaluator achieves 72.5% accuracy on Transformer."
    }, {
      "heading" : "2 Rewriter-Evaluator",
      "text" : "Rewriter-Evaluator consists of iterative processes involving a rewriting process ψ and an evaluation process φ. The process of translating an n-length source sentence x = [x1, x2, · · · , xn] is an application of the above processes. Assume we are at the k-th iteration (k ≥ 1). The rewriter ψ gener-\nates a target sequence z(k) = [z(k)1 , z (k) 2 , · · · , z (k) lk\n] given the source sentence x and the past translation z(k−1) = [z(k−1)1 , z (k−1) 2 , · · · , z (k−1) lk−1\n] from the (k − 1)-th turn. lk and lk−1 are the sentence lengths. The evaluator φ estimates the translation quality score q(k) of the new translation z(k), which is used for determining whether to end the multiturn process. Formally, the k-th pass of a translation process is defined as{\nz(k) = ψ(x, z(k−1)) q(k) = φ(x, z(k)) . (1)\nInitially, z(0) and q(0) are respectively set as an empty string and −∞.\nThe above procedure is repeatedly carried out until not much improvement in the estimated quality score can be achieved, i.e.,\nq(k) + < q(k−1), > 0, (2)\nwhere is a small value tuned on the development set. Alternatively, the procedure is terminated if a certain number of iterations K > 0 is reached. In the former case, we adopt z(k−1) as the final translation. In the latter case, the last translation z(K) is accepted."
    }, {
      "heading" : "2.1 Architecture",
      "text" : "A general architecture of Rewriter-Evaluator using Encoder-Decoder is illustrated in Fig. 1. The rewriter ψ consists of a source encoder fSE , a target decoder fTE , and a decoder gDEC . The evaluator φ shares encoders with the rewriter and contains an estimator gEST .\nAssume it is at the k-th pass. Firstly, the source encoder fSE casts the source sentence x into word\nAlgorithm 1: Prioritized Gradient Descent (PGD) Input: rewriter ψ, evaluator φ, training set T , batch size B, and expected iteration number E. Output: well-trained rewriter ψ and well-trained evaluator φ.\n1 Initialize an empty priority queue A with the capacity C ← B × E. 2 while Models are not converged do 3 Pop B cases with high quality scores from priority queue A and discard them. 4 Randomly sample a B-sized batch of training cases S from T . 5 for (x,y) ∈ S do 6 Push the quadruple (x,y, [“SOS”, “EOS”],−∞) into queue A. 7 Initialize an empty priority queue D of limited size C. 8 Initialize an empty list F to collect samples for training. 9 for (x,y, z(k−1), r(k−1)) ∈ A do\n10 Obtain translation z(k) and quality score q(k), respectively, using Eq. (5) and Eq. (6). 11 Push sample (x,y, z(k), q(k)) into list F . 12 Compute quality rate r(k) using Eq. (9). 13 Push quadruple (x,y, z(k), r(k)) into queue D.\n14 Optimize rewriter ψ with the samples in list F to reduce loss in Eq. (7). 15 Optimize evaluator φ with the samples in list F to reduce loss in Eq. (8). 16 Update priority queue A: A← D.\nrepresentations hi, 1 ≤ i ≤ n:\nH = [h1;h2; · · · ;hn] = fSE(x), (3)\nwhere operation [; ] is row-wise vector concatenation. Similarly, the translation z(k−1) from the previous turn k − 1 is encoded as\nP(k−1) = [p (k−1) 1 ;p (k−1) 2 ; · · · ;p (k−1) lk−1 ]\n= fTE(z(k−1)) . (4)\nThen, the decoder gDEC of the rewriter ψ produces a new translation z(k) as\nz(k) = gDEC(H,P(k−1)). (5)\nUltimately, the evaluator φ scores the new translation z(k) with the estimator gEST :{\nP(k) = fTE(z(k))\nq(k) = gEST (H,P(k)) . (6)\nThe implementation can be applied to a variety of architectures. The encoders, fSE and fTE , can be any sequence model, such as CNN (Kim, 2014). The decoder gDEC is compatible with any language model (e.g., Transformer). The estimator gEST is a text matching model, e.g., ESIM (Chen et al., 2017). In Sec. 4, we apply this implementation to improve generic NMT models."
    }, {
      "heading" : "2.2 Training Criteria",
      "text" : "We represent the ground truth target sentence as a (m + 1)-length sequence y = [y0, y1, · · · , ym]. The rewriter ψ is trained via teacher forcing. We use oi to denote the probability of the i-th target word, which is the prediction of feeding its prior words [y0, y1, · · · , yi−1] into the decoder gDEC . The training loss for the rewriter is\nJ ψ = ∑\n1≤i≤m − log(oi[yi]). (7)\nwhere y0 = “[SOS]” and ym = “[EOS]”, marking the ends of a target sentence.\nFor the evaluator φ, we incur a hinge loss between the translation score of the ground truth y and that of the current translation z(k) as{\nq∗ = φ(x,y)\nJ φ = max(0, 1− q∗ + q(k)) . (8)\nAt training time, translation z(k) is generated via greedy search, instead of beam search, to reduce training time."
    }, {
      "heading" : "3 Prioritized Gradient Descent",
      "text" : "We present prioritized gradient descent (PGD) to train the proposed architecture. Instead of the random sampling used in stochastic gradient descent\n(SGD) (Bottou and Bousquet, 2008), PGD uses a priority queue to store previous training cases that receive low scores from the evaluator. Randomly sampled training cases together with those from the priority queue are used for training.\nDetails of PGD are illustrated in Algorithm 1. Initially, we set a priority queue A (1-st line) with a limited size C = B ×E. B is the batch size. E, the expected number of rewriting iterations, is set as K2 . The queue A is ordered with a quality rate in descending order, where the top one corresponds to the highest rate. The quality rate of a certain sample (x,y, z(k)) is computed as\nr(k) = (1− ρ) ∗ BLEU(z(k),y) + ρ ∗ q(k), (9)\nwhere the weight ρ is controlled by an annealing schedule jj+1 with j being the current training epoch and BLEU (Papineni et al., 2002). The rate r(k) is dominated by BLEU in the first few epochs, and is later dominated by the evaluation score q(k) with an increasing number of epochs. This design is to mitigate the cold start problem when training an evaluator φ. At every training epoch, PGD firstly discards a certain number of previous training samples with high quality rates (3-rd line) from queue A. It then replaces them with newly sampled samples S (4-th to 6-th lines). Every sample (x,y, z(k−1), r(k−1)) in queue A is then rewritten into a new translation z(k) by the rewriter. These are scored by the evaluator φ (10-th lines). These new samples are used to respectively train the rewriter ψ and the evaluator φ (14-th to 15-th lines) with Eq. (7) and Eq. (8).\nPGD keeps low-quality translations in the queue A for multi-pass rewriting until they are popped out from queue A with high scores from the eval-\nuator φ. Hence, the evaluator φ is jointly trained with the rewriter to learn discerning the quality of translations from the rewriter ψ, in order to help the rewriter reduce loss in Eq. (7).\nPGD uses a large queue (B×E) to aggregate the past translations and newly sampled cases. Computationally, this is more efficient than explicit B times of rewriting to obtain samples. This requires extra memory space in exchange for lowing training time. In Sec. 5.7, we will show that the additional increase of training time by PGD is less than 20%, which is tolerable."
    }, {
      "heading" : "4 Applications",
      "text" : "Following Sec. 2.1, we use Rewriter-Evaluator to improve RNNSearch and Transformer.\nRNNSearch w/ Rewriter-Evaluator. The improved RNNSearch is illustrated in Fig. 2. The two encoders (i.e., fSE and fTE) and the decoder gDEC are GRU (Chung et al., 2014). We omit computation details of these modules and follow their settings in Bahdanau et al. (2015). Note that, at every decoding step, the hidden state of decoder is attended to not only hi, 1 ≤ i ≤ n but also p (k−1) j , 1 ≤ j ≤ lk−1. We apply co-attention mechanism (Parikh et al., 2016) to model the estimator fEST . Firstly, we capture the semantic alignment between the source sentence x and the translation z(k−1) as αi,j = h T i Wp (k−1) j h̃i = ∑ j exp(αi,j)∑ j′ exp(αi,j′) p (k−1) j p̃ (k−1) j =\n∑ i exp(αi,j)∑ i′ exp(αi′,j) hi\n. (10)\nThen, we use average pooling to extract features and compute the quality score:\nq(k−1) = vT (∑ i h̃i n ⊕ ∑ j p̃ (k−1) j\nlk−1\n) , (11)\nwhere ⊕ is column-wise vector concatenation.\nTransformer w/ Rewriter-Evaluator. The Transformer (Vaswani et al., 2017) is modified to an architecture in Fig. 3. The input to the encoder contains a source sentence x, a special symbol “ALIGN”, and the past translation z(k−1):\nx′ = x [“ALIGN”] z(k−1), (12)\nwhere operation denotes the concatenation of two sequences.\nThe following mask matrix is applied to every layer in the encoder: 1n×n 0T1×n 0n×lk−111×n 1 11×lk−1\n0lk−1×n 0 T 1×lk−1 1lk−1×lk−1  . (13) In this way, the words in x can’t attend to those in z(k−1) and vice versa. “ALIGN” can attend to the words both in x and z(k−1). This design is to avoid cross-sentence attention in encoder layers. In earlier studies, we find it slightly improves the performances of models.\nWe denote the representation for “ALIGN” in the final encoder layer as hALIGN . The estimator fEST obtains the quality score as\nq(k−1) = vThALIGN , (14)\nin which v is a learnable vector."
    }, {
      "heading" : "5 Experiments",
      "text" : "We have conducted extensive experiments on three machine translation tasks: NIST Chinese-toEnglish (Zh→En), WMT’18 Chinese-to-English, and WMT’14 English-to-German (En→De). The results show that Rewriter-Evaluator significantly improves the performances of NMT models and notably outperforms prior post-editing methods. Oracle experiment verifies the effectiveness of the evaluator. Termination accuracy analysis shows our evaluator is much more accurate than prior methods in determining the optimal number of rewriting turns. We also perform ablation studies to explore the effects of some components."
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "For NIST Zh→En, the training set contains 1.25M sentence pairs extracted from LDC corpora, including LDC2002E18, LDC2003E07, LDC2003E14, a portion of LDC2004T07, LDC2004T08, and LDC2005T06. We adopt NIST 2002 (MT02) as the validation set. We use NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), and NIST 2006 (MT06) for tests. For WMT’18 Zh→En1, we use 18.4M preprocessed data, with byte pair encoding (BPE) tokenization (Sennrich et al., 2016). We use newstest2017 for validation and newstest2018 for test. For WMT’14 En→De2, following the same setting as in Vaswani et al. (2017), we use 4.5M preprocessed data that is tokenized via BPE with 32k merge operations and a shared vocabulary for English and German. We use newstest2013 for development and newstest2014 for test.\nWe train all the models with 150k steps for NIST Zh→En, 300k steps for WMT’18 Zh→En, and 300k steps for WMT’14 En→De. We select the model that performs the best on validations and report their performances on test sets. Using multi-bleu.perl3, we measure case-insensitive BLEU scores and case-sensitive ones for NIST Zh→En and WMT’14 En→De, respectively. For WMT’18 Zh→En, we use the case-sensitive BLEU scores calculated by mteval-v13a.pl4. The improvements of the proposed models over the baselines are statistically significant with a reject probability smaller than 0.05 (Koehn, 2004).\nFor RNNSearch, the dimensions of word embeddings and hidden layers are both 600. Encoder has 3 layers and decoder has 2 layers. Dropout rate is set to 0.2. For Transformer, we follow the setting of Transformer-Base in Vaswani et al. (2017). Both models use beam size of 4 and the maximum number of training tokens at every step is 4096. We use Adam (Kingma and Ba, 2014) for optimization. In all the experiments, the proposed models run on NVIDIA Tesla V100 GPUs. For RewriterEvaluator, the maximum number of rewriting iterations K is 6 and termination threshold is 0.05. Hyper-parameters are obtained by grid search, except for the Transformer backbone.\n1http://www.statmt.org/wmt18/translation-task.html. 2http://www.statmt.org/wmt14/translation-task.html. 3https://github.com/moses-smt/mosesdecoder/blob/\nmaster/scripts/generic/multi-bleu.perl. 4https://github.com/moses-smt/mosesdecoder/blob/ master/scripts/generic/mteval-v13a.pl."
    }, {
      "heading" : "5.2 Results on NIST Chinese-to-English",
      "text" : "We adopt the following related baselines: 1) Deliberation Networks (Xia et al., 2017) adopts a second decoder to polish the raw sequence produced by the first-pass decoder; 2) ABD-NMT (Zhang et al., 2018) uses a backward decoder to generate a translation and a forward decoder to refine it with attention mechanism; 3) Adaptive Multi-pass Decoder (Geng et al., 2018) utilizes RL to model the iterative rewriting process.\nTable 1 shows the results of the proposed models and the baselines on NIST. Baseline BLEU scores are from Geng et al. (2018). There are three observations. Firstly, Rewriter-Evaluator significantly improves the translation quality of NMT models. The averaged BLEU score of RNNSearch is raised by 3.1% and that of Transformer is increased by 1.05%. Secondly, the proposed architecture notably outperforms prior multi-pass decoding methods. The performance of RNNSearch w/ Rewriter-Evaluator surpasses those of Deliberation Network by 2.46%, ABD-NMT by 2.06%, and Adaptive Multi-pass Decoder by 1.72%. Because all of these systems use the same backbone of RNN-based NMT models, these results validate that Rewriter-Evaluator is superior to other alternative methods. Lastly, the proposed architecture can improve Transformer backbone by 1.05% on average, and the improvements are consistently observed on tasks from MT03 to MT06."
    }, {
      "heading" : "5.3 Results on WMT Tasks",
      "text" : "To further confirm the effectiveness of the proposed architecture, we make additional comparisons on WMT’14 En→De and WMT’18 Zh→En. The results are demonstrated in Table 2. Because the above methods don’t have results on the two datasets, we re-implement Adaptive Multi-pass Decoding for comparisons.\nThese results are consistent with the observations in Sec. 5.2. We can see that the new architecture can improve BLEU scores on both RNNSearch and Transformer backbones. For example, the improvements on RNNSearch backbone are 2.13% on WMT’14 and 2.24% on WMT’18. On Transformer backbone, scores are raised by 1.38% on WMT’14 and 1.43% on WMT’18 . Furthermore, RNNSearch w/ Rewriter-Evaluator outperforms Adaptive Multi-pass Decoder by 1.31% and 1.32%, respectively, on the two tasks. Interestingly, the proposed architecture on RNNSearch backbone even surpasses Transformer on these two datasets. For example, the BLEU score on WMT’14 increases from 27.53% to 27.86%."
    }, {
      "heading" : "5.4 Oracle Experiment",
      "text" : "We conduct oracle experiments on the test set of WMT’14 En→De to understand potential improvements of our architecture. An oracle selects the iteration that the corresponding rewrite has the highest BLEU score. Its BLEU scores are shown on the\nred dashed lines in Fig. 4. The numbers on the green vertical bars are the BLEU scores of adopting a fixed number of rewriting iterations. Their averaged number is shown on the dashed blue line. BLEU score from using our evaluator is shown on the solid dark-blue line.\nResults show that the evaluator, with 27.86% BLEU score and 28.91 BLEU score, is much better than the strategies of using a fixed number of rewriting turns. The gaps between oracle and the averaged performance by RNNSearch and Transformer with fixed iterations are 1.92% and 1.90%. Using the evaluator, these gaps are reduced relatively by 80.7% for RNNSearch and 75.8% for Transformer, respectively, down to 0.37% and 0.46%. These results show that the evaluator is able to learn an appropriate termination policy, approximating the performances of oracle policy."
    }, {
      "heading" : "5.5 Termination Accuracy Analysis",
      "text" : "We define a metric, percentage of accurate terminations (PAT), to measure how precise a termination policy can be. PAT is computed as\n1 |U | ∑\n(x,y)∈U\nδ(wq(x,y) = wb(x,y)), (15)\nwhere δ is the indicator function that outputs 1 if its argument is true and 0 otherwise. For each pair (x,y) in the test set U , wq(x,y) is the turn index k with the highest quality score maxk q(k) and wb(x,y) is the one with the highest BLEU score\nmaxk BLEU(z (k),y). The translations z(k), 1 ≤ k ≤ K and their scores q(k), 1 ≤ k ≤ K are obtained using Eq. 5 and Eq. 6.\nFor fair comparisons, the maximum number of rewritings is set to 6 for both Rewriter-Evaluator and Adaptive Multi-pass Decoder (Geng et al., 2018). Results in Table 3 show that PAT scores from Rewriter-Evaluator are much higher than those of Adaptive Multi-pass Decoder. For instance, RNNSearch w/ Rewriter-Evaluator surpasses Adaptive Multi-pass Decoder by 40.96% on WMT’14 and 10.35% on WMT’18."
    }, {
      "heading" : "5.6 Ablation Studies",
      "text" : "Table 4 shows the results of ablation studies on NIST, WMT’14, and WMT’18.\nParameter Sharing. The encoders from Eq. (3) and Eq. (4) are shared between the rewriter and the evaluator. We find this improves the performances of the proposed models. For example, on NIST, sharing encoders increases our BLEU score\nfrom 42.25% to 42.79% with the same maximum iteration number of K.\nMaximum Number of Iterations. Increasing the maximum number of turns K generally improves the BLEU scores. For instance, on NIST, K = 8 outperforms K = 2 by 1.0%, K = 4 by 0.46%, and K = 6 by 0.04%. However, described in Sec. 5.7, large K (e.g., 8) can increase inference time cost. Moreover, additional gains in performance from K = 8 is small. We therefore set K = 6 by default."
    }, {
      "heading" : "5.7 Running Time Comparisons",
      "text" : "While achieving improved translation quality, the models are trained with multiple passes of translation. Therefore, a natural question is on the increase of training time and test time. We report results on 4 GPUs with the maximum rewriting turns K = 6 and the beam size set to 8. Results on WMT’14 are listed in Table 5.\nIt shows that Rewriter-Evaluator increases the test time by approximately 4 times, because of multiple passes of decoding. However, training time is only relatively increased by 15% and 18%, respectively on RNNSearch and Transformer, due to the large priority queue used in PGD to store previous translation cases."
    }, {
      "heading" : "6 Related Work",
      "text" : "Multi-pass decoding has been well studied in statistical machine translation (Brown et al., 1993; Koehn et al., 2003, 2007; Och and Ney, 2004; Chiang, 2005; Dyer et al., 2013). Och (2003); Och and Ney (2002) propose training models with minimum error rate criterion on lattices from first-pass decoder. Marie and Max (2015) introduce an iterative method to refine search space generated from simple feature with additional information from more complex feature. Shen et al. (2004) investigate reranking of hypothesis using neural models trained with discriminative criterion. Neubig et al.\n(2015) propose to reconfirm effectiveness of reranking. Chen et al. (2008) present a regeneration of search space from techniques such as n-gram expansion. These approaches are however applied to shallow models such as log-linear models (Och and Ney, 2002).\nOur work is closely related to recent efforts in multi-pass decoding on NMT. In these recent works (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018), the models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in the previous turn. For example, Xia et al. (2017) propose Deliberation Networks that uses a second decoder to polish the raw sequence produced by the first-pass decoder. While these methods have achieved promising results, they lack a proper termination policy for the multi-pass translation process. Zhang et al. (2018) adopt a predefined number of decoding passes, which is not flexible. Geng et al. (2018) incorporate post-editing mechanism into NMT model via RL. However, RL can be unstable for training because of the high variance in gradient estimation. The lack of a proper termination policy results in premature terminations or over-translated sentences, which can largely limit the performance gains of these methods."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper has introduced a novel architecture, Rewriter-Evaluator, that achieves a proper termination policy for multi-pass decoding in NMT. At every translation pass, given the source sentence and its past translation, a rewriter generates a new translation, aiming at making further performance improvements over the past translations. An evaluator estimates the translation quality to determine whether to complete this iterative rewriting process. We also propose PGD that facilitates training the rewriter and the evaluator both jointly and efficiently. We have applied Rewriter-Evaluator to improve mainstream NMT models. Extensive experiments have been conducted on three translation tasks, NIST Zh→En, WMT’18 Zh→En, and WMT’14 En→De, showing that our architecture notably improves the results of NMT models and significantly outperforms other related methods. An oracle experiment and a termination accuracy analysis show that the performance gains can be attributed to the improvements in completing the rewriting process at proper iterations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was done when the first author did internship at Ant Group. We thank anonymous reviewers for their valuable suggestions."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representation.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Léon Bottou", "Olivier Bousquet." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 20, pages 161–168. Curran Associates, Inc.",
      "citeRegEx" : "Bottou and Bousquet.,? 2008",
      "shortCiteRegEx" : "Bottou and Bousquet.",
      "year" : 2008
    }, {
      "title" : "Generalization in reinforcement learning: Safely approximating the value function",
      "author" : [ "Justin A Boyan", "Andrew W Moore." ],
      "venue" : "Advances in neural information processing systems, pages 369–376.",
      "citeRegEx" : "Boyan and Moore.,? 1995",
      "shortCiteRegEx" : "Boyan and Moore.",
      "year" : 1995
    }, {
      "title" : "The mathematics of statistical machine translation: Parameter estimation",
      "author" : [ "Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer." ],
      "venue" : "Computational Linguistics, 19(2):263– 311.",
      "citeRegEx" : "Brown et al\\.,? 1993",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "Regenerating hypotheses for statistical machine translation",
      "author" : [ "Boxing Chen", "Min Zhang", "Aiti Aw", "Haizhou Li." ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 105–112, Manchester, UK.",
      "citeRegEx" : "Chen et al\\.,? 2008",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2008
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668, Vancouver,",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "A hierarchical phrase-based model for statistical machine translation",
      "author" : [ "David Chiang." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan. Association",
      "citeRegEx" : "Chiang.,? 2005",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2005
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Adaptive multi-pass decoder for neural machine translation",
      "author" : [ "Xinwei Geng", "Xiaocheng Feng", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 523–532.",
      "citeRegEx" : "Geng et al\\.,? 2018",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Lin-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz J. Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–133.",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Multi-pass decoding with complex feature guidance for statistical machine translation",
      "author" : [ "Benjamin Marie", "Aurélien Max." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Marie and Max.,? 2015",
      "shortCiteRegEx" : "Marie and Max.",
      "year" : 2015
    }, {
      "title" : "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015",
      "author" : [ "Graham Neubig", "Makoto Morishita", "Satoshi Nakamura." ],
      "venue" : "Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35–41, Kyoto, Japan.",
      "citeRegEx" : "Neubig et al\\.,? 2015",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2015
    }, {
      "title" : "Pre-translation for neural machine translation",
      "author" : [ "Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel." ],
      "venue" : "arXiv preprint arXiv:1610.05243.",
      "citeRegEx" : "Niehues et al\\.,? 2016",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan. Association for Computational Linguistics.",
      "citeRegEx" : "Och.,? 2003",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "Discriminative training and maximum entropy models for statistical machine translation",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia,",
      "citeRegEx" : "Och and Ney.,? 2002",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2002
    }, {
      "title" : "The alignment template approach to statistical machine translation",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational Linguistics, 30:417–449.",
      "citeRegEx" : "Och and Ney.,? 2004",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2004
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249–2255,",
      "citeRegEx" : "Parikh et al\\.,? 2016",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour." ],
      "venue" : "Advances in neural information processing systems, pages 1057–1063.",
      "citeRegEx" : "Sutton et al\\.,? 2000",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Bilateral multi-perspective matching for natural language sentences",
      "author" : [ "Zhiguo Wang", "Wael Hamza", "Radu Florian." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 4144–4150.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Deliberation networks: Sequence generation beyond one-pass decoding",
      "author" : [ "Yingce Xia", "Fei Tian", "Lijun Wu", "Jianxin Lin", "Tao Qin", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1784–1794.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Efficient multipass decoding for synchronous context free grammars",
      "author" : [ "Hao Zhang", "Daniel Gildea." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 209– 217, Columbus, Ohio. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang and Gildea.,? 2008",
      "shortCiteRegEx" : "Zhang and Gildea.",
      "year" : 2008
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4334–",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Asynchronous bidirectional decoding for neural machine translation",
      "author" : [ "Xiangwen Zhang", "Jinsong Su", "Yue Qin", "Yang Liu", "Rongrong Ji", "Hongji Wang." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Encoder-Decoder architecture (Sutskever et al., 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : ", 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : ", 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 213
    }, {
      "referenceID" : 29,
      "context" : ", 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 213
    }, {
      "referenceID" : 33,
      "context" : ", 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : ", 2014) has been widely used in natural language generation, especially neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 213
    }, {
      "referenceID" : 21,
      "context" : "In analogy to the development of statistical machine translation (SMT) (Och and Ney, 2002; Shen et al., 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "In analogy to the development of statistical machine translation (SMT) (Och and Ney, 2002; Shen et al., 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 32,
      "context" : "In analogy to the development of statistical machine translation (SMT) (Och and Ney, 2002; Shen et al., 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 31,
      "context" : ", 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Niehues et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 34,
      "context" : ", 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Niehues et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : ", 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Niehues et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : ", 2004; Zhang and Gildea, 2008), some recent methods in NMT attempt to improve the encoder-decoder architecture with multipass decoding (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018; Niehues et al., 2016).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 28,
      "context" : "(2018) utilize reinforcement learning (RL) (Sutton et al., 2000) to automatically decide the number of decoding passes.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "However, RL is known to be unstable due to the high variance in gradient estimation (Boyan and Moore, 1995).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 27,
      "context" : "In implementations, the rewriter is a conditional language model (Sutskever et al., 2014) and the evaluator is a text matching model (Wang et al.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 30,
      "context" : ", 2014) and the evaluator is a text matching model (Wang et al., 2017).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "Rewriter-Evaluator has been applied to improve two mainstream NMT models, RNNSearch (Bahdanau et al., 2015) and Transformer (Vaswani et al.",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "The encoders, fSE and fTE , can be any sequence model, such as CNN (Kim, 2014).",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "(SGD) (Bottou and Bousquet, 2008), PGD uses a priority queue to store previous training cases that receive low scores from the evaluator.",
      "startOffset" : 6,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "where the weight ρ is controlled by an annealing schedule j j+1 with j being the current training epoch and BLEU (Papineni et al., 2002).",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : ", fSE and fTE) and the decoder gDEC are GRU (Chung et al., 2014).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "We apply co-attention mechanism (Parikh et al., 2016) to model the estimator fEST .",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 29,
      "context" : "The Transformer (Vaswani et al., 2017) is modified to an architecture in Fig.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "4M preprocessed data, with byte pair encoding (BPE) tokenization (Sennrich et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "We adopt the following related baselines: 1) Deliberation Networks (Xia et al., 2017) adopts a second decoder to polish the raw sequence produced by the first-pass decoder; 2) ABD-NMT (Zhang et al.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : ", 2017) adopts a second decoder to polish the raw sequence produced by the first-pass decoder; 2) ABD-NMT (Zhang et al., 2018) uses a backward decoder to generate a translation and a forward decoder to refine it with attention mechanism; 3) Adaptive Multi-pass Decoder (Geng et al.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : ", 2018) uses a backward decoder to generate a translation and a forward decoder to refine it with attention mechanism; 3) Adaptive Multi-pass Decoder (Geng et al., 2018) utilizes RL to model the iterative rewriting process.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "For fair comparisons, the maximum number of rewritings is set to 6 for both Rewriter-Evaluator and Adaptive Multi-pass Decoder (Geng et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Multi-pass decoding has been well studied in statistical machine translation (Brown et al., 1993; Koehn et al., 2003, 2007; Och and Ney, 2004; Chiang, 2005; Dyer et al., 2013).",
      "startOffset" : 77,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "Multi-pass decoding has been well studied in statistical machine translation (Brown et al., 1993; Koehn et al., 2003, 2007; Och and Ney, 2004; Chiang, 2005; Dyer et al., 2013).",
      "startOffset" : 77,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "Multi-pass decoding has been well studied in statistical machine translation (Brown et al., 1993; Koehn et al., 2003, 2007; Och and Ney, 2004; Chiang, 2005; Dyer et al., 2013).",
      "startOffset" : 77,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "Multi-pass decoding has been well studied in statistical machine translation (Brown et al., 1993; Koehn et al., 2003, 2007; Och and Ney, 2004; Chiang, 2005; Dyer et al., 2013).",
      "startOffset" : 77,
      "endOffset" : 175
    }, {
      "referenceID" : 21,
      "context" : "These approaches are however applied to shallow models such as log-linear models (Och and Ney, 2002).",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 31,
      "context" : "In these recent works (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018), the models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in the previous turn.",
      "startOffset" : 22,
      "endOffset" : 79
    }, {
      "referenceID" : 34,
      "context" : "In these recent works (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018), the models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in the previous turn.",
      "startOffset" : 22,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "In these recent works (Xia et al., 2017; Zhang et al., 2018; Geng et al., 2018), the models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in the previous turn.",
      "startOffset" : 22,
      "endOffset" : 79
    } ],
    "year" : 2021,
    "abstractText" : "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of RewriterEvaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.",
    "creator" : "LaTeX with hyperref"
  }
}