{
  "name" : "2021.acl-long.456.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks",
    "authors" : [ "Jinghui Qin", "Xiaodan Liang", "Yining Hong", "Jianheng Tang", "Liang Lin" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5870–5881\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5870\nPrevious math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi duality between symbolic equation generation and problem’s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods1.\n∗Corresponding Author 1The code and the new CM17k dataset are available at\nhttps://github.com/QinJinghui/NS-Solver."
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks have achieved remarkable successes in natural language processing recently. Although neural models have demonstrated performance superior to humans on some tasks, e.g. reading comprehension (Rajpurkar et al., 2016; Devlin et al., 2019; Lan et al.), it still lacks the ability of discrete reasoning, resulting in low accuracy on math reasoning. Thus, it is hard for pure neural network approaches to tackle the task of solving math word problems (MWPs), which requires a model to be capable of natural language understanding and discrete reasoning. MWP solving aims to automatically answer a math word problem by understanding the textual description of the problem and reasoning out the underlying answer. A typical MWP is a short story that describes a partial state of the world and poses a question about an unknown quantity or multiple unknown quantities. To solve an MWP, the relevant quantities need to be identified from the text. Furthermore, the correct operators along with their computation order among these quantities need to be determined. Therefore, integrating neural networks with symbolic reasoning is crucial for solving MWPs. Inspired by the recent amazing progress on neural semantic parsing (Liang et al., 2017a) and reading comprehension (Chen et al., 2019), we address this problem by neural-symbolic computing.\nRecently, many researchers (Wang et al., 2017; Huang et al., 2018; Wang et al., 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al., 2014), apply neural networks to solve MWPs by learning the mapping function between problems and their corresponding equations, and achieve remarkable successes. The encoder uses a neural network to represent a problem as a real-valued vector, and the decoder uses another neural network to\ngenerate an equation or expression token by token. The main difference among previous methods is the way to decode expressions or equations. However, they only follow the encoder-decoder paradigm but lacking the ability to explicitly incorporate essential math symbolic constraints (e.g. commonsense constants, formulation regularization), leading to unexplainable and unreasonable predictions. Besides, most of them only focus on arithmetic MWPs without any unknown, preventing them from generalizing to various types of MWPs, such as equation set problems.\nTo address the above issues, we propose a novel Neural-Symbolic Solver (NS-Solver), which explicitly and seamlessly incorporates different levels of symbolic constraints by auxiliary learning tasks. Our NS-Solver consists of three main components, a problem reader to encode the math word problems into vector representations, a programmer to generate the symbolic grounded equations, which are executed to produce answers, and a symbolic executor to obtain final results. In addition to the supervised training objective between generated symbolic grounded equations and groundtruth equations, our solver is also optimized by four novel auxiliary objectives that enforce four levels of problem understanding and symbolic reasoning. First, we apply number prediction task to predict both the number quantity and number location in the problem in a self-supervised manner. Second, we deploy commonsense constant prediction task to predict what prior commonsense knowledge (e.g. how many legs a chicken has) is required for our solver. Third, we propose program consistency checker to compute the semantic loss between the predicted program and ground-truth equation to ensure reasonable equation mapping. Finally, we also propose a novel duality exploiting task that exploits the quasi duality between symbolic grounded equation generation and the problem’s part-of-speech generation to enhance the understanding ability of our solver. There are some key advantages of our solution. First of all, the above four auxiliary tasks can produce additional training signals, which improves the data efficiency in training and makes our solver more robust. Second, using the predicted constant to constrain the target symbolic table can reduce the search space greatly, which means that our solver can generate correct symbolic grounded equations easier and better. Third, the auxiliary tasks have been proven\nto help reduce the domain gap between seen and unseen MWPs (Sun et al., 2019, 2020), thus improving the reasoning ability of our solver.\nBesides, beyond the current large-scale highquality MWP benchmark that only includes one type of problems, we also construct a large-scale challenging Chinese MWPs dataset CM17K, which contains 4 types of MWPs (arithmetic MWPs, oneunknown linear MWPs, one-unknown non-linear MWPs, equation set problems) with more than 17K samples, to provide a more realistic and challenging benchmark for developing a universal and scalable math solver. Extensive experiments on public Math23K and our proposed CM17k demonstrate the superiority of our NS-Solver compared to stateof-the-art methods in predicting final results while ensuring intermediate equation rationality."
    }, {
      "heading" : "2 Related Work",
      "text" : "Deep learning-based MWP Solvers. Numerous methods have been proposed to tackle the MWP solving task, ranging from rule-based methods (Bakman, 2007; Yuhui et al., 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al., 2015; Koncelkedziorski et al., 2015; Huang et al., 2017; Liang et al., 2018a), to deep learning methods (Ling et al., 2017; Wang et al., 2017, 2018b; Huang et al., 2018; Wang et al., 2018a; Xie and Sun, 2019; Wang et al., 2019; Zhang et al., 2020a,b; Qin et al., 2020; Shen and Jin, 2020; Wu et al., 2020; Chen et al., 2021; Hong et al., 2021a,b). However, most deep learning-based methods only follow the encoder-decoder framework without explicitly incorporating essential math symbolic constraints, resulting in some unexplainable and unreasonable predictions. Besides, most of them only focus on arithmetic MWPs, preventing them from generalizing to various types, such as equation set problems.\nNeural-Symbolic Computing. Neural-symbolic computing has greatly promoted the development of semantic parsing. Jia and Liang (2016); Dong and Lapata (2016); Zhong et al. (2017) applied neural sequence-to-sequence and sequence-to-tree models to semantic parsing with full supervision. Liang et al. (2017b, 2018b) have advanced the stateof-the-art in weakly supervised semantic parsing on knowledge graphs and tabular databases. Al-\nthough most of the successes of semantic parsing are limited to structured data sources, it is not expensive for MWPs since it is easy to crawl lots of problems with annotated equations and answers. Therefore, MWP solving can benefit from supervised neural-symbolic computing. Self-Supervised Learning. Self-supervised auxiliary tasks have been widely used in the fields of natural language understanding (Devlin et al., 2019; Lan et al.). Devlin et al. (2019) applied two selfsupervised auxiliary tasks, masked LM and next sentence prediction, to improve the understanding ability of BERT by pretraining. ALBERT (Lan et al.) introduces sentence-order prediction task to address the ineffectiveness of the next sentence prediction task in BERT. Hendrycks et al. (2019) show that self-supervised learning can improve model robustness and uncertainty. Dual Learning. Dual learning, first proposed by He et al. (2016), is a reinforcement training process that jointly trains a primal task and its dual task. Then Xia et al. (2017) considered it as a way of supervised learning and designed a probabilistic regularization term to exploit the duality. It has been widely applied in various fields, such as machine translation (He et al., 2016), sentiment classification (Xia et al., 2017), question answering (Tang et al., 2017), visual question answering (Li et al., 2018), machine reading comprehension (Xiao et al., 2018), and code generation (Wei et al., 2019). To the best of our knowledge, we are the first to exploit the duality in MWPs. Different from previous works, we design a quasi dual learning method between symbolic grounded equation generation and problem’s part-of-speech generation to enhance the understanding ability by easing the difficulty of generating problems from symbolic equations."
    }, {
      "heading" : "3 Neural-Symbolic Solver",
      "text" : "In this section, we present the design of the proposed NS-Solver. Its backbone mainly consists of a problem reader that encodes the math word problems into vector representations, a programmer to generate the symbolic grounded programs in prefix order, and a symbolic executor to obtain final results. The overview of our NS-Solver is visualized in Fig. 1. We first introduce the backbone of our NS-Solver in section 3.1, and then we introduce other auxiliary tasks in section 3.2."
    }, {
      "heading" : "3.1 Backbone",
      "text" : "Problem Reader. Given a problem text P = {xi}ni=1 processed by number template replacement which maps numeric values in a problem to number templates (e.g., 26 and 82 to n1 and n2 in Fig. 1), the problem reader encodes each token xi in the problem text into an embedding ei. In this work, we deploy a two-layer bidirectional GRU to encode each token xi into an embedding ei = −→ hi+ ←− hi where −→ hi and ←− hi are from forward and backward GRUs, respectively. Besides, our prob-\nlem encoder also outputs a problem representation g0 = −→ hn + ←− h0 as the initial hidden state of our programmer, where −→ hn and ←− h0 are the last hidden state of forward and backward GRUs, respectively. Programmer. The programmer takes the output of the problem reader as input and the problem representation as the initial hidden state, and then decodes a problem as a sequence of tokens {yi}mi=1 which are organized as a prefix equation tree. In this work, we deploy a tree-structured decoder (Xie and Sun, 2019) with attention mechanism (Bahdanau et al., 2015) as the backbone of our programmer and modify them with UET representation (Qin et al., 2020) to support more symbols for multiple types of MWPs. In our programmer, the symbolic table consists of four parts. For each problem, the problem-specific symbolic table contains math operators (+,−, ∗, /, ,̂ =, ;), unknown variable (x and y), a series of commonsense constants (1, 3.14, etc) predicted by the Commonsense Constant Prediction Task in 3.2, and the problemspecific number templates (n1, n2, n3, etc). It should be noticed that ; is a special operator with the lowest priority to integrate multiple equation trees as an ensemble equation tree, so that equation set problems can be handled as simple as arithmetic problems. Executor. We deploy sympy2, which is a python library for symbolic mathematics, as our symbolic executor for obtaining final results by solving generated equations."
    }, {
      "heading" : "3.2 The Design of Auxiliary Tasks",
      "text" : "The MWP solving task remains challenging since previous methods did not take full advantage of the rich semantics contained in a problem and lacking the ability to explicitly incorporate essential math symbolic constraints. In this section, we introduce four auxiliary learning tasks to exploit additional training signals obtained from different tasks and exploit the result of the commonsense constant prediction task to explicitly constrain the constant symbolic table, which can reduce the search space for symbolic generation and ease the difficulty of generating correct constant. Self-supervised Number Prediction (SNP) Tasks. If a solver can fully understand the problem semantics, it should be able to identify the quantity of numbers in a problem (i.e., to count how many numeric values are in the problem) and\n2https://www.sympy.org/\ntheir corresponding locations in the problem text accurately. For example, if the solver can understand the problem in Fig. 1, it should be able to predict there are two numbers(26 and 82) in the problem, and their positions are 15 and 18, respectively. Thus, number quantity prediction and number location prediction are two critical self-supervised tasks to help the problem reader fully understand the problem semantics and measure the ability of problem understanding of a solver. Both two number prediction tasks take the mean of the problem encoder’s outputs {ei}ni=1 as their input and apply a single-layer feed-forward neural network to compute the distribution of number quantity and number locations. The training objectives of two tasks for each problem are formulated as:\nLNQP = − Q∑ i=1 qti log p (qi|P ) ,\nLNLP = − L∑ i=1 lti log p (li|P ) .\n(1)\nwhere LNQP and LNLP denote the loss for the Number Quantity Prediction (NQP) task and Number Location Prediction (NLP) task, respectively. Q and L are the maximum possible quantities of number and maximum possible number locations for a problem at the dataset level. qti and lti represent the ground-truth value on i-th index of the output probability distribution of NQP and NLP, respectively. Commonsense Constant Prediction (CCP) Task. Commonsense constants are important for solving some MWPs while most previous methods only consider the constants 1 and 3.14, which are not enough for a solver to solve problems that need other commonsense constants. However, attaching a lot of constants to the problem-specific symbolic table will enlarge the search space, increasing the difficulty of generating rational symbolic equations. Therefore, we propose a commonsense constant prediction task to predict what prior commonsense knowledge (e.g. a chicken has 2.0 legs and a rabbit has 4.0 legs for the problem in Fig. 1) is required for the solver to solve a problem according to the problem context. In this way, we can reduce the search space greatly, thus improving the performance of our solver. Similar to the number prediction tasks, the commonsense constant prediction task takes the mean of the problem\nencoder’s output {ei}ni=1 as their input and apply a single-layer feed-forward neural network to compute the distribution of number quantity and number locations The training objective for each problem is formulated as:\nLCCP = − C∑ i=1 ctj log p (ci|P ) . (2)\nwhere C is the total number of constants in the symbolic table and cti represents the true value on i-th index of the output probability distribution. Since it is impossible for the commonsense constant prediction task to achieve 100% accuracy, in addition to the predicted constants, we add three extra constants that are not predicted but with the highest probability into the symbolic table, making a better trade-off between the size of the search space and prediction accuracy. Program Consistency Checker (PCC). Although a problem can be solved by multiple equivalent but different equations, the predicted equations should be consistent with label equations as much as possible in the supervised learning setting. Therefore, we propose a program consistency checker to check the symbolic program consistency and regularize the model by computing semantic loss between the predicted symbolic program and ground-truth equation to ensure the reasonable symbolic equation mapping. Let ŷi and yi represent the predicted symbol and ground-truth symbol, pi represents the probability of ŷi, the semantic loss is obtained by computing a distance between the predicted distribution and ground-truth distribution as:\nLPCC = −log ∑ i ∏ ŷi=yi pi ∏ ŷi 6=yi (1− pi) . (3)\nDuality Exploiting (DE) Task. Many previous works (He et al., 2016; Xia et al., 2017; Xiao et al., 2018; Wei et al., 2019) have shown promising results by dual learning framework. Although intuitively, MWP solving and MWP generation are related to each other, i.e., the input of MWP solving is the output of MWP generation, and vice versa, it is very hard for the MWP generation task to generate good enough problems only by the equations without any topic information. Therefore, we propose a duality exploiting task to enhance the understanding ability of our solver by exploiting the quasi duality between symbolic grounded equation generation and the problem’s part-of-speech\ngeneration. Given a pair of a problem and its corresponding equations (P ,T ), and P ′ is the part-ofspeech of P 3, the training objective of the duality exploiting task is formulated as:\nLdual = [ log p̂(P ′) + log p (T |P )−\nlog p̂(T )− log p ( P ′|T )]2 .\n(4)\nwhere p̂(P ′) and p̂(T ) are marginal distributions, which can be modeled by their LSTM (Hochreiter and Schmidhuber, 1997)-based language models, respectively. Besides, we deploy a tree-structure encoder inspired by GTS (Xie and Sun, 2019) to encode equations in prefix for POS generation."
    }, {
      "heading" : "3.3 Training Objective",
      "text" : "Given the training dataset D={(P i, T 1), (P 2, T 2), · · · ,(PN , TN ) }, where T i is the universal expression tree of problem P i, we minimize the following loss function for our NS-Solver:\nL = ∑\n(P,T )∈D\n[Lent1 + λ1 ∗ Ldual + λ2 ∗ LPCC\n+λ3 ∗ (LNQP + LNLP ) + λ4 ∗ LCCP ] . (5)\nwhere\nLent1 = − log m∏ t=1 prob(yt|P ) (6)\nwhere m denotes the size of T, and yt denotes the t-th output. {λi}4i=1 are empirical values that will be detailed in Section 4.2.\nFor the duality exploiting task, there is another loss for training the branch of the problem’s partof-speech generation: LPOS = ∑\n(P ′,T )∈D\n[Lent2+λ5∗Ldual+λ6∗LPCC′ ].\n(7) where\nLent2 = − log n∏\nt=1\nprob(xt|T ) (8)\nwhere n denotes the size of P, and xt denotes the t-th output. LPCC′ is the semantic loss between predicted POS and the ground-truth POS. {λi}6i=5 are empirical values that will also be detailed in Section 4.2.\n3We use Jieba (https://github.com/fxsjy/jieba) to generate the POS of a problem."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 CM17K Dataset",
      "text" : "Most public MWPs datasets are quite small such as ALG514 or exist some incorrect labels such as Dolphin18K. An exception is the Math23K dataset, which contains 23161 problems labeled well with structured equations and answers. However, it only contains one-unknown linear math word problems, which is not sufficient to validate the ability of a math solver about solving multiple types of MWPs. Therefore, we introduce a new high-quality math word problems dataset, called CM17K, to validate the universality of a solver and provide a more realistic and challenging benchmark for developing a universal and scalable math solver. We collect CM17K from two education websites4. These problems are oriented grades 6-12, containing 4 types of MWPs with more than 17K samples, including 6215 arithmetic MWPs, 5193 one-unknown linear MWPs, 3129 one-unknown non-linear MWPs, and 2498 equation set problems. It should be noticed that our dataset is sufficient for validating the universality of math word problem solvers since these problems can cover most cases about MWPs. We label our data with structured equations and answers following Math23K (Wang et al., 2017). We split our CM17K into train/valid/test sets at a ratio of 8:1:1.\nThe data statistics of Math23K and CM17K are shown in Table 1. From the statistics, we can see that all statistics of CM17K are larger than Math23K. This shows that our dataset is more challenging and difficult for math word problem solvers. Besides, since CM17K contains more types of MWPs than Math23K, CM17K is more suitable\n4http://www.zxxk.com/ and http://www.jyeoo.com/\nfor validating the reasoning ability of a solver than Math23K."
    }, {
      "heading" : "4.2 Experimental Setup and Training Details",
      "text" : ""
    }, {
      "heading" : "4.2.1 Datasets, Baselines, and Metric",
      "text" : "We conduct experiments on Math23K and our CM17K. The main state-of-the-arts to be compared are as follows: DNS (Wang et al., 2017) is a universal solver based on the seq2seq model with significant number identification (SNI). GTS (Xie and Sun, 2019) is a goal-driven tree-structured MWP solver. StackDecoder (Chiang and Chen, 2019) is an universal semantically-aligned math word problems solver. (Zhang et al., 2020a) is an enhanced GTS with teacher-student distillation and multi-decoder ensemble. Again, following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we use answer accuracy as the evaluation metric: if the calculated value of the predicted equation tree equals to the true answer, it is thought as correct since the predicted expression is equivalent to the target expression."
    }, {
      "heading" : "4.2.2 Implementation Details",
      "text" : "We use Pytorch5 to implement our model on Linux with an NVIDIA RTX2080Ti GPU card. All those words with fewer than 5 occurrences are converted into a special token UNK. The size of word embeddings and all hidden states for other layers are set as 128 and 512, respectively. Our model is optimized by ADAM optimizor (Kingma and Ba, 2015) with β1 = 0.9, β2 =0.999, and = 1e−8. The mini-batch size is set as 32. The initial learning rate is set as 1e−3 and then decreases to half every 40 epochs. To prevent overfitting, we set dropout rate as 0.5 and weight decay as 1e−5. Finally, we conduct greedy search to generate symbolic equation trees. We set λ1, λ2, λ3, λ5, and λ6 as 0.0005, 0.01, 1.0, 0.005, and 0.1 for both datasets, respectively. We set λ4 as 0.000001 for Math23K while we set λ4 as 1.0 for CM17K. All constants are extracted from the training set. In each epoch, all training data is shuffled randomly and then cut into mini-batches."
    }, {
      "heading" : "4.3 Answer Accuracy",
      "text" : "Following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we conduct 5- fold cross-validation on Math23K. For CM17K, we evaluate the performance on the test set. The results are shown in Table 2. From Table 2, we can observe\n5http://pytorch.org\nthat benefiting from the four new auxiliary tasks and neural-symbolic paradigm, our NS-Solver outperforms the baselines on both datasets in terms of answer accuracy. Specifically, for Math23K and CM17K, the accuracy gains of NS-Solver over GTS are 1.37% and 5.93%, respectively. Comparing with TSN-MD, our solver outperforms it by about 0.6% on Math23K. It shows that our model is more feasible for solving multiple types of MWPs. It also shows that our NS-Solver is more effective than other state-of-the-art models on the real-world scenario that needs to solve various MWPs with a unified solver."
    }, {
      "heading" : "4.4 Comparisons on different subsets",
      "text" : "We drill down to analyze the generalization of DNS, GTS, and NS-Solver on different types of MWPs in the test subset of CM17K. Their answer accuracy on different types of MWPs is shown in Table 3. We can observe that our NS-Solver outperforms the other two models by a large margin on all subsets. Specifically, the accuracy gains of our NS-Solver over GTS on four subsets are 3.87%, 9.12%, 6.99%, and 9.44%. This shows that with the help of four auxiliary tasks, our NS-Solver obtains better generalization ability on multiple types of MWPs than baselines."
    }, {
      "heading" : "4.5 Performance on Tree Length",
      "text" : "Intuitively, the size of the symbolic equation tree is proportional to the complexity of the mathematical relationship in the problem. The more complex the mathematical relationship is, the more difficult it is to solve the problem. Here, we compare our proposed NS-Solver with GTS on CM17K to show the superiority of our NS-Solver on different equation tree sizes. The answer accuracies for different sizes of expression trees on CM17K test subset are shown in Fig. 2. We can see that there is a tendency\nfor answer accuracy to degrade with the growth of the problem complexity measured as the size of the equation tree, and our NS-Solver outperforms GTS on most cases of different equation tree sizes. This shows our NS-Solver can better model the mathematical relationships of the problem than GTS. It can also be noticed that the improvement of our NS-Solver over the GTS is increasing when the problems become more complex.\nHowever, although our model outperforms other methods, there still has room for improvement in semantic understanding and symbolic reasoning since longer equations often match with more complex MWPs which entail more complex math relationships."
    }, {
      "heading" : "4.5.1 Ablation on different auxiliary tasks",
      "text" : "We study the contribution of different auxiliary tasks of our NS-Solver. For this purpose, we consider five different combinations: 1) only the backbone [NS-Solver - CCP - SNP - PCC - DE]; 2) backbone + duality exploiting task [NS-Solver - CCP - SNP - PCC]; 3) backbone + duality exploiting task + program consistent checker [NS-Solver - CCP - SNP]; 4) backbone + duality exploiting task + program consistent checker + number prediction tasks [NS-Solver - CCP]; and 5) the proposed NS-Solver [NS-solver]. For each of these combinations, each model was trained for 80 epochs on CM17K and validated on its test subset. The learning rate decreased to half every 20 epochs. The results are provided in Fig. 4.\nAs one can see, all four auxiliary tasks can improve performance. Specifically, the accuracy gains of DE, PCC, SNP, and CCP are 1.00%, 1.41%, 1.11%, and 1.12%, respectively. Besides, the binary accuracies of the two SNP tasks are 97% (number quantity prediction) and 96.8% (number location prediction). Moreover, the accuracy of our CCP\ntask is 97.8%. This shows that our auxiliary tasks can enhance our NS-Solver to enforce better problem understanding and symbol reasoning. Overall, our proposed NS-Solver achieves the best answer accuracy."
    }, {
      "heading" : "4.6 Case Study",
      "text" : "We also present the results of our NS-Solver with different combinations of four auxiliary tasks in Fig. 3. Benefiting from explicitly exploiting the probabilistic correlation between two quasi dual tasks to regularize the training process in our duality exploiting (DE) task, our [NS-solver - CCP - SNP - PCC] can generate correct equations by understanding the problem better while [NS-solver - CCP - SNP - PCC - DE] generates error equations, as shown in Case 1. With the program consis-\ntency checker (PCC) that effectively regularizes the model’s output by constraining the distance between predicted symbols and ground-truth symbols during training, [NS-solver - CCP - SNP] can generate more consistent equations with the ground-truth than [NS-solver - CCP - SNP - PCC], as shown in Case 2. With self-supervised number prediction (SNP), [NS-solver - CCP] can generate better results and avoid generating symbols that do not belong to the problem, as shown in Case 3. With commonsense constant prediction (CCP), our NS-Solver manages to choose correct constants by constraining the constant symbolic table using predicted results of CCP. As shown in Case 4, [NS-solver - CCP] chooses error constant 10 while NS-solver chooses two correct constants. Besides, although GTS and NS-Solver generate the same symbols sometimes, our NS-Solver generates correct equations with the help of our four auxiliary objectives, as shown in Case 5. Overall, all four auxiliary tasks can improve our NS-Solver’s understanding and reasoning ability."
    }, {
      "heading" : "4.7 Extends to other backbone",
      "text" : "To show that our auxiliary tasks can be adapted to other backbones, we replace GTS’s encoder with BERT (BERT + Tree Decoder) and NS-Solver’s encoder with BERT (NS-Solver + BERT), where we adopt a Chinese BERT-base pre-trained with whole word masking (Cui et al., 2020). We conduct experiments on CM17K. The results are shown in Table 4. We can observe that with auxiliary tasks, our NS-Solver + BERT still can outperform BERT + Tree Decoder, which shows that our auxiliary tasks’ strong generalization."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by four auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate a symbolic grounded program, and a symbolic executor to obtain final results. In addition to supervised learning with target expression, our solver is also optimized via four new auxiliary objectives that enforce four levels of symbolic reasoning. Besides, we also construct a new dataset CM17K containing 4 types of MWPs with more than 17K samples, which provides a more realistic and challenging benchmark for developing a universal and scalable math solver. Extensive experiments on Math23K and CM17K demonstrate the superiority of our NS-Solver compared to state-ofthe-art methods in answer accuracy while ensuring intermediate equation rationality."
    }, {
      "heading" : "6 Ethical Impact",
      "text" : "We collected CM17K from two online education websites, which is only used for academic research, and the copyright belongs to the original websites. This work may inspire research in the field of numerical reasoning.\nAcknowledgements This work was supported in part by National Key R&D Program of China under Grant No.2020AAA0109700, National Natural Science Foundation of China (NSFC) under Grant No.U19A2073, No.61976233 and No. 61836012, the Natural Science Foundation of Guangdong Province under Grant No. 2017A030312006, Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key) Grant No.2019B1515120039,\nShenzhen Fundamental Research Program (Project No.RCYX20200714114642083 and No.JCYJ20190807154211365), Zhijiang Lab’s Open Fund (No.2020AA3AB14), CSIG Young Fellow Support Fund, and Guangdong Provincial Key Laboratory of Information Security Technology."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust understanding of word problems with extraneous information",
      "author" : [ "Yefim Bakman." ],
      "venue" : "Computing Research Repository, arXiv:math/0701393.",
      "citeRegEx" : "Bakman.,? 2007",
      "shortCiteRegEx" : "Bakman.",
      "year" : 2007
    }, {
      "title" : "GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning",
      "author" : [ "Jiaqi Chen", "Jianheng Tang", "Jinghui Qin", "Xiaodan Liang", "Lingbo Liu", "Eric P. Xing", "Liang Lin." ],
      "venue" : "arXiv preprint arXiv:2105.14517.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension",
      "author" : [ "Xinyun Chen", "Chen Liang", "Adams Wei Yu", "Denny Zhou", "Dawn Song", "Quoc V Le." ],
      "venue" : "International Conference on Learning",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantically-aligned equation generation for solving and reasoning math word problems",
      "author" : [ "Ting-Rui Chiang", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Chiang and Chen.,? 2019",
      "shortCiteRegEx" : "Chiang and Chen.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33–43, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma." ],
      "venue" : "Advances in neural information processing systems, pages 820–828.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Using self-supervised learning can improve model robustness and uncertainty",
      "author" : [ "Dan Hendrycks", "Mantas Mazeika", "Saurav Kadavath", "Dawn Song." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 15637–15648.",
      "citeRegEx" : "Hendrycks et al\\.,? 2019",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Learning by fixing: Solving math word problems with weak supervision",
      "author" : [ "Yining Hong", "Qing Li", "Daniel Ciao", "Siyuan Huang", "Song-Chun. Zhu." ],
      "venue" : "Thirty-Fifth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Hong et al\\.,? 2021a",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2021
    }, {
      "title" : "Smart: A situation model for algebra story problems via attributed grammar",
      "author" : [ "Yining Hong", "Qing Li", "Ran Gong", "Daniel Ciao", "Siyuan Huang", "Song-Chun. Zhu." ],
      "venue" : "The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI-21.",
      "citeRegEx" : "Hong et al\\.,? 2021b",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural math word problem solver with reinforcement learning",
      "author" : [ "Danqing Huang", "Jing Liu", "Chin-Yew Lin", "Jian Yin." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 213–223. Association for Computational Lin-",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning fine-grained expressions to solve math word problems",
      "author" : [ "Danqing Huang", "Shuming Shi", "Chin-Yew Lin", "Jian Yin." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 805–814. Association",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "How well do computers solve math word problems? large-scale dataset construction and evaluation",
      "author" : [ "Danqing Huang", "Shuming Shi", "Chin-Yew Lin", "Jian Yin", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12–22, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "international conference on learning representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Parsing algebraic word problems into equations",
      "author" : [ "Rik Koncelkedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Dumas Ang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:585–597.",
      "citeRegEx" : "Koncelkedziorski et al\\.,? 2015",
      "shortCiteRegEx" : "Koncelkedziorski et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay." ],
      "venue" : "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 271–281.",
      "citeRegEx" : "Kushman et al\\.,? 2014",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual question generation as dual task of visual question answering",
      "author" : [ "Yikang Li", "Nan Duan", "Bolei Zhou", "Xiao Chu", "Wanli Ouyang", "Xiaogang Wang", "Ming Zhou." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "A meaning-based statistical English math word problem solver",
      "author" : [ "Chao-Chun Liang", "Yu-Shiang Wong", "Yi-Chung Lin", "Keh-Yih Su." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Liang et al\\.,? 2018a",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision",
      "author" : [ "Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Liang et al\\.,? 2017a",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision",
      "author" : [ "Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Liang et al\\.,? 2017b",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2017
    }, {
      "title" : "Memory augmented policy optimization for program synthesis and semantic parsing",
      "author" : [ "Chen Liang", "Mohammad Norouzi", "Jonathan Berant", "Quoc V Le", "Ni Lao." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9994–10006.",
      "citeRegEx" : "Liang et al\\.,? 2018b",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "author" : [ "Wang Ling", "Dani Yogatama", "Chris Dyer", "Phil Blunsom." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ling et al\\.,? 2017",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to use formulas to solve simple arithmetic problems",
      "author" : [ "Arindam Mitra", "Chitta Baral." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2144–2153. Association for",
      "citeRegEx" : "Mitra and Baral.,? 2016",
      "shortCiteRegEx" : "Mitra and Baral.",
      "year" : 2016
    }, {
      "title" : "Semantically-aligned universal tree-structured solver for math word problems",
      "author" : [ "Jinghui Qin", "Lihui Lin", "Xiaodan Liang", "Rumin Zhang", "Liang Lin." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Solving general arithmetic word problems",
      "author" : [ "Subhro Roy", "Dan Roth." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752. Association for Computational Linguistics.",
      "citeRegEx" : "Roy and Roth.,? 2015",
      "shortCiteRegEx" : "Roy and Roth.",
      "year" : 2015
    }, {
      "title" : "Unit dependency graph and its application to arithmetic word problem solving",
      "author" : [ "Subhro Roy", "Dan Roth." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence, pages 3082–3088.",
      "citeRegEx" : "Roy and Roth.,? 2016",
      "shortCiteRegEx" : "Roy and Roth.",
      "year" : 2016
    }, {
      "title" : "Mapping to declarative knowledge for word problem solving",
      "author" : [ "Subhro Roy", "Dan Roth." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:159–172.",
      "citeRegEx" : "Roy and Roth.,? 2018",
      "shortCiteRegEx" : "Roy and Roth.",
      "year" : 2018
    }, {
      "title" : "Solving math word problems with multi-encoders and multi-decoders",
      "author" : [ "Yibin Shen", "Cheqing Jin." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2924–2934, Barcelona, Spain (Online). International Committee",
      "citeRegEx" : "Shen and Jin.,? 2020",
      "shortCiteRegEx" : "Shen and Jin.",
      "year" : 2020
    }, {
      "title" : "Automatically solving number word problems by semantic parsing and reasoning",
      "author" : [ "Shuming Shi", "Yuehui Wang", "Chin-Yew Lin", "Xiaojiang Liu", "Yong Rui." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Shi et al\\.,? 2015",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised domain adaptation through selfsupervision",
      "author" : [ "Yu Sun", "Eric Tzeng", "Trevor Darrell", "Alexei A Efros." ],
      "venue" : "arXiv preprint arXiv:1909.11825.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Test-time training with self-supervision for generalization under distribution shifts",
      "author" : [ "Yu Sun", "Xiaolong Wang", "Zhuang Liu", "John Miller", "Alexei Efros", "Moritz Hardt." ],
      "venue" : "International Conference on Machine Learning, pages 9229–9248. PMLR.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Question answering and question generation as dual tasks",
      "author" : [ "Duyu Tang", "Nan Duan", "Tao Qin", "Zhao Yan", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1706.02027.",
      "citeRegEx" : "Tang et al\\.,? 2017",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2017
    }, {
      "title" : "Translating a math word problem to a expression tree",
      "author" : [ "Lei Wang", "Yan Wang", "Deng Cai", "Dongxiang Zhang", "Xiaojiang Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1064–1069. Associa-",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Mathdqn: Solving arithmetic word problems via deep reinforcement learning",
      "author" : [ "Lei Wang", "Dongxiang Zhang", "Lianli Gao", "Jingkuan Song", "Long Guo", "Heng Tao Shen." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence, pages 5545–5552.",
      "citeRegEx" : "Wang et al\\.,? 2018b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Template-based math word problem solvers with recursive neural networks",
      "author" : [ "Lei Wang", "Dongxiang Zhang", "Zhang Jipeng", "Xing Xu", "Lianli Gao", "Bing Tian Dai", "Heng Tao Shen." ],
      "venue" : "Thirty-Third AAAI Conference on Artificial Intelligence, pages",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep neural solver for math word problems",
      "author" : [ "Yan Wang", "Xiaojiang Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845– 854. Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Code generation as a dual task of code summarization",
      "author" : [ "Bolin Wei", "Ge Li", "Xin Xia", "Zhiyi Fu", "Zhi Jin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 6559–6569.",
      "citeRegEx" : "Wei et al\\.,? 2019",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    }, {
      "title" : "A knowledge-aware sequence-to-tree network for math word problem solving",
      "author" : [ "Qinzhuo Wu", "Qi Zhang", "Jinlan Fu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dual supervised learning",
      "author" : [ "Yingce Xia", "Tao Qin", "Wei Chen", "Jiang Bian", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3789– 3798. JMLR. org.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Dual ask-answer network for machine reading comprehension",
      "author" : [ "Han Xiao", "Feng Wang", "Jianfeng Yan", "Jingyao Zheng." ],
      "venue" : "arXiv preprint arXiv:1809.01997.",
      "citeRegEx" : "Xiao et al\\.,? 2018",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2018
    }, {
      "title" : "A goal-driven tree-structured neural model for math word problems",
      "author" : [ "Zhipeng Xie", "Shichao Sun." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299–5305. International",
      "citeRegEx" : "Xie and Sun.,? 2019",
      "shortCiteRegEx" : "Xie and Sun.",
      "year" : 2019
    }, {
      "title" : "Frame-based calculus of solving arithmetic multi-step addition and subtraction word problems",
      "author" : [ "Ma Yuhui", "Zhou Ying", "Cui Guangzuo", "Ren Yun", "Huang Ronghuai." ],
      "venue" : "International Workshop on Education Technology and Computer Science, vol-",
      "citeRegEx" : "Yuhui et al\\.,? 2010",
      "shortCiteRegEx" : "Yuhui et al\\.",
      "year" : 2010
    }, {
      "title" : "Teacher-student networks with multiple decoders for solving math word problem",
      "author" : [ "Jipeng Zhang", "Roy Ka-Wei Lee", "Ee-Peng Lim", "Wei Qin", "Lei Wang", "Jie Shao", "Qianru Sun." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Ar-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graphto-tree learning for solving math word problems",
      "author" : [ "Jipeng Zhang", "Lei Wang", "Roy Ka-Wei Lee", "Yi Bin", "Yan Wang", "Jie Shao", "Ee-Peng Lim." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3928–",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1709.00103.",
      "citeRegEx" : "Zhong et al\\.,? 2017",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "Learn to solve algebra word problems using quadratic programming",
      "author" : [ "Lipu Zhou", "Shuaixiang Dai", "Liwei Chen." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822. Association for",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Inspired by the recent amazing progress on neural semantic parsing (Liang et al., 2017a) and reading comprehension (Chen et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : ", 2017a) and reading comprehension (Chen et al., 2019), we address this problem by neural-symbolic computing.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 41,
      "context" : "Recently, many researchers (Wang et al., 2017; Huang et al., 2018; Wang et al., 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al.",
      "startOffset" : 27,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Recently, many researchers (Wang et al., 2017; Huang et al., 2018; Wang et al., 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al.",
      "startOffset" : 27,
      "endOffset" : 134
    }, {
      "referenceID" : 46,
      "context" : "Recently, many researchers (Wang et al., 2017; Huang et al., 2018; Wang et al., 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al.",
      "startOffset" : 27,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Recently, many researchers (Wang et al., 2017; Huang et al., 2018; Wang et al., 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al.",
      "startOffset" : 27,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : ", 2018b, 2019; Xie and Sun, 2019; Chiang and Chen, 2019), inspired by an encoder-decoder framework (Cho et al., 2014), apply neural networks to solve MWPs by learning the mapping function between problems and their corresponding equations, and achieve remarkable successes.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Numerous methods have been proposed to tackle the MWP solving task, ranging from rule-based methods (Bakman, 2007; Yuhui et al., 2010), statistical machine learning methods (Kushman et al.",
      "startOffset" : 100,
      "endOffset" : 134
    }, {
      "referenceID" : 47,
      "context" : "Numerous methods have been proposed to tackle the MWP solving task, ranging from rule-based methods (Bakman, 2007; Yuhui et al., 2010), statistical machine learning methods (Kushman et al.",
      "startOffset" : 100,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : ", 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 51,
      "context" : ", 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 27,
      "context" : ", 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : ", 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 32,
      "context" : ", 2010), statistical machine learning methods (Kushman et al., 2014; Zhou et al., 2015; Roy and Roth, 2015, 2016; Mitra and Baral, 2016; Huang et al., 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al.",
      "startOffset" : 46,
      "endOffset" : 176
    }, {
      "referenceID" : 34,
      "context" : ", 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al., 2015; Koncelkedziorski et al., 2015; Huang et al., 2017; Liang et al., 2018a), to deep learning methods (Ling et al.",
      "startOffset" : 54,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : ", 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al., 2015; Koncelkedziorski et al., 2015; Huang et al., 2017; Liang et al., 2018a), to deep learning methods (Ling et al.",
      "startOffset" : 54,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : ", 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al., 2015; Koncelkedziorski et al., 2015; Huang et al., 2017; Liang et al., 2018a), to deep learning methods (Ling et al.",
      "startOffset" : 54,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : ", 2016; Roy and Roth, 2018), semantic parsing methods (Shi et al., 2015; Koncelkedziorski et al., 2015; Huang et al., 2017; Liang et al., 2018a), to deep learning methods (Ling et al.",
      "startOffset" : 54,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "It has been widely applied in various fields, such as machine translation (He et al., 2016), sentiment classification (Xia et al.",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 44,
      "context" : ", 2016), sentiment classification (Xia et al., 2017), question answering (Tang et al.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : ", 2017), question answering (Tang et al., 2017), visual question answering (Li et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : ", 2017), visual question answering (Li et al., 2018), machine reading comprehension (Xiao et al.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 45,
      "context" : ", 2018), machine reading comprehension (Xiao et al., 2018), and code generation (Wei et al.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 46,
      "context" : "In this work, we deploy a tree-structured decoder (Xie and Sun, 2019) with attention mechanism (Bahdanau et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "In this work, we deploy a tree-structured decoder (Xie and Sun, 2019) with attention mechanism (Bahdanau et al., 2015) as the backbone of our programmer and modify them with UET representation (Qin et al.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : ", 2015) as the backbone of our programmer and modify them with UET representation (Qin et al., 2020) to support more symbols for multiple types of MWPs.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "Many previous works (He et al., 2016; Xia et al., 2017; Xiao et al., 2018; Wei et al., 2019) have shown promising results by dual learning framework.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 44,
      "context" : "Many previous works (He et al., 2016; Xia et al., 2017; Xiao et al., 2018; Wei et al., 2019) have shown promising results by dual learning framework.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 45,
      "context" : "Many previous works (He et al., 2016; Xia et al., 2017; Xiao et al., 2018; Wei et al., 2019) have shown promising results by dual learning framework.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 42,
      "context" : "Many previous works (He et al., 2016; Xia et al., 2017; Xiao et al., 2018; Wei et al., 2019) have shown promising results by dual learning framework.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "where p̂(P ′) and p̂(T ) are marginal distributions, which can be modeled by their LSTM (Hochreiter and Schmidhuber, 1997)-based language models, respectively.",
      "startOffset" : 88,
      "endOffset" : 122
    }, {
      "referenceID" : 46,
      "context" : "Besides, we deploy a tree-structure encoder inspired by GTS (Xie and Sun, 2019) to encode equations in prefix for POS generation.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "label our data with structured equations and answers following Math23K (Wang et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 41,
      "context" : "The main state-of-the-arts to be compared are as follows: DNS (Wang et al., 2017) is a universal solver based on the seq2seq model with significant number identification (SNI).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "StackDecoder (Chiang and Chen, 2019) is an universal semantically-aligned math word problems solver.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 48,
      "context" : "(Zhang et al., 2020a) is an enhanced GTS with teacher-student distillation and multi-decoder ensemble.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 41,
      "context" : "Again, following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we use answer accuracy as the evaluation metric: if the calculated value of the predicted equation tree equals to the true answer, it is thought as correct since the predicted expression",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Again, following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we use answer accuracy as the evaluation metric: if the calculated value of the predicted equation tree equals to the true answer, it is thought as correct since the predicted expression",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 46,
      "context" : "Again, following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we use answer accuracy as the evaluation metric: if the calculated value of the predicted equation tree equals to the true answer, it is thought as correct since the predicted expression",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Our model is optimized by ADAM optimizor (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 41,
      "context" : "Following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we conduct 5fold cross-validation on Math23K.",
      "startOffset" : 22,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "Following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we conduct 5fold cross-validation on Math23K.",
      "startOffset" : 22,
      "endOffset" : 83
    }, {
      "referenceID" : 46,
      "context" : "Following prior works (Wang et al., 2017; Chiang and Chen, 2019; Xie and Sun, 2019), we conduct 5fold cross-validation on Math23K.",
      "startOffset" : 22,
      "endOffset" : 83
    }, {
      "referenceID" : 46,
      "context" : "Model BERT + Tree Decoder (Xie and Sun, 2019) NS-Solver + BERT",
      "startOffset" : 26,
      "endOffset" : 45
    } ],
    "year" : 2021,
    "abstractText" : "Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi duality between symbolic equation generation and problem’s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods1. ∗Corresponding Author The code and the new CM17k dataset are available at https://github.com/QinJinghui/NS-Solver.",
    "creator" : "LaTeX with hyperref"
  }
}