{
  "name" : "2021.acl-long.510.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
    "authors" : [ "Chen Liang", "Simiao Zuo", "Minshuo Chen", "Haoming Jiang", "Xiaodong Liu", "Pengcheng He", "Tuo Zhao", "Weizhu Chen" ],
    "emails" : [ "tourzhao}@gatech.edu", "xiaodl@microsoft.com", "penhe@microsoft.com", "wzchen@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6524–6538\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6524\nThe Lottery Ticket Hypothesis suggests that an over-parametrized network consists of “lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as “winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as “super tickets”. We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning1."
    }, {
      "heading" : "1 Introduction",
      "text" : "The Lottery Ticket Hypothesis (LTH, Frankle and Carbin (2018)) suggests that an over-parameterized network consists of “lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can 1) match the performance of the full model; and 2)\n∗Work was done at Microsoft Azure AI. 1Our codes are available at\nhttps://github.com/cliang1453/ super-structured-lottery-tickets.\noutperform randomly sampled subnetworks of the same size (i.e., “random tickets”). The existence of such a collection of tickets, which is usually referred to as “winning tickets”, indicates the potential of training a smaller network to achieve the full model’s performance. LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).\nAside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a). In natural language processing, Chen et al. (2020b); Prasanna et al. (2020) have shown existence of the winning tickets in pre-trained language models. These tickets can be identified when fine-tuning the pre-trained models on downstream tasks. As the pre-trained models are usually extremely overparameterized (e.g., BERT Devlin et al. (2019), GPT-3 Brown et al. (2020), T5 Raffel et al. (2019)), previous works mainly focus on searching for a highly compressed subnetwork that matches the performance of the full model. However, behavior of the winning tickets in lightly compressed subnetworks is largely overlooked.\nIn this paper, we study the behavior of the winning tickets in pre-trained language models, with a particular focus on lightly compressed subnetworks. We observe that generalization performance of the winning tickets selected at appropriate compression ratios can not only match, but also exceed that of the full model. In particular, we observe a phase transition phenomenon (Figure 1): The test accuracy improves as the compression ratio grows until a certain threshold (Phase I); Passing the threshold, the accuracy deteriorates, yet is still better than that of the random tickets (Phase II). In Phase III, where the model is highly compressed,\ntraining collapses. We refer to the set of winning tickets selected on that threshold as “super tickets”.\nWe interpret the phase transition in the context of trade-offs between model bias and variance (Friedman et al., 2001, Chapter 7). It is well understood that an expressive model induces a small bias, and a large model induces a large variance. We classify the tickets into three categories: non-expressive tickets, lightly expressive tickets, and highly expressive tickets. The full model has a strong expressive power due to over-parameterization, so that its bias is small. Yet its variance is relatively large. In Phase I, by removing non-expressive tickets, variance of the selected subnetwork reduces, while model bias remains unchanged and the expressive power sustains. Accordingly, generalization performance improves. We enter Phase II by further increasing the compression ratio. Here lightly expressive tickets are pruned. Consequently, model variance continues to decrease. However, model bias increases and overturns the benefit of the reduced variance. Lastly for Phase III, in the highly compressed region, model bias becomes notoriously large and reduction of the variance pales. As a result, training breaks down and generalization performance drops significantly.\nWe conduct systematic experiments and analyses to understand the phase transition. Our experiments on multiple natural language understanding (NLU) tasks in the GLUE (Wang et al., 2018) benchmark show that the super tickets can be used to improve single task fine-tuning by 0.9 points over BERTbase (Devlin et al., 2019) and 1.0 points over BERTlarge, in terms of task-average score. Moreover, our experiments show that the phase transition phenomenon is task and model dependent. It becomes more pronounced as a larger model is used to fit a task with less training data. In such a case, the set\nof super tickets forms a compressed network that exhibits a large performance gain.\nThe existence of super tickets suggests potential benefits to applications, such as Multi-task Learning (MTL). In MTL, different tasks require different capacities to achieve a balance between model bias and variance. However, existing methods do not specifically balance the bias and variance to accommodate each task. In fact, the finetuning performance on tasks with a small dataset is very sensitive to randomness. This suggests that model variance in these tasks are high due to overparameterization. To reduce such variance, we propose a tickets sharing strategy. Specifically, for each task, we select a set of super tickets during single task fine-tuning. Then, we adaptively share these super tickets across tasks.\nOur experiments show that tickets sharing improves MTL by 0.9 points over MT-DNNBASE (Liu et al., 2019) and 1.0 points over MT-DNNLARGE, in terms of task-average score. Tickets sharing further benefits downstream fine-tuning of the multi-task model, and achieves a gain of 1.0 task-average score. In addition, the multi-task model obtained by such a sharing strategy exhibits lower sensitivity to randomness in downstream fine-tuning tasks, suggesting a reduction in variance.\nWe summarize our contributions as follows: • Our result is the first to identify the phase transition phenomenon in pruning large neural language models. • Our result is the first to show that pruning can improve the generalization when the models are lightly compressed, which has been overlooked by previous works. Our analysis paves the way for understanding the connection between model compression and generalization. • Motivated by our observed phase transition,\nwe further propose a new pruning approach for multi-task fine-tuning of neural language models."
    }, {
      "heading" : "2 Background",
      "text" : "We briefly introduce the Transformer architecture and the Lottery Ticket Hypothesis."
    }, {
      "heading" : "2.1 Transformer Architecture",
      "text" : "The Transformer (Vaswani et al., 2017) encoder is composed of a stack of identical Transformer layers. Each layer consists of a multi-head attention module (MHA) followed by a feed-forward module (FFN), with a residual connection around each. The vanilla single-head attention operates as\nAtt(Q,K, V ) = Softmax ( QK>√\nd\n) V,\nwhere Q,K, V ∈ Rl×d are d-dimensional vector representations of l words in sequences of queries, keys and values. In MHA, the h-th attention head is parameterized by WQh ,W K h ,W V h ∈ Rd×dh as\nHh(q,x,W {Q,K,V } h ) = Att(qW Q h ,xW K h ,xW V h ),\nwhere q ∈ Rl×d and x ∈ Rl×d are the query and key/value vectors. In MHA, H independently parameterized attention heads are applied in parallel, and the outputs are aggregated by WOh ∈ Rdh×d:\nMHA(q,x)= H∑ h Hh(q,x,W {Q,K,V } h )W O h .\nEach FFN module contains a two-layer fully connected network. Given the input embedding z, we let FFN(z) denote the output of a FFN module."
    }, {
      "heading" : "2.2 Structured and Unstructured LTHs",
      "text" : "LTH (Frankle and Carbin, 2018) has been widely explored in various applications of deep learning (Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020). Most of existing results focus on finding unstructured winning tickets via iterative magnitude pruning and rewinding in randomly initialized networks (Frankle et al., 2019; Renda et al., 2020), where each ticket is a single parameter. Recent works further investigate learning dynamics of the tickets (Zhou et al., 2019; Frankle et al., 2020) and efficient methods to identify them (You et al., 2019; Savarese et al., 2020). Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for\nover-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a). For example, Chen et al. (2020b); Prasanna et al. (2020) have shown the existence of winning tickets when fine-tuning BERT on downstream tasks.\nThere is also a surge of research exploring whether certain structures, e.g., channels in convolutional layers and attention heads in Transformers, exhibit properties of the lottery tickets. Compared to unstructured tickets, training with structured tickets is memory efficient (Cao et al., 2019). Liu et al. (2018); Prasanna et al. (2020) suggest that there is no clear evidence that structured winning tickets exist in randomly initialized or pre-trained weights. Prasanna et al. (2020) observe that, in highly compressed BERT (e.g., the percent of weight remaining is around 50%), all tickets perform equally well. However, Prasanna et al. (2020) have not investigated the cases where the percent of weight remaining is over 50%."
    }, {
      "heading" : "3 Finding Super Tickets",
      "text" : "We identify winning tickets in BERT through structured pruning of attention heads and feed-forward layers. Specifically, in each Transformer layer, we associate mask variables ξh to each attention head and ν to the FFN (Prasanna et al., 2020):\nMHA(Q,x) = H∑ h ξhHh(Q,x,W {Q,K,V } h )W O h ,\nFFN(z) = νFFN(z).\nHere, we set ξh, ν ∈ {0, 1}, and a 0 value indicates that the corresponding structure is pruned.\nWe adopt importance score (Michel et al., 2019) as a gauge for pruning. In particular, the importance score is defined as the expected sensitivity of the model outputs with respect to the mask variables. Specifically, in each Transformer layer,\nIhMHA = E x∼Dx ∣∣∣∣∂L(x)∂ξh ∣∣∣∣ ,\nIFFN = E x∼Dx ∣∣∣∣∂L(x)∂ν ∣∣∣∣ ,\nwhere L is a loss function and Dx is the data distribution. In practice, we compute the average over the training set. We apply a layer-wise `2 normalization on the importance scores of the attention heads (Molchanov et al., 2016; Michel et al., 2019).\nThe importance score is closely tied to expressive power. A low importance score indicates that the corresponding structure only has a small contribution towards the output. Such a structure has low expressive power. On the contrary, a large importance score implies high expressive power.\nWe compute the importance scores for all the mask variables in a single backward pass at the end of fine-tuning. We perform one-shot pruning of the same percent of heads and feed-forward layers with the lowest importance scores. We conduct pruning multiple times to obtain subnetworks, or winning tickets, at different compression ratios.\nWe adopt the weight rewinding technique in Renda et al. (2020): We reset the parameters of the winning tickets to their values in the pre-trained weights, and subsequently fine-tune the subnetwork with the original learning rate schedule. The super tickets are selected as the winning tickets with the best rewinding validation performance."
    }, {
      "heading" : "4 Multi-task Learning with Tickets Sharing",
      "text" : "In multi-task learning, the shared model is highly over-parameterized to ensure a sufficient capacity for fitting individual tasks. Thus, the multi-task model inevitably exhibits task-dependent redundancy when being adapted to individual tasks. Such redundancy induces a large model variance.\nWe propose to mitigate the aforementioned model redundancy by identifying task-specific super tickets to accommodate each task’s need. Specifically, when viewing an individual task in isolation, the super tickets can tailor the multi-task model to strike an appealing balance between the model bias and variance (recall from Section 3 that super tickets retain sufficient expressive power, yet keep the model variance low). Therefore, we expect that deploying super tickets can effectively tame the model redundancy for individual tasks.\nGiven the super tickets identified by each task, we exploit the multi-task information to reinforce fine-tuning. Specifically, we propose a tickets sharing algorithm to update the parameters of the multitask model: For a certain network structure (e.g., an attention head), if it is identified as super tickets by multiple tasks, then its weights are jointly updated by these tasks; if it is only selected by one specific task, then its weights are updated by that task only; otherwise, its weights are completely pruned. See Figure 2 for an illustration.\nIn more detail, we denote the weight parameters in the multi-task model as θ. Suppose there are N tasks. For each task i ∈ {1, . . . , N}, we denote Ωi = {ξih,`} H,L h=1,`=1 ⋃ {νi`}L`=1 as the collection of the mask variables, where ` is the layer index and h is the head index. Then the parameters to be updated in task i are denoted as θi = M(θ,Ωi), where M(·,Ωi) masks the pruned parameters according to Ωi. We use stochastic gradient descenttype algorithms to update θi. Note that the taskshared and task-specific parameters are encoded by the mask variable Ωi. The detailed algorithm is given in Algorithm 1.\nTickets sharing has two major difference compared to Sparse Sharing (Sun et al., 2020): 1) Sun et al. (2020) share winning tickets, while our strategy focuses on super tickets, which can better generalize and strike a sensible balance between model bias and variance. 2) In tickets sharing, tickets are structured and chosen from pre-trained weight parameters. It does not require Multi-task Warmup, which is indispensable in Sun et al. (2020) to stabilize the sharing among unstructured tickets selected from randomly initialized weight parameters."
    }, {
      "heading" : "5 Single Task Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Data",
      "text" : "General Language Understanding Evaluation (GLUE, Wang et al. (2018)) is a standard benchmark for evaluating model generalization performance. It contains nine NLU tasks, including question answering, sentiment analysis, text similarity\nAlgorithm 1 Tickets Sharing Input: Pre-trained base model parameters θ. Num-\nber of tasks N . Mask variables {Ωi}Ni=1. Loss functions {Li}Ni=1. Dataset D = ⋃N i=1Di.\nNumber of epochs Tmax. 1: for i in N do 2: Initialize the super tickets for task i: θi = M(θ,Ωi). 3: end for 4: for epoch in 1, . . . , Tmax do 5: Shuffle dataset D. 6: for a minibatch bi of task i in D do 7: Compute Loss Li(θi). 8: Compute gradient∇θLi(θi). 9: Update θi using SGD-type algorithm.\n10: end for 11: end for\nand textual entailment. Details about the benchmark are deferred to Appendix A.1.1."
    }, {
      "heading" : "5.2 Models & Training",
      "text" : "We fine-tune a pre-trained BERT model with taskspecific data to obtain a single task model. We append a task-specific fully-connected layer to BERT as in Devlin et al. (2019). • ST-DNNBASE/LARGE is initialized with BERTbase/large followed by a task-specific layer. • SuperTBASE/LARGE is initialized with the chosen set of super tickets in BERT-base/large followed by a task-specific layer. Specifically, we prune BERT-base/large in unit of 10% heads and 10% feed-forward layers (FFN) at 8 different sparsity levels (10% heads and 10% FFN, 20% heads and 20% FFN, etc). Among them, the one with the best rewinding validation result is chosen as the set of super tickets. We randomly sample 10% GLUE development set for tickets selection.\nOur implementation is based on the MT-DNN code base3. We use Adamax (Kingma and Ba, 2014) as our optimizer. We tune the learning rate in {5×10−5, 1×10−4, 2×10−4} and batch size in {8, 16, 32}. We train for a maximum of 6 epochs with early-stopping. All training details are summarized in Appendix A.1.2."
    }, {
      "heading" : "5.3 Generalization of the Super Tickets",
      "text" : "We conduct 5 trails of pruning and rewinding experiments using different random seeds. Table 1\n3https://github.com/namisan/mt-dnn\nand 2 show the averaged evaluation results on the GLUE development and test sets, respectively. We remark that the gain of SuperTBASE/LARGE over STDNNBASE/LARGE is statistically significant. All the results4 have passed a paired student t-test with p-values less than 0.05. More validation statistics are summarized in Appendix A.1.3.\nOur results can be summarized as follows. 1) In all the tasks, SuperT consistently achieves better generalization than ST-DNN. The taskaveraged improvement is around 0.9 over STDNNBASE and 1.0 over ST-DNNLARGE.\n2) Performance gain of the super tickets is more significant in small tasks. For example, in Table 1, we obtain 3.3 points gain on RTE (2.5k data), but only 0.4/0.3 on QQP (364k data) in the SuperTBASE experiments. Furthermore, from Figure 3, note that the super tickets are more heavily compressed in small tasks, e.g., for SuperTBASE, 83% weights remaining for RTE, but 93% for QQP. These observations suggest that for small tasks, model variance is large, and removing nonexpressive tickets reduces variance and improves generalization. For large tasks, model variance is low, and all tickets are expressive to some extent.\n3) Performance of the super tickets is related to model size. Switching from SuperTBASE to SuperTLARGE, the percent of weights remaining shrinks uniformly across tasks, yet the generalization gains persist (Figure 3). This suggests that in large models, more non-expressive tickets can be pruned without performance degradation.\n4Except for STS-B (SuperTBASE, Table 1), where the pvalue is 0.37."
    }, {
      "heading" : "5.4 Phase Transition",
      "text" : "Phase transitions are shown in Figure 4. We plot the evaluation results of the winning, the random, and the losing tickets under 8 sparsity levels using BERT-base and BERT-large. The winning tickets contain structures with the highest importance scores. The losing tickets are selected reversely, i.e., the structures with the lowest importance scores are selected, and high-importance structures are pruned. The random tickets are sampled uniformly across the network. We plot the averaged scores over 5 trails using different random seeds5. Phase transitions of all the GLUE tasks are in Appendix A.5.\nWe summarize our observations: 1) The winning tickets are indeed the “winners”. In Phase I and early Phase II, the winning tickets perform better than the full model and the random tickets. This demonstrates the existence of struc-\n5Except for MNLI, where we plot 3 trails as the there are less variance among trails.\ntured winning tickets in lightly compressed BERT models, which Prasanna et al. (2020) overlook.\n2) Phase transition is pronounced over different tasks and models. Accuracy of the winning tickets increases up till a certain compression ratio (Phase I); Passing the threshold, the accuracy decreases (Phase II), until its value intersects with that of the random tickets (Phase III). Note that Phase III agrees with the observations in Prasanna et al. (2020). Accuracy of the random tickets decreases in each phase. This suggests that model bias increases steadily, since tickets with both low and high expressive power are discarded. Accuracy of the losing tickets drops significantly even in Phase I, suggesting that model bias increases drastically as highly expressive tickets are pruned.\n3) Phase transition is more pronounced in large models and small tasks. For example, in Figure 4, the phase transition is more noticeable in BERTlarge than in BERT-base, and is more pronounced in RTE (2.5k) and MRPC (3.7k) than in SST (67k) and MNLI (393k). The phenomenon becomes more significant for the same task when we only use a part of the data, e.g., Figure 5 vs. Figure 4 (bottom left)."
    }, {
      "heading" : "6 Multi-task Learning Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Model & Training",
      "text" : "We adopt the MT-DNN architecture proposed in Liu et al. (2020). The MT-DNN model consists of a set of task-shared layers followed by a set of task-specific layers. The task-shared layers take in the input sequence embedding, and generate shared semantic representations by optimizing multi-task objectives. Our implementation is based on the MT-DNN code base. We follow the same training settings in Liu et al. (2020) for multi-task learn-\ning, and in Section 5.2 for downstream fine-tuning. More details are summarized in Appendix A.2. • MT-DNNBASE/LARGE. An MT-DNN model refined through multi-task learning, with task-shared layers initialized by pre-trained BERT-base/large. • MT-DNNBASE/LARGE + ST Fine-tuning. A single task model obtained by further fine-tuning MTDNN on an individual downstream task. • Ticket-ShareBASE/LARGE. An MT-DNN model refined through the ticket sharing strategy, with task-shared layers initialized by the union of the super tickets in pre-trained BERT-base/large. • Ticket-ShareBASE/LARGE + ST Fine-tuning. A fine-tuned single-task Ticket-Share model."
    }, {
      "heading" : "6.2 Experimental Results",
      "text" : "Table 3 summarizes experimental results. The finetuning results are averaged over 5 trails using different random seeds. We have several observations:\n1) Ticket-ShareBASE and Ticket-ShareLARGE achieve 0.9 and 1.0 gain in task-average score over MT-DNNBASE and MT-DNNLARGE, respectively. In some small tasks (RTE, MRPC), Ticket-Share achieves better or on par results compared to MTDNN+Fine-tuning. This suggests that by balancing the bias and variance for different tasks, the multitask model’s variance is reduced. In large tasks (QQP, QNLI and MNLI), Ticket-Share behaves equally well with the full model. This is because task-shared information is kept during pruning and still benefits multi-task learning.\n2) Ticket-ShareBASE+Fine-tuning and TicketShareLARGE+Fine-tuning achieve 1.0 and 0.7 gains in task-average score over MT-DNNBASE+Finetuning and MT-DNNLARGE+Fine-tuning, respectively. This suggests that reducing the variance in the multi-task model benefits fine-tuning downstream tasks.\nModel 0.1% 1% 10% 100%\nSNLI (Dev Acc%) # Training Data 549 5493 54k 549k"
    }, {
      "heading" : "7 Domain Adaptation",
      "text" : "To demonstrate that super tickets can quickly generalize to new tasks/domains, we conduct few-shot domain adaptation on out-of-domain NLI datasets."
    }, {
      "heading" : "7.1 Data & Training",
      "text" : "We briefly introduce the target domain datasets. The data and training details are summarized in Appendix A.3.1 and A.3.2, respectively. SNLI. The Stanford Natural Language Inference\ndataset (Bowman et al., 2015) is one of the most widely used entailment dataset for NLI. It contains 570k sentence pairs, where the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated.\nSciTail is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018). The hypotheses are created from science questions, rendering SciTail challenging."
    }, {
      "heading" : "7.2 Experimental Results",
      "text" : "We consider domain adaptation on both single task and multi-task super tickets. Specifically, we adapt SuperTBASE and ST-DNNBASE from MNLI to SNLI/SciTail, and adapt the shared embeddings generated by Ticket-ShareBASE and by MT-DNNBASE to SNLI/SciTail. We adapt these models to 0.1%, 1%, 10% and 100% SNLI/SciTail training sets6, and evaluate the transferred models on SNLI/SciTail development sets. Table 4 shows the domain adaptation evaluation results. As we can see, SuperT and Ticket-Share can better adapt to SNLI/SciTail than ST-DNN and MT-DNN, especially under the few shot setting.\n6We use the subsets released in MT-DNN code base."
    }, {
      "heading" : "8 Analysis",
      "text" : "Sensitivity to Random Seed. To better demonstrate that training with super tickets effectively reduces model variance, we evaluate models’ sensitivity to changes in random seeds during single task fine-tuning and multi-task downstream finetuning. In particular, we investigate fitting small tasks with highly over-parametrized models (variance is often large in these models, see Section 5 and 6). As shown in Table 5, SuperTLARGE and Ticket-ShareLARGE induce much smaller standard deviation in validation results. Experimental details and further analyses are deferred to Appendix A.4.\nTickets Importance Across Tasks. We analyze the importance score of each ticket computed in different GLUE tasks. For each ticket, we compute the importance score averaged over tasks as the Ticket Importance, and the proportion of the taskspecific importance score out of the sum of all tasks’ scores as the Task Share, as illustrated in Figure 6.\nWe observe that many tickets exhibit almost equal Task Shares for over 5 out of 8 tasks (Figure 6(a)(b)). While these tickets contribute to the knowledge sharing in the majority of tasks, they are considered non-expressive for tasks such as SST-2 (see Figure 6(a)(c)(d)). This explains why SST-2 benefits little from tickets sharing. Furthermore, a small number of tickets are dominated by a single task, e.g., CoLA (Figure 6(c)), or dominated jointly by two tasks, e.g., CoLA and STS-B (Figure 6(d)). This suggests that some tickets only learn task-specific knowledge, and the two tasks may share certain task-specific knowledge."
    }, {
      "heading" : "9 Discussion",
      "text" : "Structured Lottery Tickets. LTH hypothesizes that a subset of unstructured parameters can be trained to match the full model’s performance. Instead, we question whether a subset of structured weight matrices, e.g., FFN layers and attention heads, can also be trained to match the full model’s performance. This question is more practically\nimportant than the unstructured one: training and inference on structured matrices are better optimized for hardware acceleration. Our results give a positive answer to this question, while previous works show that the structured tickets do not exist in highly compressed models (Prasanna et al., 2020)."
    }, {
      "heading" : "Searching Better Generalized Super Tickets.",
      "text" : "We select winning tickets according to the sensitivity of the model outputs with respect to the mask variables of each structure (Michel et al., 2019; Prasanna et al., 2020), as this measure is closely tied to the structure’s expressive power (Section 3). In addition, we conduct an one-shot pruning for computational simplicity. We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020). Searching Super Tickets Efficiently. Determining the compression ratio of the super tickets requires rewinding models at multiple sparsity levels. To leverage super tickets in practice, a potential direction of research is to find heuristics to determine this ratio prior or early-on in training. We leave this for future works."
    }, {
      "heading" : "10 Conclusion",
      "text" : "We study the behaviors of the structured lottery tickets in pre-trained BERT. We observe that the generalization performance of the winning tickets exhibits a phase transition phenomenon, suggesting pruning can improve generalization when models are lightly compressed. Based on the observation, we further propose a tickets sharing strategy to improve multi-task fine-tuning. Our analysis paves the way for understanding the connection between model compression and generalization.\nBroader Impact\nThis paper studies the behavior of the structured lottery tickets in pre-trained language models. Our investigation neither introduces any social/ethical bias to the model nor amplifies any bias in the data. We do not foresee any direct social consequences or ethical issues. Furthermore, our proposed method improves performance through model compression, rendering it energy efficient."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Single Task Experiments",
      "text" : "A.1.1 Data GLUE. GLUE is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Details of the GLUE benchmark, including tasks, statistics, and evaluation metrics, are summarized in Table 9."
    }, {
      "heading" : "A.1.2 Training",
      "text" : "We use Adamax as the optimizer. A linear learning rate decay schedule with warm-up over 0.1 is used. We apply a gradient norm clipping of 1. We set the dropout rate of all task specific layers as 0.1, except 0.3 for MNLI and 0.05 for CoLA. All the texts were tokenized using wordpieces, and were chopped to spans no longer than 512 tokens. All experiments are conducted on Nvidia V100 GPUs."
    }, {
      "heading" : "A.1.3 Evaluation Results Statistics",
      "text" : "We conduct 5 sets of experiments on different random seeds. Each set of experiment consists of fine-tuning, pruning, and rewinding at 8 sparsity levels. For results on GLUE dev set (Table 1), we report the average score of super tickets rewinding results over 5 sets of experiments. The standard deviation of the results is shown in Table 6. The statistics of the percent of weight remaining in the selected super tickets are shown in Table 7.\nFor results on GLUE test set (Table 2), as the evaluation server sets an limit on submission times, we only evaluate the test prediction under a single random seed that gives the best task-average validation results."
    }, {
      "heading" : "A.2 Multi-task Learning Experiments",
      "text" : ""
    }, {
      "heading" : "A.2.1 Multi-task Model Training",
      "text" : "We adopt the MT-DNN code base and adopt the exact optimization settings in Liu et al. (2020). We use Adamax as our optimizer with a learning rate of 5× 10−5 and a batch size of 32. We train for a maximum number of epochs of 5 with early stopping. A linear learning rate decay schedule with warm-up over 0.1 was used. The dropout rate of all\nthe task specific layers is set to be 0.1, except 0.3 for MNLI and 0.05 for CoLa. We clipped the gradient norm within 1. All the texts were tokenized using wordpieces, and were chopped to spans no longer than 512 tokens.\nWorth mentioning, the task-specific super tickets used in Ticket Share are all selected during the case where a matched learning rate (i.e., 5 × 10−5) is used in single task fine-tuning. We empirically find that, rewinding the super tickets selected under a matched optimization settings usually outperforms those selected under a mismatched settings (i.e. using two different learning rates in single-task finetuning and rewinding/multi-task learning). This agrees with previous observation in literature of Lottery Ticket Hypothesis, which shows that unstructured winning tickets are not only related to its weight initialization, but also model optimization path."
    }, {
      "heading" : "A.2.2 Multi-task Model Downstream Fine-tuning",
      "text" : "We follow the exact optimization setting as in Section 5.2 and in Section A.1.2, except we choose learning rate in {1×10−5, 2×10−5, 5×10−5, 1× 10−4, 2×10−4}, and choose the dropout rate of all task specific layers in {0.05, 0.1, 0.2, 0.3}."
    }, {
      "heading" : "A.3 Domain Adaptation Experiments",
      "text" : "A.3.1 Data SNLI. is one of the most widely used entailment dataset for NLI. SciTail involves assessing whether a given premise entails a given hypothesis. In contrast to other entailment datasets, the hypotheses in SciTail is created from science questions. These sentences are linguistically challenging. The corresponding answer candidates and premises come from relevant web sentences. The lexical similarity of premise and hypothesis is often high, making SciTail particularly challenging.\nDetails of the SNLI and SciTail, including tasks, statistics, and evaluation metrics, are summarized in Table 9."
    }, {
      "heading" : "A.3.2 Training",
      "text" : "For single task model domain adaptation from MNLI to SNLI/SciTail, we follow the exact optimization setting as in Section 5.2 and in Section A.1.2, except we choose the learning rate in {5× 10−5, 1× 10−4, 5× 10−4}."
    }, {
      "heading" : "A.4 Sensitivity Analysis",
      "text" : ""
    }, {
      "heading" : "A.4.1 Randomness Analysis",
      "text" : "For single task experiments in Table 5, we vary the random seeds only and keep all other hyperparameters fixed. We present the standard deviation of the validation results over 5 trails rewinding experiments. For multi-task downstream fine-tuning experiments, we present the standard deviation of the validation results over 5 trails, each result averaged over learning rates in {5×10−5, 1×10−4, 2× 10−4}. This is because the downstream fine-tuning performance is more sensitive to hyper-parameters."
    }, {
      "heading" : "A.4.2 Hyper-parameter Analysis",
      "text" : "We further analyze the sensitivity of Ticket ShareLARGE model to changes in hyper-parameters in downstream fine-tuning in some GLUE tasks. We vary the learning rate in {5 × 10−5, 1 × 10−4, 2×10−4} and keep all other hyper-parameter fixed. Table 8 shows the standard deviation of the validation results over different learning rates, each result averaged over 5 different random seeds. As can be seen, Task ShareLARGE exhibits stronger robustness to changes in learning rate in downstream fine-tuning."
    }, {
      "heading" : "A.5 Phase Transition on GLUE Tasks",
      "text" : "Figure 7 shows the phase transition plots on winning tickets on GLUE tasks absent from Figure 4. All experimental settings conform to Figure 4."
    } ],
    "references" : [ {
      "title" : "The second PASCAL recognising textual entailment challenge",
      "author" : [ "Roy Bar-Haim", "Ido Dagan", "Bill Dolan", "Lisa Ferro", "Danilo Giampiccolo." ],
      "venue" : "Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.",
      "citeRegEx" : "Bar.Haim et al\\.,? 2006",
      "shortCiteRegEx" : "Bar.Haim et al\\.",
      "year" : 2006
    }, {
      "title" : "Losing heads in the lottery: Pruning transformer attention in neural machine translation",
      "author" : [ "Maximiliana Behnke", "Kenneth Heafield." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2664–",
      "citeRegEx" : "Behnke and Heafield.,? 2020",
      "shortCiteRegEx" : "Behnke and Heafield.",
      "year" : 2020
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo", "Bernardo Magnini." ],
      "venue" : "In Proc Text Analysis Conference (TAC’09).",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.05326.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture",
      "author" : [ "Christopher Brix", "Parnia Bahar", "Hermann Ney." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3909–",
      "citeRegEx" : "Brix et al\\.,? 2020",
      "shortCiteRegEx" : "Brix et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient and effective sparse lstm on fpga with bank-balanced sparsity",
      "author" : [ "Shijie Cao", "Chen Zhang", "Zhuliang Yao", "Wencong Xiao", "Lanshun Nie", "Dechen Zhan", "Yunxin Liu", "Ming Wu", "Lintao Zhang." ],
      "venue" : "Proceedings of the 2019 ACM/SIGDA Interna-",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evalu-",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "The lottery tickets hypothesis for supervised and self-supervised pretraining in computer vision models",
      "author" : [ "Tianlong Chen", "Jonathan Frankle", "Shiyu Chang", "Sijia Liu", "Yang Zhang", "Michael Carbin", "Zhangyang Wang." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "The lottery ticket hypothesis for pre-trained bert networks",
      "author" : [ "Tianlong Chen", "Jonathan Frankle", "Shiyu Chang", "Sijia Liu", "Yang Zhang", "Zhangyang Wang", "Michael Carbin." ],
      "venue" : "arXiv preprint arXiv:2007.12223.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Evaluating lottery tickets under distributional shifts",
      "author" : [ "Shrey Desai", "Hongyuan Zhan", "Ahmed Aly." ],
      "venue" : "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 153–162.",
      "citeRegEx" : "Desai et al\\.,? 2019",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Reducing transformer depth on demand with structured dropout",
      "author" : [ "Angela Fan", "Edouard Grave", "Armand Joulin." ],
      "venue" : "arXiv preprint arXiv:1909.11556.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "author" : [ "Jonathan Frankle", "Michael Carbin." ],
      "venue" : "arXiv preprint arXiv:1803.03635.",
      "citeRegEx" : "Frankle and Carbin.,? 2018",
      "shortCiteRegEx" : "Frankle and Carbin.",
      "year" : 2018
    }, {
      "title" : "Stabilizing the lottery ticket hypothesis",
      "author" : [ "Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M Roy", "Michael Carbin." ],
      "venue" : "arXiv preprint arXiv:1903.01611.",
      "citeRegEx" : "Frankle et al\\.,? 2019",
      "shortCiteRegEx" : "Frankle et al\\.",
      "year" : 2019
    }, {
      "title" : "The early phase of neural network training",
      "author" : [ "Jonathan Frankle", "David J Schwab", "Ari S Morcos." ],
      "venue" : "arXiv preprint arXiv:2002.10365.",
      "citeRegEx" : "Frankle et al\\.,? 2020",
      "shortCiteRegEx" : "Frankle et al\\.",
      "year" : 2020
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics New York",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2001
    }, {
      "title" : "The third PASCAL recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague. Association",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "The lottery ticket hypothesis for object recognition",
      "author" : [ "Sharath Girish", "Shishira R Maiya", "Kamal Gupta", "Hao Chen", "Larry Davis", "Abhinav Shrivastava." ],
      "venue" : "arXiv preprint arXiv:2012.04643.",
      "citeRegEx" : "Girish et al\\.,? 2020",
      "shortCiteRegEx" : "Girish et al\\.",
      "year" : 2020
    }, {
      "title" : "Scitail: A textual entailment dataset from science question answering",
      "author" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Khot et al\\.,? 2018",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487–4496.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The microsoft toolkit of multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Yu Wang", "Jianshu Ji", "Hao Cheng", "Xueyun Zhu", "Emmanuel Awa", "Pengcheng He", "Weizhu Chen", "Hoifung Poon", "Guihong Cao" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking the value of network pruning",
      "author" : [ "Zhuang Liu", "Mingjie Sun", "Tinghui Zhou", "Gao Huang", "Trevor Darrell." ],
      "venue" : "arXiv preprint arXiv:1810.05270.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Are sixteen heads really better than one",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Pruning convolutional neural networks for resource efficient inference",
      "author" : [ "Pavlo Molchanov", "Stephen Tyree", "Tero Karras", "Timo Aila", "Jan Kautz." ],
      "venue" : "arXiv preprint arXiv:1611.06440.",
      "citeRegEx" : "Molchanov et al\\.,? 2016",
      "shortCiteRegEx" : "Molchanov et al\\.",
      "year" : 2016
    }, {
      "title" : "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers",
      "author" : [ "Ari S Morcos", "Haonan Yu", "Michela Paganini", "Yuandong Tian." ],
      "venue" : "arXiv preprint arXiv:1906.02773.",
      "citeRegEx" : "Morcos et al\\.,? 2019",
      "shortCiteRegEx" : "Morcos et al\\.",
      "year" : 2019
    }, {
      "title" : "Dissecting lottery ticket transformers: Structural and behavioral study of sparse neural machine translation",
      "author" : [ "Rajiv Movva", "Jason Y Zhao." ],
      "venue" : "arXiv preprint arXiv:2009.13270.",
      "citeRegEx" : "Movva and Zhao.,? 2020",
      "shortCiteRegEx" : "Movva and Zhao.",
      "year" : 2020
    }, {
      "title" : "When bert plays the lottery, all tickets are winning",
      "author" : [ "Sai Prasanna", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:2005.00561.",
      "citeRegEx" : "Prasanna et al\\.,? 2020",
      "shortCiteRegEx" : "Prasanna et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Comparing rewinding and fine-tuning in neural network pruning",
      "author" : [ "Alex Renda", "Jonathan Frankle", "Michael Carbin." ],
      "venue" : "arXiv preprint arXiv:2003.02389.",
      "citeRegEx" : "Renda et al\\.,? 2020",
      "shortCiteRegEx" : "Renda et al\\.",
      "year" : 2020
    }, {
      "title" : "Poor man’s bert: Smaller and faster transformer models",
      "author" : [ "Hassan Sajjad", "Fahim Dalvi", "Nadir Durrani", "Preslav Nakov." ],
      "venue" : "arXiv preprint arXiv:2004.03844.",
      "citeRegEx" : "Sajjad et al\\.,? 2020",
      "shortCiteRegEx" : "Sajjad et al\\.",
      "year" : 2020
    }, {
      "title" : "Winning the lottery with continuous sparsification",
      "author" : [ "Pedro Savarese", "Hugo Silva", "Michael Maire." ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Savarese et al\\.,? 2020",
      "shortCiteRegEx" : "Savarese et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning sparse sharing architectures for multiple tasks",
      "author" : [ "Tianxiang Sun", "Yunfan Shao", "Xiaonan Li", "Pengfei Liu", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8936–8943.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:1905.09418.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured pruning of large language models",
      "author" : [ "Ziheng Wang", "Jeremy Wohlwend", "Tao Lei." ],
      "venue" : "arXiv preprint arXiv:1910.04732.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Drawing early-bird tickets: Towards more efficient training of deep networks",
      "author" : [ "Haoran You", "Chaojian Li", "Pengfei Xu", "Yonggan Fu", "Yue Wang", "Xiaohan Chen", "Richard G Baraniuk", "Zhangyang Wang", "Yingyan Lin." ],
      "venue" : "arXiv preprint arXiv:1909.11957.",
      "citeRegEx" : "You et al\\.,? 2019",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2019
    }, {
      "title" : "Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp",
      "author" : [ "Haonan Yu", "Sergey Edunov", "Yuandong Tian", "Ari S Morcos." ],
      "venue" : "arXiv preprint arXiv:1906.02768.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Deconstructing lottery tickets: Zeros, signs, and the supermask",
      "author" : [ "Hattie Zhou", "Janice Lan", "Rosanne Liu", "Jason Yosinski." ],
      "venue" : "arXiv preprint arXiv:1905.01067.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Scheduled drophead: A regularization method for transformer models",
      "author" : [ "Wangchunshu Zhou", "Tao Ge", "Ke Xu", "Furu Wei", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2004.13342.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC",
      "author" : [ "jpurkar et al", "linguistic acceptability (CoLA", "Warstadt" ],
      "venue" : "Dolan and Brockett",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 46,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 44,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 20,
      "context" : "LTH has been widely explored in across various fields of deep learning (Frankle et al., 2019; Zhou et al., 2019; You et al., 2019; Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 192
    }, {
      "referenceID" : 28,
      "context" : "Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 127,
      "endOffset" : 205
    }, {
      "referenceID" : 45,
      "context" : "Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 127,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 127,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 127,
      "endOffset" : 205
    }, {
      "referenceID" : 40,
      "context" : "Our experiments on multiple natural language understanding (NLU) tasks in the GLUE (Wang et al., 2018) benchmark show that the super tickets can be used to improve single task fine-tuning by 0.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 38,
      "context" : "The Transformer (Vaswani et al., 2017) encoder is composed of a stack of identical Transformer layers.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "LTH (Frankle and Carbin, 2018) has been widely explored in various applications of deep learning (Brix et al.",
      "startOffset" : 4,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "LTH (Frankle and Carbin, 2018) has been widely explored in various applications of deep learning (Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 159
    }, {
      "referenceID" : 29,
      "context" : "LTH (Frankle and Carbin, 2018) has been widely explored in various applications of deep learning (Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 159
    }, {
      "referenceID" : 20,
      "context" : "LTH (Frankle and Carbin, 2018) has been widely explored in various applications of deep learning (Brix et al., 2020; Movva and Zhao, 2020; Girish et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "Most of existing results focus on finding unstructured winning tickets via iterative magnitude pruning and rewinding in randomly initialized networks (Frankle et al., 2019; Renda et al., 2020), where each ticket is a single parameter.",
      "startOffset" : 150,
      "endOffset" : 192
    }, {
      "referenceID" : 33,
      "context" : "Most of existing results focus on finding unstructured winning tickets via iterative magnitude pruning and rewinding in randomly initialized networks (Frankle et al., 2019; Renda et al., 2020), where each ticket is a single parameter.",
      "startOffset" : 150,
      "endOffset" : 192
    }, {
      "referenceID" : 46,
      "context" : "Recent works further investigate learning dynamics of the tickets (Zhou et al., 2019; Frankle et al., 2020) and efficient methods to identify them (You et al.",
      "startOffset" : 66,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Recent works further investigate learning dynamics of the tickets (Zhou et al., 2019; Frankle et al., 2020) and efficient methods to identify them (You et al.",
      "startOffset" : 66,
      "endOffset" : 107
    }, {
      "referenceID" : 44,
      "context" : ", 2020) and efficient methods to identify them (You et al., 2019; Savarese et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 88
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and efficient methods to identify them (You et al., 2019; Savarese et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 196,
      "endOffset" : 274
    }, {
      "referenceID" : 45,
      "context" : "Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 196,
      "endOffset" : 274
    }, {
      "referenceID" : 11,
      "context" : "Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 196,
      "endOffset" : 274
    }, {
      "referenceID" : 8,
      "context" : "Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",
      "startOffset" : 196,
      "endOffset" : 274
    }, {
      "referenceID" : 6,
      "context" : "Compared to unstructured tickets, training with structured tickets is memory efficient (Cao et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : "Specifically, in each Transformer layer, we associate mask variables ξh to each attention head and ν to the FFN (Prasanna et al., 2020):",
      "startOffset" : 112,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "We adopt importance score (Michel et al., 2019) as a gauge for pruning.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "We apply a layer-wise `2 normalization on the importance scores of the attention heads (Molchanov et al., 2016; Michel et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "We apply a layer-wise `2 normalization on the importance scores of the attention heads (Molchanov et al., 2016; Michel et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 132
    }, {
      "referenceID" : 37,
      "context" : "Tickets sharing has two major difference compared to Sparse Sharing (Sun et al., 2020): 1) Sun et al.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "The Stanford Natural Language Inference dataset (Bowman et al., 2015) is one of the most widely used entailment dataset for NLI.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "SciTail is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 30,
      "context" : "Our results give a positive answer to this question, while previous works show that the structured tickets do not exist in highly compressed models (Prasanna et al., 2020).",
      "startOffset" : 148,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "We select winning tickets according to the sensitivity of the model outputs with respect to the mask variables of each structure (Michel et al., 2019; Prasanna et al., 2020), as this measure is closely tied to the structure’s expressive power (Section 3).",
      "startOffset" : 129,
      "endOffset" : 173
    }, {
      "referenceID" : 30,
      "context" : "We select winning tickets according to the sensitivity of the model outputs with respect to the mask variables of each structure (Michel et al., 2019; Prasanna et al., 2020), as this measure is closely tied to the structure’s expressive power (Section 3).",
      "startOffset" : 129,
      "endOffset" : 173
    }, {
      "referenceID" : 39,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    }, {
      "referenceID" : 1,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    }, {
      "referenceID" : 41,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    }, {
      "referenceID" : 14,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    }, {
      "referenceID" : 47,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    }, {
      "referenceID" : 34,
      "context" : "We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 260
    } ],
    "year" : 2021,
    "abstractText" : "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of “lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as “winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as “super tickets”. We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning1.",
    "creator" : "LaTeX with hyperref"
  }
}