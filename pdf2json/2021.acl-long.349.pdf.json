{
  "name" : "2021.acl-long.349.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models",
    "authors" : [ "Sumanta Bhattacharyya", "Amirmohammad Rooshenas", "Subhajit Naskar", "Simeng Sun", "Mohit Iyyer" ],
    "emails" : [ "sbhatta9@uncc.edu", "rooshenas@uncc.edu", "snaskar@cs.umass.edu", "simeng@cs.umass.edu", "miyyer@cs.umass.edu", "mccallum@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4528–4537\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4528"
    }, {
      "heading" : "1 Introduction",
      "text" : "Autoregressive models are widely used for neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). The autoregressive factorization provides a tractable likelihood computation as well as efficient sampling. The former results in the effective maximum likelihood estimation (MLE) for training the\n∗Amirmohammad Rooshenas is the corresponding author.\nparameters of NMT models. However, optimizing likelihood does not guarantee an improvement in task-based measures such as the BLEU score, which has motivated directly optimizing task measures with reinforcement learning (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Bahdanau et al., 2017; Wu et al., 2018). However, for NMT, these training algorithms are often used in conjunction with MLE training (Wu et al., 2018) or as fine-tuning (Choshen et al., 2020).\nInterestingly, we observe that samples drawn from an NMT model trained using MLE may have higher quality (measured with BLEU) than the outputs of beam search. In particular, we draw 100 target samples for each source sentence from an NMT model trained using MLE on the IWSLT’14 German-English task, and observe that an oracle ranker – i.e. argmaxy∼PNMT(y|x) BLEU(.,y\n∗), where (x,y∗) is the pair of source and gold target sentence – achieves the high score of 67.54, while the beam decoding achieves 33.87. We also look at the distribution of the Spearman rank correlation coefficient of the drawn samples with respect to the log probability score of the baseline NMT (BaseNMT). Figure 1 shows that there is no strong correlation between the BLEU score ranking of samples and the log probability score ranking for the majority of source sentences; thus, maximum a priori (MAP) decoding is incapable of finding the desired output. In parallel to our study, Eikema and Aziz (2020) also report that the mismatch regarding MLE training of autoregressive models is attributable to the distribution of the probability mass rather than the parameter estimation, resulting in a poor MAP decoding.\nInstead of looking for an alternate algorithm for parameter estimation, these results motivate us to explore training a parametric approximation of the metric, here BLEU score: ωθ(y,x) ≈ BLEU(y,y∗). Therefore the decoding becomes:\nargmaxy∼PNMT(.|x) ωθ(y,x).\nWe use energy-based models (EBMs) to parameterize ωθ(y,x). EBMs (LeCun et al., 2006) are general parametric models that assign a scalar energy value to each configuration of input variables, thus defining an unnormalized probability distribution. Although computing the partition function is intractable for general EBMs, we only require the relative energy of the sampled sentences from the BaseNMT model, thus canceling out the normalization constant. In this paper we use two different energy-based models: marginal energy model (Marginal-EBM) defined only over target sentences and joint energy model (Joint-EBM) defined over both source and target sentences.\nFigure 1 also shows the correlation coefficient of the energy ranking and BLEU score using both Marginal-EBM and Joint-EBM. The shift in the coefficient distribution suggests that decoding based on energy scores results in better BLEU scores compared to decoding based on the log probability scores of the BaseNMT model. Also we observe that Joint-EBM works better than using MarginalEBM as Joint-EBM better captures the correlation of source and target sentences, while MarginalEBM is not directly conditioned on the source sentence.\nIn this paper, we describe how to train EBMs1 to achieve the desired ranking. Our energy ranker consistently improves the performance of Transformerbased NMT on German-English, RomanianEnglish and Italian-English tasks from IWSLT’14, the French-English task from IWSLT’17, GermanEnglish task from WMT’14, and English-German task from WMT’16, as well as the low-resource Sinhala-English and Nepali-English tasks described in the FLoRes dataset (Guzmán et al., 2019).\n1The code is available at https://github.com/ rooshenas/ebr_mt"
    }, {
      "heading" : "2 Energy-Based Reranking",
      "text" : "Using EBM Eθ to reweight the samples from an NMT defines a new probability distribution over the output sentences (see Grover et al. (2019)): Pθ(y|x) ∝ PNMT(y|x) exp(−Eθ(y,x)T ), where T is temperature. The ideal re-ranker requires an EBM with the energy function Eθ(y,x) such that Pθ(y|x) and BLEU(y,yi) have similar modes for all (xi,yi) ∈ D, where D is an empirical data distribution. To train θ we use rank-based training (Rohanimanesh et al., 2011; Rooshenas et al., 2018, 2019). Rank-based training enforces that the samples from Pθ(.) have similar ranking with respect to both the energy score and task measure (see Figure 2).\nTo sample from Pθ(y|x), we sample k sentences from PNMT(y|x) using multinomial sampling from locally normalized distributions over the output and reweight the samples based on the energy network exp(−Eθ(y,x)T ). Then we resample two sentences, y1 and y2, from the renormalized set, which defines a conditional distribution: P i(y|x) = exp(−Eθ(y,x)/T )∑\nk exp(−Eθ(yk,x)/T ) (a similar sam-\npling approach has been used in Deng et al. (2020)). Now we train the energy model such that the ranking of y1 and y2 with respect to the energy model\nis consistent with their ranking with respect to the task metric, BLEU score.\nIn general, we assume yh is the sentence with the higher BLEU score and yl is the sentence with with the lower BLEU score. Therefore, the training objective of Eθ(y,x) becomes:\nM = α(BLEU(yh,yi)− BLEU(yl,yi)) ξ(yi,xi) =M + Eθ(yh,xi)− Eθ(yl,xi)\nmin θ ∑ (yi,xi)∈D max(ξ(yi,xi), 0). (1)\nWhere ξ(yi,xi) is the margin violation and α is the margin weight. Algorithm 1 outlines the whole training procedure.\nIf we define the energy only over sentences of the target language, Eθ(y), we can share the energymodel among multiple language pairs with the same target language. In this case we have to, first, sample the language l from our language set and then sample a sentence pair from the selected language training set Dl. The probability of selecting a language is proportional to the number of sentences in its training set.\nAlgorithm 1 Rank-Based Training of EBM PNMT(y|x)← Pretrained NMT Eθ(y,x)← Energy based models for target sentences repeat L ← 0. for batch size do\nSample (xi,yi) from D Yi ← collect k samples from PNMT(.|xi) P i(y)← exp(−Eθ(y,x)/T )∑\ny∈Yi exp(−Eθ(y,x)/T ) for y ∈ Yi y1,y2 ← samples from Pi(y) yh ← argmaxy1,y2{BLEU(y1,yi), BLEU(y2,yi)} yl ← argminy1,y2{BLEU(y1,yi), BLEU(y2,yi)} M ← α(BLEU(yh,yi)− BLEU(yl,yi)) L ← L+max(M +Eθ(yh,xi)−Eθ(yl,xi), 0)\nend for θ ← θ − λ∇θL // λ is learning rate\nuntil Convergence\nIn this paper, we use BERT (Devlin et al., 2019) to parameterize both Eθ(y,x) and Eθ(y). Section 4.3 and 4.4 discuss the construction of Eθ in detail."
    }, {
      "heading" : "3 Related Work",
      "text" : "Grover et al. (2019) show that importance weights can be used to make generative models better fit the desired data distribution: pθ(y) ∝ q(y)ωθ(y), where q(y) is a generative model that we can efficiently take samples from and ωθ(y) is the importance weight function. The importance weights can\nbe determined using a discriminator that differentiates the generated samples from the target data. Rosenfeld et al.; Parshakova et al. (2001; 2019) define q(y) as autoregressive model and ωθ(y) using a log-linear model: ωθ(y) = exp(θTφ(y)), where φ(y) is the vector of sufficient statistics (features) evaluated at y. The log-linear model simplifies training the parameters θ: ∇θpθ(y) =∑\ny∈D φ(y)−Eŷ∼pθ(.)φ(ŷ). The expectation term can be estimated using rejecting sampling or importance sampling given the proposal distribution q. Deng et al. (2020) extend this approach for text generation by using unrestricted EBMs instead of log-linear models: ωθ(y) = exp(−Eθ(y)). They train the EBM using noise contrastive estimation (Gutmann and Hyvärinen, 2010). We find this less suitable for re-ranking in the translation tasks (see Section 4).\nDiscriminative re-ranking was first introduced by Shen et al. (2004) for improving the performance of machine translation (MT). They have trained a linear separator using the perceptron learning algorithm to distinguish the top r translations from the rest of the translations in the n-best possible outputs. The features for the discriminator are extracted from both source and target sentences. Mizumoto and Matsumoto (2016) combine the score of MT and the linear model using more complex syntactical features to re-rank the target sentences. Here, we rely on the features learned by BERT, and given the high capacity of the energy model, we train the energy model to respect the ranking of every pair of samples.\nGulcehre et al. (2017) describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: logPNMT(yi|y<i) + λ logPLM(yi|y<i), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation.\nRe-ranking with LM has also been explored by Ng et al. (2019), where they decode the output\nbased on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of using the direct model plus LM, nevertheless, backtranslation can also be added into our model for further improvement.\nRecently, Salazar et al. (2020) use masked language models (MLM) such as BERT to score hypotheses from NMT. Salazar et al. (2020) describe the score of a MLM as pseudo-log-likelihood score (PLL). To calculate PLL score of a sentence, each token wi in the sentence is sequentially masked, which allows the calculation of log p(wi|w\\i) from the output of the MLM. The normalized pseudolog-probability of the sentence is the average of logprobability of the masked words given the rest of the words in the sentence: 1N ∑N i=1 log p(wi|w\\i), where N is the length of the sentence. We use this approach as one of our baselines.\nIn parallel to our work, Guo et al. (2020) proposes using two different BERT models as an encoder of the source language (X-BERT) and a decoder of the target language (Y-BERT). Guo et al. (2020) add an extra trainable encoder-decoder adaption module followed by a feed-forward module to each layer of the decoder and a feed-forward module to each layer of the encoder. (Please see Guo et al. (2020) for more detail on the architecture.) For fine-tuning XY-BERT for translation tasks, Guo et al. (2020) keep all XY-BERT’s parameters fixed except the parameters of the new modules, and use mask-predict decoding (Ghazvininejad et al., 2019) for running test-time inference. Guo et al. (2020) report a significant improvement over prior non-autoregressive models and superior performance comparing to autoregressive methods on IWSLT’14 German-English task. Their finding is consistent with our improvement using the pretained BERT model. However, our Joint-EBM model is a different way of using BERT for translation, which does not require separate BERT models for source and target language. Please see Section 4.9 for a detailed comparison.\nFinally, other works also discuss using BERT to improve the performance of NMT. Clinchant et al. (2019) describe initializing the embedding or\nthe whole encoder with BERT’s parameters. Zhu et al. (2020) use an attention model to incorporate the output of BERT into encoder and decoder of NMT. In our approach, we use BERT as an external energy-based ranker."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We use German-English (De→En), RomanianEnglish (Ro→En) and Italian-English (It→En) from IWSLT’14 datasets and French-English (Fr→En) from IWSLT’17 translation tasks. We also use IWSLT’14 English-German (En→De) to show that the proposed method can be expanded to translation tasks with a different target language. All sentences were preprocessed using byte-pairencoding (Sennrich et al., 2016b). For all language pairs in IWSLT’14 and IWSLT’17, we merge the test datasets tst2010, tst2011, tst2012 and report BLEU on the merged dataset. We also use GermanEnglish (De→En) from the WMT’14 and EnglishGerman (En→De) from WMT’16 translation tasks.\nFinally, we use low-resource translation tasks Nepali-English (Ne→En) and Sinhala-English (Si→En) from FLoRes (Guzmán et al., 2019) translation tasks. We follow dataset distribution and preprocessing steps described in Guzmán et al. (2019) using the FLoRes implementation. FLoRes dataset contains development (dev), devtest and test dataset for both language pairs. Similar to Guzmán et al. (2019) we use the devtest dataset for all our evaluations."
    }, {
      "heading" : "4.2 Base Model",
      "text" : "We use the Transformer2(Vaswani et al., 2017) as our BaseNMT. Our Transformer architecture includes six encoder and six decoder layers, and the number of attention heads, embedding dimension and inner-layer dimension are 8, 512 and 4096, respectively. We use dropout, weight decay, label smoothing to regularize our models. We use layer normalization and early stopping. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and = 1e−8 and we use the same learning rate scheduler as Ott et al. (2019). We trained our models on 1 Nvidia TITANX GPU.\n2We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al., 2019) toolkits."
    }, {
      "heading" : "4.3 Marginal-EBM",
      "text" : "To construct the energy network over the sentences of the target language, we use a pretrained BERT (Devlin et al., 2019) from Huggingface (Wolf et al., 2019) as our pretrained language model and project the hidden state of BERT for each output token into a scalar value and define the energy value of the target sentence as the average of the scalar values. We use the BERT-base uncased model with 12 encoder layers, 768 hidden state dimension, 12 attention heads and 110M parameters. For the projection layer, we use a 2-layer MLP with 256 hidden variables. In our experiments, we only train the parameters of the projection layer and the rest of BERT’s parameters remain frozen. We use margin weight of α = 10 and temperature T = 1000 for our experiments. We regularize the projection layer using L2 regularization. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and = 1e−8 and a learning rate of 0.01. We run all experiments on 1 Nvidia TESLA M40 GPU."
    }, {
      "heading" : "4.4 Joint-EBM",
      "text" : "Joint-EBM must assign a score to a pair of sentences from source and target languages, so to construct the Joint-EBM, similar to Marginal-EBM, we need a Joint-BERT. We feed the sentence pairs from source and target languages jointly to BERT, thus the name Joint-BERT. Since JointBERT has not been pre-trained to accept pairs of sentences from two different languages, we finetune it for 12 epochs using the input format of [CLS]Source[SEP]Target[SEP] with the pairs of source and target sentences for each translation\ntask. For fine-tuning, we only mask the tokens of the target sentence. For all translation tasks we use the BERT-Base, Multilingual Cased model with 12 encoder layers, 768 hidden state dimension, 12 attention heads and 110M parameters. After finetuning Joint-BERT, we follow the same architecture as Marginal-EBM for the Joint-EBM."
    }, {
      "heading" : "4.5 Methods",
      "text" : "As the main baseline, we run beam decoding with a beam size of five over the trained BaseNMT (BaseNMT+Beam). We also use the samples drawn from the BaseNMT and report the BLEU score of the sample with the highest log-probability score on BaseNMT (BaseNMT+Sample). For all methods we use 100 target samples for each source sentence. BaseNMT+LM draws samples from the BaseNMT and uses logPNMT(y|x) + λ logPLM (y) to rank the samples (λ = 0.01 out of the set of {0.001, 0.01, 0.1} results in the best performance).\nIn our BaseNMT+LM baseline, we use pretrained language model to calculate logPLM (y). For the {De, Fr, It, Ro, Si, Ne}−→En tasks, we use a pretrained Transformer-XL (Dai et al., 2019) transfo-xl-wt103 and for the En−→De task we use a pretrained XLM (Lample and Conneau, 2019) xlm-mlm-ende-1024 from Huggingface (Wolf et al., 2019). BaseNMT+MLM is similar to BaseNMT+LM but it uses logPNMT(y|x) + λ logPMLM (y), where PMLM is the average pseudo-log-probability of sample y calculated using BERT. We use the same architecture of BERT as Marginal-EBM, but we fine-tuned BERT for MLM over the target sentences in training sets for 10 epochs. We tuned λ similar to BaseNMT+LM.\nEBR is our method that uses rank-based training for EBMs. We explore EBR with Marginal-EBM (Marginal-EBR) and Joint-EBM (ConditionalEBR). We also use noise-contrastive estimation to train our Marginal-EBM, similar to Deng et al. (2020), which we refer to as NCE-EBR. Next,\nwe have Shared-EBR that trains single MarginalEBM for the tasks with the same target language. Shared-EBR is only trained on IWSLT and FLoRes tasks with English target. For this method, we first sample a translation task and then sample a batch from that task and follow Algorithm 1 for the training of the Marginal-EBM. Finally, as an upper bound for the best achievable result, we also extract the translations from the sample that are closest to the gold data (based on BLEU score)."
    }, {
      "heading" : "4.6 Results",
      "text" : "Table 1 shows the performance of the described methods for IWSLT, FLoRes, and WMT translation tasks.3 BaseNMT+Sample achieves a better score than beam decoding suggesting that our multinomial sampling supports the modes of the distribution defined by the BaseNMT. Similarly, oracle values are high, indicating that the samples also support the desired distribution. This satisfies the necessary condition for Pθ(y|x) ∝ PNMT(y|x) exp(−Eθ(y,x)/T ) to be closer to the desired distribution. Re-ranking with a language model using BaseNMT+LM improves over BaseNMT+Sample for De→En, Fr→En, It→En, and En→De, but fails on Ro→En, Si→En, and Ne→En. However, in all of these tasks, the difference between BaseNMT+Sample and BaseNMT+LM is not substantial. BaseNMT+MLM is consistently better than BaseNMT+LM. The performance of BaseNMT+MLM is attributable to PLL scoring, as the encoder has the global information over the sentence. Marginal-EBR performs considerably better than BaseNMT+{Beam, Sample, LM, MLM} and better than NCE-EBR on all tasks except on Ne→En, where NCE-EBR outperforms MarginalEBR. The main advantage of Marginal-EBR over NCE-EBR is the use of only sampled data instead of gold data for training. See Section 4.7 for detailed discussion.\nShared-EBR has a significant improvement over the Marginal-EBR, especially it improves the lowresource task of Si→En by more than 2 BLEU points. For this task, we also show that how using more language pairs in training improves performance (Table 2).\nConditional-EBR outperforms Shared-EBR on all tasks. The performance of Conditional-EBR is\n3We use SacreBLEU (Post, 2018) as a consistent BLEU implementation for all of our experiments.\ndue to the use of Joint-EBM model, which enables the model to define different energy landscapes for different source sentences. Therefore, samples from the target language are more separable given the source sentence, while Marginal-EBM may not distinguish target sentences for different source sentences.\nThe translation improvement of using EBR on IWSLT and FLoRes translation tasks are more considerable than the improvement of using EBR on WMT tasks. We believe that pre-trained BERT helps low-resource tasks more than large-scale translation tasks."
    }, {
      "heading" : "4.7 Effect of Using Gold Data",
      "text" : "Noise-contrastive estimation (NCE) trains the energy model using a discriminative training to distinguish gold data from the sampled data (Gutmann and Hyvärinen, 2010; Deng et al., 2020). In contrast to the NCE-EBR, EBR does not directly use gold data in the training of the EBM, but only exploit it to determine the rank of two points as well as the margin. To show that our approach is effective, we introduce parameter γ as the percentage of the time that we can use gold data as one of the points (for example, yh in Algorithm 1). Table 3 shows the results for both De→En and Fr→En tasks using Marginal-EBR. As we increase the value of γ, the performance of Marginal-EBR drops. The main reason is that BaseNMT rarely produces the exact correct translation in the sample set, thus learning the ranking with respect to the gold data is not very informative. When the γ is zero, the Marginal-EBM learns to re-rank the samples with respect to their distance to the gold data."
    }, {
      "heading" : "4.8 Regularized Training",
      "text" : "We hypothesize that the performance of EBR improves as we increase the support of the base distribution toward the mode of the true distribution. To show that we add an entropy regularization term to the likelihood training of BaseNMT:\nmax θ ∑ (x,y)∈D ∑ i log p(yi|y<i,x)\n− β ∑ i p(yi) log p(yi). (2)\nEntropy regularization improves the diversity of samples, and as a result, Oracle’s score increases by 0.67 BLEU points. While BaseNMT only benefits less than 0.1 BLEU points from the regularization, Conditional-EBR improves by 0.3 BLEU points (see Table 4). For this study we explored β from {0.01, 0.1}, and reported results use β = 0.01 selected based on the validation set. BaseNMT trained with β = 0.1 has the Oracle score of 65.76 on the test set (comparing to the Oracle score of 68.21 for β = 0.01), which indicates that stronger regularization reduces the sample quality."
    }, {
      "heading" : "4.9 Using XY-BERT for Joint-EBM",
      "text" : "To explore the effect of a different way of conditioning on the source language, we compare the EBM constructed using the Joint-BERT model with EBM constructed using recently introduced XY-BERT (Guo et al., 2020). To construct EBM from XY-BERT, we remove the output layer and project each hidden-state of the final layer to a scalar energy value similar to how we build EBM from BERT. We compare these two models on IWSLT’14 De→En task. For XY-BERT we use German BERT for the encoder and English BERT for the decoder, following Guo et al. (2020). Our Joint-BERT uses Multilingual BERT because we feed both source and target sentences to BERT jointly. Conditional-EBR with XY-BERT achieves 38.33 BLEU score, which is 0.75 BLEU points higher than Conditional-EBR with Joint-BERT and improves the performance of XY-BERT with mask-predict decoding (Ghazvininejad et al., 2019) by 1.84 BLEU points.4 We believe that the improvement in Conditional-EBR using XY-BERT is mostly attributable to using specialized BERT models. Moreover, XY-BERT has extra trainable modules, so we could fine-tune XY-BERT on the trans-\n4Guo et al. (2020) report 36.49 BLEU score using XYBERT with 10 iterations of mask-predict decoding.\nlation task for 60 epochs, while keeping the rest of the parameters fixed without causing catastrophic forgetting. Joint-BERT, on the other hand, does not have any extra parameters, so we fine-tuned all parameters for only 15 epochs. Further training of Joint BERT resulted in poor performance. We leave adding extra modules for better fine-tuning of Joint BERT for future studies."
    }, {
      "heading" : "4.10 Maximizing Expected Score",
      "text" : "As another comparison, we train our models by directly maximizing the expected BLEU score (compared to rank-based training):\nmax θ\nEyp∼pθ(.|x)[BLEU(yp,y ∗)] (3)\nWe use log-trick to calculate the gradient of the above objective:\n∇θEpθ [BLEU(yp,y ∗)]\n= Eyp∼pθ [BLEU(yp,y ∗)[−∇θEθ(yp,x) + Ey′∼pθ [∇θE(y ′,x)]]]. (4)\nWe use self-normalized importance sampling to draw samples from the energy-based model. We use one sample to approximate the outer expectation and 10 samples to approximate the inner expectation. We train both Marginal-EBM and Joint-EBM by maximizing the expected BLEU score on IWSLT’14 DE-EN. The former obtains a score of 34.20 BLEU and the latter achieves 34.77 BLEU points. Both models underperform rankbased training."
    }, {
      "heading" : "4.11 Inference Time",
      "text" : "We compare the inference latency of EBR variations with BaseNMT (Table 5). We use 100 samples for re-ranking using Marginal-EBR, Conditional-EBR with Joint-BERT and Conditional EBR with XY-BERT (Guo et al., 2020). Inference on Marginal-EBR takes on average about 170 milliseconds per sentence more than inference in BaseNMT as we have to sample 100 sentences from BaseNMT and evaluate them on the energy model. We evaluate the Marginal-EBR only on the target sentences, while we evaluate ConditionalEBR for sequences from both source and target language, so the input sequence of Conditional-EBR is longer, thus having higher latency comparing to Marginal-EBR. We also measure the latency of Conditional-EBR when we use XY-BERT architecture to construct Joint-EBM. In this case, we have\ntwo separate BERT models for source and target languages, increasing the number of parameters by 3.3 million and latency by about 90 milliseconds per sentence compared to Conditional-EBR that uses the Joint-BERT model."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we study the sentence preference of Marginal-EBR created by the energy ranking."
    }, {
      "heading" : "5.1 Qualitative Analysis",
      "text" : "We qualitatively investigate how the output of Marginal-EBR differs from that of BaseNMT model. On the IWSLT’14 test set, we examined 200 examples on which Marginal-EBR did better than NMT and 200 examples where BaseNMT is better. We find that about 30% of the time, the Marginal-EBR model chooses a translation with changed pronoun. Another frequent ‘preference’ Marginal-EBR makes compared to BaseNMT is to use the contraction form. Since this IWSLT data set is from TED talk, we conjecture that the energy model favors the translations that are in more oral style. Besides, it is also common for the Marginal-EBR model to prefer rephrases, for example, instead of using ‘will’ as used in BaseNMT, Marginal-EBR chooses the form ‘am going to’. Finally, we find, for some pairs, Marginal-EBR chooses a different tense compared to the BaseNMT model (from MAP decoding).\nTable 6 presents quintessential examples we find after examining 400 examples on IWSLT’14 De→En test set. It is worth to mention that examples do not strictly land in only one category. For example, the sentences we show in the ‘Rephrase‘ type will also be counted as the change of pronouns. With this in mind, we compute statistics over the 400 sentences and find each of the ‘Pronoun’, ‘Contraction’ and ‘Rephrase’ appears approximately 30% of the time while 10% of the sentences change ‘Tense’. The other less frequent types are changing of determiners, prepositions and deletion (comparing the MAP decoding of BaseNMT and preferred\noutput by Marginal-EBR)."
    }, {
      "heading" : "5.2 BLEU Gains by Length",
      "text" : "Besides the qualitative analysis, we are also curious to see whether the improvement is affected by length. Table 7 shows the BLEU scores on the IWSLT’14 test set, which is divided into three bins according to the target length. Shorter sentences have the largest increase in BLEU, and the gain is decreasing as length increases. We reckon that it is easier for EBR to cover larger training space for sentences of shorter length and thus has the largest improvement in BLEU for these sentences."
    }, {
      "heading" : "5.3 Random Sentences",
      "text" : "In the absence of access to the source sentence, the energy model ranks the outputs purely according to the features of target sentences. We hypothesize that the energy model is better at differentiating incoherent and coherent sentences and manage to show that through the following analysis. We apply two kinds of shuffle on IWSLT’14 test set targets: (1) global shuffle: tokens in the sentence are randomly shuffled (2) local shuffle: we first randomly select a token and randomly shuffle the tokens within a local window of three. Then we compute the energy scores of these shuffled sentences as well as the untouched ones. The energy scores are listed in Table 8. (The energy model assign a lower energy to its preference.) We find 87%\nof the time, the energy model is able to distinguish the original sentence from a local shuffled one, and 90.5% from the global shuffled one. This supports our hypothesis that the energy model is capable of capturing the fluency of generated candidates."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We introduce energy-based re-ranking (EBR) to improve the performance of autoregressive neural machine translation. Despite its superior performance, EBR suffers from high latency because of its dependency on sampling from an autoregressive model. Directly sampling from the underlying EBM can speed up the inference, which is our future direction in order to benefit from the power of energy-based models for machine translation."
    } ],
    "references" : [ {
      "title" : "An actor-critic algorithm for sequence prediction",
      "author" : [ "Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2017",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "On the weaknesses of reinforcement learning for neural machine translation",
      "author" : [ "Leshem Choshen", "Lior Fox", "Zohar Aizenbud", "Omri Abend." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Choshen et al\\.,? 2020",
      "shortCiteRegEx" : "Choshen et al\\.",
      "year" : 2020
    }, {
      "title" : "On the use of BERT for neural machine translation",
      "author" : [ "Stephane Clinchant", "Kweon Woo Jung", "Vassilina Nikoulina." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 108–117.",
      "citeRegEx" : "Clinchant et al\\.,? 2019",
      "shortCiteRegEx" : "Clinchant et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov." ],
      "venue" : "CoRR, abs/1901.02860.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Residual energybased models for text generation",
      "author" : [ "Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proc. of ICLR",
      "citeRegEx" : "Deng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Using targetside monolingual data for neural machine translation through multi-task learning",
      "author" : [ "Tobias Domhan", "Felix Hieber." ],
      "venue" : "Proc. of EMNLP, pages 1500–1505.",
      "citeRegEx" : "Domhan and Hieber.,? 2017",
      "shortCiteRegEx" : "Domhan and Hieber.",
      "year" : 2017
    }, {
      "title" : "Is map decoding all you need? the inadequacy of the mode in neural machine translation",
      "author" : [ "Bryan Eikema", "Wilker Aziz." ],
      "venue" : "arXiv:2005.10283.",
      "citeRegEx" : "Eikema and Aziz.,? 2020",
      "shortCiteRegEx" : "Eikema and Aziz.",
      "year" : 2020
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language (EMNLP-IJCNLP),",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Bias correction of learned generative models using likelihood-free importance weighting",
      "author" : [ "Aditya Grover", "Jiaming Song", "Ashish Kapoor", "Kenneth Tran", "Alekh Agarwal", "Eric J Horvitz", "Stefano Ermon." ],
      "venue" : "Advances in Neural Information Processing Sys-",
      "citeRegEx" : "Grover et al\\.,? 2019",
      "shortCiteRegEx" : "Grover et al\\.",
      "year" : 2019
    }, {
      "title" : "On integrating a language model into neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Computer Speech & Language, 45:137–148.",
      "citeRegEx" : "Gulcehre et al\\.,? 2017",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2017
    }, {
      "title" : "Incorporating bert into parallel sequence decoding with adapters",
      "author" : [ "Junliang Guo", "Zhirui Zhang", "Linli Xu", "Hao-Ran Wei", "Boxing Chen", "Enhong Chen." ],
      "venue" : "Advances in Neural Information Processing Systems 33.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Michael Gutmann", "Aapo Hyvärinen." ],
      "venue" : "Proc. of AISTATS, pages 297–304.",
      "citeRegEx" : "Gutmann and Hyvärinen.,? 2010",
      "shortCiteRegEx" : "Gutmann and Hyvärinen.",
      "year" : 2010
    }, {
      "title" : "Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english",
      "author" : [ "Francisco Guzmán", "Peng-Jen Chen", "Myle Ott", "Juan Pino", "Guillaume Lample", "Philipp Koehn", "Vishrav Chaudhary", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Guzmán et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2019
    }, {
      "title" : "Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english",
      "author" : [ "Francisco Guzmán", "Peng-Jen Chen", "Myle Ott", "Juan Pino", "Guillaume Lample", "Philipp Koehn", "Vishrav Chaudhary", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Guzmán et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Opennmt: Open-source toolkit for neural machine translation",
      "author" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "CoRR, abs/1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "A tutorial on energy-based learning",
      "author" : [ "Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang." ],
      "venue" : "Predicting structured data, 1(0).",
      "citeRegEx" : "LeCun et al\\.,? 2006",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2006
    }, {
      "title" : "Discriminative reranking for grammatical error correction with statistical machine translation",
      "author" : [ "Tomoya Mizumoto", "Yuji Matsumoto." ],
      "venue" : "Proc. of NAACL-HLT, pages 1133–1138.",
      "citeRegEx" : "Mizumoto and Matsumoto.,? 2016",
      "shortCiteRegEx" : "Mizumoto and Matsumoto.",
      "year" : 2016
    }, {
      "title" : "Facebook FAIR’s WMT19 news translation task submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "Reward augmented maximum likelihood for neural structured prediction",
      "author" : [ "Mohammad Norouzi", "Samy Bengio", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Norouzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2016
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proc. of NAACLHLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Global autoregressive models for data-efficient sequence learning",
      "author" : [ "Tetiana Parshakova", "Jean-Marc Andreoli", "Marc Dymetman." ],
      "venue" : "Proc. of CoNLL, pages 900–909.",
      "citeRegEx" : "Parshakova et al\\.,? 2019",
      "shortCiteRegEx" : "Parshakova et al\\.",
      "year" : 2019
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "In Proc. of ICLR",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Samplerank: Training factor graphs with atomic gradients",
      "author" : [ "Khashayar Rohanimanesh", "Kedar Bellare", "Aron Culotta", "Andrew McCallum", "Michael L Wick." ],
      "venue" : "Proc. of ICML, pages 777–784.",
      "citeRegEx" : "Rohanimanesh et al\\.,? 2011",
      "shortCiteRegEx" : "Rohanimanesh et al\\.",
      "year" : 2011
    }, {
      "title" : "Training structured prediction energy networks with indirect supervision",
      "author" : [ "Amirmohammad Rooshenas", "Aishwarya Kamath", "Andrew McCallum." ],
      "venue" : "Proc. of NAACL-HLT.",
      "citeRegEx" : "Rooshenas et al\\.,? 2018",
      "shortCiteRegEx" : "Rooshenas et al\\.",
      "year" : 2018
    }, {
      "title" : "Searchguided, lightly-supervised training of structured prediction energy networks",
      "author" : [ "Amirmohammad Rooshenas", "Dongxu Zhang", "Gopal Sharma", "Andrew McCallum." ],
      "venue" : "Advances in Neural",
      "citeRegEx" : "Rooshenas et al\\.,? 2019",
      "shortCiteRegEx" : "Rooshenas et al\\.",
      "year" : 2019
    }, {
      "title" : "Whole-sentence exponential language models: a vehicle for linguistic-statistical integration",
      "author" : [ "Ronald Rosenfeld", "Stanley F Chen", "Xiaojin Zhu." ],
      "venue" : "Computer Speech & Language, 15(1):55–73.",
      "citeRegEx" : "Rosenfeld et al\\.,? 2001",
      "shortCiteRegEx" : "Rosenfeld et al\\.",
      "year" : 2001
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712. Association for Computa-",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proc. of ACL, pages 86–96.",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proc. of ACL, pages 1715– 1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "Proc. of NAACL-HLT.",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "Proc. of ACL, pages 1683–1692.",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "A study of reinforcement learning for neural machine translation",
      "author" : [ "Lijun Wu", "Fei Tian", "Tao Qin", "Jianhuang Lai", "TieYan Liu." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Incorporating BERT into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tieyan Liu." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).",
      "startOffset" : 221,
      "endOffset" : 301
    }, {
      "referenceID" : 23,
      "context" : "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).",
      "startOffset" : 221,
      "endOffset" : 301
    }, {
      "referenceID" : 36,
      "context" : "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).",
      "startOffset" : 221,
      "endOffset" : 301
    }, {
      "referenceID" : 39,
      "context" : "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).",
      "startOffset" : 221,
      "endOffset" : 301
    }, {
      "referenceID" : 1,
      "context" : "Autoregressive models are widely used for neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "Autoregressive models are widely used for neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 37,
      "context" : "Autoregressive models are widely used for neural machine translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 39,
      "context" : "However, for NMT, these training algorithms are often used in conjunction with MLE training (Wu et al., 2018) or as fine-tuning (Choshen et al.",
      "startOffset" : 92,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "EBMs (LeCun et al., 2006) are general parametric models that assign a scalar energy value to each configuration of input variables, thus defining an unnormalized probability distribu-",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "Our energy ranker consistently improves the performance of Transformerbased NMT on German-English, RomanianEnglish and Italian-English tasks from IWSLT’14, the French-English task from IWSLT’17, GermanEnglish task from WMT’14, and English-German task from WMT’16, as well as the low-resource Sinhala-English and Nepali-English tasks described in the FLoRes dataset (Guzmán et al., 2019).",
      "startOffset" : 365,
      "endOffset" : 386
    }, {
      "referenceID" : 28,
      "context" : "To train θ we use rank-based training (Rohanimanesh et al., 2011; Rooshenas et al., 2018, 2019).",
      "startOffset" : 38,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we use BERT (Devlin et al., 2019) to parameterize both Eθ(y,x) and Eθ(y).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "They train the EBM using noise contrastive estimation (Gutmann and Hyvärinen, 2010).",
      "startOffset" : 54,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "(2020) keep all XY-BERT’s parameters fixed except the parameters of the new modules, and use mask-predict decoding (Ghazvininejad et al., 2019) for running test-time inference.",
      "startOffset" : 115,
      "endOffset" : 143
    }, {
      "referenceID" : 34,
      "context" : "All sentences were preprocessed using byte-pairencoding (Sennrich et al., 2016b).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "Finally, we use low-resource translation tasks Nepali-English (Ne→En) and Sinhala-English (Si→En) from FLoRes (Guzmán et al., 2019) translation tasks.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 37,
      "context" : "We use the Transformer2(Vaswani et al., 2017) as our BaseNMT.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : "We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "trained BERT (Devlin et al., 2019) from Huggingface (Wolf et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 38,
      "context" : ", 2019) from Huggingface (Wolf et al., 2019) as our pretrained language model and project the hidden state of BERT for each output token into a scalar value and define the energy value of the target sentence as the average of",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "For the {De, Fr, It, Ro, Si, Ne}− →En tasks, we use a pretrained Transformer-XL (Dai et al., 2019) transfo-xl-wt103 and for the En− →De task we use a pretrained XLM (Lample and Conneau, 2019) xlm-mlm-ende-1024 from Huggingface (Wolf et al.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : ", 2019) transfo-xl-wt103 and for the En− →De task we use a pretrained XLM (Lample and Conneau, 2019) xlm-mlm-ende-1024 from Huggingface (Wolf et al.",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 38,
      "context" : ", 2019) transfo-xl-wt103 and for the En− →De task we use a pretrained XLM (Lample and Conneau, 2019) xlm-mlm-ende-1024 from Huggingface (Wolf et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 26,
      "context" : "We use SacreBLEU (Post, 2018) as a consistent BLEU implementation for all of our experiments.",
      "startOffset" : 17,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Noise-contrastive estimation (NCE) trains the energy model using a discriminative training to distinguish gold data from the sampled data (Gutmann and Hyvärinen, 2010; Deng et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "Noise-contrastive estimation (NCE) trains the energy model using a discriminative training to distinguish gold data from the sampled data (Gutmann and Hyvärinen, 2010; Deng et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : "tioning on the source language, we compare the EBM constructed using the Joint-BERT model with EBM constructed using recently introduced XY-BERT (Guo et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "75 BLEU points higher than Conditional-EBR with Joint-BERT and improves the performance of XY-BERT with mask-predict decoding (Ghazvininejad et al., 2019) by 1.",
      "startOffset" : 126,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "We use 100 samples for re-ranking using Marginal-EBR, Conditional-EBR with Joint-BERT and Conditional EBR with XY-BERT (Guo et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 137
    } ],
    "year" : 2021,
    "abstractText" : "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energybased model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT’14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT’16 English-German tasks.",
    "creator" : "LaTeX with hyperref"
  }
}