{
  "name" : "2021.acl-long.338.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability",
    "authors" : [ "Jiaao Chen", "Dinghan Shen", "Weizhu Chen", "Diyi Yang" ],
    "emails" : [ "jchen896@gatech.edu", "dyang888@gatech.edu", "dishen@microsoft.com", "wzchen@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4380–4390\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4380"
    }, {
      "heading" : "1 Introduction",
      "text" : "Fine-tuning large-scale pre-trained language models (PLMs) has become a dominant paradigm in the natural language processing community, achieving state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020). Despite the great success, due to the huge gap between the number of model parameters and that of task-specific data available, the majority of the information within the multi-layer self-attention networks is typically redundant and ineffectively utilized for downstream tasks (Guo et al., 2020; Gordon et al., 2020; Dalvi et al., 2020).\nAs a result, after task-specific fine-tuning, models are very likely to overfit and make predictions based on spurious patterns (Tu et al., 2020; Kaushik et al., 2020), making them less generalizable to outof-domain distributions (Zhu et al., 2019; Jiang et al., 2019; Aghajanyan et al., 2020).\nIn order to improve the generalization abilities of over-parameterized models with limited amount of task-specific data, various regularization approaches have been proposed, such as adversarial training that injects label-preserving perturbations in the input space (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019), generating augmented data via carefully-designed rules (McCoy et al., 2019; Xie et al., 2020; Andreas, 2020; Shen et al., 2020), and annotating counterfactual examples (Goyal et al., 2019; Kaushik et al., 2020). Despite substantial improvements, these methods often require significant computational and memory overhead (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019; Xie et al., 2020) or human annotations (Goyal et al., 2019; Kaushik et al., 2020).\nIn this work, to alleviate the above issues, we rethink the simple and commonly-used regularization technique—dropout (Srivastava et al., 2014)— in pre-trained transformer models (Vaswani et al., 2017). With multiple self-attention heads in transformers, dropout converts some hidden units to zeros in a random and independent manner. Although PLMs have already been equipped with the dropout regularization, they still suffer from inferior performances when it comes to out-of-distribution cases (Tu et al., 2020; Kaushik et al., 2020). The underlying reasons are two-fold: (1) the linguistic relations among words in a sentence is ignored while dropping the hidden units randomly. In reality, these masked features could be easily inferred from surrounding unmasked hidden units with the self-attention networks. Therefore, redundant information still exists and gets passed to the upper\nlayers. (2) The standard dropout assumes that every hidden unit is equally important with the random sampling procedure, failing to characterize the different roles these features play in distinct tasks. As a result, the learned representations are not generalized enough while applied to other data and tasks. To drop the information more effectively, Shen et al. (2020) recently introduce Cutoff to remove tokens/features/spans in the input space. Even though models will not see the removed information during training, examples with large noise may be generated when key clues for predictions are completely removed from the input.\nTo overcome these limitations, we propose a simple yet effective data augmentation method, HiddenCut, to regularize PLMs during the fine-tuning stage. Specifically, the approach is based on the linguistic intuition that hidden representations of adjacent words are more likely to contain similar and redundant information. HiddenCut drops hidden units more structurally by masking the whole hidden information of contiguous spans of tokens after every encoding layer. This would encourage models to fully utilize all the task-related information, instead of learning spurious patterns during training. To make the dropping process more efficient, we dynamically and strategically select the informative spans to drop by introducing an attentionbased mechanism. By performing HiddenCut in the hidden space, the impact of dropped information is only mitigated rather than completely removed, avoiding injecting too much noise to the input. We further apply a Jensen-Shannon Divergence consistency regularization between the original and these augmented examples to model the consistent relations between them.\nTo demonstrate the effectiveness of our methods, we conduct experiments to compare our HiddenCut with previous state-of-the-art data augmentation method on 8 natural language understanding tasks from the GLUE (Wang et al., 2018) benchmark for in-distribution evaluations, and 5 challenging datasets that cover single-sentence tasks, similarity and paraphrase tasks and inference tasks for out-ofdistribution evaluations. We further perform ablation studies to investigate the impact of different selecting strategies on HiddenCut’s effectiveness. Results show that our method consistently outperforms baselines, especially on out-of-distribution and challenging counterexamples. To sum up, our contributions are:\n• We propose a simple data augmentation method, HiddenCut, to regularize PLMs during fine-tuning by cutting contiguous spans of representations in the hidden space.\n• We explore and design different strategic sampling techniques to dynamically and adaptively construct the set of spans to be cut.\n• We demonstrate the effectiveness of HiddenCut through extensive experiments on both indistribution and out-of-distribution datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Adversarial Training",
      "text" : "Adversarial training methods usually regularize models through applying perturbations to the input or hidden space (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) with additional forward-backward passes, which influence the model’s predictions and confidence without changing human judgements. Adversarial-based approaches have been actively applied to various NLP tasks in order to improve models’ robustness and generalization abilities, such as sentence classification (Miyato et al., 2017), machine reading comprehension (MRC) (Wang and Bansal, 2018) and natural language inference (NLI) tasks (Nie et al., 2020). Despite its success, adversarial training often requires extensive computation overhead to calculate the perturbation directions (Shafahi et al., 2019; Zhang et al., 2019a). In contrast, our HiddenCut adds perturbations in the hidden space in a more efficient way that does not require extra computations as the designed perturbations can be directly derived from self-attentions."
    }, {
      "heading" : "2.2 Data Augmentation",
      "text" : "Another line of work to improve the model robustness is to directly design data augmentation methods to enrich the original training set such as creating syntactically-rich examples (McCoy et al., 2019; Min et al., 2020) with specific rules, crowdsourcing counterfactual augmentation to avoid learning spurious features (Goyal et al., 2019; Kaushik et al., 2020), or combining examples in the dataset to increase compositional generalizabilities (Jia and Liang, 2016; Andreas, 2020; Chen et al., 2020b,a). However, they either require careful design (McCoy et al., 2019; Andreas, 2020) to infer labels for generated data or extensive human annotations (Goyal et al., 2019; Kaushik et al., 2020),\nwhich makes them hard to generalize to different tasks/datasets. Recently Shen et al. (2020) introduce a set of cutoff augmentation which directly creates partial views to augment the training in a more task-agnostic way. Inspired by these prior work, our HiddenCut aims at improving models’ generalization abilities to out-of-distribution via linguistic-informed strategically dropping spans of hidden information in transformers."
    }, {
      "heading" : "2.3 Dropout-based Regularization",
      "text" : "Variations of dropout (Srivastava et al., 2014) have been proposed to regularize neural models by injecting noise through dropping certain information so that models do not overfit training data. However, the major efforts have been put to convolutional neural networks and trimmed for structures in images recently such as DropPath (Larsson et al., 2017), DropBlock (Ghiasi et al., 2018), DropCluster (Chen et al., 2020c) and AutoDropout (Pham and Le, 2021). In contrast, our work takes a closer look at transformer-based models and introduces HiddenCut for natural language understanding tasks. HiddenCut is closely related to DropBlock (Ghiasi et al., 2018), which drops contiguous regions from a feature map. However, different from images, hidden dimensions in PLMs that contain syntactic/semantic information for NLP tasks are more closely related (e.g., NER and POS information), and simply dropping spans of features in certain hidden dimensions might still lead to information redundancy."
    }, {
      "heading" : "3 HiddenCut Approach",
      "text" : "To regularize transformer models in a more structural and efficient manner, in this section, we introduce a simple yet effective data augmentation technique, HiddenCut, that reforms dropout to cutting contiguous spans of hidden representations after each transformer layer (Section 3.1). Intuitively, the proposed approach encourages the models to fully utilize all the hidden information within the self-attention networks. Furthermore, we propose an attention-based mechanism to strategically and judiciously determine the specific spans to cut (Section 3.2). The schematic diagram of HiddenCut, applied to the transformer architecture (and its comparison to dropout) are shown in Figure 1."
    }, {
      "heading" : "3.1 HiddenCut",
      "text" : "For an input sequence s = {w0, w1, ..., wL} with L tokens associated with a label y, we employ a pre-trained transformer model f1:M (·) with M layers like RoBERTa (Liu et al., 2019) to encode the text into hidden representations. Thereafter, an inference network g(·) is learned on top of the pretrained models to predict the corresponding labels. In the hidden space, after layer m, every word wi in the input sequence is encoded into a D dimensional vector hmi ∈ RD and the whole sequence could be viewed as a hidden matrix Hm ∈ RL×D.\nWith multiple self-attention heads in the transformer layers, it is found that there is extensive redundant information across hmi ∈ H that are linguistically related (Dalvi et al., 2020) (e.g., words that share similar semantic meanings). As a result, the removed information from the standard dropout operation may be easily inferred from the remaining unmasked hidden units. The resulting model might easily overfit to certain high-frequency features without utilizing all the important task-related information in the hidden space (especially when task-related data is limited). Moreover, the model also suffers from poor generalization ability while being applied to out-of-distribution cases.\nInspired by Ghiasi et al. (2018); Shen et al. (2020), we propose to improve the dropout regularization in transformer models by creating augmented training examples through HiddenCut, which drops a contiguous span of hidden information encoded in every layer, as shown in Figure 1 (c). Mathematically, in every layer m, a span of hidden vectors, S ∈ Rl×D, with length l = αL in the hidden matrix Hm ∈ RL×D are converted to 0, and the corresponding attention masks are adjusted to 0, where α is a pre-defined hyper-parameter indicating the dropping extent of HiddenCut. After being encoded and hiddencut through all the hidden layers in pre-trained encoders, augmented training data fHiddenCut(s) is created for learning the inference network g(·) to predict task labels."
    }, {
      "heading" : "3.2 Strategic Sampling",
      "text" : "Different tasks rely on learning distinct sets of information from the input to predict the corresponding task labels. Performing HiddenCut randomly might be inefficient especially when most of the dropping happens at task-unrelated spans, which fails to effectively regularize model to take advantage of all the task-related features. To this end, we\npropose to select the spans to be cut dynamically and strategically in every layer. In other words, we mask the most informative span of hidden representations in one layer to force models to discover other useful clues to make predictions instead of relying on a small set of spurious patterns.\nAttention-based Sampling Strategy The most direct way is to define the set of tokens to be cut by utilizing attention weights assigned to tokens in the self-attention layers (Kovaleva et al., 2019). Intuitively, we can drop the spans of hidden representations that are assigned high attentions by the transformer layers. As a result, the information redundancy is alleviated and models would be encourage to attend to other important information. Specifically, we first derive the average attention for each token, ai, from the attention weights matrix A ∈ RP×L×L after self-attention layers, where P is the number of attention heads and L is the sequence length:\nai =\n∑P j ( ∑L k A[j][k][i])\nP .\nWe then sample the start token hi for HiddenCut from the set that contains top βL tokens with higher average attention weights (β is a pre-defined parameter). Then HiddenCut is performed to mask the hidden representations between hi and hi+l. Note that the salient sets are different across different layers and updated throughout the training.\nOther Sampling Strategies We also explore other widely used word importance discovery meth-\nods to find a set of tokens to be strategically cut by HiddenCut, including:\n• Random: All spans of tokens are viewed as equally important, thus are randomly cut.\n• LIME (Ribeiro et al., 2016) defines the importance of tokens by examining the locally faithfulness where weights of tokens are assigned by classifiers trained with sentences whose words are randomly removed. We utilized LIME on top of a SVM classifier to pre-define a fixed set of tokens to be cut.\n• GEM (Yang et al., 2019b) utilizes orthogonal basis to calculate the novelty scores that measure the new semantic meaning in tokens, significance scores that estimate the alignment between the semantic meaning of tokens and the sentence-level meaning, and the uniqueness scores that examine the uniqueness of the semantic meaning of tokens. We compute the GEM scores using the hidden representations at every layer to generate the set of tokens to be cut, which are updated during training.\n• Gradient (Baehrens et al., 2010): We define the set of tokens to be cut based on the rankings of the absolute values of gradients they received at every layer in the backward-passing. This set would be updated during training."
    }, {
      "heading" : "3.3 Objectives",
      "text" : "During training, for an input text sequence s with a label y, we generate N augmented examples {fHiddenCut1 (s), ..., fHiddenCutN (s)} through performing HiddenCut in pre-trained encoder f(·). The whole model g(f(·)) is then trained though several objectives including general classification loss (Lori and Laug) on data-label pairs and consistency regularization (Ljs) (Miyato et al., 2017, 2018; Clark et al., 2018; Xie et al., 2019; Shen et al., 2020) across different augmentations:\nLori = CE(g(f(s)), y) Laug = ∑ N CE(g(fHiddenCuti (s)), y)\nLjs = ∑ N KL[p(y|g(fHiddenCuti (s))||pavg]\nwhere CE and KL represent the cross-entropy loss and KL-divergence respectively. pavg stands for the average predictions across the original text and all the augmented examples.\nCombining these three losses, our overall objective function is:\nL = Lori + γLaug + ηLjs\nwhere γ and η are the weights used to balance the contributions of learning from the original data and augmented data."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conducted experiments on both in-distribution datasets and out-of-distribution datasets to demonstrate the effectiveness of our proposed HiddenCut.\nIn-Distribution Datasets We mainly trained and evaluated our methods on the widely-used GLUE benchmark (Wang et al., 2018) which covers a wide range of natural language understanding tasks: single-sentence tasks including: (i) Stanford Sentiment Treebank (SST-2) which predict the sentiment of movie reviews to be positive or negative, and (ii) Corpus of Linguistic Acceptability (CoLA) which predict whether a sentence is linguistically acceptable or not; similarity and paraphrase tasks including (i) Quora Question Pairs (QQP) which predict whether two question are paraphrases, (ii) Semantic Textual Similarity Benchmark (STS-B) which predict the similarity ratings between two sentences, and (iii) Microsoft Research Paraphrase Corpus\n(MRPC) which predict whether two given sentences are semantically equivalent; inference tasks including (i) Multi-Genre Natural Language Inference (MNLI) which classified the relationships between two sentences into entailment, contradiction, or neutral, (ii) Question Natural Language Inference (QNLI) which predict whether a given sentence is the correct answer to a given question, and (iii) Recognizing Textual Entailment (RTE) which predict whether the entailment relation holds between two sentences. Accuracy was used as the evaluation metric for most of the datasets except that Matthews correlation was used for CoLA and Spearman correlation was utilized for STS-B.\nOut-Of-Distribution Datasets To demonstrate the generalization abilities of our proposed methods, we directly evaluated on 5 different out-ofdistribution challenging sets, using the models that are fine-tuned on GLUE benchmark datasets:\n• Single Sentence Tasks: Models fine-tuned from SST-2 are directly evaluated on two recent challenging sentiment classification datasets: IMDB Contrast Set (Gardner et al., 2020) including 588 examples and IMDB Counterfactually Augmented Dataset (Kaushik et al., 2020) including 733 examples. Both of them were constructed by asking NLP researchers (Gardner et al., 2020) or Amazon Mechanical Turkers (Kaushik et al., 2020) to make minor edits to examples in the original IMDB dataset (Maas et al., 2011) so that the sentiment labels change while the major contents keep the same.\n• Similarity and Paraphrase Tasks: Models fine-tuned from QQP are directly evaluated on the recently introduced challenging paraphrase dataset PAWS-QQP (Zhang et al., 2019b) that has 669 test cases. PAWS-QQP contains sentence pairs with high word overlap but different semantic meanings created via word-swapping and back-translation from the original QQP dataset.\n• Inference Tasks: Models fine-tuned from MNLI are directly evaluated on two challenging NLI sets: HANS (McCoy et al., 2019) with 30,000 test cases and Adversarial NLI (A1 dev sets) (Nie et al., 2020) including 1,000 test cases. The former one was constructed by using syntactic rules (lexical overlap, subsequence and constituent) to generate\nnon-entailment examples with high premisehypothesis overlap from MNLI. The latter one was created by adversarial human-and-modelin-the-loop framework (Nie et al., 2020) to create hard examples based on BERT-Large models(Devlin et al., 2019) pre-trained on SNLI (Bowman et al., 2015) and MNLI."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We compare our methods with several baselines:\n• RoBERTa (Liu et al., 2019) is used as our base model. Note that RoBERTa is regularized with dropout during fine-tuning.\n• ALUM (Liu et al., 2020) is the state-of-theart adversarial training method for neural language models, which regularizes fine-tuning via perturbations in the embedding space.\n• Cutoff (Shen et al., 2020) is a recent data augmentation for natural language understanding tasks by removing information in the input space, including three variations: token cutoff, feature cutoff, and span cutoff."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "We used the RoBERTa-base model (Liu et al., 2019) to initialize all the methods. Note that HiddenCut is agnostic to different types of pre-trained models. We followed Liu et al. (2019) to set the linear decay scheduler with a warmup ratio of 0.06 for training. The maximum learning rate was selected from {5e−6, 8e−6, 1e−5, 2e−5} and the\nmax number of training epochs was set to be either 5 or 10. All these hyper-parameters are shared for all the models. The HiddenCut ratio α was set 0.1 after a grid search from {0.05, 0.1, 0.2, 0.3, 0.4}. The selecting ratio β in the important sets sampling process was set 0.4 after a grid search from {0.1, 0.2, 0.4, 0.6}. The weights γ and η in our objective function were both 1. All the experiments were performed using a GeForce RTX 2080Ti."
    }, {
      "heading" : "4.4 Results on In-Distribution Datasets",
      "text" : "Based on Table 1, we observed that, compared to RoBERTa-base with only dropout regularization, ALUM with perturbations in the embedding space through adversarial training has better results on most of these GLUE tasks. However, the extra additional backward passes to determine the perturbation directions in ALUM can bring in significantly more computational and memory overhead. By masking different types of input during training, Cutoff increased the performances while being more computationally efficient.\nIn contrast to Span Cutoff, HiddenCut not only introduced zero additional computation cost, but also demonstrated stronger performances on 7 out of 8 GLUE tasks, especially when the size of training set is small (e.g., an increase of 1.1 on RTE and 1.5 on CoLA). Moreover, HiddenCut achieved the best average result compared to previous stateof-the-art baselines. These in-distribution improvements indicated that, by strategically dropping contiguous spans in the hidden space, HiddenCut not\nonly helps pre-trained models utilize hidden information in a more effective way, but also injects less noise during the augmentation process compared to cutoff, e.g., Span Cutoff might bring in additional noises for CoLA (which aims to judge whether input sentences being linguistically acceptable or not) when one span in the input is removed, since it might change the labels."
    }, {
      "heading" : "4.5 Results on Out-Of-Distribution Datasets",
      "text" : "To validate the better generalizability of HiddenCut, we tested our models trained on SST-2, QQP and MNLI directly on 5 out-of-distribution/outof-domain challenging sets in zero-shot settings. As mentioned earlier, these out-of-distribution sets were either constructed with in-domain/out-ofdomain data and further edited by human to make them harder, or generated by rules that exploited spurious correlations such as lexical overlap, which made them challenging to most existing models.\nAs shown in Table 2, Span Cutoff slightly improved the performances compared to RoBERTa by adding extra regularizations through creating restricted input. HiddenCut significantly outperformed both RoBERTa and Span Cutoff. For example, it outperformed Span Cutoff. by 2.3%(87.8% vs. 85.5%) on IMDB-Conts, 2.7%(41.5% vs. 38.8%) on PAWS-QQP, and 2.8%(71.2% vs 68.4%) on HANS consistently. These superior results demonstrated that, by dynamically and strategically dropping contiguous span of hidden representations, HiddenCut was able to better utilize all the important task-related information which improved the model generalization to out-of-distribution and challenging adversary examples."
    }, {
      "heading" : "4.6 Ablation Studies",
      "text" : "This section presents our ablation studies on different sampling strategies and the effect of important hyper-parameters in HiddenCut."
    }, {
      "heading" : "4.6.1 Sampling Strategies in HiddenCut",
      "text" : "We compared different ways to cut hidden representations (DropBlock (Ghiasi et al., 2018) which randomly dropped spans in certain random hidden dimensions instead of the whole hidden space) and different sampling strategies for HiddenCut described in Section 3.2 (including Random, LIME (Ribeiro et al., 2016), GEM (Yang et al., 2019b), Gradient (Yeh et al., 2019), Attention) based on the performances on SST-2 and QNLI. For these strategies, we also experimented with a reverse set\ndenoted by “-R” where we sampled outside the important set given by above strategies.\nFrom Table 3, we observed that (i) sampling from important sets resulted in better performances than random sampling. Sampling outside the defined importance sets usually led to inferior performances. These highlights the importance of strategically selecting spans to drop. (ii) Sampling from dynamic sets sampled by their probabilities often outperformed sampling from predefined fixed sets (LIME), indicating the effectiveness of dynamically adjusting the sampling sets during training. (iii) The attention-based strategy outperformed all other sampling strategies, demonstrating the effectiveness of our proposed sampling strategies for HiddenCut. (iv) Completely dropping out the spans of hidden representations generated better results than only removing certain dimensions in the hidden space, which further validated the benefit of HiddenCut over DropBlock in natural language understanding tasks."
    }, {
      "heading" : "4.6.2 The Effect of HiddenCut Ratios",
      "text" : "The length of spans that are dropped by HiddenCut is an important hyper-parameter, which is controlled by the HiddenCut ratio α and the length of input sentences. α could also be interpreted as the extent of perturbations added to the hidden space. We presented the results of HiddenCut on MNLI with a set of different α including {0.05, 0.1, 0.2, 0.3, 0.4} in Table 5. HiddenCut achieved the best performance with α = 0.1, and\nthe performance gradually decreased with higher α since larger noise might be introduced when dropping more hidden information. This suggested the importance of balancing the trade-off between applying proper perturbations to regularize models and injecting potential noises."
    }, {
      "heading" : "4.6.3 The Effect of Sampling Ratios",
      "text" : "The number of words that are considered important and selected by HiddenCut is also an influential hyper-parameter controlled by the sampling ratio β and the length of input sentences. As shown in Table 6, we compared the performances on SST-2 by adopting different β including {0.1, 0.2, 0.4, 0.6}. When β is too small, the number of words in the important sets is limited, which might lead HiddenCut to consistently drop certain hidden spans during the entire training process. The low diversities reduce the improvements over baselines. When β is too large, the important sets might cover all the words except stop words in sentences. As a result, the Attention-based Strategy actually became Random Sampling, which led to lower gains over baselines. The best performance was achieved when β = 0.4, indicating a reasonable trade-off between diversities and efficiencies."
    }, {
      "heading" : "4.7 Visualization of Attentions",
      "text" : "To further demonstrate the effectiveness of HiddenCut, we visualize the attention weights that the special start token (“<s>”) assigns to other tokens at the last layer, via several examples and their coun-\nterfactual examples in Table 4. We observed that RoBERTa only assigned higher attention weights on certain tokens such as “8 stars”, “intriguing” and especially the end special token “</s>”, while largely ignored other context tokens that were also important to make the correct predictions such as scale descriptions (e.g., “out of 10”) and qualifier words (e.g., “more and more”). This was probably because words like “8 stars” and “intriguing” were highly correlated with positive label and RoBERTa might overfit such patterns without probable regularization. As a result, when the scale of ratings (e.g., from “10” to “20”) or the qualifier words changed (e.g., from “more and more” to “only slightly more”), RoBERTa still predicted the label as positive even when the groundtruth is negative. With HiddenCut, models mitigated the impact of tokens with higher attention weights and were encouraged to utilize all the related information. So the attention weights in HiddenCut were more uniformly distributed, which helped models make the correct predictions for out-of-distribution counterfactual examples. Taken together, HiddenCut helps improve model’s generalizability by facilitating it to learn from more task-related information."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we introduced a simple yet effective data augmentation technique, HiddenCut, to improve model robustness on a wide range of natural language understanding tasks by drop-\nping contiguous spans of hidden representations in the hidden space directed by strategic attentionbased sampling strategies. Through HiddenCut, transformer models are encouraged to make use of all the task-related information during training rather than only relying on certain spurious clues. Through extensive experiments on indistribution datasets (GLUE benchmarks) and outof-distribution datasets (challenging counterexamples), HiddenCut consistently and significantly outperformed state-of-the-art baselines, and demonstrated superior generalization performances."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We would like to thank the anonymous reviewers, and the members of Georgia Tech SALT group for their feedback. This work is supported in part by grants from Amazon and Salesforce."
    } ],
    "references" : [ {
      "title" : "Better fine-tuning by reducing representational collapse",
      "author" : [ "Armen Aghajanyan", "Akshat Shrivastava", "Anchit Gupta", "Naman Goyal", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "arXiv preprint arXiv:2008.03156.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2020",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2020
    }, {
      "title" : "Good-enough compositional data augmentation",
      "author" : [ "Jacob Andreas." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556–7566, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Andreas.,? 2020",
      "shortCiteRegEx" : "Andreas.",
      "year" : 2020
    }, {
      "title" : "How to explain individual classification decisions",
      "author" : [ "David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "KlausRobert Müller." ],
      "venue" : "Journal of Machine Learning Research, 11(61):1803–1831.",
      "citeRegEx" : "Baehrens et al\\.,? 2010",
      "shortCiteRegEx" : "Baehrens et al\\.",
      "year" : 2010
    }, {
      "title" : "Unilmv2: Pseudo-masked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Songhao Piao", "Jianfeng Gao", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Bao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Local additivity based data augmentation for semi-supervised ner",
      "author" : [ "Jiaao Chen", "Zhenghui Wang", "Ran Tian", "Zichao Yang", "Diyi Yang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
      "author" : [ "Jiaao Chen", "Zichao Yang", "Diyi Yang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2147–",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Dropcluster: A structured dropout for convolutional networks",
      "author" : [ "Liyan Chen", "P. Gautier", "Sergül Aydöre." ],
      "venue" : "ArXiv, abs/2002.02997.",
      "citeRegEx" : "Chen et al\\.,? 2020c",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pre-training text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V Le", "Christopher D Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Analyzing redundancy in pretrained transformer models",
      "author" : [ "Fahim Dalvi", "Hassan Sajjad", "Nadir Durrani", "Yonatan Belinkov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4908–",
      "citeRegEx" : "Dalvi et al\\.,? 2020",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "F. Liu", "Phoebe Mulcaire", "Qiang Ning", "Sameer Singh", "Noah A. Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dropblock: A regularization method for convolutional networks",
      "author" : [ "G. Ghiasi", "Tsung-Yi Lin", "Quoc V. Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Ghiasi et al\\.,? 2018",
      "shortCiteRegEx" : "Ghiasi et al\\.",
      "year" : 2018
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Compressing BERT: Studying the effects of weight pruning on transfer learning",
      "author" : [ "Mitchell Gordon", "Kevin Duh", "Nicholas Andrews." ],
      "venue" : "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143–155, Online. Association for Com-",
      "citeRegEx" : "Gordon et al\\.,? 2020",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual visual explanations",
      "author" : [ "Yash Goyal", "Ziyan Wu", "Jan Ernst", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "ICML, pages 2376–2384.",
      "citeRegEx" : "Goyal et al\\.,? 2019",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2019
    }, {
      "title" : "Parameter-efficient transfer learning with diff pruning",
      "author" : [ "Demi Guo", "Alexander M. Rush", "Yoon Kim" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12–22, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2019",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning the difference that makes a difference with counterfactually-augmented data",
      "author" : [ "Divyansh Kaushik", "Eduard Hovy", "Zachary Lipton." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kaushik et al\\.,? 2020",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2020
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Fractalnet: Ultra-deep neural networks without residuals",
      "author" : [ "Gustav Larsson", "M. Maire", "Gregory Shakhnarovich." ],
      "venue" : "ArXiv, abs/1605.07648.",
      "citeRegEx" : "Larsson et al\\.,? 2017",
      "shortCiteRegEx" : "Larsson et al\\.",
      "year" : 2017
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial training for large neural language models",
      "author" : [ "Xiaodong Liu", "Hao Cheng", "Pengcheng He", "Weizhu Chen", "Yu Wang", "Hoifung Poon", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2004.08994.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu." ],
      "venue" : "arXiv preprint arXiv:1706.06083.",
      "citeRegEx" : "Madry et al\\.,? 2017",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2017
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntactic data augmentation increases robustness to inference heuristics",
      "author" : [ "Junghyun Min", "R. Thomas McCoy", "Dipanjan Das", "Emily Pitler", "Tal Linzen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M. Dai", "Ian J. Goodfellow." ],
      "venue" : "arXiv: Machine Learning.",
      "citeRegEx" : "Miyato et al\\.,? 2017",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2017
    }, {
      "title" : "Virtual adversarial training: a regularization method for supervised and semisupervised learning",
      "author" : [ "Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Shin Ishii." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–",
      "citeRegEx" : "Miyato et al\\.,? 2018",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Autodropout: Learning dropout patterns to regularize deep networks",
      "author" : [ "Hieu Pham", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Pham and Le.,? \\Q2021\\E",
      "shortCiteRegEx" : "Pham and Le.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "why should i trust you?\": Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial training for free",
      "author" : [ "Ali Shafahi", "Mahyar Najibi", "Mohammad Amin Ghiasi", "Zheng Xu", "John Dickerson", "Christoph Studer", "Larry S Davis", "Gavin Taylor", "Tom Goldstein" ],
      "venue" : null,
      "citeRegEx" : "Shafahi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Shafahi et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple but tough-to-beat data augmentation approach for natural language understanding and generation",
      "author" : [ "Dinghan Shen", "M. Zheng", "Y. Shen", "Yanru Qu", "W. Chen." ],
      "venue" : "ArXiv, abs/2009.13818.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15(56):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Ernie: Enhanced representation through knowledge integration",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Han Zhang", "Xin Tian", "Danxiang Zhu", "Hao Tian", "Hua Wu." ],
      "venue" : "arXiv preprint arXiv:1904.09223.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus." ],
      "venue" : "arXiv preprint arXiv:1312.6199.",
      "citeRegEx" : "Szegedy et al\\.,? 2013",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:621–633.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "L. Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "BlackboxNLP@EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Robust machine comprehension models via adversarial training",
      "author" : [ "Yicheng Wang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Wang and Bansal.,? 2018",
      "shortCiteRegEx" : "Wang and Bansal.",
      "year" : 2018
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1904.12848.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Xie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–",
      "citeRegEx" : "Yang et al\\.,? 2019a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Parameter-free sentence embedding via orthogonal basis",
      "author" : [ "Ziyi Yang", "Chenguang Zhu", "Weizhu Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Yang et al\\.,? 2019b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "On the (in)fidelity and sensitivity of explanations",
      "author" : [ "Chih-Kuan Yeh", "Cheng-Yu Hsieh", "Arun Sai Suggala", "David I. Inouye", "Pradeep Ravikumar." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Yeh et al\\.,? 2019",
      "shortCiteRegEx" : "Yeh et al\\.",
      "year" : 2019
    }, {
      "title" : "You only propagate once: Painless adversarial training using maximal principle",
      "author" : [ "Dinghuai Zhang", "Tianyuan Zhang", "Yiping Lu", "Zhanxing Zhu", "Bin Dong." ],
      "venue" : "arXiv preprint arXiv:1905.00877, 2(3).",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "PAWS: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Freelb: Enhanced adversarial training for natural language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Tom Goldstein", "Jingjing Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 48,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 21,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 40,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 8,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 25,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 3,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 18,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 35,
      "context" : "state-of-the-art performances in a wide range of natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019a; Joshi et al., 2019; Sun et al., 2019; Clark et al., 2019; Lewis et al., 2020; Bao et al., 2020; He et al., 2020; Raffel et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 276
    }, {
      "referenceID" : 17,
      "context" : "Despite the great success, due to the huge gap between the number of model parameters and that of task-specific data available, the majority of the information within the multi-layer self-attention networks is typically redundant and ineffectively utilized for downstream tasks (Guo et al., 2020; Gordon et al., 2020; Dalvi et al., 2020).",
      "startOffset" : 278,
      "endOffset" : 337
    }, {
      "referenceID" : 15,
      "context" : "Despite the great success, due to the huge gap between the number of model parameters and that of task-specific data available, the majority of the information within the multi-layer self-attention networks is typically redundant and ineffectively utilized for downstream tasks (Guo et al., 2020; Gordon et al., 2020; Dalvi et al., 2020).",
      "startOffset" : 278,
      "endOffset" : 337
    }, {
      "referenceID" : 10,
      "context" : "Despite the great success, due to the huge gap between the number of model parameters and that of task-specific data available, the majority of the information within the multi-layer self-attention networks is typically redundant and ineffectively utilized for downstream tasks (Guo et al., 2020; Gordon et al., 2020; Dalvi et al., 2020).",
      "startOffset" : 278,
      "endOffset" : 337
    }, {
      "referenceID" : 42,
      "context" : "As a result, after task-specific fine-tuning, models are very likely to overfit and make predictions based on spurious patterns (Tu et al., 2020; Kaushik et al., 2020), making them less generalizable to outof-domain distributions (Zhu et al.",
      "startOffset" : 128,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "As a result, after task-specific fine-tuning, models are very likely to overfit and make predictions based on spurious patterns (Tu et al., 2020; Kaushik et al., 2020), making them less generalizable to outof-domain distributions (Zhu et al.",
      "startOffset" : 128,
      "endOffset" : 167
    }, {
      "referenceID" : 53,
      "context" : "in the input space (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019), generating augmented data via carefully-designed rules (McCoy et al.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "in the input space (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019), generating augmented data via carefully-designed rules (McCoy et al.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "in the input space (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019), generating augmented data via carefully-designed rules (McCoy et al.",
      "startOffset" : 19,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : ", 2019), generating augmented data via carefully-designed rules (McCoy et al., 2019; Xie et al., 2020; Andreas, 2020; Shen et al., 2020), and annotating counterfactual examples (Goyal et al.",
      "startOffset" : 64,
      "endOffset" : 136
    }, {
      "referenceID" : 47,
      "context" : ", 2019), generating augmented data via carefully-designed rules (McCoy et al., 2019; Xie et al., 2020; Andreas, 2020; Shen et al., 2020), and annotating counterfactual examples (Goyal et al.",
      "startOffset" : 64,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : ", 2019), generating augmented data via carefully-designed rules (McCoy et al., 2019; Xie et al., 2020; Andreas, 2020; Shen et al., 2020), and annotating counterfactual examples (Goyal et al.",
      "startOffset" : 64,
      "endOffset" : 136
    }, {
      "referenceID" : 38,
      "context" : ", 2019), generating augmented data via carefully-designed rules (McCoy et al., 2019; Xie et al., 2020; Andreas, 2020; Shen et al., 2020), and annotating counterfactual examples (Goyal et al.",
      "startOffset" : 64,
      "endOffset" : 136
    }, {
      "referenceID" : 53,
      "context" : "Despite substantial improvements, these methods often require significant computational and memory overhead (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019; Xie et al., 2020) or human annotations (Goyal et al.",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Despite substantial improvements, these methods often require significant computational and memory overhead (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019; Xie et al., 2020) or human annotations (Goyal et al.",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 20,
      "context" : "Despite substantial improvements, these methods often require significant computational and memory overhead (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019; Xie et al., 2020) or human annotations (Goyal et al.",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 47,
      "context" : "Despite substantial improvements, these methods often require significant computational and memory overhead (Zhu et al., 2019; Liu et al., 2020; Jiang et al., 2019; Xie et al., 2020) or human annotations (Goyal et al.",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 39,
      "context" : "In this work, to alleviate the above issues, we rethink the simple and commonly-used regularization technique—dropout (Srivastava et al., 2014)— in pre-trained transformer models (Vaswani et al.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 43,
      "context" : ", 2014)— in pre-trained transformer models (Vaswani et al., 2017).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 42,
      "context" : "Although PLMs have already been equipped with the dropout regularization, they still suffer from inferior performances when it comes to out-of-distribution cases (Tu et al., 2020; Kaushik et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 201
    }, {
      "referenceID" : 22,
      "context" : "Although PLMs have already been equipped with the dropout regularization, they still suffer from inferior performances when it comes to out-of-distribution cases (Tu et al., 2020; Kaushik et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 201
    }, {
      "referenceID" : 44,
      "context" : "method on 8 natural language understanding tasks from the GLUE (Wang et al., 2018) benchmark for in-distribution evaluations, and 5 challenging datasets that cover single-sentence tasks, similarity and paraphrase tasks and inference tasks for out-ofdistribution evaluations.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 41,
      "context" : "models through applying perturbations to the input or hidden space (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) with additional forward-backward passes, which influence the model’s predictions and confidence without",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "models through applying perturbations to the input or hidden space (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) with additional forward-backward passes, which influence the model’s predictions and confidence without",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "models through applying perturbations to the input or hidden space (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) with additional forward-backward passes, which influence the model’s predictions and confidence without",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : "Adversarial-based approaches have been actively applied to various NLP tasks in order to improve models’ robustness and generalization abilities, such as sentence classification (Miyato et al., 2017), machine reading comprehension (MRC) (Wang and Bansal, 2018)",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 45,
      "context" : ", 2017), machine reading comprehension (MRC) (Wang and Bansal, 2018)",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "and natural language inference (NLI) tasks (Nie et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 37,
      "context" : "Despite its success, adversarial training often requires extensive computation overhead to calculate the perturbation directions (Shafahi et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 172
    }, {
      "referenceID" : 51,
      "context" : "Despite its success, adversarial training often requires extensive computation overhead to calculate the perturbation directions (Shafahi et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 172
    }, {
      "referenceID" : 29,
      "context" : "methods to enrich the original training set such as creating syntactically-rich examples (McCoy et al., 2019; Min et al., 2020) with specific rules, crowdsourcing counterfactual augmentation to avoid learning spurious features (Goyal et al.",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "methods to enrich the original training set such as creating syntactically-rich examples (McCoy et al., 2019; Min et al., 2020) with specific rules, crowdsourcing counterfactual augmentation to avoid learning spurious features (Goyal et al.",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : ", 2020) with specific rules, crowdsourcing counterfactual augmentation to avoid learning spurious features (Goyal et al., 2019; Kaushik et al., 2020), or combining examples in the dataset to increase compositional generalizabilities (Jia and Liang, 2016; Andreas, 2020; Chen et al.",
      "startOffset" : 107,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : ", 2020) with specific rules, crowdsourcing counterfactual augmentation to avoid learning spurious features (Goyal et al., 2019; Kaushik et al., 2020), or combining examples in the dataset to increase compositional generalizabilities (Jia and Liang, 2016; Andreas, 2020; Chen et al.",
      "startOffset" : 107,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "However, they either require careful design (McCoy et al., 2019; Andreas, 2020) to infer labels for generated data or extensive human annotations (Goyal et al.",
      "startOffset" : 44,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "However, they either require careful design (McCoy et al., 2019; Andreas, 2020) to infer labels for generated data or extensive human annotations (Goyal et al.",
      "startOffset" : 44,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : ", 2019; Andreas, 2020) to infer labels for generated data or extensive human annotations (Goyal et al., 2019; Kaushik et al., 2020),",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : ", 2019; Andreas, 2020) to infer labels for generated data or extensive human annotations (Goyal et al., 2019; Kaushik et al., 2020),",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 39,
      "context" : "Variations of dropout (Srivastava et al., 2014) have been proposed to regularize neural models by injecting noise through dropping certain information so that models do not overfit training data.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 24,
      "context" : "tures in images recently such as DropPath (Larsson et al., 2017), DropBlock (Ghiasi et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : ", 2017), DropBlock (Ghiasi et al., 2018), DropCluster (Chen et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : ", 2018), DropCluster (Chen et al., 2020c) and AutoDropout (Pham and Le, 2021).",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "HiddenCut is closely related to DropBlock (Ghiasi et al., 2018), which drops contiguous regions from a feature map.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "With multiple self-attention heads in the transformer layers, it is found that there is extensive redundant information across hi ∈ H that are linguistically related (Dalvi et al., 2020) (e.",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 23,
      "context" : "Attention-based Sampling Strategy The most direct way is to define the set of tokens to be cut by utilizing attention weights assigned to tokens in the self-attention layers (Kovaleva et al., 2019).",
      "startOffset" : 174,
      "endOffset" : 197
    }, {
      "referenceID" : 36,
      "context" : "• LIME (Ribeiro et al., 2016) defines the importance of tokens by examining the locally faithfulness where weights of tokens are assigned by classifiers trained with sentences whose",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 49,
      "context" : "• GEM (Yang et al., 2019b) utilizes orthogonal basis to calculate the novelty scores that measure the new semantic meaning in tokens, significance scores that estimate the alignment between the semantic meaning of tokens and the sentence-level meaning, and the uniqueness scores that examine the uniqueness of the semantic meaning of tokens.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "• Gradient (Baehrens et al., 2010): We define the set of tokens to be cut based on the rankings of the absolute values of gradients they received at every layer in the backward-passing.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "The whole model g(f(·)) is then trained though several objectives including general classification loss (Lori and Laug) on data-label pairs and consistency regularization (Ljs) (Miyato et al., 2017, 2018; Clark et al., 2018; Xie et al., 2019; Shen et al., 2020) across different augmentations:",
      "startOffset" : 177,
      "endOffset" : 261
    }, {
      "referenceID" : 46,
      "context" : "The whole model g(f(·)) is then trained though several objectives including general classification loss (Lori and Laug) on data-label pairs and consistency regularization (Ljs) (Miyato et al., 2017, 2018; Clark et al., 2018; Xie et al., 2019; Shen et al., 2020) across different augmentations:",
      "startOffset" : 177,
      "endOffset" : 261
    }, {
      "referenceID" : 38,
      "context" : "The whole model g(f(·)) is then trained though several objectives including general classification loss (Lori and Laug) on data-label pairs and consistency regularization (Ljs) (Miyato et al., 2017, 2018; Clark et al., 2018; Xie et al., 2019; Shen et al., 2020) across different augmentations:",
      "startOffset" : 177,
      "endOffset" : 261
    }, {
      "referenceID" : 44,
      "context" : "In-Distribution Datasets We mainly trained and evaluated our methods on the widely-used GLUE benchmark (Wang et al., 2018) which covers a",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : ", 2020) or Amazon Mechanical Turkers (Kaushik et al., 2020) to make minor edits to examples in the original",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "IMDB dataset (Maas et al., 2011) so that the sentiment labels change while the major contents keep the same.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "• Inference Tasks: Models fine-tuned from MNLI are directly evaluated on two challenging NLI sets: HANS (McCoy et al., 2019) with 30,000 test cases and Adversarial NLI (A1 dev sets) (Nie et al.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : ", 2019) with 30,000 test cases and Adversarial NLI (A1 dev sets) (Nie et al., 2020) including 1,000 test cases.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : "was created by adversarial human-and-modelin-the-loop framework (Nie et al., 2020) to create hard examples based on BERT-Large models(Devlin et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : ", 2020) to create hard examples based on BERT-Large models(Devlin et al., 2019) pre-trained on SNLI (Bowman et al.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : ", 2019) pre-trained on SNLI (Bowman et al., 2015) and MNLI.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "• ALUM (Liu et al., 2020) is the state-of-theart adversarial training method for neural language models, which regularizes fine-tuning via perturbations in the embedding space.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 38,
      "context" : "• Cutoff (Shen et al., 2020) is a recent data augmentation for natural language understanding tasks by removing information in the input",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "We compared different ways to cut hidden representations (DropBlock (Ghiasi et al., 2018) which randomly dropped spans in certain random hidden dimensions instead of the whole hidden space) and different sampling strategies for HiddenCut described in Section 3.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 36,
      "context" : "2 (including Random, LIME (Ribeiro et al., 2016), GEM (Yang et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 49,
      "context" : ", 2016), GEM (Yang et al., 2019b), Gradient (Yeh et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 50,
      "context" : ", 2019b), Gradient (Yeh et al., 2019), Attention) based on the performances on SST-2 and QNLI.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "The sentences in the first section are from IMDB with positive labels and the sentences in the second section is constructed by changing ratings or diminishing via qualifiers (Kaushik et al., 2020) to flip their corresponding labels.",
      "startOffset" : 175,
      "endOffset" : 197
    } ],
    "year" : 2021,
    "abstractText" : "Fine-tuning large pre-trained models with taskspecific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the selfattention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/ GT-SALT/HiddenCut.",
    "creator" : "LaTeX with hyperref package"
  }
}