{
  "name" : "2021.acl-long.35.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions",
    "authors" : [ "Weijia Shi", "Mandar Joshi", "Luke Zettlemoyer", "Paul G. Allen" ],
    "emails" : [ "lsz}@cs.washington.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 415–427\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n415"
    }, {
      "heading" : "1 Introduction",
      "text" : "Entity knowledge has been shown to play an important role in various applications including language modeling (Peters et al., 2019), open-domain question answering (Xu et al., 2016), and dialogue generation (Qin et al., 2019). Recent studies suggest that such entity knowledge can be provided by simple textual descriptions (Chen et al., 2019), which can be incorporated to improve downstream task performance (Nie et al., 2018; Logeswaran\n1Data and code available at github.com/swj0419/DESCGEN\net al., 2019). However, manually curating entity descriptions is labor-intensive and it is challenging to keep pace with the ever growing emergence of new entities. In this paper, we present a new dataset DESCGEN for automatically generating entity descriptions from relevant documents and mentions, which provides high quality supervision for a highly abstractive version of this task that targets early description of new entities as they emerge. For example, in Table 13, machines are required to generate a description of Carl Menger, given multiple documents mentioning him.\nDESCGEN contains 37K entity descriptions extracted from Wikipedia and Fandom2. Fandom\n2Fandom is a set of encyclopedias centered around forms of entertainment such as movies, games etc.\nallows us to capture the key challenge of generating descriptions for emerging entities that are not in Wikipedia because they are less popular or have just been introduced to the public. To obtain source documents of the entities, we collect web documents and news articles where entity mentions are linked using web hyperlinks or an entity linker. Our dataset is distantly supervised in that these heuristically collected documents are not guaranteed to contain all the facts required to generate the description—as would be seen for natural text collections describing emerging entities. We also carefully annotate a subset of 1,000 examples to support more reliable evaluation (see Table 2 for dataset statistics).\nUnlike multi-document summarization that makes the assumption that a set of documents to be summarized are written on the same topic (Zopf et al., 2016), DESCGEN only assumes that source documents mention the entity. In contrast to an existing entity summarization benchmark (Liu et al., 2018, WikiSum), DESCGEN is more abstractive and better approximates challenges faced when describing new entities. Section 4.4 provides more details on these comparisons. Overall, our documents for generating a description can cover a much wider range of topics as well as text genres, including news, blog posts, and scientific articles. For instance, the documents 1 and 2 mentioning Carl Menger in Figure 13 discuss topics on bitcoins and the Austrian School of Economics.\nFinally, we also propose a two-stage method that first extracts salient sentences relevant to the entity and then abstracts them into a description. We test a range of models to establish baseline results with both automatic and human evaluation. The best model based on BART (Lewis et al., 2020b) achieves 28.2% in the ROUGE-L F measure with a significant gap compared to the human performance 48.1%, suggesting there was great room for future improvement. In summary, our contributions include:\n• We propose a new dataset DESCGEN that includes challenging, abstractive entity summaries. Our dataset contains over 37K pairs of entity descriptions and their associated documents, along with a human-annotated subset of 1,000 pairs.\n• We conduct an extensive analysis of properties of the dataset and identify its challenges— extractive content selection from large\namounts of text and abstractive generation from it, particularly for emerging entities.\n• We present a two-stage method and benchmark various models on our dataset, aiming to facilitate future work on this dataset."
    }, {
      "heading" : "2 Related work",
      "text" : "Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descriptions. However, knowledge graphs, often mined from text corpora, are overwhelmingly incomplete on real-world entities and may not be updated in real-time (Dong et al., 2014). Therefore, we focus on generating descriptions from natural language sources such as web texts and news because they are often primary sources for entities and have better coverage of entities across multiple domains. DESCGEN is most related to WikiSum, a recent dataset for generating Wikipedia summaries from textual sources (Liu et al., 2018). WikiSum source documents primarily come from high-quality articles cited in the Wikipedia pages which makes their data more extractive (Section 4.4). In contrast, we collect our source documents heuristically using web texts and news, providing a better proxy for emerging entities where high-quality citation sources may not be available. In addition, their evaluation is conducted only on distantly supervised test data. However, our experiments demonstrate that manually annotated data allows for much better evaluation of model performance (Table 7).\nMulti-document summarization aims to condense a cluster of thematically-related documents into a short and informative summary. A wide range of multi-document summarization datasets have been built for the Document Understanding and Text Analysis Conferences (Over and Yen,\n2004; Owczarzak and Dang, 2011), news (Fabbri et al., 2019), events (Gholipour Ghalandari et al., 2020) and Wikipedia summaries (Liu et al., 2018). Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summarization (Banerjee et al., 2015; Chali et al., 2017; Nayeem et al., 2018). However, existing datasets typically are not entity focused and assume the input documents are at least loosely centered around a coherent topic or event.\nWikipedia generation Our work is also related to research on generating Wikipedia articles. For instance, Sauper and Barzilay (2009) learn to build content templates using an integer linear program to generate full articles. Similarly, Banerjee and Mitra (2016) generate Wikipedia pages by building a topic classifier to assign web retrieved contents into relevant sections. We focus on a different task – generating a short text description that can identify and best summarize an entity."
    }, {
      "heading" : "3 Dataset Collection",
      "text" : "Task definition Given a collection of documents D = {Di|i = 1...n} with mentions linked to the same entity e, the goal is to generate a description of e. For example, Table 13 shows a description of an entity (Carl Menger) and three source documents with mentions.\nDistant supervision We make use of existing knowledge bases, such as Wikipedia and Fandom, to collect entity descriptions. To obtain source documents and mentions for each entity, we use a combination of hyperlinks to Wikipedia pages and an entity linker that links entity mentions in text. Our dataset is distantly supervised in that these heuristically collected documents are not guaranteed to contain all the facts required to generate the description. To analyze the quality of distant supervision, we collect a smaller verified set of entity descriptions using human annotators. In contrast with our work, WikiSum (Liu et al., 2018) used documents cited in the Wikipedia pages or web pages returned by Google as source documents to generate Wikipedia lead sections. Because highquality citation sources constitute a substantial part of overall documents (75%), their dataset is less abstractive than DESCGEN and unsuited for emerging entities where citations are not available.\nSources We paired entity descriptions with source documents from three sources: Wikilinks, RealNews, and Fandom using distant supervision. To capture the challenge of emerging entities, we retrieve source documents that are not in Wikipedia using Wikilinks and RealNews. We also include specialized entities in Fandom that do not have Wikipedia pages. For quality control, we filter out entities for which the unigram recall of the entity description against its concatenated source documents is lower than 0.6."
    }, {
      "heading" : "3.1 Distantly supervised data collection",
      "text" : "Wikilinks Wikilinks (Singh et al., 2012) is a large dataset designed for cross-document coreference. It consists of non-Wikipedia web pages (discovered using the Google search index) containing entities that are hyperlinked to Wikipedia. For each entity, we retrieve a collection of web pages in Wikilink with the anchor text linked to it and use the lead section of target Wikipedia page as its description. We further parse the HTML texts of the web pages and extract contents as source documents.\nReal News To expand the collection of source documents, we extract entity mentions in RealNews (Zellers et al., 2019), a large corpus of news articles from Common Crawl. We first conduct a longest prefix match between the entity surface form and text tokens via trie, a prefix tree structure that supports efficient string searching. More specifically, we build a trie of entity names where each node is a word and its children indicate all possible continuations from the prefix. After retriving candidates for entity mentions, we use an offthe-shelf entity linking model (Gupta et al., 2017) to rank the candidates and add the corresponding news articles as source documents of the rank-1 candidate.\nFandom Fandom3 is a collection of encyclopedias, centered around particular subjects and themes such as movies, TV shows, and games. It contains specialized entities that require domain experts with background knowledge to make edits. Entities and their source documents can be automatically extracted by internal links. We filter out entities and only keep those without Wikipedia pages, which can be viewed as new or emerging entities. The description of the entity is extracted\n3https://www.fandom.com/\nfrom the lead section of its Fandom page. We collect data from the 32 largest Fandom Wikis."
    }, {
      "heading" : "3.2 Human-authored entity descriptions",
      "text" : "Entity descriptions extracted from Wikipedia and Fandom have been authored and edited by multiple community contributors largely independently of our source documents. We collected additional entity descriptions via Upwork,4 a freelancing platform, to better analyze how descriptions sourced from documents in our dataset contrast with those from Wikipedia and Fandom. We provided the entity and its source documents to annotators on Upwork, and asked them to write the entity descriptions. The annotators are also asked to mark sentences they used to write the description. Each entity was assigned to 2 annotators. We collected 500 entity descriptions for dev examples and 500 descriptions for test examples.\nWe control the quality of the crowdsourced descriptions by filtering annotators who produced low-quality descriptions. We ask every candidate to annotate the same 20 examples and use two criteria for narrowing down candidates: (1) missing key information in descriptions (2) unjustified information in descriptions that cannot be inferred from source documents alone. Eventually, we filtered out 4 annotators and accepted 7 qualified annotators. The total annotation cost was around $3500."
    }, {
      "heading" : "3.3 Experimental setup",
      "text" : "All 37K entity description and document pairs in the dataset are randomly split into train, development and test sets. In addition to automatically collected descriptions from Wikipedia and Fandom, we use the human-authored descriptions (Section 3.2) as verified subsets into dev and test splits. Table 3 shows basic statistics of the final dataset. We report model performance on automatically collected descriptions (distant) and human-authored descriptions (verified).\nThe next section provides a detailed analysis of the data quality, including annotator agreement and\n4https://www.upwork.com/\nother aggregate statistics."
    }, {
      "heading" : "4 Dataset Analysis",
      "text" : "An analysis of the data shows that DESCGEN contains a high proportion of emerging entities from diverse domains, and is more extractive compared to other multi-document summarization datasets."
    }, {
      "heading" : "4.1 Statistics",
      "text" : "Table 2 shows data statistics. DESCGEN contains about 37K entity descriptions from Wikipedia and Fandom. On average, each entity has nine source documents. We can see that 36% percent of entities come from Fandom, and therefore have never had a Wikipedia page written about them.\nDomain diversity Figure 1 shows that DESCGEN covers a diverse set of entity domains. For analysis, we associate entities in Wikipedia with domains (GPE, LOC, PER, ORG, EVENT, COMPANY, GROUP and MISC) by querying the DBPedia knowledge-base (Lehmann et al., 2015). Each entity in Fandom is manually categorized into 5 domains: movie, game, fiction, TV series and cartoon based on its source Wiki. An analysis of baseline performance by entity type and domain (Section 7.3) reveals a notable drop for less popular domains such as Games and Fiction, highlighting generalization challenges."
    }, {
      "heading" : "4.2 Inter-annotator agreement",
      "text" : "Each entity in the verified subset has two descriptions written by two annotators. Following previous work (Chen et al., 2015), we quantify interannotator agreement on descriptions by treating one of the descriptions as the prediction and the other as the reference to compute ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). Table 4 shows high inter-annotator agreement of 47.7 in terms of ROUGE-L.\nWe additionally measure the agreement on content selection using sentences marked by annotators. In particular, agreement is achieved when both annotators selected the exact same sentences in all source documents for an entity. Cohen’s Kappa is 0.38, which indicates high agreement (Brennan and Prediger, 1981) considering the strict criterion of reaching agreement."
    }, {
      "heading" : "4.3 Comparison between human-authored and Wikipedia/Fandom descriptions",
      "text" : "To understand how human-authored descriptions differ with Wikipedia and Fandom descriptions in terms of content and style, we compare them using automatic metrics (ROUGE) and manual evaluation.\nROUGE Table 5 shows the averaged ROUGE scores of human-authored descriptions against Wikipedia and Fandom descriptions. Humanauthored descriptions have higher word overlap with Wikipedia descriptions than with Fandom descriptions.\nPairwise comparison Can humans distinguish between Wikipedia/Fandom and human-authored descriptions? We have two human assessors evaluate 50 randomly sampled pairs of human-authored and Wikipedia/Fandom descriptions in a blind pairwise comparison, and ask them to classify descriptions into two categories: human-authored or Wikipedia/Fandom. The classification accuracy in Wikipedia and Fandom is 64.4% and 61.1% respectively and the inter-annotator agreement is 0.67 in Cohen’s Kappa. The relatively low classification accuracy suggests that there is no substantial\nquality and style difference in human-authored and Wikipedia/Fandom descriptions.\nQuality analysis of distant supervision We are interested in understanding if automatically gathered documents can provide enough signals for writing the entity descriptions. To study the quality of distant supervision, we manually analyze 40 human-authored descriptions that have low ngrams overlap with Wikipedia/Fandom descriptions, in terms of paraphrasing (does the humanauthored description express the same meaning but use different words?), missing information (does the human-authored description miss any information in Wikipedia/Fandom description?) and extra details (does the human-authored description contain extra details not included in the Wikpedia/Fandom description?). We use Wikipedia and Fandom descriptions as the ground truth and classify each human-authored description into one or more categories. The results are shown in Table 6. We find that the difference between the two sources of descriptions are mainly caused by paraphrasing and missing information. This suggests that even for entities that have very different human-authored and extracted descriptions, most of the information in the Wikipedia/Fandom descriptions is present in the documents."
    }, {
      "heading" : "4.4 Extraction vs abstraction",
      "text" : "Generating entity descriptions involves extracting essential information about the entity and condensing them into a short description. To measure how much DESCGEN requires paraphasing and compressing, we quantify the extractive nature of our dataset by the measuring extractive fragment coverage and density defined in Grusky et al. (2018). Extractive fragment coverage computes the percentage of words in summary that appear in source documents:\nCoverage(A,S) = 1 |S| ∑ f∈F |f |\nwhere A is a concatenation of the source documents, S is the description and F is the set of shared token sequences in A and S. Likewise, extractive fragment density is related to the average length of shared token sequences. For example, an entity description with high coverage and low density shares many individual words with source documents but almost no long phrases.\nDensity(A,S) = 1 |S| ∑ f∈F |f |2\nWe compare our dataset with several multidocument summarization datasets, including CNN / Daily Mail, Multi-News (Fabbri et al., 2019) and WikiSum (Liu et al., 2018). Figure 2 presents the density and coverage distribution. The density of Multi-News, CNN / Daily Mail and WikiSum are high, showing that there is much copying of long sequences with respect to source documents. DESCGEN shows high coverage but low density, suggesting it is not common to copy long sequences and the data overall is much more abstractive."
    }, {
      "heading" : "5 Baselines",
      "text" : "In this section, we introduce several new baseline methods, building on state-of-the-art pre-trained models. The input documents can be long (Section 8), making it computationally infeasible to train end-to-end models. We instead introduce a pipelined approach to generate an entity description in two stages. In the first extractive stage, a selector is used to identify representative sentences relevant to the entity from multiple source documents. In the second abstractive stage, a neural generation\nmodel is used to fuse the selected sentences to a description of the entity. We compare a number of different approaches for each stage, as summarized in the subsections below."
    }, {
      "heading" : "5.1 Extractive stage",
      "text" : "Trivial concatenates all sentences that mention the entity, along with one sentence before and after each. The content is truncated to the first 1,000 tokens to fit the token limit of models in the abstractive stage.\nCheating ranks sentences according to their unigram recall against the description and selects the top 15 sentences. This heuristic demonstrates the effect of extraction on final performance.\nBERT (Devlin et al., 2019) with a classifier uses a linear layer stacked on top of the BERT outputs and predict whether a sentence should be selected. The model is trained on our training dataset in which sentences are labeled by the cheating method."
    }, {
      "heading" : "5.2 Abstractive stage",
      "text" : "We compare three pre-trained language generation models, including BART (Lewis et al., 2020b), T5 (Raffel et al., 2019) and MARGE (Lewis et al., 2020a) to generate abstractive entity descriptions. We fine-tuned these models on our training dataset in a sequence-to-sequence fashion.\nT5 is a text-to-text transformer pre-trained on a multi-task mixture of unsupervised and supervised tasks. We consider models of two sizes: base and large containing 220M and 770M parameters respectively. We use the Hugging Face version.5\nBART introduces a denoising autoencoder combining a bidirectional encoder and auto-regressive decoder. It is trained by reconstructing text corrupted with a noising function. We consider the base model with 139M parameters.\nMARGE is a multi-lingual sequence-to-sequence model trained by reconstructing target documents retrieving paraphrased documents in other languages. It has around 960M parameters."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Evaluation metrics",
      "text" : "Following other summarization tasks, we evaluate the quality of generated descriptions by ROUGE\n5https://github.com/huggingface/transformers\nF1-score (Lin, 2004), which measures the overlap of unigram (R-1), bigram (R-2), and the longest matching sequence of words (R-L). In addition, we evaluate content selection by unigram and bigram recall to assess the importance of the extractive stage. Lastly, in addition to automatic evaluation, we also conduct human evaluation for nonredudancy, fluency, informativeness, and accuracy."
    }, {
      "heading" : "6.2 Experimental results",
      "text" : "Automatic evaluation In Table 8, we report the experimental results in the extractive stage. We observe that BERT consistently outperforms the unsupervised method Trivial, suggesting that training a model to predict sentence relevance can bring in immediate improvement in content selection. Meanwhile, the performance of BERT still lags behind the upper bound defined by Cheating by 1.7-7.3% in unigram.\nTable 7 presents ROUGE scores of various baselines in the abstractive stage. T5-large and BART\nshow similar performance and outperform other models for both distant supervision and verified subsets, by a small margin. Increasing model size from T5-base (220M) to T5-large (770M) parameters leads to a relatively large performance gain. The human baseline is superior to all the models and maintains a R-L score over 33 in distant supervision and 48 in the verified subset. The large gap between the human baseline and the bestperforming model shows there is much room for future work.\nManual evaluation We present two human assessors with source documents and descriptions generated from different abstractive models and asked them to rate descriptions in terms of nonredundancy (does the description avoid repeating information?), fluency (Is the description wellformed and gramatically correct?), informativeness (does the description capture the salient information about the entity?) and faithfulness (Is the description faithful to the source text?). We compared BART, T5-Large, and T5-Base. For each model, we selected 100 descriptions and showed outputs of models to assessors side by side without revealing which model generates them. The score for each description was averaged between two assessors. As can be seen from Table 9, BART shows strong performance on all dimensions, except for fluency. Overall, all three models can generate fluent descriptions (high fluency) but struggle with producing accurate statements (low faithfulness). In most cases of low faithfulness, we observe that the model directly copies words from the input that are not relevant to the entity as part of the description or synthesize information that are not directly inferable from the input."
    }, {
      "heading" : "7 Analysis",
      "text" : "In this section, we perform qualitative and quantitative analysis of baseline results to better understand strengths and weaknesses of models, and hypothesize avenues for future work."
    }, {
      "heading" : "7.1 Case study",
      "text" : "A qualitative analysis of model predictions suggests that these models tend not to generate novel words in the description, and mostly copy words from the original text. The entity-centric nature of DESCGEN makes extractive content selection difficult as evidenced by the gap between BERT extraction and the Cheating model (Section 6.2). For example, Table 10 shows the model-generated entity descriptions for Carl Menger using source documents from Table 13. BART, one of the best performing baselines, generates a description that has highest overlap with the Wikipedia description, but it still misses some important facts. T5-Base and MARGE confuse Carl Menger and his son, and incorrectly include information that does not describe the target entity."
    }, {
      "heading" : "7.2 Entity knowledge in pre-trained models",
      "text" : "BART, T5, and MARGE are language models pretrained on text corpora including Wikipedia and Common Crawl. The parameters of the models appear to contain substantial linguistic and factual information (Petroni et al., 2019; Peters et al., 2018). In particular, we wonder if entity-related knowledge is captured in the pretraining stage and investigate the following questions: (a) Can the model memorize entity descriptions in pretraining stage? (b) Does the memorized knowledge improve model performance on generating entity descriptions?\nTo investigate the questions, we test the model’s ability to write a description given only the entity name instead of source documents. We train the model on our training dataset to adapt to the style of Wikipedia in a similar way. The results are shown in Table 11. Considering the name-only baselines, we can see that all of them perform worse on Fandom entities than Wikipedia entities. However, the regular baselines perform similarly on Fandom and Wikipedia. This result suggests that facts about entities learnt in pretraining stage have much less influence on model performance when source documents are provided."
    }, {
      "heading" : "7.3 Entity type",
      "text" : "To understand how the performance of the models varies with different types of entities, we report the performance breakdown for different entity types in Table 12. Among domains in Wikipedia, our model obtains low scores on group and company, suggest-\ning that they are more challenging than other domains. In Fandom, entities from the game domain prove to be most difficult.\nIn summary, our analysis suggests there is room for improvement in extractive content selection and abstractive generation, particularly for new and emerging entities from less popular domains."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work, we introduce DESCGEN, a new dataset for generating entity descriptions from mentions. DESCGEN contains 37K pairs of entity descriptions from Wikipedia and Fandom, and 481K automatically gathered source documents based on distant supervision. We also present a clean human-authored subset of 1,000 pairs for test. We show that, as compared to existing benchmarks, DESCGEN requires more abstractive summaries, which we argue better approximate the challenge of describing emerging entities. We also show that the performance of state-of-art models is far from human levels, suggesting that our task remains a significant challenge with room for improvement. Our study points to an interesting research direction on modeling entity knowledge from contexts. We hope it will facilitate future work on incorporating entity knowledge into downstream tasks and generating descriptions for emerging entities."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by the ARO (AROW911NF-16-1-0121) and the NSF (IIS1562364). The authors would like to thank Ari Holtzman, Bhargavi Paranjape, Elizabeth Clark, Terra Blevins and anonymous reviewers for helpful comments."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Experimental Details All the abstractive models are initialized from the pretrained models. The BART, T5-base and T5-large are adopted by the huggingface framework (Wolf et al., 2020). The MARGE model is adopted by the official authors (Lewis et al., 2020a). We apply the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.999, = 1e − 08. The learning rate is selected from {1e-3, 0.5e-3, 1e-4, 0.5e-4, 1e-5, 0.5e-5}. The best learning rate for BART, T5-base, T5-large and MARGE is 1e-5, 1e-5, 0.5e-5,0.5e-4. We use beam searching with beam-size 5 as decoding algorithm, which is selected from {5, 10, 15, 20}. We use the batch size of 5 for all models due to memory limit.\nA.2 More examples See next page."
    } ],
    "references" : [ {
      "title" : "Wikiwrite: Generating wikipedia articles automatically",
      "author" : [ "Siddhartha Banerjee", "Prasenjit Mitra." ],
      "venue" : "IJCAI, pages 2740–2746.",
      "citeRegEx" : "Banerjee and Mitra.,? 2016",
      "shortCiteRegEx" : "Banerjee and Mitra.",
      "year" : 2016
    }, {
      "title" : "Multi-document abstractive summarization using ilp based multi-sentence compression",
      "author" : [ "Siddhartha Banerjee", "Prasenjit Mitra", "Kazunari Sugiyama." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Banerjee et al\\.,? 2015",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2015
    }, {
      "title" : "Coefficient kappa: Some uses, misuses, and alternatives",
      "author" : [ "Robert L Brennan", "Dale J Prediger." ],
      "venue" : "Educational and psychological measurement, 41(3):687–699.",
      "citeRegEx" : "Brennan and Prediger.,? 1981",
      "shortCiteRegEx" : "Brennan and Prediger.",
      "year" : 1981
    }, {
      "title" : "Towards abstractive multi-document summarization using submodular function-based framework, sentence compression and merging",
      "author" : [ "Yllias Chali", "Moin Tanvee", "Mir Tafseer Nayeem." ],
      "venue" : "Proceedings of the Eighth International Joint Con-",
      "citeRegEx" : "Chali et al\\.,? 2017",
      "shortCiteRegEx" : "Chali et al\\.",
      "year" : 2017
    }, {
      "title" : "Enteval: A holistic evaluation benchmark for entity representations",
      "author" : [ "Mingda Chen", "Zewei Chu", "Yang Chen", "Karl Stratos", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C Lawrence Zitnick." ],
      "venue" : "arXiv e-prints, pages arXiv–1504.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "ENTDESC: Entity description generation by exploring knowledge graph",
      "author" : [ "Liying Cheng", "Dekun Wu", "Lidong Bing", "Yan Zhang", "Zhanming Jie", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the ninth workshop on statistical machine translation, pages 376–380.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang." ],
      "venue" : "Proceedings of the 20th ACM",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander R Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir R Radev." ],
      "venue" : "arXiv preprint arXiv:1906.01749.",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "A large-scale multi-document summarization dataset from the Wikipedia current events portal",
      "author" : [ "Demian Gholipour Ghalandari", "Chris Hokamp", "Nghia The Pham", "John Glover", "Georgiana Ifrim" ],
      "venue" : null,
      "citeRegEx" : "Ghalandari et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ghalandari et al\\.",
      "year" : 2020
    }, {
      "title" : "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
      "author" : [ "Max Grusky", "Mor Naaman", "Yoav Artzi" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "Grusky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grusky et al\\.",
      "year" : 2018
    }, {
      "title" : "Entity Linking via Joint Encoding of Types, Descriptions, and Context",
      "author" : [ "Nitish Gupta", "Sameer Singh", "Dan Roth." ],
      "venue" : "Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Gupta et al\\.,? 2017",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia",
      "author" : [ "Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick Van Kleef", "Sören Auer" ],
      "venue" : null,
      "citeRegEx" : "Lehmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Pre-training via paraphrasing",
      "author" : [ "Mike Lewis", "Marjan Ghazvininejad", "Gargi Ghosh", "Armen Aghajanyan", "Sida Wang", "Luke Zettlemoyer." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Lewis et al\\.,? 2020a",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "arXiv preprint arXiv:1801.10198.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot entity linking by reading entity descriptions",
      "author" : [ "Lajanugen Logeswaran", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova", "Jacob Devlin", "Honglak Lee." ],
      "venue" : "arXiv preprint arXiv:1906.07348.",
      "citeRegEx" : "Logeswaran et al\\.,? 2019",
      "shortCiteRegEx" : "Logeswaran et al\\.",
      "year" : 2019
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "Abstractive unsupervised multidocument summarization using paraphrastic sentence fusion",
      "author" : [ "Mir Tafseer Nayeem", "Tanvir Ahmed Fuad", "Yllias Chali." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics,",
      "citeRegEx" : "Nayeem et al\\.,? 2018",
      "shortCiteRegEx" : "Nayeem et al\\.",
      "year" : 2018
    }, {
      "title" : "Mention and entity description co-attention for entity disambiguation",
      "author" : [ "Feng Nie", "Yunbo Cao", "Jinpeng Wang", "Chin-Yew Lin", "Rong Pan." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Nie et al\\.,? 2018",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2018
    }, {
      "title" : "The E2E dataset: New challenges for endto-end generation",
      "author" : [ "Jekaterina Novikova", "Ondřej Dušek", "Verena Rieser." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206, Saarbrücken, Germany. Association",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "An introduction to duc-2004",
      "author" : [ "Paul Over", "James Yen" ],
      "venue" : null,
      "citeRegEx" : "Over and Yen.,? \\Q2004\\E",
      "shortCiteRegEx" : "Over and Yen.",
      "year" : 2004
    }, {
      "title" : "Overview of the tac 2011 summarization track: Guided task and aesop task",
      "author" : [ "Karolina Owczarzak", "Hoa Trang Dang." ],
      "venue" : "Proceedings of the Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November.",
      "citeRegEx" : "Owczarzak and Dang.,? 2011",
      "shortCiteRegEx" : "Owczarzak and Dang.",
      "year" : 2011
    }, {
      "title" : "Dissecting contextual word embeddings: Architecture and representation",
      "author" : [ "Matthew Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowledge enhanced contextual word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Robert Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A Smith." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity-consistent end-to-end task-oriented dialogue system with KB retriever",
      "author" : [ "Libo Qin", "Yijia Liu", "Wanxiang Che", "Haoyang Wen", "Yangming Li", "Ting Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Qin et al\\.,? 2019",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically generating wikipedia articles: A structure-aware approach",
      "author" : [ "Christina Joan Sauper", "Regina Barzilay." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Sauper and Barzilay.,? 2009",
      "shortCiteRegEx" : "Sauper and Barzilay.",
      "year" : 2009
    }, {
      "title" : "Wikilinks: A large-scale cross-document coreference corpus labeled via links to wikipedia",
      "author" : [ "Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum." ],
      "venue" : "University of Massachusetts, Amherst, Tech. Rep. UM-CS-2012, 15.",
      "citeRegEx" : "Singh et al\\.,? 2012",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "Extractive multi-document summarization using multilayer networks",
      "author" : [ "Jorge V Tohalino", "Diego R Amancio." ],
      "venue" : "Physica A: Statistical Mechanics and its Applications, 503:526–539.",
      "citeRegEx" : "Tohalino and Amancio.,? 2018",
      "shortCiteRegEx" : "Tohalino and Amancio.",
      "year" : 2018
    }, {
      "title" : "Sentence generation for entity description with content-plan attention",
      "author" : [ "Bayu Trisedya", "Jianzhong Qi", "Rui Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9057–9064.",
      "citeRegEx" : "Trisedya et al\\.,? 2020",
      "shortCiteRegEx" : "Trisedya et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Question answering on Freebase via relation extraction and textual evidence",
      "author" : [ "Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "Defending against neural fake news",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9054–9065.",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "The next step for multi-document summarization: A heterogeneous multi-genre corpus built with a novel construction approach",
      "author" : [ "Markus Zopf", "Maxime Peyrard", "Judith EckleKohler." ],
      "venue" : "Proceedings of COLING 2016, the 26th International",
      "citeRegEx" : "Zopf et al\\.,? 2016",
      "shortCiteRegEx" : "Zopf et al\\.",
      "year" : 2016
    }, {
      "title" : "Bagman’s brother, Otto Bagman, by smoothing over a problem involving a lawn mower enchanted with magical powers. As thanks, Ludo got Arthur prime tickets to the 1994 Quidditch World Cup... Fandom Description Otto Bagman was the brother of Ludovic Bagman. He once had a problem with a magical lawn mower, a Muggle artifact",
      "author" : [ "helped Ludovic" ],
      "venue" : "Arthur Weasley helped him out with the problem,",
      "citeRegEx" : "Ludovic,? \\Q1994\\E",
      "shortCiteRegEx" : "Ludovic",
      "year" : 1994
    }, {
      "title" : "T5-base Otto Bagman was an English footballer who played for the Wimbourne Wasps and the English National Quidditch team. He also played dirty when gambling and betting",
      "author" : [ "Otto Bagman" ],
      "venue" : null,
      "citeRegEx" : "Bagman,? \\Q1994\\E",
      "shortCiteRegEx" : "Bagman",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Entity knowledge has been shown to play an important role in various applications including language modeling (Peters et al., 2019), open-domain question answering (Xu et al.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 37,
      "context" : ", 2019), open-domain question answering (Xu et al., 2016), and dialogue generation (Qin et al.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "Recent studies suggest that such entity knowledge can be provided by simple textual descriptions (Chen et al., 2019), which can be incorporated to improve downstream task performance (Nie et al.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 40,
      "context" : "be summarized are written on the same topic (Zopf et al., 2016), DESCGEN only assumes that source documents mention the entity.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descrip-",
      "startOffset" : 71,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descrip-",
      "startOffset" : 71,
      "endOffset" : 137
    }, {
      "referenceID" : 35,
      "context" : "Existing Entity Description Generation Task and Dataset Previous works (Novikova et al., 2017; Cheng et al., 2020; Trisedya et al., 2020) mainly take as input some structured data such as knowledge graphs to generate entity descrip-",
      "startOffset" : 71,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "However, knowledge graphs, often mined from text corpora, are overwhelmingly incomplete on real-world entities and may not be updated in real-time (Dong et al., 2014).",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "DESCGEN is most related to WikiSum, a recent dataset for generating Wikipedia summaries from textual sources (Liu et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : "Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summariza-",
      "startOffset" : 40,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summariza-",
      "startOffset" : 40,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : "Recent work has studied both extractive (Yasunaga et al., 2017; Nallapati et al., 2017; Tohalino and Amancio, 2018) and abstractive summariza-",
      "startOffset" : 40,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "In contrast with our work, WikiSum (Liu et al., 2018) used documents cited in the Wikipedia pages or web pages returned by Google as source documents to generate Wikipedia lead sections.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "Wikilinks Wikilinks (Singh et al., 2012) is a large dataset designed for cross-document coreference.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 39,
      "context" : "documents, we extract entity mentions in RealNews (Zellers et al., 2019), a large corpus of news articles from Common Crawl.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "After retriving candidates for entity mentions, we use an offthe-shelf entity linking model (Gupta et al., 2017)",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "For analysis, we associate entities in Wikipedia with domains (GPE, LOC, PER, ORG, EVENT, COMPANY, GROUP and MISC) by querying the DBPedia knowledge-base (Lehmann et al., 2015).",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "ous work (Chen et al., 2015), we quantify interannotator agreement on descriptions by treating one of the descriptions as the prediction and the other as the reference to compute ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014).",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : ", 2015), we quantify interannotator agreement on descriptions by treating one of the descriptions as the prediction and the other as the reference to compute ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014).",
      "startOffset" : 164,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : ", 2015), we quantify interannotator agreement on descriptions by treating one of the descriptions as the prediction and the other as the reference to compute ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014).",
      "startOffset" : 187,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "38, which indicates high agreement (Brennan and Prediger, 1981) considering the strict criterion of reaching agreement.",
      "startOffset" : 35,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "We compare our dataset with several multidocument summarization datasets, including CNN / Daily Mail, Multi-News (Fabbri et al., 2019) and",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "BERT (Devlin et al., 2019) with a classifier uses a linear layer stacked on top of the BERT outputs and",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : ", 2019) and MARGE (Lewis et al., 2020a) to generate abstractive entity descriptions.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "F1-score (Lin, 2004), which measures the overlap of unigram (R-1), bigram (R-2), and the longest",
      "startOffset" : 9,
      "endOffset" : 20
    }, {
      "referenceID" : 29,
      "context" : "The parameters of the models appear to contain substantial linguistic and factual information (Petroni et al., 2019; Peters et al., 2018).",
      "startOffset" : 94,
      "endOffset" : 137
    }, {
      "referenceID" : 27,
      "context" : "The parameters of the models appear to contain substantial linguistic and factual information (Petroni et al., 2019; Peters et al., 2018).",
      "startOffset" : 94,
      "endOffset" : 137
    } ],
    "year" : 2021,
    "abstractText" : "Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description. DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks to the Wikipedia and Fandom entity pages, which together provide high quality distant supervision. The resulting summaries are more abstractive than those found in existing datasets, and provide a better proxy for the challenge of describing new and emerging entities. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-the-art models and human performance, suggesting that the data will support significant future work.1",
    "creator" : "LaTeX with hyperref"
  }
}