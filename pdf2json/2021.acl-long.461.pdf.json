{
  "name" : "2021.acl-long.461.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews",
    "authors" : [ "Junhao Liu", "Zhen Hai", "Min Yang", "Lidong Bing" ],
    "emails" : [ "min.yang}@siat.ac.cn", "l.bing}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5927–5936\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5927"
    }, {
      "heading" : "1 Introduction",
      "text" : "Product reviews are essential information sources for consumers to acquire useful information and\n∗This work was conducted when Junhao Liu was an intern at DAMO Academy, Alibaba Group.\n†Min Yang is the corresponding author.\nmake purchase decisions. Many e-commerce sites such as Amazon.com offer reviewing functions that encourage consumers to share their opinions and experiences. However, the user-generated reviews vary a lot in their qualities, and we are continuously bombarded with ever-growing, noise information. Therefore, it is critical to examine the quality of reviews and present consumers with useful reviews.\nMotivated by the demand of gleaning insights from such valuable data, review helpfulness prediction has gained increasing interest from both academia and industry communities. Earlier review helpfulness prediction methods rely on a wide range of handcrafted features, such as semantic features (Yang et al., 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu et al., 2017), to train a classifier. The success of these methods generally relies heavily on feature engineering which is labor-intensive and highlights the weakness of conventional machine learning methods. In recent years, deep neural networks such as CNN (Chen et al., 2018, 2019) and LSTM (Fan et al., 2019) have become dominant in the literature due to their powerful performance for helpfulness prediction by learning text representation automatically. Note that these existing works on review helpfulness prediction mainly focus on the pure textual data.\nAs multimodal data become increasingly popular in online reviews, Multimodal Review Analysis (MRA) has become a valuable research direction. In this paper, we propose the Multimodal Review Helpfulness Prediction (MRHP) task which aims at exploring multimodal clues that often convey comprehensive information for review helpfulness prediction. In particular, for the multimodal reviews, the helpfulness of reviews is not only determined by the textual content but rather the combined expression (e.g., coherence) of multimodality data (e.g., texts and images). Taking the reviews in Table 1\nas an example, we cannot identify the helpfulness score of Review 3 solely from the text content until reading the attached images that are totally irrelevant to the product “Teflon Pans”. The reviews that have incoherent text content and images tend to be unhelpful, even be malicious reviews. In contrast, a helpful review (e.g., Review 2) should contain not only concise and informative textual content but also coherent text content and images.\nIn this paper, we explore both text and images in product reviews to improve the performance of review helpfulness prediction. We design a novel Multi-perspective Coherent Reasoning method (denoted as MCR) to tackle the MRHP task. Concretely, we propose a product-review coherent reasoning module to effectively capture the intra- and inter-modal coherence between the target product and the review. In addition, we also devise an intrareview coherent reasoning module to capture the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. Finally, we formulate the helpfulness prediction as a ranking problem and employ a pairwise ranking objective to optimize the whole model.\nWe summarize our main contributions as follows. (1) To the best of our knowledge, this is the first attempt to explore both text and images in reviews for helpfulness prediction, which is defined as the MRHP task. (2) We propose a multi-perspective coherent reasoning method for the MRHP task to conduct joint reasoning over texts and images from both the product and the review, and aggregate the signals to predict the helpfulness of multimodal reviews. (3) We present two newly-collected multimodal review datasets for helpfulness prediction of multimodal reviews. To facilitate research in this area, we will release the datasets and source code proposed in this paper, which would push forward the research in this field. (4) Extensive experiments on two collected datasets demonstrate that our MCR method significantly outperforms other methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "Most conventional approaches on review helpfulness prediction focus solely on the text of reviews, which can be generally divided into two categories based on the way of extracting predictive features: machine learning based methods with hand-crafted features (Kim et al., 2006; Krishnamoorthy, 2015)\nand deep learning based methods (Chen et al., 2019; Fan et al., 2018; Chen et al., 2018). The machine learning based methods employ domain-specific knowledge to extract a variety of hand-crafted features, such as structure features (Kim et al., 2006), lexical features (Krishnamoorthy, 2015), emotional features (Martin and Pu, 2014), and argument features (Liu et al., 2017), from the textural reviews, which are then fed into conventional classifiers such as SVM (Kim et al., 2006) for helpfulness prediction. These methods rely heavily on feature engineering, which is time-consuming and labor intensive. Motivated by the remarkable progress of deep neural networks, several recent studies attempt to automatically learn deep features from textual reviews with deep neural networks. Chen et al. (2019) employs a CNN model to capture the\nmulti-granularity (character-level, word-level, and topic-level) features for helpfulness prediction. Fan et al. (2018) proposes a multi-task neural learning model to identify helpful reviews, in which the primary task is helpfulness prediction and the auxiliary task is star rating prediction.\nSubsequently, several works have been proposed to explore not only the reviews but also the users and target products for helpfulness prediction of reviews. Fan et al. (2019) argued that the helpfulness of a review should be aware of the meta-data (e.g., title, brand, category, description) of the target product besides the textual content of the review itself. To this end, a deep neural architecture was proposed to capture the intrinsic relationship between the meta-data of a product and its numerous reviews. Qu et al. (2020) proposed to leverage the reviews, the users, and items together for helpfulness prediction of reviews and devised a categoryaware graph neural networks with one shared and many item-specific graph convolutions to learn the common features and each item’s specific criterion for helpfulness prediction.\nDifferent from the above methods, we take full advantage of the text content and images of reviews by proposing a novel hierarchical coherent reasoning method to learn the coherence between text content and images in a review and the coherence between the target product and the review."
    }, {
      "heading" : "3 Methodology",
      "text" : "The overall architecture of our MCR method is illustrated in Figure 1. Our multi-perspective coherent reasoning consists of two perspectives of coherence: (i) the intra- and inter-modal coherence between a review and the target product and (ii) the intra-review coherence between the text content and images in the review. In the following sections, we will provide the problem definition of review helpfulness prediction and introduce each component of our MCR model in detail."
    }, {
      "heading" : "3.1 Problem Definition",
      "text" : "As mentioned by Diaz and Ng (2018), we formulate the multimodal review helpfulness prediction problem as a ranking task. Specifically, given a product item Pi consisting of product related information pi and an associated review setRi = {ri,1, · · · , ri,N}, where N is the number of reviews for pi. Each review has a scalar label si,j ∈ {0, · · · , S} indicating the helpfulness score of the review ri,j . The\nground-truth ranking of Ri is the descending sort order determined by the helpfulness scores. The goal of review helpfulness prediction is to predict helpfulness scores for Ri which can rank the set of reviews Ri into the ground-truth result. The predicted helpfulness score ŝi,j for the review ri,j is defined as follows:\nŝi,j = f(pi, ri,j), (1)\nwhere f is the helpfulness prediction function taking a product-review pair 〈pi, ri,j〉 as input. In multimodal review helpfulness prediction task, the product pi consists of associated description Tp and pictures Ip, while review ri,j consists of userposted text Tr and images Ir."
    }, {
      "heading" : "3.2 Feature Representation",
      "text" : "Given a text (Tp or Tr) consisting of lT text tokens {w1, · · · , wlT } and an image set (Ip or Ir), we adopt a convolutional neural network to learn the contextualized text representation. Meanwhile, we use a self-attention mechanism on image region features to obtain the image representations. To prevent conceptual confusion, we use the subscripts p and r to indicate variables that are related to the product and the review, respectively.\nText Representation Inspired by the great success of convolutional neural network (CNN) in natural language processing (Kim, 2014; Dai et al., 2018), we also apply CNN to learn the text representation. First, we convert each token wi in a review into an embedding vector wi ∈ Rd via an embedding layer. Then, we pass the learned word embeddings to a one-dimensional CNN so as to extract multi-gram representations. Specifically, the k-gram CNN transforms the token embedding vectors wi into k-gram representations Hk:\nHk = CNNk({w1, · · · ,wlT }), (2)\nwhere k ∈ {1, · · · , kmax} represents the kernel size. kmax represents the maximum kernel size. Hk ∈ RlT×dT is the k-gram representation. All the k-gram representations are stacked to form the final text representation, denoted as H = [H1, · · · ,Hkmax ]. Here, we use Hp and Hr to represent the representations of text content of the product and the review, respectively.\nImage Representation We use pre-trained Faster R-CNN to extract the region of interest (RoI) pooling features (Anderson et al., 2018) for the\nreview and product images, obtaining the finegrained object-aware representations. All the RoI features vi extracted from image sets Ip and Ir are then encoded by a self-attention module (Vaswani et al., 2017), resulting in a dI -dimensional semantic space with non-local understanding:\nV = SelfAttn({v1, · · · ,vlI}), (3)\nwhere V ∈ RlI×dI represents the visual semantic representation and lI is the number of extracted RoI features. Here, we use Vp and Vr to represent the product and review image features, respectively."
    }, {
      "heading" : "3.3 Product-Review Coherent Reasoning",
      "text" : "The helpfulness of a review should be fully aware of the product besides the review itself. In this paper, we propose a product-review coherent reasoning module to effectively capture the intra- and inter-modal coherence between the target product and the review.\nIntra-modal Coherence We propose the intramodal coherent reasoning to measure two kinds of intra-modal coherence: (i) the semantic alignments between the product text and the review text, and (ii) the semantic alignments between product images and review images. The cosine similarity is utilized to derive the intra-modal coherence matrix. For text representations Hip and H j r, we compute the corresponding coherence matrix as follow:\nSHi,j = cosine(H i p,H j r),\n∀i, j ∈ {1, . . . , kmax}, (4)\nwhere SHi,j has the shape of R lTp×lTr , lTp and lTr indicate the text length of the product and the review, respectively. All the coherence matrices are stacked to form the whole coherence features SH. Without loss of generality, we also compute the image coherence matrix between Vp and Vr via cosine similarity. In this way, we obtain the image coherence matrix SV with the shape of RlIp×lIr , where lIp and lIr indicate the number of RoI features of the product and review images, respectively.\nSubsequently, the text and image coherence matrix (i.e., SH and SV) are passed to a CNN, and the top-K values in each feature map are selected as the pooling features:\nointraM = TopK(CNN([S H,SV])), (5)\nwhere ointraM ∈ RK∗M is the intra-modal coherent reasoning features. M is the number of filters used in the CNN module.\nInter-modal Coherence The intra-modal coherence ignores the cross-modal relationship between the product and the review. In order to mitigate this problem, we propose the inter-modal coherent reasoning to capture two kinds of inter-modal coherence: (i) the coherence between the review text and the product images, and (ii) the coherence between the review images and the product text. Since the text representation H and the image representation V lie in two different semantic spaces, we first project them into a dc-dimensional common latent space by:\nFH = Tanh(W1H+ b1), (6)\nFV = Tanh(W2V + b2), (7)\nwhere FH ∈ RlT×dc and FV ∈ RlI×dc are text and image representations in the common latent space, respectively.\nTaking the coherence of review image and product text as an example, our inter-modal coherent reasoning aligns the features in review images FVr based on the product text FHp . Specifically, we define the review images as the query Qr = WQFVr and the product text as the key Kp = WKFHp , where WQ,WK ∈ Rdc×dc are learnable parameter matrices. Hence, the inter-modal relationship IVr can be formulated as follows:\nMr = softmax(QrK T p ), (8)\nIVr = F V r +MrF H p , (9)\nwhere Mr ∈ RlI×lT is the query attended mask. A mean-pooling operation is then conducted to get an aggregated vector of the inter-modal coherence features between the review images and the product text: ĨVr :\nĨVr = Mean(I V r ) ∈ Rdc . (10)\nFollowing Equations 8-10, the same procedure is employed to learn the coherence features ĨHr between the review text and the product images. Finally, we concatenate ĨVr and Ĩ H r to form the final inter-modal coherence features ointerM :\nointerM = [Ĩ V r , Ĩ H r ], (11)\nwhere [·] denotes the concatenate operation."
    }, {
      "heading" : "3.4 Intra-review Coherent Reasoning",
      "text" : "Generally, consumers usually express their opinions in textual reviews and post images as a kind of evidence to support their opinions. To capture the coherence between the text content and images of the review, we should grasp sufficient relational and logical information between them. To this end, we devise an intra-review coherent reasoning module to learn the coherence between the text content and images of the review, which performs message propagation among semantic nodes of a review evidence graph and then obtains an intra-review coherence score of the multimodal review.\nSpecifically, we construct a review evidence graph Gr by taking each feature (each row) of FHr and F V r as a semantic node, and connects all node pairs with edges, resulting in a fullyconnected review evidence graph with lT + lI nodes. In a similar manner, we can construct a\nproduct evidence graph Gp with lT + lI nodes from FHp and F V p . The hidden states of nodes at layer t are denoted as Gtr = {gtr,1, . . . ,gtr,n} and Gtp = {gtp,1, . . . ,gtp,n} for the review and product evidence graphs respectively, where n = lT + lI and t denotes the number of hops for graph reasoning. We compute the edge weights of semantic node pairs with an adjacency matrix that can be automatically learned through training. Taking the review evidence graph Gr as an example, we initialize the i-th semantic node at the first layer with g0i = [F H r,i,F V r,i], i ∈ {1, · · · , lT + lI}. Then, the adjacency matrix At representing edge weights at layer t is computed as follows:\nÃti,j = MLP t−1([gt−1r,i ,g t−1 r,j ]), (12)\nAt = softmax(Ãt), (13)\nwhere MLPt−1 is an MLP at layer t − 1. Ãti,j represents semantic coefficients between a node i with its neighbor j ∈ Ni. Softmax operation is used to normalize semantic coefficients Ãt. Then, we can obtain the reasoning features at layer t by:\ngtr,i = ∑ j∈Ni Ati,jg t−1 r,j . (14)\nBy stackingL graph reasoning layers, the semantic nodes can perform coherence relation reasoning by passing messages with each other. We use gLr,n and gLp,n to denote the final reasoning hidden states of the review and product evidence graphs. Subsequently, to obtain the product-related intra-review coherent reasoning features, we adopt an attention mechanism to filter the features that are irrelevant to the product:\np = Mean(hLp,∗), (15) α̃i = MLP([p,g L r,i]), (16)\nwhere a mean pooling operation is employed to derive the product coherent graph embedding p. MLP is an attention layer to calculate the productrelated features and output the attention weight α̃i for the i-th node. After normalizing the attention weight with a softmax function, we use a linear combination to aggregate the intra-review coherent reasoning results oIRC :\nα = softmax(α̃), (17) oIRC = ∑ i αig L r,i. (18)"
    }, {
      "heading" : "3.5 Review Helpfulness Prediction",
      "text" : "We concatenate the intra-modal product-review coherence features ointraM , the inter-modal productreview coherence features ointerM , and the intrareview coherence features oIRC to form the final multi-perspective coherence features ofinal = [ointraM ,ointerM ,oIRC ]. The final helpfulness prediction layer feeds ofinal into a linear layer to calculate a ranking score:\nf(pi, ri,j) = Wrofinal + br, (19)\nwhere Wr and br denote the projection parameter and bias term. pi represents information of the i-th product and ri,j is the j-th review for pi.\nThe standard pairwise ranking loss is adopted to train our model:\nL = ∑ i max(0, β−f(pi, r+)+f(pi, r−)) (20)\nwhere r+, r− ∈ Ri are an arbitrary pair of reviews for pi where r+ has a higher helpfulness score than r−. β is a scaling factor that magnifies the difference between the score and the margin. Since our MCR model is fully differentiable, it can be trained by gradient descent in an end-to-end manner."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "To the best of our knowledge, there is no benchmark dataset for the Multimodal Review Helpfulness Prediction task (MRHP). Hence, we construct two benchmark datasets (Lazada-MRHP and Amazon-MRHP) from popular e-commerce platforms to evaluate our method.\nLazada-MRHP in Indonesian Lazada.com is a popular platform in Southeast Asia, which is in the Indonesian language. We construct the LazadaMRHP dataset by crawling the product information (title, description, and images) and user-generated reviews (text content and images) from Lazada. To make sure that the user feedback of helpfulness voting is reliable, we strictly extract the reviews which were published spanning from 2018 to 2019. We focus on three product categories, including Clothing, Shoes & Jewelry (CS&J), Electronics (Elec.), and Home & Kitchen (H&K).\nAmazon-MRHP in English The Amazon review dataset (Ni et al., 2019) was collected from Amazon.com, containing meta-data of products\nand customer reviews from 1996 to 2018. We extract the product information and associated reviews published from 2016 to 2018. Since there are no review images in the original Amazon dataset, we crawl the images for each product and review from the Amazon.com platform. Similar to Lazada-MRHP, the products and reviews also belong to three categories: Clothing, Shoes & Jewelry (CS&J), Electronics (Elec.), and Home & Kitchen (H&K).\nLearning from user-feedback in review helpfulness prediction has been revealed effective in (Fan et al., 2019; Chen et al., 2019). Specifically, the helpfulness voting received by each review can be treated as the pseudo label indicating the helpfulness level of the review. Following the same data processing as in (Fan et al., 2019), we filter the reviews that received 0 votes in that they are under an unknown user feedback state. Based on the votes received by a review, we leverage a logarithmic interval to categorize reviews into five helpfulness levels. Specifically, we map the number of votes into five intervals (i.e., [1,2), [2, 4), [4, 8), [8, 16), [16,∞)) based on an exponential with base 2. The five intervals correspond to five helpfulness scores si,j ∈ {0, 1, 2, 3, 4}, where the higher the score, the more helpful the review. Finally, the statistics of the two datasets are shown by Table 2. For both Lazada-MRHP and Amazon-MRHP, we utilize 20% of the training set per category as the validation data."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "For a fair comparison, we adopt the same data processing for all baselines. We use the ICU tokenizer1 and NLTK toolkit (Loper and Bird, 2002) to separate text data in Lazada-MRHP and AmazonMRHP, respectively. Each image is extracted as RoI features with 2048 dimensions. For the net-\n1http://site.icu-project.org\nwork configurations, we initialize the word embedding layers with the pre-trained 300D GloVE word embeddings2 for Amazon-MRHP and the fastText multilingual word vectors3 for Lazada-MRHP. The text n-gram kernels are set as 1, 3, and 5 with 128 hidden dimensions. For the image representations, we set the encoded size of feature dlI as 128, and the size of common latent space dc is set to 128. We stack two graph reasoning layers (i.e., L = 2) where the hidden dimension of each layer is set to 128. We adopt the Adam optimizer (Kingma and Ba, 2014) to train our model, and the batch size is set to 32. The margin hyperparameter β is set to 1."
    }, {
      "heading" : "4.3 Compared Methods",
      "text" : "We compare MCR with several state-of-the-art review helpfulness methods. First, we compare MCR with four strong methods that rely only on the text content of reviews, including the Bilateral Multi-Perspective Matching (BiMPM) model (Wang et al., 2017), Embedding-gated CNN (EGCNN) (Chen et al., 2018), Convolutional Kernelbased Neural Ranking Model (Conv-KNRM) (Dai et al., 2018), the Product-aware Helpfulness Prediction Network (PRHNet) (Fan et al., 2019).\nWe are the first to leverage images in the re-\n2http://nlp.stanford.edu/data/glove.6B.zip 3https://fasttext.cc/docs/en/crawl-vectors.html\nview for helpfulness prediction of multimodal reviews, thereby we compare our MCR model with two strong multimodal reasoning techniques: SSE-Cross (Abavisani et al., 2020) that leverages stochastic shared embedding to fuse different modality representations and D&R Net (Xu et al., 2020) that adopts a decomposition and relation network to model both cross-modality contrast and semantic association."
    }, {
      "heading" : "4.4 Evaluation Metrics",
      "text" : "In this paper, we propose a pairwise ranking loss function for review helpfulness prediction, which fully benefits from the sampling of informative negative examples. Since the output of MCR is a list of reviews ranked by their helpfulness scores, we adopt two authoritative ranking-based metrics to evaluate the model performance: Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG@N) (Järvelin and Kekäläinen, 2017). Here, the value of N is set to 3 and 5 in the experiments for NDCG@N. MAP is a widely-used measure method evaluating the general ranking performance on the whole candidate review set, while NDCG@N merely takes into account the top N reviews in the scenario that the customers only read a limited number of reviews."
    }, {
      "heading" : "5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 Main Results",
      "text" : "Since we adopt the pairwise ranking loss for review helpfulness prediction, we treat the product text as the query, and the associated reviews are viewed as candidates for ranking. Table 3 and Table 4 report the results of MCR and baselines on Lazada-MRHP and Amazon-MRHP, respectively. From the results, we can make the following observations. First, EG-CNN performs worse than other text-only baselines, because EG-CNN only considers the hidden features from the review text, while other text-only methods additionally utilize the product information as a helpfulness signal. Second, the multimodal baselines (SSE-Cross and D&R Net) perform significantly better than textonly baselines. This verifies that multimodal information of reviews can help the models to discover helpful reviews. Third, MCR performs even better than strong multimodal competitors. For example, on Lazada-MRHP, MAP and NDCG@3 increase by 2.9% and 3.5% respectively over the best baseline method (i.e., D&R Net). We can observe similar trends on Amzaon-MRHP. The advantage of MCR comes from its capability of capturing the product-review and intra-review coherence."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "To analyze the effectiveness of different components of MCR, we conduct detailed ablation studies in terms of removing intra-review coherence (denoted as w/o intra-review), removing intra-modal coherence between product and review images (denoted as w/o intra-modal-I), removing intra-modal coherence between product and review texts (denoted as w/o intra-modal-II), removing inter-modal coherence between review text and product images (denoted as w/o inter-modal-I), and removing inter-modal coherence between review images and product text (denoted as w/o inter-modal-II). The ablation test results on the CS&J category of Lazada and Amazon datasets are summarized in Table 5. We can observe that the intra-review coherent reasoning has the largest impact on the performance of MCR. This suggests that the images within a review are informative evidence for review helpfulness prediction. The improvements of the intra-modal and inter-modal coherent reasoning in the product-review coherent reasoning module are also significant. However, intra-modal-I and intramodal-II have a smaller impact on MCR than the\nother two variants. This may be because most product images have been always beautified, and there are significant differences between the product images and the images posted by the consumers. It is no surprise that combining all components achieves the best performance on both datasets."
    }, {
      "heading" : "5.3 Case Study",
      "text" : "To gain more insight into the multimodal review helpfulness prediction task, we use an exemplary case that is selected from the test set of Home & Kitchen category of Amazon-MRHP to empirically investigate the effectiveness of our model. Table 6 shows a product and two associated reviews with ground-truth helpfulness scores voted by consumers. These two reviews are ranked correctly by our MCR method while being wrongly ranked by strong baselines (e.g., Conv-KNRM and PRHNet). The text content of both reviews contains negative emotion words (e.g., “disappointed” and “sad”) and expresses similar information “the product size does not meet my expectation”. It is hard for text-only methods to discriminate the helpfulness of these two reviews via solely considering the text content of reviews. After analyzing the images within the reviews, we can reveal that the Review 1 is helpful since it provides two appropriate bed images with a brought comforter as evidence that can well support his/her claim in the text content. However, Review 2 provides an inappropriate image with the product package, which cannot well support the claim of product size. This verifies that it is essential to capture the complex semantic relationship between the images and text content within a review for helpfulness prediction."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Multimodal review analysis (MRA) is extremely important for helping businesses and consumers quickly acquire valuable information from usergenerated reviews. This paper is the first attempt to explore the multimodal review helpfulness prediction (MRHP) task, which aims at analyzing the review helpfulness from text and images. We propose a multi-perspective coherent reasoning (MCR) method to solve MRHP task, which fully explores the product-review coherence and intra-review coherence from both textual and visual modalities. In addition, we construct two multimodal review datasets to evaluate the effectiveness of MCR, which may push forward the research in this field. Extensive experimental results demonstrate that MCR significantly outperforms baselines by comprehensively exploiting the images associated with the reviews."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by National Natural Science Foundation of China (No. 61906185), Natural Science Foundation of Guangdong Province of China (No. 2019A1515011705), Youth Innovation Promotion Association of CAS China (No. 2020357), Shenzhen Science and Technology Innovation Program (Grant No. KQTD20190929172835662), Shenzhen Basic Research Foundation (No. JCYJ20200109113441941)."
    } ],
    "references" : [ {
      "title" : "Multimodal categorization of crisis events in social media",
      "author" : [ "Mahdi Abavisani", "Liwei Wu", "Shengli Hu", "Joel Tetreault", "Alejandro Jaimes." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14679–",
      "citeRegEx" : "Abavisani et al\\.,? 2020",
      "shortCiteRegEx" : "Abavisani et al\\.",
      "year" : 2020
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-domain gated cnn for review helpfulness prediction",
      "author" : [ "Cen Chen", "Minghui Qiu", "Yinfei Yang", "Jun Zhou", "Jun Huang", "Xiaolong Li", "Forrest Sheng Bao." ],
      "venue" : "The World Wide Web Conference, pages 2630–2636.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-domain review helpfulness prediction based on convolutional neural networks with auxiliary domain discriminators",
      "author" : [ "Cen Chen", "Yinfei Yang", "Jun Zhou", "Xiaolong Li", "Forrest Bao." ],
      "venue" : "Proceedings of the 2018 Conference of the North Amer-",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural networks for soft-matching n-grams in ad-hoc search",
      "author" : [ "Zhuyun Dai", "Chenyan Xiong", "Jamie Callan", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the eleventh ACM international conference on web search and data mining, pages 126–",
      "citeRegEx" : "Dai et al\\.,? 2018",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2018
    }, {
      "title" : "Modeling and prediction of online product review helpfulness: a survey",
      "author" : [ "Gerardo Ocampo Diaz", "Vincent Ng." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 698–708.",
      "citeRegEx" : "Diaz and Ng.,? 2018",
      "shortCiteRegEx" : "Diaz and Ng.",
      "year" : 2018
    }, {
      "title" : "Product-aware helpfulness prediction of online reviews",
      "author" : [ "Miao Fan", "Chao Feng", "Lin Guo", "Mingming Sun", "Ping Li." ],
      "venue" : "The World Wide Web Conference, pages 2715–2721.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task neural learning architecture for end-to-end identification of helpful reviews",
      "author" : [ "Miao Fan", "Yue Feng", "Mingming Sun", "Ping Li", "Haifeng Wang", "Jianmin Wang." ],
      "venue" : "2018 IEEE/ACM International Conference on Advances in Social Networks Analy-",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "IR evaluation methods for retrieving highly relevant documents",
      "author" : [ "Kalervo Järvelin", "Jaana Kekäläinen." ],
      "venue" : "ACM SIGIR Forum, volume 51, pages 243–250. ACM New York, NY, USA.",
      "citeRegEx" : "Järvelin and Kekäläinen.,? 2017",
      "shortCiteRegEx" : "Järvelin and Kekäläinen.",
      "year" : 2017
    }, {
      "title" : "Automatically assessing review helpfulness",
      "author" : [ "Soo-Min Kim", "Patrick Pantel", "Timothy Chklovski", "Marco Pennacchiotti." ],
      "venue" : "EMNLP, pages 423–430.",
      "citeRegEx" : "Kim et al\\.,? 2006",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2006
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Linguistic features for review helpfulness prediction",
      "author" : [ "Srikumar Krishnamoorthy." ],
      "venue" : "Expert Systems with Applications, 42(7):3751–3759.",
      "citeRegEx" : "Krishnamoorthy.,? 2015",
      "shortCiteRegEx" : "Krishnamoorthy.",
      "year" : 2015
    }, {
      "title" : "Using argument-based features to predict and analyse review helpfulness",
      "author" : [ "Haijing Liu", "Yang Gao", "Pin Lv", "Mengxue Li", "Shiqiang Geng", "Minglan Li", "Hao Wang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "NLTK: The natural language toolkit",
      "author" : [ "Edward Loper", "Steven Bird." ],
      "venue" : "arXiv preprint cs/0205028.",
      "citeRegEx" : "Loper and Bird.,? 2002",
      "shortCiteRegEx" : "Loper and Bird.",
      "year" : 2002
    }, {
      "title" : "Prediction of helpful reviews using emotions extraction",
      "author" : [ "Lionel Martin", "Pearl Pu." ],
      "venue" : "AAAI, pages 1551–1557.",
      "citeRegEx" : "Martin and Pu.,? 2014",
      "shortCiteRegEx" : "Martin and Pu.",
      "year" : 2014
    }, {
      "title" : "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
      "author" : [ "Jianmo Ni", "Jiacheng Li", "Julian McAuley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Ni et al\\.,? 2019",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2019
    }, {
      "title" : "Categoryaware graph neural networks for improving ecommerce review helpfulness prediction",
      "author" : [ "Xiaoru Qu", "Zhao Li", "Jialin Wang", "Zhipeng Zhang", "Pengcheng Zou", "Junxiao Jiang", "Jiaming Huang", "Rong Xiao", "Ji Zhang", "Jun Gao." ],
      "venue" : "Pro-",
      "citeRegEx" : "Qu et al\\.,? 2020",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Bilateral multi-perspective matching for natural language sentences",
      "author" : [ "Zhiguo Wang", "Wael Hamza", "Radu Florian." ],
      "venue" : "Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 4144–4150.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association",
      "author" : [ "Nan Xu", "Zhixiong Zeng", "Wenji Mao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantic analysis and helpfulness prediction of text for online product reviews",
      "author" : [ "Yinfei Yang", "Yaowei Yan", "Minghui Qiu", "Forrest Bao." ],
      "venue" : "ACL, pages 38–44.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Earlier review helpfulness prediction methods rely on a wide range of handcrafted features, such as semantic features (Yang et al., 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : ", 2015), lexical features (Martin and Pu, 2014), and argument based features (Liu",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : ", 2018, 2019) and LSTM (Fan et al., 2019) have become dominant in the literature due to their powerful performance for helpfulness prediction by learning text representation automatically.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "Most conventional approaches on review helpfulness prediction focus solely on the text of reviews, which can be generally divided into two categories based on the way of extracting predictive features: machine learning based methods with hand-crafted features (Kim et al., 2006; Krishnamoorthy, 2015) Product Information Teflon Pans 1 Set of 3 pcs 1042-Non-stick Set of 3",
      "startOffset" : 260,
      "endOffset" : 300
    }, {
      "referenceID" : 12,
      "context" : "Most conventional approaches on review helpfulness prediction focus solely on the text of reviews, which can be generally divided into two categories based on the way of extracting predictive features: machine learning based methods with hand-crafted features (Kim et al., 2006; Krishnamoorthy, 2015) Product Information Teflon Pans 1 Set of 3 pcs 1042-Non-stick Set of 3",
      "startOffset" : 260,
      "endOffset" : 300
    }, {
      "referenceID" : 9,
      "context" : "The machine learning based methods employ domain-specific knowledge to extract a variety of hand-crafted features, such as structure features (Kim et al., 2006),",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "lexical features (Krishnamoorthy, 2015), emotional features (Martin and Pu, 2014), and argument features (Liu et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "lexical features (Krishnamoorthy, 2015), emotional features (Martin and Pu, 2014), and argument features (Liu et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "lexical features (Krishnamoorthy, 2015), emotional features (Martin and Pu, 2014), and argument features (Liu et al., 2017), from the textural reviews, which are then fed into conventional classifiers such as SVM (Kim et al.",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : ", 2017), from the textural reviews, which are then fed into conventional classifiers such as SVM (Kim et al., 2006) for helpfulness prediction.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "Image Representation We use pre-trained Faster R-CNN to extract the region of interest (RoI) pooling features (Anderson et al., 2018) for the",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "All the RoI features vi extracted from image sets Ip and Ir are then encoded by a self-attention module (Vaswani et al., 2017), resulting in a dI -dimensional semantic space with non-local understanding:",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "Amazon-MRHP in English The Amazon review dataset (Ni et al., 2019) was collected from Amazon.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Learning from user-feedback in review helpfulness prediction has been revealed effective in (Fan et al., 2019; Chen et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "Learning from user-feedback in review helpfulness prediction has been revealed effective in (Fan et al., 2019; Chen et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Following the same data processing as in (Fan et al., 2019), we filter the reviews that received 0 votes in that they are under an unknown user feedback state.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "We use the ICU tokenizer1 and NLTK toolkit (Loper and Bird, 2002) to separate text data in Lazada-MRHP and AmazonMRHP, respectively.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "We adopt the Adam optimizer (Kingma and Ba, 2014) to train our model, and the batch size is set to 32.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "First, we compare MCR with four strong methods that rely only on the text content of reviews, including the Bilateral Multi-Perspective Matching (BiMPM) model (Wang et al., 2017), Embedding-gated CNN (EGCNN) (Chen et al.",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : ", 2017), Embedding-gated CNN (EGCNN) (Chen et al., 2018), Convolutional Kernelbased Neural Ranking Model (Conv-KNRM) (Dai et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : ", 2018), Convolutional Kernelbased Neural Ranking Model (Conv-KNRM) (Dai et al., 2018), the Product-aware Helpfulness Prediction Network (PRHNet) (Fan et al.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : ", 2018), the Product-aware Helpfulness Prediction Network (PRHNet) (Fan et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "html view for helpfulness prediction of multimodal reviews, thereby we compare our MCR model with two strong multimodal reasoning techniques: SSE-Cross (Abavisani et al., 2020) that lever-",
      "startOffset" : 152,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "ages stochastic shared embedding to fuse different modality representations and D&R Net (Xu et al., 2020) that adopts a decomposition and relation network to model both cross-modality contrast and semantic association.",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "is a list of reviews ranked by their helpfulness scores, we adopt two authoritative ranking-based metrics to evaluate the model performance: Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG@N) (Järvelin and Kekäläinen, 2017).",
      "startOffset" : 221,
      "endOffset" : 252
    } ],
    "year" : 2021,
    "abstractText" : "As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities. Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness. Concretely, we first propose a productreview coherent reasoning module to measure the intraand inter-modal coherence between the target product and the review. In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task. Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model. The source code and datasets can be obtained from https:// github.com/jhliu17/MCR.",
    "creator" : "LaTeX with hyperref"
  }
}