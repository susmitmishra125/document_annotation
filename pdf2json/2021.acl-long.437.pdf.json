{
  "name" : "2021.acl-long.437.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders",
    "authors" : [ "Bin Sun", "Shaoxiong Feng", "Yiwei Li", "Jiamou Liu", "Kan Li" ],
    "emails" : [ "binsun@bit.edu.cn", "shaoxiongfeng@bit.edu.cn", "liyiwei@bit.edu.cn", "likan@bit.edu.cn", "jiamou.liu@auckland.ac.nz" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5624–5637\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5624"
    }, {
      "heading" : "1 Introduction",
      "text" : "When conversing with a human user, an opendomain dialogue system is expected to generate human-like responses – responses that not only are diverse and informative, but also contain relevant and cohesive information that correctly addresses the context dialogue. Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018). Yet, it is challenging for a CVAE-based dialogue generation model to keep the responses relevant and\ncoherent. The challenge arises as human dialogues inherently exhibit the one-to-many and many-toone phenomena (Csaky et al., 2019), meaning that the same context could lead to very different responses, and different contexts could lead to the same response, respectively. As a result, the latent variables sampled by CVAE often fail to capture the correct contextual semantics, as shown in Fig. 1, leaving open the possibility that similar contexts producing drastically different latent variables. This has two particular drawbacks:\nFirst, the discrepancy between latent variables could lead to irrelevant and incoherent generated responses. Different latent variables in a continuous latent space correspond to different responses (Bowman et al., 2016). As dissimilar latent variables may be sampled for similar contexts, the generated responses for contexts in the test set could be drastically different from responses to similar contexts in the training set. For instance, given a context “Everything about this movie is awesome!”, a standard CVAE may generate response as dis-\nsimilar as“Smartphones of the best games!.” and “Caves would never say yes, but I’d love to know.” (Gao et al., 2019). Thus this approach sacrifices too much relevance and coherence for diversity and informativeness.\nSecond, the disparity between contexts and latent variables hurts model generalizability. Model generalizability is often evaluated using a separate dataset taken from a similar distribution as the training set (e.g., a validation or a noisy version of the training set). High generalizability is indicated if the model can transfer favourable abilities from the training set to this second dataset, in the sense that it produces consistent responses between similar contexts across the two datasets. This suggests that the model has acquired certain semantic relations between sentences from the training set. However, if the sampled latent variable departs significantly from the contextual semantics, the model may perform quite differently on the second dataset from the training set.\nTo address these drawbacks, we propose a novel model, namely Self-Separated Conditional Variational Autoencoder (SepaCVAE). SepaCVAE proactively partitions the input data into a number of groups, and then widens the absolute differences between data pairs across different groups while narrowing the relative distance between data pairs within the same group. In this way, SepaCVAE aims to put the contexts that sample similar latent variables into the same groups, thereby regularizing the latent variables. The design of SepaCVAEinvolves three components that are built on top of standard CVAE. First, inspired from image augmentation, we propose a dialogue augmentation method to partition data without any prior knowledge. For this, we construct N orthogonal vectors to classify data into N groups, which retain the original semantic relationships of data within a group. We directly enlarge the semantic distance of the data across different groups. Then, we propose a gradient blocking algorithm to select the most suitable group for each data according to gains obtained from different groups. Here, the gains are evaluated using reconstruction loss. Finally, inspired from the contrastive learning paradigm (Cai et al., 2020; Chen et al., 2020a,b; Mitrovic et al., 2020), we propose relationship enhancement to increase similarity between the representations of data within the same group, and differentiate the representations of data between different groups.\nContributions: Our first contribution is a theoretical analysis on why sampled latent variables fail to reflect the contexts’ semantics. The next contribution lies in the proposal of SepaCVAE to overcome issues of irrelevant and incoherent responses caused by standard CVAE. Our third contribution involves a series of experiments. The results show that our SepaCVAE can generate more relevant and coherent responses compared to existing methods."
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 Dialogue models",
      "text" : "Open-domain dialogue generation is a challenging task in natural language processing. Early dialogue models (Shang et al., 2015; Sordoni et al., 2015b) often tend to generate dull responses. To improve the quality of these responses, two pathways have been adopted: one is to introduce external semantic information, such as dialogue history (Sordoni et al., 2015a; Serban et al., 2016), topic (Xing et al., 2017), sentiment (Huber et al., 2018), knowledge (Ghazvininejad et al., 2018), persona-style (Li et al., 2016c), and other information (Li et al., 2016a; Wang et al., 2017; Baheti et al., 2018; Feng et al., 2020b). The other is through more complex models or frameworks, such as attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015), reinforcement learning (RL) (Li et al., 2016d; Zhang et al., 2018a; Liu et al., 2020), generative adversarial network (GAN) (Yu et al., 2017; Li et al., 2017a; Zhang et al., 2018b; Feng et al., 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).\nCVAE models are conversational models that are based on variational reasoning. Many existing CVAE models have achieved state-of-the-art performance by generating diverse and informative responses. Moreover, as opposed to methods that introduce external semantic information, CVAE models use latent variables to represent such information. Hence they can be applied when external information is not available. Comparing with the models based on RL or GAN, CVAE models are simpler and can be easily trained. In addition, CVAE models can be enhanced by methods that use RL or GAN as generators to further improve their performances.\nHowever, empirical evidences (Gao et al., 2019; Gu et al., 2019) have indicated that while the use of\nlatent variables may make the generated responses more diverse and informative, it could also reduce relevance and coherence. To alleviate this apparent issue, CVAE models have been used in combination with external information such as persona information, dialogue history and dialogue act (Shen et al., 2017; Serban et al., 2017; Zhao et al., 2017). However, simply borrowing external information is not sufficient to resolve the one-to-many issue, especially when the amount of data is very large. No existing model resolves the core issue of the problem, that is, the latent variable inherits little semantic information from the context sentence, a consequence of the inherent one-to-many and many-to-one phenomena of human conversations. To address this issue, we propose the SepaCVAE model which trains latent variables that inherit contextual semantics."
    }, {
      "heading" : "2.2 Self-supervised method used in dialogue generation task",
      "text" : "Recently, self-supervised methods such as contrastive learning – popularized in computer vision (Chen et al., 2020a,b) – are drawing increasing attention in NLP (Wu et al., 2019; Clark et al., 2020; Cai et al., 2020). Generally speaking, the major issue with applying contrastive learning is how positive and negative examples are constructed. Many existing work explore ways to design reasonable pairs of positive and negative examples to accurately capture the semantic relations of these pairs, so that the obtained representation can be betterused on downstream tasks."
    }, {
      "heading" : "3 Problem formulation",
      "text" : "The problem with the standard CVAE model lies in that the sampled latent variables may not accurately reflect the contextual semantics due to the apparent one-to-many (one context may correspond to many responses) and many-to-one (many contexts may also correspond to one response) phenomena. This leads to irrelevant and incoherent responses, and harms model generalizability. Our aim is to adapt sampled latent variables to capture the contextual semantics, so that the effects of these phenomena are neutralized. This will in turn be helpful to generate relevant and coherent responses. With this goal, we focus on single-turn dialogue datasets where the one-to-many situations appear more frequently than multi-turn dialogue datasets."
    }, {
      "heading" : "3.1 Preconditions",
      "text" : "This section formally analyzes the many-to-one and one-to-many phenomena and we present several important assumptions and contextual information (i.e., preconditions) for the CVAE model. Notations: θ and φ are parameters of CVAE’s recognition network and prior network, respectively; c represents the condition information, x and r represent the generation target, and z represents the latent variable. Precondition 1: Bowman et al. (2016) confirmed that the latent space is continuous; the latent variable z is highly correlated with the target data x, meaning that different z will reconstruct different x. Precondition 2: CVAE has a recognition network qφ(z|c, x) and a prior network pθ(z|c) to approximate the true posterior distribution p(z|c, x) and prior distribution p(z|c), respectively. These distributions are assumed to follow the Gaussian distribution, e.g., qφ(z|c, x) ∼ N(µ, σ2). Precondition 3: To efficiently train a CVAE model, the Stochastic Gradient Variational Bayes (SGVB) framework (Sohn et al., 2015; Yan et al., 2016; Kingma and Welling, 2014) is adopted which aims to maximize the variational lower bound of the conditional log likelihood:\nL(θ, φ; c, x) = −KL(qφ(z|c, x)||pθ(z|c)) +Eqφ(z|c,x) [log p(x|z, c)] (1)\nwhere KL represents Kullback–Leibler divergence. During training, the σ of q(z|x, c) will get smaller and smaller, and the µ of q(z|x, c) will get closer and closer to z that corresponding to x, which aims to stabilize the Eqφ(z|x,c) [log p(x|z, c)] and make it converge."
    }, {
      "heading" : "3.2 Demonstrating the existence of the problem",
      "text" : "We use Fig. 2 to illustrate the impact of one-tomany phenomenon and many-to-one phenomenon on a trained standard CVAE model. Consider the situation in Fig. 2(a) where the context c1 has two different responses r1 and r2. By Precondition 2, we assume two approximate posterior distributions p(z|c1, r1) ∼ N(µ1, σ21), p(z|c1, r2) ∼ N(µ2, σ22) and one approximate prior distribution p(z|c1) ∼ N(µ, σ2). By Precondition 3, during training, µ1 and µ2 will get closer to the latent variables that could be reconstructed to r1 and r2, respectively. By Precondition 1, as r1 is different from\nr2, µ1 should also be different from µ2. Otherwise, the latent variables sampled from p(z|c1, r1) and p(z|c1, r2) tend to be the same, making these latent variables irrelevant to the responses. This leads to the vanishing latent variable problem (Bowman et al., 2016). Therefore, µ1 and µ2 cannot be the same, and their discrepancy can be considered stable; only in this way we can ensure one-to-one correspondence between latent variables and responses. From Precondition 3, it is easy to see that p(z|c) is only affected by p(z|c, r). Hence, we ignore E∗ [·] in Eq. (1) and use KL(p(z|c, r)||p(z|c)) to analyze the trend of p(z|c) during training. Considering Fig. 2(a) where KL(·) of (c1, r1) and (c1, r2) equals to KL(p(z|c1, r1)||p(z|c1)) + KL(p(z|c1, r2)||p(z|c1)). We provide details of the computation in Appendix A. The formulation can then be simplified as: log ( σ2\nσ1σ2\n) +\nσ21+σ 2 2+(µ1−µ)2+(µ2−µ)2\n2σ2 − 1.\nHence, we can compute µ∗ and σ∗ that minimizes the above using Lagrange multiplier:\nµ∗ = (µ1 + µ2)/2 σ∗ = √\n(σ21 + σ 2 2)/2 + (µ1 − µ2)2/4.\nThe derivation above provides insights on the\nproblem caused by the one-to-many phenomena in Fig. 2(a): After training, the prior conditional probability p(z|c1) ∼ N(µ∗, σ∗2), which will be used in inference. If the difference between r1 and r2 widens, the difference between µ1 and µ2 will also widen and µ∗ will become further away from µ1 and µ2. During inference, the latent variables sampled from p(z|c1) have a high probability to differ from those sampled from p(z|c1, r1) and p(z|c1, r2). These latent variables will introduce irrelevant information and contribute to the generation of irrelevant responses. In addition, as one response r1 may correspond to different contexts c1 and c2, as shown in Fig. 2(b), p(z|c1) and p(z|c2) tend to be the same, which contributes to the phenomenon that different context could sample similar latent variables. In a word, similar contexts could correspond to different latent variables and different contexts could correspond to similar latent variables, which explains why the latent variables can not accurately reflect the contexts’ semantics."
    }, {
      "heading" : "4 Method",
      "text" : "In this section, we introduce in detail the proposed SepaCVAE model and its three key components, dialogue augmentation, gradient blocking, and relationship enhancement.\n4.1 Self-Separated CVAE\nAs shown in Fig. 3, SepaCVAE uses G(·) to separate the contexts into different groups. For the one-to-many phenomenon, the contexts in different groups will have different prior distributions p(z|G∗(·)), which is easily affected by the different posterior distributions. As for the many-to-one phenomenon, SepaCVAE makes the contexts (c1, c2) generate latent variables related to the response r1 only when it contains group information G1(·). The other group would help the contexts to align with the other latent variables."
    }, {
      "heading" : "4.2 Dialogue augmentation",
      "text" : "In SepaCVAE, we first propose dialogue augmentation (see Algorithm 1), which designs a group of orthogonal vectors (y1, y2, . . . , yN ) to separate the contexts into different groups. These vectors (y1, y2, . . . , yN ) are called group information.\nAlgorithm 1 Dialogue augmentation\nInput: Cori1×m : the vector representation of original context sentence after word embedding process; N : the hyper-parameter; m : the dimension of word embedding; Output: CextN×m : vector representations of context sentences after augmentation; Y extN×1 : the labels of the augmented contexts;\n1: Initialize CextN×m and Y ext N×1; 2: Set d← the integer of m/N ; 3: for i = 1 to N do 4: Initialize augment vector yi ← (0, 0, . . . , 0)1×m; 5: Set yi((i − 1) × d + 1 : i × d) ← (1, 1, . . . , 1)1×d; 6: CextN×m(i, :)← Cori1×m + yi; 7: Y extN×1(i)← i; 8: end for 9: return CextN×m, Y ext N×1\nIn SepaCVAE, we apply Algorithm 1 to extend each dialogue pair (ci, ri) to [(ci + y1, ri), (ci + y2, ri), . . . , (ci + yN , ri)] before feeding them to start training. If different contexts ci, cj , . . . have the same yi added, then these contexts belong to the same group. In this way, all contexts will keep a certain relationship within the same group. In this work, the value N is set to 8. Since we use c+ y to replace the original c, the variational lower bound of SepaCVAE is re-written as:\nL(θ, φ;r, c, y) = Eqφ(z|r,c+y)[log p(r|z, c+ y)] −KL(qφ(z|r, c+ y)||Pθ(z|c+ y)) (2)"
    }, {
      "heading" : "4.3 Gradient blocking",
      "text" : "Before the gradient back-propagation, we propose gradient blocking (see Algorithm 2 in Appendix B for implementation details) to filter the gradients. Since we extend the dialogue pair (c, r) to [(c+ y1, r), (c+ y2, r), . . . , (c+ yN , r)], if we optimize the model through all calculated gradients, y1, y2, . . . , yN would be regarded as noise. Therefore, We choose the largest variational lower bound\nthat is calculated through the dialogue pair (c, r) with the positive group information y+, which can be represented as (3):\nL(θ, φ; r, c, y+) = max θ,φ,yi∈Y L(θ, φ; r, c, yi) (3)\nFor each [(c+ y1, r), (c+ y2, r), . . . , (c+ yN , r)], we only pass L(·, y+) to optimize the model."
    }, {
      "heading" : "4.4 Relationship enhancement",
      "text" : "Through dialogue augmentation and gradient blocking, the positive y+ for each dialogue pair (c, r) is captured. We then propose relationship enhancement, which is inspired from contrastive learning, to adjust the separated results. Those responses under the same y+ are considered to be in the same group, and thus can be seen as positive samples; similarly, those responses under different y+ are seen as negative samples. From the perspective of contrastive learning, we design a relationship-enhancement-loss named Lre to help our model achieve the representation learning:\nLre = (4)\n− log e ∑Pos j=1 f(x ′ i) T f(x ′+ j )\ne ∑Pos j=1 f(x ′ i) T f(x ′+ j ) + e ∑Neg m=1 f(x ′ i )T f(x ′− m ) N−1 ,\nwhere x ′\nrepresents the embedded generated response, f(·) represents our model’ encoder, Pos means the number of positive samples, and Neg means the number of negative samples.\nIn addition, we introduce an MLP to predict y+ based on vector representation of the generated response f(x ′ ). We therefore define LY :\nLY = Epψ(x|z,c+y+) [ log(p(y+|x′)) ] (5)\nOverall, SepaCVAE is trained by maximizing:\nLall = L(θ, φ; r, c, y+)− α ∗ Lre − LY (6)\nQuoting the KL annealing trick (Bowman et al., 2016), α increases linearly from 0 to 1 in the first 10,000 batches."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Dataset",
      "text" : "We use two public dialogue datasets in our experiments, and change them as single-turn dialog data. The first dataset, named DailyDialog (Li et al., 2017b), consists of dialogues that resemble human\ndaily communication. The second dataset, named OpenSubtitles (Tiedemann, 2009), includes a large collection of conversations converted from movie transcripts in English."
    }, {
      "heading" : "5.2 Data pre-processing",
      "text" : "In this work, we extract single-turn dialogues from two dialogue datasets, DailyDialog and OpenSubtitles. From a multi-turn dialogue (u1, u2, ..., uT ), we can extract T − 1 single-turn dialogues [(u1, u2), (u2, u3), ..., (uT−1, uT )], where u represents an utterance. As discussed above, compared with multi-turn dialogue dataset the single-turn dialogue dataset contains a more serious one-to-many problem. Therefore, using the single-turn dialogue dataset for experimentations can highlight the problem of general CVAE model and reflect the effect of our method.\nWe utilize 300-dimensional GloVe embeddings (Pennington et al., 2014) to represent these dialogues in vectors. Since the tokens in GloVe do not cover all tokens in DailyDialog and OpenSubtitles datasets, we extract the token-list of GloVe to filter these datasets. Table 1 lists key statistics of the dataset after processing. In addition, we count the one-to-many samples of both datasets and found that 408 contexts in DailyDialog and 90,149 contexts in OpenSubtitles have multiple responses. In particular, a context in OpenSubtitles has a maximum of 623 responses, while a context in DailyDialog has a maximum of 29 responses, which shows that the one-to-many phenomenon is more prevalent in OpenSubtitles dataset."
    }, {
      "heading" : "5.3 Automatic evaluation metrics",
      "text" : "We use ppl (Neubig, 2017), response length and distinct-n (Li et al., 2016b) to evaluate the diversity of generated responses. We also use BLEU (Papineni et al., 2002) to evaluate the degree of the word-overlap between generated responses and ground truth. Moreover, we use Embedding Average (Average) (Liu et al., 2016)) to evaluate the semantic relationship of generated responses and ground-truth responses. Finally, we introduce the\ncoherence (Xu et al., 2018b) to assess the coherence between contexts and generated responses."
    }, {
      "heading" : "5.4 Human evaluation",
      "text" : "We conduct human evaluation to further evaluate our model and baseline models. Following the work of Li et al. (2017a); Xu et al. (2018a), we randomly extract 200 samples from the test sets of the two dialogue datasets, respectively. Each sample contains one context and the response generated by different models. Three annotators are invited to rank the generated responses with respect to three aspects: diversity, relevance and fluency. Ties are allowed. Diversity indicates how much the generated response provides specific information, rather than generic and repeated information. Relevance means how likely the generated response is relevant to the context. Fluency specifies how likely the generated response is produced by human."
    }, {
      "heading" : "5.5 Baseline models",
      "text" : "Our baseline models include sequence-to-sequence (Seq2Seq) model, CVAE model, and cluster-CVAE model. They are all implemented based on a 2- layer GRU kgCVAE model (Zhao et al., 2017). The cluster-CVAE model represents that kgCVAE utilize the cluster results as the knowledge. We employ three cluster methods, i.e. K-means(K), Spectral(S), Agglomerative(A)."
    }, {
      "heading" : "5.6 Training details",
      "text" : "For a fair comparison among all models, we utilized 300-dimensional GloVe embeddings as the word embedding matrix. The numbers of hidden nodes are all set to 300. The parameter max len is set to 25. We set the batch sizes to 64 and 32 for DailyDialog and OpenSubtitles datasets, respectively. Adam is utilized for optimization. The parameter init lr is set to 0.001. We train all models in 50 epochs on a RTX 2080Ti GPU card with Tensorflow, and save the generated responses when the ppl reaching minimum. Greedy search is used to generate responses for evaluation."
    }, {
      "heading" : "6 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Automatic evaluation results",
      "text" : "Table 2 and Table 3 report the automatic evaluation results of SepaCVAE and baseline models on validation and test data of both two datasets, respectively. For the validation stage, we first select and save the positive group information (y+)\nfor each context, and then generate responses under this y+. For the test data where no ground truth response is available to select the positive group information, we first generate N responses for each context through N group information, and then choose the most possible generated response through calculating the cosine score between the generated responses and context. Both generated responses and contexts are input into SepaCVAE’s encoder to obtain the vector representations.\nSpectral and Agglomerative cluster methods would not work well under the large-scale dataset (i.e. OpenSubtitles), and the general CVAE model suffers from the vanishing latent variable problem while training on such dataset. Therefore, we remove the results of S-CVAE+BOW, ACVAE+BOW and CVAE on Table 2 and Table 3.\nAs shown in Table 2 and Table 3, the results on large-scale dataset (OpenSubtitles) are better\nthan that on small dataset (DailyDialog), that is, the results on OpenSubtitles show an obvious pattern that verifies our hypothesis. On both validation and test data of OpenSubtitles, CVAE and KCVAE achieve better performance on diversity metric (distinct) but worse performance on relevant metrics (i.e. BLEU, Average and coherence) than Seq2Seq model. Moreover, our proposed SepaCVAE outperforms all baseline models in terms of all metrics with statistical significance. However, the results obtained on the DailyDialog dataset do not show a clear pattern. For DailyDialog’s validation data, SepaCVAE achieves good performance on diversity but on relevance the results is unimpressive. On the other hand, for test data, SepaCVAE achieves good performance on relevance but generally poor results on diversity. We believe that the reason for this phenomenon is related to the level of prevalence of the one-to-many phenomenon in the\ndataset. For instance, only 66,260 contexts have multiple responses among the 90,149 contexts on the OpenSubtitles that was added the cluster results. Moreover, one context has a maximum of 296 responses, which amounts to almost half of 623. Since the DailyDialog dataset is very small and contains few samples that we focus on, which cause the not specific tendency on its results. In a word, the evaluation results illustrate the effectiveness of SepaCVAE in terms of improving the relevance and coherence of responses."
    }, {
      "heading" : "6.2 Human evaluation results",
      "text" : "The results of the human evaluation are shown in Table 4. To evaluate the consistency of the ranking results assessed by three annotators, we use Pearson’s correlation coefficient. This coefficient is 0.22 on diversity, 0.63 on relevance, and 0.70 on fluency, with p < 0.0001 and below 0.001, which indicates high correlation and agreement. Similarly with the automatic evaluation results in Table 3, this result shows that our SepaCVAE significantly outperforms baselines in term of relevance and diversity. Except the ground-truth responses, our SepaCVAE achieve the best scores of relevance and diversity metrics. The fluency result of SepaCVAE on the DailyDialog dataset is slightly worse than that of baselines, which is mainly due to the length of responses generated by SepaCVAE is almost two times than that of baselines (see Table 3). When the response lengths are similar on the Opensubtitles dataset, SepaCVAE could also achieve the best fluency score."
    }, {
      "heading" : "6.3 Effectiveness analysis",
      "text" : "We further analyze the effectiveness of SepaCVAE on regularizing latent variables. For the contexts\n.\nin the validation data of DailyDialog dataset, we collect their generated responses and the sampled latent variables of both SepaCVAE and baseline models on the first 2,500 batches. Then we calculate the average inner-group distance and the average inter-group distance for each context based on jointly vector representations (concatenating the context vector and the latent variable). All distances are calculated by cosine scores, and the higher the distance, the greater the similarity.\nFor each context, SepaCVAE outputs a positive group information y+, which is used to distinguish whether other contexts are in the same group. As for the standard CVAE, we set a threshold of the cosine score to replace the group information. In this work, the threshold is set to 0.9. Finally, we take the average of all contexts’ inner-group distance results and inter-group distance results as inner-dis. and inter-dis. of each batch, which are shown in Fig. 4. SepaCVAE achieves significantly higher\ninner-dis. than baseline (standard CVAE) model, while the inter-dis. are similar. Meanwhile, our method also gets the similar average distance of all jointly vectors with the standard CVAE.\nIn addition, past studies conjecture that the posterior z sampled from the recognition network should cluster the responses into meaningful groups that correlate with the knowledge. Fig. 5 visualizes the posterior z of responses in the validation data of DailyDialog dataset in 2D space using t-SNE (van der Maaten and Hinton, 2008). We found that the learned latent space of our SepaCVAE is more correlated with the group information. These results demonstrate that SepaCVAE can effectively regularize latent variables."
    }, {
      "heading" : "6.4 Case study",
      "text" : "We collected the generated responses of contexts in validation and test set, which are similar to the training set, and showed a sample in Table 4. The context in training set has two contradictory responses. As we analyzed, the standard CVAE and CVAE+BOW generated irrelevant and incoherent response for the similar context in validation and test set. In contrast, our SepaCVAE outputted sure, it will be happy and sure. i go with my parents are more relevant and coherent than the response generated by baselines, and it also similar with the true response 1 (oh, that sounds great!), which means the SepaCVAE is able to handle the one-to-many situation."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we theoretically prove that latent variables hardly reflect the semantics of contexts due to the one-to-many and many-to-one phenomena of dialogues. For the standard CVAE model, these issues lead to irrelevant and incoherent responses during the validation or test stage, and also damaging the generalization performance. To address these problems, we proposed the SepaCVAE model. There are three main technical novelties of SepaCVAE: dialogue augmentation, gradient blocking, and relationship enhancement, which enable the latent variables to reflect semantic relationships between contexts. As demonstrated in the experimental results, SepaCVAE could get the best performance for large-scale dataset."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their constructive comments. This research is supported by Beijing Natural Science Foundation (No. L181010 and 4172054), National Key R&D Program of China (No. 2016YFB0801100). Kan Li is the corresponding author."
    }, {
      "heading" : "A The computation of prior probability distribution through KL-divergence on the one-to-many situation",
      "text" : "We assume that p(z|c1, r1) ∼ N(µ1, σ21), p(z|c1, r2) ∼ N(µ2, σ22) and p(z|c1) ∼ N(µ, σ2). Then, we have:\nKL(p(z|c1, r1)||p(z|c1))\n= ∫ p(z|c1, r1) log\np(z|c1, r1) p(z|c1) dz\n= ∫ p(z|c1, r1)[log p(z|c1, r1)− log p(z|c1)]dz\n= ∫ p(z|c1, r1)[log e − (z−µ1) 2\n2σ21√ 2πσ21\n− log e − (z−µ)\n2\n2σ2\n√ 2πσ2 ]dz\n= ∫ p(z|c1, r1)[− 1\n2 log 2π − log σ1\n− (z − µ1) 2\n2σ21 +\n1 2 log 2π + log σ +\n(z − µ)2\n2σ2 ]dz\n= ∫ p(z|c1, r1)[log σ\nσ1\n+ ( (z − µ)2 2σ2 − (z − µ1) 2\n2σ21 )]dz\n= ∫ p(z|c1, r1) log σ\nσ1 dz\n+ ∫ p(z|c1, r1) (z − µ)2\n2σ2 dz − ∫ p(z|c1, r1) (z − µ1)2\n2σ21 dz.\nSince the log σσ1 is a constant, and the∫ p(z|c1, r1)dz = 1, we have:∫\np(z|c1, r1) log σ\nσ1 dz = log\nσ\nσ1 .\nSince p(z|c1, r1) = 1√2πσ1 e − (z−µ1)\n2\n2σ2 , the∫ p(z|c1, r1) (z−µ1) 2\n2σ21 dz can be calculated as follow:∫\np(z|c1, r1) (z − µ1)2\n2σ21 dz\n= ∫ 1√ 2πσ1 e− (z−µ1) 2 2σ2 (z − µ1)2 2σ21 dz\n= ∫ 1√ 2πσ1 e− (z−µ1) 2 2σ2 (z − µ1)2 2σ21 √ 2σ1d z − µ1√ 2σ1\n= ∫ 1√ π e− (z−µ1) 2 2σ2 (z − µ1)2 2σ21 d z − µ1√ 2σ1 .\nLet the x= z−µ1√ 2σ1\n, we have:∫ p(z|c1, r1) (z − µ1)2\n2σ21 dz\n= 1√ π\n∫ e−x 2 x2dx\n= − 1 2 √ π\n∫ xde−x 2\n= − 1 2 √ π (xe−x\n2 |+∞−∞ − ∫ e−x 2 dx).\nAccording to the L’Hospital’s rule, the limx→−∞ xe −x2=limx→+∞ xe−x 2 = 0.\nTo calculate the ∫ e−x 2 dx, we first compute the\n( ∫ +∞ 0 e −x2dx)2, so we have:\n( ∫ +∞ 0 e−x 2 dx)2 = ∫ +∞ 0 e−x 2 dx\n· ∫ +∞ 0 e−y 2 dy\n= ∫ +∞ 0 ∫ +∞ 0 e−x 2−y2dxdy.\nLet x = r sin θ and y = r cos θ, we have:∫ +∞ 0 ∫ +∞ 0 e−x 2−y2dxdy\n=\n∫ π 2\n0 ∫ +∞ 0 e−r 2 rdrdθ\n= π\n2 ∫ +∞ 0 e−r 2 rdr = π 4 .\nTherefore, the ∫ +∞ 0 e −x2dx = √ π 2 . According\nto the symmetry, the ∫ +∞ −∞ e\n−x2dx= √ π. and the∫\np(z|c1, r1) (z−µ1) 2\n2σ21 dz = 12 . For the ∫ p(z|c1, r1) (z−µ) 2\n2σ2 dz, we have:∫\np(z|c1, r1) (z − µ)2\n2σ2 dz\n= ∫ p(z|c1, r1) (z − µ1 + µ1 − µ)2\n2σ2 dz\n= 1\n2σ2 [\n∫ (z − µ1)2p(z|c1, r1)dz\n+ ∫ (µ1 − µ)2p(z|c1, r1)dz\n+ ∫ (z − µ1)(µ1 − µ2)p(z|c1, r1)dz]\n= 2σ21\n∫ (z−µ1)2 2σ21 p(z|c1, r1)dz + (µ1 − µ)2\n2σ2\n= σ21 + (µ1 − µ)2\n2σ2 .\nTherefore, we have:\nKL(p(z|c1, r1)||p(z|c1))\n= log σ σ1 + σ21 + (µ1 − µ)2 2σ2 − 1 2 .\nIn the same way, the KL(p(z|c1, r2)||p(z|c1)) equals log σσ2 + σ22+(µ2−µ)2 2σ2\n− 12 . And then, we can know:\nKL(p(z|c1, r1)||p(z|c1)) +KL(p(z|c1, r2)||p(z|c1))\n= log( σ2\nσ1σ2 )\n+ σ21 + σ 2 2 + (µ1 − µ)2 + (µ2 − µ)2\n2σ2 − 1.\nSince the Latent Vanish problem is not expected by the VAE and CVAE methods, the p(z|c1, r1) should be different from p(z|c1, r2), which means the N(µ1, σ1) is different from the N(µ2, σ2).\nAfter that, we use the φ(µ, σ) represent the KL(p(z|c1, r1)||p(z|c1)) + KL(p(z|c1, r2)||p(z|c2)), then we have:\nφ(µ, σ) = log( σ2\nσ1σ2 )\n+ σ21 + σ 2 2 + (µ1 − µ)2 + (µ2 − µ)2\n2σ2 − 1.\nAccording to the Lagrange Multiplier Method, we can calculate the conditional extremum and the extreme point (µ∗,σ∗) of φ(µ, σ).\nTo obtain the µ∗, we have to calculate the ∂φ(µ,σ) ∂µ :\n∂φ(µ, σ) ∂µ = ∂ (µ1−µ) 2+(µ2−µ)2 2σ2 ∂µ\n= 2µ− µ1 − µ2\nσ2 .\nLet the ∂φ(µ,σ)∂µ equals 0, we have the µ ∗=µ1+µ22 .\nIn the same way, to obtain the σ∗, we have:\n∂φ(µ, σ) ∂σ = ∂ log( σ\n2\nσ1σ2 )\n∂σ\n+ [σ21 + σ 2 2 + (µ1 − µ)2 + (µ2 − µ)2]\n∂ 1 2σ2\n∂σ\n= 2 σ − σ\n2 1 + σ 2 2 + (µ1 − µ)2 + (µ2 − µ)2\nσ3\n= 2σ2 − [σ21 + σ22 + (µ1 − µ)2 + (µ2 − µ)2]\nσ3 ,\nwhere a means the base of the logarithmic formula. Let the ∂φ(µ,σ)∂σ = 0, since the σ\n3 can not be 0, we have:\n2σ2 − [σ21 + σ22 + (µ1 − µ)2 + (µ2 − µ)2] = 0.\nTherefore, the σ∗ is:\nσ∗ =\n√ σ21 + σ 2 2 + (µ1 − µ)2 + (µ2 − µ)2\n2 .\nReplace the µ with the µ∗, we have:\nσ∗ =\n√ σ21 + σ 2 2 + (µ1−µ2)2 2\n2 .\nWe use a constant C to replace (µ1−µ2) 2 4 , the σ ∗ equals √ σ21+σ 2 2\n2 + C. The µ∗=µ1+µ22 means the latent variables sampled from this prior probability distribution easily tend to be different from the latent variables sampled form the posterior probability distributions. Since the latent variables are highly correlated with the generated responses, the responses generated through prior probability distribution would be different from that generated from posterior probability distributions. If the difference between µ1 and µ2 is very large, the σ∗ would be large too, thus resulting in high probability of more irrelevant latent variables.\nB The implementation of gradient blocking\nWe present the implementation of gradient blocking method in Algorithm 2. In Algorithm 2, we build a mask tensor Loss Mask to filter the loss results form each batch data, which can same obstruct the gradient backpropagation. Since we used gradient descent to optimize the neural model, the smallest loss result equals the largest variational lower bound. The elements in Loss Mask are 0 or 1, so Loss ∗ Loss Mask can be considered as the selection of the existing Loss.\nAlgorithm 2 Gradient blocking Input: Loss : loss-results of extended dialogue\ndata in one batch; N : the number of group information; BatchSize : the number of data contained on one Batch; Output: Loss Mask : the mask tensor with [0,1] elements;\n1: Loss← tf.reshape(Loss, [BatchSize, N ]) 2: ministLossPOSs ← tf.argmin(Loss, 1) #\nfind the posision of the minist loss; 3: ones← OnesTensor(1, dtype=tf.float32) 4: zeros← ZerosVector(1, dtype=tf.float32) 5: Loss Mask ← tf.cond(\ntf.equal(ministLossPOSs[0], tf.constant([0])[0], lambda:ones, lambda:zeros)\n6: for i = 1 to BatchSize do 7: for j = 1 to N do 8: if i = 1 and j = 1 then 9: continue\n10: else 11: Loss Mask ← tf.concat([\nLoss Mask, tf.cond( tf.equal(ministLossPOSs[i], tf.constant([j]))[0], lambda:ones, lambda:zeros)],0)\n12: end if 13: end for 14: end for 15: Pass Loss← Loss*Loss Mask 16: return Pass Loss"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating more interesting responses in neural conversation models with distributional",
      "author" : [ "Ashutosh Baheti", "Alan Ritter", "Jiwei Li", "Bill Dolan" ],
      "venue" : null,
      "citeRegEx" : "Baheti et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Baheti et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal Józefowicz", "Samy Bengio." ],
      "venue" : "CoNLL, pages 10–21. ACL.",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Group-wise contrastive learning for neural dialogue generation",
      "author" : [ "Hengyi Cai", "Hongshen Chen", "Yonghao Song", "Zhuoye Ding", "Yongjun Bao", "Weipeng Yan", "Xiaofang Zhao." ],
      "venue" : "EMNLP, pages 793– 802. Association for Computational Linguistics.",
      "citeRegEx" : "Cai et al\\.,? 2020",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical variational memory network for dialogue generation",
      "author" : [ "Hongshen Chen", "Zhaochun Ren", "Jiliang Tang", "Yihong Eric Zhao", "Dawei Yin." ],
      "venue" : "WWW, pages 1653–1662. ACM.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey E. Hinton." ],
      "venue" : "ICML, volume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved baselines with momentum contrastive learning",
      "author" : [ "Xinlei Chen", "Haoqi Fan", "Ross B. Girshick", "Kaiming He." ],
      "venue" : "CoRR, abs/2003.04297.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural conversational models with entropy-based data filtering",
      "author" : [ "Richard Csaky", "Patrik Purgai", "Gábor Recski." ],
      "venue" : "ACL (1), pages 5650–5669. Association for Computational Linguistics.",
      "citeRegEx" : "Csaky et al\\.,? 2019",
      "shortCiteRegEx" : "Csaky et al\\.",
      "year" : 2019
    }, {
      "title" : "Posterior-gan: Towards informative and coherent response generation with posterior generative adversarial network",
      "author" : [ "Shaoxiong Feng", "Hongshen Chen", "Kan Li", "Dawei Yin." ],
      "venue" : "AAAI, pages 7708–7715. AAAI Press.",
      "citeRegEx" : "Feng et al\\.,? 2020a",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Regularizing dialogue generation by imitating implicit scenarios",
      "author" : [ "Shaoxiong Feng", "Xuancheng Ren", "Hongshen Chen", "Bin Sun", "Kan Li", "Xu Sun." ],
      "venue" : "EMNLP, pages 6592–6604. Association for Computational Linguistics.",
      "citeRegEx" : "Feng et al\\.,? 2020b",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly optimizing diversity and relevance in neural response generation",
      "author" : [ "Xiang Gao", "Sungjin Lee", "Yizhe Zhang", "Chris Brockett", "Michel Galley", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "NAACL-HLT (1), pages 1229–1238. Association for Computational Linguis-",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "A knowledge-grounded neural conversation model",
      "author" : [ "Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley." ],
      "venue" : "AAAI, pages 5110–5117. AAAI Press.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2018",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogwae: Multimodal response generation with conditional wasserstein autoencoder",
      "author" : [ "Xiaodong Gu", "Kyunghyun Cho", "Jung-Woo Ha", "Sunghun Kim." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Emotional dialogue generation using image-grounded language models",
      "author" : [ "Bernd Huber", "Daniel J. McDuff", "Chris Brockett", "Michel Galley", "Bill Dolan." ],
      "venue" : "CHI, page 277. ACM.",
      "citeRegEx" : "Huber et al\\.,? 2018",
      "shortCiteRegEx" : "Huber et al\\.",
      "year" : 2018
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "HLT-NAACL, pages 110–119. The Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "HLT-NAACL, pages 110–119. ACL.",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P. Spithourakis", "Jianfeng Gao", "William B. Dolan." ],
      "venue" : "ACL (1). ACL.",
      "citeRegEx" : "Li et al\\.,? 2016c",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao." ],
      "venue" : "EMNLP, pages 1192–1202. ACL.",
      "citeRegEx" : "Li et al\\.,? 2016d",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial learning for neural dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Tianlin Shi", "Sébastien Jean", "Alan Ritter", "Dan Jurafsky." ],
      "venue" : "EMNLP, pages 2157–2169. ACL.",
      "citeRegEx" : "Li et al\\.,? 2017a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "IJCNLP(1), pages 986–995. Asian Federation of Natural Language Processing.",
      "citeRegEx" : "Li et al\\.,? 2017b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "EMNLP,",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "You impress me: Dialogue generation via mutual persona perception",
      "author" : [ "Qian Liu", "Yihong Chen", "Bei Chen", "Jian-Guang Lou", "Zixuan Chen", "Bin Zhou", "Dongmei Zhang." ],
      "venue" : "ACL, pages 1417–1427. Association for Computational Linguistics.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1412– 1421. The Association for Computational Linguistics.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9(86):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Representation learning via invariant causal mechanisms",
      "author" : [ "Jovana Mitrovic", "Brian McWilliams", "Jacob Walker", "Lars Buesing", "Charles Blundell." ],
      "venue" : "CoRR, abs/2010.07922.",
      "citeRegEx" : "Mitrovic et al\\.,? 2020",
      "shortCiteRegEx" : "Mitrovic et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation and sequence-to-sequence models: A tutorial",
      "author" : [ "Graham Neubig." ],
      "venue" : "CoRR, abs/1703.01619.",
      "citeRegEx" : "Neubig.,? 2017",
      "shortCiteRegEx" : "Neubig.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL, pages 311– 318. ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543. ACL.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "AAAI, pages 3776–3784. AAAI Press.",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "AAAI, pages 3295–3301. AAAI Press.",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "ACL (1), pages 1577–1586. ACL.",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "A conditional variational framework for dialog generation",
      "author" : [ "Xiaoyu Shen", "Hui Su", "Yanran Li", "Wenjie Li", "Shuzi Niu", "Yang Zhao", "Akiko Aizawa", "Guoping Long." ],
      "venue" : "ACL (2), pages 504–509. Association for Computational Linguistics.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "NIPS, pages 3483–3491.",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie." ],
      "venue" : "CIKM, pages 553–562. ACM.",
      "citeRegEx" : "Sordoni et al\\.,? 2015a",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "HLT-NAACL,",
      "citeRegEx" : "Sordoni et al\\.,? 2015b",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "News from OPUS—A Collection of Multilingual Parallel Corpora with Tools and Interfaces",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "Steering output style and topic in neural response generation",
      "author" : [ "Di Wang", "Nebojsa Jojic", "Chris Brockett", "Eric Nyberg." ],
      "venue" : "EMNLP, pages 2140–2150. Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Self-supervised dialogue learning",
      "author" : [ "Jiawei Wu", "Xin Wang", "William Yang Wang." ],
      "venue" : "ACL (1), pages 3857–3867. Association for Computational Linguistics.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Topic aware neural response generation",
      "author" : [ "Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma." ],
      "venue" : "AAAI, pages 3351– 3357. AAAI Press.",
      "citeRegEx" : "Xing et al\\.,? 2017",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2017
    }, {
      "title" : "Diversity-promoting GAN: A crossentropy based generative adversarial network for diversified text generation",
      "author" : [ "Jingjing Xu", "Xuancheng Ren", "Junyang Lin", "Xu Sun." ],
      "venue" : "EMNLP, pages 3940– 3949. ACL.",
      "citeRegEx" : "Xu et al\\.,? 2018a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Better conversations by modeling, filtering, and optimizing for coherence and diversity",
      "author" : [ "Xinnuo Xu", "Ondrej Dusek", "Ioannis Konstas", "Verena Rieser." ],
      "venue" : "EMNLP, pages 3981–3991. ACL.",
      "citeRegEx" : "Xu et al\\.,? 2018b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attribute2image: Conditional image generation from visual attributes",
      "author" : [ "Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee." ],
      "venue" : "ECCV (4), volume 9908 of Lecture Notes in Computer Science, pages 776–791. Springer.",
      "citeRegEx" : "Yan et al\\.,? 2016",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2016
    }, {
      "title" : "Seqgan: Sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "AAAI, pages 2852–2858. AAAI Press.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Reinforcing coherence for sequence to sequence model in dialogue generation",
      "author" : [ "Hainan Zhang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Xueqi Cheng." ],
      "venue" : "IJCAI, pages 4567–4573. ijcai.org.",
      "citeRegEx" : "Zhang et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating informative and diverse conversational responses via adversarial information maximization",
      "author" : [ "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan." ],
      "venue" : "NeurIPS, pages 1815–1825.",
      "citeRegEx" : "Zhang et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
      "author" : [ "Tiancheng Zhao", "Ran Zhao", "Maxine Eskénazi." ],
      "venue" : "ACL (1), pages 654–664. ACL.",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 170,
      "endOffset" : 269
    }, {
      "referenceID" : 31,
      "context" : "Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 170,
      "endOffset" : 269
    }, {
      "referenceID" : 33,
      "context" : "Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 170,
      "endOffset" : 269
    }, {
      "referenceID" : 47,
      "context" : "Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 170,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "Through using sampled latent variables, Conditional Variational AutoEncoders (CVAE) are powerful tools to ensure diversity and informativeness of the generated responses (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 170,
      "endOffset" : 269
    }, {
      "referenceID" : 8,
      "context" : "The challenge arises as human dialogues inherently exhibit the one-to-many and many-toone phenomena (Csaky et al., 2019), meaning that the same context could lead to very different responses, and different contexts could lead to the same response, respectively.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Different latent variables in a continuous latent space correspond to different responses (Bowman et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Finally, inspired from the contrastive learning paradigm (Cai et al., 2020; Chen et al., 2020a,b; Mitrovic et al., 2020), we propose relationship enhancement to increase similarity between the representations of data within the same group, and differentiate the representations of data between different groups.",
      "startOffset" : 57,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "Finally, inspired from the contrastive learning paradigm (Cai et al., 2020; Chen et al., 2020a,b; Mitrovic et al., 2020), we propose relationship enhancement to increase similarity between the representations of data within the same group, and differentiate the representations of data between different groups.",
      "startOffset" : 57,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : "Early dialogue models (Shang et al., 2015; Sordoni et al., 2015b) often tend to generate dull responses.",
      "startOffset" : 22,
      "endOffset" : 65
    }, {
      "referenceID" : 36,
      "context" : "Early dialogue models (Shang et al., 2015; Sordoni et al., 2015b) often tend to generate dull responses.",
      "startOffset" : 22,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "To improve the quality of these responses, two pathways have been adopted: one is to introduce external semantic information, such as dialogue history (Sordoni et al., 2015a; Serban et al., 2016), topic (Xing et al.",
      "startOffset" : 151,
      "endOffset" : 195
    }, {
      "referenceID" : 30,
      "context" : "To improve the quality of these responses, two pathways have been adopted: one is to introduce external semantic information, such as dialogue history (Sordoni et al., 2015a; Serban et al., 2016), topic (Xing et al.",
      "startOffset" : 151,
      "endOffset" : 195
    }, {
      "referenceID" : 40,
      "context" : ", 2016), topic (Xing et al., 2017), sentiment (Huber et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : ", 2017), sentiment (Huber et al., 2018), knowledge (Ghazvininejad et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : ", 2018), knowledge (Ghazvininejad et al., 2018), persona-style (Li et al.",
      "startOffset" : 19,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : ", 2018), persona-style (Li et al., 2016c), and other information (Li et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "The other is through more complex models or frameworks, such as attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015), reinforcement learning (RL) (Li et al.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "The other is through more complex models or frameworks, such as attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015), reinforcement learning (RL) (Li et al.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : ", 2015), reinforcement learning (RL) (Li et al., 2016d; Zhang et al., 2018a; Liu et al., 2020), generative adversarial network (GAN) (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 94
    }, {
      "referenceID" : 45,
      "context" : ", 2015), reinforcement learning (RL) (Li et al., 2016d; Zhang et al., 2018a; Liu et al., 2020), generative adversarial network (GAN) (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : ", 2015), reinforcement learning (RL) (Li et al., 2016d; Zhang et al., 2018a; Liu et al., 2020), generative adversarial network (GAN) (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 94
    }, {
      "referenceID" : 44,
      "context" : ", 2020), generative adversarial network (GAN) (Yu et al., 2017; Li et al., 2017a; Zhang et al., 2018b; Feng et al., 2020a), and variational reasoning (Bowman et al.",
      "startOffset" : 46,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : ", 2020), generative adversarial network (GAN) (Yu et al., 2017; Li et al., 2017a; Zhang et al., 2018b; Feng et al., 2020a), and variational reasoning (Bowman et al.",
      "startOffset" : 46,
      "endOffset" : 122
    }, {
      "referenceID" : 46,
      "context" : ", 2020), generative adversarial network (GAN) (Yu et al., 2017; Li et al., 2017a; Zhang et al., 2018b; Feng et al., 2020a), and variational reasoning (Bowman et al.",
      "startOffset" : 46,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : ", 2020), generative adversarial network (GAN) (Yu et al., 2017; Li et al., 2017a; Zhang et al., 2018b; Feng et al., 2020a), and variational reasoning (Bowman et al.",
      "startOffset" : 46,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : ", 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : ", 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : ", 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 135
    }, {
      "referenceID" : 47,
      "context" : ", 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : ", 2020a), and variational reasoning (Bowman et al., 2016; Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017; Chen et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "However, empirical evidences (Gao et al., 2019; Gu et al., 2019) have indicated that while the use of",
      "startOffset" : 29,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "However, empirical evidences (Gao et al., 2019; Gu et al., 2019) have indicated that while the use of",
      "startOffset" : 29,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "To alleviate this apparent issue, CVAE models have been used in combination with external information such as persona information, dialogue history and dialogue act (Shen et al., 2017; Serban et al., 2017; Zhao et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 31,
      "context" : "To alleviate this apparent issue, CVAE models have been used in combination with external information such as persona information, dialogue history and dialogue act (Shen et al., 2017; Serban et al., 2017; Zhao et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 47,
      "context" : "To alleviate this apparent issue, CVAE models have been used in combination with external information such as persona information, dialogue history and dialogue act (Shen et al., 2017; Serban et al., 2017; Zhao et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 224
    }, {
      "referenceID" : 39,
      "context" : ", 2020a,b) – are drawing increasing attention in NLP (Wu et al., 2019; Clark et al., 2020; Cai et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : ", 2020a,b) – are drawing increasing attention in NLP (Wu et al., 2019; Clark et al., 2020; Cai et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : ", 2020a,b) – are drawing increasing attention in NLP (Wu et al., 2019; Clark et al., 2020; Cai et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 108
    }, {
      "referenceID" : 34,
      "context" : "Precondition 3: To efficiently train a CVAE model, the Stochastic Gradient Variational Bayes (SGVB) framework (Sohn et al., 2015; Yan et al., 2016; Kingma and Welling, 2014) is adopted which aims to maximize the variational lower bound of the conditional log likelihood:",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 43,
      "context" : "Precondition 3: To efficiently train a CVAE model, the Stochastic Gradient Variational Bayes (SGVB) framework (Sohn et al., 2015; Yan et al., 2016; Kingma and Welling, 2014) is adopted which aims to maximize the variational lower bound of the conditional log likelihood:",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "Precondition 3: To efficiently train a CVAE model, the Stochastic Gradient Variational Bayes (SGVB) framework (Sohn et al., 2015; Yan et al., 2016; Kingma and Welling, 2014) is adopted which aims to maximize the variational lower bound of the conditional log likelihood:",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "This leads to the vanishing latent variable problem (Bowman et al., 2016).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Quoting the KL annealing trick (Bowman et al., 2016), α increases linearly from 0 to 1 in the first 10,000 batches.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "The first dataset, named DailyDialog (Li et al., 2017b), consists of dialogues that resemble human",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : "The second dataset, named OpenSubtitles (Tiedemann, 2009), includes a large collection of conversations converted from movie transcripts in English.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 29,
      "context" : "We utilize 300-dimensional GloVe embeddings (Pennington et al., 2014) to represent these dialogues in vectors.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "We use ppl (Neubig, 2017), response length and distinct-n (Li et al.",
      "startOffset" : 11,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "We use ppl (Neubig, 2017), response length and distinct-n (Li et al., 2016b) to evaluate the diversity of generated responses.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "We also use BLEU (Papineni et al., 2002) to evaluate the degree of the word-overlap between generated responses and ground truth.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Moreover, we use Embedding Average (Average) (Liu et al., 2016)) to evaluate the semantic relationship of generated responses and ground-truth responses.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 42,
      "context" : "Finally, we introduce the coherence (Xu et al., 2018b) to assess the coherence between contexts and generated responses.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 47,
      "context" : "They are all implemented based on a 2layer GRU kgCVAE model (Zhao et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 79
    } ],
    "year" : 2021,
    "abstractText" : "Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-toone phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts’ semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses’ relevance and coherence while maintaining their diversity and informativeness. SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group. Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in wellestablished open-domain dialogue datasets.",
    "creator" : "LaTeX with hyperref"
  }
}