{
  "name" : "2021.acl-long.261.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction",
    "authors" : [ "Hanqi Yan", "Lin Gui", "Gabriele Pergola", "Yulan He" ],
    "emails" : [ "yulan.he}@warwick.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3364–3375\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3364"
    }, {
      "heading" : "1 Introduction",
      "text" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020). In a typical dataset for Emotion Cause Extract (ECE) (Gui\n1Our code can be accessed at https://github.com /hanqi-qi/Position-Bias-Mitigation-in-Em otion-Cause-Analysis\net al., 2017), a document consists of multiple clauses, one of which is the emotion clause annotated with a pre-defined emotion class label. In addition, one or more clauses are annotated as the cause clause(s) which expresses triggering factors leading to the emotion expressed in the emotion clause. An emotion extraction model trained on the dataset is expected to classify a given clause as a cause clause or not, given the emotion clause.\nHowever, due to the difficulty in data collection, the ECE datasets were typically constructed by using emotion words as queries to retrieve relevant contexts as candidates for emotion cause annotation, which might lead to a strong positional bias (Ding and Kejriwal, 2020). Figure 1 depicts the distribution of positions of cause clauses relative to the emotion clause in the ECE dataset (Gui et al., 2016). Most cause clauses are either immediately preceding their corresponding emotion clauses or are the emotion clauses themselves. Existing ECE models tend to exploit such relative position information and have achieved good results on emotion cause detection. For example, The Rel-\native Position Augmented with Dynamic Global Labels (PAE-DGL) (Ding et al., 2019), RNNTransformer Hierarchical Network (RTHN) (Xia et al., 2019) and Multi-Attention-based Neural Network (MANN) (Li et al., 2019) all concatenate the relative position embeddings with clause semantic embeddings as the clause representations.\nWe argue that models utilising clause relative positions would inherently suffer from the dataset bias, and therefore may not generalise well to unseen data when the cause clause is not in proximity to the emotion clause. For example, in a recently released emotion cause dataset, only 25-27% cause clauses are located immediately before the emotion clause (Poria et al., 2020). To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop.\nTo alleviate the position bias problem, we propose to leverage the commonsense knowledge to enhance the semantic dependencies between a candidate clause and the emotion clause. More concretely, we build a clause graph, whose node features are initialised by the clause representations, and has two types of edges i.e., Sequence-Edge (SEdge) and Knowledge-Edge (K-Edge). A S-Edge links two consecutive clauses to capture the clause neighbourhood information, while a K-Edge links a candidate clause with the emotion clause if there exists a knowledge path extracted from the ConceptNet (Speer et al., 2017) between them. We extend Relation-GCNs (Schlichtkrull et al., 2018) to update the graph nodes by gathering information encoded in the two types of edges. Finally, the cause clause is detected by performing node (i.e., clause) classification on the clause graph. In summary, our contributions are three-fold:\n• We investigate the bias in the Emotion Cause Extraction (ECE) dataset and propose a novel strategy to generate adversarial examples in which the position of a candidate clause relative to the emotion clause is no longer the indicative feature for cause extraction. • We develop a new emotion cause extraction approach built on clause graphs in which nodes are clauses and edges linking two nodes capture the neighbourhood information as\nwell as the implicit reasoning paths extracted from a commonsense knowledge base between clauses. Node representations are updated using the extended Relation-GCN. • Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust when evaluating on the adversarial examples."
    }, {
      "heading" : "2 Related Work",
      "text" : "The presented work is closely related to two lines of research in emotion cause extraction: positioninsensitive and position-aware models. Position-insensitive Models. A more traditional line of research exploited structural representations of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al., 2015) for emotion cause extraction. Machine learning methods leveraged text features (Gui et al., 2017) and combined them with multi-kernel Support Vector Machine (SVM) (Xu et al., 2017). More recent works developed neural architectures to generate effective semantic features. Cheng et al. (2017b) employed LSTM models, Gui et al. (2017) made use of memory networks, while Li et al. (2018) devised a Convolutional Neural Network (CNN) with a co-attention mechanism. (Chen et al., 2018) used the emotion classification task to enhance cause extraction results. Position-aware Models. More recent methodologies have started to explicitly leverage the positions of cause clauses with respect to the emotion clause. A common strategy is to concatenate the clause relative position embedding with the candidate clause representation (Ding et al., 2019; Xia et al., 2019; Li et al., 2019). The Relative Position Augmented with Dynamic Global Labels (PAE-DGL) (Ding et al., 2019) reordered clauses based on their distances from the target emotion clause, and propagated the information of surrounding clauses to the others. Xu et al. (2019) used emotion dependent and independent features to rank clauses and identify the cause. The RNN-Transformer Hierarchical Network (RTHN) (Xia et al., 2019) argued there exist relations between clauses in a document and proposed to classify multiple clauses simultaneously. Li et al. (2019) proposed a Multi-Attention-based Neural Network (MANN) to model the interactions between a candidate clause and the emotion clause.\nThe generated representations are fed to a CNN layer for emotion cause extraction. The Hierarchical Neural Network (Fan et al., 2019) aimed at narrowing the gap between the prediction distribution p and the true distribution of the cause clause relative positions."
    }, {
      "heading" : "3 Knowledge-Aware Graph (KAG) Model for Emotion Cause Extraction",
      "text" : "We first define the Emotion Cause Extraction (ECE) task here. A document D contains N clauses D = {Ci}Ni=1, one of which is annotated as an emotion clause CE with a pre-defined emotion class label, Ew. The ECE task is to identify one or more cause clauses, Ct, 1 ≤ t ≤ N , that trigger the emotion expressed in CE . Note that the emotion clause itself can be a cause clause.\nWe propose a Knowledge-Aware Graph (KAG) model as shown in Figure 2, which incorporates knowledge paths extracted from ConceptNet for emotion cause extraction. More concretely, for each document, a graph is first constructed by representing each clause in the document as a node. The edge linking two nodes captures the sequential relation between neighbouring clauses (called the Sequence Edge or S-Edge). In addition, to bet-\nter capture the semantic relation between a candidate clause and the emotion clause, we identify keywords in the candidate clause which can reach the annotated emotion class label by following the knowledge paths in the ConceptNet. The extracted knowledge paths from ConceptNet are used to enrich the relationship between the candidate clause and the emotion clause and are inserted into the clause graph as the Knowledge Edge or K-Edge. We argue that by adding the K-Edges, we can better model the semantic relations between a candidate clause and the emotion clause, regardless of their relative positional distance.\nIn what follows, we will first describe how to extract knowledge paths from ConceptNet, then present the incorporation of the knowledge paths into context modelling, and finally discuss the use of Graphical Convolutional Network (GCN) for learning node (or clause) representations and the prediction of the cause clause based on the learned node representations."
    }, {
      "heading" : "3.1 Knowledge Path Extraction from ConceptNet",
      "text" : "ConceptNet is a commonsense knowledge graph, which represents entities as nodes and relationship between them as edges. To explore the causal re-\nlation between a candidate clause and the emotion clause, we propose to extract cause-related paths linking a word in the candidate clause with the annotated emotion word or the emotion class label, Ew, in the emotion clause. More concretely, for a candidate clause, we first perform word segmentation using the Chinese segmentation tool, Jieba2, and then extract the top three keywords ranked by Text-Rank3. Based on the findings in (Fan et al., 2019) that sentiment descriptions can be relevant to the emotion cause, we also include adjectives in the keywords set.\nWe regard each keyword in a candidate clause as a head entity, eh, and the emotion word or the emotion class label in the emotion clause as the tail entity, et. Similar to (Lin et al., 2019), we apply networkx4 to perform a depth-first search on the ConceptNet to identify the paths which start from eh and end at et, and only keep the paths which contain less than two intermediate entities. This is because shorter paths are more likely to offer reliable reasoning evidence (Xiong et al., 2017). Since not all relations in ConceptNet are related to or indicative of causal relations, we further remove the paths which contain any of these four relations: ‘antonym’, ‘distinct from’, ‘not desires’, and ‘not capable of ’. Finally, we order paths by their lengths in an ascending order and choose the top K paths as the result for each candidateemotion clause pair5.\nAn example is shown in Figure 3. The 5-th\n2https://github.com/fxsjy/jieba 3We have also experimented with other keyword extraction strategies, such as extracting words with higher TFIDF values or keeping all words after removing the stop words. But we did not observe improved emotion cause detection results.\n4http://networkx.github.io/ 5We set K to 15, which is the median of the number of paths between all the candidate-emotion clause pairs in our dataset.\nclause is annotated as the emotion clause and the emotion class label is ‘happiness’. For the keyword, ‘adopted’, in the first clause, we show two example paths extracted from ConceptNet, each of which links the word ‘adopted’ with ‘happiness’. One such a path is “adopted −related to→ acceptance −has subevent→ make better world −causes→ happiness”."
    }, {
      "heading" : "3.2 Knowledge-Aware Graph (KAG) Model",
      "text" : "As shown in Figure 2, there are four components in our model: a document encoding module, a context-aware path representation learning module, a GCN-based graph representation updating module, and finally a softmax layer for cause clause classification.\nInitial Clause/Document Representation Learning For each clause Ci, we derive its representation, Ci, by using a Bi-LSTM operating on its constituent word vectors, where each word vector wi ∈ Rd is obtained via an embedding layer. To capture the sequential relationship (S-Edges) between neighbouring clauses in a document, we feed the clause sequence into a transformer architecture. Similar to the original transformer incorporating the position embedding with the word embedding, we utilise the clause position information to enrich the clause representation. Here, the position embedding oi of each clause is concatenated with its representation Ci generated by Bi-LSTM.\nĈi = Transformer(Ci ||oi) (1)\nWe consider different ways for encoding position embeddings using either relative or absolute clause positions and explore their differences in the experiments section. In addition, we will also show the results without using position embeddings at all.\nSince the aim of our task is to identify the cause clause given an emotion clause, we capture the dependencies between each candidate clause and the emotion clause. Therefore, in the document context modelling, we consider the emotion clause ĈE , generated in a similar way as Ĉi, as the query vector, and the candidate clause representation Ĉi as both the key and value vectors, in order to derive the document representation, D ∈ Rd.\nContext-Aware Path Representation In Section 3.1, we have chosen a maximum of K paths {pt}Kt=1 linking each candidate Ci with the emotion clause. However, not every path correlates equally to the document context. Taking the document shown in Figure 3 as an example, the purple knowledge path is more closely related to the document context compared to the green path. As such, we should assign a higher weight to the purple path than the green one. We propose to use the document-level representation D obtained above as the query vector, and a knowledge path as both key and value vectors, in order to calculate the similarity between the knowledge path and the document context. For each pair of a candidate clause Ci and the emotion clause, we then aggregate the K knowledge paths to derive the contextaware path representation si ∈ Rd below:\nsi = K∑ t=1 αtpt αt = softmax( DTpt∑K j=1 D Tpj ) (2)\nwhere D is the document representation, pt is the path representation obtained from Bi-LSTM on a path expressed as an entity-relation word sequence.\nUpdate of Clause Representations by GCN After constructing a clause graph such as the one shown in Figure 2(c), we update the clause/node representations via S-Edges and K-Edges. Only clauses with valid knowledge paths to the emotion clause are connected with the emotion clause node.\nAfter initialising the node (or clause) in the clause graph with Ĉi and the extracted knowledge path with si, we update clause representation using an extended version of GCN, i.e. RelationGCNs (aka. R-GCNs) (Schlichtkrull et al., 2018), which is designed for information aggregation over multiple different edges:\nh`+1i = σ( ∑\nr∈RNi\n∑ j∈Ni 1 ci,r W `rh ` j +W ` 0h ` i) (3)\nwhere W `rh ` j is the linear transformed information from the neighbouring node j with relation r at\nthe `-th layer, W `r ∈ Rd×d is relation-specific, Ni is the set of neighbouring nodes of the i-th node, RNj is the set of distinct edges linking the current node and its neighbouring nodes.\nWhen aggregating the neighbouring nodes information along the K-Edge, we leverage the path representation si to measure the node importance. This idea is inspired by the translation-based models in graph embedding methods (Bordes et al., 2013). Here, if a clause pair contains a possible reasoning process described by the K-Edge, then ĥE ≈ ĥi + si holds. Otherwise, ĥi + si should be far away from the emotion clause representation ĥE .6 Therefore, we measure the importance of graph nodes according to the similarity between (hi + si) and hE . Here, we use the scaled DotAttention to calculate the similarity eiE and obtain the updated node representation zi.\nzi = softmax(eE)h`E eiE = (hi + si) ThE√ d (i 6= E)\n(4)\nwhere eE is {eiE}N−1i=1 . d is the dimension of graph node representations, and N rk is a set of neighbours by the K-Edge.\nThen, we combine the information encoded in SEdge with zi as in Eq. 3, and perform a non-linear transformation to update the graph node representation h`+1i :\nh`+1i = σ ( z`i + ∑ j∈Nrsi (Wjhj) )\n(5)\nwhere N rsi is a set of i-th neighbours connected by the S-Edges.\nCause Clause Detection Finally, we concatenate the candidate clause node hi and the emotion node representation he generated by the graph, and apply a softmax function to yield the predictive class distribution ŷi.\nŷi = softmax ( W (hLi ||hLE) + b ) , (6)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We conduct a thorough experimental assessment of the proposed approach against several state-of-theart models7.\n6Here, we do not consider the cases when the candidate clause is the emotion clause (i.e., ĥi = ĥE), as the similarity between ĥE + si and ĥE will be much larger than the other pairs.\n7Training and hyper-parameter details can be found in Appendix A.\nDataset and Evaluation Metrics The evaluation dataset (Gui et al., 2016) consists of 2,105 documents from SINA city news. As the dataset size is not large, we perform 10-fold cross-validation and report results on three standard metrics, i.e. Precision (P), Recall (R), and F1-Measure, all evaluated at the clause level.\nBaselines We compare our model with the position-insensitive and position-aware baselines: RB (Lee et al., 2010) and EMOCause (Russo et al., 2011) are rules-based methods. Multi-Kernel (Gui et al., 2016) and Ngrams+SVM (Xu et al., 2017) leverage Support Vector Machines via different textual feature to train emotion cause classifiers. CNN (Kim, 2014) and CANN (Li et al., 2018) are vanilla or attention-enhanced approaches. Memnet (Gui et al., 2017) uses a deep memory network to re-frame ECE as a question-answering task. Position-aware models use the relative position embedding to enhance the semantic features. HCS (Yu et al., 2019) uses separate hierarchical and attention module to obtain context and information. Besides that, PAE-DGL (Ding et al., 2019) and RTHN (Xia et al., 2019) use similar Global Prediction Embedding (GPE) to twist the clauses’ first-round predictions. MANN (Li et al., 2019) performs multi-head attention in CNN to jointly encode the emotion and candidate clauses. LambdaMART (Xu et al., 2019) uses the relative position, word-embedding similarity and topic similarity as emotion-related feature to extract cause."
    }, {
      "heading" : "4.1 Main Results",
      "text" : "Table 1 shows the cause clause classification results on the ECE dataset. Two rule-based methods have poor performances, possibly due to their pre-defined rules. Multi-Kernel performs better than the vanilla SVM, being able to leverage more contextual information. Across the other three groups, the precision scores are higher than recall scores, and it is probably due to the unbalanced number of cause clauses (18.36%) and non-cause clauses (81.64%), leading the models to predict a clause as non-cause more often.\nModels in the position-aware group perform better than those in the other groups, indicating the importance of position information. Our proposed model outperforms all the other models except RHNN in which its recall score is slightly lower. We have also performed ablation studies by removing either K-Edge or S-Edge, or both of them (w/o R-GCNs). The results show that removing the RGCNs leads to a drop of nearly 4.3% in F1. Also, both the K-Edge and S-Edge contributes to emotion cause extraction. As contextual modelling has considered the position information, the removal of S-Edge leads to a smaller drop compared to the removal of K-Edge."
    }, {
      "heading" : "4.2 Impact of Encoding Clause Position Information",
      "text" : "In order to examine the impact of using the clause position information in different models, we replace the relative position information of the candidate clause with absolute positions. In the extreme case, we remove the position information from the models. The results are shown in Figure 4. It can be observed that the best results are achieved using relative positions for all models. Replacing relative positions using either absolution positions or no position at all results in a significant performance drop. In particular, MANN and PAE-DGL have over 50-54% drop in F1. The performance degradation is less significant for RTHN, partly due to its use of the Transformer architecture for context modeling. Nevertheless, we have observed a decrease in F1 score in the range of 20-35%. Our proposed model is less sensitive to the relative positions of candidate clauses. Its robust performance partly attributes to the use of (1) hierarchical contextual modeling via the Transformer structure, and (2) the K-Egde which helps explore causal links via commonsense knowledge regardless of a clause’s\nrelative position."
    }, {
      "heading" : "4.3 Performance under Adversarial Samples",
      "text" : "In recent years, there have been growing interests in understanding vulnerabilities of NLP systems (Goodfellow et al., 2015; Ebrahimi et al., 2017; Wallace et al., 2019; Jin et al., 2020). Adversarial examples explore regions where the model performs poorly, which could help understanding and improving the model. Our purpose here is to evaluate if KAG is vulnerable as existing ECE models when the cause clauses are not in proximity to the emotion clause.Therefore, we propose a principled way to generate adversarial samples such that the relative position is no longer an indicative feature for the ECE task.\nGeneration of adversarial examples We generate adversarial examples to trick ECE models, which relies on swapping two clauses Cr1 and Cr2 , where r1 denotes the position of the most likely cause clause, while r2 denotes the position of the least likely cause clause.\nWe identify r1 by locating the most likely cause clause based on its relative position with respect to the emotion clause in a document. As illustrated in Figure 1, over half of the cause clauses are immediately before the emotion clause in the dataset. We assume that the position of a cause clause can be modelled by a Gaussian distribution and estimate the mean and variance directly from the data, which are, {µ, σ2} = {−1, 0.5445}. The position index r1 can then be sampled from the Gaussian distribution. As the sampled value is continuous, we round the value to its nearest integer:\nr1 ← bge, g v Gaussian(µ, σ2). (7)\nTo locate the least likely cause clause, we propose to choose the value for r2 according to the attention score between a candidate clause and the emotion clause. Our intuition is that if the emotion clause has a lower score attended to a candidate clause, then it is less likely to be the cause clause. We use an existing emotion cause extraction model to generate contextual representations and use the Dot-Attention (Luong et al., 2015) to measure the similarity between each candidate clause and the emotion clause. We then select the index i which gives the lowest attention score and assign it to r2:\nr2 = argmin i {λi}Ni=1, λi = Dot-Att.(Ĉi, ĈE), (8)\nwhere Ĉi is the representation of the i-th candidate clause, ĈE is the representation of the emotion clause, and N denotes a total of N clauses in a document.\nHere, we use existing ECE models as different discriminators to generate different adversarial samples.8 The desirable adversarial samples will fool the discriminator to predict the inverse label. We use leave-one-model-out to evaluate the performance of ECE models. In particular, one model is used as a Discriminator for generating adversarial samples which are subsequently used to evaluate the performance of other models.\nResults The results are shown in Table 2. The attacked ECE models are merely trained on the original dataset. The generated adversarial examples are used as the test set only. We can observe a significant performance drop of 23-32% for the existing ECE models, some of which even perform worse than the earlier rule-based methods, showing their sensitivity to the positional bias in the dataset. We also observe the performance degradation of our proposed KAG. But its performance drop is less significant compared to other models. The results verify the effectiveness of capturing the semantic dependencies between a candidate clause and the emotion clause via contextual and commonsense knowledge encoding."
    }, {
      "heading" : "4.4 Case Study and Error Analysis",
      "text" : "To understand how KAG aggregate information based on different paths, we randomly choose two examples to visualise the attention distributions (Eq. 4) on different graph nodes (i.e., clauses)\n8The adversarial sample generation is independent from their training process.\nin Figure 5.9 These attention weights show the ‘distance’ between a candidate clause and the emotion clause during the reasoning process. The cause clauses are underlined, and keywords are in bold. Ci in brackets indicate the relative clause position to the emotion clause (which is denoted as C0).\nEx.1 The crime that ten people were killed shocked the whole country (C−4). This was due to personal grievances (C−3). Qiu had arguments with the management staff (C−2), and thought the Taoist temple host had molested his wife (C−1). He became angry (C0), and killed the host and destroyed the temple (C1).\nIn Ex.1, the emotion word is ‘angry’, the knowledge path identified by our model from ConceptNet is, “arguments→ fight→angry” for Clause C−2, and “molest→ irritate→exasperate→angry” for Clause C−1. Our model assigns the same attention weight to the clauses C−2, C−1 and the emotion clause, as shown in Figure 5. This shows that both paths are equally weighted by our model. Due to the K-Edge attention weights, our model can correctly identify both C−2 and C−1 clauses as the cause clauses.\nEx.2 The LongBao Primary school locates between the two villages (C−2). Some unemployed people always cut through the school to take a shortcut (C−1). Liu Yurong worried that it would affect children’s study (C0). When he did not have teaching duties (C1), he stood guard outside the school gate (C2).\nIn Ex.2, the path identified by our model from ConceptNet for Clause (C−1) is “unemployment → situation→ trouble/danger→ worried”. It has\n9More cases can be found in the Appendix.\nbeen assigned the largest attention weight as shown in Figure 5. Note that the path identified is spurious since the emotion of ‘worried’ is triggered by ‘unemployment’ in the ConceptNet, while in the original text, ‘worried’ is caused by the event, ‘Unemployed people cut through the school’. This shows that simply using keywords or entities searching for knowledge paths from commonsense knowledge bases may lead to spurious knowledge extracted. We will leave the extraction of event-driven commonsense knowledge as future work."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we examine the positional bias in the annotated ECE dataset and investigate the degree of reliance of the clause position information in existing ECE models. We design a novel approach for generating adversarial samples. Moreover, we propose a graph-based model to enhance the semantic dependencies between a candidate clause and a given emotion clause by extracting relevant knowledge paths from ConceptNet. The experimental results show that our proposed method achieves comparative performance to the state-of-the-art methods, and is more robust against adversarial attacks. Our current model extracts knowledge paths linking two keywords identified in two separate clauses. In the future, we will exploit how to incorporate the event-level commonsense knowledge to improve the performance of emotion cause extraction."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was funded by the EPSRC (grant no. EP/T017112/1, EP/V048597/1). HY receives the PhD scholarship funded jointly by the University of Warwick and the Chinese Scholarship Council. YH is supported by a Turing AI Fellowship funded by the UK Research and Innovation (grant no. EP/V020579/1). We thank Yizhen Jia and\nDaoye Zhu for their valuable work on earlier code framework of this paper. We also thank the anonymous reviewers for their valuable comments."
    }, {
      "heading" : "A Model Architecture",
      "text" : "In this section, we describe the details of the four main components in our model: contextual modelling, knowledge path encoding, clause graph update and cause clause classification.\nThe dataset has 2,105 documents. The maximum number of clauses in a document is 75 and the maximum number of words per clause is 45. So we first pad the input documents into a matrix I with the shape of [2105, 75, 45].\nA.1 Contextual Modelling\na. token→ clause We first apply a 1-layer BiLSTM of 100 hidden units to obtain word embeddings, w ∈ R200. We then use two linear transformation layers (hidden units are [200,200],[200,1]) to map the original w to a scalar attention score α, then perform a weighted aggregation to generate the clause representation Ĉi ∈ R200. b. clause→ document We feed the clause representations into a Transformer. It has 3 stacked blocks, with the multi-head number set to 5, and the dimension of key, value, query is all set to 200. The query vector is the emotion clause representation ĈE ∈ R200, the key and value representations are candidate clause representations, also with 200 dimensions. Finally, the updated clause representations are aggregated via Dot-Attention to generate the document representation D ∈ R200.\nA.2 Knowledge Path Encoding\nFor each candidate clause and the emotion clause, we extract knowledge paths from ConceptNet and only select K paths. The values of K is set to 15, since the median of the number of paths between a candidate clause and the emotion clause is 15 in our dataset.\nWe use the same Bi-LSTM described in Section A.1 to encode each knowledge path and generate the K number of path representations {pit}Kt=1 between the i-th clause and the emotion clause. Then, the document representation D is applied as the query to attend to each path in {pit} to generate the final context-aware path representation si ∈ R200.\nA.3 Clause Graph Update\nThe graph nodes are initialised by clause presentations, with the feature dimension 200. To calculate the attention weights eiE in R-GCNs, We use the non-linearly transformed hi + si as the query, the non-linearly transformed hE as the value and key.\nThe non-linear functions are independent Selu layers.\nA.4 Cause Clause Classification The MLP with [400,1] hidden units takes the concatenation of each candidate node {hLi }Ni=1 and the emotion node representation hLE to predict the logit, after which, a softmax layer is applied to predict the probability of the cause clause."
    }, {
      "heading" : "B Training Details for KAG",
      "text" : "We randomly split the datasets into 9:1 (train/test). For each split, we run 50 iterations to get the best model on the validation set, which takes an average time of around 23 minutes per split, when conducted on a NVIDIA GTX 1080Ti. For each split, we test the model on the test set at the end of each iteration and keep the best resulting F1 of the split. The number of model parameters is 1,133,002.\nHyper-parameter Search We use the grid search to find the best parameters for our model on the validation data, and report in the following the hyper-parameter values providing the best performance.\n• The word embeddings used to initialise the Bi-LSTM is provided by NLPCC10. It was pre-trained on a 1.1 million Chinese Weibo corpora following the Word2Vec algorithm. The word embedding dimension is set to 200.\n• The position embedding dimension is set to 50, randomly initialised with the uniform distribution (-0.1,0.1).\n• The number of Transformer blocks is 2 and the number of graph layers is 3.\n• To regularise against over-fitting, we employ dropout (0.5 in the encoder, 0.2 in the graph layer).\n• The network is trained using the the Adam optimiser with a mini-batch size 64 and a learning rate η = 0.005. The parameters of our model are initialised with Glorot initialisation."
    }, {
      "heading" : "C Error Analysis",
      "text" : "We perform error analysis to identify the limitations of the proposed model. In the following examples (Ex.1 and Ex.2), the cause clauses are in bold, our predictions are underlined.\n10https://github.com/NUSTM/RTHN/tree/master/data\nEx.1 Some kind people said (C−6), if Wu Xiaoli could find available kidneys (C−5), they would like to donate for her surgery (C−4). 4000RMB donation had been sent to Xiaoli (C−3), Qiu Hua said (C−2). The child’s desire to survival shocked us (C−1). The family’s companion was touching (C0). Wish kind people will be ready to give a helping hand (C1). Help the family in difficulty (C2).\nIn the first example Ex.1, our model identifies the keyword survival in C−1 and extracts several paths from ‘survival’ to ‘touching’. However, the main event in clause C−1 concerns desire rather than survival. Our current model detects the emotion reasoning process from ConceptNet based on keywords identified in text, and inevitably introduces spurious knowledge paths to model learning.\nEx.2 I have only one daughter (C0), and a granddaughter of 8 year-old (C−10). I would like to convey these memory to her (C−9). Last Spring Festival (C−8), I gave the DVD away to my granddaughter (C−7). I hope she can inherit my memory (C−6). Thus (C−5), I feel like that my ages become eternity (C−4). Sun Qing said (C−3). His father is a sensitive and has great passion for his life (C−2). He did so (C−1). Making me feel touched (C0). His daughter said (C1).\nIn the Ex 2, our model detected the passion as a keyword and extracted knowledge paths between the clause C−2 and the emotion clause. However, it ignores the semantic dependency between the clause C−1 and the emotion clause. It is therefore more desirable to consider semantic dependencies or discourse relations between clauses/sentences for emotion reasoning path extraction from external commonsense knowledge sources."
    }, {
      "heading" : "D Human Evaluation on the Generated Adversarial Samples",
      "text" : "The way adversarial examples generated changes the order of the original document clauses. Therefore, we would like to find out if such clause reordering changes the original semantic meaning and if these adversarial samples can be used to evaluate on the same emotion cause labels.\nWe randomly selected 100 adversarial examples and ask two independent annotators to manually annotate emotion cause clauses based on the same annotation scheme of the ECE dataset. Compared to the original annotations, Annotator 1 achieved 0.954 agreement with the cohen’s kappa value of\n0.79, while Annotator 2 achieved 0.938 agreement with the cohen’s kappa value of 0.72. This aligns with our intuition that an emotion expressed in text is triggered by a certain event, rather than determined by relative clause positions. A good ECE model should be able to learn a correlation between an event and its associated emotion. This also motivates our proposal of a knowledge-aware model which leverages commonsense knowledge to explicitly capture event-emotion relationships."
    } ],
    "references" : [ {
      "title" : "2017a. An emotion cause",
      "author" : [ "Li", "Guodong Zhou" ],
      "venue" : null,
      "citeRegEx" : "Li and Zhou.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li and Zhou.",
      "year" : 2017
    }, {
      "title" : "2017b. An emotion cause",
      "author" : [ "Guodong Zhou" ],
      "venue" : null,
      "citeRegEx" : "Zhou.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhou.",
      "year" : 2017
    }, {
      "title" : "Hotflip: White-box adversarial",
      "author" : [ "jing Dou" ],
      "venue" : null,
      "citeRegEx" : "Dou.,? \\Q2017\\E",
      "shortCiteRegEx" : "Dou.",
      "year" : 2017
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "A question answering approach for emotion cause extraction",
      "author" : [ "Lin Gui", "Jiannan Hu", "Yulan He", "Ruifeng Xu", "Qin Lu", "Jiachen Du." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,",
      "citeRegEx" : "Gui et al\\.,? 2017",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2017
    }, {
      "title" : "Event-driven emotion cause extraction with corpus construction",
      "author" : [ "Lin Gui", "Dongyin Wu", "Ruifeng Xu", "Qin Lu", "Yu Zhou." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,",
      "citeRegEx" : "Gui et al\\.,? 2016",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2016
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Who feels what and why? annotation of a literature corpus with semantic roles of emotions",
      "author" : [ "Evgeny Kim", "Roman Klinger." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1345–1359, Santa Fe, New",
      "citeRegEx" : "Kim and Klinger.,? 2018",
      "shortCiteRegEx" : "Kim and Klinger.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "A text-driven rule-based system for emotion cause detection",
      "author" : [ "Sophia Yat Mei Lee", "Ying Chen", "Chu-Ren Huang." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages",
      "citeRegEx" : "Lee et al\\.,? 2010",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2010
    }, {
      "title" : "Context-aware emotion cause analysis with multi-attention-based neural network",
      "author" : [ "Xiangju Li", "Shi Feng", "Daling Wang", "Yifei Zhang." ],
      "venue" : "KnowledgeBased Systems, 174:205 – 218.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "A co-attention neural network model for emotion cause analysis with emotional context awareness",
      "author" : [ "Xiangju Li", "Kaisong Song", "Shi Feng", "Daling Wang", "Yifei Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portu-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence labeling vs",
      "author" : [ "Laura Oberländer", "Roman Klinger." ],
      "venue" : "clause classification for english emotion stimulus detection. In Proceedings of the 9th Joint Conference on Lexical and Computational Semantics (*SEM 2020), Barcelona, Spain. Association for",
      "citeRegEx" : "Oberländer and Klinger.,? 2020",
      "shortCiteRegEx" : "Oberländer and Klinger.",
      "year" : 2020
    }, {
      "title" : "Recognizing emotion cause in conversations",
      "author" : [ "Soujanya Poria", "Navonil Majumder", "Devamanyu Hazarika", "Deepanway Ghosal", "Rishabh Bhardwaj", "Samson Yu Bai Jian", "Romila Ghosh", "Niyati Chhaya", "Alexander Gelbukh", "Rada Mihalcea." ],
      "venue" : "arXiv",
      "citeRegEx" : "Poria et al\\.,? 2020",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling naive psychology of characters in simple commonsense stories",
      "author" : [ "Hannah Rashkin", "Antoine Bosselut", "Maarten Sap", "Kevin Knight", "Yejin Choi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Rashkin et al\\.,? 2018",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2018
    }, {
      "title" : "Emocause: An easy-adaptable approach to extract emotion cause contexts",
      "author" : [ "Irene Russo", "Tommaso Caselli", "Francesco Rubino", "Ester Boldrini", "Patricio Martı́nez-Barco" ],
      "venue" : "In Proceedings of the 2nd Workshop on Computational Approaches",
      "citeRegEx" : "Russo et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2011
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne vanden Berg", "Max Welling." ],
      "venue" : "European Semantic Web Conference.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing nlp",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:1908.07125.",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "author" : [ "Rui Xia", "Zixiang Ding." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:",
      "citeRegEx" : "Xia and Ding.,? 2019",
      "shortCiteRegEx" : "Xia and Ding.",
      "year" : 2019
    }, {
      "title" : "RTHN: A rnn-transformer hierarchical network for emotion cause extraction",
      "author" : [ "Rui Xia", "Mengran Zhang", "Zixiang Ding." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, Au-",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Deeppath: A reinforcement learning method for knowledge graph reasoning",
      "author" : [ "Wenhan Xiong", "Thien Hoang", "William Yang Wang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), Copen-",
      "citeRegEx" : "Xiong et al\\.,? 2017",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Extracting emotion causes using learning to rank methods from an information retrieval perspective",
      "author" : [ "B. Xu", "H. Lin", "Y. Lin", "Y. Diao", "L. Yang", "K. Xu." ],
      "venue" : "IEEE Access, 7:15573–15583.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "An ensemble approach for emotion cause detection with event extraction and multikernel svms",
      "author" : [ "Ruifeng Xu", "Jiannan Hu", "Qin Lu", "Dongyin Wu", "Lin Gui." ],
      "venue" : "Tsinghua Science and Technology, 22(6):646–659.",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "Multiple level hierarchical network-based clause selection for emotion cause extraction",
      "author" : [ "Xinyi Yu", "Wenge Rong", "Zhuo Zhang", "Yuanxin Ouyang", "Zhang Xiong." ],
      "venue" : "IEEE Access, 7:9071–9079.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020).",
      "startOffset" : 159,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020).",
      "startOffset" : 159,
      "endOffset" : 293
    }, {
      "referenceID" : 21,
      "context" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020).",
      "startOffset" : 159,
      "endOffset" : 293
    }, {
      "referenceID" : 7,
      "context" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020).",
      "startOffset" : 159,
      "endOffset" : 293
    }, {
      "referenceID" : 14,
      "context" : "Instead of detecting sentiment polarity from text, recent years have seen a surge of research activities that identify the cause of emotions expressed in text (Gui et al., 2017; Cheng et al., 2017a; Rashkin et al., 2018; Xia and Ding, 2019; Kim and Klinger, 2018; Oberländer and Klinger, 2020).",
      "startOffset" : 159,
      "endOffset" : 293
    }, {
      "referenceID" : 5,
      "context" : "Figure 1: The distribution of positions of cause clauses relative to their corresponding emotion clauses in the ECE dataset (Gui et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Figure 1 depicts the distribution of positions of cause clauses relative to the emotion clause in the ECE dataset (Gui et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : ", 2019), RNNTransformer Hierarchical Network (RTHN) (Xia et al., 2019) and Multi-Attention-based Neural Network (MANN) (Li et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : ", 2019) and Multi-Attention-based Neural Network (MANN) (Li et al., 2019) all concatenate the relative position embeddings with clause semantic embeddings as the clause representations.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "For example, in a recently released emotion cause dataset, only 25-27% cause clauses are located immediately before the emotion clause (Poria et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "exists a knowledge path extracted from the ConceptNet (Speer et al., 2017) between them.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "We extend Relation-GCNs (Schlichtkrull et al., 2018) to update the graph nodes by gathering information encoded in the two types of edges.",
      "startOffset" : 24,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Machine learning methods leveraged text features (Gui et al., 2017) and combined them with",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "multi-kernel Support Vector Machine (SVM) (Xu et al., 2017).",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "A common strategy is to concatenate the clause relative position embedding with the candidate clause representation (Ding et al., 2019; Xia et al., 2019; Li et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "A common strategy is to concatenate the clause relative position embedding with the candidate clause representation (Ding et al., 2019; Xia et al., 2019; Li et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "The RNN-Transformer Hierarchical Network (RTHN) (Xia et al., 2019) argued there exist relations between clauses in a document and proposed to classify multiple clauses simultaneously.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Similar to (Lin et al., 2019), we apply networkx4 to perform a depth-first search on the ConceptNet to identify the paths which start from",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "This is because shorter paths are more likely to offer reliable reasoning evidence (Xiong et al., 2017).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "R-GCNs) (Schlichtkrull et al., 2018), which is designed for information aggregation over multiple different edges:",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Dataset and Evaluation Metrics The evaluation dataset (Gui et al., 2016) consists of 2,105 documents from SINA city news.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Baselines We compare our model with the position-insensitive and position-aware baselines: RB (Lee et al., 2010) and EMOCause (Russo et al.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : ", 2010) and EMOCause (Russo et al., 2011) are rules-based methods.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "Multi-Kernel (Gui et al., 2016) and Ngrams+SVM (Xu et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : ", 2016) and Ngrams+SVM (Xu et al., 2017) leverage Support Vector Machines via different textual feature to train emotion cause classifiers.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "CNN (Kim, 2014) and CANN (Li et al., 2018) are vanilla or attention-enhanced approaches.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Memnet (Gui et al., 2017) uses a deep memory network to re-frame ECE as a question-answering task.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "HCS (Yu et al., 2019) uses separate hierarchical and attention module to obtain context and information.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and RTHN (Xia et al., 2019) use similar Global Prediction Embedding (GPE) to twist the clauses’ first-round predictions.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "MANN (Li et al., 2019) performs multi-head attention in CNN to jointly encode the emotion and candidate clauses.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "LambdaMART (Xu et al., 2019) uses the relative position, word-embedding similarity and topic similarity as emotion-related feature to extract cause.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "We use an existing emotion cause extraction model to generate contextual representations and use the Dot-Attention (Luong et al., 2015) to measure the similarity between each candidate clause and the emotion clause.",
      "startOffset" : 115,
      "endOffset" : 135
    } ],
    "year" : 2021,
    "abstractText" : "The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing stateof-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.1",
    "creator" : "LaTeX with hyperref"
  }
}