{
  "name" : "2021.acl-long.241.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Database Reasoning Over Text",
    "authors" : [ "James Thorne", "Majid Yazdani", "Marzieh Saeidi", "Fabrizio Silvestri", "Sebastian Riedel", "Alon Halevy" ],
    "emails" : [ "jt719@cam.ac.uk", "myazdani@fb.com", "marzieh@fb.com", "@diag.uniroma1.it", "sriedel@fb.com", "ayh@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3091–3104\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3091"
    }, {
      "heading" : "1 Introduction",
      "text" : "Question answering (QA) over text has made significant strides in recent years owing to the availability of new datasets and models. Machines have surpassed human performance on the well-known SQUaD task (Rajpurkar et al., 2016) where models extract answer spans from a short passage of text. The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen\n1https://github.com/facebookresearch/ NeuralDB\net al., 2017; Lewis et al., 2020b; Izacard and Grave, 2020). More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, which may have low lexical or semantic similarity with the question.\nThis paper considers the problem of answering questions similar to database queries, such as those shown in Figure 1. For example, the query “List all the female athletes in Wikipedia who were born in the 20th century”, requires reasoning over hundreds or thousands of facts, retrieved from multiple Wikipedia pages, and applying set-based filters to them (e.g., gender, birth date). If our query further asked how many such athletes exist, we would\nhave to perform an aggregation function to count the result set. The ability to answer the aforementioned queries would enable a new kind of database (Thorne et al., 2021) where facts can be described in natural language and would therefore obviate the need for a pre-defined schema, which is a major limitation of current database systems. An example application for such flexible text databases exists in the area of storing knowledge for personal assistants where users store data about their habits and experiences, their friends and their preferences, for which designing a schema is impractical.\nWe introduce WIKINLDB, a benchmark dataset for exploring database reasoning over facts expressed in natural language. WIKINLDB contains a number of query types that require systems to return large set-based answers and aggregate over these (with operators such as count, min, and max). Our dataset is generated using publicly available knowledge graph data, enabling large volumes of instances to be generated with minimal effort. Most queries in WIKINLDB require reasoning over hundreds of facts to generate answers, exposing limitations in current neural models. In contrast to DROP (Dua et al., 2019) where queries are answered over single passages, and bAbI (Weston et al., 2015), where each query is based on a context of less than 20 facts, our dataset scales from databases of 25 instances to 1000, and could be extended further.\nWe also introduce a modular architecture to support database reasoning over text and characterize its behavior on our reference dataset. We find that even on small databases of 25 facts, naive application of transformers is insufficient. When provided with only the relevant facts, the baseline yields an answer accuracy of 85%, whereas applying our proposed architecture yields 90% by better answering queries, such as count, that require computation. It is well known that transformer models do not scale well to large inputs due to the use of selfattention. We found that mechanisms such as Fusion in Decoder (Izacard and Grave, 2020, FiD) and LongFormer (Beltagy et al., 2020), which mitigate the scaling issue, harm the model: combining more than 2 facts with FiD resulted in answer accuracies of 76% and 39%, respectively. These issues were mitigated by our approach which generates intermediate query-based derivations of small numbers of facts in the database, before using conventional computation to aggregate the results."
    }, {
      "heading" : "2 Answering Database Queries over Text",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Definition",
      "text" : "We refer to corpora that consist of unordered collections of facts expressed as short natural language sentences as Natural Language Databases (NLDBs). For example, a corpus may include all the utterances given to a personal assistant by its user, or all the claims uttered by a political figure. The texts in our corpora are similar to databases as they are sets of stand-alone facts. But unlike a database, they are not expressed as rows or triples in a pre-defined schema. For example, a sentence containing a single fact, “Gustavo likes espresso” or multiple facts, such as “Robertson Howard, who attended the University of Virginia, is buried in the Congressional Cemetery”.\nA query Q over a database, D, produces a set of answers: Q(D) = {a1, . . . , al}. We consider the following four query types (see examples in Table 5): (1) Set queries are extractive queries that return a list of spans, such as entities, from the facts. (2) Boolean queries return a True/False answer. (3) Aggregation queries require computation over answer sets with an operator, such as count, min and max. For example: “How many people work for Yale Law School?”). (4) Join queries require the combination of two (or more) facts to produce each answer. We combine join operations with set, Boolean and aggregation queries. For example, the query “Who works in a company in France?” considers both the relationship between people and employer as well as company locations."
    }, {
      "heading" : "2.2 Challenges",
      "text" : "The NLP treatment of question answering, where systems encode the query and context (containing the background knowledge), forms a good starting point for NLDBs. Common model architectures are based on the transformer (Vaswani et al., 2017) in an encoder-decoder configuration. The encoder uses self-attention to conditionally encode the context with the query and the decoder allows conditional generation of outputs that are not necessarily present in the input. To scale question answering to reason over large knowledge-sources such as Wikipedia, task formulations typically retrieve textspans from a corpus to condition answer generation (Chen et al., 2017; Dhingra et al., 2017). However, several challenges encountered in NLDBs preclude direct application of these techniques:\nScale To scale neural reasoning to databases of non-trivial size, it would not be feasible to encode the entire database as input to the transformer. Question answering systems combine a retrieval mechanism to select relevant spans from knowledge sources as context. This task is usually referred to as open-domain QA (Lewis et al., 2020a; Izacard and Grave, 2020). It is common to use a maximum input size of 512 or 1024 tokens for context. While extensions such as Linformer (Wang et al., 2020), Longformer (Beltagy et al., 2020) and Fusion in Decoder (Izacard and Grave, 2020) enable larger contexts to be encoded, their application of self-attention varies and the number of tokens that may be encoded is limited by GPU memory.\nMultiple answer spans The NLP formulation of question answering typically requires extracting a span from a single document or generating a short answer. Answering queries in a NLDB may require processing a large number of facts, generating a large number of items as answer, hundreds or thousands, and performing aggregations over large sets.\nLocality and document structure NLDBs do not enjoy the locality properties that usually hold in open-domain QA. In NLDBs, a query may be dependent on multiple facts that can be anywhere in the database. In fact, by definition, the current facts in a database can be reordered and the query answers should not change. In contrast, in opendomain QA, the fact needed to answer a given question is typically located in a paragraph or document with multiple sentences about the same subject, in combination with a document title, where this additional context may help information recall.\nConditional retrieval Similar to open-domain question answering, NLDBs mandate an information retrieval component. When determining which\nfacts to input to the model, NLDBs may require conditional retrieval from the database. For example, to answer the query “Whose spouse is a doctor?” we’d first need to fetch spouses and then their professions. Recent work on multi-hop query answering (e.g., Asai et al. (2019)), has started considering this issue but is restricted to the case where we’re looking for a single answer. In NLDBs, we may need to perform multi-hops for sets of facts."
    }, {
      "heading" : "3 Architecture for querying NLDBs",
      "text" : "To address the aforementioned challenges, we propose an instance of a Neural Database architecture (Thorne et al., 2021) that operates over textual facts with parallelizable non-blocking operators before aggregating the results. The three core components of the architecture, shown in Figure 2, are a Support Set Generator (SSG) which retrieves small sets of relevant facts called support sets, a parallelizable non-blocking Select-Project-Join (SPJ) operator which generates intermediate answers that can be unioned to produce the final answer, and an optional aggregation stage which uses conventional computation to perform numerical reasoning. The key insight underlying our architecture is to leverage neural models for what they excel at, namely, reasoning over a small set of facts.\nNeural SPJ Operator Given a single support set and a query, the SPJ (Select-Project-Join) operator outputs a machine readable intermediate representation of the answer that can be generated from the support set. For example, given the query “Who was born in Montevideo?” and the support set {“Mario Sagario was born in Montevideo, Uruguay, ...”}, the Neural SPJ would output the entity literal Mario Sagario. Examples of outputs are provided in Figure 3.\nThe SPJ operator is performing three functions:\n(1) for support sets that are insufficient to answer a question, the operator should return no output; (2) for queries that require short chains of reasoning over multiple facts, the SPJ operator joins the facts when generating the output; and (3) the SPJ generates a projection of the support set to a machine readable format dependent on the given query, and whether computation or aggregation is required.\nBecause the SPJ operator is run in parallel, it can scale independently of the limitations on the size of the input of a single transformer. In contrast, the use of self-attention when encoding all facts as one input precludes parallelization, has high latency, and is limited by the memory required to compute the self-attention. By using the SPJ operator to perform query-dependent information extraction, aggregations can be performed over the generated outputs using conventional computation, which trivially scales to thousands of operands. Furthermore, this allows large result sets to be generated by the model, whereas accurately decoding long sequences using an encoder-decoder architecture remains an open challenge (Hupkes et al., 2020).\nSupport Set Generator (SSG) A support set contains the minimal subset of sentences from the database needed to generate one single operand for the aggregation module by the SPJ operator. For example, for queries that are answered by a single sentence, e.g., “Who is Sheryl’s husband?”, the support set containing a single fact should be returned, e.g., {“Sheryl is Nicholas’s spouse”}. The output of the support set generator is a set of support sets, each of which is fed independently to a downstream SPJ module. Support sets may not be pairwise disjoint because some facts may be required for multiple answers.\nThe SSG output should satisfy the following two properties: (1) If multiple facts are needed to produce an intermediate answer, they should all be in the support set. For example, if we queried “When was Sheryl’s husband born?”, the support set should include a fact stating who the spouse is and a fact describing when they were born. (2) When performing aggregation, or outputting a set of answers, multiple support sets must be generated, each containing enough information to generate the intermediate results that are aggregated. For example, for the query “Who is the oldest person?”, each of the support sets would independently contain a fact that includes a person and indicates their age.\nAggregation The outputs of the SPJ modules are intermediate answers to the query. For some queries, e.g., “who lives in London?”, the final answer is simply the union of the intermediate answers. In other cases, e.g., “how many countries grow coffee?”, an aggregation operator needs to be applied to the union of intermediate answers. Because output of the SPJ operators are machine readable, we can hence guarantee accuracy and scalability by performing aggregation using conventional computation. In this paper, we consider the aggregation functions min, max and count."
    }, {
      "heading" : "4 The WIKINLDB dataset",
      "text" : "In this section we introduce WIKINLDB, a novel dataset for training NLDBs which is generated by transforming structured data from Wikidata (Vrandečić and Krötzsch, 2014) into natural language facts and queries. Wikidata stores triples of the form (S,R,O), where R is a relationship between the subject S and the object O, e.g., (Tim Cook, employedBy, Apple). The scale and breadth of Wikidata enables us to generate databases of many sizes and variety.\nFacts To automate generation of questions and answers, sentences must be grounded in Wikidata identifiers. One approach to generate facts would be to use templates or collect them through grounded information extraction datasets such as T-REx (Elsahar et al., 2018). However, to ensure wider linguistic variety as well as accuracy of the mapping, we use verbalizations of knowledge graph triples that are synthesized through a sequence to sequence model. Concretely, we use generated sentences from KELM (Agarwal et al., 2020), which are not grounded with Wikidata IDs, and generate a post-hoc mapping back to Wikidata.For example, given the sentence: “The Slice of Life manga series The Film Lives On was written by Osamu Tezuka.” we map it to the Wikidata triple (Q11332517,P50,Q193300). Our mapping is a two-step process: firstly, we look up entity names from Wikipedia, returning multiple matches for Osamu Tezuka, and secondly filter these based on which have an author relations to The Slice of Life in the Wikidata graph. While out of scope for this paper, this technique could be applied to generate training datasets for novel domains. WIKINLDB uses both atomic facts in KELM (about a single relation of an entity) or composite facts (about multiple relations).\nQueries Following previous work on large-scale question answering (Hartmann et al., 2018; Talmor and Berant, 2018), queries are generated using templates. For each relation and operator, multiple templates were written by the authors where placeholders can be replaced with the subject and objects for each relation. While multiple templates are used to ensure variety, these are limited in diversity in comparison to the facts. Templates were generated for the first 25 relations on Wikidata with mapped data in KELM. To generate queries that require joins we apply the same technique, combining to combine two or more connected relations, chaining the entities. We further select the 15 most popular relations and generate additional templates which chain the two relations. For example, we chain (Y,locatedIn,Z) and (X,employedBy,Y) to create a template for the query “Does $X work at a company based in $Z?”.\nData Quality We manually inspect randomly selected queries and facts and score them using the categories introduced in this section. For queries, we sample 70 instances, 10 for each query type. We score each query for fluency and intelligibility. Out of 70 queries, only one question was marked as non-fluent due to a typo which was corrected for the final dataset. All 70 queries were intelligible. We observed that the clarity of some queries depended on the facts in the database to provide context (e.g. “Who is male?”), but otherwise met the task requirements.\nTo assess the quality of mapped facts from KELM, a sample of 50 was evaluated based on 6 categories: intelligibility, fluency, inclusivity (conveying information from all the mapped relations), faithfulness to these relations, and whether extraneous information (not in the mapped relations) is present. 49/50 facts were intelligible and 45/50 facts were fluent. The remaining 5 had redundant information or missing conjunctions. 50/50 facts contained all mapped relations and 48/50 were faithful to these relations. 8/50 facts had extraneous information for relations that could not be mapped. The relations that could not be mapped are not used for query generation and did not affect how answers were automatically generated.\nWIKINLDB Statistics We create databases over 25 common relationships from Wikidata, and create 643 templates from which queries are phrased. For join-type queries, we chain a fur-\nther 15 relations with a further 86 template fragments. The relations we chose were selected from a weighted sample of the most common entity types in KELM. In total, we generate five variants of the dataset containing databases of size 25 to 1000 facts where each fact has between 30-50 tokens. Dataset statistics are reported in Table 1."
    }, {
      "heading" : "5 Models",
      "text" : ""
    }, {
      "heading" : "5.1 Neural Select-Project-Join",
      "text" : "The SPJ operator is trained as a sequence-tosequence model to generate intermediate results from a support set and a given query. All facts in the support set are concatenated with the query before being input to a transformer model.\nThe model is trained to output different derivations depending on the query type. For the min, max operators, the projection is a machinereadable key-value pair, illustrated in Figure 3. For example “which place has the highest yearly number of visitors?” has the projection of the form: (place, number of visitors) allowing an argmax operation by the downstream aggregation module. For queries with Boolean answers, the output is a token indicating whether the answer is true or false. And for all other queries where a set of results is returned or counted, the output is\nsimply a span, such as an entity or numerical value, extracted from the support set.\nEven though we use intermediary annotation for the SPJ operator, we believe that collecting such annotation is a simpler labeling task compare to collecting the answers to the queries. For example, given the fact “Serena Jameka Williams (born September 26, 1981) is an American professional tennis player and former world No.” and the query “List all the female athletes who were born in 20th centure.”, it seems relatively simple to provide the label “Serena Jameka Williams”. However, it is non-trivial to produce a list of potentially hundreds of entities as answer (e.g. [“Serena Jameka Williams, Simona Halep, Mary Lou Retton, Megan Rapinoe, Kim Simmone, Mary Abichi, . . .”]). The training of the components in our proposed architecture does not depend on the final answer and instead, on the simpler intermediary labels.\nPredicting Aggregation Operator Rather than using a separate classifier to predict the question type, we encode the choice of operator as a special token that is predicted by the SPJ operator prepended to the model output (Figure 3). The aggregation operator is chosen using a majority vote over all generated derivations from all support sets.\nNegative Example Generation It is important for the SPJ to be resilient to extraneous facts that might be returned by a low-precision high-recall SSG. Negative instances for training are generated in two ways: (1) queries are paired with randomly sampled facts and the model is trained to generate a NULL projection (indicating the support set does not contribute to the answer). For example, a fact about someone’s date of birth isn’t useful when answering a query about the visitor count of an attraction. (2) for a portion of the training instances, we additionally sample extraneous unrelated facts and append these to the support sets simulating false-positive facts from the SSG."
    }, {
      "heading" : "5.2 Support Set Generator",
      "text" : "For simple queries over single facts, conventional information retrieval, such as TF·IDF could be considered a primitive SSG. However, this would not scale for joins, aggregation queries or for queries outputting a set of answers as generating relevant sets requires incremental decoding, conditioning on already retrieved facts.\nNaively generating the set of all relevant support sets, SSGQ(D) ⊂ P(D), would be intractable as\nAlgorithm 1: SSG modeled as multi-label classification: using maximum inner product search (MIPS) over vector encodings of facts U and state V\nInput: Bi-encoders C: CU (for actions), CV (for state), Database D, Query Q, Threshold τ Output: Set of support sets (D̂1, . . . , D̂b) ⊂ P(D) open := {{}} closed := {}; U := [CU (u1); . . . ;CU (un);CU (STOP)] for ui ∈ D; while open 6= {} do\nnext := {}; for D̂k in open do\nV := [CV (Q, u1 . . . um)], for ui ∈ D̂k; A := MIPS(U ,V ,τ ); for aj in A do\nif aj == STOP then closed := closed ∪{D̂k}; else next := next ∪{{aj ∪ D̂k}};\nopen := next; return closed;\nit is akin to enumerating the powerset. We construct support sets efficiently by taking an incremental approach, starting from the empty set (see Algorithm 1). At each step, the classifier considers the partially generated support set D̂k and the query and predicts which candidate facts ui ∈ D from the database should be added, or whether to stop the iteration, these choices being modeled as a multilabel classification task. If STOP is predicted, the partial result set D̂k is closed (i.e., it forms part of the output); otherwise, for each fact added, a new intermediate (open) support set is generated which is explored in the next iteration. For efficiency, we use a bi-encoder architecture that independently encodes the facts in the database and the state (query and a partial support set) and computes the inner product between the encoded representations to generate a score: CU (ui)TCV (Q, D̂k). The encoders are pre-trained transformers fine-tuned to yield a high inner product between the state’s encodings and relevant facts to be added. At prediction time, the vectors encoding the facts are static and are pre-computed offline. At each step, t, we encode the state using a transformer by concatenating the query tokens and the facts in the partially generated support set Dk. The SSG is trained with full supervision of all partial support sets from the dataset and trained to predict which facts to add to the support set using a contrastive loss.\nComplexity of SSG The inner loop of Algorithm 1 involves a Maximum Inner Product Search (MIPS) between the encoded state and the encodings of the facts, which is linear in the number of facts. Approximate search, such as FAISS (Johnson et al., 2019), accelerate retrieval to O(log2 n). If we assume a query needs a maximum of b support sets, and the average size of a support set is m, then the complexity of the SSG algorithm is O(bm log2 n). Both b and m are bounded by the number of facts in the database n, but in practice we’d expect only one of b or m factors to be large. However, there is fertile ground for developing methods for indexing (and/or clustering) the facts in the database so that only few facts need to be considered in each iteration of the inner loop of the algorithm, leading to significant speedups."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compare our proposed architecture to transformer-based models that explore the effect of three attention mechanisms representative of the state-of-the-art. Self-attention in transformers captures both inter-fact as well as intra-fact interactions between tokens. However, computing self-attention is quadratic with respect to memory and scaling beyond 1024 tokens is non-trivial. In our baselines, the task formulation is a sequence to sequence model, similar to that used in question answering. All (relevant) facts are encoded with the query and the transformer is trained to predict the answer without using any intermediate representations. We compare full self-attention against independently encoding the facts (in the context of the query) and fusing the embeddings in the decoder (Izacard and Grave, 2020, Fusion in Decoder (FiD)). Because FiD independently encodes contexts, run-time complexity is reduced to be linear with respect to the number of facts at the expense of not having inter-fact attention. We additionally compare to using windowed attention over facts with global attention to the query using Longformer (Beltagy et al., 2020). Inter-fact attention is captured only within the window."
    }, {
      "heading" : "6 Implementation",
      "text" : "We use the HuggingFace (Wolf et al., 2020) transformers library and its implementations of T5 and Longformer. For SSG, we use BERT to generate encodings, which has a comparable architecture to T5. The learning-rate for fine-tuning and number\nof epochs were selected through maximizing the Exact-Match (EM) accuracy on a held-out validation set for the tasks. For each experiment, we train 3 separate models with different seeds and report mean accuracy. The SPJ models are only trained on the small database of 25 facts and applied to larger databases at test time.\nFor most queries, we measure correctness using Exact Match (EM), which is 1 if the answer string generated by the model is exactly equal to the reference answer and 0 otherwise. This metric is used to score outputs where either a Boolean, null answer, string or numeric answer is expected. When a set of results is returned, we compute the F1 score considering exact matches of set elements. When comparing models and reporting results, we report macro-averages over all instances in the test set. We collectively refer to this as Answer Accuracy."
    }, {
      "heading" : "7 Experiments & Results",
      "text" : "We first consider the suitability of transformer models over small databases of 25 facts comparing two information retrieval settings: PerfectIR, which is representative of other question answering approaches that combine an information retrieval system to select only the facts needed to answer a query, and WholeDB, where the entire database is encoded by the model, assessing resilience to unrelated information and noise.\nThe overall scores, in Table 2, indicate that without a retrieval mechanism (i.e., WholeDB), all models were susceptible to distractor facts. Furthermore, encoding all facts in a single model is not a viable solution to answer queries posed to NLDBs as this approach does not accurately answer queries that combine multiple support sets, illustrated in Figure 4, and cannot easily scale to thousands of facts. Using a transformer yields errors when the query requires computation, such as counting, highlighted when comparing rows 1 and 3 of Table 3.\nInter-fact attention Applying FiD, which does not capture inter-fact attention, to scale to larger databases would not be successful because answer accuracy further decreases with with support set size. Applying Longformer, which captures interfact attention within a window could yield outcomes similar to the T5 transformer baseline where relevant facts are encoded with similar locality. However, in the limit, where context falls between different attention windows, the model could degrade to be similar to FiD."
    }, {
      "heading" : "7.1 Evaluating the SSG+SPJ architecture",
      "text" : "Our architecture consists of a support set generator (SSG), a select-project-join (SPJ) operator that generates derivations over the support sets and an aggregation function over the results of the SPJ operators. Assuming a perfect SSG, the SPJ accurately answers more queries than the T5 transformer baseline (Table 2) because of the computation within the aggregation function that yields higher scores for min/max and count queries, displayed in Table 3. In combination with SSG, the overall score decreases to 67% due to retrieval errors. However, SSG+SPJ still exceeds the WholeDB baselines.\nIt is tricky to evaluate the SSG in isolation because errors here not necessarily translate into errors in query answers. For example, the SSG may return a superset of a support set, but the SPJ may still generate the correct answer. Table 4 shows the performance of the SSG for a database of 25 facts. An output is considered an exact match if it is exactly the same as a support set in the reference data and soft match if it is a superset thereof.\nDecoding machine-readable outputs The aggregation operator was selected by predicting a\nspecial token decoded by the SPJ. For 1.4% of instances, an incorrect choice of aggregation function was made or the machine-readable outputs from the SPJ could not be parsed."
    }, {
      "heading" : "7.2 Scaling to larger databases",
      "text" : "We scale the baseline transformers to larger databases using TF-IDF and DPR to retrieve appropriate facts. However, these models are still limited by the encoder size of the transformer. In contrast, the SPJ operates over support sets of 1-2 facts and, in combination with the SSG, can scale to arbitrarily large databases, illustrated in Figure 5. For Boolean queries, the combination of T5 and TF-IDF scored 89%, exceeding the accuracy of the SSG+SPJ. This is because TF-IDF exploits token matching between the query and facts. For larger databases, the retrieval errors resulted in lower answer accuracy. While, with a perfect SSG, the the SPJ accurately answers most query types, as database size increases, the propagation of errors from the SSG resulted in erroneous answers."
    }, {
      "heading" : "8 Related Work",
      "text" : "Database queries require reasoning over a large set of relevant and non-redundant facts and performing aggregation. While in-roads have been made to perform discrete reasoning and computation over passages (Dua et al., 2019), with explicit computation (Andor et al., 2019) or differentiable modules\n(Gupta et al., 2020), these use only a single passage rather than requiring aggregation over large numbers of facts from different texts.\nMulti-hop question answering requires finding supporting evidence in multiple documents (see (Welbl et al., 2018; Talmor and Berant, 2018; Wolfson et al., 2020) for datasets facilitating this research). In answering multi-hop questions, the works decompose the question into simpler sub questions (Min et al., 2019; Wolfson et al., 2020), or condition each hop on the previously retrieved documents (Asai et al., 2019; Xiong et al., 2020).\nWhile tasks such as ComplexWebQuestions (Talmor and Berant, 2018) and BREAK (Wolfson et al., 2020) focus on complex queries that can be broken down into simpler ones, our focus is on setbased and aggregation queries where the complexity comes from the need to retrieve and process a large number of non-redundant relevant facts. In contrast to the set and count tasks in bAbI (Weston et al., 2015), where each query is based on a small context (less than 20 facts), our dataset scales from databases of 25 facts to 1000.\nBridging the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research (Halevy et al., 2003). The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system. There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013). More recently, systems such as\nBREAK (Wolfson et al., 2020) and ShARC (Saeidi et al., 2018) have trained models to translate a natural language query into a sequence of relational operators (or variants thereof)."
    }, {
      "heading" : "9 Conclusions",
      "text" : "Database systems are the workhorse of data analysis but they require a pre-defined schema. Part of their power stems from the fact that a data analyst can explore the data by easily posing a wide variety of queries. Given the rise in the amount of data that is becoming available in text, images and other modalities, we would like to build systems that enable the flexibility of posing complex queries against such data, but without the need for a pre-defined schema.\nThis paper proposed an architecture for neural databases and the associated WIKINLDB dataset, as first steps towards realizing a system for querying multi-modal data. Our architecture is capable of overcoming the limitations of transformer models because it runs multiple transformers in parallel, each taking a small set of facts. Consequently, NLDBs can scale to large databases.\nAdditional research is required in order to scale NLDBs to larger datasets, more complex queries, and to multi-modal data. In particular, one of the key components of the architecture is the SSG module that retrieves the relevant facts to feed to each instance of the neural SPJ. We believe that in practice, the semantics of the application will provide a strong hint on which facts may be relevant. For example, when querying a large corpus of socialmedia posts, each post is a candidate support set as long as the query does not require joining data from multiple posts. In addition, we assumed that our databases describe a snapshot of the world. In practice, we may have facts that override previous ones (e.g., ‘Samantha works for Apple’, followed by ‘Samantha works for Twitter’) and we would need to reason about which facts should be ignored."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Yann LeCun and Antoine Bordes for the initial discussion that sparked the idea of neural databases. This work was performed while James Thorne and Fabrizio Silvestri were at Facebook.\nBroader Impact Statement\nEthical Concerns A NL database is very similar to a traditional database in terms of applications with a difference that it extends the use of databases on unstructured text. For example, NL databases can be used to produce analytics on data expressed in natural language. For an NL database to be applicable in the context of a virtual assistance, they will likely need to be trained on real-world conversations. Privacy preserving ML methods should be considered for such applications.\nEnvironmental Concerns Large transformerbased models take a lot of computational resources and energy for pre-training and fine-tuning. As a result such models raise environmental concerns. In our proposed architecture, we only fine-tune transformer models on small support sets. We then use several instances of such models in parallel for inference, instead of a single large model, even on large datasets. Therefore, the model is relatively efficient, both during the fine-tuning and during the inference."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Sample Data and Dataset Statistics"
    } ],
    "references" : [ {
      "title" : "Large scale knowledge graph based synthetic corpus generation for knowledgeenhanced language model pre-training",
      "author" : [ "Oshin Agarwal", "Heming Ge", "Siamak Shakeri", "Rami Al-Rfou" ],
      "venue" : null,
      "citeRegEx" : "Agarwal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2020
    }, {
      "title" : "Giving BERT a calculator: Finding operations and arguments with reading comprehension",
      "author" : [ "Daniel Andor", "Luheng He", "Kenton Lee", "Emily Pitler." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Andor et al\\.,? 2019",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural Language Interfaces to Databases - an Introduction",
      "author" : [ "I Androutsopoulos", "G D Ritchie", "P Thanisch." ],
      "venue" : "Natural Language Engineering, 1(1):29–",
      "citeRegEx" : "Androutsopoulos et al\\.,? 1995",
      "shortCiteRegEx" : "Androutsopoulos et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning to retrieve reasoning paths over wikipedia graph for question answering",
      "author" : [ "Akari Asai", "Kazuma Hashimoto", "Hannaneh Hajishirzi", "Richard Socher", "Caiming Xiong." ],
      "venue" : "arXiv preprint arXiv:1911.10470.",
      "citeRegEx" : "Asai et al\\.,? 2019",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan" ],
      "venue" : null,
      "citeRegEx" : "Beltagy et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantic parsing on Freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-step retrieverreader interaction for scalable open-domain question answering",
      "author" : [ "Rajarshi Das", "Shehzaad Dhuliawala", "Manzil Zaheer", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1905.05733.",
      "citeRegEx" : "Das et al\\.,? 2019",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2019
    }, {
      "title" : "Quasar: Datasets for question answering by search and reading",
      "author" : [ "Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:1707.03904.",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "T-REx: A large scale alignment of natural language with knowledge base triples",
      "author" : [ "Hady Elsahar", "Pavlos Vougiouklis", "Arslen Remaci", "Christophe Gravier", "Jonathon Hare", "Frederique Laforest", "Elena Simperl." ],
      "venue" : "Proceedings of the Eleventh Interna-",
      "citeRegEx" : "Elsahar et al\\.,? 2018",
      "shortCiteRegEx" : "Elsahar et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural module networks for reasoning over text",
      "author" : [ "Nitish Gupta", "Kevin Lin", "Dan Roth", "Sameer Singh", "Matt Gardner." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Crossing the structure chasm",
      "author" : [ "Alon Y. Halevy", "Oren Etzioni", "AnHai Doan", "Zachary G. Ives", "Jayant Madhavan", "Luke K. McDowell", "Igor Tatarinov." ],
      "venue" : "CIDR 2003, First Biennial Conference on Innovative Data Systems Research, Asilomar,",
      "citeRegEx" : "Halevy et al\\.,? 2003",
      "shortCiteRegEx" : "Halevy et al\\.",
      "year" : 2003
    }, {
      "title" : "Generating a large dataset for neural question answering over the DBpedia knowledge base",
      "author" : [ "Ann-Kathrin Hartmann", "Edgard Marx", "Tommaso Soru" ],
      "venue" : null,
      "citeRegEx" : "Hartmann et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hartmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Compositionality Decomposed: How do Neural Networks Generalise",
      "author" : [ "Dieuwke Hupkes", "Verna Dankers", "Mathijs Mul", "Elia Bruni" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Hupkes et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
      "author" : [ "Gautier Izacard", "Edouard Grave" ],
      "venue" : null,
      "citeRegEx" : "Izacard and Grave.,? \\Q2020\\E",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2020
    }, {
      "title" : "Billion-scale similarity search with GPUs",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Herve Jegou." ],
      "venue" : "IEEE Transactions on Big Data, pages 1–1.",
      "citeRegEx" : "Johnson et al\\.,? 2019",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:452–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. Retrieval-Augmented Generation",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Constructing an Interactive Natural Language Interface for Relational Databases",
      "author" : [ "Fei Li", "H V Jagadish." ],
      "venue" : "Proceedings of the VLDB Endowment2, 8(1):73–84.",
      "citeRegEx" : "Li and Jagadish.,? 2014",
      "shortCiteRegEx" : "Li and Jagadish.",
      "year" : 2014
    }, {
      "title" : "Multi-hop reading comprehension through question decomposition and rescoring",
      "author" : [ "Sewon Min", "Victor Zhong", "Luke Zettlemoyer", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
      "citeRegEx" : "Pasupat and Liang.,? 2015",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392. Asso-",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Interpretation of natural language rules in conversational machine reading",
      "author" : [ "Marzieh Saeidi", "Max Bartolo", "Patrick Lewis", "Sameer Singh", "Tim Rocktäschel", "Mike Sheldon", "Guillaume Bouchard", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2018 Confer-",
      "citeRegEx" : "Saeidi et al\\.,? 2018",
      "shortCiteRegEx" : "Saeidi et al\\.",
      "year" : 2018
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "From natural language processing to neural databases",
      "author" : [ "James Thorne", "Majid Yazdani", "Marzieh Saeidi", "Fabrizio Silvestri", "Sebastian Riedel", "Alon Y. Halevy." ],
      "venue" : "Proc. VLDB Endow., 14.",
      "citeRegEx" : "Thorne et al\\.,? 2021",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Lilon Jones", "Aidan Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Wikidata: a free collaborative knowledgebase",
      "author" : [ "Denny Vrandečić", "Markus Krötzsch." ],
      "venue" : "Communications of the ACM, 57(10):78–85.",
      "citeRegEx" : "Vrandečić and Krötzsch.,? 2014",
      "shortCiteRegEx" : "Vrandečić and Krötzsch.",
      "year" : 2014
    }, {
      "title" : "Linformer: Self-Attention with Linear Complexity",
      "author" : [ "Sinong Wang", "Belinda Z. Li", "Madian Khabsa", "Han Fang", "Hao Ma." ],
      "venue" : "2048(2019).",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1502.05698.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Scao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Break it down: A question understanding benchmark",
      "author" : [ "Tomer Wolfson", "Mor Geva", "Ankit Gupta", "Matt Gardner", "Yoav Goldberg", "Daniel Deutch", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:183–198.",
      "citeRegEx" : "Wolfson et al\\.,? 2020",
      "shortCiteRegEx" : "Wolfson et al\\.",
      "year" : 2020
    }, {
      "title" : "Answering complex open-domain questions with multi-hop dense retrieval",
      "author" : [ "Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Xiong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    }, {
      "title" : "Photon: A robust cross-domain text-to-SQL system",
      "author" : [ "Jichuan Zeng", "Xi Victoria Lin", "Steven C.H. Hoi", "Richard Socher", "Caiming Xiong", "Michael Lyu", "Irwin King." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Machines have surpassed human performance on the well-known SQUaD task (Rajpurkar et al., 2016) where models extract answer spans from a short passage of text.",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 17,
      "context" : "The subsequent body of work has further considered incorporating retrieval from large corpora such as Wikipedia (Dhingra et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) to identify relevant information, conditioning answer generation (Chen",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 35,
      "context" : "More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, which may have low lexical or semantic similarity with the question.",
      "startOffset" : 96,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "More sophisticated architectures have been proposed with incremental retrieval for multi-hop QA (Xiong et al., 2020; Das et al., 2019), where several passages are required, which may have low lexical or semantic similarity with the question.",
      "startOffset" : 96,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "The ability to answer the aforementioned queries would enable a new kind of database (Thorne et al., 2021) where facts can be described in natural language and would therefore obviate the need for a pre-defined schema, which is a major limitation of current database systems.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "are answered over single passages, and bAbI (Weston et al., 2015), where each query is based on a context of less than 20 facts, our dataset scales from databases of 25 instances to 1000, and could be extended further.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "We found that mechanisms such as Fusion in Decoder (Izacard and Grave, 2020, FiD) and LongFormer (Beltagy et al., 2020), which mitigate the scaling issue, harm the model: combining more than 2 facts with FiD resulted in answer accuracies of 76% and 39%, respectively.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 28,
      "context" : "Common model architectures are based on the transformer (Vaswani et al., 2017) in an encoder-decoder configuration.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "To scale question answering to reason over large knowledge-sources such as Wikipedia, task formulations typically retrieve textspans from a corpus to condition answer generation (Chen et al., 2017; Dhingra et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "To scale question answering to reason over large knowledge-sources such as Wikipedia, task formulations typically retrieve textspans from a corpus to condition answer generation (Chen et al., 2017; Dhingra et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : "This task is usually referred to as open-domain QA (Lewis et al., 2020a; Izacard and Grave, 2020).",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "While extensions such as Linformer (Wang et al., 2020), Longformer (Beltagy et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : ", 2020), Longformer (Beltagy et al., 2020) and Fusion in Decoder (Izacard and Grave, 2020) enable larger contexts to be encoded, their application of self-attention varies and the number of tokens",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : ", 2020) and Fusion in Decoder (Izacard and Grave, 2020) enable larger contexts to be encoded, their application of self-attention varies and the number of tokens",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "pose an instance of a Neural Database architecture (Thorne et al., 2021) that operates over textual facts with parallelizable non-blocking operators before aggregating the results.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, this allows large result sets to be generated by the model, whereas accurately decoding long sequences using an encoder-decoder architecture remains an open challenge (Hupkes et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 29,
      "context" : "novel dataset for training NLDBs which is generated by transforming structured data from Wikidata (Vrandečić and Krötzsch, 2014) into natural language facts and queries.",
      "startOffset" : 98,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Concretely, we use generated sentences from KELM (Agarwal et al., 2020), which are not grounded with Wikidata IDs, and generate a post-hoc mapping back to Wikidata.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "3095 Queries Following previous work on large-scale question answering (Hartmann et al., 2018; Talmor and Berant, 2018), queries are generated using templates.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "3095 Queries Following previous work on large-scale question answering (Hartmann et al., 2018; Talmor and Berant, 2018), queries are generated using templates.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "Approximate search, such as FAISS (Johnson et al., 2019), accelerate retrieval to O(log(2) n).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "We additionally compare to using windowed attention over facts with global attention to the query using Longformer (Beltagy et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "While in-roads have been made to perform discrete reasoning and computation over passages (Dua et al., 2019), with explicit computation (Andor et al.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : ", 2019), with explicit computation (Andor et al., 2019) or differentiable modules",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "In answering multi-hop questions, the works decompose the question into simpler sub questions (Min et al., 2019; Wolfson et al., 2020), or condition each hop on the previously retrieved",
      "startOffset" : 94,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : "In answering multi-hop questions, the works decompose the question into simpler sub questions (Min et al., 2019; Wolfson et al., 2020), or condition each hop on the previously retrieved",
      "startOffset" : 94,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "While tasks such as ComplexWebQuestions (Talmor and Berant, 2018) and BREAK (Wolfson et al.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 34,
      "context" : "While tasks such as ComplexWebQuestions (Talmor and Berant, 2018) and BREAK (Wolfson et al., 2020) focus on complex queries that can be bro-",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 32,
      "context" : "In contrast to the set and count tasks in bAbI (Weston et al., 2015), where each query is based on a small context (less than 20 facts), our dataset scales from databases of 25 facts to 1000.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Bridging the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research (Halevy et al., 2003).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al.",
      "startOffset" : 134,
      "endOffset" : 206
    }, {
      "referenceID" : 21,
      "context" : "There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al.",
      "startOffset" : 134,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known (Androutsopoulos et al., 1995; Li and Jagadish, 2014; Zeng et al., 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al.",
      "startOffset" : 134,
      "endOffset" : 206
    }, {
      "referenceID" : 23,
      "context" : ", 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013).",
      "startOffset" : 69,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : ", 2020), with extensions to semi-structured data and knowledge bases (Pasupat and Liang, 2015; Berant et al., 2013).",
      "startOffset" : 69,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : "More recently, systems such as BREAK (Wolfson et al., 2020) and ShARC (Saeidi et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : ", 2020) and ShARC (Saeidi et al., 2018) have trained models to translate a natural language query into a sequence of relational operators (or variants thereof).",
      "startOffset" : 18,
      "endOffset" : 39
    } ],
    "year" : 2021,
    "abstractText" : "Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as “List/Count all female athletes who were born in 20th century”, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WIKINLDB,1 a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.",
    "creator" : "LaTeX with hyperref"
  }
}