{
  "name" : "2021.acl-long.182.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference",
    "authors" : [ "Ziye Chen", "Cheng Ding", "Zusheng Zhang", "Yanghui Rao", "Haoran Xie" ],
    "emails" : [ "chenzy35@mail2.sysu.edu.cn,", "dingch6@mail2.sysu.edu.cn,", "zhangzsh3@mail2.sysu.edu.cn,", "raoyangh@mail.sysu.edu.cn,", "hrxie2@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2343–2353\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2343"
    }, {
      "heading" : "1 Introduction",
      "text" : "Topic models (Blei et al., 2003; Griffiths et al., 2004) are important tools for discovering latent semantic patterns in a corpus. These models can be grouped into flat models and hierarchical models. In many domains, topics can be naturally organized into a tree, where the hierarchical relationships among topics are valuable for data analysis and exploration. Tree-structured topic model (Griffiths et al., 2004) was thus developed to learn coherent topics from text without disrupting the inherent hierarchical structure. Such a method has been proven as useful in various downstream applications, including hierarchical categorization of Web pages (Ming et al., 2010), aspects hierarchies extraction in reviews (Kim et al., 2013), and hierarchies discovery of research topics in academic repositories (Paisley et al., 2014).\n∗The corresponding author.\nDespite the practical importance and potential advantages, tree-structured topic models still face the following challenges. Firstly, the hierarchical structure of topics should be reasonable (Viegas et al., 2020). Typically, topics near the root are more general while the ones close to the leaves are more specific. Besides, child topics should be coherent with their corresponding parent topics. Secondly, low redundancy is necessary for the extracted topics, in order to prevent the distributions associated with parent topics and their children being extremely similar (Griffiths et al., 2004). Thirdly, the number of topics in each hierarchy level should be automatically determined by the model, because it is usually unknown and can not be previously set to a predefined value (Kim et al., 2012). Finally, it is difficult for probabilistic models to enhance the data scalability (Isonuma et al., 2020). Previously, several tree-structured topic models (Griffiths et al., 2004; Kim et al., 2012; Isonuma et al., 2020) have been developed. But these methods can not fully overcome the aforementioned challenges.\nIn this paper, we focus on grouping topics into a reasonable tree structure, based on the neural variational inference (NVI) framework (Kingma and Welling, 2014; Rezende et al., 2014) with a nonparametric prior. Owing to the excellent function fitting ability, neural network has been widely introduced into topic modeling. Nonetheless, few neural methods explicitly model the dependencies among different layers and get explainable hierarchical topics, which is largely due to the weak interpretability of neural networks. Furthermore, the inflexibility of neural networks also makes it difficult to learn an unbounded number of topics at each level. To address these limitations, we propose a novel nonparametric neural method to generate tree-structured topic hierarchies, namely nonparametric Tree-Structured Neural Topic Model\n(nTSNTM)1. By connecting the network layers with dependency matrices, the model is able to extract an explainable tree-structured hierarchy. Firstly, the topic affiliations among hierarchy levels can be determined by the dicrete vectors of the dependency matrices. Secondly, to control redundancy among topics, we allow the model to freely generate topics without duplicating their corresponding parent topics. Thirdly, we couple a stick-breaking process with NVI to equip the topic tree with self-determined widths, which can help the model determine the number of topics automatically. Finally, due to the advantages of neural networks, our model can scale to larger datasets conveniently. Experiments indicate that our model outperforms baselines on several widely adopted metrics and two new measurements developed for tree-structured topic models.\nThe rest of this paper is organized as follows. We describe related work in Section 2. Then, we detail the proposed nTSNTM in Section 3. Section 4 presents our experimental results and discussions. Finally, we draw conclusions in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "In (Griffiths et al., 2004), a tree-structured topic model called hLDA was first proposed by introducing a nested Chinese restaurant process (nCRP). For hLDA, a topic tree is constructed through Gibbs sampling given a certain depth. Based on hLDA, Xu et al. (2018) proposed a knowledged-based HTM to generate topic hierarchies from multiple domains corpora, but the hierarchical relation between the ancestor topic and the offspring one may be unclear, because a document is generated by the topics along a single path of the tree. To overcome this issue, Kim et al. (2012) proposed a recursive CRP (rCRP), in which a document possesses a distribution over the entire tree. Although rCRP has shown remarkable competitiveness in hierarchical topic modeling, it suffers from the major limitation of data scalability (Isonuma et al., 2020). Several other methods focused on hierarchical text clustering. For instance, Ghahramani et al. (2010) applied nested stick-breaking processes to cluster data into a tree structure. Unfortunately, the above method only models a document by a single node of the tree. Liu et al. (2014) developed a model named HLTA for topic detection, in which words and top-\n1The code of our model is available in public at: https: //github.com/hostnlp/nTSNTM.\nics are clustered by employing the Bridged-Islands algorithm iteratively. However, HLTA is unable to cope with polysemous words, which is quite important for topic models.\nTo couple nonparametric processes with NVI, Miao et al. (2017) used Gaussian distributions to generate stick-breaking fractions. Nalisnick and Smyth (2017) first described how to use stochastic gradient variational Bayes for posterior inference of the weights in stick-breaking processes. Experiments indicated that the latent representations of the above model were more discriminative than those of the Gaussian variant. Then, Ning et al. (2020) developed two nonparametric neural topic models by treating topics as trainable parameters. Unfortunately, the aforementioned methods can only learn topics with a flat structure.\nFor tree-structured neural topic modeling, a feasible way is to decompose the distribution over the topic tree into a path distribution and a level distribution. Following (Wang and Blei, 2009), where a tree-based stick-breaking construction of nCRP was first derived to draw topic paths, and then a level distribution was learned to sample topics along the path, Isonuma et al. (2020) proposed a tree-structured neural topic model (TSNTM) by parameterizing an unbounded ancestral and fraternal topic distribution. TSNTM applies a doublyrecurrent neural network (DRNN) to obtain topic embeddings via ancestral and fraternal edges, then generates breaking fractions by the dot product between document embeddings and topic embeddings. However, TSNTM fails to learn a reasonable topic tree for the following reasons. Firstly, the breaking fractions do not obey the Beta distributions adopted in the stick-breaking process (SBP). Secondly, the structure of DRNN in TSNTM is simplified, where the topic embeddings are generated directly by an initialized root embedding and two parameter matrices (i.e., ancestral and fraternal connections). This prevents the model from learning appropriate semantic embeddings for topics. Finally, TSNTM relies on heuristic rules to update the tree structure.\nAnother stream of work is to generate a document by a directed acyclic graph (DAG) structured topic hierarchy. For instance, Li and McCallum (2006) introduced the pachinko allocation model (PAM) to capture correlations between topics using a DAG. Mimno et al. (2007) proposed the hierarchical PAM by connecting the root topic to lower-level\ntopics through multinomial distributions. Nonprobabilistic matrix factorization was also used to extract the topic structure. Liu et al. (2018) used non-negative matrix factorization (NMF) with three optimization constraints, including global independence, local independence, and information consistency, to preserve topic coherence and a reasonable structure. Viegas et al. (2020) incorporated pre-trained word embeddings into NMF to further improve topic coherence. The main limitation of NMF-based methods, however, is that a time-consuming process (e.g., measure the stability of results by running multiple random samplings) is necessary to determine the number of topics at each level. This is because nonparametric priors are intractable to be included in these models."
    }, {
      "heading" : "3 Tree-Structured Neural Topic Model with Nonparametric Prior",
      "text" : "In this section, we firstly describe the stickbreaking process. Then, we introduce the modeling of tree-structured topic hierarchy. Finally, we detail the inference method of our nTSNTM."
    }, {
      "heading" : "3.1 Stick-breaking Process",
      "text" : "For nonparametric models, stick-breaking prior is a random measure with the form G = ∑∞ k=1 πkδζk , where δζk is a discrete measure concentrated at ζk ∼ G0 (Ishwaran and James, 2001)2, i.e, a draw from the base measure. The πks are random weights independent of G0 (Nalisnick and Smyth, 2017). This constructive definition is known as SBP (Sethuraman, 1994), which implies that the weights π = (πk)∞k=1 can be drawn according to the procedure of iteratively breaking off segments from a unit stick.\n2In topic models, ζk represents the kth topic and G0 represents the topic space.\nAs shown in Figure 1, we break the unit stick and get the first component with length v1. If a fraction v2 of the remaining stick is broken off, then we obtain the second component with length v2(1− v1) and a remaining stick with length (1− v1)(1− v2). The following breaks are taken on the remaining stick by the same operation. Given a truncation level T , the length of the last component will be ∏T−1 j=1 (1−vj). Formally, the length of each component is defined as:\nπk =\n{ v1 if k = 1,\nvk ∏ t<k (1− vt) for k > 1,\n(1)\nwhere vk ∼ Beta(α0,β0), with α0 and β0 being the prior parameters. Note that the component weights π satisfy 0 ≤ πk ≤ 1 and ∑∞ k=1 πk = 1, thus we can interpret π as random probabilities. Particularly, when vk ∼ Beta(1,β0), the joint distribution for π is the GEM distribution (Pitman, 2006) with concentration parameter β0, and the corresponding SBP is one of the constructions for the Dirichlet process, a popular nonparametric random process for topic modeling (Teh et al., 2005).\nIn our method, we take component weights π as the path distribution of a document. We assume that the words of a document come from several topic paths. Due to the sequentiality of the stickbreaking operation, paths with smaller serial numbers are more likely to be activated to represent the documents, while paths with larger serial numbers tend to be unactivated. The number of activated paths can be adjusted by SBP automatically."
    }, {
      "heading" : "3.2 Tree-Structured Topic Hierarchy",
      "text" : "To conveniently describe our method, we here compare the sampling processes for an example document of different tree-structured topic models. As shown in Figure 2, hLDA (Griffiths et al., 2004) considers that a document is generated by topics of a single path, which violates the multi-topics assumption of topic models (i.e., a document may span several topics). Considering this issue, rCRP (Kim et al., 2012) and TSNTM (Isonuma et al., 2020) assume that a document can be generated by any topic in the tree. We follow the above assumption adopted in rCRP and TSNTM to model a tree-structured topic hierarchy, but the difference is that our model takes the sampling from the bottom up rather than from the top down as in rCRP and TSNTM. Particularly, rCRP samples topics from the root using recursive CRP. TSNTM samples\npaths from the root by applying a DRNN (AlvarezMelis and Jaakkola, 2017), and it needs to update the tree structure frequently by heuristic rules. On the contrary, our model directly samples the leaf topics, and the paths toward the root are determined automatically. Specifically, we use a common stickbreaking construction to infer the distribution over leaf topics, which corresponds to the path distribution. Besides, we use dependency matrices to keep track of the affiliations among topics. Thus the tree structure can be updated through back propagation.\nFigure 3 shows the graphical representation of nTSNTM. For our model, the number of leaf topics is determined by SBP, and the numbers of non-leaf topics are adjusted through dependency matrices M between network layers. The lth item of M, i.e., Ml ∈ [0, 1]Kl∗Kl+1 , is the dependency matrix between layers l and l+1, where Kl and Kl+1 represent the maximum numbers of topics at level l and level l+1, respectively. In particular, Ml,k,j is the probability of topic j at level l being the parent of topic k at level l+1 with ∑ j′Ml,k,j′ = 1. As mentioned in (Griffiths et al., 2004), a clear tree structure indicates that each sub-topic has a relationship with no more than one super-topic. So a softmax function with low temperature (Hinton et al., 2015) is applied to ensure that Ml,k approximates a discrete one-hot vector. In this way, the topic tree can be built through the introduced M from bottom up. Furthermore, the topic hierarchy can be updated automatically according to the update of M.\nAfter determining the topic hierarchy by M, the generative process of each word in nTSNTM can be described as follows:\n1. For each document xd ∈ {x1, ...,xD}:\nDraw SBP weights: πd ∼ GEM(β0); (2) Draw Gaussian samples: gd ∼ N (0, I2); (3) Draw level distributions: ηd = fη(gd). (4)\n2. For each word wd,n ∈ {wd,1, ..., wd,Nd} in xd:\nDraw a path: cd,n ∼ Multi(πd); (5) Draw a level: rd,n ∼ Multi(ηd); (6) Draw a word: wd,n ∼ Multi(φcd,n[rd,n]). (7)\nIn the above, D is the number of documents, Nd is the number of words in xd. φcd,n[rd,n] ∈ 4 V−1 is the word distribution of the topic at level rd,n of path cd,n, and V is the vocabulary size. fη(·) is a neural perceptron with softmax activation to transform a Gaussian sample to a level distribution."
    }, {
      "heading" : "3.3 Parameter Inference",
      "text" : "Since the Beta distribution does not have a differentiable non-centered parametrization that NVI requires (Kingma and Welling, 2014), we choose the Kumaraswamy distribution (Kumaraswamy, 1980) to approximate GEM(β0), i.e., the conjunction of Beta(1,β0) and a stick-breaking operation (Nalisnick and Smyth, 2017). For the Kumaraswamy distribution, the probability density function on the unit interval is defined as Kumaraswamy(x; a, b) = abxa−1(1−xa)b−1 for x ∈ (0, 1) and a, b > 0. Samples can be drawn via the inverse transform: x ∼ (1 − u 1 b ) 1 a where u ∼ Uniform(0, 1). Then the KL-divergence between the Kumaraswamy distribution and the Beta distribution can be closely approximated in the closed-form. We describe the parameter inference process of our nTSNTM as follows.\nFirstly, we estimate the component weights of document xd, i.e., π̂d, by the following stick-\nbreaking operation with fractions vd:\nαd = fα(xd), βd = fβ(xd), (8) vd ∼ Kumaraswamy(αd,βd), (9)\nπ̂d = (vd,1, ..., T−1∏ j=1 (1− vd,j)), (10)\nwhere the bag-of-words representation is used for xd. To ensure positive outputs, fα(·) and fβ(·) are neural perceptrons with softplus activation.\nSecondly, we infer the level distributions η̂d by:\nµd = fµ(xd), σd = fσ(xd), (11) ĝd ∼ N (µd,σ2d), η̂d = fη(ĝd), (12)\nwhere fµ(·) and fσ(·) are linear transformations. In practice, we reparameterize ĝd = µd+ ̂ ∗σd with the sample ̂ ∼ N (0, I2) (Rezende et al., 2014).\nThirdly, we obtain the topic distributions of xd, i.e., θ̂d = {θ̂d,1, ..., θ̂d,L} by:\nθ̂d,l =\n{ η̂d,Lπ̂d if l = L,\nη̂d,lπ̂d ∏ l′≥lMl′ for l < L, (13)\nwhere L denotes the depth of the topic tree, and∑ l ∑ k θ̂d,l,k = 1.\nThen, we follow (Miao et al., 2017) to explicitly model topic-word distributions by: φ = softmax(u ∗ tT ), where u ∈ RV ∗H and t ∈ R ∑\nlKl∗H are word vectors and topic vectors, and H denotes the dimension of word/topic vectors. Given topic-word distributions φ and topic distributions θ̂d obtained from Eq. (13), our model reconstructs each document xd by: p(wd,n|φ, θ̂d) =∑\nzd,n [p(wd,n|φzd,n)p(zd,n|θ̂)] = θ̂d ∗ φ, where\nzd,n is the topic assignment for wd,n. Finally, the variational lower-bound of xd is:\nL =Eq(πd,ηd|xd)[ ∑ n log(p(wd,n|φ, θ̂d))]\n−DKL[q(πd|xd)||p(πd)] −DKL[q(ηd|xd)||p(ηd)],\n(14)\nwhere q(πd|xd) and q(ηd|xd) are posteriors modeled by the inference network. p(πd) is the prior for πd, i.e., GEM(β0), and p(η) is the prior for η, i.e., the standard Gaussian transformed by fη(·).\nThe parameter inference method for nTSNTM is presented in Algorithm 1. We use the variational lower-bound to calculate gradients and apply Adam (Kingma and Ba, 2015) to update parameters.\nAlgorithm 1: Parameter Inference Algorithm Input: GEM priors β0 and documents\n{x1, ...,xD}; Output: Document-topic distribution θ,\ntopic-word distribution φ, and topic tree Tr.\n1 Randomly initialize dependency matrices M and topic-word distribution φ; 2 repeat 3 for document xd ∈ {x1, ...,xD} do 4 Estimate π̂d and η̂d by Eqs. (8–12); 5 Compute θ̂d by Eq. (13); 6 for wd,n ∈ xd do 7 p(wd,n|θ̂d,φ) = θ̂d ∗ φ ; 8 end 9 Compute L by Eq. (14);\n10 Update fα(·), fβ(·), fµ(·), fσ(·), fη(·), φ, and M; 11 end 12 until convergence; 13 Build Tr according to M and φ."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct experiments on four widely used benchmark datasets: 20NEWS (Miao et al., 2017), Reuters (Wu et al., 2020), Wikitext-103 (Nan et al., 2019), and Rcv1-v2 (Miao et al., 2017). 20NEWS and Reuters are two news corpora. Wikitext-103 is a language modeling dataset extracted from Wikipedia, and Rcv1-v2 is a large version of Reuters. Table 1 presents the statistics of these datasets, where the vocabulary is obtained by following the same preprocessing steps in the original paper. For each corpus, we randomly select 5% of training samples as the validation set."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "For tree-structured topic models, we adopt hLDA (Griffiths et al., 2004)3, rCRP (Kim et al., 2012),\n3Note that hLDA was named as nCRP (Blei et al., 2010) in (Isonuma et al., 2020).\nand TSNTM (Isonuma et al., 2020) as our baselines. For all these models, the max-depth of topic tree is set to 3 by following (Isonuma et al., 2020).\nFor nonparametric or flat topic models, we adopt HDP (Teh et al., 2005), GSM & GSB (Miao et al., 2017), NB-NTM & GNB-NTM (Wu et al., 2020), and iTM-VAE & HiTM-VAE (Ning et al., 2020) as baselines. HDP is a classical nonparametric topic model that allows potentially an infinite number of topics. GSM & GSB are two NVI-based models using Gaussian priors. In particular, GSB uses Gaussian distributions to generate stick-breaking fractions. NB-NTM & GNB-NTM are two flat neural topic models based on Negative Binomial and Gamma Negative Binomial processes respectively. For iTM-VAE & HiTM-VAE, they extended the method in (Nalisnick and Smyth, 2017) to introduce nonparametric processes into the NVI framework by extracting the potential infinite topics.\nWe directly use the publicly available codes of hLDA4, rCRP5, TSNTM6, HDP7, NB-NTM & GNB-NTM8, and iTM-VAE & HiTM-VAE9. Besides, we implement GSM & GSB based on the original paper. For all parametric models, the number of topics is set to 50 and 200 as in (Miao et al., 2017). For nonparametric models based on SBP, the truncation level is set to 200, and the concentration parameter β0 for the GEM distribution is chosen from [5, 10, 15, 20, 25, 30] using each validation set. In particular, we sequentially choose the topics, of which the sum of probabilities in the whole corpus exceeds 95%, as the active ones. For neural baselines and our proposed model, we set the size of hidden layers to 256 and use one sample for NVI by following (Miao et al., 2017).\nAll the experiments are conducted on a workstation in Python/Java environment equipped with 40G memory. In the following, we do not report the results of hLDA and rCRP on Rcv1-v2 since they failed to achieve convergence in 48 hours."
    }, {
      "heading" : "4.3 Topic Hierarchy Analysis",
      "text" : "As mentioned in (Viegas et al., 2020), a reasonable topic hierarchy means that topics near the root should be more general while the ones close to\n4https://github.com/joewandy/hlda 5https://github.com/uilab-github/rCRP 6https://github.com/misonuma/tsntm 7https://github.com/arnim/HDP 8https://github.com/mxiny/NB-NTM 9https://github.com/walkerning/itmvae_\npublic\nthe leaves should be more specific. To this end, we adopt topic specialization (Kim et al., 2012) as an indicator for the evaluation of topical hierarchy. The specialization of a topic is the cosine distance between the word distribution of the topic and the term frequency vector of the entire corpus. A higher specialization score implies that the topic is more specialized. Figure 4 presents the average topic specialization scores of each level for different tree-structured models. The results indicate that nTSNTM and rCRP can achieve a reasonable pattern of topic specialization at different levels, i.e., the scores become higher as the level becomes deeper. We also observe that the baseline of TSNTM generates more specific topics at the second level than the third level, which indicates an unreasonable topic hierarchy. For the baseline of hLDA, there is a leap of topic specialization from level 2 to level 3, especially for 20NEWS. The reason may be that each document is generated by topics along a single path for hLDA, which renders the large specialization of the topics at level 3 since they are all restricted to one topic from level 2.\nA reasonable topic hierarchy also indicates that child topics are coherent with their corresponding parent topics (Viegas et al., 2020). To measure the relations of two connected topics, we develop a new metric named cross-level NPMI (CLNPMI) to measure the relations of two connected topics by calculating the average NPMI value of every two different topic words from a parent topic and its child. In the above, NPMI was proposed by Lau et al. (2014) which evaluates the relation between\ntwo words wi and wj as follows:\nNPMI(wi, wj) = log\nP (wi,wj) P (wi)P (wj)\n− log(P (wi, wj)) . (15)\nBased on NPMI, we define CLNPMI as:\nCLNPMI(Wp,Wc)\n= 1 |W ′p||W ′c| ∑\nwi∈W ′p ∑ wj∈W ′c NPMI(wi, wj), (16)\nwhere W ′p = Wp −Wc and W ′c = Wc −Wp, in which, Wp and Wc denote the top N words of a parent topic and one of its children. To avoid degenerating into NPMI when the parent and the child topics are highly similar, CLNPMI is estimated by the distinct words between every two topics.\nTo evaluate the topic redundancy for a tree, we introduce a new measurement named the averaged overlap rate (OR) and adopt the widely-used topic uniqueness (TU) (Nan et al., 2019). OR measures the averaged repetition ratio of top N words between parent topics and their children, which is defined as: OR(Wp,Wc) = |Wp∩Wc| N . TU calculates the uniqueness of all topics by TU = 1 K ∑K k=1TU(k), where K is the number of topics and TU(k) is defined as:\nTU(k) = 1\nN N∑ n=1\n1\ncnt(n, k) . (17)\nIn the above, cnt(n, k) is the total number of times the nth top word in topic k appears in the top N words across all topics.\nFor each of the aforementioned metrics, we calculate the average scores of 5, 10, and 15 top words. Table 2 shows the performance of different models,\nwhere each method is run for 5 times and the average values are presented. The results indicate that our model significantly outperforms the baselines in most cases, with p-values less than 0.05. For hLDA and our nTSNTM on the 20NEWS dataset, the difference is not statistically significant on the OR metric, with a p-value equal to 0.391. This validates the effectiveness of the bottom-up structure for nTSNTM, in which, non-leaf topics are activated when their offsprings are chosen.\nWe also present the hierarchical affinity (Kim et al., 2012) for each model to measure whether the parent topic is more similar to its child topics than the descendants of other parent topics. The average cosine similarities of the parent topic’s word distribution to children topics and non-children topics are shown in Figure 5. For parent topics, both rCRP and nTSNTM clearly show stronger affinities with children topics than non-children topics. But rCRP suffers from the high redundancy, which can be indicated by the high similarities (0.73 ∼ 0.82) between parent topics and sub-topics. To intuitively demonstrate the ability of our model in generating a topic tree, we present several topics extracted from 20NEWS by our nTSNTM and the existing NVI-based TSNTM in Figures 6 and 7, respectively. The results indicate that our model is able to learn a reasonable tree-structured topic hierarchy with low redundancy. While for TSNTM, we notice that there is a low degree of discrimination between topics at the second and the third levels. In addition, topics of the same group at the third level are highly repetitive, including “rec.sport.baseball” and “talk.politics.misc”. For completeness, we further check topics extracted from 20NEWS by hLDA and rCRP. The results indicate that each topic at the second level is too general to represent\na topic branch and the affiliations are unclear for hLDA. Although rCRP can generate meaningful topics with appropriate affiliations between different levels, it suffers from a high topic redundancy."
    }, {
      "heading" : "4.4 Comparison on Topic Interpretability",
      "text" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10. As mentioned in (Lau et al., 2014), the NPMI is a measurement of topic coherence which closely corresponds to the ranking of topic interpretability by human annotators. Table\n10We do not estimate the perplexity for the following two reasons. First, the perplexity of sampling-based and NVIbased models is difficult to compare directly (Isonuma et al., 2020). Second, the prior of NVI-based methods has a large influence on the perplexity since the KL-divergence may vary greatly for different priors (Burkhardt and Kramer, 2019).\n3 shows the NPMI of 50 and 200 topics for parametric topic models and topics induced automatically for nonparametric topic models. We run each model for 5 times and present the average results. Firstly, nTSNTM outperforms all tree-structured baselines, and the difference is statistically significant at the level of 0.05 (except for TSNTM on the Rcv1-v2 dataset). Secondly, nTSNTM shows competitive performance when compared with the best flat baselines. In particular, except for HiTMVAE on the Reuters dataset, the results of all the other top-performing baselines are not significantly better than those of our model."
    }, {
      "heading" : "4.5 Evaluating Data Scalability",
      "text" : "To evaluate data scalability, we randomly sample several numbers of documents (12.5k, 25k, 50k, 100k, 200k, 400k, and all) from the training\nset of Rcv1-v2 to run our model and other treestructured baselines. The sampling-based models (i.e., hLDA and rCRP) are run on an Intel Xeon Skylake 6133 CPU with 8 cores, and NVI-based models (i.e., TSNTM and nTSNTM) are tested on an Nvidia Tesla V100 GPU. Figure 8 shows the training time of these topic models. Our nTSNTM shows an advantage in data scalability when compared with baselines. Although TSNTM is also scalable to a large corpus by GPU acceleration, it applies a doubly-recurrent network which largely slows down the model speed. hLDA and rCRP spend considerable computation time on path sampling, which is much more serious when dealing with a large-scale dataset. Additionally, these two sampling-based models are serial, which means they can only utilize one core of the CPU."
    }, {
      "heading" : "4.6 Impact of the Concentration Parameter",
      "text" : "We further validate the nonparametric property of our model. Figure 9 shows the impact of β0 on the\nnumber of active topics. Firstly, we can see that the topic numbers of all models grow when increasing β0. The reason is that β0 controls the smoothness of SBP, and that a larger value leads to a smoother degree, i.e., more topics. Secondly, compared with iTM-VAE and HiTM-VAE, the number of topics found by nTSNTM is closer to the one extracted by HDP, which demonstrates that our model is able to approximate the nonparametric property of HDP."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a nonparametric treestructured neural topic model named nTSNTM. Our method explicitly models the dependency of latent variables from different layers, and combines them to reconstruct the input text. By coupling SBP with dependency matrices, we can update the tree structure automatically. Extensive experiments validate the effectiveness of our nTSNTM on generating a reasonable topic tree with low topic redundancies. Furthermore, our model can be trained 2 times faster than the existing NVI-based TSNTM with approximately 800k documents. In the future, we plan to apply our method to aspect extraction."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We are grateful to the reviewers for their constructive comments and suggestions on this study. This work has been supported by the National Natural Science Foundation of China (61972426) and Guangdong Basic and Applied Basic Research Foundation (2020A1515010536)."
    } ],
    "references" : [ {
      "title" : "Treestructured decoding with doubly-recurrent neural networks",
      "author" : [ "David Alvarez-Melis", "T. Jaakkola." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations.",
      "citeRegEx" : "Alvarez.Melis and Jaakkola.,? 2017",
      "shortCiteRegEx" : "Alvarez.Melis and Jaakkola.",
      "year" : 2017
    }, {
      "title" : "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies",
      "author" : [ "David M. Blei", "Thomas L. Griffiths", "Michael I. Jordan." ],
      "venue" : "Journal of the ACM, 57(2):1–30.",
      "citeRegEx" : "Blei et al\\.,? 2010",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2010
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "Journal of Machine Learning Research, 3(1):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model",
      "author" : [ "Sophie Burkhardt", "Stefan Kramer." ],
      "venue" : "Journal of Machine Learning Research, 20(131):1–27.",
      "citeRegEx" : "Burkhardt and Kramer.,? 2019",
      "shortCiteRegEx" : "Burkhardt and Kramer.",
      "year" : 2019
    }, {
      "title" : "Tree-structured stick breaking for hierarchical data",
      "author" : [ "Zoubin Ghahramani", "Michael Jordan", "Ryan P Adams." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 19–27.",
      "citeRegEx" : "Ghahramani et al\\.,? 2010",
      "shortCiteRegEx" : "Ghahramani et al\\.",
      "year" : 2010
    }, {
      "title" : "Hierarchical topic models and the nested chinese restaurant process",
      "author" : [ "Thomas L. Griffiths", "Michael I. Jordan", "Joshua Tenenbaum", "David M. Blei." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 17–24.",
      "citeRegEx" : "Griffiths et al\\.,? 2004",
      "shortCiteRegEx" : "Griffiths et al\\.",
      "year" : 2004
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "CoRR, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Gibbs sampling methods for stick-breaking priors",
      "author" : [ "Hemant Ishwaran", "Lancelot F. James." ],
      "venue" : "Journal of the American Statistical Association, 96(453):161–173.",
      "citeRegEx" : "Ishwaran and James.,? 2001",
      "shortCiteRegEx" : "Ishwaran and James.",
      "year" : 2001
    }, {
      "title" : "Tree-structured neural topic model",
      "author" : [ "Masaru Isonuma", "Junichiro Mori", "Danushka Bollegala", "Ichiro Sakata." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 800–806.",
      "citeRegEx" : "Isonuma et al\\.,? 2020",
      "shortCiteRegEx" : "Isonuma et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling topic hierarchies with the recursive chinese restaurant process",
      "author" : [ "Joon Hee Kim", "Dongwoo Kim", "Suin Kim", "Alice H. Oh." ],
      "venue" : "Proceedings of the 21st ACM International Conference on Information and Knowledge Management, pages 783–792.",
      "citeRegEx" : "Kim et al\\.,? 2012",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2012
    }, {
      "title" : "A hierarchical aspectsentiment model for online reviews",
      "author" : [ "Suin Kim", "Jianwen Zhang", "Zheng Chen", "Alice H. Oh", "Shixia Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, pages 526–533.",
      "citeRegEx" : "Kim et al\\.,? 2013",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "Proceedings of the 2nd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "A generalized probability density function for double-bounded random processes",
      "author" : [ "P. Kumaraswamy." ],
      "venue" : "Journal of Hydrology, 46:79–88.",
      "citeRegEx" : "Kumaraswamy.,? 1980",
      "shortCiteRegEx" : "Kumaraswamy.",
      "year" : 1980
    }, {
      "title" : "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
      "author" : [ "Jey Han Lau", "David Newman", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Lau et al\\.,? 2014",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2014
    }, {
      "title" : "Pachinko allocation: Dag-structured mixture models of topic correlations",
      "author" : [ "Wei Li", "Andrew McCallum." ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning, pages 577–584.",
      "citeRegEx" : "Li and McCallum.,? 2006",
      "shortCiteRegEx" : "Li and McCallum.",
      "year" : 2006
    }, {
      "title" : "Neural variational correlated topic modeling",
      "author" : [ "Luyang Liu", "Heyan Huang", "Yang Gao", "Yongfeng Zhang", "Xiaochi Wei." ],
      "venue" : "Proceeding of The World Wide Web Conference, pages 1142–1152.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Topic splitting: A hierarchical topic model based on nonnegative matrix factorization",
      "author" : [ "Rui Liu", "Xingguang Wang", "Deqing Wang", "Yuan Zuo", "He Zhang", "Xianzhu Zheng." ],
      "venue" : "Journal of Systems Science and Systems Engineering, 27:479–496.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical latent tree analysis for topic detection",
      "author" : [ "Tengfei Liu", "Nevin L Zhang", "Peixian Chen." ],
      "venue" : "Proceedings of the 2014 European Conference on Machine Learning and Knowledge Discovery in Databases, pages 256–272.",
      "citeRegEx" : "Liu et al\\.,? 2014",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Discovering discrete latent topics with neural variational inference",
      "author" : [ "Yishu Miao", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, pages 2410–2419.",
      "citeRegEx" : "Miao et al\\.,? 2017",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2017
    }, {
      "title" : "Mixtures of hierarchical topics with pachinko allocation",
      "author" : [ "David Mimno", "Wei Li", "Andrew McCallum." ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning, pages 633–640.",
      "citeRegEx" : "Mimno et al\\.,? 2007",
      "shortCiteRegEx" : "Mimno et al\\.",
      "year" : 2007
    }, {
      "title" : "Prototype hierarchy based clustering for the categorization and navigation of web collections",
      "author" : [ "Zhao-Yan Ming", "Kai Wang", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information",
      "citeRegEx" : "Ming et al\\.,? 2010",
      "shortCiteRegEx" : "Ming et al\\.",
      "year" : 2010
    }, {
      "title" : "Stickbreaking variational autoencoders",
      "author" : [ "Eric T. Nalisnick", "Padhraic Smyth." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations.",
      "citeRegEx" : "Nalisnick and Smyth.,? 2017",
      "shortCiteRegEx" : "Nalisnick and Smyth.",
      "year" : 2017
    }, {
      "title" : "Topic modeling with wasserstein autoencoders",
      "author" : [ "Feng Nan", "Ran Ding", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6345–6381.",
      "citeRegEx" : "Nan et al\\.,? 2019",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2019
    }, {
      "title" : "Nonparametric topic modeling with neural inference",
      "author" : [ "Xuefei Ning", "Yin Zheng", "Zhuxi Jiang", "Yu Wang", "Huazhong Yang", "Junzhou Huang", "Peilin Zhao." ],
      "venue" : "Neurocomputing, 399:296–306.",
      "citeRegEx" : "Ning et al\\.,? 2020",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2020
    }, {
      "title" : "Nested hierarchical dirichlet processes",
      "author" : [ "John Paisley", "Chong Wang", "David M. Blei", "Michael I. Jordan." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):256–270.",
      "citeRegEx" : "Paisley et al\\.,? 2014",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2014
    }, {
      "title" : "Combinatorial stochastic processes",
      "author" : [ "Jim Pitman." ],
      "venue" : "Technical Report 621, Dept. Statistics, UC Berkeley.",
      "citeRegEx" : "Pitman.,? 2006",
      "shortCiteRegEx" : "Pitman.",
      "year" : 2006
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra." ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, pages 1278–1286.",
      "citeRegEx" : "Rezende et al\\.,? 2014",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "A constructive definition of dirichlet priors",
      "author" : [ "Jayaram Sethuraman." ],
      "venue" : "Statistica Sinica, pages 639–650.",
      "citeRegEx" : "Sethuraman.,? 1994",
      "shortCiteRegEx" : "Sethuraman.",
      "year" : 1994
    }, {
      "title" : "Sharing clusters among related groups: Hierarchical dirichlet processes",
      "author" : [ "Yee Teh", "Michael Jordan", "Matthew Beal", "David Blei." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1385–1392.",
      "citeRegEx" : "Teh et al\\.,? 2005",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2005
    }, {
      "title" : "Cluhtm - semantic hierarchical topic modeling based on cluwords",
      "author" : [ "Felipe Viegas", "Washington Cunha", "Christian Gomes", "Antônio Pereira", "Leonardo C. da Rocha", "Marcos André Gonçalves." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Viegas et al\\.,? 2020",
      "shortCiteRegEx" : "Viegas et al\\.",
      "year" : 2020
    }, {
      "title" : "Variational inference for the nested chinese restaurant process",
      "author" : [ "Chong Wang", "David M. Blei." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1990–1998.",
      "citeRegEx" : "Wang and Blei.,? 2009",
      "shortCiteRegEx" : "Wang and Blei.",
      "year" : 2009
    }, {
      "title" : "Neural mixed counting models for dispersed topic discovery",
      "author" : [ "Jiemin Wu", "Yanghui Rao", "Zusheng Zhang", "Haoran Xie", "Qing Li", "Fu Lee Wang", "Ziye Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical topic modeling with automatic knowledge mining",
      "author" : [ "Yueshen Xu", "Jianwei Yin", "Jianbin Huang", "Yuyu Yin." ],
      "venue" : "Expert Systems with Applications, 103:106–117.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Topic models (Blei et al., 2003; Griffiths et al., 2004) are important tools for discovering latent semantic patterns in a corpus.",
      "startOffset" : 13,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Topic models (Blei et al., 2003; Griffiths et al., 2004) are important tools for discovering latent semantic patterns in a corpus.",
      "startOffset" : 13,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Tree-structured topic model (Griffiths et al., 2004) was thus developed to learn coherent topics from text without disrupting the inherent hierarchical structure.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "Such a method has been proven as useful in various downstream applications, including hierarchical categorization of Web pages (Ming et al., 2010), aspects hierarchies extraction in reviews (Kim et al.",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : ", 2010), aspects hierarchies extraction in reviews (Kim et al., 2013), and hierarchies discovery of research topics in academic repositories (Paisley et al.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : ", 2013), and hierarchies discovery of research topics in academic repositories (Paisley et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "Firstly, the hierarchical structure of topics should be reasonable (Viegas et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "Secondly, low redundancy is necessary for the extracted topics, in order to prevent the distributions associated with parent topics and their children being extremely similar (Griffiths et al., 2004).",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 9,
      "context" : "Thirdly, the number of topics in each hierarchy level should be automatically determined by the model, because it is usually unknown and can not be previously set to a predefined value (Kim et al., 2012).",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "Finally, it is difficult for probabilistic models to enhance the data scalability (Isonuma et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Previously, several tree-structured topic models (Griffiths et al., 2004; Kim et al., 2012; Isonuma et al., 2020) have been developed.",
      "startOffset" : 49,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "Previously, several tree-structured topic models (Griffiths et al., 2004; Kim et al., 2012; Isonuma et al., 2020) have been developed.",
      "startOffset" : 49,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Previously, several tree-structured topic models (Griffiths et al., 2004; Kim et al., 2012; Isonuma et al., 2020) have been developed.",
      "startOffset" : 49,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we focus on grouping topics into a reasonable tree structure, based on the neural variational inference (NVI) framework (Kingma and Welling, 2014; Rezende et al., 2014) with a nonparametric prior.",
      "startOffset" : 135,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "In this paper, we focus on grouping topics into a reasonable tree structure, based on the neural variational inference (NVI) framework (Kingma and Welling, 2014; Rezende et al., 2014) with a nonparametric prior.",
      "startOffset" : 135,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "In (Griffiths et al., 2004), a tree-structured topic model called hLDA was first proposed by introducing a nested Chinese restaurant process (nCRP).",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Although rCRP has shown remarkable competitiveness in hierarchical topic modeling, it suffers from the major limitation of data scalability (Isonuma et al., 2020).",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "Following (Wang and Blei, 2009), where a tree-based stick-breaking construction of nCRP was first derived to draw topic paths, and then a level distribution was learned to sample topics along the path, Isonuma et al.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "For nonparametric models, stick-breaking prior is a random measure with the form G = ∑∞ k=1 πkδζk , where δζk is a discrete measure concentrated at ζk ∼ G0 (Ishwaran and James, 2001)2, i.",
      "startOffset" : 156,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "The πks are random weights independent of G0 (Nalisnick and Smyth, 2017).",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "This constructive definition is known as SBP (Sethuraman, 1994), which implies that the weights π = (πk)k=1 can be drawn according to the procedure of iteratively breaking off segments from a unit stick.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "Particularly, when vk ∼ Beta(1,β0), the joint distribution for π is the GEM distribution (Pitman, 2006) with concentration parameter β0, and the corresponding SBP is one of the constructions for the Dirichlet process, a popular nonparametric random process for topic modeling (Teh et al.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "Particularly, when vk ∼ Beta(1,β0), the joint distribution for π is the GEM distribution (Pitman, 2006) with concentration parameter β0, and the corresponding SBP is one of the constructions for the Dirichlet process, a popular nonparametric random process for topic modeling (Teh et al., 2005).",
      "startOffset" : 276,
      "endOffset" : 294
    }, {
      "referenceID" : 5,
      "context" : "As shown in Figure 2, hLDA (Griffiths et al., 2004) considers that a document is generated by topics of a single path, which violates the multi-topics assumption of topic models (i.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Considering this issue, rCRP (Kim et al., 2012) and TSNTM (Isonuma et al.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : ", 2012) and TSNTM (Isonuma et al., 2020) assume that a document can be generated by any topic in the tree.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "Figure 2: Sampling process of an example document for hLDA (Griffiths et al., 2004), rCRP (Kim et al.",
      "startOffset" : 59,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : ", 2004), rCRP (Kim et al., 2012), TSNTM (Isonuma et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "As mentioned in (Griffiths et al., 2004), a clear tree structure indicates that each sub-topic has a relationship with no more than one super-topic.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "So a softmax function with low temperature (Hinton et al., 2015) is applied to ensure that Ml,k approximates a discrete one-hot vector.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Since the Beta distribution does not have a differentiable non-centered parametrization that NVI requires (Kingma and Welling, 2014), we choose the Kumaraswamy distribution (Kumaraswamy, 1980) to approximate GEM(β0), i.",
      "startOffset" : 106,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "Since the Beta distribution does not have a differentiable non-centered parametrization that NVI requires (Kingma and Welling, 2014), we choose the Kumaraswamy distribution (Kumaraswamy, 1980) to approximate GEM(β0), i.",
      "startOffset" : 173,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : ", the conjunction of Beta(1,β0) and a stick-breaking operation (Nalisnick and Smyth, 2017).",
      "startOffset" : 63,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "In practice, we reparameterize ĝd = μd+ \u000F̂ ∗σd with the sample \u000F̂ ∼ N (0, I2) (Rezende et al., 2014).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "Then, we follow (Miao et al., 2017) to explicitly model topic-word distributions by: φ = softmax(u ∗ tT ), where u ∈ RV ∗H and t ∈ R ∑ lKl∗H are word vectors and topic vectors, and H denotes the dimension of word/topic vectors.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "We use the variational lower-bound to calculate gradients and apply Adam (Kingma and Ba, 2015) to update parameters.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "We conduct experiments on four widely used benchmark datasets: 20NEWS (Miao et al., 2017), Reuters (Wu et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 32,
      "context" : ", 2017), Reuters (Wu et al., 2020), Wikitext-103 (Nan et al.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : ", 2020), Wikitext-103 (Nan et al., 2019), and Rcv1-v2 (Miao et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "For tree-structured topic models, we adopt hLDA (Griffiths et al., 2004)3, rCRP (Kim et al.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Note that hLDA was named as nCRP (Blei et al., 2010) in (Isonuma et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "For all these models, the max-depth of topic tree is set to 3 by following (Isonuma et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "For nonparametric or flat topic models, we adopt HDP (Teh et al., 2005), GSM & GSB (Miao et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : ", 2005), GSM & GSB (Miao et al., 2017), NB-NTM & GNB-NTM (Wu et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 32,
      "context" : ", 2017), NB-NTM & GNB-NTM (Wu et al., 2020), and iTM-VAE & HiTM-VAE (Ning et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : ", 2020), and iTM-VAE & HiTM-VAE (Ning et al., 2020) as baselines.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "For iTM-VAE & HiTM-VAE, they extended the method in (Nalisnick and Smyth, 2017) to introduce nonparametric processes into the NVI framework by extracting the potential infinite topics.",
      "startOffset" : 52,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "For all parametric models, the number of topics is set to 50 and 200 as in (Miao et al., 2017).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "For neural baselines and our proposed model, we set the size of hidden layers to 256 and use one sample for NVI by following (Miao et al., 2017).",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "As mentioned in (Viegas et al., 2020), a reasonable topic hierarchy means that topics near the root should be more general while the ones close to",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "To this end, we adopt topic specialization (Kim et al., 2012) as an indicator for the evaluation of topical hierarchy.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 30,
      "context" : "A reasonable topic hierarchy also indicates that child topics are coherent with their corresponding parent topics (Viegas et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "To evaluate the topic redundancy for a tree, we introduce a new measurement named the averaged overlap rate (OR) and adopt the widely-used topic uniqueness (TU) (Nan et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : "We also present the hierarchical affinity (Kim et al., 2012) for each model to measure whether the parent topic is more similar to its child topics than the descendants of other parent topics.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10.",
      "startOffset" : 45,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10.",
      "startOffset" : 45,
      "endOffset" : 140
    }, {
      "referenceID" : 32,
      "context" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10.",
      "startOffset" : 45,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10.",
      "startOffset" : 45,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "In this part, we use the widely adopted NPMI (Miao et al., 2017; Liu et al., 2019; Wu et al., 2020; Ning et al., 2020; Isonuma et al., 2020) to evaluate topic interpretability10.",
      "startOffset" : 45,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "As mentioned in (Lau et al., 2014), the NPMI is a measurement of topic coherence which closely corresponds to the ranking of topic interpretability by human annotators.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "First, the perplexity of sampling-based and NVIbased models is difficult to compare directly (Isonuma et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Second, the prior of NVI-based methods has a large influence on the perplexity since the KL-divergence may vary greatly for different priors (Burkhardt and Kramer, 2019).",
      "startOffset" : 141,
      "endOffset" : 169
    } ],
    "year" : 2021,
    "abstractText" : "Topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure. Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process, such methods have data scalability issues. In this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference. Particularly, the latent components of the stickbreaking process are first learned for each document, then the affiliations of latent components are modeled by the dependency matrices between network layers. Utilizing this network structure, we can efficiently extract a tree-structured topic hierarchy with reasonable structure, low redundancy, and adaptable widths. Experiments on real-world datasets validate the effectiveness of our method.",
    "creator" : "LaTeX with hyperref"
  }
}