{
  "name" : "2021.acl-long.32.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries",
    "authors" : [ "Yi Yu", "Adam Jatowt", "Antoine Doucet", "Kazunari Sugiyama", "Masatoshi Yoshikawa", "Michael Jordan" ],
    "emails" : [ "yuyi@db.soc.i.kyoto-u.ac.jp", "adam.jatowt@uibk.ac.at,", "antoine.doucet@univ-lr.fr", "yoshikawa}@i.kyoto-u.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 377–387\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n377"
    }, {
      "heading" : "1 Introduction",
      "text" : "Nowadays, online news articles are one of the most popular Web documents. However, due to a huge amount of news articles available online, it is getting difficult for users to effectively search, understand, and track the entire news stories. To solve this problem, a research area of TimeLine Summarization (TLS) has been established, which can alleviate the redundancy and complexity inherent in news article collections thereby helping users better understand the news landscape.\nAfter the influential work on temporal summaries by Swan and Allan (2000), TLS has attracted researchers’ attention. Most of works on TLS (Martschat and Markert, 2018; Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020) have focused on improving the performance of summarization. However, their drawbacks are as follows: (a) the methods work essentially on a homogeneous type of datasets such as ones compiled from the search results of an unambiguous query (e.g., “BP Oil Spill”). The requirements imposed on the input dataset make it hard for TLS systems\nto generalize; (b) the output is usually a single timeline regardless of the size and the complexity of the input dataset.\nWe propose here the Multiple TimeLine Summarization (MTLS) task that enhances and further generalizes TLS. MTLS automatically generates a set of timelines that summarize disparate yet important stories, rather than always generating a single timeline as is in the case of TLS. An effective MTLS framework should: (a) detect key events including both short- and long-term events, (b) link events related to the same story and separate events belonging to other stories, and (c) provide informative summaries of constituent events to be incorporated into the generated timelines.\nMTLS can also help to deal with the ambiguity, which is common in information retrieval. For example, suppose that a user wants to get an overview of news about a basketball player, Michael Jordan, from a large collection of news articles. However, when a search engine over such a collection takes “Michael Jordan” as a query, it would likely return documents constituting a mixture of news about different persons having the same name. Then, how can a typical TLS system return meaningful results if only a single timeline can be generated? Similarly, ambiguous queries such as “Apple”, “Amazon”, “Java” require MTLS solutions to produce high quality results.\nTo address this task, we further propose a TwoStage Affinity Propagation Summarization framework (2SAPS). It uses temporal information embedded in sentences to discover important events, and their linking information latent in news articles to construct timelines. 2SAPS has several advantages: firstly, it is entirely unsupervised which is especially suited to TLS-related tasks as there are very few gold summaries available for training supervised systems; secondly, both the number of events and the number of generated timelines are\nself-determined. This allows our framework to be dependent only on the input document collection, instead of on human efforts.\nFurthermore, the current TLS evaluation measures allow only 1-to-1 comparison (system- to human-generated timeline), which is not suitable for MTLS task where multiple timelines must be compared to (typically) multiple ground-truth timelines. Therefore, we also propose a quantitative evaluation measure for MTLS based on the adaptation of the previous TLS evaluation framework.\nGiven these points, our contributions in this work are summarized as follows:\n1. We propose a novel task (MTLS), which automatically generates multiple, informative, and diverse timelines from an input time-stamped document collection.\n2. We introduce a superior MTLS model that outperforms all TLS-adapted MTLS baselines.\n3. We design an evaluation measure for MTLS systems by extending the original TLS evaluation framework."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Timeline Summarization",
      "text" : "Since the first work on timeline summarization (Swan and Allan, 2000; Allan et al., 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021). In the following, we review the major approaches.\nChieu and Lee (2004) constructed timeline by directly selecting the top ranked sentences based on the summed similarities within n-day long window. Yan et al. (2011b) proposed evolutionary timeline summarization (ETS) to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date. Shahaf et al. (2012) created information maps (Maps) to help users understand domain-specific knowledge. However, the output consists of a set of storylines that have intersections or overlaps, which is not appropriate for a dataset that may contain quite different topics. Nguyen et al. (2014) proposed a pipeline to generate timelines consisting of date selection, sentence clustering and sentence ranking.\nRecently, Martschat and Markert (2018) adapted a submodular function model for TLS task, which\nis originally used for multi-document summarization (MDS). Duan et al. (2020) introduced the task of Comparative Timeline Summarization (CTS), which captures important comparative aspects of evolutionary trajectories in two input sets of documents. The output of the CTS system is, however, always two timelines generated in a contrastive way. Then, Gholipour Ghalandari and Ifrim (2020) examined different TLS strategies and categorized TLS frameworks into the following three types: direct summarization approaches, date-wise approaches, and event detection approaches.\nTo the best of our knowledge, the idea of multiple timeline summarization has not been formally proposed yet. Table 1 compares the related tasks."
    }, {
      "heading" : "2.2 Timeline Evaluation",
      "text" : "Some works (Yan et al., 2011b; Chen et al., 2019; Duan et al., 2020) evaluate timeline by only computing ROUGE scores (Lin, 2004). This way ignores the temporal aspect of a timeline, which is important in timeline summarization. Martschat and Markert (2017) then proposed a framework, called tilse, to assess timelines from both textual and temporal aspects. Subsequently, TLS works (Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020; Born et al., 2020) have followed this framework to evaluate their models. Some researches (Tran et al., 2015; Shahaf et al., 2012; Alonso and Shiells, 2013) also involved user studies, in which users are required to score systemgenerated timelines based on varying criteria such as relevance and understandability. In Section 5, we will adapt the tilse framework to MTLS task."
    }, {
      "heading" : "3 Problem Definition",
      "text" : "We formulate MTLS task as follows: Input: A time-stamped news article collection D = {d1, d2, ..., d|D|}. The collection can be standalone or compiled from search results returned by a news search engine.\nOutput: A set of timelines, T = {T1, T2, . . . , Tk} is generated based on D, so that each timeline Ti includes a sequence of time/date1 and summary pairs (tTi1 , s Ti 1 ), . . . , (t Ti l , s Ti l ) where sTij (i = 1, . . . , k) are the summary sentences for the time tTij (j = 1, . . . , l) and l is the length of Ti. Each timeline in T should be consistent and coherent, yet different from other timelines.\n1In this paper, time and date are used as synonyms.\nWe note that while the traditional TLS task is limited as a document collection for it is typically coherent and homogeneous, MTLS is more flexible as the input news collection can be diverse. For example, the input collection can be generated using a search query q composed of multiple entities or concepts like q = {“egypt”, “h1n1”, “iraq”} or by using an ambiguous query like q = {“michael”, “jordan”}, or it can also consist of news articles crawled over a certain time span from multiple news sources. Generally, the more heterogeneous D is, the more timelines could be produced. The intuition behind this idea is that users will need more structured information to help them understand a relatively complex document collection."
    }, {
      "heading" : "4 Framework",
      "text" : "Next, we present two key components of our framework: event generation module (Sec. 4.1) and timeline generation module (Sec. 4.2).\nWe first make the following two assumptions: Assumption 1: News articles sometimes retrospectively mention past events for providing necessary context to the target event, for underlying continuation, causality, etc. Assumption 2: Sentences mentioning similar dates have higher probability to refer to the same event than sentences with different dates."
    }, {
      "heading" : "4.1 Event Generation Module",
      "text" : "In this module, we extract important historical events from a document collection. Gholipour Ghalandari and Ifrim (2020) constructed events by simply grouping articles with close publication dates into clusters, resulting in lower accuracy. Note that Assumption 1 implies that a single news article may contain multiple events. Accordingly, in our work, the concept of event is more fine-grained. We define event as a set of sentences that describe the same real-world occurrence, typically using the same identifying information (e.g., actions, entities, locations). This information is captured by sentence-BERT (Reimers and Gurevych, 2019): a\npre-trained model on a transformer network where similar meanings are positioned nearby in semantic vector space. We then employ Affinity Propagation (AP) (Frey and Dueck, 2007) following Steen and Markert (2019) for clustering similar sentences. AP algorithm groups data points by selecting a set of exemplars along with their followers due to message passing. It operates over an affinity matrix S, where S(i, j) denotes similarity between data points xi and xj .\nWe observe that high semantic similarity does not always guarantee that sentences refer to the same event. Especially, for some periodic events, similar happenings might have occurred several times. For example, a news article could include sentences reporting that Brazil won the gold medal in the World Cup (in 2002) while some other sentences in this document could recall that Brazil has won the first place in the World Cup in 1994. It is clear that those sentences describe two distinct events, which would be grouped into one event if only semantic similarity is considered.\nTherefore, based on Assumption 2, we introduce another key factor, temporal similarity, which enhances the confidence of how likely two sentences will refer to the same event. We define each element S1(vi, vj) of affinity matrix S1 as follows:\nS1(vi, vj) = α1 ·Sdate(ti, tj)+(1− α1)·Scos(vi, vj), (1) where vi and vj denote different sentences, and ti and tj denote dates mentioned by vi and vj , respectively.2 In addition, Sdate and Scos denote the temporal and semantic similarities, respectively. While we employ cosine similarity for the semantic similarity, we define temporal similarity Sdate(i, j) to quantify how similar two dates are using Equation (2):\nSdate(ti, tj) = 1\nexpγ·|ti−tj | , (2)\nwhere γ3 is the decay rate of the exponential func2We use Heideltime (Strötgen and Gertz, 2013) for resolving temporal expressions. If a sentence does not explicitly mention any date, we assume it refers to the publication date of the article.\n3We set γ = 0.05 in the experiments.\ntion. The larger the time gap between two dates, the smaller the value of Sdate.\nBy passing messages of both semantic and temporal information between sentences, clusters consisting of exemplar and non-exemplar sentences are constructed to form the candidate event set E. Each cluster represents an event.\nEvent Selection. In a timeline, it is not necessary to show all events of a story as users usually care about the most important events only. We design an event selection step that is helpful for handling excessive number of events. The selection relies on two measures: Salience and Consistency defined by Equations (3) and (4), respectively:\nSalience(e) = log(| e |) log(| D |) , (3)\nConsistency(e) =\n∑ vi∈e,vi 6=veScos(vi,ve)\n| e | −1 , (4)\nwhere ve is the exemplar sentence in event e; | e | and | D | denote the number of sentences in e and document collection D, respectively.\nIntuitively, important historical events would often be mentioned by future news reports. Salience of event is used to evaluate such importance and is computed as the relative frequency of sentences about that event compared with all sentences in the collection. On the other hand, Consistency ensures high quality of events. We then rank all candidate events based on the weighted summed score of these two measures. Hereafter, we denote the weight of Event Salience as ζ1 and that of Event Consistency as 1− ζ1.\nWe select the top-scored events obtaining a new event setE∗ by setting a threshold. To avoid tuning its value, we set the value to one standard deviation from the mean (lower end)."
    }, {
      "heading" : "4.2 Timeline Generation Module",
      "text" : "While TLS systems directly link all the identified events, MTLS requires their deeper understanding. As described in Section 1, an effective MTLS framework should link events related to the same story and separate other unrelated events to different timelines. To achieve this, we explain the following steps in this module: Event Linking, Timeline Selection, and Timeline Summarizing.\nEvent Linking. According to Assumption 1, current events can refer to related past events. We thus define a reference matrix R, in which each element R(ei, ej) denotes the degree of reference\nbetween two events ei and ej . As events in our work are represented by sentences and a sentence belongs to a single event, R(ei, ej) can be reflected by counting patterns of sentence co-occurrences in documents. Formally, R(vi, vj) represents the case where two sentences vj and vi refer to each other as defined by Equation (5):\nR(vi, vj)= { 1 vi,vj ∈ d ∧vi∈ ek,vj ∈el, ek 6= el 0 otherwise, (5)\nwhere d is an article, ek and el are elements in E∗. The degree of reference between ei and ej is then defined as follows:\nR(ei, ej) =\n∑ v1∈ei ∑ v2∈ej R(v1, v2)\n| ei | · | ej | , (6)\nwhere |ei| and |ej | are sizes of ei, ej , respectively. We then construct a graph of events where each node is an e ∈ E∗, and the value of an edge reflects the connection degree between a pair of two events. We reuse AP algorithm to detect the community of events over the affinity matrix S2 defined by Equation (7):\nS2(ei, ej) = α2 ·R(ei, ej) + (1− α2) · Scos(ei, ej), (7)\nwhere Scos(ei, ej) denotes cosine similarity between ei and ej to capture semantic similarity. Based on the affinity matrix S2, AP finally generates clusters, i.e., the initial timeline set, T .\nTimeline Selection. In order to ensure the quality of constructed timelines, we define criteria to select high-quality timelines from T . Similar to event selection described in Section 4.1, we also use two indicators to evaluate the quality of a timeline. We define Timeline Salience as the average score of Event Salience of all events within the timeline, and Timeline Coherence as the average of semantic similarity scores between any chronologically4 adjacent events defined by Equation (8):\nCoherence(T ) =\n∑ ei,ei+1∈T Scos(ei, ei+1)\n| T | −1 , (8)\nwhere | T | is the size of a timeline, i.e., the number of events in this timeline.\nIntuitively, important timelines, which reflect important stories in the document collection, are more likely to be preferred by users. Timeline Salience captures this importance by passing the importance of its components (i.e., events), while Timeline Coherence ensures that the story expressed by the timeline is consistent.\n4The time of an event e is given by its exemplar sentence.\nWe rank timelines based on a weighted sum of Timeline Salience and Timeline Coherence. The weight of Timeline Salience is denoted as ζ2; thus the weight of Timeline Coherence is 1−ζ2. We then select the top-scored elements from the timeline set T based on a threshold. Same as before, we set the value to one standard deviation from the mean.\nTimeline Summarizing. By previous steps, we have now obtained multiple timelines {T1, T2, ...}, where T is a list of events {e1, e2, ...}. However, it is not feasible to show all contents of each e as it usually contains many sentences. We use only the exemplar sentence in event since exemplar is the most typical and representative member in the group.\nIn addition, it is possible that two events ei and ej occur on the same day. In this case, we concatenate their exemplar sentences.\nTimeline Tagging. This step is an add-on to MTLS systems. To better understand the stories of constructed timelines, we believe that it should be helpful for users to also obtain a label for each timeline. As described in Section 1, the input document collection may be composed of different topics or of one topic discussed through different aspects. For example, among the timelines generated based on the topic syria, one timeline might summarize the story about Syrian civil war while another might be about Syrian political elections. A label should then help people understand the story of the timeline. We simply select the 3 most frequent words among events (excluding stopwords) for each timeline as its label."
    }, {
      "heading" : "5 Evaluation Framework",
      "text" : ""
    }, {
      "heading" : "5.1 TLS Evaluation",
      "text" : "TLS evaluation relies on ROUGE score and its variants as follows:\nConcatenation-based ROUGE (concat). It considers only the textual overlap between concatenated system summaries and ground-truth, while ignoring all date information of timeline (Yan et al., 2011b; Nguyen et al., 2014; Wang et al., 2016).\nDate-agreement ROUGE (agreement). It measures both textual and temporal information overlap by computing ROUGE score only when the date in the system-generated timelines matches the one of the ground-truth timeline (Tran et al., 2013). Otherwise, its value is 0.\nAlignment-based ROUGE. It linearly penalizes the ROUGE score by the distances of dates or/and summary contents. Martschat and Markert (2017) proposed three types of this metric: align, align+, align+m:1 (align by date, align by date and contents, align by date and contents where the map function is non-injective, respectively).\nDate selection (d-select). It evaluates how well the model works in selecting correct dates in the ground-truth (Martschat and Markert, 2018)."
    }, {
      "heading" : "5.2 MTLS evaluation",
      "text" : "The evaluation methods for TLS cannot directly assess the performance of MTLS systems as there are multiple output timelines and multiple ground-truth timelines. Concretely, given an input collection D, corresponding ground-truth timeline set G = {G1, G2, ...Gk1} (k1 ≥ 1), and system-generated timeline set T = {T1, T2, ..., Tk2} (k2 ≥ 1), evaluation metrics need information to automatically “match” the ground-truth timeline when evaluating Ti. Therefore, we make the system find the closest ground-truth G∗ to timeline T as follows:\nG∗ = argmax G∈G fm(T,G), (9)\nwhere fm is the TLS evaluation function to compute the score between T and G based on metric m, which can be either concat, agreement, align, align+, align+m:1, or d-select. Then, the overall performance of the MTLS models is computed by taking the average of all the members in T ."
    }, {
      "heading" : "6 Experimental Setup",
      "text" : "The goal of our experiments is to answer the following research questions (RQs):\nRQ1: Do MTLS models produce more meaningful output than TLS models?\nRQ2: How does 2SAPS framework perform on MTLS task compared with other MTLS baselines?\nRQ3: How effective are the components of the modules in 2SAPS? How do parameter changes in the model affect the results?"
    }, {
      "heading" : "6.1 Datasets",
      "text" : "We note that there is no available dataset for MTLS task, thus we construct MTLS datasets5 extending existing TLS datasets. Tran et al. released Timeline17 (Binh Tran et al., 2013) and Crisis (Tran et al., 2015) datasets for TLS over news articles.\n5The datasets are now available at https://yiyualt.github.io/mtlsdata/.\nTable 2 shows their statistics. To assure high complexity of data, we generate multiple datasets from TLS datasets by varying degree of story mixtures. We construct MTLS datasets based on combining TLS datasets, according to the following procedure: (1) set the number of topics L used to generate a new dataset; (2) from TLS datasets, randomly choose L topics, then merge their document collections into a new datasetD along with grouping their associated ground-truth timelines into G.6 (3) repeat steps (1) and (2). Here, the value of L reflects the complexity of the dataset. The more topics the dataset contains, the more complex it is.\nWe repeated the steps (1)~(3) on Timeline177\nand finally created 25 datasets as shown in Table 3. Timeline17 contains 9 document collections, covering the following topics: “BP Oil Spill” (bpoil), “Influenza H1N1” (h1n1), “Michael Jackson death” (mj), “Libyan War” (libya), “Egyptian Protest” (egypt), “Financial Crisis” (finan), “Haiti Earthquake” (haiti), “Iraq War” (iraq), “Syrian Crisis” (syria)."
    }, {
      "heading" : "6.2 Baselines",
      "text" : "As there are no ready models for MTLS task, we design the baselines as “divide-and-summarize” approaches. The underlying idea is: first segment the input dataset into sub-datasets (subsequently called\n6If a topic has multiple ground-truth timelines, we pick one that has length closest to the average length of the timelines for that topic.\n7We note that Crisis contains only 4 topics, resulting in few possible combinations, so we finally decided to skip it.\nsegments) by partition/division algorithms; then adopt TLS techniques to generate a timeline for each sub-dataset (segment). We now describe the choices for each step.\nDataset Division Approaches:\n• Random. We randomly decide the number of segments from 1 to 10. Then, we assign a news article to a random segment.\n• LDA (Latent Dirichlet Allocation) (Blei et al., 2003). Given a dataset, we first use LDA to detect the main topics in the dataset. Then, we assign each news article to its dominant topic.\n• K-means (MacQueen et al., 1967). We use k-means algorithm in scikit-learn.8\nTLS Approaches:\n• CHIEU2004 (Chieu and Lee, 2004): It is a frequently used unsupervised TLS baseline which selects the top-ranked sentences based on summed similaries within n-day window.\n• MARTSCHAT2018 (Martschat and Markert, 2018): It is one of the state-of-the-art TLS models and is also the first work to establish formal experimental settings for TLS task. We use the implementation given by the authors.9\n• GHALANDARI2020 (Gholipour Ghalandari and Ifrim, 2020): It constructs timeline by first predicting the important dates via a simple regression model and then selecting important sentences for each date.10\nWe combine the above 3 dataset division approaches and 3 TLS approaches and thus yield 9 baselines."
    }, {
      "heading" : "6.3 Experimental Settings",
      "text" : "Concerning the characteristics of MTLS task and our datasets, the experimental settings differ from the TLS settings applied in Martschat and Markert (2018). In particular, the settings are:\n• When generating timelines, none of the compared models knows the actual value of L (i.e., L is not an input data). The stratification given in Table 3 is shown only for the reader to explain the datasets’ construction method.\n8https://scikit-learn.org/ 9https://github.com/smartschat/tilse.\n10https://github.com/complementizer/ news-tls.\n• For the dataset-division algorithms, LDA and k-means, we use different techniques to find optimal number of segments. For LDA, we evaluate topic coherence measure (Cv score) (Röder et al., 2015) for topic numbers ranging from 1 to 10, and then choose the optimal number. For k-means, we use silhouette value (Rousseeuw, 1987) to determine the optimal number of segments.\n• All the compared methods do not take the information of the ground-truth as input. That is, the number of dates, the average number of summary sentences per date, the total number of summary sentences, the ground-truth start dates, and end dates are all unknown.\n• We set the length of timelines to 20 and summary length to 2 sentences per date."
    }, {
      "heading" : "7 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "7.1 MTLS vs. TLS",
      "text" : "We first address RQ1 to show the necessity of MTLS and to demonstrate that TLS performs poorly when an input dataset contains mixture of documents on different stories. To achieve this, we compare results of MTLS baselines with a standard TLS approach. Table 4 shows the performance comparison between TLS and MTLS baselines based on MARTCHAT2018. For fair comparison in this first experiment, we select only one timeline from MTLS outputs that is most similar to the timeline generated by TLS. We observe that when L = 1, 2, MTLS underperforms TLS by 15.1%, 4.8% in terms of align+m:1 ROUGE-1, respectively. However, it outperforms TLS by 150%, 117.1%, and 94.7% when L equals 3,4,5, respectively. This indicates that as the complexity of input document collection increases (higher L values), TLS systems do not produce good results when compared to MTLS ones. In real world scenarios, it is rather rare that the input dataset is clean enough to contain only a single topic. Thus, these results suggest that MTLS approach should in practice be more useful than TLS. The results for the other two TLS algorithms introduced in Section 6.2 show a similar trend, too. Furthermore, the example outputs of TLS and MTLS systems are also available as supplementary materials."
    }, {
      "heading" : "7.2 Performance of 2SAPS",
      "text" : "We now investigate the performance of our framework to answer RQ2. Table 5 shows the overall performance of MTLS systems. We observe that 2SAPS achieves the best performance in terms of all ROUGE metrics. In particular, when compared with CHIEU2004, MARTSCHAT2018 and GHALANDARI2020 in terms of concat ROUGE1 score, it outperforms them by 52.9%, 12.2%, and 16.4%, respectively. We also observe that GHALANDARI2020 method still achieves the best performance among baselines except for concat ROUGE-1. Furthermore, it is worth noticing that kmeans works best in dividing datasets. On average, k-means outperforms Random and LDA by 15% and 7.2%, respectively, in terms of concat ROUGE1. Finally, compared with the best-performing baseline, k-means-GHALANDARI2020, our 2SAPS outperforms it by 9.9%, 15.1%, 0%, 10%, 4.7%, 3.6%, 19.1%, in terms of concat (ROUGE-1,ROUGE2), align+m:1 (ROUGE-1,ROUGE-2), agreement (ROUGE-1,ROUGE-2) and d-select, respectively."
    }, {
      "heading" : "7.3 Ablation Study",
      "text" : "We turn to the first part of RQ3. We conduct ablation tests on Event Selection (ES) and Timeline Selection (TS) components. Table 6 shows the changes of different models. We observe that without ES, d-select and align+m:1 ROUGE-2 scores decrease 14.6% and 42.2% compared with 2SAPS. The plausible reason is that without ES, many unimportant dates and events are included in a timeline, resulting in low recall of correct dates. On the other hand, without TS component, the generated timeline set tends to contain noisy timelines, causing low ROUGE-1 as the performance drops by 18.8%."
    }, {
      "heading" : "7.4 Parameter Impact",
      "text" : "We now analyze the impact of key parameters, α1, α2, ζ1, ζ2. α1 and α2 directly influence the quality of generated events and timelines, while ζ1 and ζ2 indirectly affect the model’s performance by controlling the selection steps. Figure 1 shows the performance of 2SAPS under concat ROUGE-1, align+m:1 ROUGE-1, and agreement ROUGE-1.\nIn particular, we observe that: a smaller value of α1 (from 0.1 to 0.4) gives better results than a larger value (Figure 1a). When α1 turns to 1, AP algorithm does not converge, and the values of all measures become 0. The plausible reason for this could be that when sentence dates are very\nclose, the elements of transition matrix differ only slightly, resulting in non-convergence.\nFigure 1b shows the impact of the reference relation in linking events. The values of all metrics increase as α2 increases. It makes sense that reference relation exerts an important role in linking events into timelines, thus a higher value is necessary. However, when α2 is over 0.9, the performance drops because when news articles provide few contextual events (e.g., background events, related events, etc.), then the reference relation between events becomes unreliable.\nζ1 controls the impact of Event Salience described in Section 4.1. Another corresponding factor is Event Consistency, which is weighted by 1-ζ1. Figure 1c shows that the model with larger values of ζ1 underperforms the ones with relatively small values of ζ1 (from 0.2 to 0.4), indicating that con-\nsistency of event matters more than its salience in selecting high-quality events. Finally, in Figure 1d, we observe that along with the increase of ζ2, the performance of all metrics decrease, suggesting that the coherence of timeline is more effective than salience in selecting good timelines."
    }, {
      "heading" : "7.5 Limitations",
      "text" : "Our 2SAPS model works essentially on the unit of sentences and constructs a graph where each sentence is a node and edge is the relation between\nsentences. It has then a complexity of O(n2). Future work could address this by simplifying graph structure and providing approximate solutions to cover also the cases of processing large datasets. Another solution is to select only important sentences from news articles using the combination of classification, summarization or filtering."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We introduced MTLS task to generalize the timeline summarization problem. MTLS improves the performance of timeline summarization by generating multiple summaries. We conducted experiments to first show that given a heterogeneous time-stamped news article collection, TLS usually does not produce satisfactory result. We further proposed 2SAPS, a two-stage clustering-based framework, to effectively solve MTLS task. Furthermore, we extended TLS datasets to MTLS datasets, as well as introduced a novel evaluation measure for MTLS. Experimental results show that 2SAPS outperforms MTLS baselines which follow the “divide-and-summarize” strategy. Our work significantly improves the generalization ability of timeline summarization and can provide users with easier access to news collections. As an unsupervised approach that does not require costly training data, it can be applied to any potential datasets and languages.\nIn future work, we plan to test our approach on additional MTLS datasets. We will also investigate scenarios in which MTLS can enhance information retrieval systems operating over news article collections. For users searching over large temporal collections, structuring the returned results into a series of timelines could prove beneficial, instead of returning a usual list of interwoven documents that relate to different stories or periods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We greatly appreciate the authors in CoNLL’18 paper (Martschat and Markert, 2018) for making their data public. In particular, we wish to thank Sebastian Martschat for his great support in discussions about the experiment setup and reproduction. We also want to thank anonymous reviewers for their invaluable feedback."
    } ],
    "references" : [ {
      "title" : "Temporal Summaries of New Topics",
      "author" : [ "James Allan", "Rahul Gupta", "Vikas Khandelwal." ],
      "venue" : "Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’01), pages 10–18.",
      "citeRegEx" : "Allan et al\\.,? 2001",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 2001
    }, {
      "title" : "Clustering and Exploring Search Results Using Timeline Constructions",
      "author" : [ "Omar Alonso", "Michael Gertz", "Ricardo BaezaYates." ],
      "venue" : "Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM ’09), pages 97–",
      "citeRegEx" : "Alonso et al\\.,? 2009",
      "shortCiteRegEx" : "Alonso et al\\.",
      "year" : 2009
    }, {
      "title" : "Timelines as Summaries of Popular Scheduled Events",
      "author" : [ "Omar Alonso", "Kyle Shiells." ],
      "venue" : "Proceedings of the 22nd International Conference on World Wide Web (WWW ’13), pages 1037–1044.",
      "citeRegEx" : "Alonso and Shiells.,? 2013",
      "shortCiteRegEx" : "Alonso and Shiells.",
      "year" : 2013
    }, {
      "title" : "Predicting Relevant News Events for Timeline Summaries",
      "author" : [ "Giang Binh Tran", "Mohammad Alrifai", "Dat Quoc Nguyen." ],
      "venue" : "Proceedings of the 22nd International Conference on World Wide Web (WWW ’13), pages 91–92.",
      "citeRegEx" : "Tran et al\\.,? 2013",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2013
    }, {
      "title" : "Latent Dirichlet Allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of Machine Learning Research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Dataset Reproducibility and IR Methods in Timeline Summarization",
      "author" : [ "Leo Born", "Maximilian Bacher", "Katja Markert." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference (LREC’20), pages 1763–1771.",
      "citeRegEx" : "Born et al\\.,? 2020",
      "shortCiteRegEx" : "Born et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning Towards Abstractive Timeline Summarization",
      "author" : [ "Xiuying Chen", "Zhangming Chan", "Shen Gao", "MengHsuan Yu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Query Based Event Extraction Along a Timeline",
      "author" : [ "Hai Leong Chieu", "Yoong Keok Lee." ],
      "venue" : "Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’04), pages 425–432.",
      "citeRegEx" : "Chieu and Lee.,? 2004",
      "shortCiteRegEx" : "Chieu and Lee.",
      "year" : 2004
    }, {
      "title" : "Comparative Timeline Summarization via Dynamic Affinity-Preserving Random Walk",
      "author" : [ "Yijun Duan", "Adam Jatowt", "Masatoshi Yoshikawa." ],
      "venue" : "Proceedings of the 24th European Conference on Artificial Intelligence (ECAI’20), pages 1778–1785.",
      "citeRegEx" : "Duan et al\\.,? 2020",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2020
    }, {
      "title" : "Clustering by Passing Messages Between Data Points",
      "author" : [ "Brendan J Frey", "Delbert Dueck." ],
      "venue" : "Science, 315(5814):972–976.",
      "citeRegEx" : "Frey and Dueck.,? 2007",
      "shortCiteRegEx" : "Frey and Dueck.",
      "year" : 2007
    }, {
      "title" : "Examining the State-of-the-Art in News Timeline Summarization",
      "author" : [ "Demian Gholipour Ghalandari", "Georgiana Ifrim." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL’20), pages 1322–1334.",
      "citeRegEx" : "Ghalandari and Ifrim.,? 2020",
      "shortCiteRegEx" : "Ghalandari and Ifrim.",
      "year" : 2020
    }, {
      "title" : "Evolutionary Hierarchical Dirichlet Process for Timeline Summarization",
      "author" : [ "Jiwei Li", "Sujian Li." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), pages 556–560.",
      "citeRegEx" : "Li and Li.,? 2013",
      "shortCiteRegEx" : "Li and Li.",
      "year" : 2013
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics (ACL’04), pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "James MacQueen" ],
      "venue" : "Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281– 297.",
      "citeRegEx" : "MacQueen,? 1967",
      "shortCiteRegEx" : "MacQueen",
      "year" : 1967
    }, {
      "title" : "Improving Rouge for Timeline Summarization",
      "author" : [ "Sebastian Martschat", "Katja Markert." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL’17), pages 285–290.",
      "citeRegEx" : "Martschat and Markert.,? 2017",
      "shortCiteRegEx" : "Martschat and Markert.",
      "year" : 2017
    }, {
      "title" : "A Temporally Sensitive Submodularity Framework for Timeline Summarization",
      "author" : [ "Sebastian Martschat", "Katja Markert." ],
      "venue" : "Proceedings of the 22nd Conference on Computational Natural Language Learning (CONLL’18), pages 230–240.",
      "citeRegEx" : "Martschat and Markert.,? 2018",
      "shortCiteRegEx" : "Martschat and Markert.",
      "year" : 2018
    }, {
      "title" : "Ranking Multidocument Event Descriptions for Building Thematic Timelines",
      "author" : [ "Kiem-Hieu Nguyen", "Xavier Tannier", "Véronique Moriceau." ],
      "venue" : "Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014), pages",
      "citeRegEx" : "Nguyen et al\\.,? 2014",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2014
    }, {
      "title" : "TLS-Covid19: A New Annotated Corpus for Timeline Summarization",
      "author" : [ "Arian Pasquali", "Ricardo Campos", "Alexandre Ribeiro", "Brenda Santana", "Alípio Jorge", "Adam Jatowt." ],
      "venue" : "Proceedings of the 43rd European Conference on Information Retrieval",
      "citeRegEx" : "Pasquali et al\\.,? 2021",
      "shortCiteRegEx" : "Pasquali et al\\.",
      "year" : 2021
    }, {
      "title" : "Interactive System for Automatically Generating Temporal Narratives",
      "author" : [ "Arian Pasquali", "Vítor Mangaravite", "Ricardo Campos", "Alípio Mário Jorge", "Adam Jatowt." ],
      "venue" : "Proceedings of the 41st European Conference on Information Retrieval (ECIR 2019),",
      "citeRegEx" : "Pasquali et al\\.,? 2019",
      "shortCiteRegEx" : "Pasquali et al\\.",
      "year" : 2019
    }, {
      "title" : "SentenceBERT: Sentence Embeddings using Siamese BERTNetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP 2019), pages 3982–3992.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Exploring the Space of Topic Coherence Measures",
      "author" : [ "Michael Röder", "Andreas Both", "Alexander Hinneburg." ],
      "venue" : "Proceedings of the 8th ACM International Conference on Web Search and Data Mining (WSDM ’15), pages 399–408.",
      "citeRegEx" : "Röder et al\\.,? 2015",
      "shortCiteRegEx" : "Röder et al\\.",
      "year" : 2015
    }, {
      "title" : "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
      "author" : [ "Peter J Rousseeuw." ],
      "venue" : "Journal of Computational and Applied Mathematics, 20:53–65.",
      "citeRegEx" : "Rousseeuw.,? 1987",
      "shortCiteRegEx" : "Rousseeuw.",
      "year" : 1987
    }, {
      "title" : "Trains of Thought: Generating Information Maps",
      "author" : [ "Dafna Shahaf", "Carlos Guestrin", "Eric Horvitz." ],
      "venue" : "Proceedings of the 21st International Conference on World Wide Web (WWW ’12), pages 899–908.",
      "citeRegEx" : "Shahaf et al\\.,? 2012",
      "shortCiteRegEx" : "Shahaf et al\\.",
      "year" : 2012
    }, {
      "title" : "Abstractive Timeline Summarization",
      "author" : [ "Julius Steen", "Katja Markert." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization (NewSum’19), pages 21–31.",
      "citeRegEx" : "Steen and Markert.,? 2019",
      "shortCiteRegEx" : "Steen and Markert.",
      "year" : 2019
    }, {
      "title" : "Multilingual and Cross-Domain Temporal Tagging",
      "author" : [ "Jannik Strötgen", "Michael Gertz." ],
      "venue" : "Language Resources and Evaluation, 47(2):269–298.",
      "citeRegEx" : "Strötgen and Gertz.,? 2013",
      "shortCiteRegEx" : "Strötgen and Gertz.",
      "year" : 2013
    }, {
      "title" : "On-line Summarization of Time-Series Documents Using a Graph-Based Algorithm",
      "author" : [ "Satoko Suzuki", "Ichiro Kobayashi." ],
      "venue" : "Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing (PACLIC’14), pages 470–478.",
      "citeRegEx" : "Suzuki and Kobayashi.,? 2014",
      "shortCiteRegEx" : "Suzuki and Kobayashi.",
      "year" : 2014
    }, {
      "title" : "Automatic Generation of Overview Timelines",
      "author" : [ "Russell Swan", "James Allan." ],
      "venue" : "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’00), pages 49–56.",
      "citeRegEx" : "Swan and Allan.,? 2000",
      "shortCiteRegEx" : "Swan and Allan.",
      "year" : 2000
    }, {
      "title" : "Summarizing a Document Stream",
      "author" : [ "Hiroya Takamura", "Hikaru Yokono", "Manabu Okumura." ],
      "venue" : "Proceedings of the 33rd European Conference on Information Retrieval (ECIR 2011), pages 177–188.",
      "citeRegEx" : "Takamura et al\\.,? 2011",
      "shortCiteRegEx" : "Takamura et al\\.",
      "year" : 2011
    }, {
      "title" : "Timeline Summarization From Relevant Headlines",
      "author" : [ "Giang Tran", "Mohammad Alrifai", "Eelco Herder." ],
      "venue" : "Proceedings of the 37th European Conference on Information Retrieval (ECIR 2015), pages 245–256.",
      "citeRegEx" : "Tran et al\\.,? 2015",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2015
    }, {
      "title" : "Leveraging Learning to Rank in an Optimization Framework for Timeline Summarization",
      "author" : [ "Giang Binh Tran", "Tuan A Tran", "Nam-Khanh Tran", "Mohammad Alrifai", "Nattiya Kanhabua." ],
      "venue" : "Proceedings of SIGIR 2013 Workshop on Time-aware",
      "citeRegEx" : "Tran et al\\.,? 2013",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2013
    }, {
      "title" : "A Low-Rank Approximation Approach to Learning Joint Embeddings of News Stories and Images for Timeline Summarization",
      "author" : [ "William Yang Wang", "Yashar Mehdad", "Dragomir Radev", "Amanda Stent." ],
      "venue" : "Proceedings of the 15th Conference of the",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Timeline Generation Through Evolutionary Trans-temporal Summarization",
      "author" : [ "Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Yan et al\\.,? 2011a",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2011
    }, {
      "title" : "Evolutionary Timeline Summarization: a Balanced Optimization Framework via Iterative Substitution",
      "author" : [ "Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang." ],
      "venue" : "Proceedings of the 34th international ACM SIGIR Confer-",
      "citeRegEx" : "Yan et al\\.,? 2011b",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2011
    }, {
      "title" : "Timeline Generation with Social Attention",
      "author" : [ "Xin Wayne Zhao", "Yanwei Guo", "Rui Yan", "Yulan He", "Xiaoming Li." ],
      "venue" : "Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’13),",
      "citeRegEx" : "Zhao et al\\.,? 2013",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Most of works on TLS (Martschat and Markert, 2018; Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020) have focused on improving the performance of summarization.",
      "startOffset" : 21,
      "endOffset" : 113
    }, {
      "referenceID" : 23,
      "context" : "Most of works on TLS (Martschat and Markert, 2018; Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020) have focused on improving the performance of summarization.",
      "startOffset" : 21,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "Since the first work on timeline summarization (Swan and Allan, 2000; Allan et al., 2001), this topic has received much attention over the years (Alonso et al.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Since the first work on timeline summarization (Swan and Allan, 2000; Allan et al., 2001), this topic has received much attention over the years (Alonso et al.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 31,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 33,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 3,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 11,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 25,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 30,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 27,
      "context" : ", 2001), this topic has received much attention over the years (Alonso et al., 2009; Yan et al., 2011a; Zhao et al., 2013; Tran et al., 2013; Li and Li, 2013; Suzuki and Kobayashi, 2014; Wang et al., 2016; Takamura et al., 2011; Pasquali et al., 2019, 2021).",
      "startOffset" : 63,
      "endOffset" : 257
    }, {
      "referenceID" : 32,
      "context" : "Some works (Yan et al., 2011b; Chen et al., 2019; Duan et al., 2020) evaluate timeline by only computing ROUGE scores (Lin, 2004).",
      "startOffset" : 11,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Some works (Yan et al., 2011b; Chen et al., 2019; Duan et al., 2020) evaluate timeline by only computing ROUGE scores (Lin, 2004).",
      "startOffset" : 11,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Some works (Yan et al., 2011b; Chen et al., 2019; Duan et al., 2020) evaluate timeline by only computing ROUGE scores (Lin, 2004).",
      "startOffset" : 11,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : ", 2020) evaluate timeline by only computing ROUGE scores (Lin, 2004).",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Subsequently, TLS works (Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020; Born et al., 2020) have followed this framework to evaluate their models.",
      "startOffset" : 24,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Subsequently, TLS works (Steen and Markert, 2019; Gholipour Ghalandari and Ifrim, 2020; Born et al., 2020) have followed this framework to evaluate their models.",
      "startOffset" : 24,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "Some researches (Tran et al., 2015; Shahaf et al., 2012; Alonso and Shiells, 2013) also involved user studies, in which users are required to score systemgenerated timelines based on varying criteria such as relevance and understandability.",
      "startOffset" : 16,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Some researches (Tran et al., 2015; Shahaf et al., 2012; Alonso and Shiells, 2013) also involved user studies, in which users are required to score systemgenerated timelines based on varying criteria such as relevance and understandability.",
      "startOffset" : 16,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "Some researches (Tran et al., 2015; Shahaf et al., 2012; Alonso and Shiells, 2013) also involved user studies, in which users are required to score systemgenerated timelines based on varying criteria such as relevance and understandability.",
      "startOffset" : 16,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "This information is captured by sentence-BERT (Reimers and Gurevych, 2019): a pre-trained model on a transformer network where similar meanings are positioned nearby in semantic vector space.",
      "startOffset" : 46,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "We then employ Affinity Propagation (AP) (Frey and Dueck, 2007) following Steen",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "We use Heideltime (Strötgen and Gertz, 2013) for resolving temporal expressions.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "It considers only the textual overlap between concatenated system summaries and ground-truth, while ignoring all date information of timeline (Yan et al., 2011b; Nguyen et al., 2014; Wang et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 201
    }, {
      "referenceID" : 16,
      "context" : "It considers only the textual overlap between concatenated system summaries and ground-truth, while ignoring all date information of timeline (Yan et al., 2011b; Nguyen et al., 2014; Wang et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 201
    }, {
      "referenceID" : 30,
      "context" : "It considers only the textual overlap between concatenated system summaries and ground-truth, while ignoring all date information of timeline (Yan et al., 2011b; Nguyen et al., 2014; Wang et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 201
    }, {
      "referenceID" : 3,
      "context" : "It measures both textual and temporal information overlap by computing ROUGE score only when the date in the system-generated timelines matches the one of the ground-truth timeline (Tran et al., 2013).",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "It evaluates how well the model works in selecting correct dates in the ground-truth (Martschat and Markert, 2018).",
      "startOffset" : 85,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : ", 2013) and Crisis (Tran et al., 2015) datasets for TLS over news articles.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "• LDA (Latent Dirichlet Allocation) (Blei et al., 2003).",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "• CHIEU2004 (Chieu and Lee, 2004): It is a frequently used unsupervised TLS baseline which selects the top-ranked sentences based on summed similaries within n-day window.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "• MARTSCHAT2018 (Martschat and Markert, 2018): It is one of the state-of-the-art TLS models and is also the first work to establish formal experimental settings for TLS task.",
      "startOffset" : 16,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "For LDA, we evaluate topic coherence measure (Cv score) (Röder et al., 2015) for topic numbers ranging from 1 to 10, and then choose the optimal number.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : "For k-means, we use silhouette value (Rousseeuw, 1987) to determine the optimal number of segments.",
      "startOffset" : 37,
      "endOffset" : 54
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of TimeLine Summarization (TLS). Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding timeline for each story. To achieve this, we propose a novel unsupervised summarization framework based on the two-stage affinity propagation process. We also introduce a quantitative evaluation measure for MTLS based on the previous TLS evaluation methods. Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can provide better results than TLS.",
    "creator" : "LaTeX with hyperref"
  }
}