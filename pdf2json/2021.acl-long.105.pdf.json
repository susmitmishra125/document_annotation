{
  "name" : "2021.acl-long.105.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study",
    "authors" : [ "Yash Khemchandani", "Sarvesh Mehtani", "Vaidehi Patil", "Abhijeet Awasthi", "Partha Talukdar", "Sunita Sarawagi" ],
    "emails" : [ "yashkhem@cse.iitb.ac.in", "smehtani@cse.iitb.ac.in", "awasthi@cse.iitb.ac.in", "sunita@cse.iitb.ac.in", "vaidehipatil@ee.iitb.ac.in,", "partha@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1312–1323\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1312"
    }, {
      "heading" : "1 Introduction",
      "text" : "BERT-based pre-trained language models (LMs) have enabled significant advances in NLP (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). Pretrained LMs have also been developed for the multilingual setting, where a single multilingual model is capable of handling inputs from many different\n∗Authors contributed equally\nlanguages. For example, the Multilingual BERT (mBERT) (Devlin et al., 2019) model was trained on 104 different languages. When fine-tuned for various downstream tasks, multilingual LMs have demonstrated significant success in generalizing across languages (Hu et al., 2020; Conneau et al., 2019). Thus, such models make it possible to transfer knowledge and resources from resource rich languages to Low Web-Resource Languages (LRL). This has opened up a new opportunity towards rapid development of language technologies for LRLs.\nHowever, there is a challenge. The current paradigm for training Mutlilingual LM requires text corpora in the languages of interest, usually in large volumes. However, such text corpora is often available in limited quantities for LRLs. For example, in Figure 1 we present the size of Wikipedia, a common source of corpora for training LMs, for top-few scheduled Indian languages1 and English. The top-2 languages are just one-fiftieth the size of\n1According to Indian Census 2011, more than 19,500 languages or dialects are spoken across the country, with 121 of them being spoken by more than 10 thousand people.\nEnglish, and yet Hindi is seven times larger than the O(20,000) documents of languages like Oriya and Assamese which are spoken by millions of people. This calls for the development of additional mechanisms for training multilingual LMs which are not exclusively reliant on large monolingual corpora.\nRecent methods of adapting a pre-trained multilingual LM to a LRL include fine-tuning the full model with an extended vocabulary (Wang et al., 2020), training a light-weight adapter layer while keeping the full model fixed (Pfeiffer et al., 2020b), and exploiting overlapping tokens to learn embeddings of the LRL (Pfeiffer et al., 2020c). These are general-purpose methods that do not sufficiently exploit the specific relatedness of languages within the same family.\nWe propose RelateLM for this task. RelateLM exploits relatedness between the LRL of interest and a Related Prominent Language (RPL). We focus on Indic languages, and consider Hindi as the RPL. The languages we consider in this paper are related along several dimensions of linguistic typology (Dryer and Haspelmath, 2013; Littell et al., 2017): phonologically, phylogenetically as they are all part of the Indo-Aryan family, geographically, and syntactically matching on key features like the Subject-Object-Verb (SOV) order as against the Subject-Verb-Object (SVO) order in English. Even though the scripts of several Indic languages differ, they are all part of the same Brahmic family, making it easier to design rulebased transliteration libraries across any language pair. In contrast, transliteration of Indic languages to English is harder with considerable phonetic variation in how words are transcribed. The geographical and phylogenetic proximity has lead to significant overlap of words across languages. This implies that just after transliteration we are able to exploit overlap with a Related Prominent Language (RPL) like Hindi. On three Indic languages we discover between 11% and 26% overlapping tokens with Hindi, whereas with English it is less than 8%, mostly comprising numbers and entity names. Furthermore, the syntax-level similarity between languages allows us to generate high quality data augmentation by exploiting pre-existing bilingual dictionaries. We generate pseudo parallel data by converting RPL text to LRL and vice-versa. These allow us to further align the learned embed-\ndings across the two languages using the recently proposed loss functions for aligning contextual embeddings of word translations (Cao et al., 2020; Wu and Dredze, 2020). In this paper, we make the following contributions:\n• We address the problem of adding a Low WebResource Language (LRL) to an existing pretrained LM, especially when monolingual corpora in the LRL is limited. This is an important but underexplored problem. We focus on Indian languages which have hundred of millions of speakers, but traditionally understudied in the NLP community.\n• We propose RelateLM which exploits relatedness among languages to effectively incorporate a LRL into a pre-trained LM. We highlight the relevance of transliteration and pseudo translation for related languages, and use them effectively in RelateLM to adapt a pre-trained LM to a new LRL.\n• Through extensive experiments, we find that RelateLM is able to gain significant improvements on benchmark datasets. We demonstrate how RelateLM adapts mBERT to Oriya and Assamese, two low web-resource Indian languages by pivoting through Hindi. Via ablation studies on bilingual models we show that RelateLM is able to achieve accuracy of zero-shot transfer with limited data (20K documents) that is not surpassed even with four times as much data in existing methods.\nThe source code for our experiments is available at https://github.com/yashkhem1/RelateLM."
    }, {
      "heading" : "2 Related Work",
      "text" : "Transformer (Vaswani et al., 2017) based language models like mBERT (Devlin et al., 2019), MuRIL (Khanuja et al., 2021), IndicBERT (Kakwani et al., 2020), and XLM-R (Conneau et al., 2019), trained on massive multilingual datasets have been shown to scale across a variety of tasks and languages. The zero-shot cross-lingual transferability offered by these models makes them promising for lowresource domains. Pires et al. (2019) find that cross-lingual transfer is even possible across languages of different scripts, but is more effective for typologically related languages. However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly pre-trained. A primary reason behind poor transfer\nis the lack of model’s capacity to accommodate all languages simultaneously. This has led to increased interest in adapting multilingual LMs to LRLs and we discuss these in the following two settings.\nLRL adaptation using monolingual data For eleven languages outside mBERT, Wang et al. (2020) demonstrate that adding a new target language to mBERT by simply extending the embedding layer with new weights results in better performing models when compared to bilingual-BERT pre-training with English as the second language. Pfeiffer et al. (2020c) adapt multilingual LMs to the LRLs and languages with scripts unseen during pre-training by learning new tokenizers for the unseen script and initializing their embedding matrix by leveraging the lexical overlap w.r.t. the languages seen during pre-training. Adapter (Pfeiffer et al., 2020a) based frameworks like (Pfeiffer et al., 2020b; Artetxe et al., 2020; Üstün et al., 2020) address the lack of model’s capacity to accommodate multiple languages and establish the advantages of adding language-specific adapter modules in the BERT model for accommodating LRLs. These methods generally assume access to a fair amount of monolingual LRL data and do not exploit relatedness across languages explicitly. These methods provide complimentary gains to our method of directly exploiting language relatedness.\nLRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show that pre-training on concatenated parallel sentences results in improved cross-lingual transfer. Methods like Cao et al. (2020); Wu and Dredze (2020) discuss advantages of explicitly bringing together the contextual embeddings of aligned words in a translated pair. Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020). These methods typically involve data augmentation for a LRL with help of a related high resource language (RPL) or to first learn the NMT model for a RPL followed by finetuning on the LRL. Wang et al. (2019) propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs to improve encoder representations for LRLs. Gao et al. (2020) address the issue of generating fluent\nLRL sentences in NMT by extending the softdecoupled encoding approach to improve decoder representations for LRLs. Xia et al. (2019) utilize data augmentation techniques for LRL-English translation using RPL-English and RPL-LRL parallel corpora induced via bilingual lexicons and unsupervised NMT. Goyal et al. (2020) utilize transliteration and parallel data from related Indo-Aryan languages to improve NMT systems. Similar to our approach they transliterate all the Indian languages to the Devanagri script. Similarly, Song et al. (2020) utilize Chinese-English parallel corpus and transliteration of Chinese to Japanese for improving Japanese-English NMT systems via data augmentation.\nTo the best of our knowledge no earlier work has explored the surprising effectiveness of transliteration to a related existing prominent language, for learning multilingual LMs, although some work exists in NMT as mentioned above."
    }, {
      "heading" : "3 Low Web-Resource Adaptation in RelateLM",
      "text" : "Problem Statement and Notations Our goal is to augment an existing multilingual language model M, for example mBERT, to learn representations for a new LRL L for which available monolingual corpusDL is limited. We are also told that the language to be added is related to another language R on which the modelM is already pretrained, and is of comparatively higher resource. However, the script of DL may be distinct from the scripts of existing languages inM. In this section we present strategies for using this knowledge to\nbetter adaptM to L than the existing baseline of fine-tuningM using the standard masked language model (MLM) loss on the limited monolingual data DL (Wang et al., 2020). In addition to the monolingual data DR in the RPL and DL in the LRL, we have access to a limited bilingual lexicon BL→R that map a word in language L to a list of synonyms in language R and vice-versa BR→L.\nWe focus on the case where the RPL, LRL pairs are part of the Indo-Aryan language families where several levels of relatedness exist. Our proposed approach, consists of three steps, viz., Transliteration to RPL’s script, Pseudo translation, and Adaptation through Pre-training. We describe each of these steps below. Figure 2 presents an overview of our approach."
    }, {
      "heading" : "3.1 Transliteration",
      "text" : "First, the scripts of Indo-Aryan languages are part of the same Brahmic script. This makes it easier to design simple rule-based transliterators to convert a corpus in one script to another. For most languages transliterations are easily available. Example, the Indic-Trans Library 2 (Bhat et al., 2015). We use DLR to denote the LRL corpus after transliterating to the script of the RPL. We then propose to further pre-train the modelM with MLM on the transliterated corpus DLR instead of DL. Such a strategy could provide little additional gains over the baseline, or could even hurt accuracy, if the two languages were not sufficiently related. For languages in the Indo-Aryan family because of strong phylogenetic and geographical overlap, many words across the two languages overlap and preserve the\n2https://github.com/libindic/ indic-trans\nsame meaning. In Table 1 we provide statistics of the overlap of words across several transliterated Indic languages with Hindi and English. Note that for Hindi the fraction of overlapping words is much higher than with English which are mostly numbers, and entity names. These overlapping words serve as anchors to align the representations for the non-overlapping words of the LRL that share semantic space with words in the RPL."
    }, {
      "heading" : "3.2 Pseudo Translation with Lexicons",
      "text" : "Parallel data between a RPL and LRL language pair has been shown to be greatly useful for efficient adaptation to LRL (Conneau and Lample, 2019; Cao et al., 2020). However, creation of parallel data requires expensive supervision, and is not easily available for many low web-resource languages. Back-translation is a standard method of creating pseudo parallel data but for low webresource languages we cannot assume the presence of a well-trained translation system. We exploit the relatedness of the Indic languages to design a pseudo translation system that is motivated by two factors:\n• First, for most geographically proximal RPLLRL language pairs, word-level bilingual dictionaries have traditionally been available to enable communication. When they are not, crowd-sourcing creation of wordlevel dictionaries3 requires lower skill and resources than sentence level parallel data. Also, word-level lexicons can be created semiautomatically (Zhang et al., 2017) (Artetxe et al., 2019) (Xu et al., 2018).\n• Second, Indic languages exhibit common syntactic properties that control how words are composed to form a sentence. For example, they usually follow the Subject-ObjectVerb (SOV) order as against the Subject-VerbObject (SVO) order in English.\nWe therefore create pseudo parallel data between R and L via a simple word-by-word translation using the bilingual lexicon. In a lexicon a word can be mapped to multiple words in another language. We choose a word with probability proportional to its frequency in the monolingual corpus DL. We experimented with a few other methods of selecting words that we discuss in Section 4.4. In Table 2 we present BLEU scores obtained by our pseudo translation model of three Indic languages from\n3Wiktionary is one such effort\nHindi and from English. We observe much high BLEU for translation from Hindi highlighting the syntactic relatedness of the languages.\nLet (DR, BR→LR(DR)) denote the parallel corpus formed by pseudo translating the RPL corpus via the transliterated RPL to LRL lexicon. Likewise let (DLR , BLR→R(DLR)) be formed by pseudo translating the transliterated low web-resource corpus via the transliterated LRL to RPL lexicon."
    }, {
      "heading" : "3.3 Alignment Loss",
      "text" : "The union of the two pseudo parallel corpora above, collectively called P , is used for fine-tuning M using an alignment loss similar to the one proposed in (Cao et al., 2020). This loss attempts to bring the multilingual embeddings of different languages closer by aligning the corresponding word embeddings of the source language sentence and the pseudo translated target language sentence. Let C be a random batch of source and (pseudo translated) target sentence pairs from P , i.e. C = ((s1, t1), (s2, t2), ..., (sN , tN )), where s and t are the source and target sentences respectively. Since our parallel sentences are obtained via word-level translations, the alignment among words is known and monotonic. Alignment loss has two terms: L = Lalign+Lreg where Lalign is used to bring the contextual embeddings closer and Lreg is the regularization loss which prevents the new embeddings from deviating far away from the pre-trained embeddings. Each of these are defined below: Lalign = ∑\n(s,t)∈C #word(s)∑ i=1 ||f(s, ls(i))−f(t, lt(i))||22\nLreg = ∑\n(s,t)∈C #tok(s)∑ j=1 ||(f(s, j)− f0(s, j)||22\n+ #tok(t)∑ j=1 ||f(t, j)− f0(t, j)||22  where ls(i) is the position of the last token of ith word in sentence s and f(s, j) is the learned contextual embedding of token at j-th position in sentence s, i.e, for Lalign we consider only the last tokens of words in a sentence, while for Lreg we consider all the tokens in the sentence. f0(s, j) denotes the fixed pre-trained contextual embedding of the token at j-th position in sentence s. #word(s) and #tok(s) are the number of (whole) words and tokens in sentence s respectively."
    }, {
      "heading" : "4 Experiments",
      "text" : "We carry out the following experiments to evaluate RelateLM’s effectiveness in LRL adaptation:\n• First, in the full multilingual setting, we evaluate whether RelateLM is capable of extending mBERT with two unseen low-resource Indic languages: Oriya (unseen script) and Assamese (seen script). (Section 4.2)\n• We then move to the bilingual setting where we use RelateLM to adapt a model trained on a single RPL to a LRL. This setting allowed us to cleanly study the impact of different adaptation strategies and experiment with many RPL-LRL language pairs. (Section 4.3)\n• Finally, Section 4.4, presents an ablation study on dictionary lookup methods, alignment losses, and corpus size.\nWe evaluate by measuring the efficacy of zeroshot transfer from the RPL on three different tasks: NER, POS and text classification."
    }, {
      "heading" : "4.1 Setup",
      "text" : "LM Models We take m-BERT as the modelM for our multilingual experiments. For the bilingual experiments, we start with two separate monolingual language models on each of Hindi and English language to serve as M. For Hindi we trained our own Hi-BERT model over the 160K monolingual Hindi Wikipedia articles using a vocab size of 20000 generated using WordPiece tokenizer. For English we use the pre-trained BERT model which is trained on almost two orders of magnitude Wikipedia articles and more. When the LRL is added in its own script, we use the bert-base-cased model and when the LRL is added after transliteration to English, we use the bert-base-uncased model.\nLRLs, Monolingual Corpus, Lexicon As LRLs we consider five Indic languages spanning four different scripts. Monolingual data was obtained from Wikipedia as summarized in Table 4. We extend m-BERT with two unseen low webresource languages: Assamese and Oriya. Since it was challenging to find Indic languages with taskspecific labeled data but not already in m-BERT, we could not evaluate on more than two languages. For the bilingual model experiments, we adapt each of Hi-BERT and English BERT with three different languages: Punjabi, Gujarati and Bengali. For these languages we simulated the LRL setting by\ndownsampling their Wikipedia data to 20K documents. For experiments where we require English monolingual data for creating pseudo translations, we use a downsampled version of English Wikipedia having the same number of documents as the Hindi Wikipedia dump.\nThe addition of a new language toM was done by adding 10000 tokens of the new language generated by WordPiece tokenization to the existing vocabulary, with random initialization of the new parameters. For all the experiments, we use libindic’s indictrans library (Bhat et al., 2015) for transliteration. For pseudo translation we use the union of Bilingual Lexicons obtained from CFILT 4 and Wiktionary 5 and their respective sizes for each language are summarized in Table 4\nTasks for zero-shot transfer evaluation After adding a LRL inM, we perform task-specific fine-\n4https://www.cfilt.iitb.ac.in/ 5https://hi.wiktionary.org/wiki/\ntuning on the RPL separately for three tasks: NER, POS and Text classification. Table 3 presents a summary of the training, validation data in RPL and test data in LRL on which we perform zero-shot evaluation. We obtained the NER data from WikiANN (Pan et al., 2017) and XTREME (Hu et al., 2020) and the POS and Text Classification data from the Technology Development for Indian Languages (TDIL)6. We downsampled the TDIL data for each language to make them class-balanced. The POS tagset used was the BIS Tagset (Sardesai et al., 2012). For the English POS Dataset, we had to map the PENN tagset in to the BIS tagset. We have provided the mapping that we used in the Appendix (B)\nMethods compared We contrast RelateLM with three other adaptation techniques: (1) EBERT (Wang et al., 2020) that extends the vocabulary and tunes with MLM on DL as-is, (2) RelateLM without pseudo translation loss, and (3) m-BERT when the language exists in m-BERT.\nTraining Details For pre-training on MLM we chose batch size as 2048, learning rate as 3e-5 and maximum sequence length as 128. We used whole word masking for MLM and BertWordPieceTokenizer for tokenization. For pre-training Hi-BERT the duplication was taken as 5 with training done for 40K iterations. For all LRLs where monolingual data used was 20K documents, the duplication factor was kept at 20 and and training was done for 24K iterations. For Assamese, where monolingual data was just 6.5K documents, a duplication factor of 60 was used with the same 24K training iterations. The MLM pre-training was done on Google v3-8 Cloud TPUs.\nFor alignment loss on pseudo translation we chose learning-rate as 5e-5, batch size as 64 and\n6https://www.tdil-dc.in\nmaximum sequence length as 128. The training was done for 10 epochs also on Google v3-8 Cloud TPUs. For task-specific fine-tuning we used learning-rate 2e-5 and batch size 32, with training duration as 10 epochs for NER, 5 epochs for POS and 2400 iterations for Text Classification. The models were evaluated on a separate RPL validation dataset and the model with the minimum F1-score, accuracy and validation loss was selected for final evaluation for NER, POS and Text Classification respectively. All the fine-tuning experiments were done on Google Colaboratory. The results reported for all the experiments are an average of 3 independent runs."
    }, {
      "heading" : "4.2 Multilingual Language Models",
      "text" : "We evaluate RelateLM’s adaptation strategy on mBERT, a state of the art multilingual model with two unseen languages: Oriya and Assamese. The script of Oriya is unseen whereas the script of Assamese is the same as Bengali (already in m-BERT). Table 6 compares different adaptation strategies in-\ncluding the option of treating each of Hindi and English as RPL for transliteration into. For both LRLs, transliterating to Hindi as RPL provides gains over EBERT that keeps the script as-is and English transliteration. We find that these gains are much more significant for Oriya than Assamese, which could be because Oriya is a new script. Further augmentation with pseudo translations with Hindi as RPL, provides significant added gains. We have not included the NER results for Assamese due to the absence of good quality evaluation dataset."
    }, {
      "heading" : "4.3 Bilingual Language Models",
      "text" : "For more extensive experiments and ablation studies we move to bilingual models. Table 5 shows the results of different methods of adaptingM to a LRL with Hi-BERT and BERT as two choices of M. We obtain much higher gains when the LRL is transliterated to Hindi than to English or keeping the script as-is. This suggests that transliteration to a related language succeeds in parameter sharing between a RPL and a LRL. Note that the English BERT model is trained on a much larger English corpus than the Hi-BERT model is trained on the Hindi corpus. Yet, because of the relatedness of the languages we get much higher accuracy when adding transliterated data to Hindi rather than to English. Next observe that pre-training with alignment loss on pseudo translated sentence pairs improves upon the results obtained with transliteration. This shows that pseudo translations is a decent alternative when a parallel translation corpora is not available.\nOverall, we find that RelateLM provides substantial gains over the baseline. In many cases RelateLM is even better than mBERT which was pretrained on a lot more monolingual data in that language. Among the three languages, we obtain lowest gains for Bengali since the phonetics of Bengali\nvaries to some extent from other Indo-Aryan languages, and Bengali shows influence from TibetoBurman languages too (Kunchukuttan and Bhattacharyya, 2020). This is also evident in the lower word overlap and lower BLEU in Table 1 and Table 2 compared to other Indic languages. We further find that in case of Bengali, the NER results are best when Bengali is transliterated to English rather than Hindi, which we attribute to the presence of English words in the NER evaluation dataset."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "Methods of Dictionary Lookups We experimented with various methods of choosing the translated word from the lexicon which may have multiple entries for a given word. In Table 7 we compare four methods of picking entries: first - en-\ntry at first position, max-entry with maximum frequency in the monolingual data, weighted - entry with probability proportional to that frequency and root-weighted - entry with probability proportional to the square root of that frequency. We find that these four methods are very close to each other, with the weighted method having a slight edge.\nAlignment Loss We compare the MSE-based loss we used with the recently proposed contrastive loss (Wu and Dredze, 2020) for Lalign but did not get any significant improvements. We have provided the results for additional experiments in the Appendix (A)\nIncreasing Monolingual size In Figure 3 we increase the monolingual LRL data used for adapting EBERT four-fold and compare the results. We observe that even on increasing monolingual data, in most cases, by being able to exploit language relatedness, RelateLM outperforms the EBERT model with four times more data. These experiments show that for zero-shot generalization on NLP tasks, it is more important to improve the alignment among languages by exploiting their relatedness, than to add more monolingual data."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We address the problem of adapting a pre-trained language model (LM) to a Low Web-Resource Language (LRL) with limited monolingual corpora. We propose RelateLM, which explores relatedness between the LRL and a Related Prominent Language (RPL) already present in the LM. RelateLM exploits relatedness along two dimensions – script relatedness through transliteration, and sentence structure relatedness through pseudo translation. We focus on Indic languages, which have hundreds of millions of speakers, but are understudied in the NLP community. Our experiments provide evidence that RelateLM is effective in adapting multilingual LMs (such as mBERT) to various LRLs. Also, RelateLM is able to achieve zero-shot transfer with limited LRL data (20K documents) which is not surpassed even with 4X more data by existing baselines. Together, our experiments establish that using a related language as pivot, along with data augmentation through transliteration and bilingual dictionary-based pseudo translation, can be an effective way of adapting an LM for LRLs, and that this is more effective than direct training or pivoting through English.\nIntegrating RelateLM with other complementary methods for adapting LMs for LRLs (Pfeiffer et al., 2020b,c) is something we plan to pursue next. We are hopeful that the idea of utilizing relatedness to adapt LMs for LRLs will be effective in adapting LMs to LRLs in other languages families, such as South-east Asian and Latin American languages. We leave that and exploring other forms of relatedness as fruitful avenues for future work.\nAcknowledgements We thank Technology Development for Indian Languages (TDIL) Programme initiated by the Ministry of Electronics Information Technology, Govt. of India for providing us datasets used in this study. The experiments reported in the paper were made possible by a Tensor Flow Research Cloud (TFRC) TPU grant. The IIT Bombay authors thank Google Research India for supporting this research. We thank Dan Garrette and Slav Petrov for providing comments on an earlier draft."
    }, {
      "heading" : "A Additional Experiments with Contrastive Loss",
      "text" : "Apart from MSE loss, we also experimented with the recently proposed Contrastive Loss. We present the results of using contrastive loss with various methods of dictionary lookups as described in Section 4 of the paper, in Table 8"
    }, {
      "heading" : "B POS Tagset mapping between Penn Treebank Tagset and BIS Tagset",
      "text" : "For the POS experiments involving m-BERT as the base model, we fine-tune our trained model with both English and Hindi training data and calculate zero-shot results on the target language. However, the English dataset that we used was annotated using Penn Treebank Tagset while the rest of the languages were annotated using BIS Tagset. We came up with a mapping between the Penn Tags and the BIS Tags so that the English POS dataset becomes consistent with the Hindi counterpart. Table 9 contains the mapping that we used for the said conversion. Note that since we are using toplevel tags (e.g Pronouns) instead of sub-level tags\n(e.g Personal Pronouns, Possessive Pronouns) for the POS classification, the mapping is also done to reflect the same."
    } ],
    "references" : [ {
      "title" : "Bilingual lexicon induction through unsupervised machine translation",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5002–5007, Florence, Italy. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2019",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2019
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Iiit-h system submission for fire2014 shared task on transliterated search",
      "author" : [ "Irshad Ahmad Bhat", "Vandan Mujadia", "Aniruddha Tammewar", "Riyaz Ahmad Bhat", "Manish Shrivastava." ],
      "venue" : "Proceedings of the Forum for Information Retrieval Evaluation, FIRE",
      "citeRegEx" : "Bhat et al\\.,? 2015",
      "shortCiteRegEx" : "Bhat et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilingual alignment of contextual word representations",
      "author" : [ "Steven Cao", "Nikita Kitaev", "Dan Klein." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving target-side lexical transfer in multilingual neural machine translation",
      "author" : [ "Luyu Gao", "Xinyi Wang", "Graham Neubig." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3560–3566, Online. Association for",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Embedding time expressions for deep temporal ordering models",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4400– 4406, Florence, Italy. Association for Computational",
      "citeRegEx" : "Goyal and Durrett.,? 2019",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2019
    }, {
      "title" : "Efficient neural machine translation for lowresource languages via exploiting related languages",
      "author" : [ "Vikrant Goyal", "Sourav Kumar", "Dipti Misra Sharma." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student",
      "citeRegEx" : "Goyal et al\\.,? 2020",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2020
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "CoRR, abs/2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "inlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian",
      "author" : [ "Divyanshu Kakwani", "Anoop Kunchukuttan", "Satish Golla", "NC Gokul", "Avik Bhattacharyya", "Mitesh M Khapra", "Pratyush Kumar" ],
      "venue" : null,
      "citeRegEx" : "Kakwani et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kakwani et al\\.",
      "year" : 2020
    }, {
      "title" : "Muril: Multilingual representations for indian languages",
      "author" : [ "Talukdar." ],
      "venue" : "arXiv preprint arXiv:2103.10730.",
      "citeRegEx" : "Talukdar.,? 2021",
      "shortCiteRegEx" : "Talukdar.",
      "year" : 2021
    }, {
      "title" : "Utilizing language relatedness to improve machine translation: A case study on languages of the indian subcontinent",
      "author" : [ "Anoop Kunchukuttan", "Pushpak Bhattacharyya" ],
      "venue" : null,
      "citeRegEx" : "Kunchukuttan and Bhattacharyya.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kunchukuttan and Bhattacharyya.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut" ],
      "venue" : null,
      "citeRegEx" : "Lan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
      "author" : [ "Patrick Littell", "David R. Mortensen", "Ke Lin", "Katherine Kairis", "Carlisle Turner", "Lori Levin." ],
      "venue" : "Proceedings of the 15th Conference of the Euro-",
      "citeRegEx" : "Littell et al\\.,? 2017",
      "shortCiteRegEx" : "Littell et al\\.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Rapid adaptation of neural machine translation to new languages",
      "author" : [ "Graham Neubig", "Junjie Hu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875–880, Brussels, Belgium. Association for Com-",
      "citeRegEx" : "Neubig and Hu.,? 2018",
      "shortCiteRegEx" : "Neubig and Hu.",
      "year" : 2018
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of ACL 2017, pages 1946–1958.",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "AdapterHub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Em-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Unks everywhere: Adapting multilingual language models to new scripts",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulic", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "CoRR, abs/2012.15562.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020c",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "BIS annotation standards with reference to Konkani language",
      "author" : [ "Madhavi Sardesai", "Jyoti Pawar", "Shantaram Walawalikar", "Edna Vaz." ],
      "venue" : "Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing, pages 145–",
      "citeRegEx" : "Sardesai et al\\.,? 2012",
      "shortCiteRegEx" : "Sardesai et al\\.",
      "year" : 2012
    }, {
      "title" : "Pretraining via leveraging assisting languages for neural machine translation",
      "author" : [ "Haiyue Song", "Raj Dabre", "Zhuoyuan Mao", "Fei Cheng", "Sadao Kurohashi", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "UDapter: Language adaptation for truly Universal Dependency parsing",
      "author" : [ "Ahmet Üstün", "Arianna Bisazza", "Gosse Bouma", "Gertjan van Noord." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Üstün et al\\.,? 2020",
      "shortCiteRegEx" : "Üstün et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilingual neural machine translation with soft decoupled encoding",
      "author" : [ "Xinyi Wang", "Hieu Pham", "Philip Arthur", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:1902.03499.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Extending multilingual BERT to low-resource languages",
      "author" : [ "Zihan Wang", "Karthikeyan K", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2649–2656, Online. Association for Computa-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Do explicit alignments robustly improve multilingual encoders? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4471–4482, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Wu and Dredze.,? 2020",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "Generalized data augmentation for low-resource translation",
      "author" : [ "Mengzhou Xia", "Xiang Kong", "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5786–",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual transfer of word embedding spaces",
      "author" : [ "Ruochen Xu", "Yiming Yang", "Naoki Otani", "Yuexin Wu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2465–2474, Brussels, Bel-",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial training for unsupervised bilingual lexicon induction",
      "author" : [ "Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "BERT-based pre-trained language models (LMs) have enabled significant advances in NLP (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "BERT-based pre-trained language models (LMs) have enabled significant advances in NLP (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "BERT-based pre-trained language models (LMs) have enabled significant advances in NLP (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "For example, the Multilingual BERT (mBERT) (Devlin et al., 2019) model was trained on 104 different languages.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "demonstrated significant success in generalizing across languages (Hu et al., 2020; Conneau et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "demonstrated significant success in generalizing across languages (Hu et al., 2020; Conneau et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "Recent methods of adapting a pre-trained multilingual LM to a LRL include fine-tuning the full model with an extended vocabulary (Wang et al., 2020), training a light-weight adapter layer while keeping the full model fixed (Pfeiffer et al.",
      "startOffset" : 129,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : ", 2020), training a light-weight adapter layer while keeping the full model fixed (Pfeiffer et al., 2020b), and exploiting overlapping tokens to learn embeddings of the LRL (Pfeiffer et al.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : ", 2020b), and exploiting overlapping tokens to learn embeddings of the LRL (Pfeiffer et al., 2020c).",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "The languages we consider in this paper are related along several dimensions of linguistic typology (Dryer and Haspelmath, 2013; Littell et al., 2017): phonologically, phylogenetically as they are all part of the Indo-Aryan family, geographically, and syntactically matching on key features like the Subject-Object-Verb (SOV) order as against the Subject-Verb-Object (SVO) order in English.",
      "startOffset" : 100,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "1314 dings across the two languages using the recently proposed loss functions for aligning contextual embeddings of word translations (Cao et al., 2020; Wu and Dredze, 2020).",
      "startOffset" : 135,
      "endOffset" : 174
    }, {
      "referenceID" : 30,
      "context" : "1314 dings across the two languages using the recently proposed loss functions for aligning contextual embeddings of word translations (Cao et al., 2020; Wu and Dredze, 2020).",
      "startOffset" : 135,
      "endOffset" : 174
    }, {
      "referenceID" : 27,
      "context" : "Transformer (Vaswani et al., 2017) based language models like mBERT (Devlin et al.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : ", 2017) based language models like mBERT (Devlin et al., 2019), MuRIL (Khanuja et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : ", 2021), IndicBERT (Kakwani et al., 2020), and XLM-R (Conneau et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : ", 2020), and XLM-R (Conneau et al., 2019), trained on massive multilingual datasets have been shown to scale across a variety of tasks and languages.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly pre-trained.",
      "startOffset" : 22,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly pre-trained.",
      "startOffset" : 22,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly pre-trained.",
      "startOffset" : 22,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "Adapter (Pfeiffer et al., 2020a) based frameworks like (Pfeiffer et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : ", 2020a) based frameworks like (Pfeiffer et al., 2020b; Artetxe et al., 2020; Üstün et al., 2020) address the lack of model’s capacity to accommodate multiple languages and establish the advantages of",
      "startOffset" : 31,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : ", 2020a) based frameworks like (Pfeiffer et al., 2020b; Artetxe et al., 2020; Üstün et al., 2020) address the lack of model’s capacity to accommodate multiple languages and establish the advantages of",
      "startOffset" : 31,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : ", 2020a) based frameworks like (Pfeiffer et al., 2020b; Artetxe et al., 2020; Üstün et al., 2020) address the lack of model’s capacity to accommodate multiple languages and establish the advantages of",
      "startOffset" : 31,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 149
    }, {
      "referenceID" : 25,
      "context" : "Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "better adaptM to L than the existing baseline of fine-tuningM using the standard masked language model (MLM) loss on the limited monolingual data DL (Wang et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "Example, the Indic-Trans Library 2 (Bhat et al., 2015).",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "Parallel data between a RPL and LRL language pair has been shown to be greatly useful for efficient adaptation to LRL (Conneau and Lample, 2019; Cao et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "Parallel data between a RPL and LRL language pair has been shown to be greatly useful for efficient adaptation to LRL (Conneau and Lample, 2019; Cao et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 162
    }, {
      "referenceID" : 33,
      "context" : "Also, word-level lexicons can be created semiautomatically (Zhang et al., 2017) (Artetxe et al.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "For all the experiments, we use libindic’s indictrans library (Bhat et al., 2015) for transliteration.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "We obtained the NER data from WikiANN (Pan et al., 2017) and XTREME (Hu et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : ", 2017) and XTREME (Hu et al., 2020) and the POS and Text Classification data from the Technology Development for Indian Languages (TDIL)6.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "The POS tagset used was the BIS Tagset (Sardesai et al., 2012).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "Methods compared We contrast RelateLM with three other adaptation techniques: (1) EBERT (Wang et al., 2020) that extends the vocabulary and tunes with MLM on DL as-is, (2) RelateLM without pseudo translation loss, and (3) m-BERT when the language exists in m-BERT.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "varies to some extent from other Indo-Aryan languages, and Bengali shows influence from TibetoBurman languages too (Kunchukuttan and Bhattacharyya, 2020).",
      "startOffset" : 115,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "Alignment Loss We compare the MSE-based loss we used with the recently proposed contrastive loss (Wu and Dredze, 2020) for Lalign but did not get any significant improvements.",
      "startOffset" : 97,
      "endOffset" : 118
    } ],
    "year" : 2021,
    "abstractText" : "Recent research in multilingual language models (LM) has demonstrated their ability to effectively handle multiple languages in a single model. This holds promise for low web-resource languages (LRL) as multilingual models can enable transfer of supervision from high resource languages to LRLs. However, incorporating a new language in an LM still remains a challenge, particularly for languages with limited corpora and in unseen scripts. In this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of LRLs, and propose RelateLM. We focus on Indian languages, and exploit relatedness along two dimensions: (1) script (since many Indic scripts originated from the Brahmic script), and (2) sentence structure. RelateLM uses transliteration to convert the unseen script of limited LRL text into the script of a Related Prominent Language (RPL) (Hindi in our case). While exploiting similar sentence structures, RelateLM utilizes readily available bilingual dictionaries to pseudo translate RPL text into LRL corpora. Experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot, along with transliteration and pseudo translation based data augmentation, can be an effective way to adapt LMs for LRLs, rather than direct training or pivoting through English.",
    "creator" : "LaTeX with hyperref"
  }
}