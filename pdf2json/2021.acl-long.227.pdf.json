{
  "name" : "2021.acl-long.227.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ERNIE-DOC: A Retrospective Long-Document Modeling Transformer",
    "authors" : [ "Siyu Ding", "Junyuan Shang", "Shuohuan Wang", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang" ],
    "emails" : [ "wanghaifeng}@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2914–2927\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2914"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformers (Vaswani et al., 2017) have achieved remarkable improvements in a wide range of natural language tasks, including language modeling (Dai et al., 2019), text classification (Yang et al., 2019), and question answering (Devlin et al., 2018; Radford et al., 2019). This success is largely due to the self-attention mechanism, which enables the network to capture contextual information from the\n*indicates equal contribution. 1Source code and pre-trained checkpoints can be found at https://github.com/PaddlePaddle/ERNIE/ tree/repro/ernie-doc.\nentire input sequence. Nevertheless, the memory usage and computation complexity caused by the self-attention mechanism grows quadratically with the sequence length, incurring excessive cost when processing a long document on existing hardware.\nCurrently, the most prominent pretrained models, such as BERT (Devlin et al., 2018), are used on fixed-length input segments of a maximum of 512 tokens owing to the aforementioned limitation. Thus, a long document input must be partitioned into smaller segments of manageable sizes. However, this leads to the loss of important crosssegment information, that is, the context fragmentation problem (Dai et al., 2019), as shown in Fig. 1(a). To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers (Dai et al., 2019; Rae et al., 2019) permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation;\nthis enables the modeling of long documents. In addition, Sparse Attention Transformers (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.\nWe argue that existing strategies are not sufficiently effective or reliable, because the contextual information of a complete document is still not available for each segment during the training phase. As depicted in Fig. 1, when training on segment S2, the model is ideally optimized by maximizing P (y | (S1, S2, S3)) conditioned on the contextual information of the entire document D = {S1, S2, S3}, in contrast to the following suboptimal solutions: P (y | S2) for Vanilla/Sparse Transformers2 and P (y | (S1, S2)) for Recurrence Transformers.\nTo address this limitation, we propose ERNIEDOC (A Retrospective Long-Document Modeling Transformer) based on the Recurrence Transformer paradigm. Inspired by the human reading behavior of skimming a document first and then looking back upon it attentively, we design a retrospective feed mechanism in which segments from a document are fed twice as input. As a result, each segment in the retrospective phase could explicitly fuse the semantic information of the entire document learned in the skimming phase, which prevents context fragmentation.\nHowever, simply incorporating the retrospective feed mechanism into Recurrence Transformers is infeasible because the maximum effective context length is limited by the number of layers (Dai et al., 2019), as shown in Fig. 1 (b). Thus, we present an enhanced recurrence mechanism, a drop-in replacement for a Recurrence Transformer, by changing the shifting-one-layer-downwards recurrence to the same-layer recurrence. In this manner, the maximum effective context length can be expanded, and past higher-level representations can be exploited to enrich future lower-level representations.\nMoreover, we introduce a segment-reordering objective to pretrain a document-level model. Specifically, it is a document-aware task of predicting the correct order of the permuted set of segments of a document, to model the relationship among segments directly. This allows ERNIE-\n2For Sparse Transformers, the length of segment S2 could be up to 4,096 in Beltagy et al. (2020); Zaheer et al. (2020).\nDOC to build full document representations for prediction. This is analogous to the sentencereordering task in ERNIE 2.0 (Sun et al., 2020b) but at a segment level of granularity, spanning (commonly) multiple training steps.\nWe first evaluate ERNIE-DOC on autoregressive word-level language modeling using the enhanced recurrence mechanism, which, in theory, allows the model to process a document with infinite words. ERNIE-DOC achieves state-of-theart (SOTA) results on the WiKiText-103 benchmark dataset, demonstrating its effectiveness in long-document modeling. Then, to evaluate the potential of ERNIE-DOC on document-level natural language understanding (NLU) tasks, we pretrained the English ERNIE-DOC on the text corpora utilized in BigBird (Zaheer et al., 2020) from the RoBERTa-released checkpoint, and the Chinese ERNIE-DOC on the text corpora utilized in ERNIE 2.0 (Sun et al., 2020b) from scratch. After pretraining, we fine-tuned ERNIE-DOC on a wide range of English and Chinese downstream tasks, including text classification, question answering and keypharse extraction. Empirically, ERNIE-DOC consistently outperformed RoBERTa on various benchmarks and showed significant improvements over other high-performance long-text pretraining models for most tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sparse Attention Transformers have been extensively explored (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer (Child et al., 2019) uses a dilated sliding window that reduces the complexity to O(L √ L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L logL) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-\nanisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires Ω(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.\nRecurrence Transformers (Dai et al., 2019; Rae et al., 2019) have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = ∏L t=1 p(xt|x<t), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism (Dai et al., 2019). This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a compressive memory bank to sufficiently store old activations instead of discarding them, which facilitates long-range sequence learning. However, these methods operate from left to right, which limits their capacity for discriminative language understanding tasks that require bidirectional information. XLNet (Yang et al., 2019) proposed a permutation language modeling objective to construct bidirectional information and achieve superior performance in multiple NLP tasks; however, its application to long-document modeling tasks remains largely unexplored. ERNIE-DOC builds on the ideas of the Recurrence Transformers to 1) tackle the limitation of Recurrence Transformers for utilizing bidirectional contextual information and 2) improve the behavior of the segment recurrence mechanism to capture longer dependencies.\nHierarchical Transformers (Zhang et al., 2019; Lin et al., 2020) have enabled significant progress on numerous document-level tasks, such as document summarization (Zhang et al., 2019) and document ranking (Lin et al., 2020). Similar to Vanilla Transformers, Hierarchical Transformers also split long documents into shorter segments with manageable lengths and then feed them independently to produce corresponding segment-level semantic representations. Unlike in Vanilla Transformers,\nhowever, separate Transformer layers are used in Hierarchical Transformers to process the concatenation of these representations. Hierarchical Transformers ignore the contextual information from the remaining segments when processing each segment of a long document, thus suffering from the context fragmentation problem."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "In this section, we first describe the background (Sec. 3.1) that ERNIE-DOC builds on. Then, we present the implementation of ERNIE-DOC, including the retrospective feed mechanism in Sec. 3.2, the enhanced recurrence mechanism in Sec. 3.3, and the segment-reordering objective in Sec. 3.4."
    }, {
      "heading" : "3.1 Background",
      "text" : "Formally, a long document D is sliced into T sequential segments, denoted as {S1, S2, ..., ST }, where Sτ = {xτ,1, xτ,2, ..., xτ,L} is the τ -th segment with L tokens; x denotes a single token. Vanilla, Sparse, and Recurrence Transformers employ different strategies to produce the hidden state hnτ ∈ RL×d for segment Sτ at the n-th layer:\nh̃n−1τ+1 =\n{ hn−1τ+1 , Vanilla or Sparse Transformers\n[SG(hn−1τ ) ◦ hn−1τ+1 ], Recurrence Transformers,\nqnτ+1,k n τ+1,v n τ+1 = h n−1 τ+1W > q , h̃ n−1 τ+1W > k , h̃ n−1 τ+1W > v . hnτ+1 = Transformer-Block (q n τ+1,k n τ+1,v n τ+1).\n(1)\nwhere q ∈ RL×d, k, and v ∈ R(L+m)×d are the query, key and value vectors, respectively with hidden dimension d and memory length m (Note that m = 0 for Vanilla or Sparse Transformers); h̃n−1τ+1 ∈ R(L+m)×d is the extended context; W∗ ∈ Rd∗×d represents learnable linear projection parameters; the function SG(·) denotes the stop-gradient operation; and the notation [◦] denotes the concatenation of two hidden states along the length dimension. In contrast to Vanilla or Sparse Transformers, where hnτ+1 is produced using only itself, Recurrence Transformers introduce a segment-level recurrence mechanism to promote interaction across segments. The hidden state computed for the previous segment hn−1τ is cached as an auxiliary context to help process the current segment hnτ . However, from the concatenation part in Eq. 1, i.e., [SG(hn−1τ ) ◦ hn−1τ+1 ], there is apparently a constraint that the current hidden state can only fuse information from the previous segments. In\nother words, the contextual information of an entire document is not available for each segment."
    }, {
      "heading" : "3.2 Retrospective Feed Mechanism",
      "text" : "ERNIE-DOC employs a retrospective feed mechanism to address the unavailability of the contextual information of a complete document for each segment. The segments from a long document are twice fed as input. Mimicking the human reading behavior, we refer to the first and second inputtaking phases as the skimming and retrospective phases, respectively. In the skimming phase, we employ a recurrence mechanism to cache the hidden states for each segment. In the retrospective phase, we reuse the cached hidden states from the skimming phase to enable bi-directional information flow. Naively, we can rewrite Eq. 1 to obtain the contextual information of an entire document in the skimming phase to be utilized in the retrospective phase as follows,\nĤ = [Ĥ11:T ◦ Ĥ21:T · · · ◦ ĤN1:T ], (skim. phase)\nh̃n−1τ+1 = [SG(Ĥ ◦ h n−1 τ ) ◦ hn−1τ+1 ], (retro. phase)\n(2) where Ĥ ∈ R(L∗T∗N)×d denotes the cached hidden states in the skimming phase with T segments, L length of each segment and total N layers, and Ĥi1:T = [ĥ i 1 ◦ ĥi2 · · · ◦ ĥiT ] is the concatenation of i-th layer’s hidden states of the skimming phase. Thus, the extended context h̃n−1τ+1 is guaranteed to capture the bidirectional contextual information of the entire document. However, it will incur massive\nmemory and computation cost for directly employing Ĥ in self-attention mechanism. Henceforth, the main issue is how Ĥ should be implemented in a memory- and computation-efficient manner.\nBy rethinking segment-level recurrence (Dai et al., 2019), we observe that the largest possible context dependency length increases linearly w.r.t the number of layers (N ). For instance, at i-th layer, ĥiτ have the longest dependency to ĥ 1 τ−(i−1). Thus, to minimize memory and computation consumption, hidden states from the N -th layer (toplayer) are included at a stride of N , which is sufficient to build the contextual information of an entire document. Formally, Ĥ can be reduced to Ĥr = [ĥ N N ◦ ĥN2∗N · · ·◦ ĥNbT/Nc∗N ] (Note that when T is not evenly divisible by N , the last hidden state ĥNT need to be included). However, for a long document input, the extra computational and memory cost of Ĥr ∈ RdT/Ne×d where T N is still excessive on existing hardware."
    }, {
      "heading" : "3.3 Enhanced Recurrence Mechanism",
      "text" : "To effectively utilize the retrospective feed mechanism in practice, an ideal strategy is to ensure that the cached hidden state hn−1τ already contains the contextual information of an entire document without explicitly taking Ĥ or Ĥr as input. Essentially, we should tackle the problem of limited effective context length in the segment-level recurrence mechanisms. Herein, we introduce the enhanced recurrence mechanism, a drop-in replacement for the segment-level recurrence mechanism,\nby changing the shifting-one-layer-downwards recurrence to the same-layer recurrence as follows:\nh̃n−1τ+1 = [ SG(hnτ ) ◦ h n−1 τ+1 ] (3)\nwhere the cached hidden state hn−1τ in Eq. 1 and Eq. 2 is replaced with hnτ in Eq. 3.\nAs shown in Fig. 2, when the retrospective feed mechanism is combined with the enhanced recurrence mechanism, every segment in the retrospective phase (shown in the box with a green dotted border) has bidirectional contextual information of the entire text input. We successfully modeled a larger effective context length (shown in the box with a orange dotted border) than traditional Recurrence Transformers can without extra memory and computation costs. Another benefit of the enhanced recurrence scheme is that past higher-level representations can be exploited to enrich future lower-level representations."
    }, {
      "heading" : "3.4 Segment-Reordering Objective",
      "text" : "In addition to the masked language model (MLM) objective (Devlin et al., 2018), we introduce an additional document-aware task called segment-reordering objective for pretraining. Benefitting from the much larger effective context length provided by the enhanced recurrence mechanism, the goal of the segment-reordering objective is to predict the correct order for the permuted set of segments of a long document, to explicitly learn the relationships among segments. During the pretraining process of this task, a long text input D is first randomly partitioned into 1 to m chunks; then, all the combinations are shuffled in a random order. As shown in Fig. 3, D is partitioned into three chunks and then permuted, that is, D = {C1, C2, C3} =⇒ D̂ = {C2, C3, C1}, where Ci denotes the i-th chunk. Subsequently, the permuted long context D̂ is split into T sequential segments as a common practice, denoted as D̂ = {S1, S2, ..., ST }. We let the pretrained model reorganize these permuted segments, modeled as a K-class classification problem, where K = ∑m i=1 i!.\nThe pretraining objective is summarized as follows for the τ -th input segment:\nmax θ\nlog pθ(Sτ |Ŝτ ) + 1τ=T log pθ(D|D̂)\nwhere Ŝτ is the corrupted version of Sτ , which is obtained by randomly setting a portion of tokens\nto [MASK]; D̂ is the permutated version of D; θ is the model parameter; and 1τ=T indicates that the segment-reordering objective is optimized only at the T -th step."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Autoregressive Language Modeling",
      "text" : "Autoregressive language modeling aims to estimate the probability distribution of an existing token/character based on previous tokens/characters in an input sequence. For comparison with previous work, we conducted experiments on wordlevel LM, that is, WikiText-103 (Merity et al., 2016), which is a document-level language modeling dataset."
    }, {
      "heading" : "4.1.1 Experimental Setup",
      "text" : "For autoregressive language modeling, we use a memory-enhanced Transformer-XL (Dai et al., 2019), that is, we employ our enhanced recurrence mechanism to replace the primitive one used in the Transformer-XL. Additionally, as proposed by Segatron (Bai et al., 2020), we introduce the segment-aware mechanism into Transformer-XL. Based on Transformer-XL, we trained a base-size model (L=16, H=410, A=10) and a large-size model (L=18, H=1,024, A=16)3. The models were trained for 200K/400K steps using a batch size of 64/128 for the base/large configurations. During the training phase, the sequence length and memory length were limited to 150 and 384 for the base and the large model, respectively. The remaining hyper-parameters were identical to those of Transformer-XL.\n3We denote the number of Transformer layers as L, the hidden size as H, and the number of self-attention heads as A."
    }, {
      "heading" : "4.1.2 Results",
      "text" : "Tab. 1 summarizes the evaluation results for WikiText-103. ERNIE-DOC achieves an impressive improvement compared with Transformer-XL: the perplexity (PPL) decreases by 3.0 for the base model and by 1.5 for the large model. Finally, we improve the state-of-the-art result of PPL to 21.0 (the base model) and 16.8 (the large model)."
    }, {
      "heading" : "4.2 Pretraining and Finetuning",
      "text" : ""
    }, {
      "heading" : "4.2.1 Pretraining Text Corpora",
      "text" : "English Data. To allow ERNIE-DOC to capture long dependencies in pretraining, we compiled a corpus from four standard datasets: WIKIPEDIA, BOOKSCORPUS (Zhu et al., 2015), CC-NEWS4, and STORIES (Trinh and Le, 2018) (details listed in Tab. 2). We tokenized the corpus using the RoBERTa wordpieces tokenizer (Liu et al., 2019) and duplicated the pretraining data 10 times. Chinese Data. The Chinese text corpora used in ERNIE 2.0 (Sun et al., 2020b) were adopted for pretraining ERNIE-DOC."
    }, {
      "heading" : "4.2.2 Experimental Setup",
      "text" : "Pretraining. We trained three sizes of models for English tasks: small (L=6, H=256, A=4), base (L=12, H=768, A=12), and large (L=24, H=1,024,\n4We used news-please to crawl English news articles published between September 2016 and February 2019 and adopted Message Digest Algorithm5 (MD5) for deduplication.\nA=16). For Chinese tasks, we used only one size, i.e., base (L=12, H=768, A=12). We limited the length of the sentences in each mini-batch to 512 tokens and the length of the memory to 128. The models were trained for 500K/400K/100K steps using a batch size of 2,560/2,560/3,920 sentences for the small/base/large configurations. ERNIE-DOC was optimized with the Adam (Kingma and Ba, 2014) optimizer. The learning rate was warmed up over the first 4,000 steps to a peak value of 1e-4, and then it linearly decayed. The remaining pretraining hyperparameters were the same as those of RoBERTa (Liu et al., 2019) (see Tab. 12). Additionally, we employed relative positional embedding (Shaw et al., 2018) in our model pretraining because it is necessary for reusing hidden state without causing temporal confusion (Dai et al., 2019).\nFinetune. In contrast to previous models, such as BERT, RoBERTa, and XLNet, the proposed model employs the retrospective feed mechanism and the enhanced recurrence mechanism during the finetuning phase to fully utilize the advantages of these two strategies."
    }, {
      "heading" : "4.2.3 Results on English Tasks",
      "text" : "Results on Long-Text Classification Tasks. We consider two datasets: IMDB reviews (Maas et al., 2011) and Hyperpartisan News Detection (HYP) (Kiesel et al., 2019). The former is a widely used sentiment analysis dataset containing 50,000 movie reviews, labeled as positive or negative. The latter contains news that takes extreme left-wing or right-wing standpoints. The documents in HYP are extremely long (50% of the samples contain more than 537 tokens) and are thus suitable for testing long-text classification ability. Tab. 3 summarizes the results of the ERNIE-DOC-Base and ERNIE-DOC-Large models for long-text classification tasks, and ERNIE-DOC achieves a SOTA result. On IMDB, we observed a modest perfor-\nmance gain compared with RoBERTa. This is because nearly 90% of the samples in the dataset consist of fewer than 569 tokens. Unlike on IMDB, ERNIE-DOC surpasses the baseline models on HYP by a substantial margin, demonstrating its capability of utilizing information from a long document input. Note that we include XLNet-Large, the previous SOTA pretraining model on the IMDB dataset, as the baseline for a large model setting; ERNIE-DOC achieves a result comparable to that of XLNet-Large. Results on Document-level QuestionAnswering Tasks. We utilized two documentlevel QA datasets (Wikipedia setting of TriviaQA (TQA) (Joshi et al., 2017) and distractor setting of HotpotQA (HQA) (Yang et al., 2018)) to evaluate the reasoning ability of the models over long documents. TQA and HQA are extractive QA tasks, and we follow the simple QA model of BERT (Devlin et al., 2018) to predict an answer with the maximum sum of start and end logits across multiple segments of a sample. In addition, we use a modified cross-entropy loss (Clark and Gardner, 2017) for the TQA dataset and use a two-stage model (Groeneveld et al., 2020) with the backbone of ERNIE-DOC for the HQA dataset. Tab. 4. shows that ERNIE-DOC outperforms RoBERTa and Longformer by a considerable margin on these two datasets, and is comparable to current SOTA long-document model, i.e., BigBird on HQA in large-size model setting. Results on the Keyphrase Extraction Task. We include OpenKP (Xiong et al., 2019) dataset to eval-\nuate ERNIE-DOC’s ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-DOC performs stably better on all metrics on the OpenKP dataset."
    }, {
      "heading" : "4.2.4 Results on Chinese Tasks",
      "text" : "We conducted extensive experiments on seven Chinese natural language understanding (NLU) tasks, including machine reading comprehension (CMRC2018 (Cui et al., 2018), DRCD (Shao et al., 2018), DuReader (He et al., 2017), C3 (Sun et al., 2019a)), semantic similarity (CAIL2019SCM (CAIL) (Xiao et al., 2019)), and long-text classification (IFLYTEK (IFK) (Xu et al., 2020), THUCNews (THU)5 (Sun et al., 2016)). The documents in all the aforementioned datasets are sufficiently long to be used to evaluate the effectiveness of ERNIE-DOC on long-context tasks (see detailed datasets statistics in Tab. 9). We reported the mean results with five runs for the seven Chinese tasks in Tab. 6, and summarized the hyperparameters in Tab. 16. ERNIE-DOC outperforms previous models across these Chinese NLU tasks by a significant margin in the base-size model group."
    }, {
      "heading" : "4.2.5 Ablation Studies",
      "text" : ""
    }, {
      "heading" : "No. Models TQA HYP",
      "text" : "5We use a subset of THUCNews which can be found at https://github.com/gaussic/ text-classification-cnn-rnn.\nNo.IV and No.V, we see that segment-level recurrence is necessary for modeling long documents and produces 2.74 and 3.95 % points improvement on the TQA and HYP dateset, respectively. Moreover, a substantial improvement is achieved using the enhance recurrence mechanism (2.29% point on TQA and 1.40% point on HYP, see No.III - IV). Retrospective feed mechanism further improves 0.21% point on TQA and 1.33% point on HYP (No.II - No.III). Considering different types of tasks, we observe that on HYP, an extremely long text classification dataset, a substantial improvement is achieved using the segment-reordering objective (1.5% point). This indicates that the [CLS] token, pretrained using the segment-reordering objective, is more adaptable to the document-level text classification task.\nEffect of enhanced recurrence mechanism with regard to different maximum sequence lengths. As depicted in Fig. 4, the enhanced recurrence mechanism plays an important role in pretraining an effective language model with lower PPL and\nhigher accuracy under both the maximum sequence input lengths of 128 and 512. The effect of the enhanced recurrence mechanism is more significant under a smaller maximum sequence length, even makes the ERNIE-DOC-Small (max-len:128) comparable to ERNIE-DOC-Small w/o en recur (max-len:512) w.r.t accuracy. This intriguing property of the enhanced recurrence mechanism enables more efficient model training and inference by reducing maximum sequence length while remaining comparable modeling capability."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed ERNIE-DOC, a document-level language pretraining model based on the Recurrence Transformers paradigm. Two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ERNIE-DOC, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. Additionally, ERNIE-DOC is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. Experiments on various downstream tasks demonstrate that ERNIEDOC outperforms existing strong pretraining models such as RoBERTa, Longformer, and BigBird and achieves SOTA results on several language modeling and language understanding benchmarks. In future studies, we will evaluate ERNIE-DOC on language generation tasks, such as generative question answering and text summarization. We will also investigate its potential applicability in other areas, such as computational biology. Another possibility is to incorporate graph neural networks into ERNIE-DOC to enhance its modeling capability\nfor tasks that require multi-hop reasoning and longdocument modeling ability."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Key Research and Development Project of China (No. 2018AAA0101900)."
    }, {
      "heading" : "A Appendices",
      "text" : ""
    }, {
      "heading" : "A.1 Tasks",
      "text" : "Following previous work, we evaluate ERNIEDOC on various tasks that require the ability to model a long document.\nDocument-level Language Modeling Task. We employ WikiText-103 (Merity et al., 2016) in language modeling experiments. WikiText-103 is the largest available word-level benchmark with long-term dependency for language modeling, which consists of 28K articles, where each article has 3.6K tokens on average, thus 103M training tokens in total.\nLong Text classification. We consider two English datasets: IMDB reviews (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019) (see Tab. 8), and two Chinese datasets: IFLYTEK (Xu et al., 2020) and THUCNews (Sun et al., 2016) (see Tab. 9). IMDB is a widely used sentiment analysis dataset containing 50,000 movie reviews labeled as positive or negative. Training and dev dataset is equally split. Hyperpartisan contains news that takes an extreme left-wing or right-wing standpoint. Documents are extremely long in Hyperpartisan which makes it a good test for long text classification. We use the same split as Longformer by dividing 654 documents into train/dev/test sets. IFLYTEK contains 17,332 app descriptions. The task is to assign each description into one of 119 categories, such as food, car rental and education. THUCNews is generated by filtering historical data of Sina News RSS subscription channel from 2005 to 2011, including 740,000 news documents and 14 categories. In this paper, we employ the subset version instead of the full one 6, which contains 10 categories, each with 5,000 pieces of data.\nFor the above four long text classification datasets, we concatenate [CLS] token with each segment and takes as input multiple segments of a text sequentially. Each segment is generated by slicing the text with a sliding window of 128 tokens. We apply binary cross entropy loss on the [CLS] token of the last segment.\nLong Text Semantic Similarity. Considering that there is no available long text semantic similarity dataset in English, we evaluate the effectiveness\n6The subset version is also released and can be downloaded from the official website of THUCTC.\nof ERNIE-DOC on semantic similarity task only depending on Chinese dataset CAIL2019-SCM. According to Xiao et al. (2019), CAIL2019-SCM is a sub-task of the Chinese AI and Law Challenge (CAIL) competition in 2019, which contains 8,964 triplets of legal documents collected from China Judgments Online. Every document in a majority of triplet has more than 512 characters, therefore, the total length of a triplet is quite long. CAIL2019-SCM requires researchers to decide which two cases are more similar in a triplet. Specifically, given a triplet (A,B,C), where A, B, C are fact descriptions of three cases. The model needs to predict whether sim(A,B) > sim(A,C) or sim(A,C) > sim(A,B), in which sim denotes the similarity between two cases. Instead of separately feeding the document A, B, C into the model to get the feature h, we use the combinations of (A,B) and (A,C) as input. We generate multiple segments for (A,B) or (A,C) with a sliding window of 128 tokens and feed them as input sequentially. The binary cross entropy loss is applied to the difference of [CLS] token output of each segment.\nDocument-level Question answering. We utilize two English question answering datasets (TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018)) (see Tab. 8) and four Chinese question answering datasets (CMRC2018 (Cui et al., 2018), DRCD (Shao et al., 2018), DuReader (He et al., 2017), C3 (Sun et al., 2019a)) (see Tab. 9) to evaluate models’ reasoning ability over long documents.\nTriviaQA is a large scale QA dataset that contains over 650K question-answer pairs. We evaluate models on its Wikipedia setting where documents are Wikipedia articles, and answers are named entities mentioned in multiple documents. The dataset is distantly supervised meaning that there is no golden span, thus we find all superficial identical answers in provided documents7. We use the following input format for each segment: “[CLS] context [q] question [/q]” where context is generated by slicing multidocuments input with a sliding window of 128 tokens. We take as input multiple segments of a sample sequentially and attach a linear layer to each token in a segment to predict the answer span. We\n7We use the same preprocessing code for TriviaQA dataset as BigBird, see https://github.com/ tensorflow/models/blob/master/official/ nlp/projects/triviaqa/preprocess.py\nuse a modified cross entropy loss (Clark and Gardner, 2017) assuming that each segment contains at least one correct answer span. The final prediction for each question is a span with the maximum sum of start and end logit across multiple segments.\nHotpotQA is a QA dataset where golden spans of an answer and sentence-level supporting facts are provided. Thus, it contains two tasks namely, answer span prediction and supporting facts prediction. In the distractor setting, each question is associated with 10 documents where only 2 documents contain supporting facts. It requires the model to find and reason over multiple documents to find answers, and explain the predicted answers using predicted supporting facts. Following Groeneveld et al. (2020), we implemented a two-stage model based on ERNIE-DOC and use the following input format for each segment: “[CLS] title1 [p] sent1,1 [SEP] sent1,2 [SEP] ... title2 [p] sent2,1 [SEP] sent2,2 [SEP] ... [q] question [/q]” For evidence prediction, we apply 2 layer feedforward networks over the special token [SEP] and [p] representing a sentence and a paragraph separately. Then we use binary cross entropy loss to do binary classification. For answer span prediction, we train the model with a multi-task objective: 1) question type (yes/no/span) classification on the [CLS] token. 2) supporting evidence prediction on [SEP] and [p]. 3) span prediction on the start and end token of a golden span.\nCMRC2018, DRCD and DuReader are common Chinese QA datasets with same format, which have been evaluated in numerous popular pretrain-\ning models, such as BERT (Devlin et al., 2018), ERNIE 1.0 (Sun et al., 2019b), ERNIE 2.0 (Sun et al., 2020b) and etc. The detailed descriptions of three datasets can refer to Cui et al. (2018), Shao et al. (2018) and He et al. (2017). We adopt the same input format as TriviaQA for each segment, denotes as “[CLS] context [SEP] question [SEP]“ where context is generated by slicing multi-documents input with a sliding window of 128 tokens. We take as input multiple segments of a sample sequentially and attach a linear layer to each token in a segment to predict the answer span. Then, we apply a softmax and use the cross entropy loss with the correct answer. The final prediction for each question is a span with the maximum sum of start and end logit across multiple segments.\nThe multiple Choice Chinese machine reading Comprehension dataset (C3) (Sun et al., 2019a) is the first Chinese free-form multi-choice dataset where each question is associated with at most four choices and a single document. According to (Sun et al., 2019a), m segments are constructed for a question, in which m denotes the number of choice for that question. We use the following input format for each segment: “[CLS] context [SEP] question [SEP] choicei [SEP] ” where context is generated by slicing document input with a sliding window of 128 tokens stride. We take as input multiple segments of a sample in a single batch and attach a linear layer to [CLS] that outputs an unnormalized logit. Then we obtain the final prediction for a question by applying a softmax layer over the unnormalized logits of all choices\nassociated with it.\nKeyphrase Extraction. We include OpenKP (Xiong et al., 2019) dataset 8 to evaluate ERNIE-DOC’s ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We clean the dataset by removing some nonsense words such as the HTTP links. In detail, we apply five CNNs on BERT’s output with the kernel size ranging from 1 to 5. Since each word is composed of several sub-tokens, we take the first token’s embedding as the input for CNNs. Finally, we use the binary cross entropy loss as the optimization objective."
    }, {
      "heading" : "A.2 Ablation Studies",
      "text" : "Tab. 10 shows the performance of ERNIE-DOCSmall on English tasks after ablating each proposed component. All models were pretrained and finetuned with the same experimental setup, and we report the mean results of five runs. In the last column in Tab. 10, we see that the segment-reordering objective is improved ERNIE-DOC by 0.81% on average (#1 - #0), the retrospective feed mechanism is improved ERNIE-DOC by an average of 0.58% (#2 - #1), and the enhanced recurrence mechanism makes a large contribution of 2.55 percentage points on average (#3 - #2). By comparing #3 with #4, we see that segment-level recurrence is necessary for modeling long documents and produces a 4.92 percentage point improvement on average. Considering different types of tasks, we observe that on Hyperpartisan, an extremely long text classification dataset, a substantial improvement is achieved using the segment-reordering ob-\n8The dataset can be downloaded from https:// github.com/thunlp/BERT-KPE\njective (1.5% point). This indicates that the [CLS] token, pretrained using the segment-reordering objective, is more adaptable to the document-level text classification task. Moreover, we observed a stable performance gain across all tasks using the enhanced recurrence mechanism."
    }, {
      "heading" : "A.3 Hyperparameters for Language Modeling",
      "text" : "In Tab. 11, we present the detailed hyperparameters used for our experiments, which are the same as the hyperparameters employed in Transformer-XL (Dai et al., 2019)."
    }, {
      "heading" : "A.4 Hyperparameters for Pre-Training",
      "text" : "As shown in Tab. 12, we present the detailed hyperparameters adopted to pretraining ERNIEDOC on English text corpora and Chinese text corpora. For comparisons, we follow the same optimization hyperparameters of RoBERTaBASE or RoBERTaLARGE (Liu et al., 2019) for base-size or large-size model in English domain. As for Chinese ERNIE-DOC, we follow the same optimization hyperparameters of ERNIE 2.0BASE."
    }, {
      "heading" : "A.5 Hyperparameters for Fine-Tuning",
      "text" : ""
    }, {
      "heading" : "A.5.1 Long Text Classification tasks",
      "text" : "The finetuning hyperparameters for IMDB (Maas et al., 2011) and Hyperpartisan (Kiesel et al., 2019)\nare presented in Tab. 13.\nA.5.2 Document-level Question answering tasks\nThe finetuning hyperparameters for TriviaQA (Welbl et al., 2018) and HotpotQA (Yang et al., 2018) are presented in Tab. 14. HQA-sent. is the model for coarse-grained evidence prediction, and we choose the evidence with the probability larger than a pre-defined threshold 1e-3 and 1e-5 for base and large models, respectively. HQA-span. is the model for span prediction."
    }, {
      "heading" : "A.5.3 Keyphrase Extraction task",
      "text" : "The finetuning hyperparameters for the OpenKP (Xiong et al., 2019) dataset are presented in Tab. 15."
    }, {
      "heading" : "Hyperparameters OpenKP",
      "text" : ""
    }, {
      "heading" : "A.5.4 Chinese NLU tasks",
      "text" : "Tab. 16 lists the finetuning hyperparameters for Chinese NLU tasks including IFLYTEK (Xu et al., 2020), THUCNews (Sun et al., 2016), CMRC2018 (Cui et al., 2018), DRCD (Shao et al., 2018), DuReader He et al. (2017), C3 (Sun et al., 2019a) and CAIL2019-SCM (Xiao et al., 2019)."
    }, {
      "heading" : "B Attention Complexity",
      "text" : "Given a long document with length L, Longformer and BigBird usually applies a local attention with a window size of 512 tokens on the entire input resulting in L ∗ 512 token-to-token calculations. While the long document is fed twice as input and each input is sliced with a sliding window size of 512 tokens in ERNIE-DOC, which resulting in 2 ∗ L512 ∗ 512 ∗ (512 + m) token-to-token calculations where m is the memory length. Since 512 L and m L, the attention complexity of ERNIE-DOC is comparable to Longformer and BigBird which scales linearly with respect to the input length L, i.e., O(L). Notably, the segments produced from the long document are fed one by one in ERNIE-DOC, leading to the lower spatial complexity."
    } ],
    "references" : [ {
      "title" : "Etc: Encoding long and structured inputs in transformers",
      "author" : [ "Joshua Ainslie", "Santiago Ontanon", "Chris Alberti", "Vaclav Cvicek", "Zachary Fisher", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai", "Qifan Wang", "Li Yang." ],
      "venue" : "Proceedings of the 2020 Con-",
      "citeRegEx" : "Ainslie et al\\.,? 2020",
      "shortCiteRegEx" : "Ainslie et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptive input representations for neural language modeling",
      "author" : [ "Alexei Baevski", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1809.10853.",
      "citeRegEx" : "Baevski and Auli.,? 2018",
      "shortCiteRegEx" : "Baevski and Auli.",
      "year" : 2018
    }, {
      "title" : "Segatron: Segment-aware transformer for language modeling and understanding",
      "author" : [ "He Bai", "Peng Shi", "Jimmy Lin", "Yuqing Xie", "Luchen Tan", "Kun Xiong", "Wen Gao", "Ming Li" ],
      "venue" : null,
      "citeRegEx" : "Bai et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating long sequences with sparse transformers",
      "author" : [ "Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever." ],
      "venue" : "CoRR, abs/1904.10509.",
      "citeRegEx" : "Child et al\\.,? 2019",
      "shortCiteRegEx" : "Child et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "arXiv preprint arXiv:1710.10723.",
      "citeRegEx" : "Clark and Gardner.,? 2017",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2017
    }, {
      "title" : "Revisiting pretrained models for chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:2004.13922.",
      "citeRegEx" : "Cui et al\\.,? 2020a",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Cui et al\\.,? 2020b",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training with whole word masking for chinese bert",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:1906.08101.",
      "citeRegEx" : "Cui et al\\.,? 2019",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "A span-extraction dataset for chinese machine reading comprehension",
      "author" : [ "Yiming Cui", "Ting Liu", "Wanxiang Che", "Li Xiao", "Zhipeng Chen", "Wentao Ma", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:1810.07366.",
      "citeRegEx" : "Cui et al\\.,? 2018",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov." ],
      "venue" : "CoRR, abs/1901.02860.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Language modeling with gated convolutional networks",
      "author" : [ "Yann N Dauphin", "Angela Fan", "Michael Auli", "David Grangier." ],
      "venue" : "International conference on machine learning, pages 933–941. PMLR.",
      "citeRegEx" : "Dauphin et al\\.,? 2017",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural language models with a continuous cache",
      "author" : [ "Edouard Grave", "Armand Joulin", "Nicolas Usunier." ],
      "venue" : "arXiv preprint arXiv:1612.04426.",
      "citeRegEx" : "Grave et al\\.,? 2016",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple yet strong pipeline for hotpotqa",
      "author" : [ "Dirk Groeneveld", "Tushar Khot", "Ashish Sabharwal" ],
      "venue" : "arXiv preprint arXiv:2004.06753",
      "citeRegEx" : "Groeneveld et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Groeneveld et al\\.",
      "year" : 2020
    }, {
      "title" : "Dureader: a chinese machine reading comprehension dataset from real-world applications",
      "author" : [ "Wei He", "Kai Liu", "Jing Liu", "Yajuan Lyu", "Shiqi Zhao", "Xinyan Xiao", "Yuan Liu", "Yizhong Wang", "Hua Wu", "Qiaoqiao She" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1705.03551.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Semeval2019 task 4: Hyperpartisan news detection",
      "author" : [ "Johannes Kiesel", "Maria Mestre", "Rishabh Shukla", "Emmanuel Vincent", "Payam Adineh", "David Corney", "Benno Stein", "Martin Potthast." ],
      "venue" : "Proceedings of the 13th International Workshop on Se-",
      "citeRegEx" : "Kiesel et al\\.,? 2019",
      "shortCiteRegEx" : "Kiesel et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretrained transformers for text ranking: Bert and beyond",
      "author" : [ "Jimmy Lin", "Rodrigo Nogueira", "Andrew Yates." ],
      "venue" : "arXiv preprint arXiv:2010.06467.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "In",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "An analysis of neural language modeling at multiple scales",
      "author" : [ "Stephen Merity", "Nitish Shirish Keskar", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1803.08240.",
      "citeRegEx" : "Merity et al\\.,? 2018",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2018
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "CoRR, abs/1609.07843.",
      "citeRegEx" : "Merity et al\\.,? 2016",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2016
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Compressive transformers for long-range sequence modelling",
      "author" : [ "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Timothy P. Lillicrap." ],
      "venue" : "CoRR, abs/1911.05507.",
      "citeRegEx" : "Rae et al\\.,? 2019",
      "shortCiteRegEx" : "Rae et al\\.",
      "year" : 2019
    }, {
      "title" : "Drcd: a chinese machine reading comprehension dataset",
      "author" : [ "Chih Chieh Shao", "Trois Liu", "Yuting Lai", "Yiying Tseng", "Sam Tsai." ],
      "venue" : "arXiv preprint arXiv:1806.00920.",
      "citeRegEx" : "Shao et al\\.,? 2018",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-attention with relative position representations",
      "author" : [ "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani." ],
      "venue" : "arXiv preprint arXiv:1803.02155.",
      "citeRegEx" : "Shaw et al\\.,? 2018",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing prior knowledge needed in challenging chinese machine reading comprehension",
      "author" : [ "Kai Sun", "Dian Yu", "Dong Yu", "Claire Cardie." ],
      "venue" : "CoRR, abs/1904.09679.",
      "citeRegEx" : "Sun et al\\.,? 2019a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Thuctc: an efficient chinese text classifier",
      "author" : [ "M Sun", "J Li", "Z Guo", "Z Yu", "Y Zheng", "X Si", "Z Liu." ],
      "venue" : "GitHub Repository.",
      "citeRegEx" : "Sun et al\\.,? 2016",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint keyphrase chunking and salience ranking with bert",
      "author" : [ "Si Sun", "Chenyan Xiong", "Zhenghao Liu", "Zhiyuan Liu", "Jie Bao." ],
      "venue" : "arXiv preprint arXiv:2004.13639.",
      "citeRegEx" : "Sun et al\\.,? 2020a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Ernie 2.0: A continual pre-training framework for language understanding",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yu-Kun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Sun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Ernie: Enhanced representation through knowledge integration",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Han Zhang", "Xin Tian", "Danxiang Zhu", "Hao Tian", "Hua Wu." ],
      "venue" : "arXiv preprint arXiv:1904.09223.",
      "citeRegEx" : "Sun et al\\.,? 2019b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Sparse sinkhorn attention",
      "author" : [ "Yi Tay", "Dara Bahri", "Liu Yang", "Donald Metzler", "DaCheng Juan." ],
      "venue" : "arXiv preprint arXiv:2002.11296.",
      "citeRegEx" : "Tay et al\\.,? 2020",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple method for commonsense reasoning",
      "author" : [ "Trieu H Trinh", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1806.02847.",
      "citeRegEx" : "Trinh and Le.,? 2018",
      "shortCiteRegEx" : "Trinh and Le.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, 30:5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "Cail2019-scm: A dataset of similar case matching",
      "author" : [ "Chaojun Xiao", "Haoxi Zhong", "Zhipeng Guo", "Cunchao Tu", "Zhiyuan Liu", "Maosong Sun", "Tianyang Zhang", "Xianpei Han", "Heng Wang", "Jianfeng Xu" ],
      "venue" : null,
      "citeRegEx" : "Xiao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Open domain web keyphrase extraction beyond language modeling",
      "author" : [ "Lee Xiong", "Chuan Hu", "Chenyan Xiong", "Daniel Campos", "Arnold Overwijk." ],
      "venue" : "arXiv preprint arXiv:1911.02671.",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "Clue: A chinese language understanding evaluation",
      "author" : [ "Liang Xu", "Xuanwei Zhang", "Lu Li", "Hai Hu", "Chenjie Cao", "Weitang Liu", "Junyi Li", "Yudong Li", "Kai Sun", "Yechen Xu" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W Cohen", "Ruslan Salakhutdinov", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1809.09600.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Bp-transformer: Modelling long-range context via binary partitioning",
      "author" : [ "Zihao Ye", "Qipeng Guo", "Quan Gan", "Xipeng Qiu", "Zheng Zhang." ],
      "venue" : "arXiv preprint arXiv:1911.04070.",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "author" : [ "Xingxing Zhang", "Furu Wei", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1905.06566.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE inter-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In this paper, we propose ERNIE-DOC, a document-level language pretraining model based on Recurrence Transformers (Dai et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 36,
      "context" : "Transformers (Vaswani et al., 2017) have achieved remarkable improvements in a wide range of natural language tasks, including language model-",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "ing (Dai et al., 2019), text classification (Yang et al.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 41,
      "context" : ", 2019), text classification (Yang et al., 2019), and question answering (Devlin et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "Currently, the most prominent pretrained models, such as BERT (Devlin et al., 2018), are used on fixed-length input segments of a maximum of 512 tokens owing to the aforementioned limita-",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "However, this leads to the loss of important crosssegment information, that is, the context fragmentation problem (Dai et al., 2019), as shown in Fig.",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers (Dai et al., 2019; Rae et al., 2019) permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation;",
      "startOffset" : 127,
      "endOffset" : 163
    }, {
      "referenceID" : 26,
      "context" : "To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers (Dai et al., 2019; Rae et al., 2019) permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation;",
      "startOffset" : 127,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "In addition, Sparse Attention Transformers (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.",
      "startOffset" : 43,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : "In addition, Sparse Attention Transformers (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.",
      "startOffset" : 43,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "In addition, Sparse Attention Transformers (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.",
      "startOffset" : 43,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "infeasible because the maximum effective context length is limited by the number of layers (Dai et al., 2019), as shown in Fig.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "Sparse Attention Transformers have been extensively explored (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "Sparse Attention Transformers have been extensively explored (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "Sparse Attention Transformers have been extensively explored (Child et al., 2019; Tay et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "For instance, the Sparse Transformer (Child et al., 2019) uses a dilated sliding window that reduces the complexity to O(L √ L), where L is the sequence length.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Reformer (Kitaev et al., 2020) further reduces the complexity to O(L logL) using locality-sensitive hashing attention to compute the nearest neighbors.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 43,
      "context" : "BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Recurrence Transformers (Dai et al., 2019; Rae et al., 2019) have been successfully applied in generative language modeling.",
      "startOffset" : 24,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Recurrence Transformers (Dai et al., 2019; Rae et al., 2019) have been successfully applied in generative language modeling.",
      "startOffset" : 24,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Compressive Transformer (Rae et al., 2019) adds a compressive memory bank to sufficiently store old",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "XLNet (Yang et al., 2019) proposed a permutation language modeling objective to construct bidirectional information and achieve superior performance in multiple NLP tasks; however, its application to long-document modeling tasks remains largely unexplored.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 44,
      "context" : "Hierarchical Transformers (Zhang et al., 2019; Lin et al., 2020) have enabled significant progress on numerous document-level tasks, such as document summarization (Zhang et al.",
      "startOffset" : 26,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "Hierarchical Transformers (Zhang et al., 2019; Lin et al., 2020) have enabled significant progress on numerous document-level tasks, such as document summarization (Zhang et al.",
      "startOffset" : 26,
      "endOffset" : 64
    }, {
      "referenceID" : 44,
      "context" : ", 2020) have enabled significant progress on numerous document-level tasks, such as document summarization (Zhang et al., 2019) and document ranking (Lin et al.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "By rethinking segment-level recurrence (Dai et al., 2019), we observe that the largest possible context dependency length increases linearly w.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "In addition to the masked language model (MLM) objective (Devlin et al., 2018), we introduce an additional document-aware task called segment-reordering objective for pretraining.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "For comparison with previous work, we conducted experiments on wordlevel LM, that is, WikiText-103 (Merity et al., 2016), which is a document-level language modeling dataset.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "For autoregressive language modeling, we use a memory-enhanced Transformer-XL (Dai et al., 2019), that is, we employ our enhanced recurrence mechanism to replace the primitive one used in the Transformer-XL.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Additionally, as proposed by Segatron (Bai et al., 2020), we introduce the segment-aware mechanism into Transformer-XL.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "PPL Results of base models LSTM (Grave et al., 2016) - 48.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "0 SegaTransformer-XL Base (Bai et al., 2020) 151M 22.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Results of large models Adaptive Input (Baevski and Auli, 2018) 247M 18.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "3 Compressive Transformer (Rae et al., 2019) 247M 17.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "1 SegaTransformer-XL Large (Bai et al., 2020) 247M 17.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 45,
      "context" : "To allow ERNIE-DOC to capture long dependencies in pretraining, we compiled a corpus from four standard datasets: WIKIPEDIA, BOOKSCORPUS (Zhu et al., 2015), CC-NEWS4, and STORIES (Trinh and Le, 2018) (details listed in Tab.",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 35,
      "context" : ", 2015), CC-NEWS4, and STORIES (Trinh and Le, 2018) (details listed in Tab.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "We tokenized the corpus using the RoBERTa wordpieces tokenizer (Liu et al., 2019) and duplicated the pretraining data 10 times.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "ERNIE-DOC was optimized with the Adam (Kingma and Ba, 2014) optimizer.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "The remaining pretraining hyperparameters were the same as those of RoBERTa (Liu et al., 2019) (see Tab.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "Additionally, we employed relative positional embedding (Shaw et al., 2018) in our",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "model pretraining because it is necessary for reusing hidden state without causing temporal confusion (Dai et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "We consider two datasets: IMDB reviews (Maas et al., 2011) and Hyperpartisan News Detection",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "We utilized two documentlevel QA datasets (Wikipedia setting of TriviaQA (TQA) (Joshi et al., 2017) and distractor setting of HotpotQA (HQA) (Yang et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 42,
      "context" : ", 2017) and distractor setting of HotpotQA (HQA) (Yang et al., 2018)) to evaluate the reasoning ability of the models over long",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "TQA and HQA are extractive QA tasks, and we follow the simple QA model of BERT (Devlin et al., 2018) to predict an answer with the maximum sum of start and end logits across multiple segments of a sample.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "In addition, we use a modified cross-entropy loss (Clark and Gardner, 2017) for the TQA dataset and use a two-stage model (Groeneveld et al.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "In addition, we use a modified cross-entropy loss (Clark and Gardner, 2017) for the TQA dataset and use a two-stage model (Groeneveld et al., 2020) with the backbone of ERNIE-DOC for the HQA dataset.",
      "startOffset" : 122,
      "endOffset" : 147
    }, {
      "referenceID" : 39,
      "context" : "We include OpenKP (Xiong et al., 2019) dataset to evaluate ERNIE-DOC’s ability to extract keyphrases from a long document.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "Each document contains up to three short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : ", 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "We conducted extensive experiments on seven Chinese natural language understanding (NLU) tasks, including machine reading comprehension (CMRC2018 (Cui et al., 2018), DRCD (Shao et al.",
      "startOffset" : 146,
      "endOffset" : 164
    }, {
      "referenceID" : 27,
      "context" : ", 2018), DRCD (Shao et al., 2018), DuReader (He et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 38,
      "context" : ", 2019a)), semantic similarity (CAIL2019SCM (CAIL) (Xiao et al., 2019)), and long-text classification (IFLYTEK (IFK) (Xu et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 40,
      "context" : ", 2019)), and long-text classification (IFLYTEK (IFK) (Xu et al., 2020), THUCNews (THU)5 (Sun et al.",
      "startOffset" : 54,
      "endOffset" : 71
    } ],
    "year" : 2021,
    "abstractText" : "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-DOC, a document-level language pretraining model based on Recurrence Transformers (Dai et al., 2019). Two welldesigned techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-DOC 1, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-DOC to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
    "creator" : "LaTeX with hyperref"
  }
}