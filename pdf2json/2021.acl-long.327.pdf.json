{
  "name" : "2021.acl-long.327.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Verb Metaphor Detection via Contextual Relation Learning",
    "authors" : [ "Wei Song", "Shuhui Zhou", "Ruiji Fu", "Ting Liu", "Lizhen Liu" ],
    "emails" : [ "liz_liu7480}@cnu.edu.cn,", "rjfu@iflytek.com,", "tliu@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4240–4251\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4240"
    }, {
      "heading" : "1 Introduction",
      "text" : "Metaphor is ubiquitous in our daily life for effective communication (Lakoff and Johnson, 1980). Metaphor processing has become an active research topic in natural language processing due to its importance in understanding implied meanings.\nThis task is challenging, requiring contextual semantic representation and reasoning. Various contexts and linguistic representation techniques have been explored in previous work.\nEarly methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored.\nRecently, token-level neural metaphor detection draws more attention. Several approaches discov-\n∗These authors contributed equally to this work.\nered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021).\nDespite the progress of exploiting sentential context, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work. More importantly, as shown in Figure 1, most token-level metaphor detection methods formulate metaphor detection as either a single-word classification or a sequence labeling problem (Gao et al., 2018). The context information is mainly used for learning contextual representations of tokens, rather than modeling the interactions between the target word and its contexts (Zayed et al., 2020).\nIn this paper, we focus on token-level verb metaphor detection, since verb metaphors are of the most frequent type of metaphoric expressions (Shutova and Teufel, 2010a). As shown in Figure 1, we propose to formulate verb metaphor detection as a relation extraction problem, instead of token classification or sequence labeling formulations. In analogy to identify the relations between entities, our method models the relations between a target verb and its various contexts, and determines the verb’s metaphoricity based on the relation representation rather than only the verb’s (contextual) representation.\nWe present a simple yet effective model — Metaphor-relation BERT (MrBERT), which is adapted from a BERT (Devlin et al., 2019) based state-of-the-art relation learning model (Bal-\ndini Soares et al., 2019). Our model has three highlights, as illustrated in Figure 2. First, we explicitly extract and represent context components, such as a verb’s arguments as the local context, the whole sentence as the global context, and its basic meaning as a distant context. So multiple contexts can be modeled interactively and integrated together. Second, MrBERT enables modeling the metaphorical relation between a verb and its context components, and uses the relation representation for determining the metaphoricity of the verb. Third, the model is flexible to incorporate sophisticated relation modeling methods and new types of contexts.\nWe conduct experiments on the largest metaphor detection corpus VU Amsterdam Metaphor Corpus (VUA) (Steen, 2010). Our method obtains competitive results on the large VUA dataset. Detail analysis demonstrates the benefits of integrating various types of contexts for relation classification. The results on relatively small datasets, such as MOH-X and TroFi, also show good performance and model transferability."
    }, {
      "heading" : "2 Formulating Verb Metaphor Detection",
      "text" : "This section briefly summarizes the common formulations of token-level verb metaphor detection as a background, and discusses the relation between this paper and previous work. The task A given sentence contains a sequence of n tokens x = x1, ..., xn, and a target verb in this sentence is xi. Verb metaphor detection is to judge whether xi has a literal or a metaphorical sense. Basic formulations Most neural networks based approaches cast the task as a classification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018). As shown in Figure 1, the classification paradigm predicts a single binary la-\nbel to indicate the metaphoricity of the target verb, while the sequence labeling paradigm predicts a sequence of binary labels to all tokens in a sentence.\nBased on the basic formulations, various approaches have tried to enhance feature representations by using globally trained contextual word embeddings (Gao et al., 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al., 2018; Mao et al., 2019) and Transformers (Dankers et al., 2019; Su et al., 2020). Limitations and recent trends However, the above two paradigms have some limitations.\nFirst, contextual information is mostly used to enhance the representation of the target word, but the interactions between the target word and its contexts are not explicitly modeled (Zayed et al., 2020; Su et al., 2020). To alleviate this, Su et al. (2020) proposed a new paradigm by viewing metaphor detection as a reading comprehension problem, which uses the target word as a query and captures its interactions with the sentence and clause. A concurrent work to this work (Choi et al., 2021) adopted a pre-trained contextualized model based late interaction mechanism to compare the basic meaning and the contextual meaning of a word.\nSecond, exploiting wider context will bring in more noise and may lose the focus. Fully depending on data-driven models to discover useful contexts is difficult, given the scale of available datasets for metaphor detection is still limited. The grammar structures, such as verb arguments, are important for metaphor processing (Wilks, 1978), but is not well incorporated into neural models. Stowe et al. (2019) showed that data augmentation based on syntactic patterns can enhance a standard model. Le et al. (2020) adopted graph convolutional networks to incorporate dependency graphs, but did\nnot consider specific grammatical relations. It is interesting to further explore how to integrate explicit linguistic structures for contextual modeling.\nThis paper presents a new paradigm for verb metaphor detection to overcome these limitations, by viewing the task as a relation extraction task. We assume a target verb and its multiple contexts are entities, and metaphor detection is to determine whether a metaphorical relation holds between the verb and its contexts.\nWe will introduce the proposed model in Section 3. Before diving into details, we argue that viewing metaphor as a relation is reasonable and consistent with existing metaphor theories. According to Wilks (1978), metaphors show a violation of selectional preferences in a given context. The conceptual metaphor theory views metaphors as transferring knowledge from a familiar, or concrete domain to an unfamiliar, or more abstract domain (Lakoff and Johnson, 1980; Turney et al., 2011). The metaphor identification procedure (MIP) theory (Group, 2007) aims to identify metaphorically used words in discourse based on comparing their use in particular context and their basic meanings. All the theories care about a kind of relations between a target word and its contexts, which may help identify metaphors."
    }, {
      "heading" : "3 Metaphor-Relation BERT (MrBERT)",
      "text" : "We propose the Metaphor-relation BERT (MrBERT) model to realize verb metaphor detection as a relation classification task.\nFigure 2 shows the architecture of MrBERT. We use the pre-trained language model BERT as the backbone model. There are three main procedures: (1) extract and represent contexts; (2) model the contextual relations between the target verb and its contexts; (3) manipulate the contextual relations for predicting the verb’s metaphoricity."
    }, {
      "heading" : "3.1 Contexts and their Representations",
      "text" : ""
    }, {
      "heading" : "3.1.1 Types of Contexts",
      "text" : "A metaphor can result when a target word interacts with a certain part in a sentence. Previous work often explored individual context types, such as verb arguments through grammatical relations or the whole sentence/clause. Little work has attempted to summarize and combine different contexts.\nWe summarize the following contexts, which would help determine verbs’ metaphoricity:\n• Global context: We view the whole sentence as the global context. A metaphorically used word may seem divergent to the meaning or topic of the sentence.\n• Local context: We view the words that have a close grammatical relation to the target words as the local context, which is widely studied to capture selectional preference violations.\n• Distant context: Motivated by the MIP theory, the difference between the contextual usage of a word and its basic meaning may indicate a metaphor so that we view the basic meaning of the target verb as a distant context.\nThen, we have to extract and represent these contexts."
    }, {
      "heading" : "3.1.2 Context Extraction and Representation",
      "text" : "We call the target verb’s contexts as context components. To get the contextual or basic meanings of these components. we use the deep transformer models, such as BERT.\nWe first use Stanford dependency parser (Chen and Manning, 2014) to parse each sentence and extract verb-subject and verb-direct object relations with VB head and NN dependent. The nominal subjects and objects are used as the local context components.\nMotivated by (Baldini Soares et al., 2019), we introduce 6 component marker tokens, [subj], [/subj], [verb], [/verb], [obj] and [/obj], to explicitly label the boundaries of the target verb, its subject and object in each sentence. We also use [CLS] and [SEP ] to mark the whole sentence. For example, the marker inserted token sequence for the sentence He absorbed the costs for the accident is shown in Figure 2. The whole token sequence is fed into BERT’s tokenizer, and then the transformer layers.\nTo get the contextual representations, we use the hidden states of the final transformer layer. For each marked component, we use the start marker (e.g., [subj]) or the averaged embedding between the start and the end markers (e.g., [subj] and [/subj]) as the component representation.\nThe contextual representation of the whole sentence is read from the final hidden state of [CLS].\nTo represent the basic meaning of the verb, we use the output from the BERT tokenizer to get the context independent verb representation. If word pieces exist, their averaged embedding is used."
    }, {
      "heading" : "3.2 Modeling the Contextual Relation",
      "text" : "The relation between the target verb and one of its contexts is called a contextual relation. Our\npurpose is to utilize the contextual relation(s) to determine the metaphoricity of the verb.\nThe representations of the verb and a context component are denoted as v ∈ Rd and c ∈ Rk, respectively. We adopt three ways to explicitly define the form of the relation r for capturing the interactions between v and c.\n• Linear model We use a parameter vector Vr ∈ Rd+k and a bias br to represent the relation r, and the probability of the relation being metaphorical is computed according to\np(r|v, c) = σ(V >r ( v c ) + br), (1)\nwhere σ is the sigmoid function.\n• Bilinear model We use a parameter matrix Ar ∈ Rd×k and a bias br to represent the relation r:\np(r|v, c) = σ(v>Arc+ br). (2)\nThe components and the relation can interact more sufficiently with each other in this way.\n• Neural tensor model We also exploit a simplified neural tensor model for relation representation:\np(r|v, c) = σ(v>Arc+V >r ( v c ) + br). (3)"
    }, {
      "heading" : "3.3 Integrating Contextual Relations for Prediction",
      "text" : "We focus on 3 types of contextual relations:\n• Verb-global relation The relation between the contextual representations of the verb v and the whole sentence cCLS .\n• Verb-local relation The relation between the contextual representations of the verb v and its subject csubj or object cobj .\n• Verb-distant relation The relation between the verb v and its basic meaning vbsc.\nThe representations of csubj , cobj , cCLS and vbsc can be obtained as described in Section 3.1.2. We try three ways to integrate the contextual relations. The first two ways build a combined context c first:\n• Context concatenation We can concatenate the representations of context components together as the combined context, i.e., c = csubj ⊕ cobj ⊕ cCLS ⊕ vbsc.\n• Context average Similarly, we can use the averaged representation of all context components as the combined context, i.e., c = average(csubj , cobj , cCLS , vbsc).\nThen we compute the probability that the relation is metaphorical, i.e., p(r|v, c), where either linear, bilinear or neutral tensor model can be applied.\nThe other way is to choose the most confident single prediction, i.e.,\n• Context maxout The prediction is based on max{p(r|v, c)}, where c belongs to {cCLS , csubj , cobj , vbsc}.\nTo train the relation-level prediction model, we use binary cross-entropy as the loss function,\nL0 = − 1\nN N∑ i=1 (ŷiyi + (1− ŷi)(1− yi)), (4)\nwhere N is the number of training samples; ŷi is the golden label of a verb with ŷi = 1 indicating a metaphorical usage and ŷi = 0 indicating a literal usage; yi is the probability of being metaphorical predicted by our model.\nWe further combine relation-level and sequencelevel metaphor detection via multi-task learning. The sequence metaphor detection uses the hidden states of the final layer and a softmax layer for predicting the metaphoricity of each token. We use cross-entropy as the loss function and denote the average loss over tokens in training samples as L1. The final loss of MrBERT is L = L0 + L1."
    }, {
      "heading" : "4 Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "4.1.1 Datasets and Evaluation Metrics",
      "text" : "VUA dataset We mainly conduct experiments on the VUA (Steen, 2010) dataset. It is the largest publicly available metaphor detection dataset and has been used in metaphor detection shared tasks (Leong et al., 2018, 2020). This dataset has a training set and a test set. Previous work utilized the training set in different ways (Neidlein et al., 2020). We use the preprocessed version of the VUA dataset provided by Gao et al. (2018). The first reason is that this dataset has a fixed development set so that different methods can adopt the same model selection strategy. The second reason is that several recent important methods used the same dataset (Mao et al., 2018; Dankers et al.,\n2019; Stowe et al., 2019; Le et al., 2020). Therefore it is convenient for us to compare the proposed method with previous work.\nThere are two tracks: Verb and All-POS metaphor detection. Some basic statistics of the dataset are shown in Table 1. We focus on the Verb track since we mainly model metaphorical relations for verbs. We use MrBERT’s relation-level predictions for the verb track and use its sequence labeling module to deal with the All-POS track. MOH-X and TroFi datasets MOH-X (Mohammad et al., 2016) and TroFi (Birke and Sarkar, 2006) are two relatively smaller datasets compared with VUA. Only a single target verb is annotated in each sentence. We will report the results on MOHX and TroFi in three settings: zero-shot transfer, re-training and fine-tuning. Metrics The evaluation metrics are accuracy (Acc), precision (P), recall (R) and F1-score (F1), which are most commonly used in previous work."
    }, {
      "heading" : "4.1.2 Baselines",
      "text" : "We compare with the following approaches.\n• Gao et al. (2018) use contextual embeddings ELMo to enhance word representations and use BiLSTM as the encoder. It has two settings: classification (CLS) and sequence labeling (SEQ).\n• Mao et al. (2019) exploit two linguistic theory motivated intuitions based on the basis of (Gao et al., 2018). This work motivates us to further explore contextual relation modeling with pre-trained language models.\n• Stowe et al. (2019) exploit grammatical relations for data augmentation to enhance (Gao et al., 2018).\n• Le et al. (2020) propose a multi-task learning approach with graph convolutional neural networks and use word sense disambiguation as an auxiliary task.\n• Neidlein et al. (2020) (BERT-SEQ) provide a detail setting for a BERT based sequence labeling model. This method is used as a main pre-trained language model based baseline.\nThe above methods all used Gao et al. (2018)’s dataset for evaluation so that their results can be directly read from their papers for comparison.\n• Su et al. (2020) (DeepMet) view metaphor detection as a reading comprehension problem with RoBERTa as the backbone model. It obtained the best performance on 2020 metaphor detection shared task.\n• Choi et al. (2021) (MelBERT) present a concurrent work to ours. The method shares similar ideas and architecture with us, but it does not consider the grammatical relations.\nNotice that the systems participating in the VUA metaphor detection shared tasks (Leong et al., 2018, 2020) can use any way to manipulate the training set for model selection and ensemble learning so that the reported results in the task report are not directly comparable to us. The results of DeepMet and MelBERT are based on the single model evaluation in (Choi et al., 2021).\nThe first four baselines do not utilize pre-trained language models, while the last three baselines use BERT or RoBERTa. These baselines support comprehensive comparisons from multiple aspects."
    }, {
      "heading" : "4.1.3 Parameter Configuration",
      "text" : "During context component extraction, if the target verb does not have a subject or an object, we use a fixed zero vector instead. We use the bert-baseuncased model and the standard tokenizer. The values of hyper-parameters are shown in Table 2.\nFor MrBERT, we view the ways of component representation (start marker or averaged embedding, see Section 3.1.2), relation modeling (linear, bilinear, and neural tensor (NT)) models, see Section 3.2) and context integration (context concatenation, average and maxout, see Section 3.3)\nstrategies as hyper-parameters as well. We run each model for 10 epoches, and choose the best combination according to the performance on the development set. The best combination uses the averaged embeddings, the bilinear model and the context average strategy, and it will represent MrBERT for performance report in Section 4.2."
    }, {
      "heading" : "4.2 Main Results on VUA Dataset",
      "text" : "Table 3 shows the results of the baselines and MrBERT. Except for (Gao et al., 2018)-CLS, all methods use the annotation information of all tokens. For the All-POS track, we report the performance on either all POS tags or 4 main POS tags for comparison with previous work.\nWe can see that MrBERT achieves superior or competitive performance compared with previous work on verb metaphor detection. The use of pretrained language models improves the performance in general, compared with several LSTM based methods. Recent proposed models, such as DeepMet, MelBERT and MrBERT, gain further improvements compared with BERT-SEQ.\nMrBERT outperforms (Stowe et al., 2019) and (Le et al., 2020) largely. The two baselines attempt to make use of grammar information, through data augmentation or graph neural networks. In contrast, MrBERT provides a simple yet effective way to incorporate verb arguments and new contexts into a pre-trained language model.\nMrBERT also has competitive performance compared with DeepMet and MelBERT. We share the similar idea to enhance interactions between the target verb and its contexts, but implement in different ways. DeepMet and MelBERT base on the pretrained model RoBERTa and use additional POS or FGPOS information. Moreover, these two models are trained for every token so that the training might be more sufficient. In contrast, we mainly model metaphorical relation for verbs. This is perhaps also the reason that on the All-POS metaphor detection track, MrBERT has slightly worse results compared with MelBERT. However, our model is flexible and can be applied to tokens with other POS tags as well. We leave this as future work."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "We further analyze the effects of modeling contextual relations from several aspects. Relation modeling and context integration strategies Table 4 shows the results of different\ncombinations of relation modeling and context integration strategies.\nBERT-SEQ here refers to the re-trained baseline with model selection based on the performance on the development set, and surpasses the reported results in (Neidlein et al., 2020). We can see that most combinations outperform BERT-SEQ, and have consistent performance. The bilinear and neural tensor models perform better than the linear model. This means that sophisticated relation modelling techniques can benefit the performance.\nContext average and context maxout strategies perform better than context concatenation. The reason may be that context concatenation is more difficult to be trained due to more parameters. Effects of different contexts Table 5 shows the performance of MrBERT when it considers the global context (MrBERT-G), the global and the local contexts (MrBERT-GL), and the full model with the distant context (MrBERT-GLD). Each model is trained separately, with the same model selection procedure. We can see that integrating multiple contexts leads to better performance.\nMrBERT explicitly incorporates verb arguments through grammatical relations as the local context, which differs from other methods. We are interested in the effect of such information.\nWe analyze MrBERT-G and MrBERT-GL. Table 6 shows the distribution of auto-extracted verbsubject and verb-direct object relations in the VUA test dataset. ∆F1 values indicate the improvements of MrBERT-G compared with BERT-SEQ in F1. We can see that MrBERT-G outperforms BERTSEQ mainly when verb’s arguments are incomplete. For verbs with complete verb-subject and verb-direct object structures, little improvement is gained.\nTable 7 shows the corresponding performance of MrBERT-GL. Better performance is obtained for verbs with all status of grammatical relations. The improvement on verbs in the lower right corner is obvious. In these cases, the verbs are usually intransitive verbs or used as a noun or an adjective. The benefit of involving grammatical relations may be that it helps keep a dynamic and balanced focus between the global and local contexts according to the signals expressed by the grammatical structure.\nIntuitively, the effect of incorporating grammatical relations should be more obvious for metaphor detection in long sentences, since the local and global contexts are quite different. To verify this, we divide sentences in the test dataset into bins\naccording to the number of clauses. Figure 3 confirms our hypothesis that MrBERT obtains larger improvements on sentences with more clauses, indicating that incorporating grammatical relations can help filter noisy information.\nFinally, the use of distant context obtains a further improvement. This observation is consistent with the conclusion of (Choi et al., 2021). It also indicates that the BERT tokenizer’s embedding can be used to approximate the representation of the target verb’s basic meaning."
    }, {
      "heading" : "4.4 Results on MOH-X and TroFi Datasets",
      "text" : "Table 8 shows the results on the MOH-X and TroFi datasets.\nIn the zero-shot transfer setting, MrBERT obtains better performance compared with DeepMet and MelBERT on both datasets. The performance of DeepMet and MelBERT is read from (Choi et al.,\n2021). The results means MrBERT has good zeroshot transferability, although these datasets have quite different characteristics.\nIn the 10-fold cross-validation setting, the retrained MrBERT can also obtain superior or competitive results compared with previous work. If we continue to fine-tune the pre-trained MrBERT on the target datasets, better performance can be obtained, especially on the MOH-X dataset."
    }, {
      "heading" : "5 Related Work",
      "text" : "Metaphor detection is a key task in metaphor processing (Veale et al., 2016). It is typically viewed as a classification problem. The early methods were based on rules (Fass, 1991; Narayanan, 1997),\nwhile most recent methods are data-driven. Next, we summarize data-driven methods from the perspective of context types that have been explored.\nGrammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments.\nFeature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based concept representations.\nThe above handcrafted features heavily rely on linguistic resources and expertise. Recently, distributed representations are exploited for grammatical relation-level metaphor detection. Distributed word embeddings were used as features (Tsvetkov et al., 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018). Visual distributed representations were also proven to be useful (Shutova et al., 2016). Rei et al. (2017) designed a supervised similarity network to capture interactions between words. Song et al. (2020) modeled metaphors as attribute-dependent domain mappings and presented a knowledge graph embedding approach for modeling nominal metaphors. Zayed et al. (2020) identified verb-noun and adjective-noun phrasal metaphoric expressions by modeling phrase representations as a context.\nToken-level detection Another line of work formulates metaphor detection as a single token classification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018; Mao et al., 2019). These approaches are mostly based on neural network architectures and learn representations in an end-to-end fashion. These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010).\nBiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019). Recently, Transformer based pre-trained language models become\nthe most popular architecture in the metaphor detection shared task (Leong et al., 2020). Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al., 2020) have been exploited as well. Discussion The grammatical relation-level and token-level metaphor detection consider different aspects of information. Grammatical relations incorporate syntactic structures, which are well studied in selectional preferences (Wilks, 1975, 1978) and provide important clues for metaphor detection. However, sentential context is also useful but is ignored. In contrast, token-level metaphor detection explores wider context and gains improvements, but syntactic information is neglected and as discussed in (Zayed et al., 2020), interactions between metaphor components are not explicitly modeled.\nThis paper aims to combine the grammatical relation-level, token-level and semantic-level information through pre-trained language model based contextual relation modeling."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presented the Metaphor-relation BERT (MrBERT) model for verb metaphor detection. We propose a new view to formulate the task as modeling the metaphorical relation between the target verb and its multiple context components, i.e., contextual relations. We propose and evaluate various ways to extract, model and integrate contextual relations for metaphoricity prediction. We conduct comprehensive experiments on the VUA dataset. The evaluation shows that MrBERT achieves superior or competitive performance compared with previous methods. We also observe that incorporating grammatical relations can help balance local and global contexts, and the basic meaning of the verb as a distant context is effective. Further experiments on small datasets MOH-X and TroFi also show good model transferability of MrBERT."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the National Natural Science Foundation of China (Nos. 61876113, 61876112), Beijing Natural Science Foundation (No. 4192017), Support Project of Highlevel Teachers in Beijing Municipal Universities in the Period of 13th Five-year Plan (CIT&TCD20170322). Lizhen Liu is the corresponding author."
    } ],
    "references" : [ {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "A clustering approach for nearly unsupervised recognition of nonliteral language",
      "author" : [ "Julia Birke", "Anoop Sarkar." ],
      "venue" : "11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Associa-",
      "citeRegEx" : "Birke and Sarkar.,? 2006",
      "shortCiteRegEx" : "Birke and Sarkar.",
      "year" : 2006
    }, {
      "title" : "Modelling metaphor with attribute-based semantics",
      "author" : [ "Luana Bulat", "Stephen Clark", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,",
      "citeRegEx" : "Bulat et al\\.,? 2017",
      "shortCiteRegEx" : "Bulat et al\\.",
      "year" : 2017
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Associa-",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Go figure! multi-task transformer-based architecture for metaphor detection using idioms: ETS team",
      "author" : [ "Xianyang Chen", "Chee Wee (Ben) Leong", "Michael Flor", "Beata Beigman Klebanov" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "MelBERT: Metaphor detection via contextualized late interaction using metaphorical identification theories",
      "author" : [ "Minjin Choi", "Sunkyung Lee", "Eunseong Choi", "Heesoo Park", "Junhyuk Lee", "Dongwon Lee", "Jongwuk Lee." ],
      "venue" : "Proceedings of the 2021",
      "citeRegEx" : "Choi et al\\.,? 2021",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2021
    }, {
      "title" : "Being neighbourly: Neural metaphor identification in discourse",
      "author" : [ "Verna Dankers", "Karan Malhotra", "Gaurav Kudva", "Volodymyr Medentsiy", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing,",
      "citeRegEx" : "Dankers et al\\.,? 2020",
      "shortCiteRegEx" : "Dankers et al\\.",
      "year" : 2020
    }, {
      "title" : "Modelling the interplay of metaphor and emotion through multitask learning",
      "author" : [ "Verna Dankers", "Marek Rei", "Martha Lewis", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Dankers et al\\.,? 2019",
      "shortCiteRegEx" : "Dankers et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Tokenlevel metaphor detection using neural networks",
      "author" : [ "Erik-Lân Do Dinh", "Iryna Gurevych." ],
      "venue" : "Proceedings of the Fourth Workshop on Metaphor in NLP, pages 28–33, San Diego, California. Association for Computational Linguistics.",
      "citeRegEx" : "Dinh and Gurevych.,? 2016",
      "shortCiteRegEx" : "Dinh and Gurevych.",
      "year" : 2016
    }, {
      "title" : "met*: A method for discriminating metonymy and metaphor by computer",
      "author" : [ "Dan Fass." ],
      "venue" : "Computational linguistics, 17(1):49–90.",
      "citeRegEx" : "Fass.,? 1991",
      "shortCiteRegEx" : "Fass.",
      "year" : 1991
    }, {
      "title" : "Neural metaphor detection in context",
      "author" : [ "Ge Gao", "Eunsol Choi", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 607–613, Brussels, Belgium. Association for",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Mip: A method for identifying metaphorically used words in discourse",
      "author" : [ "Pragglejaz Group." ],
      "venue" : "Metaphor and symbol, 22(1):1–39.",
      "citeRegEx" : "Group.,? 2007",
      "shortCiteRegEx" : "Group.",
      "year" : 2007
    }, {
      "title" : "Literal and metaphorical senses in compositional distributional semantic models",
      "author" : [ "E. Dario Gutiérrez", "Ekaterina Shutova", "Tyler Marghetis", "Benjamin Bergen." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Gutiérrez et al\\.,? 2016",
      "shortCiteRegEx" : "Gutiérrez et al\\.",
      "year" : 2016
    }, {
      "title" : "Metaphors we live by",
      "author" : [ "George Lakoff", "Mark Johnson." ],
      "venue" : "University of Chicago press.",
      "citeRegEx" : "Lakoff and Johnson.,? 1980",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 1980
    }, {
      "title" : "Multitask learning for metaphor detection with graph convolutional neural networks and word sense disambiguation",
      "author" : [ "Duong Le", "My Thai", "Thien Nguyen." ],
      "venue" : "AAAI, pages 8139–8146.",
      "citeRegEx" : "Le et al\\.,? 2020",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "A report on the 2020 VUA and TOEFL metaphor detection shared task",
      "author" : [ "Chee Wee (Ben) Leong", "Beata Beigman Klebanov", "Chris Hamill", "Egon Stemle", "Rutuja Ubale", "Xianyang Chen" ],
      "venue" : "In Proceedings of the Second Workshop on Figurative",
      "citeRegEx" : "Leong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Leong et al\\.",
      "year" : 2020
    }, {
      "title" : "A report on the 2018 VUA metaphor detection shared task",
      "author" : [ "Chee Wee (Ben) Leong", "Beata Beigman Klebanov", "Ekaterina Shutova" ],
      "venue" : "In Proceedings of the Workshop on Figurative Language Processing,",
      "citeRegEx" : "Leong et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Leong et al\\.",
      "year" : 2018
    }, {
      "title" : "Word embedding and WordNet based metaphor identification and interpretation",
      "author" : [ "Rui Mao", "Chenghua Lin", "Frank Guerin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Mao et al\\.,? 2018",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end sequential metaphor identification inspired by linguistic theories",
      "author" : [ "Rui Mao", "Chenghua Lin", "Frank Guerin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3888–3898, Flo-",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphor as a medium for emotion: An empirical study",
      "author" : [ "Saif Mohammad", "Ekaterina Shutova", "Peter Turney." ],
      "venue" : "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 23–33.",
      "citeRegEx" : "Mohammad et al\\.,? 2016",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic signatures for example-based linguistic metaphor detection",
      "author" : [ "Michael Mohler", "David Bracewell", "Marc Tomlinson", "David Hinote." ],
      "venue" : "Proceedings of the First Workshop on Metaphor in NLP, pages 27–35, Atlanta, Georgia. Association",
      "citeRegEx" : "Mohler et al\\.,? 2013",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowledge-based action representations for metaphor and aspect (KARMA)",
      "author" : [ "Srini Narayanan." ],
      "venue" : "Ph.D. thesis, Ph. D. thesis, University of California at Berkeley.",
      "citeRegEx" : "Narayanan.,? 1997",
      "shortCiteRegEx" : "Narayanan.",
      "year" : 1997
    }, {
      "title" : "An analysis of language models for metaphor recognition",
      "author" : [ "Arthur Neidlein", "Philip Wiesenbach", "Katja Markert." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3722–3736.",
      "citeRegEx" : "Neidlein et al\\.,? 2020",
      "shortCiteRegEx" : "Neidlein et al\\.",
      "year" : 2020
    }, {
      "title" : "Grasping the finer point: A supervised similarity network for metaphor detection",
      "author" : [ "Marek Rei", "Luana Bulat", "Douwe Kiela", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rei et al\\.,? 2017",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2017
    }, {
      "title" : "Verbal multiword expressions for identification of metaphor",
      "author" : [ "Omid Rohanian", "Marek Rei", "Shiva Taslimipoor", "Le An Ha." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2890–2895, On-",
      "citeRegEx" : "Rohanian et al\\.,? 2020",
      "shortCiteRegEx" : "Rohanian et al\\.",
      "year" : 2020
    }, {
      "title" : "Black holes and white rabbits: Metaphor identification with visual features",
      "author" : [ "Ekaterina Shutova", "Douwe Kiela", "Jean Maillard." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Shutova et al\\.,? 2016",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised metaphor identification using hierarchical graph factorization clustering",
      "author" : [ "Ekaterina Shutova", "Lin Sun." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Shutova and Sun.,? 2013",
      "shortCiteRegEx" : "Shutova and Sun.",
      "year" : 2013
    }, {
      "title" : "Metaphor corpus annotated for source - target domain mappings",
      "author" : [ "Ekaterina Shutova", "Simone Teufel." ],
      "venue" : "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta. Euro-",
      "citeRegEx" : "Shutova and Teufel.,? 2010a",
      "shortCiteRegEx" : "Shutova and Teufel.",
      "year" : 2010
    }, {
      "title" : "Metaphor corpus annotated for source-target domain mappings",
      "author" : [ "Ekaterina Shutova", "Simone Teufel." ],
      "venue" : "LREC, volume 2, pages 2–2. Citeseer.",
      "citeRegEx" : "Shutova and Teufel.,? 2010b",
      "shortCiteRegEx" : "Shutova and Teufel.",
      "year" : 2010
    }, {
      "title" : "A knowledge graph embedding approach for metaphor processing",
      "author" : [ "Wei Song", "Jingjin Guo", "Ruiji Fu", "Ting Liu", "Lizhen Liu." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:406–420.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "A method for linguistic metaphor identification: From MIP to MIPVU, volume 14",
      "author" : [ "Gerard Steen." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Steen.,? 2010",
      "shortCiteRegEx" : "Steen.",
      "year" : 2010
    }, {
      "title" : "Linguistic analysis improves neural metaphor detection",
      "author" : [ "Kevin Stowe", "Sarah Moeller", "Laura Michaelis", "Martha Palmer." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 362–371, Hong",
      "citeRegEx" : "Stowe et al\\.,? 2019",
      "shortCiteRegEx" : "Stowe et al\\.",
      "year" : 2019
    }, {
      "title" : "DeepMet: A reading comprehension paradigm for token-level metaphor detection",
      "author" : [ "Chuandong Su", "Fumiyo Fukumoto", "Xiaoxi Huang", "Jiyi Li", "Rongbo Wang", "Zhiqun Chen." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "Metaphor detection with cross-lingual model transfer",
      "author" : [ "Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Tsvetkov et al\\.,? 2014",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2014
    }, {
      "title" : "Cross-lingual metaphor detection using common semantic features",
      "author" : [ "Yulia Tsvetkov", "Elena Mukomel", "Anatole Gershman." ],
      "venue" : "Proceedings of the First Workshop on Metaphor in NLP, pages 45– 51, Atlanta, Georgia. Association for Computational",
      "citeRegEx" : "Tsvetkov et al\\.,? 2013",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2013
    }, {
      "title" : "Literal and metaphorical sense identification through concrete and abstract context",
      "author" : [ "Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical",
      "citeRegEx" : "Turney et al\\.,? 2011",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2011
    }, {
      "title" : "Metaphor: A computational perspective",
      "author" : [ "Tony Veale", "Ekaterina Shutova", "Beata Beigman Klebanov." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 9(1):1–160.",
      "citeRegEx" : "Veale et al\\.,? 2016",
      "shortCiteRegEx" : "Veale et al\\.",
      "year" : 2016
    }, {
      "title" : "A preferential, pattern-seeking, semantics for natural language inference",
      "author" : [ "Yorick Wilks." ],
      "venue" : "Artificial intelligence, 6(1):53–74.",
      "citeRegEx" : "Wilks.,? 1975",
      "shortCiteRegEx" : "Wilks.",
      "year" : 1975
    }, {
      "title" : "Making preferences more active",
      "author" : [ "Yorick Wilks." ],
      "venue" : "Artificial intelligence, 11(3):197–223.",
      "citeRegEx" : "Wilks.,? 1978",
      "shortCiteRegEx" : "Wilks.",
      "year" : 1978
    }, {
      "title" : "Contextual modulation for relationlevel metaphor identification",
      "author" : [ "Omnia Zayed", "John P. McCrae", "Paul Buitelaar." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 388–406, Online. Association for Com-",
      "citeRegEx" : "Zayed et al\\.,? 2020",
      "shortCiteRegEx" : "Zayed et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Metaphor is ubiquitous in our daily life for effective communication (Lakoff and Johnson, 1980).",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016).",
      "startOffset" : 161,
      "endOffset" : 235
    }, {
      "referenceID" : 35,
      "context" : "Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016).",
      "startOffset" : 161,
      "endOffset" : 235
    }, {
      "referenceID" : 13,
      "context" : "Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016).",
      "startOffset" : 161,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : "Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021).",
      "startOffset" : 80,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021).",
      "startOffset" : 80,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "More importantly, as shown in Figure 1, most token-level metaphor detection methods formulate metaphor detection as either a single-word classification or a sequence labeling problem (Gao et al., 2018).",
      "startOffset" : 183,
      "endOffset" : 201
    }, {
      "referenceID" : 40,
      "context" : "information is mainly used for learning contextual representations of tokens, rather than modeling the interactions between the target word and its contexts (Zayed et al., 2020).",
      "startOffset" : 157,
      "endOffset" : 177
    }, {
      "referenceID" : 28,
      "context" : "In this paper, we focus on token-level verb metaphor detection, since verb metaphors are of the most frequent type of metaphoric expressions (Shutova and Teufel, 2010a).",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "We present a simple yet effective model — Metaphor-relation BERT (MrBERT), which is adapted from a BERT (Devlin et al., 2019) based state-of-the-art relation learning model (Bal-",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Basic formulations Most neural networks based approaches cast the task as a classification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018).",
      "startOffset" : 120,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "Based on the basic formulations, various approaches have tried to enhance feature representations by using globally trained contextual word embeddings (Gao et al., 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al.",
      "startOffset" : 151,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : ", 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al., 2018; Mao et al., 2019) and Trans-",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : ", 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al., 2018; Mao et al., 2019) and Trans-",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 40,
      "context" : "First, contextual information is mostly used to enhance the representation of the target word, but the interactions between the target word and its contexts are not explicitly modeled (Zayed et al., 2020; Su et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 221
    }, {
      "referenceID" : 33,
      "context" : "First, contextual information is mostly used to enhance the representation of the target word, but the interactions between the target word and its contexts are not explicitly modeled (Zayed et al., 2020; Su et al., 2020).",
      "startOffset" : 184,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "A concurrent work to this work (Choi et al., 2021) adopted a pre-trained contextualized model based late interaction mechanism to compare the basic meaning and the contextual meaning of a word.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 39,
      "context" : "The grammar structures, such as verb arguments, are important for metaphor processing (Wilks, 1978), but is not well incorporated into neural models.",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "The conceptual metaphor theory views metaphors as transferring knowledge from a familiar, or concrete domain to an unfamiliar, or more abstract domain (Lakoff and Johnson, 1980; Turney et al., 2011).",
      "startOffset" : 151,
      "endOffset" : 198
    }, {
      "referenceID" : 36,
      "context" : "The conceptual metaphor theory views metaphors as transferring knowledge from a familiar, or concrete domain to an unfamiliar, or more abstract domain (Lakoff and Johnson, 1980; Turney et al., 2011).",
      "startOffset" : 151,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "The metaphor identification procedure (MIP) theory (Group, 2007) aims to identify metaphorically used words in discourse based on comparing their use in particular context and their basic meanings.",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "We first use Stanford dependency parser (Chen and Manning, 2014) to parse each sentence and extract verb-subject and verb-direct object relations with VB head and NN dependent.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 31,
      "context" : "VUA dataset We mainly conduct experiments on the VUA (Steen, 2010) dataset.",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 23,
      "context" : "Previous work utilized the training set in different ways (Neidlein et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Table 1: Basic statistics of the preprocessed VUA dataset provided by (Gao et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "MOH-X and TroFi datasets MOH-X (Mohammad et al., 2016) and TroFi (Birke and Sarkar, 2006) are two relatively smaller datasets compared with VUA.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : ", 2016) and TroFi (Birke and Sarkar, 2006) are two relatively smaller datasets compared with VUA.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "(2019) exploit two linguistic theory motivated intuitions based on the basis of (Gao et al., 2018).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "(2019) exploit grammatical relations for data augmentation to enhance (Gao et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "The results of DeepMet and MelBERT are based on the single model evaluation in (Choi et al., 2021).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "Except for (Gao et al., 2018)-CLS, all methods use the annotation information of all tokens.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "BERT-SEQ here refers to the re-trained baseline with model selection based on the performance on the development set, and surpasses the reported results in (Neidlein et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "This observation is consistent with the conclusion of (Choi et al., 2021).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : "Metaphor detection is a key task in metaphor processing (Veale et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "The early methods were based on rules (Fass, 1991; Narayanan, 1997),",
      "startOffset" : 38,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "The early methods were based on rules (Fass, 1991; Narayanan, 1997),",
      "startOffset" : 38,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "Grammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016).",
      "startOffset" : 190,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : ", 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : ", 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "Visual distributed representations were also proven to be useful (Shutova et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Token-level detection Another line of work formulates metaphor detection as a single token classification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018; Mao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 199
    }, {
      "referenceID" : 19,
      "context" : "Token-level detection Another line of work formulates metaphor detection as a single token classification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018; Mao et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 199
    }, {
      "referenceID" : 31,
      "context" : "These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010).",
      "startOffset" : 101,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "Recently, Transformer based pre-trained language models become the most popular architecture in the metaphor detection shared task (Leong et al., 2020).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al.",
      "startOffset" : 19,
      "endOffset" : 100
    }, {
      "referenceID" : 25,
      "context" : "Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al.",
      "startOffset" : 19,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al.",
      "startOffset" : 19,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al.",
      "startOffset" : 19,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : ", 2020) and discourse context (Dankers et al., 2020) have been exploited as well.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 40,
      "context" : "In contrast, token-level metaphor detection explores wider context and gains improvements, but syntactic information is neglected and as discussed in (Zayed et al., 2020), interactions between metaphor components are not explicitly modeled.",
      "startOffset" : 150,
      "endOffset" : 170
    } ],
    "year" : 2021,
    "abstractText" : "Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word. Recent neural models achieve progress on verb metaphor detection by viewing it as sequence labeling. In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts. We propose the Metaphor-relation BERT (MrBERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts. We evaluate our method on the VUA, MOH-X and TroFi datasets. Our method gets competitive results compared with state-of-the-art approaches.",
    "creator" : "LaTeX with hyperref"
  }
}