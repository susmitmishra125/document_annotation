{
  "name" : "2021.acl-long.13.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Transferable Dialogue Systems and User Simulators",
    "authors" : [ "Bo-Hsiang Tseng", "Yinpei Dai", "Florian Kreyssig", "Bill Byrne" ],
    "emails" : [ "bht26@cam.ac.uk", "flk24@cam.ac.uk", "wjb31@cam.ac.uk", "yinpei.dyp@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 152–166\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n152"
    }, {
      "heading" : "1 Introduction",
      "text" : "This work aims to develop a modelling framework in which dialogue systems (DSs) converse with user simulators (USs) about complex topics using natural language. Although the idea of joint learning of two such agents has been proposed before, this paper is the first to successfully train both agents on complex multi-domain human-human dialogues and to demonstrate a capacity for transfer learning to low-resource scenarios without requiring re-redesign or re-training of the models.\nOne of the challenges in task-oriented dialogue modelling is to obtain adequate and relevant training data. A practical approach in moving to a new domain is via transfer learning, where pre-\ntraining on a general domain with rich data is first performed and then fine-tuning the model on the target domain. End-to-end DS (Wen et al., 2017; Li et al., 2017; Dhingra et al., 2017) are particularly suitable for transfer learning, in that such models are optimised as a single system. By comparison, pipe-lined based DSs with multiple individual components (Young et al., 2013) require fine-tuning of each component system. These separate steps can be done independently, but it becomes difficult to ensure optimality of the overall system.\nA similar problem arises in the data-driven US as commonly used in interaction with the DS. Though many USs have been proposed and been widely studied, they usually operate at the level of semantic representation (Kreyssig et al., 2018; El Asri et al., 2016). These models can capture user intent, but are otherwise somewhat artificial as user simulators in that they do not consume and produce natural language. As discussed above for DSs, the end-to-end architecture for the US also offers simplicity in transfer learning across domains.\nThere are also potential advantages to continued joint training of the DS and the US. If a user model is less than perfectly optimised after supervised learning over a fixed training corpus, further learning through interaction between the two agents offers the US the opportunity to refine its behavior. Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents. Takanobu et al. (2020) address refinement of the dialogue policy alone at the semantic level, but do not address end-to-end system architectures. Liu and Lane (2017b); Papangelis et al. (2019) address single-domain dialogues (Henderson et al., 2014), but not the more realistic and\ncomplex multi-domain dialogues. This paper proposes a novel learning framework for developing dialogue systems that performs Joint Optimisation with a User SimulaTor (JOUST).1 Through the pre-training on complex multi-domain datasets, two agents are able to interact using natural language, and further create more diverse and rich dialogues. Using reinforcement learning (RL) to optimise both agents enables them to depart from known strategies learned from a fixed limited corpus, to explore new, potentially better policies. Importantly, the end-to-end designs in the framework makes it easier for transfer learning of two agents from one domain to another. We also investigate and compare two reward designs within this framework: 1) the common choice of task success at dialogue level; 2) a fine-grained reward that operates at turn level. Results on MultiWOZ dataset (Budzianowski et al., 2018) show that our method is effective in boosting the performance of the DS in complicated multi-domain conversation. To further test our method in more realistic scenarios, we design specific experiments on two low-resource setups that address different aspects of data sparsity. Our contributions can be summarised as follows:\n• Novel contributions in joint optimisation of a fully text-to-text dialogue system with a matched user simulator on complex, multidomain human-human dialogues.\n• Extensive experiments, including exploring different types of reward, showing that our framework with a learnable US boost overall performance and reach new state-of-the-art performance on MultiWOZ.\n• Demonstration that our framework is effective in two transfer learning tasks of practical benefit in low-resources scenarios with in-depth analysis of the source of improvements."
    }, {
      "heading" : "2 Pre-training the Dialogue System and User Simulator",
      "text" : "In our joint learning framework, we first pre-train the DS and US using supervised learning so that two models are able to interact via natural language. This section presents the architectures of\n1The code is released at https://github.com/ andy194673/joust.\ntwo agents, illustrated in Fig. 1, and the objectives used for supervised learning."
    }, {
      "heading" : "2.1 Dialogue system",
      "text" : "Dialogue state tracking (DST) The first task of a DS is to process the dialogue history in order to maintain the belief state which records essential information of the dialogue. A DST model is utilized to predict the set of slot-value pairs which constitute the constraints of the entity for which the user is looking for, e.g. {hotel_area=north, hotel_name=gonville_hotel}.\nThe DST model used here is an encoder-decoder model with attention mechanism (Bahdanau et al., 2015). The set of slot-value pairs is formulated as a slot sequence together with a value sequence. For the tth dialogue turn, the DST model first encodes the dialogue context and the most recent user utterance xust−1 using a bi-directional LSTM (Graves et al., 2005) to obtain hidden statesHenct = {henc1 , ..., hencj , ...}. At the ith decoding step of turn t, the previous decoder hidden state hdeci−1 is used to attend over Henct to obtain the attention vector ai. The decoder takes ai, hdeci−1 and the embedding of the slot token predicted at i− 1 to produce the current hidden state hdeci . The h dec i is then passed through separate affine transforms followed by the softmax function to predict a slot token and value for step i. The final belief state is the aggregation of predicted slot-value pairs of all decoding steps.\nDatabase Query Based on the updated belief state, the system searches the database and retrieves the matched entities. In addition, a one-hot vector of size 3 characterises the result of every query.\nContext Encoding To capture the dialogue flow, a hierarchical LSTM (Serban et al., 2016) encodes the dialogue context from turn to turn throughout the dialogue. At each turn t, the most recent user utterance xust−1 is encoded by an LSTM-based sentence encoder to obtain a sentence embedding eust and hidden states Hust . Another LSTM is used as the context encoder, which encodes eust as well as the output of the context encoder on the user side cust−1 from the previous turn (see Fig. 1). The context encoder produces the next dialogue context state cdst for the downstream dialogue manager.\nPolicy The dialogue manager determines the system dialogue act based on the current state of the dialogue. The system dialogue act is treated as a sequence of tokens in order to handle cases in\nwhich multiple system actions exist in the same turn.The problem is therefore formulated as a sequence generation task using an LSTM. At each decoding step, the inputs to the policy decoder are: 1) the embedding of the act token predicted at the previous step; 2) the previous hidden state; 3) the attention vector obtained by attending over the hidden states of the user utterance Hust using 2) as query; 4) the database retrieval vector; 5) the summarized belief state, which is a binary vector where each entry corresponds to a domain-slot pair. The output space contains all possible act tokens. For better modeling of the dialogue flow, the initialization of the hidden state is set to the context state cdst obtained by the context encoder.\nNatural language generation (NLG) The final task of the DS is to generate the system response, based on the predicted system dialogue act. To generate the word sequence another LSTM is used as the NLG model. At each decoding step, the previous hidden state serves as a query to attend over the hidden states of the policy decoder. The resulting attention vector and the embedding of the previous output word are the inputs to an LSTM whose output is the word sequence with delexicalized tokens. These delexicalized tokens will be replaced by retrieval results to form the final utterance."
    }, {
      "heading" : "2.2 User Simulator",
      "text" : "As in the DS, the proposed US has a dialogue manager, an NLG model and a dialogue context encoder. However, in place of a DST to maintain the belief state, the US maintains an internal goal state to track progress towards satisfying the user goals.\nGoal State The goal state is modelled as a binary vector that summarises the dialogue goal. Each entry of the vector corresponds to a domain-slot pair in the ontology. At the beginning of a dialogue,\ngoal state entries are turned on for all slots that make up the goal. At each dialogue turn, the goal state is updated based on the previous user dialogue act. If a slot appears in the previous dialogue act, either as information from the user or as a request by the US, the corresponding entry is turned off."
    }, {
      "heading" : "Context encoding, Policy & NLG in the US",
      "text" : "These steps follow their implementations in the DS. For context encoding in the US, a sentence encoder first encodes the system response using an LSTM to obtain hidden states Hdst and sentence embedding edst . The context encoder takes e ds t and DS context state cdst as inputs to produce the dialogue context state cust which is passed to the DS at the next turn.\nAlso as in the DS, the policy and the NLG model of the US are based on LSTMs. The input to the policy are goal state, hidden states of the sentence encoder Hdst and context state c us t , to produce the user dialogue act, represented as in the DS as a sequence of tokens. The NLG model takes the hidden states of policy decoder as input to generate the user utterance, which is then lexicalised by replacing delexicalised tokens using the user goal."
    }, {
      "heading" : "2.3 Supervised Learning",
      "text" : "For each dialogue turn, the ground truth dialogue acts and the output word sequences are used as supervision for both the DS and the US. The losses of the policy and the NLG model are the crossentropy losses of the predicted sequence probability p and the ground-truth y:\nL∗pol = |A|∑ i=1 −y∗a,i log p∗a,i\nL∗nlg = |W |∑ i=1 −y∗w,i log p∗w,i\n(1)\nIn the above, * can be either ds or us, referring either to the DS or the US: e.g. pdsa,i is the probability of the system act token at the ith decoding step in a given turn. The ground-truth y contains both word sequences and act sequences with W and A as their lengths.\nThe DST annotations are also used as supervision for the DS. The loss of the DST model is defined as the sum of the cross-entropy losses for slot and value:\nLdsdst = |SV |∑ i=1 −ydss,i log pdss,i − ydsv,i log pdsv,i (2)\nwhere |SV | is the number of slot-value pairs in a turn; i is the decoding step index. pdss,i and p ds v,i are the predictions of slot and value at the ith step. The overall losses for the DS and the US are:\nLds(θds) = Ldsdst + L ds pol + L ds nlg Lus(θus) = Luspol + L us nlg\n(3)\nwhere θds and θus are the parameters of DS and US, respectively. The two agents are updated jointly to minimize the sum of the losses (Lds+Lus). The success rate of the generated dialogues is used as the stopping criterion for supervised learning."
    }, {
      "heading" : "3 RL Optimisation of the Dialogue System and User Simulator",
      "text" : "After the DS and US models are pre-trained from the corpus using supervised learning, they are finetuned using reinforcement learning (RL) based on the dialogues generated during their interactions. Two reward designs are presented after which the optimisation strategy is given."
    }, {
      "heading" : "3.1 Dialogue-Level Reward",
      "text" : "Following common practice (El Asri et al., 2014; Su et al., 2017; Casanueva et al., 2018; Zhao et al., 2019), the success of the simulated dialogues is used as the reward, which can only be observed at the end of the dialogue. A small penalty is given at each turn to discourage lengthy dialogues. When updating the US jointly with the DS during interaction using RL, the reward is shared between two agents."
    }, {
      "heading" : "3.2 Turn-Level Reward",
      "text" : "While the dialogue-level reward is straight-forward, it only considers the final task success rate of the\ndialogues and neglects the quality of the individual turns. For complex multi-domain dialogues there is a risk that this will make it difficult for the system to learn the relationship between actions and rewards. We thus propose a turn-level reward function that encapsulates the desired behavioural features of fundamental dialogue tasks. The rewards are designed separately for the US and the DS according to their characteristics.\nDS Reward A good DS should learn to refine the search by requesting needs from the user and providing the correct entities, with their attributes, that the user wishes to know. Therefore at the current turn a positive reward is assigned to DS if: 1) it requests slots that it has not requested before; 2) it successfully provides an entity; or 3) is answers correctly all additional attributes requested by the user. Otherwise, a negative reward is given.\nUS Reward A good US should not repeatedly give the same information or request attributes that have already been provided by the DS. Therefore, a positive reward is assigned to the US if: 1) it provides new information about slots; 2) it asks new attributes about a certain entity, or 3) it replies correctly to a request from the DS. Otherwise a penalty is given."
    }, {
      "heading" : "3.3 Optimization",
      "text" : "We apply the Policy Gradient Theorem (Sutton et al., 2000) to the space of (user/system) dialogue acts. In the tth dialogue turn, the reward rdst or r us t is assigned to the two agents at final last step of their generated act sequence. The return for the action at the ith step is R∗i = γ\n|A∗|−ir∗t , where ∗ denotes ds or us, and |A∗| is the length of the act sequence of each agent. γ∈ [0, 1] is a discounting factor. The policy gradient of each turn can then be written as:\n∇θ∗J∗(θ∗) = |A∗|∑ i R∗i∇θ∗ log p∗a,i (4)\nwhere p∗a,i is the probability of the act token at the ith step in the predicted dialogue act sequence. The two agents are updated using Eqn. (4) at each turn within the entire simulated dialogue."
    }, {
      "heading" : "4 Experiments",
      "text" : "Dataset The MultiWOZ 2.0 dataset (Budzianowski et al., 2018) is used for all\nexperiments. It contains 10.4k dialogues with an average of 13.6 turns. Each dialogue can span up to three domains. Compared to previous benchmark corpora such as DSTC2 (Williams et al., 2016) or WOZ2.0 (Wen et al., 2017), MultiWOZ is more challenging because 1) its rich ontology contains 39 slots across 7 domains; 2) the DS can take multiple actions in a single turn; 3) the complex dialogue flow makes it difficult to hand-craft a rule-based DS or an agenda-based US. Lee et al. (2019) provided the user act labels.\nTraining Details The positive and negative RL rewards of Sec. 3 are tuned in the range [-5, 5] based on the dev set. The user goals employed for interaction during RL are taken from the training data without synthesizing new goals. Further training details can be found in Appendix A.1.\nEvaluation Metrics The proposed model is evaluated in terms of the inform rate (Info), the success rate (Succ), and BLEU.2 The inform rate measures whether the DS provides the correct entity matching the user goal, while the success rate further requires the system to answer all user questions correctly. Following (Mehri et al., 2019), the combined performance (Comb) is also reported, calculated as 0.5 ∗ (Info + Succ) + BLEU."
    }, {
      "heading" : "4.1 Interaction Quality",
      "text" : "First, it is examined whether the proposed learning framework improves the discourse between dialogue system and user simulator. Several variants of our model are examined: 1) two agents are pre-trained using supervised learning, serving as baseline; 2) RL is used to fine-tune only the DS (RL-DS) or both agents (RL-Joint). In each RL case, we can either use rewards at the dialogue level (dial-R, Sec. 3.1) or rewards at the turn-level (turn-R, Sec. 3.2). The two trained agents interact based on 1k user goals from the test corpus, with the generated dialogues being evaluated using the metrics above.\nFrom Table 1, we can see that the application of RL in our framework improves the success rate by more than 10% (b-e vs. a). This indicates that the DS learns through interaction with the learned US, and the designed rewards, to be better at completing the task successfully. Moreover, the joint\n2For a fair comparison to previously proposed models, the same evaluation script provided by the MultiWOZ organizers https://github.com/budzianowski/multiwoz is used and the official data split for train/dev/test is followed.\noptimisation of both the US and the DS provides dialogues with higher success rate than only optimising the DS (c&e vs. b&d). It shows that the behaviour of the US is realistic enough and diverse enough to interact with the DS, and its behavior can be improved together during RL optimisation. Finally, by comparing two reward designs, the finegrained rewards at the turn level seem to be more effective towards guiding two agents’ interaction (b&c vs. d&e), which is reasonable since they reflect more than simple success rate in terms of the nature of the tasks. Some real, generated dialogues through the interactions are provided in Appendix A.6; we note that after RL, both agents respond to requests more correctly and also learn not to repeat the same information, leading to a more successful and smooth interaction without loops in the dialogue. The corresponding error analysis of each of the agents is provided later in Sec. 4.4.1."
    }, {
      "heading" : "4.2 Benchmark Results",
      "text" : "We conduct experiments on the official test set for comparison to existing end-to-end DSs. The trained DS is used to interact with the fixed test corpus following the same setup of Budzianowski et al. (2018). Results are reported using a predicted belief state (Table 2) and using an oracle belief state (Table 3). In general, we can observe similar performance trends as in Sec. 4.1 with RL optimization\nof our model. Joint learning of two agents using RL with the fine-grained rewards reaches the best combined score and success rate. This implies that the exploration of more dialogue states and actions in the simulated interactions reinforces the behaviors that lead to higher success rate, and that these generalise well to unfamiliar states encountered in the test corpus.\nOur best RL model produces competitive results in Table 2 when using predicted belief state, and can further outperform the previous work in Table 3 when using oracle belief state. Note that we do not leverage the powerful pre-trained transformerbased models like SOLOIST or MinTL-BART model. We found that with RL optimisation, our LSTM-based models can still perform competitively. In terms of DS model structure, the most similar work would be the DAMD model. The performance gain found in comparing \"JOUST Supervised Learning\" to DAMD is partially due to the better performance of our DST model.3\nWe also conduct experiments using only 50% of the training data for supervised learning to verify the efficacy of the proposed method under different amounts of data. As shown in Table 4, it is observed that our method also improves the model upon supervised learning when trained with less data and the improvements are consistent with the complete data scenario."
    }, {
      "heading" : "4.3 Transfer Learning",
      "text" : "In this section, we demonstrate the capability of transfer learning of the proposed framework under two low-resource setups: Domain Adaptation and Single-to-Multiple Domain Transfer. Two finetuning methods are adopted: the straightforward fine-tuning without any constraints (Naive) and\n3In correspondence, the DAMD authors report a DST model with joint accuracy of ca. 35%, while ours is 45%.\nelastic weight consolidation (EWC) (Kirkpatrick et al., 2017). We show that the proposed RL can be further applied to both methods and produces significantly improved results. Here we experiment the best RL variants using turn-level rewards (same as (e) in Table 1).\nDomain Adaptation In these experiments, each of five domains is selected as the target domain. Taking the hotel domain for example, 300 dialogues4 involving the hotel domain are sampled from the training corpus as adaptation data. The rest of the dialogues, not involving the hotel domain, form the source data. Both the DS and the US are first trained on the source data (Source), and then fine-tuned on the limited data of the target domain (Naive, EWC). Afterwards, the pair of agents is trained in interaction using the proposed RL training regime (+RL).\nResults in the form of the combined score are given in Table 5 (corresponding success rates are provided in Appendix A.5). As expected, models pre-trained on source domains obtain low combined scores on target domains. Fine-tuning using Naive or EWC method significantly bootstraps the systems, where the regularization in EWC benefits more for the low-resource training. By applying our proposed framework to the two sets of finetuned models, the performance can be further improved by 7-10% in averaged numbers, with both predicted and oracle belief states. This indicates that through the interaction with the US, the DS is not constrained by having seen only a very limited amount of target domain data, and that it can learn effectively from the simulated dialogues using the simple reward structure (the RL learning curve is presented in Sec. 4.4.3). With a better initialization points such as EWC models, the models can learn from a higher quality interaction and produce better results (EWC+RL vs Naive+RL). On aver-\n4For each domain, 300 dialogues accounts for 10% of all target-domain data. Refer to Appendix A.2 for data statistics.\nage, the final performance obtained by EWC+RL model doubles that of Source model, which demonstrates the efficacy of the proposed method in domain adaptation.\nSingle-to-Multiple Domain Transfer Another transfer learning scenario is investigated where only limited multi-domain data is accessible but sufficient single-domain dialogues are available. This setup is based on a practical fact that singledomain dialogues are often easier to collect than multi-domain ones. All single-domain dialogues in the training set form the source data. For each target multi-domain combination, 100 dialogues5 are sampled as adaptation data. As before, the DS and the US are first pre-trained on the source data and then fine-tuned on the adaptation data. Afterwards, two agents improve themselves through interaction. The models are tested using the multi-domain dialogues of the test corpus.\nResults in the form of the combined score are given in Table 6 (refer to Appendix A.5 for success rates). Although the Source models capture individual domains, they cannot manage the complex flow of multi-domain dialogues and hence produce poor combined scores, with worst results on combinations of three domains. Fine-tuning improves performance significantly, as the systems learn to transition between domains in the multi-domain dialogue flow. Finally, applying our RL optimization further increases the performance by 6-9% on average. This indicates that the dialogue agents can learn more complicated policies through exploring more dialogue states and actions while interacting with user simulator. We analyse the sources of improvements in the following section.\n5There are 6 types of domain combinations in MultiWOZ, as shown in Table 6. For each multi-domain combination, 100 dialogues accounts for 11% of its multi-domain data."
    }, {
      "heading" : "Model H+T R+T A+T A+H+X H+R+X A+R+X Avg.",
      "text" : ""
    }, {
      "heading" : "4.4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.4.1 Error Analysis",
      "text" : "We first examine the behavior of the US and the DS to understand the improved success rate in transfer learning. The models are those of Table 5 and are examined after fine-tuning using Naive method (Naive) and then after reinforcement learning (Naive+RL). For the DS, the rates of missing entities (Miss Ent.) and of wrong answers (Wrong Ans.) are reported. For the US, rates of repetitions of attributes (Rep. Att.) and of missing answers (Miss Ans.) are reported. The results shown in Table 7 are averaged over the five adaptation domains6. We see that with RL optimisation the errors made by the two agents are reduced significantly. Notably, the user model learns not to repeat the information already provided and attempts to answer more of the questions from the dialogue agent. These are the behaviors the reward structure of Sec. 3.2 are intended to encourage, and they lead to more successful interactions in policy learning."
    }, {
      "heading" : "4.4.2 Exploration of States and Actions",
      "text" : "We now investigate whether our framework encourages exploration through increased interaction in transfer learning. We report the number of unique belief states in the training corpus and in the dialogues generated during RL interaction, as well as the unique action sequences per state that each\n6Results for each domain can be found in Appendix A.3.\nagent predicts. As shown in Table 8, the DS encounters more states in interaction with the US and also takes more unique actions in reinforcement learning relative to what it sees in supervised learning. In this way the DS considers additional strategies during the simulated training dialogues, with the opportunity to reach better performance even with only limited supervised data. Detailed results for each adaptation case are provided in Appendix A.4."
    }, {
      "heading" : "4.4.3 RL Learning Curve",
      "text" : "Here we show that the designed reward structure is indeed a useful objective for training. Figure 2 shows learning curves of the model performance and the received (turn-level) rewards during RL training. The two examples are from the domain adaptation experiments in Sec. 4.3, where restaurant (left) and hotel (right) are the target domain. We can see that both the reward value and model performance are consistently improved during RL, and their high correlation verifies the efficacy of the proposed reward design for training task-oriented dialogue systems."
    }, {
      "heading" : "4.5 Human Evaluation",
      "text" : "The human assessment of dialogue quality is performed to confirm the improvements of the proposed methods. 400 dialogues, generated by the two trained agents, are evaluated by 14 human assessors. Each assessor is shown a comparison of two dialogues where one dialogue is generated by\nthe models using supervised learning (SL) and another is generated by the models after RL optimization. Note that here we are evaluating the performance gain during interactions between two agents (Sec. 4.1), instead of the gain in benchmark results by interacting with the static corpus (Sec. 4.2). This is why the baseline is our SL model instead of the existing state-of-the-art systems.\nThe assessor offers judgement regarding: • Which dialogue system completes the task\nmore successfully (DS Success)?\n• Which user simulator behaves more like a real human user (US Human-like)?\n• Which dialogue is more natural, fluent and efficient (Dialogue Flow)?\nThe results with relative win ratio, shown in Table 9, are consistent with the automatic evaluation. With the proposed RL optimisation, the DS is more successful in dialogue completion. More importantly, joint optimisation of the US is found to produce more human-like behavior. The improvement under the two agents leads to a more natural and efficient dialogue flow."
    }, {
      "heading" : "5 Related Work",
      "text" : "In the emerging field of end-to-end DSs, in which all components of a system are trained jointly (Liu and Lane, 2017a; Wen et al., 2017; Lei et al., 2018). RL methods have been used effectively to optimize end-to-end DSs in (Dhingra et al., 2017; Liu et al., 2017; Zhao et al., 2019), although using rule-based USs or a fixed corpus for interaction. Recent works utilise powerful transformers such as GPT-2 (Peng et al., 2020; Hosseini-Asl et al., 2020) or T5 (Lin et al., 2020b) for dialogue modeling and reach stateof-the-art performance; however, the area of having a user simulator involved during training is unexplored. By comparison, this work uses a learned US as the environment for RL. The two agents we propose are able to generate abundant high-quality dialog examples and they can be extended easily to unseen domains. By utilizing an interactive envi-\nronment instead of a fixed corpus, more dialogue strategies are explored and more dialogue states are visited.\nThere have been various approaches to building USs. In the research literature of USs, one line of research is rule-based simulation such as the agenda-based user simulator (ABUS) (Schatzmann and Young, 2009; Li et al., 2016). The ABUS’s structure is such that it has to be re-designed for different tasks, which presents challenges in shifting to new scenarios. Another line of work is datadriven modelling. El Asri et al. (2016) modelled user simulation as a seq2seq task, where the output is a sequence of user dialogue acts the level of semantics. Gur et al. (2018) proposed a variational hierarchical seq2seq framework to introduce more diversity in generating the user dialogue act. Kreyssig et al. (2018) introduced the Neural User Simulator (NUS), a seq2seq model that learns the user behaviour entirely from a corpus, generates natural language instead of dialogue acts and possesses an explicit goal representation. The NUS outperformed the ABUS on several metrics. Kreyssig (2018) also compared the NUS and ABUS to a combination of the ABUS with an NLG component. However, none of these prior works are suitable for modelling complex, multi-domain dialogues in an end-to-end fashion. By contrast, the user model proposed here consumes and generates text and so can be directly employed to interact with the DS, communicating via natural language.\nThe literature on joint optimization of the DS and the US is line of research most relevant to our work. Takanobu et al. (2020) proposed a hybrid value network using MARL (Lowe et al., 2017) with roleaware reward decomposition used in optimising the dialogue manager. However, their model requires separate NLU/NLG models to interact via natural language, which hinders its application in the transfer learning to new domains. Liu and Lane (2017b); Papangelis et al. (2019) learn both the DS and the US in a (partially) end-to-end manner. However, their systems are designed for the single-domain dataset (DSTC2) and cannot handle the complexity of multi-domain dialogues: 1) their models can only predict one dialogue act per turn, which is not sophisticated enough for modelling multiple concurrent dialogue acts; 2) the simple DST components cannot achieve satisfactory performance in the multi-domain setup; 3) the user goal change is not modelled along the dialogue proceeds, which\nwe found in our experiments very important for learning complex behaviors of user simulators. Relative to these three publications, this paper focuses on joint training of two fully end-to-end agents that are able to participate in complex multi-domain dialogues. More importantly, it is shown that the proposed framework is highly effective for transfer learning, which is a novel contribution relative to previous work."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We propose a novel joint learning framework of training both the DS and the US for complex multidomain dialogues. Under the low-resource scenarios, the two agents can generate more dialogue data through interacting with each other and their behaviors can be significantly improved using RL through this self-play strategy. Two types of reward are investigated and the turn-level reward benefits more due to its fine-grained structure. Experiments shows that our framework outperforms previously published results on the MultiWOZ dataset. In two transfer learning setups, our method can further improves the well-performed EWC models and bootstraps the final performance largely. Future work will focus on improving the two agents’ underlying capability with the powerful transformer-based models."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Bo-Hsiang Tseng is supported by Cambridge Trust and the Ministry of Education, Taiwan. Florian Kreyssig is funded by an EPSRC Doctoral Training Partnership Award. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (http://www.hpc.cam.ac.uk) funded by EPSRC Tier-2 capital grant EP/P020259/1."
    }, {
      "heading" : "A Appendices",
      "text" : ""
    }, {
      "heading" : "A.1 Training Details",
      "text" : "Both the DS and the US are trained in an end-to-end fashion using the Adam optimizer. The sizes of the embedding and of the hidden layers are set to 300. During supervised training, the batch size is 100 and the learning rate is 0.001, while during RL, 10 is used as the batch size and 0.0001 as the learning rate for stability. We set the discounting factor γ to 1. The computing infrastructure used is Linux 4.4.0- 138-generic x86_64 with the NVIDIA GPU GTX1080. Average run time per model using 100% training data is around 6 hours. Model parameters is around 11M in total.\nThe turn-level rewards used for the best models in benchmark results are reported in Table 10 below. All rewards are tuned based on the combined score of the validation performance averaged over three seeds. As for dialogue-level rewards, a positive reward 1.0 will be given if a dialogue is successful."
    }, {
      "heading" : "A.2 Details of Dataset",
      "text" : "As noted in the paper, we follow the original split of the MultiWOZ dataset and the number of dialogues for train/dev/test split is 8420/1000/1000. Data statistics of the number of dialogues in the two transfer learning scenarios are provided in Tables 11 and 12."
    }, {
      "heading" : "A.3 Error Analysis",
      "text" : "The error analysis of each domain adaptation cases are provided in Tables 13 and 14."
    }, {
      "heading" : "A.4 Exploration",
      "text" : "The detailed numbers of explored dialogue states and the average of unique dialogue actions per state in each case of two transfer learning scenarios are provided in Tables 15 and 16."
    }, {
      "heading" : "Model H+T R+T A+T A+H+X H+R+X A+R+X Average",
      "text" : ""
    }, {
      "heading" : "Model Restaurant Hotel Attraction Train Taxi Average",
      "text" : ""
    }, {
      "heading" : "A.5 Transfer Learning",
      "text" : "Here we provide the results in success rate in two transfer learning setups."
    }, {
      "heading" : "Model Restaurant Hotel Attraction Train Taxi Avg.",
      "text" : ""
    }, {
      "heading" : "Model H+T R+T A+T A+H+X H+R+X A+R+X Avg.",
      "text" : ""
    }, {
      "heading" : "A.6 Generated Dialogue Examples",
      "text" : "Here we provides two dialogue pairs examples that are generated by the two agents’ interaction using supervised learning (SL) and using RL respectively in the following pages. As seen in the Table 19 first dialogue, since the SL user model answers wrong to the system’s request, the system keeps asking the unsolved question, leading to an unsuccessful dialogue with loop (highlighted in color). On the other hand, the interaction between RL models (second dialogue) is much smoother and efficient since both agents answer correctly to each other. This indicates the designed rewards are useful to improve the agents’ behaviors. Similar trend can be observed in Table 20, where the user model keeps stating the same information about restaurant."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Iñigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gasic." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Feudal dialogue management with jointly learned feature extractors",
      "author" : [ "Iñigo Casanueva", "Paweł Budzianowski", "Stefan Ultes", "Florian Kreyssig", "Bo-Hsiang Tseng", "Yen-Chen Wu", "Milica Gašić." ],
      "venue" : "Proceedings of the 19th Annual SIGdial Meeting on Dis-",
      "citeRegEx" : "Casanueva et al\\.,? 2018",
      "shortCiteRegEx" : "Casanueva et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards end-to-end reinforcement learning of dialogue agents for information access",
      "author" : [ "Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "A sequence-to-sequence model for user simulation in spoken dialogue systems",
      "author" : [ "Layla El Asri", "Jing He", "Kaheer Suleman." ],
      "venue" : "Proceedings of the 17th Annual Conference of the International Speech Communication Association, San Francisco.",
      "citeRegEx" : "Asri et al\\.,? 2016",
      "shortCiteRegEx" : "Asri et al\\.",
      "year" : 2016
    }, {
      "title" : "Task completion transfer learning for reward inference",
      "author" : [ "Layla El Asri", "Romain Laroche", "Olivier Pietquin." ],
      "venue" : "Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Asri et al\\.,? 2014",
      "shortCiteRegEx" : "Asri et al\\.",
      "year" : 2014
    }, {
      "title" : "Paraphrase augmented task-oriented dialog generation",
      "author" : [ "Silin Gao", "Yichi Zhang", "Zhijian Ou", "Zhou Yu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber." ],
      "venue" : "International Conference on Artificial Neural Networks, pages 799–804. Springer.",
      "citeRegEx" : "Graves et al\\.,? 2005",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "User modeling for task oriented dialogues",
      "author" : [ "Izzeddin Gur", "Dilek Z. Hakkani-Tür", "Gökhan Tür", "Pararth Shah." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages 900–906.",
      "citeRegEx" : "Gur et al\\.,? 2018",
      "shortCiteRegEx" : "Gur et al\\.",
      "year" : 2018
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D. Williams." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263–272, Philadelphia,",
      "citeRegEx" : "Henderson et al\\.,? 2014",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "A simple language model for task-oriented dialogue",
      "author" : [ "Ehsan Hosseini-Asl", "Bryan McCann", "Chien-Sheng Wu", "Semih Yavuz", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:2005.00796.",
      "citeRegEx" : "Hosseini.Asl et al\\.,? 2020",
      "shortCiteRegEx" : "Hosseini.Asl et al\\.",
      "year" : 2020
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep learning for user simulation in a dialogue system",
      "author" : [ "Florian Kreyssig." ],
      "venue" : "Master’s thesis, University of Cambridge, June.",
      "citeRegEx" : "Kreyssig.,? 2018",
      "shortCiteRegEx" : "Kreyssig.",
      "year" : 2018
    }, {
      "title" : "Neural user simulation for corpus-based policy optimisation of spoken dialogue systems",
      "author" : [ "Florian Kreyssig", "Iñigo Casanueva", "Paweł Budzianowski", "Milica Gašić." ],
      "venue" : "Proc. SIGDIAL, Melbourne.",
      "citeRegEx" : "Kreyssig et al\\.,? 2018",
      "shortCiteRegEx" : "Kreyssig et al\\.",
      "year" : 2018
    }, {
      "title" : "ConvLab: Multi-domain end-to-end dialog system platform",
      "author" : [ "Sungjin Lee", "Qi Zhu", "Ryuichi Takanobu", "Zheng Zhang", "Yaoqin Zhang", "Xiang Li", "Jinchao Li", "Baolin Peng", "Xiujun Li", "Minlie Huang", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meet-",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end taskcompletion neural dialogue systems",
      "author" : [ "Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "A user simulator for task-completion dialogues",
      "author" : [ "Xiujun Li", "Zachary Chase Lipton", "Bhuwan Dhingra", "Lihong Li", "Jianfeng Gao", "Yun-Nung Chen." ],
      "venue" : "ArXiv, abs/1612.05688.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "MinTL: Minimalist transfer learning for task-oriented dialogue systems",
      "author" : [ "Zhaojiang Lin", "Andrea Madotto", "Genta Indra Winata", "Pascale Fung." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lin et al\\.,? 2020a",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Mintl: Minimalist transfer learning for task-oriented dialogue systems",
      "author" : [ "Zhaojiang Lin", "Andrea Madotto", "Genta Indra Winata", "Pascale Fung." ],
      "venue" : "In",
      "citeRegEx" : "Lin et al\\.,? 2020b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "An end-to-end trainable neural network model with belief tracking for taskoriented dialog",
      "author" : [ "Bing Liu", "Ian Lane." ],
      "venue" : "Interspeech 2017.",
      "citeRegEx" : "Liu and Lane.,? 2017a",
      "shortCiteRegEx" : "Liu and Lane.",
      "year" : 2017
    }, {
      "title" : "Iterative policy learning in end-to-end trainable task-oriented neural dialog models",
      "author" : [ "Bing Liu", "Ian Lane." ],
      "venue" : "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 482– 489.",
      "citeRegEx" : "Liu and Lane.,? 2017b",
      "shortCiteRegEx" : "Liu and Lane.",
      "year" : 2017
    }, {
      "title" : "End-to-end optimization of task-oriented dialogue model with deep reinforcement learning",
      "author" : [ "Bing Liu", "Gokhan Tur", "Dilek Hakkani-Tur", "Pararth Shah", "Larry Heck." ],
      "venue" : "NIPS Workshop on Conversational AI.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Multiagent actor-critic for mixed cooperative-competitive environments",
      "author" : [ "Ryan Lowe", "YI WU", "Aviv Tamar", "Jean Harb", "OpenAI Pieter Abbeel", "Igor Mordatch." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and",
      "citeRegEx" : "Lowe et al\\.,? 2017",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "Structured fusion networks for dialog",
      "author" : [ "Shikib Mehri", "Tejas Srinivasan", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 165–177, Stockholm, Sweden. Association for Computational Linguistics.",
      "citeRegEx" : "Mehri et al\\.,? 2019",
      "shortCiteRegEx" : "Mehri et al\\.",
      "year" : 2019
    }, {
      "title" : "Collaborative multi-agent dialogue model training via reinforcement learning",
      "author" : [ "Alexandros Papangelis", "Yi-Chia Wang", "Piero Molino", "Gokhan Tur." ],
      "venue" : "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 92–102, Stockholm,",
      "citeRegEx" : "Papangelis et al\\.,? 2019",
      "shortCiteRegEx" : "Papangelis et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrospective and prospective mixture-of-generators for task-oriented dialogue response generation",
      "author" : [ "Jiahuan Pei", "Pengjie Ren", "Christof Monz", "Maarten de Rijke." ],
      "venue" : "ECAI.",
      "citeRegEx" : "Pei et al\\.,? 2020",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2020
    }, {
      "title" : "Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model",
      "author" : [ "Baolin Peng", "Chunyuan Li", "Jinchao Li", "Shahin Shayandeh", "Lars Liden", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2005.05298.",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Teacherstudent framework enhanced multi-domain dialogue generation",
      "author" : [ "Shuke Peng", "Xinjing Huang", "Zehao Lin", "Feng Ji", "Haiqing Chen", "Yin Zhang." ],
      "venue" : "arXiv preprint arXiv:1908.07137.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "The hidden agenda user simulation model",
      "author" : [ "Jost Schatzmann", "Steve J. Young." ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, 17:733–747.",
      "citeRegEx" : "Schatzmann and Young.,? 2009",
      "shortCiteRegEx" : "Schatzmann and Young.",
      "year" : 2009
    }, {
      "title" : "Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management",
      "author" : [ "Pei-Hao Su", "Paweł Budzianowski", "Stefan Ultes", "Milica Gašić", "Steve Young." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and",
      "citeRegEx" : "Su et al\\.,? 2017",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2017
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutton et al\\.,? 2000",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Multi-agent task-oriented dialog policy learning with role-aware reward decomposition",
      "author" : [ "Ryuichi Takanobu", "Runze Liang", "Minlie Huang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Takanobu et al\\.,? 2020",
      "shortCiteRegEx" : "Takanobu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-domain dialogue acts and response co-generation",
      "author" : [ "Kai Wang", "Junfeng Tian", "Rui Wang", "Xiaojun Quan", "Jianxing Yu." ],
      "venue" : "arXiv preprint arXiv:2004.12363.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A networkbased end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrkšić", "Milica Gasic", "Lina M. Rojas Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young." ],
      "venue" : "EACL, pages 438–449, Valencia, Spain.",
      "citeRegEx" : "Wen et al\\.,? 2017",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "The dialog state tracking challenge series: A review",
      "author" : [ "Jason D. Williams", "Antoine Raux", "Matthew Henderson." ],
      "venue" : "Dialogue Discourse, 7:4–33.",
      "citeRegEx" : "Williams et al\\.,? 2016",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2016
    }, {
      "title" : "Alternating recurrent dialog model with largescale pre-trained language models",
      "author" : [ "Qingyang Wu", "Yichi Zhang", "Yu Li", "Zhou Yu." ],
      "venue" : "arXiv preprint arXiv:1910.03756.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pomdp-based statistical spoken dialog systems: A review",
      "author" : [ "S. Young", "M. Gašić", "B. Thomson", "J.D. Williams." ],
      "venue" : "Proceedings of the IEEE, 101(5):1160–1179.",
      "citeRegEx" : "Young et al\\.,? 2013",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Taskoriented dialog systems that consider multiple appropriate responses under the same context",
      "author" : [ "Yichi Zhang", "Zhijian Ou", "Zhou Yu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models",
      "author" : [ "Tiancheng Zhao", "Kaige Xie", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "End-to-end DS (Wen et al., 2017; Li et al., 2017; Dhingra et al., 2017) are particularly suitable for transfer learning, in that such models are optimised as a single system.",
      "startOffset" : 14,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "End-to-end DS (Wen et al., 2017; Li et al., 2017; Dhingra et al., 2017) are particularly suitable for transfer learning, in that such models are optimised as a single system.",
      "startOffset" : 14,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "End-to-end DS (Wen et al., 2017; Li et al., 2017; Dhingra et al., 2017) are particularly suitable for transfer learning, in that such models are optimised as a single system.",
      "startOffset" : 14,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : "By comparison, pipe-lined based DSs with multiple individual components (Young et al., 2013) require fine-tuning of each component system.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "Though many USs have been proposed and been widely studied, they usually operate at the level of semantic representation (Kreyssig et al., 2018; El Asri et al., 2016).",
      "startOffset" : 121,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents.",
      "startOffset" : 123,
      "endOffset" : 192
    }, {
      "referenceID" : 25,
      "context" : "Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents.",
      "startOffset" : 123,
      "endOffset" : 192
    }, {
      "referenceID" : 32,
      "context" : "Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents.",
      "startOffset" : 123,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "(2019) address single-domain dialogues (Henderson et al., 2014), but not the more realistic and",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "The DST model used here is an encoder-decoder model with attention mechanism (Bahdanau et al., 2015).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "For the tth dialogue turn, the DST model first encodes the dialogue context and the most recent user utterance xus t−1 using a bi-directional LSTM (Graves et al., 2005) to obtain hidden statesHenc t = {henc 1 , .",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 30,
      "context" : "Following common practice (El Asri et al., 2014; Su et al., 2017; Casanueva et al., 2018; Zhao et al., 2019), the success of the simulated dialogues is used as the reward, which can only be observed at the end of the dialogue.",
      "startOffset" : 26,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Following common practice (El Asri et al., 2014; Su et al., 2017; Casanueva et al., 2018; Zhao et al., 2019), the success of the simulated dialogues is used as the reward, which can only be observed at the end of the dialogue.",
      "startOffset" : 26,
      "endOffset" : 108
    }, {
      "referenceID" : 39,
      "context" : "Following common practice (El Asri et al., 2014; Su et al., 2017; Casanueva et al., 2018; Zhao et al., 2019), the success of the simulated dialogues is used as the reward, which can only be observed at the end of the dialogue.",
      "startOffset" : 26,
      "endOffset" : 108
    }, {
      "referenceID" : 31,
      "context" : "We apply the Policy Gradient Theorem (Sutton et al., 2000) to the space of (user/system) dialogue acts.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : "Compared to previous benchmark corpora such as DSTC2 (Williams et al., 2016) or WOZ2.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 34,
      "context" : "0 (Wen et al., 2017), MultiWOZ is more challenging because 1) its rich ontology contains 39 slots across 7 domains; 2) the DS can take multiple actions in a single turn; 3) the complex dialogue flow makes it difficult to hand-craft a rule-based DS or an agenda-based US.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "Following (Mehri et al., 2019), the combined performance (Comb) is also reported, calculated as 0.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "157 Model Info Succ BLEU Comb SimpleTOD∗(Hosseini-Asl et al., 2020) 88.",
      "startOffset" : 40,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "elastic weight consolidation (EWC) (Kirkpatrick et al., 2017).",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "In the emerging field of end-to-end DSs, in which all components of a system are trained jointly (Liu and Lane, 2017a; Wen et al., 2017; Lei et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 154
    }, {
      "referenceID" : 34,
      "context" : "In the emerging field of end-to-end DSs, in which all components of a system are trained jointly (Liu and Lane, 2017a; Wen et al., 2017; Lei et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "In the emerging field of end-to-end DSs, in which all components of a system are trained jointly (Liu and Lane, 2017a; Wen et al., 2017; Lei et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "RL methods have been used effectively to optimize end-to-end DSs in (Dhingra et al., 2017; Liu et al., 2017; Zhao et al., 2019), although using rule-based USs or a fixed corpus for interaction.",
      "startOffset" : 68,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "RL methods have been used effectively to optimize end-to-end DSs in (Dhingra et al., 2017; Liu et al., 2017; Zhao et al., 2019), although using rule-based USs or a fixed corpus for interaction.",
      "startOffset" : 68,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "RL methods have been used effectively to optimize end-to-end DSs in (Dhingra et al., 2017; Liu et al., 2017; Zhao et al., 2019), although using rule-based USs or a fixed corpus for interaction.",
      "startOffset" : 68,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "Recent works utilise powerful transformers such as GPT-2 (Peng et al., 2020; Hosseini-Asl et al., 2020) or T5 (Lin et al.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Recent works utilise powerful transformers such as GPT-2 (Peng et al., 2020; Hosseini-Asl et al., 2020) or T5 (Lin et al.",
      "startOffset" : 57,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : ", 2020) or T5 (Lin et al., 2020b) for dialogue modeling and reach stateof-the-art performance; however, the area of having a user simulator involved during training is unexplored.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : "In the research literature of USs, one line of research is rule-based simulation such as the agenda-based user simulator (ABUS) (Schatzmann and Young, 2009; Li et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "In the research literature of USs, one line of research is rule-based simulation such as the agenda-based user simulator (ABUS) (Schatzmann and Young, 2009; Li et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "(2020) proposed a hybrid value network using MARL (Lowe et al., 2017) with roleaware reward decomposition used in optimising the dialogue manager.",
      "startOffset" : 50,
      "endOffset" : 69
    } ],
    "year" : 2021,
    "abstractText" : "One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets.",
    "creator" : "LaTeX with hyperref"
  }
}