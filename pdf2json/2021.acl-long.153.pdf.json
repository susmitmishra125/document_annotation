{
  "name" : "2021.acl-long.153.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Crafting Adversarial Examples for Neural Machine Translation",
    "authors" : [ "Xinze Zhang", "Junzhe Zhang", "Zhenhua Chen", "Kun He" ],
    "emails" : [ "xinze@hust.edu.cn", "junzhezhang@hust.edu.cn", "zhenhuachen@hust.edu.cn", "brooklet60@hust.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1967–1977\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1967"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent studies have revealed that neural machine translation (NMT), which has achieved remarkable progress in advancing the quality of machine translation, is fragile when attacked by some crafted perturbations (Belinkov and Bisk, 2018; Cheng et al., 2019, 2020; Wallace et al., 2020). Even if the perturbations on inputs are small and imperceptible to humans, the translation quality could be degraded\n∗The four authors contributed equally. †Corresponding author: Kun He.\ndramatically, raising increasing attention to adversarial defenses for building robust machine translation systems as well as its prerequisite researches on building effective NMT adversarial attacks. As character level perturbations usually lead to lexical errors and are easily corrected by spell checking tools (Ren et al., 2019; Zou et al., 2020), in this work, we focus on crafting word level adversarial examples that could maintain lexical and grammatical correctness and hence are more realistic.\nAn essential issue of crafting NMT adversarial examples is how to define “what is an effective NMT adversarial attack”. Researchers have provided an intuitive definition that an NMT adversarial example should preserve the semantic meaning on the source but destroy the translation performance with respect to the reference translation (Michel et al., 2019; Niu et al., 2020). Correspondingly, the attack criteria are proposed as the absolute degradation or relative degradation against the reference translation (Ebrahimi et al., 2018; Michel et al., 2019; Niu et al., 2020; Zou et al., 2020). To craft a perturbation that maintains the semantics as well as grammatical correctness following the above definition and evaluation, a variety of methods to impose word replacements have been proposed in recent studies (Michel et al., 2019; Cheng et al., 2019, 2020; Zou et al., 2020), making it a commonly used paradigm for NMT attacks.\nHowever, there exist potential pitfalls overlooked in existing researches. First, it is possible to craft an effective attack on the NMT models by reversing the semantics on the source, as illustrated in\nTable 11. Meanwhile, since the antonyms are potentially in the neighborhood of the victim word in the embedding space, just as the same as the synonyms, it is entirely possible to produce opposing semantics when replacing a word with its neighbors, making the proposed attack method break the definition.\nFurthermore, there is a risk of evaluating the attacks directly using the reference translation. Differs to the classification tasks, even if the perturbation is small to be synonymous with the original word in the source, the actual ground-truth reference may be changed due to the substitution. Table 2 illustrates a typical failing adversarial example x′× and a successful example x\n′√, where x′× could be falsely distinguished as effective due to the missing of ground-truth reference Ref. ′×\n2. Obviously, x′ would be correctly distinguished if we have the actual ground-truth reference of x′. However, the actual ground-truth reference of the perturbed input is notoriously difficult to be built beforehand, making the NMT attack hardly to be evaluated veritably.\nIn this work, in order to craft appropriate NMT adversarial examples, we introduce new definition\n1This is a real case reported on Google translation community in October, 2020. See details in: https://support.google.com/translate/thread/78771708?hl=en.\n2BLEU(ref,y) = 39.20 → BLEU(ref,y′×) = 2.86, BLEU(x,x′×) = 61.34 → BLEU(y,y′×) = 49.83.\nand metrics for the machine translation adversaries by leveraging the round-trip translation, the process of translating text from the source to target language and translating the result back into the source language. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the input and degrades the translation dramatically, would naturally lead to a semantic destroying round-trip translation result. Based on our new definition and metrics, we propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures, e.g. RNN and Transformer.\nOur main contributions are as follows:\n• We introduce an appropriate definition of NMT adversary and the deriving evaluation metrics, which are capable of estimating the adversaries only using source information, and tackle well the challenge of missing ground-truth reference after the perturbation.\n• We propose a novel black-box word level NMT attack method that could effectively attack the mainstream NMT models, and exhibit high transferability when attacking popular online translators."
    }, {
      "heading" : "2 NMT Adversary Generation",
      "text" : "Let X denote the source language space consisting of all possible source sentences and Y denote the target language space. Given two NMT models, the primal source-to-target NMT model Mx→y aims to\nlearn a forward mapping f : X → Y to maximize P (yref |x) where x ∈ X and yref ∈ Y , while the dual target-to-source NMT model My→x aims to learn the backward mapping g : Y → X . After the training, NMT can correctly reconstruct the source sentence x̂ = g(f(x)). In the following, we first give the definition of NMT adversarial examples, then introduce our word substitution based blackbox adversarial attack method."
    }, {
      "heading" : "2.1 Definition on NMT Adversarial Examples",
      "text" : "Given a subset of (test) sentences T ∈ X and a small constant , we summarize previous works (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019) and give their conception of NMT adversarial examples as follows.\nDefinition 1 (NMT Adversarial Example). An NMT adversarial example is a sentence in A = {x′ ∈ X |∃x ∈ T , ‖x′ − x‖ < ∧\nSt(y, yref ) ≥ γ ∧ St(y′, yref ) < γ′}, where y = f(x), y′ = f(x′), and St(·, ·) is a metric for evaluating the similarity of two sentences, and γ (or γ′, γ′ < γ) is threshold we can accept (or refuse) for the translation quality .\nA smaller γ′ indicates a more strict definition of the NMT adversarial example.\nIn contrast to the adversarial examples in image domain (Szegedy et al., 2014), we argue that taking yref as the reference sentence for x′ is not appropriate because the perturbation might change the semantic of x to some extent, causing that Definition 1 is not appropriate. To address this problem, we propose to evaluate the similarity between the benign sentence x and the reconstructed sentence x̂, as well as the similarity between the adversarial sentence x′ and the reconstructed adversarial sentence x̂′. We introduce a new definition of NMT adversarial example basing on the round-trip translation.\nDefinition 2 (NMT adversarial example). An NMT adversarial example is a sentence in A = {x′ ∈ X |∃x ∈ T , ‖x′ − x‖ < ∧ St(y, yref ) ≥ γ ∧ St(x, x̂) ≥ δ ∧ E(x, x′) ≥ α}, where E(x, x′) = St(x, x̂)− St(x′, x̂′) is defined as the adversarial effect for NMT. And, the reconstructed x̂ and x̂′ are generated with round-trip translation: x̂ = g(f(x)), x̂′ = g(f(x′)).\nA larger E indicates that the generated sentence x′ can not be well reconstructed by round-trip translation when compared with the reconstruction quality of the source sentence x. Here α is a threshold\nranging in [0, 1] to determine whether x′ is an NMT adversarial example. A larger α indicates a more strict definition of the NMT adversarial example. In this work, we use the BLEU score (Papineni et al., 2002) to evaluate the similarity between two sentences.\nBased on Definition 2, we further provide two metrics, i.e., Mean Decrease (MD) and Mean Percentage Decrease (MPD) to estimate the translation adversaries appropriately. MD directly presents the average degradation of the reconstruction quality, and MPD reduces the bias of the original quality in terms of the relative degradation. The proposed MD is defined as:\nMD = 1\nN N∑ i Di, (1)\nwhere N is the number of victim sentences, Di is the decreasing reconstruction quality of the adversarial example x′i, denoted as:\nDi = { 0 if St(xi, x̂i) = 0, St(xi, x̂i)− St(x′i, x̂′i) otherwise.\n(2) Similarly, MPD is defined as:\nMPD = 1\nN N∑ i PDi, (3)\nwhere PDi is denoted as:\nPDi = { 0 if St(xi, x̂i) = 0, St(xi,x̂i)−St(x′i,x̂′i)\nSt(xi,x̂i) otherwise.\n(4)\nIn practice, except for the constraints in Definition 2, adversarial examples should also satisfy the lexical and syntactical constraints so that they are hard for human to perceive. Therefore, the correct word in the source sentence must be replaced with other correct words instead of misspelled word to meet the lexical constraint. Besides, to keep the grammatical correctness and syntax consistency, the modification should not change the syntactic relation of each word in the source sentence.\nTo meet all the above constraints, we propose a novel NMT adversarial attack method by substituting words with their neighbors selected from the parser filter to generate reasonable and effective adversarial examples."
    }, {
      "heading" : "2.2 WSLS Attack",
      "text" : "There are two phases in the proposed Word Saliency speedup Local Search (WSLS) attack\nmethod. At the first phase, we design initial strategies to obtain an initial example x′. At the second phase, we present a local search algorithm accelerated by word saliency to optimize the perturbed example."
    }, {
      "heading" : "2.2.1 Initialization Strategy",
      "text" : "Candidates. For a word wi in the source sentence x = {w1, . . . , wi, . . . , wn}, where i denotes the position of word wi in the sentence, we first build a candidate set Wi ∈ D where D is the dictionary consisting of all the legal words. In this work, we build the candidate set by finding the k closest neighbors in the word embedding space: Wi = {w1i , . . . , wki }. Then we filter the candidates based on the parsing, as shown in Part A of\nFigure 13. Note that the combination of them can impose minor shifting on the source so as to meet the lexical and semantic constraints, as discussed in Section 2.1. In our experiments, we use the pretrained mask language model (MLM) to extract the embedding space to follow the black-box setting.\n3This is important to rule out invalid victim locations wherein the token (e.g., punctuation) is nonsense, and ensure the perturbations keep grammatical correctness.\nGreedy Substitution. For each position i, we can substitute word wi with w j i ∈Wi to obtain an adversary x′ = {w1, . . . , wji , . . . , wn}, and evaluate the adversarial effect E(x, x′) by reconstruction. Then we select a word w∗i that yields the most significant degradation:\nw∗i = arg max wji∈Wi E(x, x′). (5)\nIt is straightforward to generate an initial adversary through a Random Order Greedy Replacement (ROGR) method, which is to randomly select positions expected to make substitutions, then iteratively replace the word with its neighbors by Eq. 5 on the selected positions in a random order.\nHowever, the initial result has a significant impact on the final result of the local search. If the local search phase starts with a near-optimal solution, it is likely to find a more powerful adversary after the local search process. Therefore, we design a greedy algorithm called Greedy Order Greedy Replacement (GOGR) for the initialization, which is depicted in Part B of Figure 1.\nIn the GOGR algorithm, at each step we enumerate all possible positions we haven’t attacked yet, and for each position we try to substitute word wi ∈ x with word w∗i ∈ Wi according to Eq. 5, then we choose the best w∗ among the possible positions, and iteratively substitute words until we substitute enough words.\nw∗ = arg max i∈n max wji∈Wi E(x, x′) (6)"
    }, {
      "heading" : "2.2.2 Word Saliency",
      "text" : "To speed up the local search process, we adopt the word saliency, used for text classification attack, to sort the word positions in which the word has not been replaced yet. In this way, we can skip the positions that may lead to low attack effect so as to speedup the search process.\nFor text classification task, Li et al. (2016) propose the concept of word saliency that refers to the degree of change in the output of text classification model when a word is set to the “unknown” token. Ren et al. (2019) incorporate the word saliency to generate adversarial examples for text classification. To adopt the concept of word saliency for NMT, we regard the output of a MLM for the word as a more general concept of word saliency, which is independent of the specific tasks.\nDefinition 3 (Word Saliency). For a sentence x = {w1, . . . , wi, . . . , wn} and a mask language model (MLM) M , the word saliency of wi is defined as S(x,wi) = 1− P (wi|x̄i,M) where x̄i = {w1, . . . , wi−1,mask, wi+1 . . . , wn} and “mask” means the word is masked in the sentence.\nThrough Definition 3, the higher word saliency represents the lower context-dependent probability, which can be caused by numerous reasonable substitutions or rare syntax structure, indicating weaker word positions that are easier to be attacked.\nIn this work, as shown in Part C of Figure 1, we calculate the word saliency S(x,wi) for all positions before the local search phase, making the local search efficiently inquire the word saliency."
    }, {
      "heading" : "2.2.3 Local Search Strategy",
      "text" : "In the local search phase, as shown in Part D of Figure 1 and detailed in Figure 2, there are three types of walks, namely saliency walk, random walk and certain walk, used to update x′ to promote the attack quality.\nTo explore and exploit the search space, we define some basic operations and walks to evolve the adversaries. A mute operator is to restore an executed perturbation w∗i to its original word wi to mutate the adversary. A prune operator is to exclude a portion of candidate locations where the perturbations will not be imposed to narrow down the search area. A tabu operator indicates that the last perturbed location is forbidden to be manipulated in the current iteration. As illustrated in Figure 2, the three operators are utilized in the local search walks (Part D). We interpret the three walks as follows.\nSaliency Walk. We first design an efficient walk for the search, called the saliency walk (SW), to\nmake a balanced exploration and exploitation in the neighbourhood of the well initialized solution generated by the aforementioned GOGR algorithm. During the saliency walk, as shown in Figure 2a, at the current iteration (t), we mute each perturbed word to generate a set of partial solutions, sorted in the ascending order of the saliency score, so as to give higher priority to the perturbations with higher word saliency on the locations. Then we prune other unperturbed words according to the descending order of the saliency score, and query candidate substitutions for each of the remaining words. Then candidate adversaries, consisting of the concatenation of each partial solution with each candidate substitution, are evaluated by Eq. 2 iteratively.\nTo accelerate the saliency walk, we have an early stop strategy: if the current best adversarial effect in the enumeration of the candidate adversaries at the present iteration (t), denoted as pbest(t) = E∗, is better than pbest(t−1) (the best adversarial effect at the previous iteration (t − 1)), i.e. pbest(t) ≥ pbest(t−1), then we terminate the enumeration of the candidates and pass the state of pbest(t) as well as the tabu operator to the next walk, otherwise the state of pbest(t−1) will be passed to the next walk and the tabu location is expired.\nRandom Walk. To avoid the current adversarial example get trapped in a local optimum, we design an effective mutation walk, called the random walk (RW), to mutate the current solution. During the random walk, as shown in Figure 2b, we randomly mute a perturbed word to generate a partial solution, and query the candidate substitutions for each of the unperturbed words as in saliency walk. Then we concatenate the partial solution with each candidate substitution to build the candidate adver-\nsaries, among which the best solution is used to update pbest(t). After that, the tabu operator will be forcibly passed to the next walk, reinforcing the exploration ability of the WSLS algorithm.\nCertain Walk. To do a sufficient exploitation after the random walk as a mutation, we design the certain walk (CW). As shown in Figure 2c, certain walk is similar to saliency walk but it removes the prune operation to enlarge the neighborhood space.\nTo trade off the efficiency and search time, we adopt one saliency walk followed by random walk, certain walk, random walk and certain walk, to construct one round of local search, denoted as {SW, RW, CW, RW, CW}, as shown in Part D of Figure 1. Besides, we bring an early-stop-finetune mechanism to the WSLS method. For any walk in WSLS, if there exists an adversarial candidate that updates the historically best adversarial effect, this adversarial candidate will be immediately set as the initial solution to start a new local search. Otherwise, the WSLS will stop after the ending of the current round 4."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "We conduct experiments on the Chinese-English (Zh-En), English-German (En-De), and EnglishRussian (En-Ru) translation tasks.\nFor the Zh→En translation task, we use LDC corpus5 consisting of 1.25M sentence pairs, and use NIST (MT) datasets6 to craft the attacks. Following the preprocessing in Zhang et al. (2019), we limit the source and target vocabulary to the most frequent 30K words, remove sentences longer than 50 words from the training data, and use NIST 2002 as the validation set for the model selection. For this translation task, we implement our attacks on two state-of-art word-level NMT models. 1) RNNsearch (Bahdanau et al., 2015) has an encoder consists of forward and backward RNNs each having 1000 hidden units and a decoder with 1000 hidden units. Denote this model as “Rnns.” for abbreviation. 2) Transformer comprises six layers of transformer with 512 hidden units and 8 heads in both encoder and decoder, which mimics the hyperparameters in (Vaswani et al., 2017). Denote this model as “Transf.” for abbreviation. For the or-\n4Code is available at https://github.com/JHL-HUST/ AdvNMT-WSLS/.\n5LDC 2002E18, 2003E14, 2004T08, 2005T06. 6NIST 2002, 2003, 2004, 2005, 2006, 2008.\nacle back-translation (En→Zh), we use a sub-word level transformer as our oracle model which was trained with LDC datasets and then finetuned with the NIST datasets.\nFor the En→De and En→Ru translation tasks, We use WMT19 test sets to craft the adversaries, and implement our attacks on the winner models of the WMT19 En→De and En→Ru sub-tracks7. Specifically, the En→De model and En→Ru model are both subword-level transformer, where a joint byte pair encodings (BPE) with 32K split operations is applied for En→De, and separate BPE encodings with 24K split operations is applied for each language in En→Ru (Ng et al., 2019). We denote these two models as “BPE-Transf.” for abbreviation. For the oracle back-translation (De→En, Ru→En), the best submitted NMT models in WMT19 are used as our oracle models which are further finetuned with 90% of the previous WMT test sets and validated with the remaining sets.\nAs for the reference result, Table 3 and Table 4 show the case-insensitive BLEU scores for forwardtranslation, back-translation, and round-trip translation on the selected language pairs. We observe that the word-level victim models (Rnns. and Transf.) achieve an average BLEU score of 36.71 and 41.55 for Zh→En translation respectively, demonstrating the accuracy of these two models on translating the original Chinese sentences. For the backtranslation, the oracle models achieve an average BLEU score of 82.9 for En→Zh translation, as well as a BLEU score of 54.83 and 57.24 for De→En and Ru→En translations respectively, indicating that the oracle models are reliable enough in the back-translation stage for the source reconstruction. Besides, the reconstruction quality of the victim models are reported in Table 3 and Table 4, where the source sentences are back-translated by the oracle models in the round-trip translation, showing that the source language is reconstructed well enough by the cooperation of forward-translation and oracle back-translation.\nFurthermore, to enhance the authenticity of the attack performance, we removed the noisy data, which could not be correctly identified as the corresponding language sentences by online translators, and we also excluded sentences longer than 50 words in the NIST datasets, ensuring that the attack\n7https://github.com/pytorch/fairseq/tree/master/examples/ translation.\nresults are credible8. As for the parameter settings of the attack methods, we use pyltp9 as the parser checking tool and generate the top 10 nearest parser-filtered words to construct the candidate sets for each word. To generate the word saliency, two state-of-art whole word masking BERT are utilized as the MLM for the Chinese10 and English11 languages respectively. And the prune operators implemented in SW and RW will reserve the highest five word saliency locations and their word candidates. Finally, the adversaries are crafted by substituting 20% words."
    }, {
      "heading" : "3.2 Attack Results",
      "text" : "To demonstrate our proposed WSLS method, we implement AST-lexcial (Cheng et al., 2018) as a black-box baseline, wherein AST-lexcial shares the same idea of random order random replacement. Besides, the naive ROGR method can be considered as another black-box counterpart of the white-box kNN method in Michel et al. (2019) that randomly selects the word positions and greedily selects the neighbor words based on the gradient loss.\n8After the preprocessing, the size of the original NIST datasets are reduced from 878 to 617 (MT02), 919 to 793 (MT03), 1788 to 1495 (MT04), 1082 to 907 (MT05), 1664 to 988 (MT06), and 1357 to 789 (MT08).\n9https://github.com/HIT-SCIR/pyltp. 10https://huggingface.co/hfl/chinese-bert-wwm-ext. 11https://huggingface.co/bert-large-uncased-whole-word-\nmasking.\nAs shown in Table 5 and Table 6, both GOGR and WSLS have the MD scores close to the original reconstruction scores for Rnns., Transf., and BPE-Transf., and their attack results are much better than that of AST-lexical as well as ROGR. It shows that both WSLS and GOGR can effectively attack various NMT models under the standard of Definition 2. WSLS is superior to GOGR, indicating that the local search phase can further promote the attack quality. Specifically, the MPD score of WSLS is almost 1.5 higher than that of GOGR, which is more obvious as compared to the MD metric, revealing the rationality of MPD also."
    }, {
      "heading" : "3.3 Ablation Study",
      "text" : "We do ablation study on the WSLS algorithm in Table 7. Here “Init” is for the method used for initialization, WS indicates whether we use word saliency to speedup the local search, LS indicates whether we use local search or other variants of walk sequence for the local search.\nFrom Table 7 we observe that: 1) The initialization of GOGR exhibits significantly better results than ROGR, and also converges faster than ROGR; 2) WSLS without word saliency speedup, denoted as WSLS1, exhibits slightly higher attack results but the running times are much longer than WSLS. Thus, we choose WSLS to have a good tradeoff on attack quality and time."
    }, {
      "heading" : "3.4 Transferability",
      "text" : "To test the transferability of our method, we transfer our crafted adversarial examples on NIST 2002 dataset to attack the online Baidu and Bing translators. As shown in Table 8, the attack effectiveness is significant. It degrades the reconstruction quality of Baidu and Bing with more than 20 BLEU points, demonstrating the high transferability.\nIn addition, we provide two adversarial examples in Table 9, generated by WSLS on the Rnns. model, that can effectively attack the online Bing\nand Baidu translators, respectively. It demonstrates that WSLS could craft adversarial examples with strong readability and high transferability."
    }, {
      "heading" : "4 Related Work",
      "text" : "In recent years, adversarial examples have attracted increasing attention in the area of natural language processing (NLP), mainly on text classification (Jia and Liang, 2017; Ren et al., 2019; Wang et al., 2021). For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).\nOn the character level, a few adversarial attacks by manipulating character perturbations have been proposed since 2018. Belinkov and Bisk (2018) confront NMT models with synthetic and natural misspelling noises, and show that character-based NMT models are easy to be attacked by character level perturbation. Ebrahimi et al. (2018) propose to attack the character level NMT models by manipulating the character-level insertion, swap and deletion. Similarly, Michel et al. (2019) perform a gradient-based attack that processes words in source sentences to maximize the translation loss. To attack against production MT systems, Wallace et al. (2020) imitate the popular online translators and manipulate the perturbations based on the gradient of the adversarial loss with the imitation models. The above four works also incorporate adversarial training to improve the robustness of NMT.\nHowever, the character level perturbations are hard to be applied into confronting practical NMT models, as these perturbations significantly reduce\nthe readability and also could be easily corrected by spell checkers (Ren et al., 2019; Zou et al., 2020). On the other hand, word level adversaries could maintain lexical and grammatical correctness, which are more realistic but more challenging to generate. Cheng et al. (2018) craft the adversaries with randomly sampled perturbed positions, and then replace the words according to the cosine similarity of the embedding vectors between the original word and the neighbors. Cheng et al. (2019) propose a gradient-based attack method that replaces the original word with the candidates generated by integrated language model. Michel et al. (2019) generate adversaries by substituting the word with its nearest neighbors, which are informed by the gradient of the victim models. (Zou et al., 2020) introduce a reinforced learning based method to craft the attacks following Michel et al. (2019) to define the reward and substitution candidate set.\nExisting word level translation attacks are mainly white-box, wherein the attacker can access all the information of the victim model. Besides, there is a risk of guiding the attacks to directly use the degradation of reference translation, since the actual references may be changed by word substitution. Thus, there exists few study on the effective word level attack for NMT, especially in the black box setting. This study fills this gap and sheds light\non black-box word level NMT attacks."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduce an appropriate definition of adversarial examples as well as the deriving evaluation measures for the adversarial attacks on neural machine translation (NMT) models. Following our definition and metrics, we propose a promising blackbox NMT attack method called the Word Saliency speedup Local Search (WSLS), in which a general definition of word saliency by leveraging the strong representation capability of pre-trained language models is also introduced. Experiments demonstrate that the proposed method could achieve powerful attack performance, that effectively breaks the mainstream RNN and Transformer based NMT models. Further, our method could craft adversaries with strong readability as well as high transferability to the popular online translators."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by National Natural Science Foundation (62076105) and Microsft Research Asia Collaborative Research Fund (99245180). We thank Xiaosen Wang for helpful suggestions on our work."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Synthetic and natural noise both break neural machine Translation",
      "author" : [ "Yonatan Belinkov", "Yonatan Bisk." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Belinkov and Bisk.,? 2018",
      "shortCiteRegEx" : "Belinkov and Bisk.",
      "year" : 2018
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "AdvAug: robust data augmentation for neural machine translation",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards robust neural machine translation",
      "author" : [ "Yong Cheng", "Zhaopeng Tu", "Fandong Meng", "Junjie Zhai", "Yang Liu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Cheng et al\\.,? 2018",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "Marta R Costa-jussà", "José AR Fonollosa." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Costa.jussà and Fonollosa.,? 2016",
      "shortCiteRegEx" : "Costa.jussà and Fonollosa.",
      "year" : 2016
    }, {
      "title" : "Bert: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "On adversarial examples for character-level neural machine translation",
      "author" : [ "Javid Ebrahimi", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics.",
      "citeRegEx" : "Ebrahimi et al\\.,? 2018",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2018
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Visualizing and understanding neural models in nlp",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "On evaluation of adversarial perturbations for sequence-to-sequence models",
      "author" : [ "Paul Michel", "Xian Li", "Graham Neubig", "Juan Pino." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Michel et al\\.,? 2019",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Facebook fair’s wmt19 news translation task submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "Proceedings of the 4th Conference on Machine Translation.",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating robustness to input perturbations for neural machine translation",
      "author" : [ "Xing Niu", "Prashant Mathur", "Georgiana Dinu", "Yaser Al-Onaizan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Niu et al\\.,? 2020",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Generating natural language adversarial examples through probability weighted word saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Imitation attacks and defenses for black-box machine translation systems",
      "author" : [ "Eric Wallace", "Mitchell Stern", "Dawn Song." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Wallace et al\\.,? 2020",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial training with fast gradient projection method against synonym substitution based text attacks",
      "author" : [ "Xiaosen Wang", "Yichen Yang", "Yihe Deng", "Kun He." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "A reinforced generation of adversarial samples for neural machine translation",
      "author" : [ "Wei Zou", "Shujian Huang", "Jun Xie", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Zou et al\\.,? 2020",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recent studies have revealed that neural machine translation (NMT), which has achieved remarkable progress in advancing the quality of machine translation, is fragile when attacked by some crafted perturbations (Belinkov and Bisk, 2018; Cheng et al., 2019, 2020; Wallace et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 284
    }, {
      "referenceID" : 19,
      "context" : "Recent studies have revealed that neural machine translation (NMT), which has achieved remarkable progress in advancing the quality of machine translation, is fragile when attacked by some crafted perturbations (Belinkov and Bisk, 2018; Cheng et al., 2019, 2020; Wallace et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 284
    }, {
      "referenceID" : 16,
      "context" : "As character level perturbations usually lead to lexical errors and are easily corrected by spell checking tools (Ren et al., 2019; Zou et al., 2020), in this work, we focus on crafting word level adversarial examples that could maintain lexical and grammatical correctness and hence are more realistic.",
      "startOffset" : 113,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "As character level perturbations usually lead to lexical errors and are easily corrected by spell checking tools (Ren et al., 2019; Zou et al., 2020), in this work, we focus on crafting word level adversarial examples that could maintain lexical and grammatical correctness and hence are more realistic.",
      "startOffset" : 113,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "Researchers have provided an intuitive definition that an NMT adversarial example should preserve the semantic meaning on the source but destroy the translation performance with respect to the reference translation (Michel et al., 2019; Niu et al., 2020).",
      "startOffset" : 215,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "Researchers have provided an intuitive definition that an NMT adversarial example should preserve the semantic meaning on the source but destroy the translation performance with respect to the reference translation (Michel et al., 2019; Niu et al., 2020).",
      "startOffset" : 215,
      "endOffset" : 254
    }, {
      "referenceID" : 7,
      "context" : "Correspondingly, the attack criteria are proposed as the absolute degradation or relative degradation against the reference translation (Ebrahimi et al., 2018; Michel et al., 2019; Niu et al., 2020; Zou et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "Correspondingly, the attack criteria are proposed as the absolute degradation or relative degradation against the reference translation (Ebrahimi et al., 2018; Michel et al., 2019; Niu et al., 2020; Zou et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "Correspondingly, the attack criteria are proposed as the absolute degradation or relative degradation against the reference translation (Ebrahimi et al., 2018; Michel et al., 2019; Niu et al., 2020; Zou et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 216
    }, {
      "referenceID" : 22,
      "context" : "Correspondingly, the attack criteria are proposed as the absolute degradation or relative degradation against the reference translation (Ebrahimi et al., 2018; Michel et al., 2019; Niu et al., 2020; Zou et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "To craft a perturbation that maintains the semantics as well as grammatical correctness following the above definition and evaluation, a variety of methods to impose word replacements have been proposed in recent studies (Michel et al., 2019; Cheng et al., 2019, 2020; Zou et al., 2020), making it a commonly used paradigm for NMT attacks.",
      "startOffset" : 221,
      "endOffset" : 286
    }, {
      "referenceID" : 22,
      "context" : "To craft a perturbation that maintains the semantics as well as grammatical correctness following the above definition and evaluation, a variety of methods to impose word replacements have been proposed in recent studies (Michel et al., 2019; Cheng et al., 2019, 2020; Zou et al., 2020), making it a commonly used paradigm for NMT attacks.",
      "startOffset" : 221,
      "endOffset" : 286
    }, {
      "referenceID" : 1,
      "context" : "Given a subset of (test) sentences T ∈ X and a small constant , we summarize previous works (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019) and give their conception of NMT adversarial examples as follows.",
      "startOffset" : 92,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "Given a subset of (test) sentences T ∈ X and a small constant , we summarize previous works (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019) and give their conception of NMT adversarial examples as follows.",
      "startOffset" : 92,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "Given a subset of (test) sentences T ∈ X and a small constant , we summarize previous works (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019) and give their conception of NMT adversarial examples as follows.",
      "startOffset" : 92,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "In contrast to the adversarial examples in image domain (Szegedy et al., 2014), we argue that taking yref as the reference sentence for x′ is not appropriate because the perturbation might change the semantic of x to some extent, causing that Definition 1 is not appropriate.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "In this work, we use the BLEU score (Papineni et al., 2002) to evaluate the similarity between two sentences.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "1) RNNsearch (Bahdanau et al., 2015) has an encoder consists of forward and backward RNNs each having 1000 hidden units and a decoder with 1000 hidden units.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "2) Transformer comprises six layers of transformer with 512 hidden units and 8 heads in both encoder and decoder, which mimics the hyperparameters in (Vaswani et al., 2017).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Specifically, the En→De model and En→Ru model are both subword-level transformer, where a joint byte pair encodings (BPE) with 32K split operations is applied for En→De, and separate BPE encodings with 24K split operations is applied for each language in En→Ru (Ng et al., 2019).",
      "startOffset" : 261,
      "endOffset" : 278
    }, {
      "referenceID" : 4,
      "context" : "To demonstrate our proposed WSLS method, we implement AST-lexcial (Cheng et al., 2018) as a black-box baseline, wherein AST-lexcial shares the same idea of random order random replacement.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "In recent years, adversarial examples have attracted increasing attention in the area of natural language processing (NLP), mainly on text classification (Jia and Liang, 2017; Ren et al., 2019; Wang et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 212
    }, {
      "referenceID" : 16,
      "context" : "In recent years, adversarial examples have attracted increasing attention in the area of natural language processing (NLP), mainly on text classification (Jia and Liang, 2017; Ren et al., 2019; Wang et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 212
    }, {
      "referenceID" : 20,
      "context" : "In recent years, adversarial examples have attracted increasing attention in the area of natural language processing (NLP), mainly on text classification (Jia and Liang, 2017; Ren et al., 2019; Wang et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 14,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 19,
      "context" : "For neural machine translation (NMT), there are also some adversary works emerging quickly (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Niu et al., 2020; Wallace et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 220
    }, {
      "referenceID" : 16,
      "context" : "the readability and also could be easily corrected by spell checkers (Ren et al., 2019; Zou et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 105
    }, {
      "referenceID" : 22,
      "context" : "the readability and also could be easily corrected by spell checkers (Ren et al., 2019; Zou et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 105
    }, {
      "referenceID" : 22,
      "context" : "(Zou et al., 2020) introduce a reinforced learning based method to craft the attacks following Michel et al.",
      "startOffset" : 0,
      "endOffset" : 18
    } ],
    "year" : 2021,
    "abstractText" : "Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used monodirectional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures. Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation. Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.",
    "creator" : "LaTeX with hyperref"
  }
}