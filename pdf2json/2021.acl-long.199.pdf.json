{
  "name" : "2021.acl-long.199.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-stage Pre-training over Simplified Multimodal Pre-training Models",
    "authors" : [ "Tongtong Liu", "Fangxiang Feng", "Xiaojie Wang" ],
    "emails" : [ "ttliu@bupt.edu.cn", "fxfeng@bupt.edu.cn", "xjwang@bupt.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2556–2565\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2556"
    }, {
      "heading" : "1 Introduction",
      "text" : "Self-attention based Transformer (Vaswani et al., 2017) effectively overcomes the problem of RNN being difficult to run in parallel, and greatly promotes the development of large-scale pre-training models. The pre-training language models, such as BERT (Devlin et al., 2019), have achieved excellent performance in many natural language processing tasks. With their big success, researchers have also developed pre-training models on multimodal tasks. A series of multimodal pre-training models have been proposed, such as\nViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), UNITER (Chen et al., 2019) etc., and have achieved excellent results in languagevision multimodal tasks.\nHowever, the current pre-training models are normally with large-scale parameters, require huge pre-training data and have very high demands on computational resources. For example, the GPT model (Radford et al., 2018) has 110 Million parameters, GPT-2 (Radford et al., 2019) has 1.5 Billion parameters, and GPT-3 (Brown et al., 2020) has a staggering 175 Billion parameters. The same is true for multimodal pre-trained models. For example, LXMERT (Tan and Bansal, 2019) has 183.5 Million parameters and requires 816 TitanX GPU hours for training on 9.18 Million text-image pairs. The sizes of these models are too huge for them to be deployed in many real-world scenarios. Therefore, the study of lightweight pre-training models, which can achieve similar performances to largescale models with smaller parameter scales and training costs, is significantly valuable.\nThere are some types of work on developing lightweight pre-trained models, including the design of the model structure, quantization, pruning and distillation. For example, ALBERT (Lan et al., 2020) is a lightweight model through structural design such as parameter sharing and parameter decomposition, and achieves better performance than original models; Q8BERT (Zafrir et al., 2019) compresses the model to 1/4 of the original model but with no more than 1% performance loss by quantizing 32bit floating point into 8bit; (Michel et al., 2019) used BERT weight pruning to compress the model and found that removing a large number of attention heads would not have a major impact on the model performance; TinyBERT (Jiao et al., 2020) reduced the model size by 7.5 times but with no more than 4% performance loss by designing a teacher-student distillation model.\nAll above works are on language pre-training models, and most of them concern scales of model parameters. There are few works on cutting training data and light weighing multimodal pretraining model. In fact, compared with language model, multimodal pre-training models should deal with data from both language and visual modal, which demand larger amounts of data and more computational resources. Meanwhile, collections of training data are more difficult. Taking for example the size of text-image pairs used for multimodal pre-training, the frequently used MS COCO (Lin et al., 2014) is a high quality dataset with only 0.82M pairs, while LAIT (Qi et al., 2020) is already a big data with 10M pairs but with average quality. Therefore, it is significantly valuable to develop lightweight multimodal pre-training models which can make use of limited data efficiently.\nExisting research on curriculum learning (Bengio et al., 2009) has shown that imitating the process of human learning by gradually increasing the difficulty of a task from simple to complex in stages helps to make better use of different types of data and effectively improve the performance of learning. Many models (Qi et al., 2020) use as much as data available but few works have been done on how to arrange the tasks for better making use of limited data. We therefore borrow the idea of curriculum learning on training pre-training models. We construct a pre-training process which makes use of data from smaller units to bigger units in stages, and design appropriate pre-training tasks for each corresponding stage.\nSpecifically, we propose a new Multi-stage Pretraining (MSP) method. The first pre-training stage is on the token units, where the text input is the category labels of the objects in the images, and the image input is the object features. An Image Features Random Shuffle (IFRS) is designed as a pre-training task for this stage. IFRS randomly shuffles the object features, and the model predicts the original object order based on the text information. The second stage focuses on phrase units. Phrase-level descriptions of the image are input on the text side and image features are input on the image side. A Topic of Image and Text for Phrase (TITP) task is designed for it. The third stage is sentence-based pre-training. Sentence-level captions are input on the text side, and image features are input on the image side. A Topic of Image and Text for Sentence (TITS) task is designed for it. We\ntake a Simplified LXMERT (LXMERT-S) which has fewer parameters and less pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in downstream tasks.\nThe main contributions of our work are as follows: (1) We propose a new MSP method that allows the model to learn different granularities of text-image correspondence information at different stages; (2) For each stage, we design pre-training tasks suitable for that stage, IFRS task for tokenbased pre-training, TITP task for phrase-based pretraining, and TITS task for sentence-based pretraining; (3) With less pre-trained data (11.76%), fewer model parameters (45.9%), less resource consumption (25%) and less training time (46.57%), the performances of downstream tasks are comparable to or even exceed that of the original model."
    }, {
      "heading" : "2 Related Works",
      "text" : "Multimodal Pre-training Models Multimodal pre-training models are mainly divided into two categories: single-stream models and two-stream models. Single-stream models such as B2T2 (Alberti et al., 2019), OSCAR (Li et al., 2020), etc., fuse image and text information at the beginning of the input; two-stream models such as ViLBERT (Lu et al., 2019), LXMERT(Tan and Bansal, 2019), etc., encode the image and text information alone first and then fuse them later. Generally two-stream models will have more parameters than single-stream models, but whether the single-stream model or the two-stream model has better performance or is related to the specific tasks require more rigorous experimental proof. We conduct follow-up experiments based on the two-stream model LXMERT by removing the coding layer of the individual modalities and keeping only the fusion coding layer, so that the simplified LXMERT model is more like the single-stream model.\nMultimodal Pre-training Data There are several different considerations on making use of data. VisualBERT (Li et al., 2019) believes that pre-training on the target dataset can improve the performance of the model, so VisualBERT first pre-trains on COCO Caption and then continues pre-training on the target dataset (e.g. VQA). ImageBERT (Qi et al., 2020), on the other hand, is trained on the out-of-domain LAIT dataset and\nthen on the in-domain datasets, such as Conceptual Captions(CC) (Sharma et al., 2018) and SBU Captions (Ordonez et al., 2011). It can be said the dataset that is most similar to the downstream task is used for training at last, and the general data is used firstly. Clearly, this way of using data is directly related to the downstream tasks. Different downstream tasks might lead to different order of data usage. In this paper, we design a staged pre-training from word-level to phrase-level to sentence-level, which is related to the size of information units. We also design suitable pretraining tasks for different phases to fully exploit the text-image information correspondence of different units in each phase, which has consistent effectiveness for different downstream tasks.\nMultimodal Pre-training Tasks The mostly employed language pre-training task is Masked Language Modeling (MLM) (Chen et al., 2019), where tokens are masked with a probability and those masked tokens are predicted by the model. Masked Region Feature Regression (MRFR) (Chen et al., 2019), which is similar to the MLM task, is a popular image pre-training task. Masked Object Classification (MOC) (Qi et al., 2020) task can be regarded as a multimodal pre-training task, which is to predict the category label of each masked object feature. Another popular multimodal pre-training task called Image-Text Matching (ITM) (Chen et al., 2019) is similar to the Next Sentence Prediction (NSP) task in BERT (Devlin et al., 2019), where an image corresponding to a text is randomly replaced with a probability of 50%, and the task is to discriminate whether the image matches the text. The existing pre-training tasks for multimodal data are limited. We design new pretraining tasks with the aim of making full use of the existing training dataset at different granularities."
    }, {
      "heading" : "3 Method",
      "text" : "The overall structure of our MSP method is shown in Figure 1. The pre-training process is divided into three stages based on different granularities of text-image correspondence from token, phrase to sentence. We design corresponding pre-training tasks for the three stages.\nWe perform the above three-stage pre-training on a simplified model of LXMERT (LXMERT-S). The simplified process of the LXMERT model is shown in Figure 2. The Cross-Modality Encoder of LXMERT-S is identical to the LXMERT. We obtain the Simplified LXMERT (LXMERT-S) by removing the Object-Relationship Encoder and Language Encoder. The image features and text features are directly input to the Cross-Modality Encoder in the LXMERT-S.\nBy removing the single modal coding layer in LXMERT, the 12-layer LXMERT is simplified to a 5-layer LXMERT-S. The amounts of parameters in simplified LXMERT-S are only 45.9% of the original model, and the whole experiment can be completed on a single GPU. The three-stage pretraining method is also fully applicable to other pre-training models."
    }, {
      "heading" : "3.1 Stage 1: Word-based Pre-training",
      "text" : "The first stage of pre-training focuses on learning the correspondence between text token units and image objects to help the model mine fine-grained information. To this end, we design the appropriate pre-training tasks and corresponding dataset for this phase of pre-training.\nPre-training Tasks We design an Image Features Random Shuffle (IFRS) pre-training task to enhance the pre-training of the token layer, based on the existing Masked Language Modeling (MLM) (Chen et al., 2019), Masked Region\nFeature Regression (MRFR) (Chen et al., 2019) and Masked Object Classification (MOC) (Qi et al., 2020).\nImage Features Random Shuffle (IFRS): Given a set of image regions R={r1, r2, r3. . . rm}, which are obtained by adding a fully-connected (FC) layer to the regions of interest (ROIs) and projecting them to the hidden size, a feature triplet is three consecutive features in R, e.g. tj=(ri, ri+1, ri+2). A shuffle on a triplet is to randomly change the order of features in the triplet with a probability of 5%. For example, the triplet tj is shuffled as t[S]j = (ri+1, ri+2, ri) = (r [S] i , r [S] i+1, r [S] i+2).The shuffled triplet t[S]j is used as input for the network, and the corresponding output is converted to the dimensionality of ROIs to obtain hθ(t [S] j )= (hθ(r [S] i ), hθ(r [S] i+1), hθ(r [S] i+2)). The ROIs extracted by Faster-RCNN corresponding to the original tj is fθ(tj)=(fθ(ri), fθ(ri+1), fθ(ri+2)),We use the L2 loss to calculate the distance between the network output hθ(t [S] j ) and fθ(tj) as in the following equation.\nL = E(W,R)∼D k=K∑ k=0 i=k′+2∑ i=k′ ||hθ(r [S] i )−fθ(ri)|| 2 2 (1)\nWhere K is the number of shuffled triples. Other pre-training tasks: We add the existing MLM, MRFR and MOC tasks to the token-based pre-training. MLM masks the token-level category labels of objects with a certain probability P, and the model predicts the masked category label based on the corresponding object feature on the image side. MRFR masks the object features, and the model predicts the original object-level features\nbased on the text-side category label and information around the object. MOC predicts the category and attribute labels of the masked object features.\nTraining Data We extract training data for IFRS task from caption-image pairs directly. For each image, 36 object features and their corresponding 36 category labels are provided by Faster-RCNN. These category labels have been unified with the text vocabulary, so they are all included in the text vocabulary. During training, the image side inputs the image features in sequence, and the text side inputs the category labels in the corresponding order. In the IFRS task, when the image side is shuffled, the order of the text side remains unchanged."
    }, {
      "heading" : "3.2 Stage 2: Phrase-based Pre-training",
      "text" : "The previous stage explores the correspondence between the image objects and their category. This stage mines the correspondence between the image object and the phrase describing of the object. Since the phrase description usually contains richer information about the attributes of the object, such as ”green old car”, building a pre-training task based on the correspondence between the phrase and the object allows the model to obtain rich information about the attributes.\nPre-training Tasks We define a Topic of Image and Text for Phrase (TITP) pre-training task that more directly supports phrase-based information mining.\nTopic of Image and Text for Phrase (TITP): Given a token sequence of image phrase-level description W = {w1, w2, w3. . . wn}, object feature sequence R = {r1, r2, r3. . . rm}, and correspondent category label sequence L={l1, l2, l3. . . lm} extracted by Faster-RCNN. Let topic set is topic= W∩L = {p1, p2. . . pq}, and label set Y = {y1, y2. . . yv}, where v is the size of the vocabulary. If yi∈topic, then yi is 1, otherwise yi is 0. We add a FC layer to the multimodal representation to get sθ(W,R), predict the correct topic from the vocabulary size v categories, and use BCELoss to calculate the gap between the model output sθ(W,R) and the label Y.\nL=E(W,R)∼D [1/v v−1∑ i=0 (yilogsθ(W,R)+(1−yi)log(1−sθ(W,R))\n(2)\nOther pre-training tasks: We add MLM, MRFR and MOC tasks to the phrase-based pre-training. MLM masks the attribute or category information\nof the phrase with a certain probability P, and the model predicts the masked information based on the corresponding object features. MRFR masks the object features of the image, and the model predicts the original object based on the phrase-level description on the text side and the surrounding object information, and MOC predicts the category and attribute of the object being masked based on the surrounding image features and the phrase-level description on the text side.\nTraining Data: We obtain the corresponding training data based on the Visual Genome (VG) (Krishna et al., 2017) dataset, which contains a large number of phrases. We eliminate the phrases containing verbs. The remaining phrases are concatenated with commas to obtain a phrase-level description of the image. During training, the spliced VG phrase is used as input on the text side and 36 object features extracted by Faster-RCNN are input on the image side."
    }, {
      "heading" : "3.3 Stage 3: Sentence-based Pre-training",
      "text" : "On the basis of the above token and phrase training, this stage uses the overall sentence-image correspondence relationship for pre-training to mine larger unit text-image related information.\nPre-training Tasks we design two sentencelevel pre-training tasks, Image-Text Matching Based on Hard Sample (ITM HS) and Topic of Image and Text for Sentence (TITS) described as follows.\nImage-Text Matching Based on Hard Sample (ITM HS): The purpose of this task is to reduce the noise brought to the model when the text-image pair does not match. We retrieve the top M most similar images for each image from difficult samples file1 as the hard sample set. In the ITM HS task, each image is replaced with a randomly selected hard sample with probability of 50% if the hard sample sets is not empty. If the set of current sample is empty, an image in the training set is randomly selected. Let the token sequence W ={w1, w2, w3. . . wn} and the image feature sequence R = {r1, r2, r3. . . rm}, the label y∈{0, 1} indicates whether the input image-text pair matches each other. We apply the FC layer on top of the multimodal representation to get sθ(T,R), which is the matching score of the image and text.\nL=E(W,R)∼D[ylogsθ(W,R)+(1−y)log(1−sθ(W,R))] (3)\n1The difficult sample comes from the difficult sample file in ViLBERT’s Image-Text Retrieval task.\nTopic of Image and Text for Sentence (TITS): The purpose of this task is to jointly predict the content described by both image and sentence information. Given a token sequence W = {w1, w2, w3. . . wn}, an image feature sequence R= {r1, r2, r3. . . rm}, category labels for object features L = {l1, l2, l3. . . lm}, topic = W∩L = {p1, p2. . . pq}, and label Y ={y1, y2. . . yv}, where v is the size of the vocabulary. If yi∈topic, then yi is 1, otherwise yi is 0. We apply the FC layer on top of the multimodal representation, convert its dimension to the vocabulary size v to get sθ(W,R), and use BCELoss to calculate the gap between the model output sθ(W,R) and the label Y.\nL=E(W,R)∼D [1/v k=K∑ k=0 (yilogsθ(W,R)+(1−yi)log(1−sθ(W,R)))\n(4)\nOther pre-training tasks: We add the existing MLM, MRFR and MOC tasks to the sentencebased pre-training. MLM masks the information in the sentence and the model predicts the masked information based on the all information on the image side. MRFR masks the object features of the image and the model predicts the original object based on the overall information at the sentence level on the text side and the surrounding object information. MOC predicts the category and attribute of the masked object based on the image features and the text-side sentence-level description.\nTraining Data In this stage, the image and its corresponding caption are directly used as input, the sentence level information caption is input on the text side, and the 36 object features provided by Faster-RCNN are input on the image side."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Pre-training Dataset",
      "text" : "In this paper, the model is pre-trained using the COCO dataset and part of the VG dataset, and only 1.08M text-image pairs are used, where 0.12M image-text pairs are used in token-based pre-training stage, 0.34M image-text pairs are used in phrase-based pre-training stage, and 0.62M image-text pairs are used in the sentence-based pre-training stage. All datasets we used are also used in initial LXMERT. Table 1 gives a comparison of the pre-training data, model parameters2 and\n2We exclude the parameters of the word embedding and pre-training task and only count the number of parameters in the Transform part.\ncomputational resources with other models."
    }, {
      "heading" : "4.2 Downstream Tasks and Data Sets",
      "text" : "Visual Question Answering (VQA): There are multiple datasets for VQA. We use three common used datasets: VQA V2.0 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), and NLVR2 (Suhr et al., 2019). Accuracy is used as to measure model performance.\nCross-modal Retrieval task: We choose Flickr30K (Young et al., 2014) dataset as the retrieval task data, and evaluate the performance of the model in Image Retrieval (IR), Text Retrieval (TR), Zero Shot Image Retrieval (ZS-IR), and Zero Shot Text Retrieval (ZS-TR) respectively, and the performance metric is the matching score of text and image pairs. Zero shot is to evaluate the performance of the pre-trained model directly on the test set without fine-tuning, and is used to evaluate the effect of the pre-trained model. Therefore ZS-IR and ZS-TR are directly loaded with model parameters to perform IR and TR tasks without fine-tuning.\nIn the fine-tuning stage, the multimodal representation of the model is passed through a FC layer as a joint representation of image and text to solve downstream tasks. For VQA tasks, we linearize the multimodal representation into the answer category dimension through the FC layer to predict the answer of each question. For the Image-Text Retrieval (Young et al., 2014) task, we randomly replace the image or text, construct three negative examples for an image-text pair, including two random negative examples and a hard sample, and use BCELoss to calculate the difference between the matching score and the text-image matching label ."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our model with both single-stream multimodal pre-training models including Unified VLP (Zhou et al., 2020), VisualBERT (Li et al., 2019) and VL-BERT (Su et al., 2020) and twostream models including ViLBERT (Lu et al., 2019) and LXMERT (Tan and Bansal, 2019).\nUnified VLP Unified VLP uses a 12 layers of shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. It conducts pre-training on the Conceptual Captions(CC) (Sharma et al., 2018) which has around 3.3 million image-text pairs, and requires 150 hours of training on the 8x V100 GPUS. Unified VLP includes only the MLM task when processing the comprehension tasks.\nVisualBERT VisualBERT contains 12 layers of transformer with 85.05M parameters. It first pretrains on COCO Caption (Lin et al., 2014) with MLM and ITM tasks and then continues pretraining on the target dataset with MLM task. The pre-training data sizes for VisuaBERT on the VQA V2.0 task are shown in Table 1. For different downstream tasks, the second stage of pre-training needs to be re-trained.\nVL-BERT VL-BERT contains 12 layers of transformer with 134.8M parameters. It pre-trains on both visual-linguistic and text-only datasets. Samples are randomly drawn from both CC and BooksCorpus (Zhu et al., 2015) & English Wikipedia (at a ratio of 1:1) in each mini-batch. VL-BERT considers ITM to be harmful to downstream tasks and therefore only includes MLM and MOC tasks.\nViLBERT ViLBERT extends the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. It trains on CC with MLM, MOC and ITM tasks.\nLXMERT LXMERT has a large-scale Transformer model that consists of three encoders and a large-scale pre-training data, including MS COCO, Visual Genome, VQA v2.0, GQA and VGQA (Zhu et al., 2016). The pre-training requires 8.5 days on the 4x TitanX GPUS. It also has many pre-training tasks, including MLM, MRFR, MOC, ITM and Image Question Answering (QA) (Tan and Bansal, 2019), and has achieved good results in downstream tasks, especially VQA tasks."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "Our Transformer backbone is the same as LXMERT, where each Transformer block has 768 hidden units and 12 attention heads. Image features are extracted by Faster-RCNN (Ren et al., 2015) model (with ResNet-101 (He et al., 2016) backbone) trained on Visual Genome (VG).\nDuring pre-training, our model is trained for about 95 hours on 1 TitanX GPU, and takes Adam (Kingma and Ba, 2015) as the optimizer with a learning rate of 1e-5. We train the tokenbased model for 10 epochs with a batch size of 64, phrase-based model for 20 epochs with a batch size of 128 and sentence-based model for 20 epochs with a batch size of 128.\nDuring Fine-tuning, the learning rate of all downstream tasks is 5e-5, and the batch size is 32. We fine-tune 6 epochs for VQA V2.0, 5 epochs for GQA, and 8 epochs for NLVR2 and Image-Text Retrieval tasks.\nFor hard samples in ITM HS task, we retrieve the top 100 most similar images from difficult samples file. For the masking strategies, we randomly mask 15% tokens, 15% object features. The codes of our models are available at https: //github.com/lttsmn/LXMERT-S."
    }, {
      "heading" : "4.5 Experimental Results",
      "text" : "Table 2 gives the results of the model on the three VQA datasets, and Table 3 gives the results of the model on the Flickr30K Image-Text Retrieval dataset.\nIt can be seen from both Table 2 and 3 that the pre-training model proposed in this paper has achieved comparable performances with the existing large models under the condition of less training data, fewer parameters and less computing resource occupation. In some cases, our small model even outperforms the big one. For example, NLVR2 task is 0.22 higher than LXMERT on Test-P, and ZS-IR is 18.42 higher than LXMERT in R@1 under the premise that the model parameters are reduced by 54.1% and the training data set is reduced by 88.24%."
    }, {
      "heading" : "4.6 Ablation Study",
      "text" : "Table 4 gives results of LXMERT-S on different tasks with different pre-training setting. The first\ncolumn gives the number of stage(s) in pre-training. The second column gives the stage(s) used, where S for sentence stage, P for phrase stage, and T for token stage, T→S means there are two stages including token-based pre-training first and then sentencebased pre-training. T→P→S means there are three stages including token-based pre-training first and then phrase-based pre-training and sentencebased pre-training last. T+P+S means to train all stages together. The third column gives the pretraining tasks used in the pre-training. We first give all the pre-training tasks used in the training stages used, then verify the validity of the pretraining tasks by removing a task based on all the pre-training tasks, “-” indicates that a pre-training task is removed.\nFrom Table 4, we can find: (1) With the orderly increase of the training phase, the performance of the model on downstream tasks is gradually improving; (2) The training granularity from small to large is the most effective training sequence; (3) The pre-training tasks we propose for each stage of pre-training can improve the performance of the model on downstream tasks, such as TITP improves VQA performance by 0.09, GQA performance by 0.4, NLVR2 performance by 0.24, IR performance by 0.48, and ZS-TR by 0.83."
    }, {
      "heading" : "5 Qualitative Analysis",
      "text" : "We visualize the impact of different pre-training stages on VQA and Image-Text Retrieval task by showing the answers probability distribution. For each example in Figure 3, the left side is the input image of the model, and the right side is the probability distribution of the top3 scoring answers in different pre-training stages.\nFor Image-text Retrieval task, we select the top 1 caption for visualization. For each sample in Figure 4, the left side is the input image and the\nright side is the highest scoring caption predicted by the model.\nFrom both Figure 3 and 4, we can find: (1) Token-based pre-training (S vs T→S) helps the model to learn object information in the images. For example, in the left sample in Figure 3 and 4, the model improves its performance on downstream tasks by adding token-based pre-training that makes the model focus on object information such as horses, man and rocks in the images; (2) Phrase-based pre-training (T→S vs T→P→S) helps the model to learn information about the attributes of the objects. As shown in right-hand image in Figure 3 and 4, the model pays attention to attribute information, i.e. blanket is white, clothes are pink, etc."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, inspired by the idea of curriculum learning, we propose a MSP method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages, we also design pretraining tasks suitable for each stage of pre-training, IFRS task for word-based pre-training, TITP task for phrase-based pretraining, and TITS task for sentence-based pretraining. Experimental results on several VQA datasets as well as one cross-modal retrieval dataset show that our method achieves similar or even better performance than a larger model in terms of accuracy in all downstream tasks under the premise that the model parameters are reduced by 54.1% and the training data set is reduced by 88.24%. In future work, we will add the above training method to other simplified pretrained models to further explore the effectiveness of MSP method."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank anonymous reviewers for their suggestions and comments. The work was supported by the National Natural Science Foundation of China (NSFC62076032), the Cooperation Poject with Beijing SanKuai Technology Co., Ltd and the National Key Research and Development Program of China (2020YFF0305302). We would like to thank Dr. Huixing Jiang and his colleagues."
    } ],
    "references" : [ {
      "title" : "Fusion of detected objects in text for visual question answering",
      "author" : [ "Chris Alberti", "Jeffrey Ling", "Michael Collins", "David Reitter." ],
      "venue" : "EMNLP-IJCNLP 2019, pages 2131–2140.",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning, pages 41–48.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "UNITER: learning universal image-text representations",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "CoRR, abs/1909.11740.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT 2019, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "CVPR 2017, pages 6325–6334. IEEE Computer Society.",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR 2016, pages 770–778. IEEE Computer Society.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "GQA: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "CVPR2019,, pages 6700–6709.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "EMNLP 2020, pages 4163– 4174.",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Visual genome: Connecting language and vision",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR 2020.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "CoRR, abs/1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "ECCV 2020, volume",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ECCV 2014, volume 8693, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "NeurIPS 2019, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Are sixteen heads really better than one",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg." ],
      "venue" : "NIPS 2011, pages 1143– 1151.",
      "citeRegEx" : "Ordonez et al\\.,? 2011",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "Imagebert: Crossmodal pre-training with large-scale weak-supervised image-text data",
      "author" : [ "Di Qi", "Lin Su", "Jia Song", "Edward Cui", "Taroon Bharti", "Arun Sacheti." ],
      "venue" : "CoRR, abs/2001.07966.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training (2018)",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Faster R-CNN: towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun." ],
      "venue" : "NIPS 2015, pages 91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "ACL 2018, pages 2556– 2565. Association for Computational Linguistics.",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "VL-BERT: pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "ICLR 2020.",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus for reasoning about natural language grounded in photographs",
      "author" : [ "Alane Suhr", "Stephanie Zhou", "Ally Zhang", "Iris Zhang", "Huajun Bai", "Yoav Artzi." ],
      "venue" : "ACL 2019,, pages 6418–6428. Association for Computational Linguistics.",
      "citeRegEx" : "Suhr et al\\.,? 2019",
      "shortCiteRegEx" : "Suhr et al\\.",
      "year" : 2019
    }, {
      "title" : "LXMERT: learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "EMNLP-IJCNLP 2019, pages 5099– 5110.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS 2017, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Q8BERT: quantized 8bit BERT",
      "author" : [ "Ofir Zafrir", "Guy Boudoukh", "Peter Izsak", "Moshe Wasserblat." ],
      "venue" : "CoRR, abs/1910.06188.",
      "citeRegEx" : "Zafrir et al\\.,? 2019",
      "shortCiteRegEx" : "Zafrir et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified vision-language pre-training for image captioning and VQA",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason J. Corso", "Jianfeng Gao." ],
      "venue" : "AAAI 2020, pages 13041–13049. AAAI Press.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual7w: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei." ],
      "venue" : "CVPR 2016, pages 4995– 5004. IEEE Computer Society.",
      "citeRegEx" : "Zhu et al\\.,? 2016",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "CoRR, abs/1506.06724.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Self-attention based Transformer (Vaswani et al., 2017) effectively overcomes the problem of RNN being difficult to run in parallel, and greatly promotes the development of large-scale pre-training models.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "The pre-training language models, such as BERT (Devlin et al., 2019), have achieved excellent performance in many natural language processing tasks.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "A series of multimodal pre-training models have been proposed, such as ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), UNITER (Chen et al.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", 2019), LXMERT (Tan and Bansal, 2019), UNITER (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : ", 2019), LXMERT (Tan and Bansal, 2019), UNITER (Chen et al., 2019) etc.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "For example, the GPT model (Radford et al., 2018) has 110 Million parameters, GPT-2 (Radford et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : ", 2018) has 110 Million parameters, GPT-2 (Radford et al., 2019) has 1.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "5 Billion parameters, and GPT-3 (Brown et al., 2020) has a staggering 175 Billion parameters.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "For example, ALBERT (Lan et al., 2020) is a lightweight model through structural design such as parameter sharing and parameter decomposition, and achieves better performance than original models; Q8BERT (Zafrir et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : ", 2020) is a lightweight model through structural design such as parameter sharing and parameter decomposition, and achieves better performance than original models; Q8BERT (Zafrir et al., 2019) compresses the model to 1/4 of the original model but with no more than 1% performance loss by quantizing 32bit floating point into 8bit; (Michel et al.",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : ", 2019) compresses the model to 1/4 of the original model but with no more than 1% performance loss by quantizing 32bit floating point into 8bit; (Michel et al., 2019) used BERT weight pruning to compress the model and found that removing a large number of attention heads would not have a major impact on the model performance; TinyBERT (Jiao et al.",
      "startOffset" : 146,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : ", 2019) used BERT weight pruning to compress the model and found that removing a large number of attention heads would not have a major impact on the model performance; TinyBERT (Jiao et al., 2020) reduced the model size by 7.",
      "startOffset" : 178,
      "endOffset" : 197
    }, {
      "referenceID" : 14,
      "context" : "Taking for example the size of text-image pairs used for multimodal pre-training, the frequently used MS COCO (Lin et al., 2014) is a high quality dataset with only 0.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 18,
      "context" : "82M pairs, while LAIT (Qi et al., 2020) is already a big data with 10M pairs but with average quality.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "Existing research on curriculum learning (Bengio et al., 2009) has shown that imitating the process of human learning by gradually increasing the difficulty of a task from simple to complex in stages helps to make better use of different types of data and effectively improve the performance of learning.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "Many models (Qi et al., 2020) use as much as data available but few works have been done on how to arrange the tasks for better making use of limited data.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Single-stream models such as B2T2 (Alberti et al., 2019), OSCAR (Li et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : ", fuse image and text information at the beginning of the input; two-stream models such as ViLBERT (Lu et al., 2019), LXMERT(Tan and Bansal, 2019), etc.",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "VisualBERT (Li et al., 2019) believes that pre-training on the target dataset can improve the performance of the model, so VisualBERT first pre-trains on COCO Caption and then continues pre-training on the target dataset (e.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "ImageBERT (Qi et al., 2020), on the other hand, is trained on the out-of-domain LAIT dataset and",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "then on the in-domain datasets, such as Conceptual Captions(CC) (Sharma et al., 2018) and SBU",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Multimodal Pre-training Tasks The mostly employed language pre-training task is Masked Language Modeling (MLM) (Chen et al., 2019), where tokens are masked with a probability and those masked tokens are predicted by the model.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "Masked Region Feature Regression (MRFR) (Chen et al., 2019), which is similar to the MLM task, is a popular image pre-training task.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "Masked Object Classification (MOC) (Qi et al., 2020) task can be regarded as a multimodal pre-training task, which is to predict the category label of each masked object feature.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "Another popular multimodal pre-training task called Image-Text Matching (ITM) (Chen et al., 2019) is similar to the Next Sentence Prediction (NSP) task in BERT (Devlin et al.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : ", 2019) is similar to the Next Sentence Prediction (NSP) task in BERT (Devlin et al., 2019), where an image corresponding to a text is randomly replaced with a probability of 50%, and the task is to discriminate whether the image matches the text.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "Pre-training Tasks We design an Image Features Random Shuffle (IFRS) pre-training task to enhance the pre-training of the token layer, based on the existing Masked Language Modeling (MLM) (Chen et al., 2019), Masked Region",
      "startOffset" : 188,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "Feature Regression (MRFR) (Chen et al., 2019) and Masked Object Classification (MOC) (Qi et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and Masked Object Classification (MOC) (Qi et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "Training Data: We obtain the corresponding training data based on the Visual Genome (VG) (Krishna et al., 2017) dataset, which contains a large number of phrases.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "0 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), and NLVR2 (Suhr et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : ", 2017), GQA (Hudson and Manning, 2019), and NLVR2 (Suhr et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : ", 2017), GQA (Hudson and Manning, 2019), and NLVR2 (Suhr et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "Cross-modal Retrieval task: We choose Flickr30K (Young et al., 2014) dataset as the retrieval task data, and evaluate the performance of the model in Image Retrieval (IR), Text Retrieval (TR), Zero Shot Image Retrieval (ZS-IR), and Zero Shot Text Retrieval (ZS-TR) respectively, and the performance metric is the matching score of text",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "For the Image-Text Retrieval (Young et al., 2014) task, we randomly replace the image or text, construct three negative examples for an image-text pair, including two random negative examples and a hard sample, and use BCELoss to calculate the difference between the matching score and the text-image matching label .",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "We compare our model with both single-stream multimodal pre-training models including Unified VLP (Zhou et al., 2020), VisualBERT (Li et al.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : ", 2020), VisualBERT (Li et al., 2019) and VL-BERT (Su et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and VL-BERT (Su et al., 2020) and twostream models including ViLBERT (Lu et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : ", 2020) and twostream models including ViLBERT (Lu et al., 2019) and LXMERT (Tan and Bansal, 2019).",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "It conducts pre-training on the Conceptual Captions(CC) (Sharma et al., 2018) which has around 3.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "It first pretrains on COCO Caption (Lin et al., 2014) with MLM and ITM tasks and then continues pretraining on the target dataset with MLM task.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : "Samples are randomly drawn from both CC and BooksCorpus (Zhu et al., 2015) & English Wikipedia (at a ratio of 1:1) in each mini-batch.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "It also has many pre-training tasks, including MLM, MRFR, MOC, ITM and Image Question Answering (QA) (Tan and Bansal, 2019), and has achieved good results in downstream tasks, especially VQA tasks.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "Image features are extracted by Faster-RCNN (Ren et al., 2015) model (with ResNet-101 (He et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : ", 2015) model (with ResNet-101 (He et al., 2016) backbone) trained on Visual Genome (VG).",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "During pre-training, our model is trained for about 95 hours on 1 TitanX GPU, and takes Adam (Kingma and Ba, 2015) as the optimizer with a learning rate of 1e-5.",
      "startOffset" : 93,
      "endOffset" : 114
    } ],
    "year" : 2021,
    "abstractText" : "Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pretrained models require large amounts of training data and have huge model sizes, which make them difficult to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train the model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERTS), which has only 45.9% parameters of the original LXMERT model and 11.76% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.",
    "creator" : "LaTeX with hyperref"
  }
}