{
  "name" : "2021.acl-long.502.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Controllable Open-ended Question Generation with A New Question Type Ontology",
    "authors" : [ "Shuyang Cao", "Lu Wang" ],
    "emails" : [ "wangluxy}@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6424–6439\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6424"
    }, {
      "heading" : "1 Introduction",
      "text" : "Question-asking has long served as an effective instrument for knowledge learning (Andre, 1979; Tobin, 1990) and assessing learning progress (Holme, 2003; Downing and Yudkowsky, 2009; Livingston, 2009). Compared to the widely studied task of generating factoid questions that inquire about “one bit” of information (Du et al., 2017; Duan et al., 2017; Li et al., 2019), this work is interested in generating open-ended questions that require deep comprehension and long-form answers (Labutov et al., 2015). Such open-ended questions are valuable in education, e.g., to facilitate complex knowledge acquisition (Lai et al., 2017) and nurture reasoning skills (Shapley, 2000), as well as in other applications like improving search engines (Han\nInput: It’s a difficult task to undertake. Teenagers tend to identify gangs with “fitting” in. Peer pressure plays a large part in it and sometimes teenagers have problems with their own identity being part of a gang deals with those issues. It also provides a little bit of respect on the street ...\nBART SAMPLING: - How do you stop a teen from joining a gang? (PROCEDURAL) - How do you get teenagers to stop being in gangs? (PROCEDURAL) - How do you get teens out of gangs? (PROCEDURAL) BART + QWORD: - How do you get a teenager out of a gang? (PROCEDURAL) - What is the best way to get teenagers out of gangs? (PROCEDURAL) - Why do teenagers join gangs? (CAUSE) TPLGEN: - How do I get [NP] to quit being in [NP]? ⇒ How do I get my son to quit being in a gang? (PROCEDURAL) - What are [NP]? ⇒ What are some programs for teenagers involved in gangs? (EXAMPLE) - Why do [NP] [V] [NP]? ⇒ Why do teenagers identify gangs? (CAUSE)\nFigure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator TPLGEN, that predicts focuses and operates with generated templates (to the left of the arrows). Questions generated by our model have diverse TYPEs.\net al., 2019) and building open-domain dialogue systems (Shum et al., 2018).\nSignificant progress has been made in generating factoid questions (Zhang and Bansal, 2019; Zhou et al., 2019b; Su et al., 2020), yet new challenges need to be addressed for open-ended questions. First, specifying the question type is crucial for constructing meaningful questions (Graesser et al., 1992). Question words such as “why” and “when” are generally seen as being indicative of types (Zhou et al., 2019b), but they underspecify the conceptual content of questions (Olney et al., 2012). Using Figure 1 as an example, different\nquestion words, i.e., both “how” and “what”, can be used for inquiring about procedures. It thus calls for a new question type ontology that can precisely capture the conceptual nature of questions. Second, constructing questions from a text with multiple sentences needs to focus on its central concepts or phenomena that necessitate extensive descriptions. New representations are needed to capture such content as question focus(es), to go beyond existing methods that rely on entities and their neighboring words (Du et al., 2017; Sun et al., 2018) even though they are effective for generating factoid questions. Third, encouraging the diversity of generated questions (Sultan et al., 2020; Wang et al., 2020) is less explored but critical for real world applications, e.g., various questions should be proposed to gauge how well students grasp the knowledge of complex subjects.\nIn this work, we aim to address the challenges of generating open-ended questions from input consisting of multiple sentences. We first introduce a new question type ontology, drawn upon researches in cognitive science and psychology (Graesser et al., 1992), to capture deeper levels of cognition, such as causal reasoning and judgments. Based on the new ontology, we collect and annotate a dataset of 4,959 questions to benefit research in both question generation and answering.1\nWe then design a type-aware framework to jointly predict question focuses (what to ask about) and generate questions (how to ask it). Different from pipeline-based approaches (e.g., Sun et al. (2018)), our framework is built on large pre-trained BART (Lewis et al., 2020), and uses shared representations to jointly conduct question focus prediction and question generation while learning taskspecific knowledge. It is further augmented by a semantic graph that leverages both semantic roles and dependency relations, facilitating long text comprehension to pinpoint salient concepts.\nMoreover, to achieve the goal of producing various types of questions from the same input, we investigate two model variants that use templates to improve controllability and generation diversity: one using pre-identified exemplars, the other employing generated templates to guide question writing, with sample outputs displayed in Figure 1.\nFor experiments, we collect two new large-scale datasets consisting of open-ended questions with\n1Our data and code are available at: https:// shuyangcao.github.io/projects/ontology_ open_ended_question.\nanswers from (1) Yahoo Answers2 L6 dataset and (2) popular question-asking communities on Reddit3, consisting of 291K and 720K question-answer pairs, respectively. Compared to existing popular QA datasets, such as SQuAD (Rajpurkar et al., 2016) and MS MARCO (Bajaj et al., 2016)), questions in our datasets ask about complex phenomena and perplexing social issues that seek solutions expressed in a long form. Automatic metrics show that our type-aware question generation model outperforms competitive comparisons, highlighting the effectiveness of semantic graph-augmented representation and joint modeling of focus prediction and question generation. Human judges also confirm that questions generated by our model have better overall quality. Adding templates further promotes question diversity, as evaluated by both automatic evaluation and human assessment."
    }, {
      "heading" : "2 Related Work",
      "text" : "Question generation has long been studied to reduce human efforts in constructing questions for knowledge learning evaluation (Mitkov and Ha, 2003; Brown et al., 2005). Early work relies on syntactic transformation to convert declarative sentences to questions (Heilman and Smith, 2010; Chali and Hasan, 2015). Recent advancements rely on sequence-to-sequence models to generate a question from a given sentence or paragraph by considering the focus, type, and general-specific relations of questions (Sun et al., 2018; Zhou et al., 2019b; Krishna and Iyyer, 2019). In particular, question likelihoods and rewards are designed to steer them toward being addressed by the given answers (Zhou et al., 2019a; Zhang and Bansal, 2019). Attempts are also made toward creating complex questions that require multi-hop reasoning over the given text, and graph-based representations have been an enabling tool to facilitate the access to both entities and relations (Pan et al., 2020; Su et al., 2020). While our model also enhances the input with a semantic graph, it boasts a richer representation by including both dependency and semantic relations, with predicted question focuses highlighted via extra node embeddings. Moreover, we create a separate layer of cross attentions that is dedicated to the semantic graph, while prior work uses the same set of attentions to attend to the concatenated text and graph representations.\n2https://answers.yahoo.com/ 3https://www.reddit.com/\nGiven the data-driven nature of question generation and answering tasks, recent studies take advantage of the availability of large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Bajaj et al., 2016), HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), inter alia. These corpora mainly contain factoid questions, while our newly collected datasets are not only larger in size but also comprise significantly more open-ended questions for querying reasons and procedures. A dataset closer to ours is ELI5 (Fan et al., 2019), which also obtains open-ended questionanswer pairs from Reddit, while one of our datasets includes more Reddit communities and thus covers a wider range of topics.\nOur work is more inline with generating deeper questions with responses that span over multiple sentences, where manually constructed templates are found effective (Olney et al., 2012). For example, Labutov et al. (2015) use crowdsourcing to collect question templates based on an ontology derived from Wikipedia and Freebase topics. Different from the topic-based ontology, our question types are more aligned with cognitive levels. Moreover, our templates are automatically learned from training data. Recent work (Rao and Daumé III, 2018, 2019) focuses on asking clarification questions based on both retrieval and generation models. As there has been no suitable framework for diverse types of questions, this work aims to fill the gap by introducing type-aware generation models which optionally leverage question templates for better controllability.\nGenerating diverse questions is much less studied, with existing approaches mainly focusing on entity replacement (Cho et al., 2019), sampling decoding (Sultan et al., 2020; Wang et al., 2020), and post-filtering (Liu et al., 2020). However, the produced diversity is driven by word choice and syntax variation, with little ability to control on question types, which is the focus of this work."
    }, {
      "heading" : "3 Data Collection and Question Type Annotation",
      "text" : ""
    }, {
      "heading" : "3.1 Open-ended Question Datasets",
      "text" : "To collect open-ended questions, we resort to online forums with active question-asking discussions. Concretely, we gather and clean question-answer pairs from Reddit and Yahoo Answers, to train generators that construct questions by taking the corresponding answer as input.\nWe choose five popular Reddit communities: r/AskHistorians, r/Ask Politics, r/askscience, r/explainlikeimfive, and r/AskReddit, where open-ended questions are actively asked. The original posts (OPs) are extracted, with their titles becoming questions. We also keep the best answer with the highest karma (i.e., upvotes minus downvotes) if it is greater than 1. A second dataset with question-answer pairs is collected from the Yahoo Answers L6 corpus4, which covers a broader range of topics than the Reddit data. For each question, the best answer is rated by the user who raises the question.\nPreprocessing. To ensure both questions and answers are well-formed, human inspection is conducted in multiple iterations to design rules to filter out improper samples. For instance, we discard samples whose answers have less than 15 content words to avoid the inclusion of factoid question. More details are provided in Table 6 in Appendix A. Ultimately, 719,988 question-answer pairs are kept for Reddit, and 290,611 for Yahoo. Each dataset is then divided into train, validation and test sets with a 90%/5%/5% split. The average lengths of questions and answers are 14.5 and 117.8 for Reddit, and 12.2 and 123.6 for Yahoo."
    }, {
      "heading" : "3.2 Question Type Ontology and Annotation",
      "text" : "Our question type ontology is adopted and modified from Olney et al. (2012), where 18 categories are originally proposed for knowledge learning as-\n4https://webscope.sandbox.yahoo.com/\nsessment. We recruited 6 native English speakers for three rounds of question type annotation. Based on the annotators’ feedback after each round, we refine the definitions, merge ambiguous types, and delete inapplicable categories. For example, an initial EXPECTATION type is merged into CAUSE due to their similarities in seeking causality. Finally, 10 types are preserved (Table 1). As can be seen, our ontology is designed to better capture the nature of questions than question words.\nAnnotating Questions with Types. After the annotation guideline is finalized, we ask the same set of annotators to label 5,000 (2 × 2,500) randomly sampled questions from both Reddit and Yahoo’s training sets. Each question is labeled by two annotators, with disagreements resolved through discussions. After removing samples without consensus, the final dataset consists of 4, 959 questions. EXAMPLE questions are most prevalent, comprising 23.4% of samples, while only 2.6% are CONSEQUENCE questions. A Krippendorff’s α of 0.67 is obtained for all samples, indicating a reasonable agreement level. The annotation guideline and examples for each question type are shown in Table 12 in Appendix A.\nTraining Question Type Classifiers. Since our type-aware question generation model requires a specified type as input, here we describe how to build two question type classifiers: (1) γq, that labels a type by reading the question and is used to provide question type labels during training; (2) γa, that predicts a type for use by taking the answer as input and is used during test.\nBoth classifiers are based on RoBERTa (Liu et al., 2019), where a prediction layer is built on top of the contextual representation of the [BOS] token to output question type probabilities. γq achieves a macro F1 score of 0.80 on a reserved test set, with data splits detailed in Appendix B. To train γa, in addition to the annotated questions, we run γq on unlabeled questions in Reddit and Yahoo and include samples whose type prediction confidence score is above 0.9. We train one γa for each dataset. γa obtains macro F1 scores of 0.48 and 0.46 on the same reserved test set over all types after training on Yahoo and Reddit, respectively.\nAfter running γq on both datasets, we find that Reddit has significantly more EXAMPLE questions (43.8% of all samples). Yahoo dataset is more balanced, with PROCEDURAL questions being the most frequent type (19.9% of all samples). Distri-\nbutions of question types for the two datasets are listed in Table 8 in Appendix B."
    }, {
      "heading" : "4 Type-aware Open-ended Question Generation",
      "text" : "In this section, we present our type-aware question generation framework. As shown in Figure 2, our model takes in a multi-sentence text and a predicted question type. Built on shared input representations, it first detects question focuses from a semantic graph, and then generates the question (§ 4.1). We also propose two model variants that consider automatically extracted template exemplars or generated templates to achieve controllability (§ 4.2), enabling the generation of diverse questions."
    }, {
      "heading" : "4.1 Joint Focus Prediction and Question Generation (JOINTGEN)",
      "text" : "Our generator is built on top of BART (Lewis et al., 2020). To facilitate the detection of salient content (i.e., focuses) to raise questions, we first augment the encoder with a semantic graph that consists of both dependency relations and semantic roles, capturing semantic relations over different scopes with varying granularities. Question focuses are first detected based on the semantic graph, which then guide question generation via cross-attentions, as shown in Figure 2. Although the joint modeling of focus prediction and question generation has been studied before, our design differs by using shared\nrepresentations consisting of the input text and semantic graph, and the prediction of focuses are included through gating mechanisms, whereas previous work, e.g. Pan et al. (2020), simply employs multi-task learning. Below, we first describe constructing the semantic graph-augmented encoder, followed by the joint modeling of two tasks.\nImproving Long Text Comprehension with Semantic Graph. To construct the semantic graph, for each sentence, we start with obtaining its dependency tree using Stanford CoreNLP (Manning et al., 2014). To better highlight core concepts, we discard less important relations, e.g., auxiliaries. The full list is included in Appendix C. Since our goal is to detect central concepts that are well connected with many other words, we can remove relations on the edges to minimize the number of parameters to learn. Moreover, as semantic roles can indicate main entities (Mannem et al., 2010), we extract semantic roles and their relations with AllenNLP (Shi and Lin, 2019). To merge the two sources of information, we add an edge in the dependency tree to connect the head word of the predicate and the head word of each semantic role. To build a connected graph from the multi-sentence input, we add an edge between each sentence’s last token and the next sentence’s first token. Finally, we merge nodes with the same surface forms or with corefered mentions. To the best of our knowledge, this is the first time that both dependency and semantic relations are encoded in the same graph for question generation, and with enhanced connectivity of the constructed graph, our design can better signal content salience.\nJoint Modeling with Cross-attentions. Given a predicted question type t and a multi-sentence text x = {x1, · · · , xn}, the BART encoder builds the contextual representation H = {h0,h1, · · · ,hn} at the last layer, where h0 is for t.\nTo encode the semantic graph, we initialize the node representation for node vi by taking the average contextual representations of its tokens and appending four bits encoding the number of nodes (capped at 10) that are merged into vi, to add frequency information. This step yields new node representations v(0)i . We then apply graph attention networks (GATs) (Veličković et al., 2018) of L layers to update the representations as follows:\nv (l) i = ∑ j∈Ni ai,jW (l)v (l−1) j (1)\nwhere W (l) is a learnable parameter for the l-th layer, and Ni denotes the neighbors of vi. The attention score ai,j is calculated as in GATs. We use L = 2 for experiments.\nTo predict focuses, the final node representation v (L) i is fed into the following feedforward network, yielding the probability of vi being a focus as:\npfocus(vi = 1) = σ(W1 tanh(W2v (L) i )) (2)\nwhere W1 and W2 are learnable parameters. Bias terms are omitted for simplicity. We construct ground-truth labels by treating a node as a focus if it contains words used in the question.\nTo generate the question, we use the gating mechanism to inform the focus prediction results, where new node representations after being weighted by the focus probability are:\nv (L)′\ni = gi v (L) i gi = pfocus(vi = 1) (3)\nOur model benefits from both large pre-training and hybrid semantic graphs by adding a separate cross attention for node presentations in each BART decoder layer. We then design separate cross attentions to attend (1) the output of the BART encoder, yielding ze, and (2) the node representations V (L) ′ , producing zv, which are formulated as:\nze = LN(zs + Attn(zs,H)) (4) zv = LN(ze + Attn(ze,V (L) ′ )) (5) z′ = LN(zv + FFN(zv)) (6)\nwhere zs denotes the output of self attentions for the current layer, and z′ is the output for the layer. Attn(·, ·) denotes the multi-head attention operation as in Vaswani et al. (2017), FFN(·) a feedforward layer, and LN(·) is layer normalization.\nOur final training objective accounts for both focus prediction and question generation objectives with equal weights."
    }, {
      "heading" : "4.2 Diversifying Questions with Templates",
      "text" : "(EXPLGEN & TPLGEN)\nAn important goal of this work is to enable the generation of questions of diverse types. However, simply adding question type as input is insufficient (discussed in § 5). We thus propose to leverage question templates to gain stronger controllability. Below we first present how to automatically\nextract templates from the training set, and then introduce two model variants that are built on the JOINTGEN framework: EXPLGEN uses exemplar templates to guide the model to generate questions of selected types, and TPLGEN adds an extra step to first generate type-specific templates.\nTemplate Extraction. While collecting templates specific to a given type, we need to ensure they remain topic-independent to be generalizable to different domains. To this end, we replace a word in the question with a template token that indicates its syntax function, e.g., [V] for a verb, if it appears in the answer after lemmatization. We further consider topically related words in the questions, by calculating word-level semantic similarities based on Numberbatch word embeddings (Speer et al., 2017), which are found to perform better on our datasets than other embeddings. Concretely, for each word in the answer, we replace the most similar word in the question with the template token. This process is repeated until 80% of content words in questions are replaced. Finally, for each noun phrase, adjective phrase, and adverb phrase, if its head word has been replaced, the whole phrase is transformed into a phrase type token. For instance, a question “What are the differences between global warming and climate change?” becomes “What are the differences between [NP] and [NP]?”\nExemplars for Guidance (EXPLGEN). Our first model variant considers adding a template exemplar for the given type as additional input, which provide more specific information to control the type of generated questions. Figure 2 shows one such example. To identify exemplars, we use templates with frequencies above 20 on Yahoo and 50 on Reddit. We then manually inspect these templates and remove the ones with topic-specific words, resulting in 66 exemplars for all types. They are listed in Table 10 in Appendix D.\nDuring training, we choose the exemplar that has the lowest edit distance with the question, which is also used for training an exemplar selector based on RoBERTa. During testing, the exemplar with the highest selector score is used. The accuracy of the exemplar selector for each question type on the test set is reported in Table 11 in Appendix D.\nGenerated Templates for Guidance (TPLGEN). We further propose another model variant where we generate a new template and feed it (instead of an exemplar template as in EXPLGEN) as part\nof the question generation input. Specifically, we reuse EXPLGEN to learn to generate a target template, as derived from the template extraction procedure. During question realization, TPLGEN uses a BART-based generator that takes as input the question type, the input text, the generated template, and the words that are predicted as focuses. We use separate cross attentions to attend the representations of the focused words, similar to how node representations are attended in JOINTGEN.\nWe recognize that having separate stages of exemplar selection and template generation introduces extra model training cost and potential errors in the pipeline. This work, however, focuses on improving the controllability as well as diversity of question generation, and we will leave the building of more efficient models in the future work."
    }, {
      "heading" : "5 Experiment Results",
      "text" : ""
    }, {
      "heading" : "5.1 Automatic Evaluation",
      "text" : "Comparisons and Metrics. We compare with DEEPQG (Pan et al., 2020), a model that uses dependency graphs for multi-hop question generation. We also compare with BART models that are finetuned on the same datasets as in our models, by using inputs of (1) the answer (BART), (2) the answer and a predicted question word (BART+QWORD), and (3) the answer and a predicted question type (BART+QTYPE). For BART+QWORD, the question word is predicted by a RoBERTa classifier that considers the answer and is trained on our training sets. We follow Liu et al. (2020) and use 9 categories of question words. For both our models and BART+QTYPE, the most confident type predicted by the classifier γa (described in § 3.2), which reads in the answer, is used as input. To test the efficacy of semantic graphs, we further compare with a variant of JOINTGEN that only uses the flat Transformer for focus prediction and question generation, denoted as JOINTGEN w/o graph.\nWe evaluate the generated questions with BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004).5\nResults on both Yahoo and Reddit datasets are reported in Table 2. Our JOINTGEN outperforms all comparisons on both datasets over all automatic evaluation metrics except for METEOR on Reddit. When taking out the semantic graphs, model performance degrades substantially, which suggests that\n5We do not consider using Q-BLEU (Nema and Khapra, 2018) since it weighs question words highly.\nhaving structured representation is useful for focus detection and the final question generation task. We also observe a huge performance gap between DEEPQG and systems based on BART, signifying the importance of leveraging pre-trained models for open-ended question generation. Meanwhile, adding question types helps BART generate more relevant questions than using question words, indicating the value of our new question type ontology.\nNotably, our template-based generators, EXPLGEN and TPLGEN, which are trained to comply with the given templates, still produce comparable scores. This highlights the possibility to control the generated questions’ types and syntax as demonstrated by the templates, without performance loss.\nQuestion Diversity Evaluation. Next, we exam-\nine the controllability of models by specifying different question types as input. The top 9 confident types6 predicted by our type predictor γa are used as input to our models, producing 9 questions for evaluation. For BART, we use nucleus sampling (Holtzman et al., 2020) with k = 10 and p = 0.7 to sample diverse questions.\nTo evaluate, we first calculate the question type accuracy by comparing whether the types of the generated questions match the specified ones, with types labeled by our classifier γq (§ 3.2). We then report the average numbers of unique question types in the 9 generated questions per sample, with higher number indicating better controllability. Finally, we consider pairwise BLEU-4 (Cho et al., 2019) by computing the BLEU-4 between pairwise generated questions per sample, where lower values suggest higher content diversity.\nFirst, our EXPLGEN and TPLGEN can generate questions with diverse types and content, as shown by the significantly higher numbers of unique types than all comparisons and lower pairwise BLEU scores than comparisons except for BART with nucleus sampling in Table 3. This implies stronger type control by template-based generators, compared to BART+QTYPE and JOINTGEN which only use the question type token as input. Results on numbers of unique types by varying numbers of question types specified in the input are displayed in Figure 3, where EXPLGEN and TPLGEN maintain steady controllability.\nSecond, our question type ontology provides a new perspective for question diversity evaluation. Among the comparisons, although BART with nucleus sampling and BART+QWORD both have low pairwise BLEU, the types of questions they can generate are limited.\n69 types are chosen because we only have 9 categories of question words for BART+QWORD."
    }, {
      "heading" : "5.2 Human Evaluation",
      "text" : "Question Diversity. We hire three annotators who have participated in our question type annotation study to evaluate 80 groups of questions generated by four selected models on each dataset. For each group, we randomly sample an answer and indicate three most probably question types to each model, to generate three corresponding questions.\nFor each sample, the annotators are asked to rank the four models from 1 (highest) to 4 (lowest) on three aspects of diversities: type–whether the three generated questions have different types, syntax–whether they use different syntax, and answer content–whether the three questions need to be addressed with different answers. Ties are allowed.\nWe find that human judges rate questions generated by our EXPLGEN and TPLGEN as having greater diversities over all aspects, except for syntax diversity on Reddit, as shown in Table 4. Among the two model variants, questions by TPLGEN yield more diverse answers. Based on our observation, TPLGEN uses automatically generated templates to produce more focused questions with different answers, compared to EXPLGEN which employs exemplars. This shows the promise of using automatically generated templates to create questions that need to be addressed with different answers. Besides Figure 1, we show more sample outputs in Figure 4, where EXPLGEN and TPLGEN exhibit stronger controllability than JOINTGEN. Question Content Quality. We use the same set of human judges to evaluate another 80 groups of questions output by five selected models and the reference. Three aspects are rated from 1 (worst)\nor the generated templates are colored with blue. Generated questions that do not match the given type are marked by strikethrough.\nto 5 (best): appropriateness–whether the question is semantically correct, without considering the answer; answerability–whether the question can be addressed by the given answer; and scope– whether the question is related to a longer span of the answer (global scope) or focuses on local content (e.g., one phrase or one sentence). We further ask the annotators to rank questions based on their overall quality and preferences, with ties allowed.\nAs shown in Table 5, our JOINTGEN model produces questions with better answerability and that cover broader content in the answers. It is also rated as the best in more than half of the evaluation instances on both datasets. Between BART+QWORD and BART+QTYPE, human judges rate the system outputs that conditioned on our question types to have better overall quality."
    }, {
      "heading" : "5.3 Further Analyses",
      "text" : "Does focus prediction correlate with question quality? We first investigate the relationship between focus prediction and question generation by using our joint model JOINTGEN. As can be seen from Figure 5, there is a strong correlation between F1 scores of focus prediction and BLEU-4 as well\nas ROUGE-L, where samples in the Yahoo and Reddit test sets are grouped into 8 bins based on the F1 scores. The Pearson correlation coefficients between BLEU-4 and focus F1 are 0.29 on Yahoo and 0.26 on Reddit. For ROUGE-L, the correlation coefficients are 0.35 on Yahoo and 0.34 on Reddit. All the correlations have p < 10−5. The strong positive correlations imply the importance of accurate focus prediction for open-ended question generation. We also show the F1 scores and BLEU-4 for selected question types on the right of Figure 5, again demonstrating the effect of focus detection on question quality.\nWhen do our models fail to respect the given types? Next, we provide insights into which types of questions are challenging to generate by using our template-based models EXPLGEN and TPLGEN. Both variants frequently fail to respect the given question type of VERIFICATION, in which cases they often produce JUDGEMENTAL questions. They also tend to confuse EXAMPLE and EXTENT with CONCEPT questions. After manually inspecting 50 generated questions for the aforementioned three types, we find that many of them can be labeled with both types, thus creating confusion for our classifier. For instance, “What are the import restrictions in the US?” can be considered as either\nasking for a definition or for examples. Therefore, future work should include designing multi-class type identification models."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation. We further annotate a new dataset with 4,959 questions based on the proposed ontology. We describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation, which is directly built on large pre-trained models. Based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates. Experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons, with similar results rated by human judges."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported in part by National Science Foundation through Grants IIS-1813341 and a CAREER award IIS-2046016. We thank three anonymous reviewers, area chair, and senior area chairs for their valuable suggestions for improving various aspects of this work.\nEthics Statement\nLarge models that are pre-trained on heterogeneous web data are shown to encode biases and can be potentially harmful for marginalized populations.\nWhile the automatically learned templates improve controllability in question generation, we also recognize that our system might be misused to create questions that contain objectionable content. We therefore advocate cautious and responsible practices in real-world deployment.\nOur data collection process for the two new datasets involves removing samples with abusive languages and human inspection on random samples. Given the data volume, however, we cannot exhaustively verify that all records are free of potentially offensive content."
    }, {
      "heading" : "A Data Collection",
      "text" : "Data Filtering. After collecting the raw data from Yahoo and Reddit, we design rules to filter out ill-formed answers and questions. These rules are listed in Table 6. Finally, we conduct human inspection on random samples from the two datasets and confirm that samples are all clean and contain open-ended questions.\nQuestion Type Annotation. We include the definition and corresponding examples for each question type in the annotation guideline, as shown in Table 12. We allow annotators to label a question with two types if they cannot decide between the two. All recruited annotators are U.S. college students, and are paid $15 per hour for the task. On average, it takes 3.5 hours to annotate 1000 questions.\n7https://gist.github.com/jamiew/ 1112488\n8https://en.wikipedia.org/wiki/List_ of_emoticons\n9https://github.com/dwyl/english-words\nFor samples with disagreed labels, we check whether agreement can be reached by considering both labeled types. For example, if annotator A labels VERIFICATION and JUDGMENTAL, and annotator B labels JUDGMENTAL, the agreed-upon type is JUDGMENTAL. We then resolve outstanding disagreements by discussion."
    }, {
      "heading" : "B Details for Question Type Classifiers",
      "text" : "To train the question type classifier γq that reads the question as input, we split the collected question type dataset into training, validation, and test sets. Sample counts and question type distributions for different data splits are shown in Table 7.\nWe then use γq to identify types for unlabeled questions in Yahoo and Reddit. The question type distributions for the two datasets are shown in Table 8."
    }, {
      "heading" : "C Details for Graph Construction",
      "text" : "We discard secondary dependency relations for graph construction, including case, mark, cc, cc:preconj, aux, aux:pass, cop, det, discourse, expl,\ndet:predet, punct, ref. The definition for each dependency can be found in Universal Dependency.10"
    }, {
      "heading" : "D Details for Templates and Exemplars",
      "text" : "Template Construction. To avoid replacing words that are representative of question types during template construction, we maintain a list of words not to be replaced for each question type, as shown in Table 9. These words are identified by frequency with additional manual inspection.\nExemplar Collection. Table 10 lists the collected template exemplars for different question types.\nExemplar Classifiers. To predict the exemplars used for question decoding, we train one exemplar classifier for each question type, on each dataset. Accuracy values of these exemplar classifiers on the reserved test sets are listed in Table 11."
    }, {
      "heading" : "E Details for Implementation",
      "text" : "We use Fairseq (Ott et al., 2019) to build our models and conduct training and decoding. For the Graph Attention Networks (GATs) in our focus predictor, we adopt the implementation by PyTorch Geometric (Fey and Lenssen, 2019). All our experiments are conducted on a Quadro RTX 8000 GPU with 48 GB of memory.\nTraining Settings. We use Adam (Kingma and Ba, 2014) for the training of all our models. Our question type classifiers and template exemplar classifiers are trained with a maximum learning rate of 1× 10−5 and a batch size of 32. For training generation models, the maximum learning rate is 3×10−5 and each batch contains at most 32,768\n10https://universaldependencies.org/\ntokens. Mixed-precision training is adopted for all\nQuestion Type Yahoo Acc Reddit Acc\nmodels except for models with GATs.\nDecoding Settings. We use beam search for decoding. A beam size of 5 and a length penalty of 1.5 are used for all models. Repeated trigram blocking is applied to question generation. The minimum and maximum lengths for generation are set to 1 and 100, respectively.\nModel Parameters. The question type classifiers and template exemplar classifiers are based on RoBERTaLarge, which has 355M parameters. Our generation model builds a GAT upon the BART model, containing 430M parameters in total.\nRunning Time. Training question type classifiers takes 23 hours. Due to the difference in training data size, the training time for template exemplar classifiers ranges from 20 minutes to 3 hours. For our generation model with focus prediction, it takes 6 hours to train on Yahoo and 12 hours to train on Reddit. Decoding on the test set of Yahoo and Reddit takes 8 minutes and 15 minutes, respectively."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2021,
    "abstractText" : "We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4, 959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity.",
    "creator" : "LaTeX with hyperref"
  }
}