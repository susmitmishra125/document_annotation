{
  "name" : "2021.acl-long.365.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs",
    "authors" : [ "Zixuan Li", "Xiaolong Jin", "Saiping Guan", "Wei Li", "Jiafeng Guo", "Yuanzhuo Wang", "Xueqi Cheng" ],
    "emails" : [ "lizixuan@ict.ac.cn", "jinxiaolong@ict.ac.cn", "guansaiping@ict.ac.cn", "liwei85@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4732–4743\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4732"
    }, {
      "heading" : "1 Introduction",
      "text" : "Temporal Knowledge Graphs (TKGs) (Boschee et al., 2015; Gottschalk and Demidova, 2018, 2019; Zhao, 2020) have emerged as a very active research area over the last few years. Each fact in TKGs has a timestamp indicating its time of occurrence. For example, the fact, (COVID-19, New medical case occur, Shop, 2020-10-2), indicates that a new medical case of COVID-19 occurred in a shop on 2020-10-2. In this paper, reasoning on TKGs aims to predict future facts (events) for timestamp t > tT , where tT is assumed to be the current timestamp (Jin et al., 2020). An example of the task is shown in Figure 1, which attempts to answer the query (COVID-19, New medical case occur, ?, 2020-12-23) with the given historical facts. Obviously, such a task may benefit many practical\napplications, such as, emerging events response (Muthiah et al., 2015; Phillips et al., 2017; Korkmaz et al., 2015), disaster relief (Signorini et al., 2011), and financial analysis (Bollen et al., 2011).\nHow do human beings predict future events? According to the dual process theory (Evans, 1984, 2003, 2008; Sloman, 1996), the first thing is to search the massive-capacity memories and find some related historical information (i.e., clues) intuitively. As shown in the left part of Figure 1, there are mainly three categories of clues vital to the query: 1) the 1-hop paths with the same relation to the query (thus called repetitive 1-hop paths), such as (COVID-19, New medical case occur, Shop); 2) the 1-hop paths with relations different from the query (called non-repetitive 1-hop paths), such as (COVID-19, New suspected case occur, Bank); and 3) the 2-hop paths, such as (COVID19, Diagnose−1, The man, Go to, Police station). Human beings recall these clues from their memories and have some intuitive candidate answers for the query. Secondly, human beings get the accurate answer by diving deeper into the clues’\ntemporal information and performing a meticulous reasoning process. As shown in the right part of Figure 1, the man went to the police station more than two months earlier than the time when he was diagnosed with COVID-19, indicating that Police station is probably not the answer. Finally, human beings derive the answer, Shop.\nExisting models mainly focus on the above second process but underestimate the first process. Some recent studies (Trivedi et al., 2017, 2018) learn the evolving embeddings of entities with all historical facts considered. However, only a few historical facts are useful for a specific prediction. Thus, some other studies (Jin et al., 2020, 2019; Zhu et al., 2020) mainly focus on encoding the 1-hop repetitive paths (repetitive facts) in the history. However, besides the 1-hop repetitive paths, there are massive other related information in the datasets. Taking the widely used dataset ICEWS18 (Jin et al., 2020) as an example, 41.2% of the training queries can get the answers through the 1-hop repetitive paths in the history. But, almost 64.6% of them can get the answers through 1- hop repetitive and non-repetitive paths, and 86.2% through the 1-hop and 2-hop paths.\nThus, we propose a new model called CluSTeR, consisting of two stages, Clue Searching (Stage 1) and Temporal Reasoning (Stage 2). At Stage 1, CluSTeR formalizes clue-searching as a Markov Decision Process (MDP) (Sutton and Barto, 2018) and learns a beam search policy to solve it. At Stage 2, CluSTeR reorganizes the clues found in Stage 1 into a series of graphs and then a Graph Convolution Network (GCN) and a Gated Recurrent Unit (GRU) are employed to deduce accurate answers from the graphs.\nIn general, this paper makes the following contributions:\n• We formulate the TKG reasoning task from the view of human cognition and propose a two-stage model, CluSTeR, which is mainly composed of a RL-based clue searching stage and a GCN-based temporal reasoning stage.\n• We advocate the importance of clue searching for the first time, and propose to learn a beam search policy via RL, which can find explicit and reliable clues for the fact to be predicted.\n• Experiments demonstrate that CluSTeR achieves consistently and significantly better performance on popular TKGs and the clues\nfound by CluSTeR can provide interpretability for the reasoning results."
    }, {
      "heading" : "2 Related Work",
      "text" : "Static KG Reasoning. Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs.\nHowever, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs.\nTemporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al. (2020). Under the former setting, models attempt to infer missing facts at historical timestamps. While the latter setting, which this paper focuses on, attempts to predict facts in the future. Orthogonal to our work, Trivedi et al. (2017, 2018) estimate the conditional probability of observing a future fact via a temporal point process taking all historical facts into consideration. Although Han et al. (2020b) extends temporal point process to model concurrent facts, they are more capable of modeling TKGs with continuous time, where no events may occur at the same timestamp. Glean (Deng et al., 2020) incorporates a word graph constructed by the summary texts of events into TKG reasoning. The most related works are RE-NET (Jin et al., 2020) and CyGNet (Zhu et al., 2020). RE-NET uses a subgraph aggregator and GRU to model the subgraph sequence consist-\ning of 1-hop facts. CyGNet uses a sequential copy network to model repetitive facts. Both of them use heuristic strategies in the clue searching stage, which may lose lots of other informative historical facts or engage some noise. Although the above two models attempt to consider other information by pre-trained global embeddings or an extra generation model, they still mainly focus on modeling repetitive facts. Besides, all the models almost can not provide interpretability for the results."
    }, {
      "heading" : "3 The Proposed CluSTeR Model",
      "text" : "We start with the notations, then introduce the model as well as its training procedure in detail."
    }, {
      "heading" : "3.1 Notations",
      "text" : "A TKG G is a multi-relational directed graph with time-stamped edges between entities. A fact in G can be formalized as a quadruple (es, r, eo, t). It describes that a fact of relation type r ∈ R occurs between subject entity es ∈ E and object entity eo ∈ E at timestamp t ∈ T , where R, E and T denote the sets of relations, entities and timestamps, respectively. TKG reasoning aims to predict the missing object entity of (es, rq, ?, ts) or the missing subject entity of (?, rq, eo, ts) given the set of historical facts before ts, denoted as G0:ts−1. Without loss of generality, in this paper, we predict the missing object entity in a fact, and the model can be easily extended to predicting the subject entity.\nIn this paper, a clue path is in the form of (es, r1, e1, ..., rk, ek, ..., rI , eI), where ek ∈ E , rk ∈ R, k = 1, ..., I , I is the maximum step number and each hop in the path can be viewed as a triple (ek−1, rk, ek). Note that, e0 = es. The clue facts are derived from the clue paths via mapping each hop (ek−1, rk, ek) in the paths to corresponding facts (ek−1, rk, ek, t1), (ek−1, rk, ek, t2, ...) ∈ G0:ts−1."
    }, {
      "heading" : "3.2 Model Overview",
      "text" : "As illustrated in Figure 2, the model consists of two stages, clue searching and temporal reasoning. The two stages are coordinated to perform fast and slow thinking (Daniel, 2017), respectively, to solve the TKG reasoning task, inspired by human cognition. Specifically, Stage 1 mainly focuses on searching the clue paths of which the compositional semantic information relates to the given query with the time constraints. Then, the clue paths and the consequent candidate entities are provided for the reasoning in Stage 2, which mainly focuses on meticulously modeling the temporal information among clue facts and gets the final results. In the CluSTeR model, these two stages interact with each other in the training phase and decide the final answer jointly in the inference phase."
    }, {
      "heading" : "3.3 Stage 1: Clue Searching",
      "text" : "The purpose of Stage 1 is to search and induce the clue paths related to the given query (es, rq, ?, ts) from history. The previous studies (Jin et al., 2019, 2020; Zhu et al., 2020) use heuristic strategies to extract 1-hop repetitive paths, losing lots of other informative clue paths. Besides, there are enormous facts in the history. Thus, a learnable and efficient clue searching strategy is of great necessity. Motivated by these observations, Stage 1 can be viewed as a sequential decision problem and solved by the RL system."
    }, {
      "heading" : "3.3.1 The RL System",
      "text" : "The RL system consists of two parts, the agent and the environment. We formulate the RL system as an MDP, which is a framework of learning from interactions between the agent and the environment to find B promising clue paths. Starting from es, the agent sequentially selects outgoing edges via randomized beam search strategy, and traverses to\nnew entities until it reaches the maximum step I . The MDP consists of the following parts:\nStates. Each state si = (ei, ti, es, rq, ts) ∈ S is a tuple, where S is the set of all the available states; ei (e0 = es) is the entity where the agent visited at step i; and ti (t0 = ts) is the timestamp of the action taken at the previous step. Note that, es, rq, and ts are shared by all the states for the given query.\nTime-constrained Actions. Compared to static KGs, the time dimension of TKGs leads to an explosively large action space. Besides, the human memories focus on the lastest occcuring events. Thus, we constrain the time interval between the timestamp of each fact and ts to be no more than m. And the time interval between the timestamp of the previous action and each available action is no more than ∆. Therefore, the set of the possible actions Ai ∈ A (A is the set of all available actions) at step i consists of the time-constrained outgoing edges of ei,\nAi = {(r′, e′, t′)|(ei, r′, e′, t′) ∈ G0:ts−1, |t′ − ti| ≤ ∆, ts − t′ ≤ m}. (1)\nTo give the agent an adaptive option to terminate, a self-loop edge is added to Ai.\nTransition. A transition function δ : S ×A → S is deterministic under the situation of TKG and just updates the state to new entities incident to the actions selected by the agent.\nRewards. The agent only receives a terminal reward R at the end of search, which is the sum of two parts, binary reward and real value reward. The binary reward is set to 1 if the destination entity eI is the correct target entity eo, and 0 otherwise. Besides, the agent gets a real value reward r̂ from Stage 2 if eI is the target entity, which will be introduced in Section 3.4."
    }, {
      "heading" : "3.3.2 Semantic Policy Network",
      "text" : "Given the time-constrained action space, the compositional semantic information implied in the clue paths and the time information of the clue facts is vital for reasoning. However, considering that modeling the time information requires to dive deeply into the complex temporal patterns of facts and is not the emphasis of Stage 1. Thus, we design a semantic policy network which calculates the probability distribution over all the actions according to the current state si and search history hi = (es, a0, ..., ai−1) without considering timestamps in Stage 1. Here, ai = (ri+1, ei+1, ti+1) is\nthe action taken at step i = 0, ..., I − 1. Note that, h0 is es. Actually, the search history without timestamps is a candidate clue path (a clue path at step i) mentioned in Section 3.1.\nThe embedding of the action ai is ai = ri+1 ⊕ ei+1, where ⊕ is the concatenation operation; ri+1, ei+1 are the embeddings of ri+1 and ei+1, correspondingly. Then, a Long Short Term Memory network (LSTM) is applied to encode the candidate clue path hi as a continuous vector hi,\nhi = LSTM(hi−1,ai−1), (2)\nwhere the initial hidden embedding h0 equals to LSTM(0, rdummy ⊕ es) and rdummy is the embedding of a special relation introduced to form a start action with es. For step i, the action space is encoded by stacking the embeddings of all the actions in Ai, which are denoted as Ai ∈ R|Ai|×2d. Here, d is the dimension of entity embeddings and relation embeddings. Then, the policy network calculates the distribution π over all the actions by a Multi-Layer Perceptron (MLP) parameterized with W1 and W2 as follows:\nπ(ai|si;Θ)=η(AiW2f(W1[ei ⊕ hi ⊕ rq]), (3)\nwhere η(·) is the softmax function, f(·) is the ReLU function (Glorot et al., 2011) and Θ is the set of all the learnable parameters in Stage 1."
    }, {
      "heading" : "3.3.3 Randomized Beam Search",
      "text" : "In the scenario of TKGs, the occurrence of a fact may result from multiple factors. Thus, multiple clue paths are necessary for the prediction. Besides, the intuitive candidates from Stage 1 should recall the right answers as many as possible. Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent.\nSpecifically, a beam contains B candidate clue paths at step i. For each candidate path, we append B most likely actions (according to Equation 3) to the end of the path, resulting in a new path pool with size B ×B. Then we either pick the highestscoring paths with probability µ or uniformly sample a random path with probability 1−µ repeatedly for B times. The score of each candidate clue path at step i equals to ∑i k=0 log π(ak|sk; Θ). Note that, at the first step, B 1-hop candidate paths starting from es are generated by choosing B paths via the above picking strategy."
    }, {
      "heading" : "3.4 Stage 2: Temporal Reasoning",
      "text" : "To dive deeper into the temporal information among clue facts at different timestamps and the structural information among concurrent clue facts, Stage 2 reorganizes all clue facts into a sequence of graphs Ĝ = {Ĝ0, ..., Ĝj , ..., Ĝts−1}, where each Ĝj is a multi-relational graph consisting of clue facts at timestamp j = 0, ...ts − 1. We use an ω-layer RGCN (Schlichtkrull et al., 2018) to model Ĝj ,\nĥl+1o,j = f 1 do ∑ (s,r)|(s,r,o,j)∈Ĝj Wlrĥ l s,j+W l loopĥ l o,j , (4) where ĥlo,j and ĥ l s,j denote the l\nth layer embeddings of entities o and s in Ĝj at timestamp j, respectively; Wlr and W l loop are the weight matrices for aggregating features from different relations and self-loop in the lth layer; do is the in-degree of entity o; the input embedding for each entity k, ĥl=0k,j is set to êk , which is different from that of Stage 1.\nThen, ĝj , the embedding of Ĝj , is calculated by the mean pooling operation of all entity embeddings calculated by Equation 4 in Ĝj . The concatenation of ês, ĝj and r̂q (the embedding of rq in Stage 2) is fed into a GRU,\nHj = GRU([ês ⊕ ĝj ⊕ r̂q],Hj−1). (5)\nThe final output of GRU, denoted as Hts−1, is fed into a MLP decoder parameterized with Wmlp to get the final scores for all the entities, i.e.,\np(e|es, rq, ts) = σ(HTts−1 ·Wmlp), (6)\nwhere σ is the sigmoid activation function. Finally, we re-rank the candidate entities according to Equation 6. To give a positive feedback to the clue paths arriving at the answer, Stage 2 gives a beam-level reward which equals to the final score of eI from Equation 6, i.e, r̂ = p(eI), to Stage 1."
    }, {
      "heading" : "3.5 Training Strategy",
      "text" : "For Stage 1, the beam search policy network is trained by maximizing the expected reward over all queries in the training set,\nJ (Θ)=E(es,rq ,eo,ts)∈G [Ea0,...aI−1 [R(eI |es, rq, ts)]]. (7)\nThe REINFORCE algorithm (Williams, 1992) is used to optimize Equation 7. For Stage 2, we\ndefine the objective function using cross-entropy:\nJ (Φ)=− 1 |G| ∑ (es,rq ,eo,ts)∈G log p(eo|es, rq, ts),\n(8) where Φ is the set of all the learnable parameters in Stage 2. The Adam (Kingma and Ba, 2014) optimizer is used to minimize Equation 8. As Stages 1 and Stage 2 are correlated mutually, they are trained jointly. Stage 1 is pre-trained with only binary reward before the joint training process starts. Then Stage 2 is trained with the parameters of Stage 1 frozen. At last, we jointly train the two stages. Such a training strategy is widely used by other RL studies (Bahdanau et al., 2016; Feng et al., 2018)."
    }, {
      "heading" : "4 Experiment",
      "text" : "We design experiments to answer the following questions: Q1. How does CluSTeR perform on the TKG reasoning task? Q2. How do the two stages contribute to the final results respectively? Q3. Which clues are found and used for reasoning? Q4. Can CluSTeR provide some interpretability for the results?"
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "Datasets and Metrics. There are four typical TKGs commonly used in previous studies, namely, ICEWS14 (Garcı́a-Durán et al., 2018), ICEWS0515 (Garcı́a-Durán et al., 2018), ICEWS18 (Jin et al., 2019) and GDELT (Jin et al., 2020). The first three datasets are from the Integrated Crisis Early Warning System (ICEWS) (Boschee et al., 2015) and the last one is from Global Database of Events, Language, and Tone (GDELT) (Leetaru and Schrodt, 2013). We evaluate CluSTeR on all these datasets. ICEWS14 and ICEWS05-15 are divided into training, validation, and test sets following the preprocessing on ICEWS18 in RE-NET (Jin et al., 2020). The details of the datasets are presented in Table 1.\nIn the experiments, the widely used Mean Reciprocal Rank (MRR) and Hits@{1,10} are employed as the metrics. Without loss of generality, only the experimental results under the raw setting are\nreported. The filtered setting is not suitable for the reasoning task under the exploration setting, as mentioned in (Han et al., 2020b; Ding et al., 2021; Jain et al., 2020). The reason is explained in terms of an example as follows: Given a test quadruple (Barack Obama, visit,?, 2015-1-25) with the correct answer India. Assume there is a quadruple (Barack Obama, visit, Germany, 2013-1-18) in the training set. The filtered setting used in the previous studies ignores time information and considers (Barack Obama, visit, Germany, 2015- 1-25) to be valid because (Barack Obama, visit, Germany, 2013-1-18) appears in the training set. It thus removes the quadruple from the corrupted ones. However, the fact (Barack Obama, visit, Germany) is temporally valid on 2013-1-18, instead of 2015-1-25. Therefore, to test the quadruple (Barack Obama, visit,?, 2015-1-25), (Barack Obama, visit, Germany, 2015-1-18) should not be removed. In this way, the filtered setting wrongly removes quite a lot of quadruples and thus leads to over-optimistic experimental performance.\nBaselines. The CluSTeR model is compared with two categories of models, i.e., models for static KG reasoning and models for TKG reasoning under the exploration setting. The typical static models DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), RGCN (Schlichtkrull et al., 2018), ConvE (Dettmers et al., 2018) and RotaE (Sun et al., 2018) are selected with the temporal information of facts ignored. We also choose MINERVA (Das et al., 2018), the RL-based multi-hop reasoning model, as the baseline. For TKG models, the representative Know-evolve (Trivedi et al., 2017), DyRep (Trivedi et al., 2018), CyGNet (Zhu et al., 2020) and RE-NET (Jin et al., 2020) are selected. Besides, following RE-NET (Jin et al., 2020), we extend two models for temporal homogeneous graphs, GCRN (Seo et al., 2018) and EvolveGCN-O (Pareja et al., 2019)), to RGCRN and EvolveRGCN by replacing GCN with RGCN. We use ConvE (Dettmers et al., 2018), a more stronger decoder to replace the MLP (Jin et al., 2020) for the two models. For Know-evolve and DyRep, RE-NET extends them to TKG reasoning task but does not release their codes. Thus, we only report the results from their papers. For other baselines, we reproduce all the results with the optimal parameters tuning on the validation set.\nImplementation Details. In the experiments, the embedding dimension d for the two stages, is\nset to 200. For Stage 1, we adopt an adaptive approach for selecting the time interval m. Specifically, for ICEWS14, ICEWS05-15, and GDELT,m is set to the last one timestamp the query pattern (es, rq, ?) appearing in the dataset before ts. And for ICEWS18, m is set to the last third timestamp. ∆ is set to 3 for all the datasets. We set the maximum step number I = 1, 2 and find I = 1 is better for all the datasets. The number of the LSTM layers is set to 2 and the dimension of the hidden layer of LSTM is set to 200 for all the datasets. The beam size is set to 32 for the three ICEWS datasets and 64 for GDELT. µ is set to 0.3 for all the datasets. For Stage 2, the maximum sequence length of GRU is set to 10, the number of the GRU layers is set to 1 and the number of the RGCN layers is set to 2 for all the datasets. For each fact in G0:ts−1, we add the corresponding inverse fact into G0:ts−1. All the experiments are carried out on Tesla V100."
    }, {
      "heading" : "4.2 Results on TKG Reasoning",
      "text" : "The results on TKG reasoning are presented in Table 2. CluSTeR consistently outperforms the baselines on all the ICEWS datasets, which convincingly verifies its effectiveness and answers Q1. Especially on ICEWS14, CluSTeR even achieves the improvements of 7.1% in MRR, 4.5% in Hits@1, and 13.7% in Hits@10 over the best baselines. Specifically, CluSTeR significantly outperforms the static models (i.e., those in the first block of Table 2) because it captures the temporal information of some important history. Moreover, CluSTeR drastically performs better than those temporal models. Compared with DyRep and Know-evolve that consider all the history, CluSTeR can focus on more vital clues. Different from RGCRN and EvolveRGCN which model all history from several latest timestamps, CluSTeR models a longer history after reducing all history to a few clues. CyGNet and RE-NET mainly focus on modeling the repetitive clues or all the 1-hop clues and show strong performance. CluSTeR also outperforms them on the three ICEWS datasets, because the RL-based Stage 1 can find more explicit and reliable clues.\nThe experimental results on GDELT demonstrate that the performance of static models and temporal ones are similarly poor, as compared with those of the other three datasets. We further analyze the GDELT dataset and find that a large number of its entities are abstract concepts which do not indicate a specific entity (e.g., PRESIDENT, POLICE\nand GOVERNMENT). Among the top 50 frequent entities, 28 are abstract concepts and 43.72% corresponding events involve abstract concepts. Those abstract concepts make future prediction under the raw setting almost impossible, since we cannot predict a president’s activities without knowing which country he belongs to."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "To answer Q2, i.e., how the two stages contribute to the final results, we report the MRR results of the variants of CluSTeR on the validation set of all the datasets in Table 3. The first two lines of Table 3 show the results only using Stage 1, where the maximum step I is set to 1 and 2, respectively. Following Lin et al. (2018), the score of the target entity is set to the highest score among the paths when more than one path leads to it. It can be observed that the results decrease when only using Stage 1, because the temporal information among facts is ignored. The third line shows the results only using Stage 2 with extracted 1-hop repetitive clues as the inputs. The results decrease on all the ICEWS datasets when only using Stage 2, demonstrating that only repetitive clues are not enough for the prediction. For GDELT, only Stage 2 achieves the best results, which demonstrates that only using repetitive clues is effective enough for it. It is\nbecause that only using the most straightforward repetitive clues in Stage 2 can alleviate the influence of noise produced by abstract concepts. It also matches our observations mentioned in Section 4.2.\nFrom the first two lines of Table 3, it can be seen that the performance of Stage 1 decreases when I is set to 2. To further analyze the reason, we extract paths from ICEWS18 without considering timestamps via AMIE+ (Galárraga et al., 2015), a widely used and accurate approach to extract logic rules (paths) in static KGs. We check the top fifty paths manually and present the top five convincing paths in Table 4. It can be seen that there are no strong dependencies between the query relations and the 2-hop paths. Thus, in this situation, longer paths bring exponential noise clues, which pull down the precision. We do experiments on all the datasets from ICEWS and GDELT and find the same conclusion. We leave it for future work to construct a more complex dataset for verifying the effectiveness of multi-hop clue paths."
    }, {
      "heading" : "4.4 Detail Analysis",
      "text" : "To answer Q3, we show some non-repetitive clues found in Stage 1 in Figure 3. We use (relation in 1-hop non-repetitive clue path, query relation) pairs on ICEWS18 to construct a clue graph. Arrows begin with the relations in the clue paths and end with the query relations. It is interesting to find that CluSTeR can actually find some causal relations. Moreover, compared to the 2-hop clue paths shown in Table 4, the 1-hop clue paths are more informative. It also gives explanations to the outperformance of the 1-hop clue paths.\nBesides, we illustrate the statistics of clue facts used during Stage 2 in Figure 4. The proportion of the repetitive clue facts is less than 7% and the proportion of the non-repetitive clue facts is more than 93% on the datasets. The abundant of the nonrepetitive clue facts used in Stage 2 also explains the outperformance of CluSTeR to a certain degree."
    }, {
      "heading" : "4.5 Case Study",
      "text" : "To answer Q4, we show how CluSTeR conducts reasoning and explains the results for the given two queris from the test set of ICEWS14 in Figure 5. For the first query: (Congress (United States), Impose sanctions, ?, 3341), we choose the top three candidates in Stage 1 and demonstrate some clue paths of the three entities in the left top part of Figure 5. The clue paths like (Congress (United States), Criticize or denounce−1, China), (Congress (United States), Engage in negotiation−1, Iran) give the evidence for candidate entities China and Iran, correspondingly. In Stage 1, CluSTeR has an intuitive candidate set including China, Iran and France. The score of China (-2.69) and Iran (-2.71) are similar but the\n1Here, 334 represents the 334th day in the year 2014.\nwrong answer, China, has a higher score than the right one, Iran. It is because Stage 1 does not take the temporal information into consideration. However, the score gap is obvious between Iran and France, which shows that Stage 1 can measure the qualities of different clue paths and distinguish the semantic-related entities from the others. In Stage 2, CluSTeR reorganizes the clue facts by their timestamps, as shown in the right top part of Figure 5. (Congress (United State), Engage in negotiation−1, Iran, 323) and (Congress (United State), Make a visit, China, 227) make Iran the more possible answer. For the second query: (China, Express intent to settle dispute, ?, 364), clue paths in the left bottom of Figure 5 are all associated with the query. Stage 1 induces all entities to only two entities through these clue paths but misleads to the wrong answer, Iran. Actually, even a human may give the wrong answer with only fasting thinking. After diving into the temporal information of clue facts and conduct slow thinking, some causal information and period information can be captured by Stage 2. Although Sign formal agreement is associated with Express intent to settle dispute, it can not be the reason for the latter. Moreover, from the subgraph sequence in the right bottom part of Figure 5, it can be seen that the cooperation period between China and Japen just begins at 363, but the cooperation period between China and Iran has been going on for several days. (China, Express intent to settle dispute, ?, 364) is more likely to be an antecedent event to the cooperation period and the answer is Japen.\nAbove all, for each fact to be predicted, CluSTeR can provide the clues for each candidate entity, which presents the insight and provides interpretability for the reasoning results. It is similar to the natural thinking pattern of human, in which only explicit and reliable clues are needed."
    }, {
      "heading" : "4.6 Performance under the Time-aware Filtered Setting",
      "text" : "As mentioned in Section 4.1, the widely adopted filtered setting in the existing studies is not suitable\nfor the temporal reasoning task addressed in this paper. The essential problem of the above filtered setting is that it ignores the time information of a fact. Therefore, we also adopt an improved filtered setting where the time information is also considered, thus called time-aware filtered setting (Han et al., 2020b; Han et al.). Specifically, only the facts occur at the predicted time are filtered. The results are in Table 5. It can been seen that the experimental results under the time-aware filtered setting are close to those under the raw setting. This is because that only a very small number of facts are removed under this filtered setting. The results also show the convincing of the raw setting."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we proposed a two-stage model from the view of human cognition, named CluSTeR, for TKG reasoning. CluSTeR consists of a RL-based clue searching stage (Stage 1) and a GCN-based temporal reasoning stage (Stage 2). In Stage 1, CluSTeR finds reliable clue paths from history and generates intuitive candidate entities via RL. With the found clue paths as input, Stage 2 reorganizes\nthe clue facts derived from the clue paths into a sequence of graphs and performs deduction on them to get the answers. By the two stages, the model demonstrates substantial advantages on TKG reasoning. Finally, it should be mentioned that, although the four TKGs adopted in the experiments were created based on the events in the real world, the motivation of this paper is to propose this TKG reasoning model only for scientific research."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We gratefully acknowledge the help and assistance from Long Bai, Yunqi Qiu, Bing Li and Bingbing Xu. Moreover, the work is supported by the National Key Research and Development Program of China under grant 2016YFB1000902, the National Natural Science Foundation of China under grants U1911401, 62002341, 61772501, U1836206 and 61722211, the GFKJ Innovation Program, Beijing Academy of Artificial Intelligence under grant BAAI2019ZD0306, and the Lenovo-CAS Joint Lab Youth Scientist Project."
    } ],
    "references" : [ {
      "title" : "An actor-critic algorithm for sequence prediction",
      "author" : [ "Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1607.07086.",
      "citeRegEx" : "Bahdanau et al\\.,? 2016",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Twitter mood predicts the stock market",
      "author" : [ "Johan Bollen", "Huina Mao", "Xiaojun Zeng." ],
      "venue" : "Journal of computational science, 2(1):1–8.",
      "citeRegEx" : "Bollen et al\\.,? 2011",
      "shortCiteRegEx" : "Bollen et al\\.",
      "year" : 2011
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Icews coded event data",
      "author" : [ "Elizabeth Boschee", "Jennifer Lautenschlager", "Sean OBrien", "Steve Shellman", "James Starz", "Michael Ward." ],
      "venue" : "Harvard Dataverse, 12.",
      "citeRegEx" : "Boschee et al\\.,? 2015",
      "shortCiteRegEx" : "Boschee et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational knowledge graph reasoning",
      "author" : [ "Wenhu Chen", "Wenhan Xiong", "Xifeng Yan", "William Yang Wang." ],
      "venue" : "Proceedings of NAACL-HLT, pages 1823–1832.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Thinking, fast and slow",
      "author" : [ "Kahneman Daniel" ],
      "venue" : null,
      "citeRegEx" : "Daniel.,? \\Q2017\\E",
      "shortCiteRegEx" : "Daniel.",
      "year" : 2017
    }, {
      "title" : "2018. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement",
      "author" : [ "Rajarshi Das", "Shehzaad Dhuliawala", "Manzil Zaheer", "Luke Vilnis", "Ishan Durugkar", "Akshay Krishnamurthy", "Alex Smola", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Das et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2018
    }, {
      "title" : "Hyte: Hyperplane-based temporally aware knowledge graph embedding",
      "author" : [ "Shib Sankar Dasgupta", "Swayambhu Nath Ray", "Partha Talukdar." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Dasgupta et al\\.,? 2018",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic knowledge graph based multievent forecasting",
      "author" : [ "Songgaojun Deng", "Huzefa Rangwala", "Yue Ning." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1585–1595.",
      "citeRegEx" : "Deng et al\\.,? 2020",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional 2d knowledge graph embeddings",
      "author" : [ "Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Dettmers et al\\.,? 2018",
      "shortCiteRegEx" : "Dettmers et al\\.",
      "year" : 2018
    }, {
      "title" : "Temporal knowledge graph forecasting with neural ode",
      "author" : [ "Zifeng Ding", "Zhen Han", "Yunpu Ma", "Volker Tresp." ],
      "venue" : "arXiv preprint arXiv:2101.05151.",
      "citeRegEx" : "Ding et al\\.,? 2021",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2021
    }, {
      "title" : "Heuristic and analytic processes in reasoning",
      "author" : [ "Jonathan St BT Evans." ],
      "venue" : "British Journal of Psychology, 75(4):451–468.",
      "citeRegEx" : "Evans.,? 1984",
      "shortCiteRegEx" : "Evans.",
      "year" : 1984
    }, {
      "title" : "In two minds: dualprocess accounts of reasoning",
      "author" : [ "Jonathan St BT Evans." ],
      "venue" : "Trends in cognitive sciences, 7(10):454–459.",
      "citeRegEx" : "Evans.,? 2003",
      "shortCiteRegEx" : "Evans.",
      "year" : 2003
    }, {
      "title" : "Dual-processing accounts of reasoning, judgment, and social cognition",
      "author" : [ "Jonathan St BT Evans." ],
      "venue" : "Annu. Rev. Psychol., 59:255–278.",
      "citeRegEx" : "Evans.,? 2008",
      "shortCiteRegEx" : "Evans.",
      "year" : 2008
    }, {
      "title" : "Reinforcement learning for relation classification from noisy data",
      "author" : [ "Jun Feng", "Minlie Huang", "Li Zhao", "Yang Yang", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of the aaai conference on artificial intelligence, volume 32.",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Fast rule mining in ontological knowledge bases with amie+",
      "author" : [ "Luis Galárraga", "Christina Teflioudi", "Katja Hose", "Fabian M Suchanek." ],
      "venue" : "The VLDB Journal, 24(6):707–730.",
      "citeRegEx" : "Galárraga et al\\.,? 2015",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning sequence encoders for temporal knowledge graph completion",
      "author" : [ "Alberto Garcı́a-Durán", "Sebastijan Dumančić", "Mathias Niepert" ],
      "venue" : "arXiv preprint arXiv:1809.03202",
      "citeRegEx" : "Garcı́a.Durán et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Garcı́a.Durán et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 315– 323.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Diachronic embedding for temporal knowledge graph completion",
      "author" : [ "Rishab Goel", "Seyed Mehran Kazemi", "Marcus Brubaker", "Pascal Poupart." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3988–3995.",
      "citeRegEx" : "Goel et al\\.,? 2020",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2020
    }, {
      "title" : "Eventkg: A multilingual event-centric temporal knowledge graph",
      "author" : [ "Simon Gottschalk", "Elena Demidova." ],
      "venue" : "European Semantic Web Conference, pages 272–287. Springer.",
      "citeRegEx" : "Gottschalk and Demidova.,? 2018",
      "shortCiteRegEx" : "Gottschalk and Demidova.",
      "year" : 2018
    }, {
      "title" : "Eventkg–the hub of event knowledge on the web– and biographical timeline generation",
      "author" : [ "Simon Gottschalk", "Elena Demidova." ],
      "venue" : "Semantic Web, (Preprint):1–32.",
      "citeRegEx" : "Gottschalk and Demidova.,? 2019",
      "shortCiteRegEx" : "Gottschalk and Demidova.",
      "year" : 2019
    }, {
      "title" : "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
      "author" : [ "Kelvin Guu", "Panupong Pasupat", "Evan Zheran Liu", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1704.07926.",
      "citeRegEx" : "Guu et al\\.,? 2017",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2017
    }, {
      "title" : "Dyernie: Dynamic evolution of riemannian manifold embeddings for temporal knowledge graph completion",
      "author" : [ "Zhen Han", "Peng Chen", "Yunpu Ma", "Volker Tresp." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Han et al\\.,? 2020a",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph hawkes neural network for forecasting on temporal knowledge graphs",
      "author" : [ "Zhen Han", "Yunpu Ma", "Yuyi Wang", "Stephan Günnemann", "Volker Tresp." ],
      "venue" : "8th Automated Knowledge Base Construction (AKBC).",
      "citeRegEx" : "Han et al\\.,? 2020b",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Temporal knowledge base completion: New algorithms and evaluation protocols",
      "author" : [ "Prachi Jain", "Sushant Rathi", "Soumen Chakrabarti" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Jain et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrent event network: Autoregressive structure inference over temporal knowledge graphs",
      "author" : [ "Woojeong Jin", "Meng Qu", "Xisen Jin", "Xiang Ren." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrent event network for reasoning over temporal knowledge graphs",
      "author" : [ "Woojeong Jin", "Changlin Zhang", "Pedro Szekely", "Xiang Ren." ],
      "venue" : "arXiv preprint arXiv:1904.05530.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Tgap: Learning to walk across time for temporal knowledge graph completion",
      "author" : [ "Jaehun Jung", "Jinhong Jung", "U Kang." ],
      "venue" : "arXiv preprint arXiv:2012.10595.",
      "citeRegEx" : "Jung et al\\.,? 2020",
      "shortCiteRegEx" : "Jung et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Combining heterogeneous data sources for civil unrest forecasting",
      "author" : [ "Gizem Korkmaz", "Jose Cadena", "Chris J Kuhlman", "Achla Marathe", "Anil Vullikanti", "Naren Ramakrishnan." ],
      "venue" : "Proceedings of the 2015 IEEE/ACM International Conference on Advances",
      "citeRegEx" : "Korkmaz et al\\.,? 2015",
      "shortCiteRegEx" : "Korkmaz et al\\.",
      "year" : 2015
    }, {
      "title" : "Deriving validity time in knowledge graph",
      "author" : [ "Julien Leblay", "Melisachew Wudage Chekol." ],
      "venue" : "Companion Proceedings of the The Web Conference 2018, pages 1771–1776. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Leblay and Chekol.,? 2018",
      "shortCiteRegEx" : "Leblay and Chekol.",
      "year" : 2018
    }, {
      "title" : "Gdelt: Global data on events, location, and tone, 1979– 2012",
      "author" : [ "Kalev Leetaru", "Philip A Schrodt." ],
      "venue" : "ISA annual convention, volume 2, pages 1–49. Citeseer.",
      "citeRegEx" : "Leetaru and Schrodt.,? 2013",
      "shortCiteRegEx" : "Leetaru and Schrodt.",
      "year" : 2013
    }, {
      "title" : "Divine: A generative adversarial imitation learning framework for knowledge graph reasoning",
      "author" : [ "Ruiping Li", "Xiang Cheng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Li and Cheng.,? 2019",
      "shortCiteRegEx" : "Li and Cheng.",
      "year" : 2019
    }, {
      "title" : "Temporal knowledge graph reasoning based on evolutional representation learning",
      "author" : [ "Zixuan Li", "Xiaolong Jin", "Wei Li", "Saiping Guan", "Jiafeng Guo", "Huawei Shen", "Yuanzhuo Wang", "Xueqi Cheng." ],
      "venue" : "arXiv preprint arXiv:2104.10353.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-hop knowledge graph reasoning with reward shaping",
      "author" : [ "Xi Victoria Lin", "Richard Socher", "Caiming Xiong." ],
      "venue" : "arXiv preprint arXiv:1808.10568.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Planned protest modeling",
      "author" : [ "Sathappan Muthiah", "Bert Huang", "Jaime Arredondo", "David Mares", "Lise Getoor", "Graham Katz", "Naren Ramakrishnan" ],
      "venue" : null,
      "citeRegEx" : "Muthiah et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Muthiah et al\\.",
      "year" : 2015
    }, {
      "title" : "Evolvegcn: Evolving graph convolutional networks for dynamic graphs",
      "author" : [ "Aldo Pareja", "Giacomo Domeniconi", "Jie Chen", "Tengfei Ma", "Toyotaro Suzumura", "Hiroki Kanezashi", "Tim Kaler", "Charles E Leisersen." ],
      "venue" : "arXiv preprint arXiv:1902.10191.",
      "citeRegEx" : "Pareja et al\\.,? 2019",
      "shortCiteRegEx" : "Pareja et al\\.",
      "year" : 2019
    }, {
      "title" : "Using social media to predict the future: a systematic literature review",
      "author" : [ "Lawrence Phillips", "Chase Dowling", "Kyle Shaffer", "Nathan Hodas", "Svitlana Volkova." ],
      "venue" : "arXiv preprint arXiv:1706.06134.",
      "citeRegEx" : "Phillips et al\\.,? 2017",
      "shortCiteRegEx" : "Phillips et al\\.",
      "year" : 2017
    }, {
      "title" : "Temporal reasoning over event knowledge graphs",
      "author" : [ "Ali Sadeghian", "Miguel Rodriguez", "Daisy Zhe Wang", "Anthony Colas." ],
      "venue" : "Workshop on Knowledge Base Construction, Reasoning and Mining.",
      "citeRegEx" : "Sadeghian et al\\.,? 2016",
      "shortCiteRegEx" : "Sadeghian et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European Semantic Web Conference, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured sequence modeling with graph convolutional recurrent networks",
      "author" : [ "Youngjoo Seo", "Michaël Defferrard", "Pierre Vandergheynst", "Xavier Bresson." ],
      "venue" : "International Conference on Neural Information Processing, pages 362–373.",
      "citeRegEx" : "Seo et al\\.,? 2018",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end structure-aware convolutional networks for knowledge base completion",
      "author" : [ "Chao Shang", "Yun Tang", "Jing Huang", "Jinbo Bi", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,",
      "citeRegEx" : "Shang et al\\.,? 2019",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2019
    }, {
      "title" : "The use of twitter to track levels of disease activity and public concern in the us during the influenza a h1n1 pandemic",
      "author" : [ "Alessio Signorini", "Alberto Maria Segre", "Philip M Polgreen." ],
      "venue" : "PloS one, 6(5):e19467.",
      "citeRegEx" : "Signorini et al\\.,? 2011",
      "shortCiteRegEx" : "Signorini et al\\.",
      "year" : 2011
    }, {
      "title" : "The empirical case for two systems of reasoning",
      "author" : [ "Steven A Sloman." ],
      "venue" : "Psychological bulletin, 119(1):3.",
      "citeRegEx" : "Sloman.,? 1996",
      "shortCiteRegEx" : "Sloman.",
      "year" : 1996
    }, {
      "title" : "Rotate: Knowledge graph embedding by relational rotation in complex space",
      "author" : [ "Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S Sutton", "Andrew G Barto." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Sutton and Barto.,? 2018",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 2018
    }, {
      "title" : "Know-evolve: deep temporal reasoning for dynamic knowledge graphs",
      "author" : [ "Rakshit Trivedi", "Hanjun Dai", "Yichen Wang", "Le Song." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3462–3471.",
      "citeRegEx" : "Trivedi et al\\.,? 2017",
      "shortCiteRegEx" : "Trivedi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dyrep: Learning representations over dynamic graphs",
      "author" : [ "Rakshit Trivedi", "Mehrdad Farajtabar", "Prasenjeet Biswal", "Hongyuan Zha" ],
      "venue" : null,
      "citeRegEx" : "Trivedi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Trivedi et al\\.",
      "year" : 2018
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard." ],
      "venue" : "International Conference on Machine Learning, pages 2071–2080.",
      "citeRegEx" : "Trouillon et al\\.,? 2016",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "Composition-based multirelational graph convolutional networks",
      "author" : [ "Shikhar Vashishth", "Soumya Sanyal", "Vikram Nitin", "Partha Talukdar." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Vashishth et al\\.,? 2019",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating graph attention mechanism into knowledge graph reasoning based on deep reinforcement learning",
      "author" : [ "Heng Wang", "Shuangyin Li", "Rong Pan", "Mingzhi Mao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning, 8(3-4):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Temp: Temporal message passing for temporal knowledge graph completion",
      "author" : [ "Jiapeng Wu", "Meng Cao", "Jackie Chi Kit Cheung", "William L Hamilton." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "A study of reinforcement learning for neural machine translation",
      "author" : [ "Lijun Wu", "Fei Tian", "Tao Qin", "Jianhuang Lai", "TieYan Liu." ],
      "venue" : "arXiv preprint arXiv:1808.08866.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficiently embedding dynamic knowledge graphs",
      "author" : [ "Tianxing Wu", "Arijit Khan", "Huan Gao", "Cheng Li." ],
      "venue" : "arXiv, pages arXiv–1910.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Deeppath: A reinforcement learning method for knowledge graph reasoning",
      "author" : [ "Wenhan Xiong", "Thien Hoang", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1707.06690.",
      "citeRegEx" : "Xiong et al\\.,? 2017",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Temporal knowledge graph completion based on time series gaussian embedding",
      "author" : [ "Chenjin Xu", "Mojtaba Nayyeri", "Fouad Alkhoury", "Hamed Yazdi", "Jens Lehmann." ],
      "venue" : "International Semantic Web Conference, pages 654–671. Springer.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "arXiv preprint arXiv:1412.6575.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "A vectorized relational graph convolutional network for multi-relational network alignment",
      "author" : [ "Rui Ye", "Xin Li", "Yujie Fang", "Hongyu Zang", "Mingzhong Wang." ],
      "venue" : "Proceedings of the TwentyEighth International Joint Conference on Artificial",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Event prediction in big data era: A systematic survey",
      "author" : [ "Liang Zhao." ],
      "venue" : "arXiv preprint arXiv:2007.09815.",
      "citeRegEx" : "Zhao.,? 2020",
      "shortCiteRegEx" : "Zhao.",
      "year" : 2020
    }, {
      "title" : "Learning from history: Modeling temporal knowledge graphs with sequential copy-generation networks",
      "author" : [ "Cunchao Zhu", "Muhao Chen", "Changjun Fan", "Guangquan Cheng", "Yan Zhan." ],
      "venue" : "arXiv preprint arXiv:2012.08492.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Temporal Knowledge Graphs (TKGs) (Boschee et al., 2015; Gottschalk and Demidova, 2018, 2019; Zhao, 2020) have emerged as a very active research area over the last few years.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 60,
      "context" : "Temporal Knowledge Graphs (TKGs) (Boschee et al., 2015; Gottschalk and Demidova, 2018, 2019; Zhao, 2020) have emerged as a very active research area over the last few years.",
      "startOffset" : 33,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "In this paper, reasoning on TKGs aims to predict future facts (events) for timestamp t > tT , where tT is assumed to be the current timestamp (Jin et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 35,
      "context" : "applications, such as, emerging events response (Muthiah et al., 2015; Phillips et al., 2017; Korkmaz et al., 2015), disaster relief (Signorini et al.",
      "startOffset" : 48,
      "endOffset" : 115
    }, {
      "referenceID" : 37,
      "context" : "applications, such as, emerging events response (Muthiah et al., 2015; Phillips et al., 2017; Korkmaz et al., 2015), disaster relief (Signorini et al.",
      "startOffset" : 48,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "applications, such as, emerging events response (Muthiah et al., 2015; Phillips et al., 2017; Korkmaz et al., 2015), disaster relief (Signorini et al.",
      "startOffset" : 48,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : ", 2015), disaster relief (Signorini et al., 2011), and financial analysis (Bollen et al.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 43,
      "context" : "How do human beings predict future events? According to the dual process theory (Evans, 1984, 2003, 2008; Sloman, 1996), the first thing is to search the massive-capacity memories and find some related historical information (i.",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 61,
      "context" : "Thus, some other studies (Jin et al., 2020, 2019; Zhu et al., 2020) mainly focus on encoding the 1-hop repetitive paths (repetitive facts) in the history.",
      "startOffset" : 25,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "Taking the widely used dataset ICEWS18 (Jin et al., 2020) as an example, 41.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 46,
      "context" : "At Stage 1, CluSTeR formalizes clue-searching as a Markov Decision Process (MDP) (Sutton and Barto, 2018) and learns a beam search policy to solve it.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 58,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 49,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 9,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 41,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 44,
      "context" : "Embedding based KG reasoning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention.",
      "startOffset" : 36,
      "endOffset" : 161
    }, {
      "referenceID" : 39,
      "context" : "Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs.",
      "startOffset" : 23,
      "endOffset" : 112
    }, {
      "referenceID" : 41,
      "context" : "Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs.",
      "startOffset" : 23,
      "endOffset" : 112
    }, {
      "referenceID" : 59,
      "context" : "Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs.",
      "startOffset" : 23,
      "endOffset" : 112
    }, {
      "referenceID" : 50,
      "context" : "Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs.",
      "startOffset" : 23,
      "endOffset" : 112
    }, {
      "referenceID" : 56,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 51,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 32,
      "context" : "Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths.",
      "startOffset" : 24,
      "endOffset" : 138
    }, {
      "referenceID" : 38,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 30,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 7,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 55,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 57,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 18,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 53,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 22,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 27,
      "context" : "Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garcı́aDurán et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al.",
      "startOffset" : 85,
      "endOffset" : 292
    }, {
      "referenceID" : 23,
      "context" : ", 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al.",
      "startOffset" : 26,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : ", 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al.",
      "startOffset" : 26,
      "endOffset" : 151
    }, {
      "referenceID" : 61,
      "context" : ", 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al.",
      "startOffset" : 26,
      "endOffset" : 151
    }, {
      "referenceID" : 33,
      "context" : ", 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al.",
      "startOffset" : 26,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Glean (Deng et al., 2020) incorporates a word graph constructed by the summary texts of events into TKG reasoning.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "The most related works are RE-NET (Jin et al., 2020) and CyGNet (Zhu et al.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "The two stages are coordinated to perform fast and slow thinking (Daniel, 2017), respectively, to solve the TKG reasoning task, inspired by human cognition.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 61,
      "context" : "The previous studies (Jin et al., 2019, 2020; Zhu et al., 2020) use heuristic strategies to extract 1-hop repetitive paths, losing lots of other informative clue paths.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "where η(·) is the softmax function, f(·) is the ReLU function (Glorot et al., 2011) and Θ is the set of all the learnable parameters in Stage 1.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 45,
      "context" : "Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent.",
      "startOffset" : 43,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent.",
      "startOffset" : 43,
      "endOffset" : 102
    }, {
      "referenceID" : 54,
      "context" : "Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent.",
      "startOffset" : 43,
      "endOffset" : 102
    }, {
      "referenceID" : 39,
      "context" : "We use an ω-layer RGCN (Schlichtkrull et al., 2018) to model Ĝj ,",
      "startOffset" : 23,
      "endOffset" : 51
    }, {
      "referenceID" : 52,
      "context" : "(7) The REINFORCE algorithm (Williams, 1992) is used to optimize Equation 7.",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "The Adam (Kingma and Ba, 2014) optimizer is used to minimize Equation 8.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "Such a training strategy is widely used by other RL studies (Bahdanau et al., 2016; Feng et al., 2018).",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Such a training strategy is widely used by other RL studies (Bahdanau et al., 2016; Feng et al., 2018).",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "There are four typical TKGs commonly used in previous studies, namely, ICEWS14 (Garcı́a-Durán et al., 2018), ICEWS0515 (Garcı́a-Durán et al.",
      "startOffset" : 79,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : ", 2018), ICEWS0515 (Garcı́a-Durán et al., 2018), ICEWS18 (Jin et al.",
      "startOffset" : 19,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : ", 2018), ICEWS18 (Jin et al., 2019) and GDELT (Jin et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "The first three datasets are from the Integrated Crisis Early Warning System (ICEWS) (Boschee et al., 2015) and the last one is from Global Database of Events, Language, and Tone (GDELT) (Leetaru and Schrodt, 2013).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : ", 2015) and the last one is from Global Database of Events, Language, and Tone (GDELT) (Leetaru and Schrodt, 2013).",
      "startOffset" : 87,
      "endOffset" : 114
    }, {
      "referenceID" : 25,
      "context" : "ICEWS14 and ICEWS05-15 are divided into training, validation, and test sets following the preprocessing on ICEWS18 in RE-NET (Jin et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "The filtered setting is not suitable for the reasoning task under the exploration setting, as mentioned in (Han et al., 2020b; Ding et al., 2021; Jain et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "The filtered setting is not suitable for the reasoning task under the exploration setting, as mentioned in (Han et al., 2020b; Ding et al., 2021; Jain et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "The filtered setting is not suitable for the reasoning task under the exploration setting, as mentioned in (Han et al., 2020b; Ding et al., 2021; Jain et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 58,
      "context" : "The typical static models DistMult (Yang et al., 2014), ComplEx (Trouillon et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 49,
      "context" : ", 2014), ComplEx (Trouillon et al., 2016), RGCN (Schlichtkrull et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 39,
      "context" : ", 2016), RGCN (Schlichtkrull et al., 2018), ConvE (Dettmers et al.",
      "startOffset" : 14,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : ", 2018), ConvE (Dettmers et al., 2018) and RotaE (Sun et al.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 44,
      "context" : ", 2018) and RotaE (Sun et al., 2018) are selected with the temporal information of facts ignored.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "We also choose MINERVA (Das et al., 2018), the RL-based multi-hop reasoning model, as the baseline.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 47,
      "context" : "For TKG models, the representative Know-evolve (Trivedi et al., 2017), DyRep (Trivedi et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 48,
      "context" : ", 2017), DyRep (Trivedi et al., 2018), CyGNet (Zhu et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 61,
      "context" : ", 2018), CyGNet (Zhu et al., 2020) and RE-NET (Jin et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "Besides, following RE-NET (Jin et al., 2020), we extend two models for temporal homogeneous graphs, GCRN (Seo et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 40,
      "context" : ", 2020), we extend two models for temporal homogeneous graphs, GCRN (Seo et al., 2018) and EvolveGCN-O (Pareja et al.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : ", 2018) and EvolveGCN-O (Pareja et al., 2019)), to RGCRN and EvolveRGCN by replacing GCN with RGCN.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "We use ConvE (Dettmers et al., 2018), a more stronger decoder to replace the MLP (Jin et al.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : ", 2018), a more stronger decoder to replace the MLP (Jin et al., 2020) for the two models.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "To further analyze the reason, we extract paths from ICEWS18 without considering timestamps via AMIE+ (Galárraga et al., 2015), a widely used and accurate approach to extract logic rules (paths) in static KGs.",
      "startOffset" : 102,
      "endOffset" : 126
    } ],
    "year" : 2021,
    "abstractText" : "Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing models. When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly. Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning (RL) to induce multiple clues from historical facts. At the temporal reasoning stage, it adopts a graph convolution network based sequence method to deduce answers from clues. Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods. Moreover, the clues found by CluSTeR further provide interpretability for the results.",
    "creator" : "LaTeX with hyperref package"
  }
}