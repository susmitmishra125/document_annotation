{
  "name" : "2021.acl-long.332.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Subsequence Based Deep Active Learning for Named Entity Recognition",
    "authors" : [ "Puria Radmard", "Yassir Fathullah", "Aldo Lipani" ],
    "emails" : [ "pr450@cam.ac.uk", "yf286@cam.ac.uk", "aldo.lipani@ucl.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4310–4321\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4310"
    }, {
      "heading" : "1 Introduction",
      "text" : "The availability of large datasets has been key to the success of deep learning in Natural Language Processing (NLP). This has galvanized the creation of larger datasets in order to train larger deep learning models. However, creating high quality datasets is expensive due to the sparsity of natural language, our inability to label it efficiently compared to other forms of data, and the amount of prior knowledge required to solve certain annotation tasks. Such a problem has motivated the development of new Active Learning (AL) strategies which aim to efficiently train models, by automatically identifying the best training examples from large amounts of\nCode is made available on: https://github.com/ puria-radmard/RFL-SBDALNER\nunlabeled data (Wei et al., 2015; Wang et al., 2017; Tong and Koller, 2002). This tremendously reduces human annotation effort as much fewer instances need to be labeled manually.\nTo minimise the amount of data needed to train a model, AL algorithms iterate between training a model, and querying information rich instances to human annotators from a pool of unlabelled data (Huang et al., 2014). This has been shown to work well when the queries are ‘atomic’—a single annotation requires a unit labour, and describes entirely the instance to be annotated. Conversely, each instance of structured data, such as sequences, require multiple annotations. Hence, such query selection methods can result in a waste of annotation budget (Settles, 2011).\nFor example, in Named Entity Recognition (NER), each sentence is usually considered an instance. However, because each token has a separate label, annotation budgeting is typically done on a token basis (Shen et al., 2017). Budget wasting may therefore arise from the heterogeneity of uncertainty across each sentence; a sentence can contain multiple subsequences (of tokens) of which the model is certain on some and uncertain on others. By making the selection at a sentence level, although some budget is spent on annotating uncertain subsequences, the remaining budget may be wasted on annotating subsequences for which an annotation is not needed.\nIt can therefore be desirable for annotators to label subsequences rather than the full sentences. This gives a greater flexibility to AL strategies to locate information rich parts of the input with improved efficiency – and reduces the cognitive demands required of annotators. Annotators may in fact perform better if they are asked to annotate shorter sequences, because longer sentences can cause boredom, fatigue, and inaccuracies (Rzeszotarski et al., 2013).\nIn this work, we aim to improve upon the efficiency of AL for NER by querying for subsequences within each sentence, and propagating labels to unseen, identical subsequences in the dataset. This strategy simulates a setup in which annotators are presented with these subsequences, and do not have access to the full context, ensuring that their focus is centred on the tokens of interest.\nWe show that AL algorithms for NER tasks that use subsequences, allowing training on partially labelled sentences, are more efficient in terms of budget than those that only query full sentences. This improvement is furthered by generalising existing acquisition functions (§ 4.1) for use with sequential data. We test our approaches on two NER datasets, OntoNotes 5.0 and CoNLL 2003. On OntoNotes 5.0, Shen et al. (2017) achieve stateof-the-art performance with 25% of the original dataset querying full sentences, while we require only 13% of the dataset querying subsequences. On CoNLL 2003, we show that the AL strategy of Shen et al. (2017) requires 50% of the dataset to achieve the same results as training on the full dataset, while ours requires only 27%.\nContributions of this paper are: 1. Improving the efficiency of AL for NER by\nallowing querying of subsequences over full sentences; 2. An entity based analysis demonstrating that subsequence querying AL strategies tend to query more relevant tokens (i.e., tokens belonging to entities); 3. An uncertainty analysis of the queries made by both full sentence and subsequence querying methods, demonstrating that querying full sentences leads to selecting more tokens to which the model is already certain."
    }, {
      "heading" : "2 Related Work",
      "text" : "AL algorithms aim to query information rich data points to annotators in order to improve the performance of the model in a data efficient way. Traditionally these algorithms choose data points which lie close to decision boundaries (Pinsler et al., 2019), where uncertainty is high, in order for the model to learn more useful information. This measure of uncertainty, measured through acquisition functions, are therefore vital to AL. Key functions include predictive entropy (MaxEnt) (Gal et al., 2017), mutual information between model posterior and predictions (BALD) (Houlsby et al., 2011;\nGal et al., 2017), or the certainty of the model when making label predictions (here called LC) (Mingkun Li and Sethi, 2006). These techniques ensure all instances used for training, painstakingly labelled by experts, have maximum impact on model performance. There has been exploration of uncertainty and deep learning based AL for NER (Chen et al., 2015; Shen et al., 2017; Settles and Craven, 2008; Fang et al., 2017). These approaches however, treat each sentence as a single query instead of a collection of individually labelled tokens. In these methods, the acquisition functions that score sentences aggregate token-wise scores (through summation or averaging).\nOther works forgo this aggregation, querying single tokens at a time (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014). These works show that AL for NER can be improved by taking the single token as a unit query, and use semi-supervision (Reddy et al., 2018; Iscen et al., 2019) for training on partially labelled sentences (Muslea et al., 2002). However, querying single-tokens is inapplicable in practise because, either a) annotators have access to the full sentence when queried but can only label one token, which would lead to frustration as they are asked to read the full sentence but only annotate a single token, or b) annotators only have access to the token of interest, which means that they would not have enough information to label tokens differently based on their context, leading to annotators labeling any unique token with the same label. Moreover, if the latter approach was somehow possible, we would be able to reduce the annotation effort to the annotation of only the unique tokens forming the dataset, its dictionary. Furthermore, all of these past works use Conditional Random Fields (CRFs) (Lafferty et al., 2001), which have since been surpassed as the state-of-the-art for NER (and most NLP tasks) by deep learning models (Devlin et al., 2019).\nIn this work we follow the approach where annotators only have access to subsequences of multiple tokens. However, instead of making use of single tokens, we will query more than one token, providing enough context to the annotators. This allows the propagation of these annotations to identical subsequences in the dataset, further reducing the total annotation effort."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Active Learning Algorithms",
      "text" : "Most AL strategies are based on a repeating score, query and fine-tune cycle. After initially training an NER model with a small pool of labelled examples, the following is repeated: (1) score all unlabelled instances, (2) query the highest scoring instances and add them to training set, and, (3) fine-tune the model using the updated training set (Huang et al., 2014).\nTo describe this further, notation and proposed training process is introduced, with details in following sections. First, the sequence tagging dataset, denoted by D = {(x(n),y(n))}Nn=1, consists of a collection of sentence and ground truth labels. The i-th token of the n-th sentence (y(n)i ) has a label y (n) i = c with c belonging to C = {c1, ..., cK}. We also differentiate between the labelled and unlabelled datasets, DL and DU , which initially are empty and equal toD. Finally, we fix A as the total number of tokens queried in each iteration."
    }, {
      "heading" : "3.2 Acquisition Functions",
      "text" : "Instances in the unlabelled pool are queried using an acquisition function. This function aims to quantify the uncertainty of the model when generating predictive probabilities over possible labels for each instance. Instances with the highest predictive uncertainty are deemed as the most informative for model training. Previously used acquisition functions such as Least Confidence (LC) and Maximum Normalized Log-Probability (MNLP) (Shen et al., 2017; Chen et al., 2015) are generalised for variable length sequences. Letting ŷ(n)<i be the history of predictions prior to the i-th input, the next output probability will be p(n)i,c = P (ŷ (n) i = c|ŷ (n) <i ,x\n(n)). Then, we define the token-wise LC score as:\nLC (n) i = −max\nc∈C log p\n(n) i,c . (1)\nThe LC acquisition function for sequences is then defined as:\nLC ( x (n) 1 , ..., x (n) ` ) = ∑̀ j=1 LC (n) j , (2)\nand, for MNLP as:\nMNLP ( x (n) 1 , ..., x (n) ` ) = 1\n` ∑̀ j=1 LC (n) j . (3)\nNote that this is similar to LC except for the normalization factor 1/`. The formulation above can be applied to other types of commonly used acquisition functions such as Maximum Entropy (MaxEnt) (Gal et al., 2017) by simply defining:\nME (n) i = − ∑ c∈C p (n) i,c log p (n) i,c , (4)\nas the token score. Given the task of quantifying uncertainty amongst the unlabelled pool of data, both of these metrics - LC and MaxEnt - provide intuitive interpretations. eq. (1) scores highly tokens for which the predicted label has lowest confidence, while eq. (4) scores highly tokens for which the whole probability mass function has higher entropy. Both of these therefore score more highly uniform predictive distributions, which indicates underlying uncertainty.\nFinally, given the similarity of performance between MNLP and Bayesian Active Learning by Disagreement (BALD) (Houlsby et al., 2011) in NER tasks (Shen et al., 2017), and the computational complexity required to calculate BALD with respect to the other activation functions, we will not compare against BALD."
    }, {
      "heading" : "4 Subsequence Acquisition",
      "text" : "In this section we describe how we build on past works, and the core contribution of this paper. Our work forms a more flexible AL algorithm that operates on subsequences, as opposed to full sentences (Shen et al., 2017). This is achieved by generalising acquisition functions for subsequences (§ 4.1) scoring and querying subsequences within sentences (§ 4.2), and performing label propagation on unseen sentences to avoid the multiple annotations of repeated subsequences (§ 4.3)."
    }, {
      "heading" : "4.1 Subsequence Acquisition Functions",
      "text" : "Since this work focuses on the querying of subsequences, from the previously defined LC and MNLP we generalize them to define a family of acquisition functions applicable for both full sentences and subsequences:\nLCα ( x (n) i+1, ..., x (n) i+` ) = 1\n`α i+∑̀ j=i+1 LC (n) j . (5)\nSpecial cases are when α = 0 and α = 1 which return the original definitions of LC in eq. (2) and MNLP in eq. (3). As noted by Shen et al. (2017),\nLC for sequences biases acquisition towards longer sentences. The tuneable normalisation factor in eq. (5) over the sequence of scores mediates the balance of shorter and longer subsequences selected. This generalisation can be applied to other types of commonly used acquisition functions such as MaxEnt and BALD by modifying the token-wise score."
    }, {
      "heading" : "4.2 Subsequence Selection",
      "text" : "Each sentence x(n) can be broken into a set of subsequences S(n) = {(x(n)i , ..., x (n) j ) |∀i < j} where all elements s ∈ S(n) can be efficiently scored by first computing the token scores, then aggregating as required. Once this has been done for all sentences in DU , a query set SQ ⊂ ∪nS(n) of non-overlapping (mutually disjoint) subsequences is found. The requirement of non-overlapping subsequences avoids the problem of relabelling tokens, but disallows simply choosing the highest scoring subsequences (since these can overlap). Instead at each round of querying, we perform a greedy selection, repeatedly choosing the highest scoring subsequence that does not overlap with previously selected subsequences. Adjustments can be made to reflect practical needs, such as restricting the length ` of the viable subsequences to [`min, `max]. This is because longer subsequences are easier to label, while shorter subsequences are more efficient in querying uncertain tokens, and so the selection is only allowed to operate within these bounds.\nAdditionally, it is easy to imagine a scenario in which a greedy selection method does not select the maximum total score that can be generated from a sentence. This scenario is illustrated in Table 1 where lengths are restricted to `min = `max = 3 for simplicity. Note that tokens can become unselectable in future rounds because they are not inside a span of unlabelled tokens of at least size `min. When the algorithm has queried all subsequences of this size range, it starts to query shorter subsequences by relaxing the length constraint. However in practise, model performance on the validation set converges before all subsequences of valid range have been exhausted. Nonetheless, when choosing subsequences of size [`min, `max] = [4, 7] these will be exhausted when roughly 90% and 80% of tokens have been labelled for the OntoNotes 5.0 and CoNLL 2003 datasets."
    }, {
      "heading" : "4.3 Subsequence Label Propagation",
      "text" : "Since a subsequence querying algorithm can result in partially labelled sentences, it raises the question of how unlabelled tokens should be handled. In previous work based on the use of CRFs (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014) this was solved by using semisupervision on tokens for which the model showed low uncertainty. However, for neural networks, the use of model generated labels could lead to the model becoming over-confident, harming performance and biasing (Arazo et al., 2020) uncertainty scores. Hence, we ensure that backpropagation only occurs from labelled tokens.\nOur final contribution to the AL algorithm is the use of another semi-supervision strategy where we propagate uniquely labelled subsequences in order to minimise the number of annotations needed. When queried for a subsequence, the annotator (in this case an oracle) is not given the contextual tokens in the remainder of the sentence. For this reason, given an identical subsequence, a consistent annotator will provide the same labels. Therefore, the proposed algorithm maintains a dictionary that maps previously queried subsequences to their provided labels. Once a queried subsequence and its label are added to the dictionary, all other matching subsequences in the unlabelled pool are given the same, but temporary, labels.\nThe tokens retain these temporary labels until they are queried themselves. After scoring and ranking members of S, the algorithm will disregard sequences that match exactly members of this dictionary, which is updated during the querying round. However, if tokens belonging to these previously seen subsequences are encountered in a different context, meaning as part of a different subsequence, they may also be queried. For example, in Table 1, if the subsequence “shop to buy” had been previously queried elsewhere in the dataset, the red subsequence will not be considered for querying, as it retains its temporary labels. Instead, the green subsequence could be queried, in which case the temporary labels of tokens 6 and 7 will be overwritten by new, permanent labels.\nTherefore, the value of `min becomes a trade-off between the improved resolution of the acquisition function, and the erroneous propagation of shorter, more frequent label subsequences to identical ones in different contexts."
    }, {
      "heading" : "4.4 Subsequence Active Learning Algorithm",
      "text" : "Finally, we summarise the AL algorithm proposed. Given a set of unlabelled data DU , we initially randomly select a proportion of sentences from DU , label them, and add these to DL. A dictionary B is also initialised. Using these labelled sentences we train a model. Then, the following proposed training cycle is repeated until DU is empty (or an early stopping condition is reached):\n1. Find all consecutive unlabelled subsequences in DU , and score them using a pre-defined acquisition function.\n2. Select the top scoring non-overlapping subsequences SQ that do not appear in B, such that the number of tokens in SQ is A, and query them to the annotators. Update DL and DU . As each sequence is selected, add it to B, mapping it to its true labels.\n3. Provide all occurrences of the keys of B in DU with their corresponding temporary labels. These will not be included in DL as these are temporary.\n4. Finetune the model on sentences with any label, temporary and permanent.\nRepeat this process until convergence."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "As in previous works (Shen et al., 2017), we use the two following NER datasets:\nOntoNotes 5.0. This is a dataset used to compare results with the full sentence querying baseline (Weischedel, Ralph et al., 2013), and comprising of\ntext coming from: news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, and talk shows. This is a BIO formatted dataset with a total of K = 37 classes and 99,333 training sentences, with an average sentence length of 17.8 tokens in its training set.\nCoNLL 2003. This is a dataset, also in BIO format, with only 4 entity types (LOC, MISC, PER, ORG) resulting in K = 9 labels (Tjong Kim Sang and De Meulder, 2003). This dataset is made from a collection of news wire articles from the Reuters Corpus (Lewis et al., 2004). The average sentence length is 12.6 tokens in its training set.\nA full list of class types and entity lengths and frequencies for both datasets can be found in the Appendix."
    }, {
      "heading" : "5.2 NER Model",
      "text" : "Following the work of Shen et al. (2017), a CNNCNN-LSTM model for combined letter- and tokenlevel embeddings was used; see Appendix for an overview of the model and hyperparameters setting and validation. Furthermore, the AL algorithm used in (Shen et al., 2017) will serve as one of the baselines following the same procedure. This represents an equivalent algorithm to that proposed, but which can only query full sentences, and does not use label propagation."
    }, {
      "heading" : "5.3 Model Training and Evaluation",
      "text" : "As the evaluation measure we use the F1 score. After the first round of random subsequence selection, the model is trained. After subsequent selections the model is finetuned - training is resumed from the previous round’s parameters. In all cases, the model training was stopped either after 30 epochs were completed, or if the F1 score for the valida-\ntion set had monotonically decreased for 2 epochs. This validation set is made up of a randomly selected 1% of sentences of the original training set. After finetuning, the model reloads its parameters from the round-optimal epoch, and its performance is evaluated on the test set. Furthermore, the AL algorithms were also stopped after all hyperparameter variations using that dataset and acquisition function family had converged to the same best F1, which we denote with F ∗1 . For the OntoNotes 5.0 dataset, F ∗1 value was achieved after 30% of the training set was labelled, and for the CoNLL 2003 dataset after 40%."
    }, {
      "heading" : "5.4 Active Learning Setup & Evaluation",
      "text" : "We choose `min = 4 to give a realistic context to the annotator, and to avoid a significant propagation of common subsequences. The upper bound of `max = 7 was chosen to ensure subsequences were properly utilised, since the average sentence length of both datasets is roughly twice this size. For the OntoNotes 5.0 dataset, every round A = 10, 000 tokens are queried, whereas for the CoNLL 2003 datasetA = 2, 000 tokens. These represent roughly 0.5% and 1% of the available training set.\nWe evaluate the efficacy and efficiency of the tested AL strategies in three ways. First, model performance over the course of the algorithm was evaluated using end of round F1 score on the test set. We compare the proportion of the dataset’s tokens labelled when the model achieves 99% of the F ∗1 score (F̂1 ∗ = 0.99×F ∗1 ). We also quantify the rate of improvement of model performance during training using the normalised Area Under the Curve (AUC) score of each F1 test curve. The normalisation ensures that the resulting AUC score is in the\nrange [0, 1], and it is achieved by dividing the AUC score by the size of the dataset. This implies that methods that converge faster to their best performance will have a higher normalized AUC. Second, we consider how quickly the algorithms can locate and query relevant tokens (named entities). Third, we finally evaluate their ability to extract the most uncertain tokens from the unlabelled pool."
    }, {
      "heading" : "6 Results & Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Active Learning Performance",
      "text" : "Figure 1 shows the LCα performance curves for α = 0, α = 1 and the best performing value for each acquisition class (based on the normalised AUC score, Table 3) for full sentence querying (FS), and only the best performing α values for subsequence querying (SUB). The figure also shows the performance of training on the complete training set (No AL), and when the both sentences and subsequences are random selected by the acquisition function. The equivalent figures for MaxEntα are available in Appendix, and follow similar trends. Then, the performance of each curve, quantified in terms of the normalised AUC is summarised in Table 3.\nTable 2 shows further analysis of the best results in Figure 1, with best referring to acquisition function and optimal α. These results first show that subsequence querying methods are more efficient than querying full sentences, achieving their final F1 with substantially less annotated data, and with higher normalised AUC scores. For OntoNotes 5.0, querying subsequences reduces final proportion required by 38.8%. For CoNLL 2003, this reduction is 36.6%. Altogether, subsequence querying holds improved efficiency over the full sentence querying baseline.\nAs a point of interest, full sentence querying can be easily improved by optimising α alone. For the OntoNotes 5.0 dataset, using LC1, 24.2% of tokens are required to achieve F ∗1 . This however, can be improved by 9.33% to only requiring 22.0% by choosing α = 0.7. For CoNLL 2003, using LC1 for full sentences, 50.0% of the dataset was required, but when using LC0.7, it was 40.7% of the tokens."
    }, {
      "heading" : "6.2 Entity Recall",
      "text" : "This section and the next aim to understand some of the underlying mechanisms that allow the subsequence querying methods to achieve results substantially better than a full sentence baseline. Namely, the ability of the different methods to extract the tokens for which the model is the most uncertain about. Given that the majority of tokens in both datasets have the same label - “O”, signifying no entity - it is likely that tokens belonging to entities, particularly rarer classes, trigger higher model uncertainty. Querying full sentences at a time, the AL algorithm will spend much of its token budget for that round labelling non-entity tokens while attempting to locate the more informative entities. Subsequence querying methods, not faced with this wasteful behaviour, allow the AL algorithm to query entity tokens quicker, locating and labelling the majority of entity tokens faster over the course of training.\nThe proportion of tokens belonging to entities that the AL algorithm has queried against the round number is plotted in Figure 2 for OntoNotes 5.0. For both datasets, the random querying methods contain a distribution of token classes that reflect the dataset at large, producing roughly linear curves for this figure. Curves for all methods that employ\nan uncertainty based acquisition function are concave, and the AUC reflects the ranking of model performance for each querying method. This relation suggests that shortly after initialisation, better performing algorithm variations query entity tokens faster. In later stages of finetuning this rate is reduced, likely because after labelling a large proportion of them, the remaining entity tokens cause little uncertainty for the model. In a practical setting where querying may have to be stopped before model performance has converged (i.e. due to accumulated cost of annotations), it is greatly beneficial to ensure that the model is exposed to a high number of relevant tokens, because this increases the likelihood of locating entity tokens belonging to underrepresented classes at an early stage."
    }, {
      "heading" : "6.3 Uncertainty Score Analysis",
      "text" : "Finally, this section compares the scores of tokens in the queried set SQ for each querying method. Comparing the distribution and development of these scores provides a direct insight to the core assumptions of why full sentence querying is outperformed. Figure 3 shows the difference in score distributions for sentence versus subsequence querying, against querying round number, for rounds preceding model performance convergence. First, it is seen that decreasing the individual query size (full sentence to subsequence) increases the median uncertainty extracted at the earlier rounds. Second, Figure 3 provides evidence for the mechanism suggested earlier: aggregating the token scores across full sentences means querying both the highly uncertain tokens, and the tokens that provide little uncertainty. Querying high scoring sentences like this can cause a distribution with two peaks as seen in\nthe figure. As the model becomes increasingly certain about its predictions, high scores are localised within smaller subsequences, and the coarse sensitivity of full sentence querying means it forfeits all the higher scoring tokens. These differences were also observed when comparing subsequence querying methods with sub-optimal α.\nThis figure only analyses behaviour of up to 9% of the training set’s tokens have been queried. Instead, Figure 4 show how the mean of token-wise scores evolve for different querying methods for the OntoNotes 5.0 dataset until convergence. This clearly shows that subsequence querying methods converge faster over the full course of the algorithm compared to full sentence querying. This is consistent with Figure 1 in terms of initial rate and final time of model performance convergence, namely that model performance plateaus alongside the uncertainty score.\nKeeping track of query scores like this is also a reasonable idea in industrial applications. When\ntraining on a very semantically specific corpus, there may not be enough fully labelled sentences to build a test set. In that case, observing the rate progress of score convergence can be used as an early stopping method for the AL algorithm (Zhu et al., 2010)."
    }, {
      "heading" : "7 Conclusion & Future Work",
      "text" : "In this study we have employed subsequence querying methods for improving the efficiency of AL for NER tasks. We have seen that these methods outperform full sentence querying in terms of annotations required for optimal model performance, requiring 38.8% and 36.6% fewer tokens for the OntoNotes 5.0 and CoNLL 2003 datasets. Optimal results for subsequence querying (and full sentence querying) were achieved by generalising previously used AL acquisition functions, defining a larger family of acquisition functions for sequential data.\nThe analysis of § 6.3 suggests that a full sentence querying causes noisy acquisition functions due to the tokens in the queried sentences that were not\nhighly scored. This added noise reduces the budget efficiency, and a subsequence querying method eliminates a large part of this effect. This efficiency also translated into a faster recall of named entities in the dataset to be queried (§ 6.2).\nLimitations and future work: Limitations of this study are largely centred on the use of an oracle to provide tokens with their labels. With human annotators, the cropped context of subsequence queries may make them produce more inaccuracies than when annotating full sentences. such studies will help reveal how context affects label accuracy, how this, in turn, affects optimal hyperparameters in the subsequence selection process (such as optimal query length), further accommodations that must be made to effectively optimise worker efficiency, and how to deal with unreliable labels. We leave to future work the evaluation of these querying methods with human annotators. There are also ways to incorporate model generated labelling methods for more robust semi-supervision into our framework that we leave to future work. Finally, there are examples of other tasks for structured data, such as audio, video, and image segmentation, where the part of an instance may be queried. A generalisation of the strategy demonstrated for the NER case may allow for more efficient active learning querying methods for these other types of data."
    }, {
      "heading" : "A Model Architecture",
      "text" : "The model architecture is built of three sections. The character-level convolutional neural network (CNN) (LeCun and Bengio, 1998) character-level encoder extracts character level features, wwordj for each token x(i)j in a sentence.\nThen, a latent token embedding wembj corresponding to that token is generated. The full representation of the token is the concatentation of the two vectors: wfullj := (w char j ,w emb j ). The token-label embeddings, wemb, are initialised using word2vec (Ling et al., 2015), and updated during training and finetuning, as per the baseline paper. A second, token-level CNN encoder is used to generate {htokenj } `i j=1, given the tokenlevel representations {wfullj } `i j=1. The final tokenlevel encoding is defined by another concatentation: hEncj := (h token j ,wfullj ).\nFinally, a tag decoder is used to generate the token-level pmfs over the C possible token classes: {hEncj } `i j=1 LSTM−−−→ {ŷ(i)j } `i j=1."
    }, {
      "heading" : "B Model & Training Parameters",
      "text" : "Table 4 lists the hyperparameter values used to train the NER model. Note that while dropout is used during training, it is turned off when generating the probabilities that contribute to the scoring of the acquisition function. Model was developed using PyTorch, and trained on a Titan RTX."
    }, {
      "heading" : "C Dataset Analysis",
      "text" : "Here, we cluster similar labels in the BIO format, reducing the total K classes to the K(r) = (K + 1)/2 class groups c(r)1 , ..., c (r) K(r) . Therefore, c(r)1 corresponds exactly to c1, the empty label, while c (r) k , k > 1 groups the raw labels c2k−2 and c2k−1.\nFigures 5 and 7 show the distribution of these class groups for the OntoNotes 5.0 and CoNLL 2003 datasets respectively. For the former, counts range from 199 tokens for the ’LANGUAGE’ to 46698 tokens for the ’ORG’ class. The full available training set totals 1766955 tokens in 99333 sentences; this is partitioned into a train and validation set during experimentation. A further test set comprises of 146253 tokens in 8057 sentences. The latter’s training set contains 172210 tokens in 13689 sentences, and its test set has 42141 tokens in 3091 sentences sentences."
    }, {
      "heading" : "D Active Learning Results for Both Datasets",
      "text" : "In Figure 9 we show the model performance plotted against the percentage of the tokens used as a training set for all the combinations of acquisition functions."
    } ],
    "references" : [ {
      "title" : "A study",
      "author" : [ "Joshua C. Denny", "Hua Xu" ],
      "venue" : null,
      "citeRegEx" : "Denny and Xu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denny and Xu.",
      "year" : 2015
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Deep Bayesian active learning with image data",
      "author" : [ "Yarin Gal", "Riashat Islam", "Zoubin Ghahramani." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Gal et al\\.,? 2017",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2017
    }, {
      "title" : "Bayesian active learning for classification and preference learning",
      "author" : [ "Neil Houlsby", "Ferenc Huszár", "Zoubin Ghahramani", "Máté Lengyel" ],
      "venue" : null,
      "citeRegEx" : "Houlsby et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2011
    }, {
      "title" : "Active learning by querying informative and representative examples",
      "author" : [ "S. Huang", "R. Jin", "Z. Zhou." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(10):1936–1949.",
      "citeRegEx" : "Huang et al\\.,? 2014",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2014
    }, {
      "title" : "Label propagation for deep semi-supervised learning",
      "author" : [ "Ahmet Iscen", "Giorgos Tolias", "Yannis Avrithis", "Ondrej Chum." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Iscen et al\\.,? 2019",
      "shortCiteRegEx" : "Iscen et al\\.",
      "year" : 2019
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando CN Pereira" ],
      "venue" : null,
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Convolutional Networks for Images, Speech, and Time Series, page 255–258",
      "author" : [ "Yann LeCun", "Yoshua Bengio." ],
      "venue" : "MIT Press, Cambridge, MA, USA.",
      "citeRegEx" : "LeCun and Bengio.,? 1998",
      "shortCiteRegEx" : "LeCun and Bengio.",
      "year" : 1998
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li." ],
      "venue" : "Journal of machine learning research, 5(Apr):361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Two/too simple adaptations of Word2Vec for syntax problems",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "An experimental comparison of active learning strategies for partially labeled sequences",
      "author" : [ "Diego Marcheggiani", "Thierry Artières." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 898–",
      "citeRegEx" : "Marcheggiani and Artières.,? 2014",
      "shortCiteRegEx" : "Marcheggiani and Artières.",
      "year" : 2014
    }, {
      "title" : "Confidence-based active learning",
      "author" : [ "Mingkun Li", "I.K. Sethi." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(8):1251–1261.",
      "citeRegEx" : "Li and Sethi.,? 2006",
      "shortCiteRegEx" : "Li and Sethi.",
      "year" : 2006
    }, {
      "title" : "Active + semi-supervised learning = robust",
      "author" : [ "Ion Muslea", "Steven Minton", "Craig A. Knoblock" ],
      "venue" : null,
      "citeRegEx" : "Muslea et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Muslea et al\\.",
      "year" : 2002
    }, {
      "title" : "Bayesian batch active learning as sparse subset approximation",
      "author" : [ "Robert Pinsler", "Jonathan Gordon", "Eric Nalisnick", "José Miguel Hernández-Lobato." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 6359–6370. Curran Asso-",
      "citeRegEx" : "Pinsler et al\\.,? 2019",
      "shortCiteRegEx" : "Pinsler et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised learning: a brief review",
      "author" : [ "Y Reddy", "Viswanath Pulabaigari", "Eswara B." ],
      "venue" : "International Journal of Engineering Technology, 7:81.",
      "citeRegEx" : "Reddy et al\\.,? 2018",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2018
    }, {
      "title" : "Inserting micro-breaks into crowdsourcing workflows",
      "author" : [ "Jeffrey Rzeszotarski", "Ed Chi", "Praveen Paritosh", "Peng Dai." ],
      "venue" : "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 1(1).",
      "citeRegEx" : "Rzeszotarski et al\\.,? 2013",
      "shortCiteRegEx" : "Rzeszotarski et al\\.",
      "year" : 2013
    }, {
      "title" : "From theories to queries: Active learning in practice",
      "author" : [ "Burr Settles." ],
      "venue" : "Active Learning and Experimental Design workshop In conjunction with AISTATS 2010, volume 16 of Proceedings of Machine Learning Research, pages 1–18, Sardinia,",
      "citeRegEx" : "Settles.,? 2011",
      "shortCiteRegEx" : "Settles.",
      "year" : 2011
    }, {
      "title" : "An analysis of active learning strategies for sequence labeling tasks",
      "author" : [ "Burr Settles", "Mark Craven." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, page 1070–1079, USA. Association",
      "citeRegEx" : "Settles and Craven.,? 2008",
      "shortCiteRegEx" : "Settles and Craven.",
      "year" : 2008
    }, {
      "title" : "Deep active learning for named entity recognition",
      "author" : [ "Yanyao Shen", "Hyokun Yun", "Zachary Lipton", "Yakov Kronrod", "Animashree Anandkumar." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 252–256, Vancouver,",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Semisupervised active learning for sequence labeling",
      "author" : [ "Katrin Tomanek", "Udo Hahn." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Tomanek and Hahn.,? 2009",
      "shortCiteRegEx" : "Tomanek and Hahn.",
      "year" : 2009
    }, {
      "title" : "Support vector machine active learning with applications to text classification",
      "author" : [ "Simon Tong", "Daphne Koller." ],
      "venue" : "J. Mach. Learn. Res., 2:45–66.",
      "citeRegEx" : "Tong and Koller.,? 2002",
      "shortCiteRegEx" : "Tong and Koller.",
      "year" : 2002
    }, {
      "title" : "Cost-effective active learning for deep image classification",
      "author" : [ "K. Wang", "D. Zhang", "Y. Li", "R. Zhang", "L. Lin." ],
      "venue" : "IEEE Transactions on Circuits and Systems for Video Technology, 27(12):2591–2600.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Active learning with subsequence sampling strategy for sequence labeling tasks",
      "author" : [ "Dittaya Wanvarie", "Hiroya Takamura", "Manabu Okumura." ],
      "venue" : "Journal of Natural Language Processing, 18(2):153–173.",
      "citeRegEx" : "Wanvarie et al\\.,? 2011",
      "shortCiteRegEx" : "Wanvarie et al\\.",
      "year" : 2011
    }, {
      "title" : "Submodularity in data subset selection and active learning",
      "author" : [ "Kai Wei", "Rishabh Iyer", "Jeff Bilmes." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1954–1963,",
      "citeRegEx" : "Wei et al\\.,? 2015",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2015
    }, {
      "title" : "Confidence-based stopping criteria for active learning for data annotation",
      "author" : [ "Jingbo Zhu", "Huizhen Wang", "Eduard Hovy", "Matthew Ma." ],
      "venue" : "ACM Trans. Speech Lang. Process., 6(3).",
      "citeRegEx" : "Zhu et al\\.,? 2010",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "puria-radmard/RFL-SBDALNER unlabeled data (Wei et al., 2015; Wang et al., 2017; Tong and Koller, 2002).",
      "startOffset" : 42,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "puria-radmard/RFL-SBDALNER unlabeled data (Wei et al., 2015; Wang et al., 2017; Tong and Koller, 2002).",
      "startOffset" : 42,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "puria-radmard/RFL-SBDALNER unlabeled data (Wei et al., 2015; Wang et al., 2017; Tong and Koller, 2002).",
      "startOffset" : 42,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "To minimise the amount of data needed to train a model, AL algorithms iterate between training a model, and querying information rich instances to human annotators from a pool of unlabelled data (Huang et al., 2014).",
      "startOffset" : 195,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "lection methods can result in a waste of annotation budget (Settles, 2011).",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "rate label, annotation budgeting is typically done on a token basis (Shen et al., 2017).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "Annotators may in fact perform better if they are asked to annotate shorter sequences, because longer sentences can cause boredom, fatigue, and inaccuracies (Rzeszotarski et al., 2013).",
      "startOffset" : 157,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "Traditionally these algorithms choose data points which lie close to decision boundaries (Pinsler et al., 2019), where uncertainty is high, in order for the model to learn more useful information.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Key functions include predictive entropy (MaxEnt) (Gal et al., 2017), mutual information between model posterior and predictions (BALD) (Houlsby et al.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : ", 2017), mutual information between model posterior and predictions (BALD) (Houlsby et al., 2011; Gal et al., 2017), or the certainty of the model when making label predictions (here called LC) (Mingkun Li and Sethi, 2006).",
      "startOffset" : 75,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : ", 2017), mutual information between model posterior and predictions (BALD) (Houlsby et al., 2011; Gal et al., 2017), or the certainty of the model when making label predictions (here called LC) (Mingkun Li and Sethi, 2006).",
      "startOffset" : 75,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "There has been exploration of uncertainty and deep learning based AL for NER (Chen et al., 2015; Shen et al., 2017; Settles and Craven, 2008; Fang et al., 2017).",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "There has been exploration of uncertainty and deep learning based AL for NER (Chen et al., 2015; Shen et al., 2017; Settles and Craven, 2008; Fang et al., 2017).",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "Other works forgo this aggregation, querying single tokens at a time (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014).",
      "startOffset" : 69,
      "endOffset" : 149
    }, {
      "referenceID" : 23,
      "context" : "Other works forgo this aggregation, querying single tokens at a time (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014).",
      "startOffset" : 69,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "Other works forgo this aggregation, querying single tokens at a time (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014).",
      "startOffset" : 69,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "These works show that AL for NER can be improved by taking the single token as a unit query, and use semi-supervision (Reddy et al., 2018; Iscen et al., 2019) for training on partially labelled",
      "startOffset" : 118,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "These works show that AL for NER can be improved by taking the single token as a unit query, and use semi-supervision (Reddy et al., 2018; Iscen et al., 2019) for training on partially labelled",
      "startOffset" : 118,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, all of these past works use Conditional Random Fields (CRFs) (Lafferty et al., 2001), which have since been surpassed as the state-of-the-art for NER (and most NLP tasks) by deep learning models (Devlin et al.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "and add them to training set, and, (3) fine-tune the model using the updated training set (Huang et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "Previously used acquisition functions such as Least Confidence (LC) and Maximum Normalized Log-Probability (MNLP) (Shen et al., 2017; Chen et al., 2015) are generalised for vari-",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "The formulation above can be applied to other types of commonly used acquisition functions such as Maximum Entropy (MaxEnt) (Gal et al., 2017) by simply defining:",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "Finally, given the similarity of performance between MNLP and Bayesian Active Learning by Disagreement (BALD) (Houlsby et al., 2011) in NER tasks (Shen et al.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : ", 2011) in NER tasks (Shen et al., 2017), and the computational complexity required to calculate BALD with respect to the other activation functions, we will not compare against BALD.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "Our work forms a more flexible AL algorithm that operates on subsequences, as opposed to full sentences (Shen et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "In previous work based on the use of CRFs (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014) this was solved by using semisupervision on tokens for which the model showed low uncertainty.",
      "startOffset" : 42,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "In previous work based on the use of CRFs (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014) this was solved by using semisupervision on tokens for which the model showed low uncertainty.",
      "startOffset" : 42,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "In previous work based on the use of CRFs (Tomanek and Hahn, 2009; Wanvarie et al., 2011; Marcheggiani and Artières, 2014) this was solved by using semisupervision on tokens for which the model showed low uncertainty.",
      "startOffset" : 42,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "As in previous works (Shen et al., 2017), we use the two following NER datasets:",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "This dataset is made from a collection of news wire articles from the Reuters Corpus (Lewis et al., 2004).",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "Furthermore, the AL algorithm used in (Shen et al., 2017) will serve as one of the baselines following the same procedure.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "In that case, observing the rate progress of score convergence can be used as an early stopping method for the AL algorithm (Zhu et al., 2010).",
      "startOffset" : 124,
      "endOffset" : 142
    } ],
    "year" : 2021,
    "abstractText" : "Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance. Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data. However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences. Additionally, this standard method requires that the annotator has access to the full sentence when labelling. In this work, we overcome these limitations by allowing the AL algorithm to query subsequences within sentences, and propagate their labels to other sentences. We achieve highly efficient results on OntoNotes 5.0, only requiring 13% of the original training data, and CoNLL 2003, requiring only 27%. This is an improvement of 39% and 37% compared to querying full sentences.",
    "creator" : "LaTeX with hyperref"
  }
}