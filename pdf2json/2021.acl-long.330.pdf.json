{
  "name" : "2021.acl-long.330.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Societal Biases in Language Generation: Progress and Challenges",
    "authors" : [ "Emily Sheng", "Kai-Wei Chang", "Premkumar Natarajan", "Nanyun Peng" ],
    "emails" : [ "ewsheng@isi.edu,", "pnataraj@isi.edu,", "kwchang@cs.ucla.edu", "violetpeng@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4275"
    }, {
      "heading" : "1 Introduction",
      "text" : "Natural language generation (NLG) is a suite of techniques that enables the generation of humanreadable language for different goals. These techniques are the core components of applications such as virtual assistants, chat bots, automatic translators, summarizers, and creative language composers. Recent advances in techniques for language generation (e.g., GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), TransformerXL (Dai et al., 2019), XLNet (Yang et al., 2019)) powered by Transformers (Vaswani et al., 2017) and an increasing repository of available data have created more capable applications. This has, in turn, channeled more interest and effort\ninto developing NLG techniques. We emphasize the importance of better understanding how societal biases manifest in NLG techniques, because NLG applications directly interact with many different users to generate novel content in various domains (e.g., chat bots for health, education, and customer support). However, when techniques are less effective or detrimental for marginalized populations, these techniques can inadvertently become gatekeepers of those populations for generation and associated language technologies. For example, an educational chat bot that produces more negative responses for topics about a specific ethnicity will discourage users of that ethnicity from interacting with the chat bot. While it is generally important to study the societal impact of NLP and AI techniques, we argue that the direct user impact of NLG techniques makes it especially important to carefully quantify the impact.\nMotivated by the importance of fairness in language generation, we present the first comprehensive survey on societal biases in language generation. By enumerating how NLG techniques contribute to biases and examining progress towards bias analysis and mitigation, we contextualize the discussion of broader trends and challenges. Specifically, we focus on techniques for NLG tasks, i.e., tasks that generate a sequence of text.1 Finding a lack of studies on biases from decoding techniques, we additionally present an experimental study to quantify the effects of various decoding techniques.\nBefore we delve into the details of biases in language generation, we first position our survey in the context of other relevant surveys and position papers. Sun et al. (2019) present a focused survey\n1Although bi-directional language models like BERT (Devlin et al., 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020). Thus, we limit the scope of this survey to the latter models.\non mitigating gender biases and Shah et al. (2020) categorize sources of biases—both largely focus on natural language understanding (NLU) tasks, while we examine biases in NLG tasks. Additionally, Blodgett et al. (2020) urge for more explicitly tying “biases” in NLP to societal normative definitions of biases and social hierarchies; with their recommendations in mind, we discuss the negative impacts of biases in NLG techniques.\nOur contributions are a comprehensive survey on societal biases in language generation and an experimental study on biases from decoding techniques. To start, we describe classes of NLG tasks (Sec. 2) and subsequently examine examples of biases and harms in NLG (Sec. 3). We then discuss NLG techniques that facilitate biases, including a study of decoding techniques (Sec. 4). Sec. 5 highlights progress and challenges, and Sec. 6 presents open problems and proposals. We hope this survey brings more visibility to the importance of carefully considering different components of NLG pipelines for potential biases and mitigation methods."
    }, {
      "heading" : "2 Language Generation Tasks",
      "text" : "To begin, we categorize generation tasks and introduce existing bias studies relevant to each task. NLG tasks broadly fall into two categories: those\nthat generate text continuations conditioned on some prompt and those that transform text from one form to another. Table 1 organizes various bias-related works for NLG tasks."
    }, {
      "heading" : "2.1 Continuation Generation Tasks",
      "text" : "The continuation class includes autocomplete and dialogue generation, where the goal is to generate text that is coherent and relevant to a prompt. Autocomplete Generation We use the term autocomplete generation to refer to conditional generation directly from language models. Language models are the core components for many NLG and NLU tasks, and this task enables directly quantifying biases in large, pre-trained language models (Bordia and Bowman, 2019; Sheng et al., 2019; Solaiman et al., 2019; Brown et al., 2020). Existing works analyzing biases in autocomplete generation have mostly examined Transformer-based models, including GPT (Shwartz et al., 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al., 2020), CTRL (Dhamala et al., 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al., 2020; Vig et al.,\n2020; Yeo and Chen, 2020), though Bordia and Bowman (2019); Qian et al. (2019) also look at LSTM-based models. Dialogue Generation Dialogue generation is conditioned on user inputs and can be for specific domains (e.g., health, customer service) and tasks (e.g., behavior intervention, booking flights) or general chit-chat. These dialogue applications directly interact with users, and any propagated biases directly affect user behavior and actions. In terms of recurrent dialogue models, Henderson et al. (2018) analyze biases in hierarchical recurrent encoder-decoder architectures and Liu et al. (2020a,b) analyze LSTM-based encoder-decoder models. Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus on Transformer-based models such as DialoGPT (Zhang et al., 2020) and other custom architectures."
    }, {
      "heading" : "2.2 Transformation Generation Tasks",
      "text" : "The transformation class includes machine translation and various formulations of text re-writing. The general goal of these tasks is to transform text into a form with targeted properties. Machine Translation Translation is the task of transforming text between languages while preserving the meaning. Existing works on biases in machine translation have almost exclusively focused on issues of gender biases2 in a variety of academic and commercial systems. The use of grammatical gender in some languages and not in others can expose unwanted gender associations (e.g., for different occupations) through translation (Prates et al., 2019). Earlier works by Vanmassenhove et al. (2018) and Elaraby et al. (2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021). While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al. (2019) also\n2For a detailed survey of gender bias in machine translation, we refer readers to Savoldi et al. (2021).\n3https://translate.google.com\nstudy Microsoft Translator,4 Amazon Translate,5 and SYSTRAN;6 Cho et al. (2019) additionally look at Naver Papago7 and Kakao Translator,8 and Cho et al. (2021) also examine Yandex.9 Re-writing We use the term re-writing to refer to tasks of revising specific words and phrases in the original text to be more aligned with a targeted attribute. Specifically, there have been studies on re-inflection (Habash et al., 2019; Zmigrod et al., 2019; Alhafni et al., 2020) and re-writing text to use neutral viewpoints (Pryzant et al., 2020), genderneutral English (Sun et al., 2021), or more agency (Ma et al., 2020). These tasks typically rely on custom encoder-decoder models."
    }, {
      "heading" : "2.3 Other Tasks",
      "text" : "There are other NLG tasks, such as the continuation tasks of story and poetry generation, and the transformation tasks of abstractive summarization and paraphrase generation. However, these other NLG tasks are not yet well-studied in the context of societal biases.10"
    }, {
      "heading" : "3 Biases and their Negative Impacts",
      "text" : "In this section, we introduce how existing studies of biases in NLG tasks commonly quantify biases and their negative impacts."
    }, {
      "heading" : "3.1 Bias Definitions and Metrics",
      "text" : "In the context of AI fairness, the term “bias” commonly refers to skews that result in undesirable impacts (Crawford, 2017) and is quantifiable with some metric. There are relatively more existing studies on biases in NLU tasks, where it is arguably simpler to define bias metrics, since we can intuitively compare the accuracy of the task (e.g., coreference resolution, hate speech detection) for different demographics. Language generation tasks often involve stochastic generation of open-ended and lengthy texts, traits that are not directly compatible with traditional algorithmic bias definitions (e.g.,\n4https://www.bing.com/translator 5https://aws.amazon.com/translate 6https://www.systransoft.com 7https://papago.naver.com 8https://translate.kakao.com 9https://translate.yandex.com\n10Lucy and Bamman (2021) is an exception that analyzes gender in generated stories. While there are studies of biases in poetry generation and summarization, they focus on non-NLG biases: Sheng and Uthus (2020) investigate biases in a poetry composition system, but in the context of information retrieval; Celis and Keswani (2020) analyze biases in extractive summarization.\nequalized odds, equal opportunity, demographic parity (Dwork et al., 2012; Hardt et al., 2016)).\nBecause of the difficulty in defining metrics, existing works define bias loosely as demographic inequality and use intermediate proxy metrics to comparatively measure bias. Examples include: • Regard Ratio: negative-neutral-positive regard\nscore ratios of text generated from bias-inducing prompts (Sheng et al., 2019) • Sentiment Ratio: negative-neutral-positive sentiment score ratios of text generated from African American English (AAE) versus White-Aligned English (WAE) prompts (Groenwold et al., 2020) • Individual and Group Fairness through Sentiment: comparisons of the sentiment distributions of generated text across demographics and prompts (Huang et al., 2020) • Gendered Word Co-occurrence Score: mean and standard deviations of the absolute log ratio of probabilities: P(word|female terms) to P(word|male terms) across all words in generated text (Bordia and Bowman, 2019)\nThere are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al., 2019; Vig et al., 2020; Dinan et al., 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020). Studies of biases in transformation generation tasks favor metrics of accuracy in terms of successfully transforming text to have a desired property. We present a more thorough comparison of metrics in Section 5.4.\nBias metrics can also be categorized by how they define associations between demographic group attributes and text. Biases can be towards people described in text, people who produce the text, or people to whom the text is addressed (Dinan et al., 2020b). Most existing works define bias metrics through the first association—these biases are relatively easier to analyze, since both the demographic and the textual signals of bias are encapsulated within the text. There are also works that define biases towards people who produce the text (Groenwold et al., 2020) or people to whom the text is addressed (Sheng et al., 2021b), though there are relatively fewer works that study these latter associations."
    }, {
      "heading" : "3.2 Negative Impacts",
      "text" : "Biases in NLG techniques are important to study because they can result in harmful, negative im-\npacts. We survey detrimental representational11 and allocational12 impacts (Crawford, 2017; Barocas et al., 2017; Blodgett et al., 2020) used to motivate existing studies of bias in NLG tasks, finding limited examples. While representational impacts are sometimes cited, it is difficult to measure the extent of the impacts. Additionally, techniques for effective NLG are relatively new, and existing studies have limited knowledge of potential allocational impacts. Finally, biases in NLG tasks give rise to a third type of negative impacts, which we call vulnerability impacts.\nRepresentational Impacts The works in Table 1 motivate (to varying degrees) studying biases in NLG through potential negative representational impacts, in the form of propagating stereotypes, misrepresentations, or denigrations of social groups. For example, Sheng et al. (2019) enumerate how generated text can propagate varying social perceptions of different demographics, and Prates et al. (2019) discuss how occupation-related gender biases could propagate stereotypes in translation. However, it is difficult to quantify the effects of representational impacts;13 while such impacts may be measured indirectly (e.g. by analyzing allocational impacts), we suggest long-term, interdisciplinary collaborations to explore the direct effects of these representational impacts.\nAllocational Impacts Harmful allocational impacts result from an unequal allocation of resources across groups. Since effective NLG techniques based on large Transformer models (Vaswani et al., 2017) are relatively new, most of the existing works on biases in NLG that list possible impacts only analyze direct representational consequences. A real example of a negative allocational impact is when machine translation errors lead to arrests (Ong, 2017). In general, technologies that are less effective or detrimental for certain populations become barriers that actively prevent those populations from using the technology, leading to diminished opportunities in jobs, education, health, etc. We discuss more details in Section 4.5. With continuous technological advances, more organizations will turn to effective NLG techniques, making it imperative to start setting norms to reduce harmful allocational impacts (Tamkin et al., 2021).\n11Unfair representations of different groups 12Unfair allocation of resources 13Kay et al. (2015) is a rare example that explicitly studies\nthe effect of representational impacts in image search.\nVulnerability Impacts Open-domain generation tasks can amplify a group’s vulnerability to manipulation and harm, which is an intermediate impact that makes a group more susceptible to representational and allocational impacts. For example, privacy-related issues (Carlini et al., 2020), misinformation (Levy et al., 2021), or radicalizing views in generated text could make a group more likely to be attributed to specific stereotypes (e.g., through action guided by misinformation) or end up with diminished opportunities (e.g., by having personal data exposed and misused). Separately identifying vulnerability impacts could help facilitate recognition of other negative impacts."
    }, {
      "heading" : "4 Contributors to NLG Biases",
      "text" : "In a pipeline from data collection to evaluation for an NLG task, each component could propagate biases.14 We emphasize the ways in which data, model architecture, decoding, evaluation, and deployment uniquely exacerbate biases in generation tasks. Additionally, we present an empirical study to show how measured biases in generated text can vary based on decoding technique."
    }, {
      "heading" : "4.1 Biases from Data",
      "text" : "Modern NLP models often rely on large pre-trained language models, which in turn rely on a large collection of data to learn explicit and implicit associations. Several recent pre-trained language models used for NLG tasks, e.g., T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020), are trained on the largest datasets used for any models. These large models for generation are commonly trained on web data, which is known to contain biased language (e.g., Ferrer et al. (2021) discover gender, religion, and ethnic biases in Reddit communities). While preprocessing is often included to filter out malformatted data and explicitly negative content (e.g., bad words and offensive phrases), those are generally the only efforts to reduce biases and associated impacts. Furthermore, by filtering out all words deemed “bad”, Bender et al. (2021) warns that we remove the discourse of marginalized populations. Paullada et al. (2020), Bender and Friedman (2018), and Gebru et al. (2018) provide more comprehensive surveys and frameworks that focus on aspects of data creation and management that\n14Task formulation and application deployment are also part of NLG task pipelines (Kiritchenko et al., 2020), though we do not focus on biases in these areas.\ncould lead to biases, and we refer readers to their works for more discussion. In the context of translation, Cho et al. (2021) find that more data can increase translation fluency but may also make the system more biased."
    }, {
      "heading" : "4.2 Biases from Model Architecture",
      "text" : "There are relatively few studies that examine model architectural properties that could lead to biases. We discuss the few efforts towards understanding model biases in NLG tasks and emphasize the need for more to generalize. For autocomplete generation, Vig et al. (2020) analyze GPT-2 variants through a causal mediation analysis, finding that larger models contain more gender bias, and bias tends to be concentrated in a small number of neurons and attention heads. Silva et al. (2021) observe amplified biases in distilled versus original models. For machine translation, Costa-jussà et al. (2020) note that language-specific architectures are less biased because they encode more gender information than shared language encoder-decoder architectures. Studies like the aforementioned are useful for designing targeted bias mitigation methods (e.g., controlled generation to target specific attention heads or regularization to retain gender information). However, more evidence would be needed to generalize findings across models.15"
    }, {
      "heading" : "4.3 Biases from Decoding",
      "text" : "While NLU and NLG models have structural similarities, NLG tasks uniquely use search or sampling techniques at inference time to generate text. Popular techniques include: • Greedy Search: at each time step, choose the\nword with the highest probability. • Beam Search: at each time step, keep the top b\nhypotheses with highest probabilities; eventually pick the hypothesis with the highest probability. • Top-k sampling (Fan et al., 2018): at each time step, re-distribute the probability mass of the top k words with highest probabilities and sample. • Nucleus sampling (Holtzman et al., 2019): at each time step, re-distribute the probability mass of the smallest set of words with a cumulative probability exceeding p and sample.\nMore constrained forms of generation such as machine translation generally use variations of beam\n15We also refer the reader to the work of Park et al. (2018) that discusses biases in NLU tasks from model components that “attend” to specific words (e.g., through attention or pooling), which could be applicable to NLG tasks as well.\nsearch; however, preferred decoding techniques are more varied for open-domain generation. Despite variations in fluency and diversity between deterministic versus stochastic, search versus sampling procedures, there are limited studies (Roberts et al., 2020) on how different decoding properties affect biases in generation. A Study on Biases from Decoding To study how decoding techniques affect biases in generation, we use existing NLG bias metrics to evaluate text generated from different decoding methods.16 We examine autocomplete generations from GPT, GPT-2, and XLNet, using the decoding techniques from Section 4.3. We evaluate with the following bias metrics: regard ratios (Sheng et al., 2019), sentiment ratios (Groenwold et al., 2020), individual and group fairness through sentiment scores (Huang et al., 2020), and gendered word co-occurrence scores (Bordia and Bowman, 2019) (as introduced in Section 3). More experimental details can be found in the Appendix.\nIn Section 5.4, we distinguish between relative and absolute score metrics to examine evaluation differences between NLG tasks. Here, we organize our results into these categories to generalize trends about decoding techniques. The ratio-based metrics are relative score metrics, since evaluation relies on comparing ratios between demographics. The latter three metrics are absolute score metrics that have target values of zero indicating no bias.\nFor the relative score metrics, search and sampling techniques generate similar outcomes. An interesting result between sampling techniques for the regard metric is that nucleus sampling is less biased yet more negative than top-k sampling. For the absolute score metrics, we find that beam search is the most unbiased technique, closely followed by greedy search and then top-k and nucleus sampling. Through our study, we discover that text diversity is not accounted for in any of the bias metrics, yet diversity can be a confounding factor. Specifically, beam search is the least diverse,17 followed by greedy search, top-k sampling, then nucleus sampling. Results indicate that the less diverse search techniques lead to better scores for individual fairness, group fairness, and gendered word co-occurrence ratios.\nWe hope these experimental results will encour-\n16Code at https://github.com/ewsheng/ decoding-biases.\n17We report average generated text length and vocabulary sizes to estimate diversity in Appendix Table 4.\nage researchers to document sampling techniques, consider how metrics can be formulated to evaluate both bias and other factors of generation quality, and inspire more comprehensive studies.18"
    }, {
      "heading" : "4.4 Biases from Evaluation",
      "text" : "Biases can arise from both general evaluations and bias evaluations for NLG tasks. General Evaluations Current standards for NLG evaluation can reinforce certain types of language and penalize others. For example, using perplexity as measured by models pre-trained on datasets largely containing non-AAE text leads to an unfair evaluation of AAE text. Additionally, the subjectivity of generation tasks means that much of NLG evaluation depends on human labels. Since humans from different backgrounds are accustomed to different societal norms and linguistic variations, the choice of human annotators could drastically influence the evaluation standards for generated text. Bias Evaluations It is difficult to evaluate societal biases in NLG tasks because NLG can be open-domain, and there are many different notions of biases from various backgrounds and cultures (Sambasivan et al., 2021). These factors lead to the use of a variety of metrics to evaluate biases (Section 3). To avoid experimental bias in evaluation, we recommend using multiple metrics to cover many types of biases at various granularities. We identify three points to emphasize the need for more comprehensive evaluations. First, most existing works on biases in generation center around one demographic dimension (often gender and from a Western perspective, e.g., using standard Western occupations). While there has been no comprehensive study on whether mitigating biases for one demographic dimension (e.g., gender) may exacerbate biases for others (e.g., race, intersectional identities), this is a possibility we must consider. Second, most works only evaluate bias through a single intermediate proxy; however, different metrics are defined at different granularities (e.g., sentiment is sentence-level, gendered word ratio is word-level). Finally, different evaluation datasets test for specific types of biases and are influenced by the backgrounds of the curators. Collectively evaluating biases across demographic dimensions and granularities can thus help reduce experimentally-biased evaluations.\n18Results are summarized in Appendix Tables 2, 3, and 5."
    }, {
      "heading" : "4.5 Biases from Deploying Systems",
      "text" : "In terms of deploying NLG systems, there is a feedback loop that benefits some communities and further disadvantages others. While this feedback loop is not unique to NLG systems, these systems that directly interact with users make good cautionary examples.\nFirst, many deployed language technologies require internet access both to use and contribute feedback, thus favoring the views and languages of those privileged with this access. For example, anyone can contribute feedback to Google Translate, but if contributions and subsequent improvements are focused on high-resource languages, this further increases the accuracy gap between the high and low resource languages, diminishing opportunities for speakers of the low resource languages, i.e., representation disparity (Hashimoto et al., 2018).\nSecond, those who are unable to achieve their goals from using these language technologies (e.g., unsuccessful translation, unhelpful or offensive chat bot) are less likely to continue using the technology. This means that there is less feedback and data to improve the technologies, reinforcing the decreased effectiveness for certain populations, i.e., disparity amplification (Hashimoto et al., 2018).\nOne way we might intervene is to follow a more targeted approach for data and feedback collection, e.g., from excluded populations. However, we acknowledge that this remains a difficult task and that it is also necessary to be aware of “community goals” and other factors in order to co-design language technologies without inflicting additional harm on marginalized populations (Bird, 2020)."
    }, {
      "heading" : "5 Progress, Trends, and Challenges",
      "text" : "Following the discussion of contributors to biases, we survey trends and challenges for reducing biases in NLG."
    }, {
      "heading" : "5.1 Data Methods",
      "text" : "Data-based methods for both bias analysis and mitigation use the general idea of counterfactual data augmentation (CDA) (Lu et al., 2020) to curate sets of counterfactual prompts. A common method for analysis is using targeted prompts to induce NLG models to reveal biases. For data-based mitigation, existing works focus on fine-tuning large models or training smaller models with datasets that are balanced with respect to targeted demographics. Curated Datasets Existing datasets to study biases in translation include parallel sentences tagged with speaker or subject gender information (Vanmassenhove et al., 2018; Habash et al., 2019) and datasets to study gender biases when translating from neutral references of a person (e.g., nurse in English, gender-neutral pronouns) to gendered instances (e.g., enfermera or enfermero in Spanish, gendered pronouns) (Cho et al., 2019; Stanovsky et al., 2019; Gonen and Webster, 2020; Kocmi et al., 2020). Renduchintala and Williams (2021) additionally provide a dataset to study translation of neutral references in unambiguous contexts. Other works present parallel corpora of biased versus unbiased framings and presuppositions (Pryzant et al., 2020) and AAE versus WAE equivalents (Groenwold et al., 2020). Sheng et al. (2019); Huang et al. (2020); Dhamala et al. (2021) additionally curate sets of prompts that can be used to evaluate biases in autocomplete generation. Bias Analysis Most bias analyses of NLG tasks use prompts to probe for different biases in generated text, e.g., regarding social perception (Sheng et al., 2019), gender in translation (Prates et al., 2019), names (Shwartz et al., 2020), sentiment distribution (Huang et al., 2020), dialects (Groenwold et al., 2020), dialogue personas (Sheng et al., 2021a), or other notions of similarity across demographics (Yeo and Chen, 2020; Henderson et al., 2018). Vig et al. (2020) also use prompts to investigate gender biases, though they do so in the context of a causal mediation analysis. Furthermore, Prates et al. (2019) and Farkas and Németh (2020) compare pronoun gender biases in translations (induced with prompts) to real-world statistics. Bias Mitigation Methods can broadly be classified into two categories based on the type of data applied. The first category encompasses methods that fine-tune or train on a balanced dataset to lessen the effects of the model relying on spurious correlations between imbalanced data and task performance. CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al., 2020). The second category is methods that attach a short prefix at training time (Vanmassenhove et al., 2018; Basta et al., 2020; Alhafni et al., 2020) or inference time (Moryossef et al., 2019). Challenges The size of state-of-the-art pretrained models and varying definitions of biases\nin generation present difficulties for creating standardized datasets that are generally effective across biases and demographics. Moreover, it remains to be seen whether data-based mitigation is as effective for open-domain NLG tasks as it is for more constrained settings."
    }, {
      "heading" : "5.2 Training Methods",
      "text" : "In addition to data-based mitigation, training-based mitigation is another popular class of methods to reduce biases in generation. Bias Mitigation Several works that use trainingbased mitigation techniques rely on regularization (Bordia and Bowman, 2019; Qian et al., 2019; Huang et al., 2020; Liu et al., 2020a; Saunders and Byrne, 2020). There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by appending a target value to inputs during training (Ma et al., 2020), by using a normative classifier to produce reward values for backpropagation (Peng et al., 2020), or through adversarial training (Liu et al., 2020b). Other techniques include using debiased word embeddings (Escudé Font and Costajussà, 2019), identifying and editing out subjective words (Pryzant et al., 2020), and using Markov random fields to preserve morpho-syntactic agreement during reinflection (Zmigrod et al., 2019). Challenges The main challenge of bias mitigation through training methods is that it is costly and impractical to re-train models for new biases encountered. In fact, most of the techniques that rely on training from scratch use smaller architectures (exceptions are from larger institutions)."
    }, {
      "heading" : "5.3 Inference Methods",
      "text" : "While the existing literature on inference time methods for bias mitigation is sparse, decoding-based methods are a promising alternative to data- and training-based methods. Specifically, these methods are compatible with any pre-trained language model for generation without additional training. Given recent development of inference-time methods for control that can reduce toxicity (e.g., PPLM (Dathathri et al., 2019), GeDi (Krause et al., 2020), DExperts (Liu et al., 2021)), there is potential for extending these methods to bias mitigation. Bias Mitigation For autocomplete and dialogue generation, Sheng et al. (2020) formulate bias triggers using gradient-based methods of Wallace et al. (2019). These triggers are appended to prompts during inference time to control text generation to\nbe more equalized towards different demographics. For translation, Saunders and Byrne (2020) present a lattice rescoring procedure that creates genderinflected search spaces to rescore text for more accurate translations, and Saunders et al. (2021) subsequently use this lattice structure to present more gendered options during beam search and rerank translation hypotheses according to gender criteria. For dialogue generation, Sheng et al. (2021b) introduce a constrained decoding method that uses n-gram similarity to guide generation away from ad hominems towards marginalized groups. For autocomplete generation, Schick et al. (2021) present a self-debiasing scheme that re-weights word probabilities to generate less undesirable words. Challenges Control methods at inference time could potentially steer the model into degenerate spaces, so it is important to also evaluate these methods for coherence, fluency, and task relevance."
    }, {
      "heading" : "5.4 Evaluation Methods",
      "text" : "There are two types of evaluations: those that rely on absolute scores and those that rely on relative scores. Absolute score evaluations use an accumulated score to summarize inequalities between demographics, whereas relative evaluations explicitly report inequalities between all demographics. While it is possible to convert between relative and absolute scores, distinguishing between how existing works choose to portray evaluations allows us to examine differences between generation tasks. Absolute Evaluations We find that the transformation class of generation tasks favors bias evaluation through absolute metrics, which is possible because these tasks involve relatively more constrained forms of generation. Examples of evaluation objectives through absolute scores include Peng et al. (2020) reducing non-normative generations, Ma et al. (2020) increasing the accuracy of the change in agency, Zmigrod et al. (2019) increasing the number of correct inflections, Huang et al. (2020) reducing individual and group fairness scores, and Sheng et al. (2021b) reducing the amount of ad hominems towards marginalized groups. Studies of gender bias in machine translation are well-suited to evaluations using absolute scores: many use BLEU and its variants to evaluate correct gender inflections and translations (Moryossef et al., 2019; Escudé Font and Costajussà, 2019; Elaraby et al., 2018; Habash et al., 2019; Alhafni et al., 2020) or accuracy on WinoMT (Saunders and Byrne, 2020; Saunders et al., 2020;\nKocmi et al., 2020; Costa-jussà and de Jorge, 2020; Costa-jussà et al., 2020; Basta et al., 2020; Choubey et al., 2021; Saunders et al., 2021). Relative Evaluations In terms of evaluation through relative scores, examples from existing works are mainly from continuation generation tasks. We infer that the less constrained, opendomain nature of continuation generation tasks makes it more preferable to evaluate mitigation through more flexible comparisons rather than absolute scores. For autocomplete generation, Sheng et al. (2019, 2020) and Groenwold et al. (2020) compare regard or sentiment scores across demographics, Shwartz et al. (2020) compare names across various intermediate metrics, Vig et al. (2020) measure proportional differences between the amount of bias under a gendered versus ambiguous reading, and Yeo and Chen (2020) compare occupations generated for different genders. Bias studies in dialogue generation use relative scores by comparing sentiment and offensive language discrepancies (Henderson et al., 2018; Liu et al., 2020a,b) and the percentage of gendered words (Dinan et al., 2020a). Challenges A trade-off between framing biases as a relative or absolute metric is that relative metrics can be more flexibly aligned to normative concerns like social perception. Absolute metrics that look for ratios of gendered words or other indicator words assume that there is a set of words that captures all the differences between demographic groups, regardless of whether these differences are related to normative definitions of harm. There are also absolute metrics such as those of Huang et al. (2020) that can incorporate intermediate metrics that are more aligned with normative behavior, though these metrics reduce the notion of biases to a single value, which could erase historical inequalities between groups."
    }, {
      "heading" : "6 Open Problems and Proposals",
      "text" : "As a fairly nascent area of exploration, the study of biases in language generation still poses many challenges. Throughout this paper, we discuss challenges associated with different components in a generation pipeline. With a heightened awareness of the relevant body of work, we conclude with recommendations for open problems. Bias-Aware Data Curation Many works have highlighted the harms and problems when collecting training datasets with limited awareness\nfor potential harms. Since effective models for NLG tasks are correlated with increasing training data sizes, biases in data collection (e.g., Englishcentric, drawn from popular Western media) remain a major contributor of biases that manifest in generation. Additionally, datasets used to study biases in generation can also be limited (e.g., only for binary gender classes). For more bias-aware data curation, we suggest diversifying datasets to include more viewpoints from various groups.\nUnderstanding Trade-Offs Different methods for analysis, mitigation, and evaluation have unique trade-offs. Existing works have been relatively small-scale and limited to a small number of biases for specific tasks. Some useful questions to consider when developing methods to study generation biases are whether we can generalize methods to a diverse set of biases and a wide range of contexts. It is also important to consider formulating metrics that would jointly mitigate biases and preserve other desired text qualities (e.g., diversity, fluency).\nInteractive and Continuous Learning The difficulties of measuring and mitigating biases in generation can be reduced with a general framework for interactive and continuous learning. Over time, such a system could learn from diverse opinions of what constitutes “fair” versus “unfair” generations across tasks. A unified framework would centralize and highlight the importance of studying biases in generation, as well as fuel the development of a more comprehensive set of evaluations that may be useful for large-scale studies of impact.\nFocusing on Negative Impacts Section 3 discusses how there are very few existing works on biases that explicitly and meaningfully engage with resulting negative impacts, even though these impacts are what motivate reducing biases. By reframing efforts on reducing negative impacts rather than biases, we may be able to define metrics and progress that better correlate with reducing harm. For example, relative framings of bias metrics could better enable metrics to be more aligned with reducing harms for particularly impacted groups."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Seraphina Goldfarb-Tarrant, Sunipa Dev, Jason Teoh, members of the Plus Lab, and our anonymous reviewers for the many helpful suggestions that went into this paper.\nEthics and Broader Implications\nIn this work, we present a survey and commentary on the progress and challenges for studying societal biases in language generation. Data We do not check the quality of the datasets used to train popular language generation models (due to limited availability and size), though we do briefly mention problems that other works have found regarding using large datasets that have been minimally filtered. Some of the surveyed datasets and metrics that are used for evaluating biases approximate binary genders using names typical of specific genders, and may be better re-formulated to avoid harms and curate a more accurate representation of different genders. On the subject of genders, the majority of bias evaluation data also only evaluate for binary genders—we point out this issue in our survey as well. Techniques Most of the techniques surveyed in this work are trained with or bias-tested with data drawn from Western sources or culture, since that is largely the focus of the existing body of work. We also refer to studies that point out how techniques for bias do not always transfer across cultures. Our decoding experiments could potentially fuel misuse by giving those with adversarial interests a better understanding of how decoding algorithms could thwart bias metrics, though we believe transparency around these results outweigh the potential for misuse."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Evaluating Biases Across Decoding Techniques and Metrics\nTo gain more insight into biases from different decoding techniques, we examine autocomplete generations from GPT (110M params), GPT-2 (small, 117M params), and XLNet (base, 110M params), using the decoding techniques described in Section 4.3 through the Transformers19 library. We use standard parameters of b = 16 for beam search, k = 40 with a temperature of 0.7 for top-k sampling, and p = 0.95 for nucleus sampling (Holtzman et al., 2019). In terms of bias metrics, we use existing NLG bias metrics: regard ratio (Sheng et al., 2019), sentiment ratio (Groenwold et al., 2020), individual and group fairness through sentiment (IF/GF) (Huang et al., 2020), and a gendered word co-occurrence scores (Bordia and Bowman, 2019). For all sentiment scores, we use the rule-based sentiment analyzer, VADER (Hutto and Gilbert, 2014).20 We run all our experiments on an RTX 2080Ti GPU. Generation takes from a couple of minutes to a few hours, depending on the number of samples generated. Regard Ratios Sheng et al. (2019) introduce 10 prompts to induce occupation- and respect-related generations (e.g., [BLANK] worked as, [BLANK] was thought of as) and six demographics (Black, White, man, woman, gay, straight) to fill in the [BLANK], for a total of 60 prompts. The authors define regard as the social perception towards a demographic, collect human annotations, and release a BERT-based regard classifier.21 We follow the original work in reporting percentages of negative, neutral, and positive regard scores per demographic. For the deterministic search methods, we do not report scores since there are only 10 samples per demographic. For the stochastic sampling methods, we generate 1000 samples per demographic. Additionally, we use the regard classifier released by the authors for our evaluations—while we acknowledge that this classifier could also have biases, we believe it is still worthwhile to use it to compare text generated from different decoding techniques.\n19https://huggingface.co/transformers 20Kiritchenko and Mohammad (2018) show that sentiment classifiers can exhibit biases. We use VADER since 1) it does not rely on learned associations and thus may be less prone to biases, and 2) it has been used to measure biases in previous works (Sheng et al., 2019; Groenwold et al., 2020).\n21https://github.com/ewsheng/nlg-bias\nSentiment Ratios for AAE and WAE Prompts Groenwold et al. (2020) curate a parallel set of 2,019 AAE and 2,019 WAE prompts and use sentiment classifiers to label text generated from the prompts. Similar to Sheng et al. (2019), this work also reports percentages of negative, neutral, and positive scores. The VADER sentiment analyzer that we use reports scores in the range of [-1, 1]. When reporting ratios, we use splits recommended by the authors (Hutto and Gilbert, 2014) to categorize sentiment values into negative (value<=−0.05), neutral (−0.05<value<0.05), and positive (value>=0.05) bins. When reporting average values, we calculate from the unrounded scores from VADER. We generate one sample per prompt for all decoding techniques.\nIndividual and Group Fairness Through Sentiment Huang et al. (2020) evaluate fairness across countries, occupations, and genders (binary, as defined through Western names typical of a gender) by first defining 10 templates per dimension (e.g., People from [BLANK] are). For each dimension, they also define a list of dimension instances (e.g., Syria as a country) to fill in the [BLANK]. In total, there are 730 prompts across the three attributes. For our experiments, we generate one sample per prompt.\nThe authors define the individual fairness metric by “...averaging the Wasserstein-1 distance between the sentiment score distribution of every evaluation sentence and each of its counterfactual sentences across all templates.” For example, we would compute the distance between the sentiment distributions of the text generated from the template People from [BLANK] are for each of the country choices for [BLANK], and sum up the distance scores for all pairs across all templates.\nFor group fairness, the authors calculate the average of the “Wasserstein-1 distance between the sentiment distributions of all generated sentences of inputs from [a] subgroup, and that over the entire evaluation set”. Here, a subgroup means each country, occupation, or binary gender. For example, we compare the distance between the sentiment distribution of text generated for Syria (across all templates) and the sentiment distribution of text generated for all countries.\nWe use Huang et al. (2020)’s prefix templates and fairness metrics exactly as defined in the original work, so we refer readers to the original work for more details.\nGendered Word Co-occurrence Scores This score is based on the one proposed by Bordia and Bowman (2019), though we use different gendered word lists and evaluate over all text generated for the other bias metrics, downsampling if necessary so that the amount and sources of generated text are consistent across decoding techniques. First, we obtain the lists of female words and male words from Zhao et al. (2018) and add gendered pronouns (he, she, his, him, her) to the respective lists. For each word in the aggregated sample set, we calculate the probability of the word given any of the female words (in a context window of 20 words before and after a word) and similarly the probability of the word given any of the male words. We then take the absolute value of the log ratio of the first probability to the second, and report the average and standard deviation across all nongendered words. More concretely, given the set of female gendered words f , the set of male gendered words m, unique non-gendered words w ∈ W in a dataset, and the probability of a word given any of the set g of gendered words P(w|g), we calculate the mean\nµ = avg(abs(log P(w|f) P(w|m) ))\nand standard deviation\nσ = stdev(abs(log P(w|f) P(w|m) )).\nSupplementary Results Supplementary to the experimental results described in the main text, Table 2 presents quantitative results. Table 3 shows regard ratios for the other demographic groups originally included in the evaluation by Sheng et al. (2019). Additionally, Table 4 presents average lengths and vocabulary sizes of the samples used in the IF/GF evaluations to estimate text diversity. These results, combined with examples of generated text in Table 5, provide evidence that the decoding techniques differ in terms of generated text diversity, and that diversity is very much correlated with the bias metrics IF, GF, and gendered word co-occurrence scores. Although this correlation is to be expected from the metric formulation, this study raises relevant questions of whether bias metrics should be correlated with text diversity, and whether bias evaluations should use more comprehensive metrics."
    } ],
    "references" : [ {
      "title" : "Persistent anti-muslim bias in large language models",
      "author" : [ "Abubakar Abid", "Maheen Farooqi", "James Zou." ],
      "venue" : "arXiv preprint arXiv:2101.05783.",
      "citeRegEx" : "Abid et al\\.,? 2021",
      "shortCiteRegEx" : "Abid et al\\.",
      "year" : 2021
    }, {
      "title" : "Gender-aware reinflection using linguistically enhanced neural models",
      "author" : [ "Bashar Alhafni", "Nizar Habash", "Houda Bouamor." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 139–150, Barcelona, Spain (On-",
      "citeRegEx" : "Alhafni et al\\.,? 2020",
      "shortCiteRegEx" : "Alhafni et al\\.",
      "year" : 2020
    }, {
      "title" : "The problem with bias: Allocative versus representational harms in machine learning",
      "author" : [ "Solon Barocas", "Kate Crawford", "Aaron Shapiro", "Hanna Wallach." ],
      "venue" : "9th Annual Conference of the Special Interest Group for Computing, Information and So-",
      "citeRegEx" : "Barocas et al\\.,? 2017",
      "shortCiteRegEx" : "Barocas et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards mitigating gender bias in a decoder-based neural machine translation model by adding contextual information",
      "author" : [ "Christine Basta", "Marta R. Costa-jussà", "José A.R. Fonollosa." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Basta et al\\.,? 2020",
      "shortCiteRegEx" : "Basta et al\\.",
      "year" : 2020
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big",
      "author" : [ "Emily M Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "Proceedings of FAccT.",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Decolonising speech and language technology",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3504–3519, Barcelona, Spain (Online). International Committee on Computational Linguistics.",
      "citeRegEx" : "Bird.,? 2020",
      "shortCiteRegEx" : "Bird.",
      "year" : 2020
    }, {
      "title" : "Language (technology) is power: A critical survey of “bias” in NLP",
      "author" : [ "Su Lin Blodgett", "Solon Barocas", "Hal Daumé III", "Hanna Wallach." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–",
      "citeRegEx" : "Blodgett et al\\.,? 2020",
      "shortCiteRegEx" : "Blodgett et al\\.",
      "year" : 2020
    }, {
      "title" : "Identifying and reducing gender bias in word-level language models",
      "author" : [ "Shikha Bordia", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Work-",
      "citeRegEx" : "Bordia and Bowman.,? 2019",
      "shortCiteRegEx" : "Bordia and Bowman.",
      "year" : 2019
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting training data from large language models",
      "author" : [ "Nicholas Carlini", "Florian Tramer", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom Brown", "Dawn Song", "Ulfar Erlingsson" ],
      "venue" : null,
      "citeRegEx" : "Carlini et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Carlini et al\\.",
      "year" : 2020
    }, {
      "title" : "Dialect diversity in text summarization on twitter",
      "author" : [ "L Elisa Celis", "Vijay Keswani." ],
      "venue" : "arXiv preprint arXiv:2007.07860.",
      "citeRegEx" : "Celis and Keswani.,? 2020",
      "shortCiteRegEx" : "Celis and Keswani.",
      "year" : 2020
    }, {
      "title" : "Conversational assistants and gender stereotypes: Public perceptions and desiderata for voice personas",
      "author" : [ "Amanda Cercas Curry", "Judy Robertson", "Verena Rieser." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Process-",
      "citeRegEx" : "Curry et al\\.,? 2020",
      "shortCiteRegEx" : "Curry et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling knowledge learned in BERT for text generation",
      "author" : [ "Yen-Chun Chen", "Zhe Gan", "Yu Cheng", "Jingzhou Liu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "On measuring gender bias in translation of gender-neutral pronouns",
      "author" : [ "Won Ik Cho", "Ji Won Kim", "Seok Min Kim", "Nam Soo Kim." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 173–181, Florence,",
      "citeRegEx" : "Cho et al\\.,? 2019",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards cross-lingual generalization of translation gender bias",
      "author" : [ "Won Ik Cho", "Jiwon Kim", "Jaeyeong Yang", "Nam Soo Kim." ],
      "venue" : "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 449–457.",
      "citeRegEx" : "Cho et al\\.,? 2021",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving gender translation accuracy with filtered self-training",
      "author" : [ "Prafulla Kumar Choubey", "Anna Currey", "Prashant Mathur", "Georgiana Dinu." ],
      "venue" : "arXiv preprint arXiv:2104.07695.",
      "citeRegEx" : "Choubey et al\\.,? 2021",
      "shortCiteRegEx" : "Choubey et al\\.",
      "year" : 2021
    }, {
      "title" : "Gender bias in multilingual neural machine translation: The architecture matters",
      "author" : [ "Marta R Costa-jussà", "Carlos Escolano", "Christine Basta", "Javier Ferrando", "Roser Batlle", "Ksenia Kharitonova." ],
      "venue" : "arXiv preprint arXiv:2012.13176.",
      "citeRegEx" : "Costa.jussà et al\\.,? 2020",
      "shortCiteRegEx" : "Costa.jussà et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-tuning neural machine translation on genderbalanced datasets",
      "author" : [ "Marta R. Costa-jussà", "Adrià de Jorge." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 26–34, Barcelona, Spain (Online).",
      "citeRegEx" : "Costa.jussà and Jorge.,? 2020",
      "shortCiteRegEx" : "Costa.jussà and Jorge.",
      "year" : 2020
    }, {
      "title" : "The trouble with bias",
      "author" : [ "Kate Crawford." ],
      "venue" : "Keynote at NeurIPS.",
      "citeRegEx" : "Crawford.,? 2017",
      "shortCiteRegEx" : "Crawford.",
      "year" : 2017
    }, {
      "title" : "Transformer-XL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Plug and play language models: A simple approach to controlled text generation",
      "author" : [ "Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu." ],
      "venue" : "International Conference on Learning Represen-",
      "citeRegEx" : "Dathathri et al\\.,? 2019",
      "shortCiteRegEx" : "Dathathri et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Bold: Dataset and metrics for measuring biases in open-ended language generation",
      "author" : [ "Jwala Dhamala", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Kai-Wei Chang", "Rahul Gupta." ],
      "venue" : "Proceedings of FAccT.",
      "citeRegEx" : "Dhamala et al\\.,? 2021",
      "shortCiteRegEx" : "Dhamala et al\\.",
      "year" : 2021
    }, {
      "title" : "Queens are powerful too: Mitigating gender bias in dialogue generation",
      "author" : [ "Emily Dinan", "Angela Fan", "Adina Williams", "Jack Urbanek", "Douwe Kiela", "Jason Weston." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Dinan et al\\.,? 2020a",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2020
    }, {
      "title" : "Multidimensional gender bias classification",
      "author" : [ "Emily Dinan", "Angela Fan", "Ledell Wu", "Jason Weston", "Douwe Kiela", "Adina Williams." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Dinan et al\\.,? 2020b",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2020
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel." ],
      "venue" : "Proceedings of the 3rd innovations in theoretical computer science conference, pages 214–226.",
      "citeRegEx" : "Dwork et al\\.,? 2012",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2012
    }, {
      "title" : "Gender aware spoken language translation applied to englisharabic",
      "author" : [ "Mostafa Elaraby", "Ahmed Y Tawfik", "Mahmoud Khaled", "Hany Hassan", "Aly Osama." ],
      "venue" : "2018 2nd International Conference on Natural Language and Speech Processing (IC-",
      "citeRegEx" : "Elaraby et al\\.,? 2018",
      "shortCiteRegEx" : "Elaraby et al\\.",
      "year" : 2018
    }, {
      "title" : "Equalizing gender bias in neural machine translation with word embeddings techniques",
      "author" : [ "Joel Escudé Font", "Marta R. Costa-jussà." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 147–154, Florence,",
      "citeRegEx" : "Font and Costa.jussà.,? 2019",
      "shortCiteRegEx" : "Font and Costa.jussà.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "How to measure gender bias in machine translation: Optimal translators, multiple reference points",
      "author" : [ "Anna Farkas", "Renáta Németh." ],
      "venue" : "arXiv preprint arXiv:2011.06445.",
      "citeRegEx" : "Farkas and Németh.,? 2020",
      "shortCiteRegEx" : "Farkas and Németh.",
      "year" : 2020
    }, {
      "title" : "Discovering and categorising language biases in reddit",
      "author" : [ "Xavier Ferrer", "Tom van Nuenen", "Jose M Such", "Natalia Criado." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 15.",
      "citeRegEx" : "Ferrer et al\\.,? 2021",
      "shortCiteRegEx" : "Ferrer et al\\.",
      "year" : 2021
    }, {
      "title" : "Datasheets for datasets",
      "author" : [ "Timnit Gebru", "Jamie Morgenstern", "Briana Vecchione", "Jennifer Wortman Vaughan", "Hanna Wallach", "Hal Daumé III", "Kate Crawford." ],
      "venue" : "arXiv preprint arXiv:1803.09010.",
      "citeRegEx" : "Gebru et al\\.,? 2018",
      "shortCiteRegEx" : "Gebru et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatically identifying gender issues in machine translation using perturbations",
      "author" : [ "Hila Gonen", "Kellie Webster." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1991–1995, Online. Association for Computational",
      "citeRegEx" : "Gonen and Webster.,? 2020",
      "shortCiteRegEx" : "Gonen and Webster.",
      "year" : 2020
    }, {
      "title" : "Investigating AfricanAmerican Vernacular English in transformer-based text generation",
      "author" : [ "Sophie Groenwold", "Lily Ou", "Aesha Parekh", "Samhita Honnavalli", "Sharon Levy", "Diba Mirza", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Confer-",
      "citeRegEx" : "Groenwold et al\\.,? 2020",
      "shortCiteRegEx" : "Groenwold et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic gender identification and reinflection in Arabic",
      "author" : [ "Nizar Habash", "Houda Bouamor", "Christine Chung." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 155–165, Florence, Italy. Association for",
      "citeRegEx" : "Habash et al\\.,? 2019",
      "shortCiteRegEx" : "Habash et al\\.",
      "year" : 2019
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nati Srebro." ],
      "venue" : "Advances in neural information processing systems, pages 3315–3323.",
      "citeRegEx" : "Hardt et al\\.,? 2016",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2016
    }, {
      "title" : "Fairness without demographics in repeated loss minimization",
      "author" : [ "Tatsunori Hashimoto", "Megha Srivastava", "Hongseok Namkoong", "Percy Liang." ],
      "venue" : "International Conference on Machine Learning, pages 1929–1938. PMLR.",
      "citeRegEx" : "Hashimoto et al\\.,? 2018",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2018
    }, {
      "title" : "Ethical challenges in data-driven dialogue systems",
      "author" : [ "Peter Henderson", "Koustuv Sinha", "Nicolas AngelardGontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and",
      "citeRegEx" : "Henderson et al\\.,? 2018",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2018
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2019",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2019
    }, {
      "title" : "you sound just like your father” commercial machine translation systems include stylistic biases",
      "author" : [ "Dirk Hovy", "Federico Bianchi", "Tommaso Fornaciari." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hovy et al\\.,? 2020",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing sentiment bias in language models via counterfactual evaluation",
      "author" : [ "Po-Sen Huang", "Huan Zhang", "Ray Jiang", "Robert Stanforth", "Johannes Welbl", "Jack Rae", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli." ],
      "venue" : "Findings of the Association for",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Vader: A parsimonious rule-based model for sentiment analysis of social media text",
      "author" : [ "Clayton Hutto", "Eric Gilbert." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 8.",
      "citeRegEx" : "Hutto and Gilbert.,? 2014",
      "shortCiteRegEx" : "Hutto and Gilbert.",
      "year" : 2014
    }, {
      "title" : "Unequal representation and gender stereotypes in image search results for occupations",
      "author" : [ "Matthew Kay", "Cynthia Matuszek", "Sean A Munson." ],
      "venue" : "In",
      "citeRegEx" : "Kay et al\\.,? 2015",
      "shortCiteRegEx" : "Kay et al\\.",
      "year" : 2015
    }, {
      "title" : "Examining gender and race bias in two hundred sentiment analysis systems",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43–53.",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2018",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2018
    }, {
      "title" : "Confronting abusive language online: A survey from the ethical and human rights perspective",
      "author" : [ "Svetlana Kiritchenko", "Isar Nejadgholi", "Kathleen C Fraser." ],
      "venue" : "arXiv preprint arXiv:2012.12305.",
      "citeRegEx" : "Kiritchenko et al\\.,? 2020",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2020
    }, {
      "title" : "How true is gpt2? an empirical analysis of intersectional occupational biases",
      "author" : [ "Hannah Kirk", "Yennie Jun", "Haider Iqbal", "Elias Benussi", "Filippo Volpin", "Frederic A Dreyer", "Aleksandar Shtedritski", "Yuki M Asano." ],
      "venue" : "arXiv preprint arXiv:2102.04130.",
      "citeRegEx" : "Kirk et al\\.,? 2021",
      "shortCiteRegEx" : "Kirk et al\\.",
      "year" : 2021
    }, {
      "title" : "Gender coreference and bias evaluation at WMT 2020",
      "author" : [ "Tom Kocmi", "Tomasz Limisiewicz", "Gabriel Stanovsky." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 357–364, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Kocmi et al\\.,? 2020",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Gedi: Generative discriminator guided sequence generation",
      "author" : [ "Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "Richard Socher", "Nazneen Fatema Rajani." ],
      "venue" : "arXiv preprint arXiv:2009.06367.",
      "citeRegEx" : "Krause et al\\.,? 2020",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2020
    }, {
      "title" : "The truth is out there: Investigating conspiracy theories in text generation",
      "author" : [ "Sharon Levy", "Michael Saxon", "William Yang Wang." ],
      "venue" : "Findings of The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Levy et al\\.,? 2021",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2021
    }, {
      "title" : "On-the-fly controlled text generation with experts and anti-experts",
      "author" : [ "Alisa Liu", "Maarten Sap", "Ximing Lu", "Swabha Swayamdipta", "Chandra Bhagavatula", "Noah A Smith", "Yejin Choi." ],
      "venue" : "The Joint Conference of the 59th Annual Meeting of the",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Does gender matter? towards fairness in dialogue systems",
      "author" : [ "Haochen Liu", "Jamell Dacon", "Wenqi Fan", "Hui Liu", "Zitao Liu", "Jiliang Tang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4403–4416, Barcelona,",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating gender bias for neural dialogue generation with adversarial learning",
      "author" : [ "Haochen Liu", "Wentao Wang", "Yiqi Wang", "Hui Liu", "Zitao Liu", "Jiliang Tang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Gender bias in neural natural language processing",
      "author" : [ "Kaiji Lu", "Piotr Mardziel", "Fangjing Wu", "Preetam Amancharla", "Anupam Datta." ],
      "venue" : "Logic, Language, and Security, pages 189–202. Springer.",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Gender and representation bias in gpt-3 generated stories",
      "author" : [ "Li Lucy", "David Bamman." ],
      "venue" : "Proceedings of the Third Workshop on Narrative Understanding, pages 48–55.",
      "citeRegEx" : "Lucy and Bamman.,? 2021",
      "shortCiteRegEx" : "Lucy and Bamman.",
      "year" : 2021
    }, {
      "title" : "PowerTransformer: Unsupervised controllable revision for biased language correction",
      "author" : [ "Xinyao Ma", "Maarten Sap", "Hannah Rashkin", "Yejin Choi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Filling gender & number gaps in neural machine translation with black-box context injection",
      "author" : [ "Amit Moryossef", "Roee Aharoni", "Yoav Goldberg." ],
      "venue" : "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 49–54,",
      "citeRegEx" : "Moryossef et al\\.,? 2019",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2019
    }, {
      "title" : "Honest: Measuring hurtful sentence completion in language models",
      "author" : [ "Debora Nozza", "Federico Bianchi", "Dirk Hovy." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Nozza et al\\.,? 2021",
      "shortCiteRegEx" : "Nozza et al\\.",
      "year" : 2021
    }, {
      "title" : "Facebook apologizes after wrong translation sees Palestinian man arrested for posting ’good morning",
      "author" : [ "Thuy Ong" ],
      "venue" : null,
      "citeRegEx" : "Ong.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ong.",
      "year" : 2017
    }, {
      "title" : "Reducing gender bias in abusive language detection",
      "author" : [ "Ji Ho Park", "Jamin Shin", "Pascale Fung." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804.",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "Data and its (dis) contents: A survey of dataset development and use in machine learning research",
      "author" : [ "Amandalynne Paullada", "Inioluwa Deborah Raji", "Emily M Bender", "Emily Denton", "Alex Hanna." ],
      "venue" : "arXiv preprint arXiv:2012.05345.",
      "citeRegEx" : "Paullada et al\\.,? 2020",
      "shortCiteRegEx" : "Paullada et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing non-normative text generation from language models",
      "author" : [ "Xiangyu Peng", "Siyan Li", "Spencer Frazier", "Mark Riedl." ],
      "venue" : "Proceedings of the 13th International Conference on Natural Language Generation, pages 374–383, Dublin, Ireland. Asso-",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing gender bias in machine translation: a case study with google translate",
      "author" : [ "Marcelo OR Prates", "Pedro H Avelar", "Luı́s C Lamb" ],
      "venue" : "Neural Computing and Applications,",
      "citeRegEx" : "Prates et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Prates et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically neutralizing subjective bias in text",
      "author" : [ "Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "Sadao Kurohashi", "Dan Jurafsky", "Diyi Yang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 480–489.",
      "citeRegEx" : "Pryzant et al\\.,? 2020",
      "shortCiteRegEx" : "Pryzant et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing gender bias in word-level language models with a gender-equalizing loss function",
      "author" : [ "Yusu Qian", "Urwa Muaz", "Ben Zhang", "Jae Won Hyun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Stu-",
      "citeRegEx" : "Qian et al\\.,? 2019",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating failures of automatic translation in the case of unambiguous gender",
      "author" : [ "Adithya Renduchintala", "Adina Williams." ],
      "venue" : "arXiv preprint arXiv:2104.07838.",
      "citeRegEx" : "Renduchintala and Williams.,? 2021",
      "shortCiteRegEx" : "Renduchintala and Williams.",
      "year" : 2021
    }, {
      "title" : "Decoding and diversity in machine translation",
      "author" : [ "Nicholas Roberts", "Davis Liang", "Graham Neubig", "Zachary C Lipton." ],
      "venue" : "arXiv preprint arXiv:2011.13477.",
      "citeRegEx" : "Roberts et al\\.,? 2020",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Re-imagining algorithmic fairness in india and beyond",
      "author" : [ "Nithya Sambasivan", "Erin Arnesen", "Ben Hutchinson", "Tulsee Doshi", "Vinodkumar Prabhakaran." ],
      "venue" : "Proceedings of FAccT.",
      "citeRegEx" : "Sambasivan et al\\.,? 2021",
      "shortCiteRegEx" : "Sambasivan et al\\.",
      "year" : 2021
    }, {
      "title" : "Reducing gender bias in neural machine translation as a domain adaptation problem",
      "author" : [ "Danielle Saunders", "Bill Byrne." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7724–7736, Online. Association",
      "citeRegEx" : "Saunders and Byrne.,? 2020",
      "shortCiteRegEx" : "Saunders and Byrne.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation doesn’t translate gender coreference right unless you make it",
      "author" : [ "Danielle Saunders", "Rosie Sallis", "Bill Byrne." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 35–43, Barcelona,",
      "citeRegEx" : "Saunders et al\\.,? 2020",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2020
    }, {
      "title" : "First the worst: Finding better gender translations during beam search",
      "author" : [ "Danielle Saunders", "Rosie Sallis", "Bill Byrne." ],
      "venue" : "arXiv preprint arXiv:2104.07429.",
      "citeRegEx" : "Saunders et al\\.,? 2021",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2021
    }, {
      "title" : "Gender bias in machine translation",
      "author" : [ "Beatrice Savoldi", "Marco Gaido", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Savoldi et al\\.,? 2021",
      "shortCiteRegEx" : "Savoldi et al\\.",
      "year" : 2021
    }, {
      "title" : "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
      "author" : [ "Timo Schick", "Sahana Udupa", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2103.00453.",
      "citeRegEx" : "Schick et al\\.,? 2021",
      "shortCiteRegEx" : "Schick et al\\.",
      "year" : 2021
    }, {
      "title" : "Predictive biases in natural language processing models: A conceptual framework and overview",
      "author" : [ "Deven Santosh Shah", "H. Andrew Schwartz", "Dirk Hovy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Shah et al\\.,? 2020",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2020
    }, {
      "title" : "Revealing persona biases in dialogue systems",
      "author" : [ "Emily Sheng", "Josh Arnold", "Zhou Yu", "Kai-Wei Chang", "Nanyun Peng." ],
      "venue" : "arXiv preprint arXiv:2104.08728.",
      "citeRegEx" : "Sheng et al\\.,? 2021a",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2021
    }, {
      "title" : "The woman worked as a babysitter: On biases in language generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Prem Natarajan", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Sheng et al\\.,? 2019",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards Controllable Biases in Language Generation",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Prem Natarajan", "Nanyun Peng." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239–3254, Online. Association for Computa-",
      "citeRegEx" : "Sheng et al\\.,? 2020",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2020
    }, {
      "title" : "nice try, kiddo”: Investigating ad hominems in dialogue responses",
      "author" : [ "Emily Sheng", "Kai-Wei Chang", "Premkumar Natarajan", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Sheng et al\\.,? 2021b",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Investigating societal biases in a poetry composition system",
      "author" : [ "Emily Sheng", "David Uthus." ],
      "venue" : "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pages 93–106, Barcelona, Spain (Online). Association for Compu-",
      "citeRegEx" : "Sheng and Uthus.,? 2020",
      "shortCiteRegEx" : "Sheng and Uthus.",
      "year" : 2020
    }, {
      "title" : "you are grounded!”: Latent name artifacts in pre-trained language models",
      "author" : [ "Vered Shwartz", "Rachel Rudinger", "Oyvind Tafjord." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6850–6861,",
      "citeRegEx" : "Shwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers",
      "author" : [ "Andrew Silva", "Pradyumna Tambwekar", "Matthew Gombolay." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Silva et al\\.,? 2021",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2021
    }, {
      "title" : "Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203",
      "author" : [ "Kreps" ],
      "venue" : null,
      "citeRegEx" : "Kreps,? \\Q2019\\E",
      "shortCiteRegEx" : "Kreps",
      "year" : 2019
    }, {
      "title" : "Mitigating gender bias in machine translation with target gender annotations",
      "author" : [ "Artūrs Stafanovičs", "Mārcis Pinnis", "Toms Bergmanis." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 629–638, Online. Association for Computa-",
      "citeRegEx" : "Stafanovičs et al\\.,? 2020",
      "shortCiteRegEx" : "Stafanovičs et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating gender bias in machine translation",
      "author" : [ "Gabriel Stanovsky", "Noah A. Smith", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for",
      "citeRegEx" : "Stanovsky et al\\.,? 2019",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2019
    }, {
      "title" : "Mitigating gender bias in natural language processing: Literature review",
      "author" : [ "Tony Sun", "Andrew Gaut", "Shirlyn Tang", "Yuxin Huang", "Mai ElSherief", "Jieyu Zhao", "Diba Mirza", "Elizabeth Belding", "Kai-Wei Chang", "William Yang Wang." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "They, them, theirs: Rewriting with gender-neutral english",
      "author" : [ "Tony Sun", "Kellie Webster", "Apu Shah", "William Yang Wang", "Melvin Johnson." ],
      "venue" : "arXiv preprint arXiv:2102.06788.",
      "citeRegEx" : "Sun et al\\.,? 2021",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2021
    }, {
      "title" : "Understanding the capabilities, limitations, and societal impact of large language models",
      "author" : [ "Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli." ],
      "venue" : "arXiv preprint arXiv:2102.02503.",
      "citeRegEx" : "Tamkin et al\\.,? 2021",
      "shortCiteRegEx" : "Tamkin et al\\.",
      "year" : 2021
    }, {
      "title" : "The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing",
      "author" : [ "Marcus Tomalin", "Bill Byrne", "Shauna Concannon", "Danielle Saunders", "Stefanie Ullmann." ],
      "venue" : "Ethics and Information Technology,",
      "citeRegEx" : "Tomalin et al\\.,? 2021",
      "shortCiteRegEx" : "Tomalin et al\\.",
      "year" : 2021
    }, {
      "title" : "Getting gender right in neural machine translation",
      "author" : [ "Eva Vanmassenhove", "Christian Hardmeier", "Andy Way." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3003–3008, Brussels, Belgium. Associa-",
      "citeRegEx" : "Vanmassenhove et al\\.,? 2018",
      "shortCiteRegEx" : "Vanmassenhove et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Investigating gender bias in language models using causal mediation analysis",
      "author" : [ "Jesse Vig", "Sebastian Gehrmann", "Yonatan Belinkov", "Sharon Qian", "Daniel Nevo", "Yaron Singer", "Stuart Shieber." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Vig et al\\.,? 2020",
      "shortCiteRegEx" : "Vig et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT has a mouth, and it must speak: BERT as a Markov random field language model",
      "author" : [ "Alex Wang", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30–36,",
      "citeRegEx" : "Wang and Cho.,? 2019",
      "shortCiteRegEx" : "Wang and Cho.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Defining and evaluating fair natural language generation",
      "author" : [ "Catherine Yeo", "Alyssa Chen." ],
      "venue" : "Proceedings of the The Fourth Widening Natural Language Processing Workshop, pages 107–109, Seattle, USA. Association for Computational Linguis-",
      "citeRegEx" : "Yeo and Chen.,? 2020",
      "shortCiteRegEx" : "Yeo and Chen.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning gender-neutral word embeddings",
      "author" : [ "Jieyu Zhao", "Yichao Zhou", "Zeyu Li", "Wei Wang", "KaiWei Chang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847–4853, Brussels, Belgium. Associa-",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
      "author" : [ "Ran Zmigrod", "Sabrina J. Mielke", "Hanna Wallach", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Zmigrod et al\\.,? 2019",
      "shortCiteRegEx" : "Zmigrod et al\\.",
      "year" : 2019
    }, {
      "title" : "2020)’s prefix templates and fairness metrics exactly as defined in the original work, so we refer readers to the original work for more details",
      "author" : [ "Huang" ],
      "venue" : null,
      "citeRegEx" : "Huang,? \\Q2020\\E",
      "shortCiteRegEx" : "Huang",
      "year" : 2020
    }, {
      "title" : "Additionally, Table 4 presents average lengths and vocabulary sizes of the samples used in the IF/GF evaluations to estimate text diversity",
      "author" : [ "Sheng" ],
      "venue" : null,
      "citeRegEx" : "Sheng,? \\Q2019\\E",
      "shortCiteRegEx" : "Sheng",
      "year" : 2019
    }, {
      "title" : "Search-based results for regard are omitted due to lack of enough prompts",
      "author" : [ "Bordia", "Bowman" ],
      "venue" : null,
      "citeRegEx" : "Bordia and Bowman,? \\Q2019\\E",
      "shortCiteRegEx" : "Bordia and Bowman",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 66,
      "context" : ", 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : ", 2019), GPT-3 (Brown et al., 2020), TransformerXL (Dai et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : ", 2020), TransformerXL (Dai et al., 2019), XLNet (Yang et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 96,
      "context" : ", 2019), XLNet (Yang et al., 2019)) powered by Transformers (Vaswani et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 92,
      "context" : ", 2019)) powered by Transformers (Vaswani et al., 2017) and an increasing repository of available data have created more capable applications.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Although bi-directional language models like BERT (Devlin et al., 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 95,
      "context" : ", 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al.",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : ", 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al.",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 82,
      "context" : ", 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 82,
      "context" : "Existing works analyzing biases in autocomplete generation have mostly examined Transformer-based models, including GPT (Shwartz et al., 2020), GPT2 (Solaiman et al.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 82,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 93,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 97,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 41,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 23,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 75,
      "context" : ", 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al.",
      "startOffset" : 14,
      "endOffset" : 186
    }, {
      "referenceID" : 9,
      "context" : ", 2021), GPT-3 (Brown et al., 2020), CTRL (Dhamala et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : ", 2020), CTRL (Dhamala et al., 2021), TransformerXL (Shwartz et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 82,
      "context" : ", 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 93,
      "context" : ", 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : ", 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus",
      "startOffset" : 31,
      "endOffset" : 79
    }, {
      "referenceID" : 98,
      "context" : "on Transformer-based models such as DialoGPT (Zhang et al., 2020) and other custom architectures.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 62,
      "context" : ", for different occupations) through translation (Prates et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 86,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 71,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 72,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 3,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 85,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 68,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 16,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 73,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 90,
      "context" : "(2018) study LSTM-based encoder-decoder translation systems, and more recent works examine Transformer-based architectures (Escudé Font and Costa-jussà, 2019; Stanovsky et al., 2019; Saunders and Byrne, 2020; Saunders et al., 2020; Costa-jussà and de Jorge, 2020; Basta et al., 2020; Stafanovičs et al., 2020; Renduchintala and Williams, 2021; Choubey et al., 2021; Saunders et al., 2021; Tomalin et al., 2021).",
      "startOffset" : 123,
      "endOffset" : 410
    }, {
      "referenceID" : 62,
      "context" : "While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al.",
      "startOffset" : 97,
      "endOffset" : 209
    }, {
      "referenceID" : 56,
      "context" : "While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al.",
      "startOffset" : 97,
      "endOffset" : 209
    }, {
      "referenceID" : 86,
      "context" : "While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al.",
      "startOffset" : 97,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al.",
      "startOffset" : 97,
      "endOffset" : 209
    }, {
      "referenceID" : 30,
      "context" : "While Google Translate3 has been the most popular commercial system to analyze for gender biases (Prates et al., 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Cho et al., 2019; Farkas and Németh, 2020), Stanovsky et al.",
      "startOffset" : 97,
      "endOffset" : 209
    }, {
      "referenceID" : 35,
      "context" : "re-inflection (Habash et al., 2019; Zmigrod et al., 2019; Alhafni et al., 2020) and re-writing text to use neutral viewpoints (Pryzant et al.",
      "startOffset" : 14,
      "endOffset" : 79
    }, {
      "referenceID" : 100,
      "context" : "re-inflection (Habash et al., 2019; Zmigrod et al., 2019; Alhafni et al., 2020) and re-writing text to use neutral viewpoints (Pryzant et al.",
      "startOffset" : 14,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "re-inflection (Habash et al., 2019; Zmigrod et al., 2019; Alhafni et al., 2020) and re-writing text to use neutral viewpoints (Pryzant et al.",
      "startOffset" : 14,
      "endOffset" : 79
    }, {
      "referenceID" : 63,
      "context" : ", 2020) and re-writing text to use neutral viewpoints (Pryzant et al., 2020), genderneutral English (Sun et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 88,
      "context" : ", 2020), genderneutral English (Sun et al., 2021), or more agency (Ma et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "In the context of AI fairness, the term “bias” commonly refers to skews that result in undesirable impacts (Crawford, 2017) and is quantifiable with some metric.",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 78,
      "context" : "• Regard Ratio: negative-neutral-positive regard score ratios of text generated from bias-inducing prompts (Sheng et al., 2019) • Sentiment Ratio: negative-neutral-positive sen-",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "timent score ratios of text generated from African American English (AAE) versus White-Aligned English (WAE) prompts (Groenwold et al., 2020) • Individual and Group Fairness through Sentiment: comparisons of the sentiment distribu-",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 41,
      "context" : "tions of generated text across demographics and prompts (Huang et al., 2020) • Gendered Word Co-occurrence Score: mean and standard deviations of the absolute log ratio of probabilities: P(word|female terms) to P(word|male terms) across all words in generated text (Bordia and Bowman, 2019)",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : ", 2020) • Gendered Word Co-occurrence Score: mean and standard deviations of the absolute log ratio of probabilities: P(word|female terms) to P(word|male terms) across all words in generated text (Bordia and Bowman, 2019)",
      "startOffset" : 196,
      "endOffset" : 221
    }, {
      "referenceID" : 82,
      "context" : "There are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 61,
      "context" : ", 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",
      "startOffset" : 34,
      "endOffset" : 73
    }, {
      "referenceID" : 97,
      "context" : ", 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",
      "startOffset" : 34,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "Biases can be towards people described in text, people who produce the text, or people to whom the text is addressed (Dinan et al., 2020b).",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "There are also works that define biases towards people who produce the text (Groenwold et al., 2020) or people to whom the text is addressed (Sheng et al.",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 80,
      "context" : ", 2020) or people to whom the text is addressed (Sheng et al., 2021b), though there are relatively fewer works that study these latter associations.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "We survey detrimental representational11 and allocational12 impacts (Crawford, 2017; Barocas et al., 2017; Blodgett et al., 2020) used to motivate existing studies of bias in NLG tasks, finding limited examples.",
      "startOffset" : 68,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "We survey detrimental representational11 and allocational12 impacts (Crawford, 2017; Barocas et al., 2017; Blodgett et al., 2020) used to motivate existing studies of bias in NLG tasks, finding limited examples.",
      "startOffset" : 68,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "We survey detrimental representational11 and allocational12 impacts (Crawford, 2017; Barocas et al., 2017; Blodgett et al., 2020) used to motivate existing studies of bias in NLG tasks, finding limited examples.",
      "startOffset" : 68,
      "endOffset" : 129
    }, {
      "referenceID" : 92,
      "context" : "Since effective NLG techniques based on large Transformer models (Vaswani et al., 2017) are relatively new, most of the existing works",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 58,
      "context" : "A real example of a negative allocational impact is when machine translation errors lead to arrests (Ong, 2017).",
      "startOffset" : 100,
      "endOffset" : 111
    }, {
      "referenceID" : 89,
      "context" : "With continuous technological advances, more organizations will turn to effective NLG techniques, making it imperative to start setting norms to reduce harmful allocational impacts (Tamkin et al., 2021).",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "For example, privacy-related issues (Carlini et al., 2020), misin-",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 49,
      "context" : "formation (Levy et al., 2021), or radicalizing views in generated text could make a group more likely to be attributed to specific stereotypes (e.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : ", 2020) and GPT-3 (Brown et al., 2020), are trained on the",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 45,
      "context" : "Task formulation and application deployment are also part of NLG task pipelines (Kiritchenko et al., 2020), though we do not focus on biases in these areas.",
      "startOffset" : 80,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "• Top-k sampling (Fan et al., 2018): at each time step, re-distribute the probability mass of the top k words with highest probabilities and sample.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "• Nucleus sampling (Holtzman et al., 2019): at each time step, re-distribute the probability mass of the smallest set of words with a cumulative probability exceeding p and sample.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 69,
      "context" : "Despite variations in fluency and diversity between deterministic versus stochastic, search versus sampling procedures, there are limited studies (Roberts et al., 2020) on how different decoding properties affect",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 34,
      "context" : "2019), sentiment ratios (Groenwold et al., 2020), individual and group fairness through sentiment scores (Huang et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 41,
      "context" : ", 2020), individual and group fairness through sentiment scores (Huang et al., 2020), and gendered word co-occurrence scores (Bordia and Bowman, 2019) (as introduced in Section 3).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : ", 2020), and gendered word co-occurrence scores (Bordia and Bowman, 2019) (as introduced in Section 3).",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 70,
      "context" : "of biases from various backgrounds and cultures (Sambasivan et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "language technologies without inflicting additional harm on marginalized populations (Bird, 2020).",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "Data-based methods for both bias analysis and mitigation use the general idea of counterfactual data augmentation (CDA) (Lu et al., 2020) to curate sets of counterfactual prompts.",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 91,
      "context" : "ases in translation include parallel sentences tagged with speaker or subject gender information (Vanmassenhove et al., 2018; Habash et al., 2019) and datasets to study gender biases when translating from neutral references of a person (e.",
      "startOffset" : 97,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : "ases in translation include parallel sentences tagged with speaker or subject gender information (Vanmassenhove et al., 2018; Habash et al., 2019) and datasets to study gender biases when translating from neutral references of a person (e.",
      "startOffset" : 97,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : ", enfermera or enfermero in Spanish, gendered pronouns) (Cho et al., 2019; Stanovsky et al., 2019; Gonen and Webster, 2020; Kocmi et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 86,
      "context" : ", enfermera or enfermero in Spanish, gendered pronouns) (Cho et al., 2019; Stanovsky et al., 2019; Gonen and Webster, 2020; Kocmi et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 33,
      "context" : ", enfermera or enfermero in Spanish, gendered pronouns) (Cho et al., 2019; Stanovsky et al., 2019; Gonen and Webster, 2020; Kocmi et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 47,
      "context" : ", enfermera or enfermero in Spanish, gendered pronouns) (Cho et al., 2019; Stanovsky et al., 2019; Gonen and Webster, 2020; Kocmi et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 63,
      "context" : "Other works present parallel corpora of biased versus unbiased framings and presuppositions (Pryzant et al., 2020) and AAE versus WAE equivalents (Groenwold et al.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 34,
      "context" : ", 2020) and AAE versus WAE equivalents (Groenwold et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 78,
      "context" : ", regarding social perception (Sheng et al., 2019), gender in translation (Prates et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 82,
      "context" : "2019), names (Shwartz et al., 2020), sentiment distribution (Huang et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 41,
      "context" : ", 2020), sentiment distribution (Huang et al., 2020), dialects (Groenwold et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : ", 2020), dialects (Groenwold et al., 2020), dialogue personas (Sheng et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 77,
      "context" : ", 2020), dialogue personas (Sheng et al., 2021a), or other notions of similarity across demographics (Yeo and Chen, 2020; Henderson et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al.",
      "startOffset" : 93,
      "endOffset" : 133
    }, {
      "referenceID" : 51,
      "context" : "CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al.",
      "startOffset" : 93,
      "endOffset" : 133
    }, {
      "referenceID" : 71,
      "context" : ", 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 124
    }, {
      "referenceID" : 85,
      "context" : ", 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussà and de Jorge, 2020; Stafanovičs et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 124
    }, {
      "referenceID" : 91,
      "context" : "tach a short prefix at training time (Vanmassenhove et al., 2018; Basta et al., 2020; Alhafni et al., 2020) or inference time (Moryossef et al.",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "tach a short prefix at training time (Vanmassenhove et al., 2018; Basta et al., 2020; Alhafni et al., 2020) or inference time (Moryossef et al.",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "tach a short prefix at training time (Vanmassenhove et al., 2018; Basta et al., 2020; Alhafni et al., 2020) or inference time (Moryossef et al.",
      "startOffset" : 37,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by ap-",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 55,
      "context" : "pending a target value to inputs during training (Ma et al., 2020), by using a normative classifier to produce reward values for backpropagation (Peng et al.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 61,
      "context" : ", 2020), by using a normative classifier to produce reward values for backpropagation (Peng et al., 2020), or through adversarial training (Liu et al.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 52,
      "context" : ", 2020), or through adversarial training (Liu et al., 2020b).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 63,
      "context" : "biased word embeddings (Escudé Font and Costajussà, 2019), identifying and editing out subjective words (Pryzant et al., 2020), and using Markov random fields to preserve morpho-syntactic agreement during reinflection (Zmigrod et al.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 100,
      "context" : ", 2020), and using Markov random fields to preserve morpho-syntactic agreement during reinflection (Zmigrod et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 48,
      "context" : ", 2019), GeDi (Krause et al., 2020), DExperts (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 50,
      "context" : ", 2020), DExperts (Liu et al., 2021)), there is potential for extending these methods to bias mitigation.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 56,
      "context" : "Studies of gender bias in machine translation are well-suited to evaluations using absolute scores: many use BLEU and its variants to evaluate correct gender inflections and translations (Moryossef et al., 2019; Escudé Font and Costajussà, 2019; Elaraby et al., 2018; Habash et al., 2019; Alhafni et al., 2020) or accuracy on WinoMT (Saunders and Byrne, 2020; Saunders et al.",
      "startOffset" : 187,
      "endOffset" : 310
    }, {
      "referenceID" : 27,
      "context" : "Studies of gender bias in machine translation are well-suited to evaluations using absolute scores: many use BLEU and its variants to evaluate correct gender inflections and translations (Moryossef et al., 2019; Escudé Font and Costajussà, 2019; Elaraby et al., 2018; Habash et al., 2019; Alhafni et al., 2020) or accuracy on WinoMT (Saunders and Byrne, 2020; Saunders et al.",
      "startOffset" : 187,
      "endOffset" : 310
    }, {
      "referenceID" : 35,
      "context" : "Studies of gender bias in machine translation are well-suited to evaluations using absolute scores: many use BLEU and its variants to evaluate correct gender inflections and translations (Moryossef et al., 2019; Escudé Font and Costajussà, 2019; Elaraby et al., 2018; Habash et al., 2019; Alhafni et al., 2020) or accuracy on WinoMT (Saunders and Byrne, 2020; Saunders et al.",
      "startOffset" : 187,
      "endOffset" : 310
    }, {
      "referenceID" : 1,
      "context" : "Studies of gender bias in machine translation are well-suited to evaluations using absolute scores: many use BLEU and its variants to evaluate correct gender inflections and translations (Moryossef et al., 2019; Escudé Font and Costajussà, 2019; Elaraby et al., 2018; Habash et al., 2019; Alhafni et al., 2020) or accuracy on WinoMT (Saunders and Byrne, 2020; Saunders et al.",
      "startOffset" : 187,
      "endOffset" : 310
    }, {
      "referenceID" : 24,
      "context" : ", 2020a,b) and the percentage of gendered words (Dinan et al., 2020a).",
      "startOffset" : 48,
      "endOffset" : 69
    } ],
    "year" : 2021,
    "abstractText" : "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
    "creator" : "LaTeX with hyperref"
  }
}