{
  "name" : "2021.acl-long.441.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "DynaEval: Unifying Turn and Dialogue Level Evaluation",
    "authors" : [ "Chen Zhang", "Yiming Chen", "Luis Fernando D’Haro", "Yan Zhang", "Thomas Friedrichs", "Grandee Lee", "Haizhou Li" ],
    "emails" : [ "zhang@u.nus.edu,", "yiming.chen@u.nus.edu,", "grandee.lee@u.nus.edu,", "haizhou.li@nus.edu.sg,", "eleyanz@nus.edu.sg,", "luisfernando.dharo@upm.es,", "thomas.friedrichs@sg.bosch.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5676–5689\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5676"
    }, {
      "heading" : "1 Introduction",
      "text" : "Modern dialogue systems (Smith et al., 2020; Zhang et al., 2020; Adiwardana et al., 2020) leveraging large-scale language model pre-training (Devlin et al., 2019; Radford et al., 2019) are capable of generating fluent and contextually relevant utterances. Yet, they still face difficulties in mimicking human conversations in the sense that they lack certain conversation-level attributes, such as coherence (Cervone et al., 2018), consistency (Welleck et al., 2019; Nie et al., 2020), diversity (Li et al., 2016; Wu et al., 2020) and engagement (Ghandeharioun et al., 2019; Ghazarian et al., 2020). One of the main reasons is the dearth of effective dialoguelevel evaluation mechanisms to guide the studies and to monitor progress.\n1https://github.com/e0397123/DynaEval\nCommonly used static metrics, such as BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004), correlate poorly with human judgements (Liu et al., 2016) rendering them unsuitable for dialogue evaluation. While some recent automatic dialogue evaluation metrics (Ghazarian et al., 2019; Mehri and Eskenazi, 2020b; Huang et al., 2020; Zhang et al., 2021b) demonstrate strong correlations with human judgement at the turn-level, they only focus on context-response pairs without explicitly modeling the interaction over an entire dialogue. To perform dialogue-level evaluation, we need to rely on the aggregation of turn-level scores over the dialogue as a proxy for a dialogue-level score.\nFurthermore, a recent study by Mehri and Eskenazi (2020a) found out that even though state-ofthe-art chatbots outperform humans across multiple turn-level evaluation criteria, such as interestingness, engagement and specificity, their dialoguelevel ratings like coherence, Likability and diversity are still far below human level. This further reinforces the idea that turn-level quality evaluation may be insufficient to assess the performance of open-domain dialogue systems.\nIn this work, we address the problem of automatic open-domain dialogue evaluation by focusing on the quality of an entire dialogue. This is a departure from the way we frame the problem as a weakly supervised next sentence prediction (Mehri and Eskenazi, 2020b; Sato et al., 2020) or language modeling tasks (Nedelchev et al., 2020; Pang et al., 2020) for context-response pairs. To this end, we need to answer two important questions: (1) How to effectively represent the entire dialogue? (2) How to incorporate this dialogue-level knowledge into our evaluation framework? We propose DynaEval to provide meaningful dialogue-level representation with explicit modeling of the interactive\ndynamics among interlocutors, for a unified turn and dialogue level quality assessment.\nThe main contributions of this work include: (1) The unified turn and dialogue level evaluation represents a departure from turn-level evaluation scheme; (2) DynaEval is one of the first few metrics where dialogue level dynamics is considered with structured graph representation. (3) Empirical results show that DynaEval outperforms the stateof-the-art dialogue coherence model and strongly correlates with human judgements at both turn and dialogue level."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Open-ended Dialogue Evaluation",
      "text" : "Turn-Level Evaluation The current trend for automatic dialogue evaluation is shifting towards the reference-free paradigm. Lately, the research community has witnessed a surge in the automatic metrics along these lines. Many of them focus on evaluating naturalness of generated responses. Typical examples include perplexity (Adiwardana et al., 2020), USR-MLM (Mehri and Eskenazi, 2020b) and GPT-2 (Radford et al., 2019) based fluency metrics (Nedelchev et al., 2020; Pang et al., 2020).\nAnother group of metrics evaluates contextual relevance of the responses. For example, RUBER (Tao et al., 2018), BERT-RUBER(Ghazarian et al., 2019) and USR-DR (Mehri and Eskenazi, 2020b) predict the relatedness between generated responses w.r.t the corresponding context by training a discriminative network to distinguish the original response from negative samples bootstrapped from the training set. Sato et al. (2020) and Lan et al. (2020) provide a better sampling strategy for bootstrapping negative samples.\nBesides these two major aspects, there are many metrics for other qualities, such as adequacy (D’Haro et al., 2019; Zhang et al., 2021a), consistency (Welleck et al., 2019; Dziri et al., 2019), engagement (Ghazarian et al., 2020).\nEven though all these automatic metrics demonstrate strong correlation with human judgements, they are laser-focused on one aspect of the evaluation. In addition, they do not explicitly model the speaker-level and utterance-level interactions, which we believe is essential for the dialogue-level representation, and eventually benefits the dialogue evaluation task.\nInteractive Evaluation A popular human evaluation method is the interactive evaluation whereby\nhuman judges converse with dialogue systems and make the assessment at the end of the conversations (See et al., 2019; Finch and Choi, 2020; Li et al., 2019; Deriu et al., 2020). It has been shown to be more reliable than turn-level static evaluation (Mehri and Eskenazi, 2020a).\nThere are few studies on fully automating this process. Ghandeharioun et al. (2019) propose a self-play scenario where the dialog system chats with itself and a combination of three metrics measuring sentiment, semantic coherence and engagement respectively along the conversation trajectory is computed to approximate dialogue-level quality estimation. Mehri and Eskenazi (2020a) propose the FED metric, which evaluates the quality of a system utterance in an interactive setting by computing the likelihood of a particular follow-up utterance responded by dialoGPT (Zhang et al., 2020). Moreover, Sinha et al. (2020) come up with MaUde, a reference-free metric tailored for online dialogue evaluation, which leverages a pre-trained DistilBERT (Sanh et al., 2019) model to extract the semantic representation of dialogue turns and uses bidirectional LSTM to explicitly model the discourse structure.\nWhile the interactive evaluation is more reliable than the turn-level static evaluation, it still relies on the aggregation of turn-level scores. An ideal approximation of the human evaluation process is a top-down approach whereby we examine the quality of the entire dialogue at macro level before zooming into the dialogue turns. Hence, a unified framework, which holistically models the entire dialogue, is highly sought after."
    }, {
      "heading" : "2.2 Dialogue Coherence",
      "text" : "Examining a dialogue at macro level is related to discourse coherence (Halliday and Hasan, 2014; Grosz et al., 1995; Barzilay and Lapata, 2008), which considers whether a piece of text is in a consistent and logical manner, as opposed to a random collection of sentences. Dialogue is a special kind of discourse structure, of which coherence assessment is an essential part of quality evaluation.\nMany studies have followed the standard discourse coherence evaluation protocol (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020). Very few have considered customizing their dialogue coherence models for evaluating the performance of dialogue systems. It is common to leverage supervised approaches (Higashinaka et al.,\n2014; Gandhe and Traum, 2016; Cervone et al., 2018; Yi et al., 2019), that is closely linked to modeling with entities and dialogue acts (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).\nHence, we are motivated to study the application of dialogue coherence modeling for automatic dialogue evaluation by designing a self-supervised framework, without dependence on any human annotations for coherence features."
    }, {
      "heading" : "2.3 Graph Modeling of Dialogue",
      "text" : "Recently, the graph neural network (GNN) (Scarselli et al., 2008; Kipf and Welling, 2017; Schlichtkrull et al., 2018) has been successfully applied in various dialogue applications. For example, Ghosal et al. (2019) adopts GCN for utterancelevel emotion recognition. Chen et al. (2018) modeled structured dialogue policy with GNN and (Qin et al., 2020) proposes a joint framework leveraging graph attention network (Veličković et al., 2018) for both dialogue act recognition and sentiment classification.\nGNN is useful for dialogue modeling, because the relative position of target and context utterances decides how past utterances influence future utterances and vice versa (Ghosal et al., 2019). The interaction of utterances can be effectively captured with a graph structure as long as they are connected by relation-aware edges. However, GNN has not been well studied for dialogue evaluation. Huang et al. (2020) recently proposes the GRADE metric, leveraging graph modeling for turn-level coherence evaluation. The way we use GNN is different from Huang et al. (2020) because GRADE is focused on turn-level coherence evaluation while we are interested in a turn-dialogue joint evaluation. Furthermore, GRADE considers the keywords in context-response pairs, and we explicitly use graph structure to model the speaker and utterance level interaction within a dialogue."
    }, {
      "heading" : "3 DynaEval Framework",
      "text" : "DyanEval represents an integration of several ideas. It takes advantage of the structured graph representation of dialogues, useful information on the utterance and speaker level interaction. It is motivated by dialogue coherence modeling.\nIn this paper, we only consider dyadic dialogues, but the formulation can be easily generalized to multi-party conversations. Formally, let A and B\ndenote the two speakers participating in the dialogue. A dialogue, D, consists of a sequence of n utterances, [uA1 , u B 2 , . . . , u A n−1, u B n ]\n2. Let D̄ represent the negative dialogue sample obtained via various sampling strategies described in Section 3.5.\nFigure 1 illustrates the learning process of DynaEval in four steps3: (1) Deriving contextualized representation, ei, for utterances within D. (Section 3.1). (2) Constructing the directed dialogue graph. The nodes are initialized with ei and the edges between node pairs represent the speaker and temporal dependencies (Section 3.2). (3) Generating utterance-level graph representation, hi, via feature transformation to aggregate useful contextual information from all connected neighbours to the current node (Section 3.3). (4) producing a dialogue-level score, which indicates whether D is preferred over D̄ (Section 3.4)."
    }, {
      "heading" : "3.1 Dialogue Utterance Representation",
      "text" : "A sentence-encoder is needed to map the individual utterances within D onto the vector space. Firstly, we fine-tune a RoBERTa-base pre-trained language model (Liu et al., 2019) with training data of the target dialogue domain, because task-adaptive finetuning of the pre-trained language model on the target domain data benefits the final performance (Gururangan et al., 2020; Lee and Li, 2020). Next, the mean pooling operation is performed on the token embeddings within each utterance of D to derive their respective utterance-level representations. Formally, let SRoBERTa denotes the sentence encoder and u∗i in D is mapped into vector representations, ui ∈ Rd, whereby\nui = SRoBERTa(u∗i ) (1)\nNote that ∗ can be either speaker A or speaker B. Then, to capture a more fine-grained temporal dependency among the utterances, a bidirectional LSTM is adopted to model the sequential flow of information withinD. The context-aware utterance representation, ei is then obtained via:\nei = ←−−→ LSTM(ei(+,−)1,ui) (2)"
    }, {
      "heading" : "3.2 Dialogue Graph Construction",
      "text" : "D is represented with a directed graph, G = (V, E). V is the sets of graph nodes and E is the set of\n2n is assumed to be even to simplify the mathematical expressions.\n3Note that all the operations from Section 3.1 through Section 3.4 are illustrated with D. They are applied in the same way on D̄.\nedges, which reflects the contextual dependencies among utterance pairs.\nGraph Nodes Each graph node corresponds to an utterance within D. Hence, for a dialogue with n utterances, V = {v1, v2, . . . , vn−1, vn}. All the graph nodes are initialized with utterance-level contextualized embeddings: vi = ei.\nEdges For short conversations, G will be a fullyconnected graph whereby all graph nodes are connected to each other, including self-connection. The intuition is that short conversations tend to focus on a single topic and thus, each utterance is contextually dependent on all the other utterances in the dialogue. For long conversations, there may be frequent topic shifts. Distant utterances within the same dialogue may not be contextually relevant to the current utterance. Sometimes, adding more context leads to diminishing performance gain or even negative impact (Zhong et al., 2019). Therefore, a context window length, M , is set, which means that vi is only connected to vj ∈ {vi−M , vi−M+1, . . . , vi, vi+1, . . . , vi+M}4. Let vij ∈ E denote the edge from vj to vi. Each edge is associated with an edge weight, aij , and a relation type, θij . They are illustrated as follows:\nEdge Weights The edge weight determines the relative importance of the neighbour nodes w.r.t the current node. A similarity based attention module\n4For simplicity purpose, we do not explicitly include the cases when i <= M or i+M is greater than the total number of utterances in a dialogue in the formula.\nis applied to determine the edge weights. For a graph node, vi, the set of weights, ai, w.r.t all its incoming edges, should sum up to 1. The attention weight is formulated in the following way:\nai = softmax(eTi We[ei−M , . . . , ei+M ]),\nwhere i+M∑ j=i−M aij = 1,We ∈ Rd×d (3)\nMore importance is placed upon neighbouring utterances on the same topic. Little attention is paid to the irrelevant utterances.\nEdge Relations Following (Ghosal et al., 2019), there are two aspects to take into account when defining the relation types. One aspect is to capture speaker dependencies. This is because we want to model the interaction between the interlocutors in a dialogue. The other aspect is to consider the temporal dependencies. This pertains to the relative position of an utterance w.r.t another. The explicit modeling of such dependency is important since the ordering of utterances within a dialogue is an essential feature for learning dialogue coherence. With these considerations, the total number of distinct types of relations5 will be 2 (u∗i occurs before or after u∗j ) × 2 (either uAi or uBi ) × 2 (either uAj or uBj ) plus the self-connection (i = j). This is depicted with different arrows connecting the graph nodes in Figure 1. We define this set of 9 relation types as Θ and θij ∈ Θ.\n5Since we are considering dyadic dialogues, there are only two speakers involved. The formulation can be generalized to multi-party dialogue."
    }, {
      "heading" : "3.3 Feature Transformation",
      "text" : "This section describes the process of transforming the initial node representation, ei, into both a speaker and context aware vector representation, hi, which captures the dynamics of interaction w.r.t u∗i . Basically, the whole process is a two-stage graph convolution.\nThe first stage aggregates information from neighbourhood nodes to the current node vi based on the relation-aware transformation motivated by (Schlichtkrull et al., 2018) whereby edges of different relation types are associated with different transformation matrix, W\n′ θ:\nh ′ i = σ( ∑ θ∈Θ ∑ j∈Sθi aij ci,θ W ′ θej + aiiW ′ 0ei)\nfor i = 1, 2, . . . , n\n(4)\nIn Equation 4, h′i is the intermediate node representation and σ denotes the activation function, such as ReLU. Sθi represents the set of indices of nodes connected to vi with their edges vij having the relation type θ ∈ Θ. aij and aii are the edge weights of vij and vii respectively. W ′ θ ∈ Rd ′×d and W ′ 0 ∈ Rd\n′×d are learnable parameters of the feature transformation. ci,θ is a problem specific normalization constant, which can be set as a learnable parameter or fixed in advance.\nThe second stage applies another graph convolution operation on the intermediate node representation, h′i and the final node representation, hi is obtained via:\nhi = σ( ∑ j∈Sθi W ′′ h ′ j +W ′′ 0 h ′ i)\nfor i = 1, 2, . . . , n\n(5)\nwhere W ′′ ∈ Rd ′′×d′ and W ′′ 0 ∈ Rd ′′×d′ are two learnable parameters in the second stage of feature transformation.\nThrough Equation 4 and Equation 5, relevant contextual information from neighbouring nodes is effectively accumulated to the current node while irrelevant information is filtered out."
    }, {
      "heading" : "3.4 The Scoring Process",
      "text" : "In the scoring step, hi is first concatenated with ei to obtain the final utterance representation, gi. Next, a mean pooling layer is applied on all the utterance representations in a conversation to derive\nthe dialogue-level representation, o:\no = ∑n\ni=1 gi | ∑n j=1 gj | (6)\nō, which corresponds to D̄, is obtained in the same way. A unified score, sdial or s ¯dial, is derived by passing o or ō through a fully-connected layer."
    }, {
      "heading" : "3.5 Training Setup",
      "text" : "Learning Objective Inspired by the preference learning approaches, the label, y for the D and D̄ pair is defined as:\ny = { 1 if D is preferred over D̄ −1 if D̄ is preferred over D\n(7)\nThe margin ranking loss function is adopted to train DynaEval.\nL = max(0,−y ∗ (sdial − s ¯dial) + 1) (8)\nSampling Strategy Two negative sampling strategies are explored in this paper to construct D̄: Utterance Replacement (UR) and Speaker Level Utterance Shuffling (SS).\nUtterance Replacement (UR) An utterance randomly selected from a dialogue is replaced with another utterance randomly chosen from a different dialogue. This sampling strategy perturbs a dialogue at the semantic level. An utterance from a different dialogue is considered topically in-congruent w.r.t the current dialogue context. It breaks down the current dialogue by suddenly injecting irrelevant information.\nSpeaker Level Utterance Shuffling (SS) With this strategy, the order of utterances from one speaker in a dialogue is kept the same while that from another speaker is shuffled. SS changes the coherence structure of a dialogue w.r.t specific speaker. This strategy is motivated by (Healey et al., 2014), which adopts a ”Chance Other” method to measure how much syntactic and lexical repetition of a speaker happen by chance. The reason why we do not randomly permute the order of all utterances in the dialogue is because random permutation of all utterances is a very simple discrimination task."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this work, we consider two experiment settings to assess the effectiveness of DynaEval. The first setting (Section 4.2) is similar to the studies on\ndialogue coherence (Cervone et al., 2018; Mesgar et al., 2020) where accuracy score is applied to evaluate its discrimination capability in distinguishing original dialogues from negative samples. The second setting (Section 4.3) is to evaluate its dialogue-level and turn-level judgement capability via correlation analysis on the human-chatbot conversational datasets. The domain of the evaluation set is different from that of human-human conversation datasets that DyanEval is trained on."
    }, {
      "heading" : "4.1 Dialogue Datasets",
      "text" : "Three bench-marking open-domain dialogue datasets are included in our experiments, Empathetic Dialogue (Rashkin et al., 2019), ConvAI2 PERSONACHAT (Zhang et al., 2018b; Dinan et al., 2020) and DialyDialog (Li et al., 2017). For training, we remove dialogues containing less than 4 utterances or more than 30 utterances. Statistics of the three human-human dialogue corpora after filtering is presented in Table 1.\nEmpathetic Dialogue is designed for mimicking the real-life human conversation scenario whereby the interlocutors need to recognize and acknowledge the others’ feelings in the conversation. This dataset pertains to the short conversation scenario where interlocutors stick to a single topic.\nConvAI2 PERSONACHAT is a crowdsourced dataset where each pair of interlocutors try to get to know each other by conditioning their conversations on their respective persona profile provided in prior. The dataset contains more number of turns per dialogue as compared to Empathetic Dialogue. Hence, topic shift is more likely to occur within a dialogue and this simulates the long conversation scenario mentioned in Section 3.2.\nDailyDialog is a high-quality human-human conversation dataset, which reflects our day-to-day communications and covers different topics about our daily life, such as relationship and health. The average dialogue length of DailyDialog lies in the middle of that of Empathetic Dialogue and ConvAI2. Topic shift in the conversations of DailyDialog occurs less frequently as compared to those in ConvAI2."
    }, {
      "heading" : "4.2 The Dialogue-level Discrimination Task",
      "text" : "Similar to the previous works (Cervone and Riccardi, 2020; Mesgar et al., 2020), 20 perturbations are created for each dialogue w.r.t both UR and SS. For each perturbation, two pairs are formed, {D, D̄} with label y = 1 and {D̄,D} with label\ny = −1. Then, we train, fine-tune, and evaluate DynaEval on the training, validation, and test sets for each sampling strategy. Note that all these sets are constructed with the same perturbation method.\nBaselines we compare DynaEval against three baselines: RANDOM, CoSim (Xu et al., 2018) and S-DiCoh (Mesgar et al., 2020). RANDOM baseline arbitrarily assigns a label to the input dialogue pairs. It suggests the peformance lower bound. CoSim is a common method for dialogue coherence assessment (Xu et al., 2018; Zhang et al., 2018a). It obtains a dialogue-level score by averaging the cosine similarities between sentence embeddings of all adjacent utterance pairs within the dialogue. For fair comparison, we apply the same procedure described in Section 3.1 to derive the sentence embedding of an utterance in CoSim. S-DiCoh (Mesgar et al., 2020) is a recent state-of-the-art dialogue coherence model. It models a dialogue with a neural network framework consisting of two bidrectional LSTM layers with attention mechanism at both the token and utterance level.\nResults and Analysis It can be observed in Table 2 that on all bench-marking dialogue datasets, DynaEval outperforms the baselines in both UR and SS category. Even though the dialogue datasets possess different characteristics as indicated in Section 4.1, DynaEval exhbits robust performance across all the datasets. This confirms our hypothesis that DynaEval provides useful dialogue-level representation for distinguishing the original dialogues from the corresponding negative samples. Especially when compared to S-Dicoh, which mod-\nels a dialogue sequentially with bidrectional LSTM and does not explicitly incoporate the speaker level interaction, the structured graph modeling of a dialogue in DynaEval is more effective for capturing both the interaction between the interlocutors and the contextual information within a dialogue.\nBased on the experimental results, it can be deduced that the discrimination task with UR strategy is more challenging compared to that with SS strategy. The accuracy scores achieved by S-DiCoh in the SS category is much higher than that in the UR category on both datasets. Similar observation can be made w.r.t CoSim and DynaEval on the ConvAI2 dataset. DynaEval performs remarkably in this task as it outperforms S-DiCoh by a significant margin of 13.97, 18.43 and 8.22 on Empathetic Dialogue, ConvAI2 and DailyDialog respectively. Given these observations, we further hypothesize that DynaEval model trained with UR strategy offers more useful dialogue representation to the dialogue evaluation task."
    }, {
      "heading" : "4.3 Dialogue Evaluation Task",
      "text" : "To validate the above hypothesis, we assess the usefulness of DynaEval in both the dialogue-level and turn-level evaluation tasks. In both settings, Spearman correlations between the scores generated by DynaEval and the corresponding human evaluation scores are computed. The performance of DynaEval is compared against several recently proposed dialogue evaluators.\nEvaluation Dataset FED (Mehri and Eskenazi, 2020a) is a bench-marking dataset useful for both dialogue-level and turn-level evaluation. It contains both human-human conversations and humanchatbot conversations, which are collected by the authors of the Meena chatbot (Adiwardana et al., 2020) in an interactive setup. In total, 124 conversations are collected, out of which 40 come from\ninteracting with the Meena Chatbot, 44 come from interacting with the Mitsuku Chatbot and 40 are drawn from human-human conversations. The average number of utterances per conversation is 13.72 and the average number of words per utterance is 9.23. Human quality annotations of these conversations are performed at both the dialogue and turn level. There are 9 quality aspects for turn-level annotations and 11 for dialog-level annotations outlined in the first column of Table 3. FED includes 3348 turn-level and 1364 dialog-level annotations, for a total of 4712. The inter-annotator agreements for all the quality aspects, which indicate the metric performance upper bound, is shown in the last column of Table 3.\nMetrics to Compare The recently proposed reference-free state-of-the-art dialogue metrics, including USR (Mehri and Eskenazi, 2020b), BERTRUBER (Ghazarian et al., 2019) (BERT-R), GPT-2 based coherence metric (Pang et al., 2020) (GPT2) and FED (Mehri and Eskenazi, 2020a)6, serve as the baseline dialogue evaluators. Since USR, BERT-R and GPT-2 are turn-level metrics, aggregation of all the turn-level scores in a dialogue is required for dialogue-level evaluation. The best correlation scores at dialogue level are reported in Table 3 among all the aggregation strategies for these three metrics. For completeness, we report their correlation scores w.r.t difference aggregation strategies in Appendix A.2. Similar to DynaEval, S-Dicoh provides a unified score for each dialogue. Based on insights from Section 4.2, the best performing model in the UR category is chosen to score the dialogues for both S-Dicoh and DynaEval.\nDialogue-level Evaluation DynaEval achieves 6The correlation scores of FED is obtained from the original paper. For each evaluation category, the highest score is reported among the scores provided by all its variants.\nthe highest correlation scores in 8 out of 11 dialogue aspects, including the overall category. For the other three categories, DynaEval attains second highest correlation scores. We can see that DynaEval significantly outperforms S-DiCoh. These results showcase that structured graph modeling of a dialogue with explicit incorporation of speaker and utterance level dependencies provides meaningful dialogue-level representations. Such representations capture information of various dialogue attributes that are beneficial for the dialogue-level evaluation task.\nMoreover, BERT-R, GPT-2 and USR are stateof-the-art turn-level evaluation metrics. They evaluate a dialogue based on aggregation of scores of all the context-response pairs within the dialogue. It can be observed that their correlation scores across individual dialogue aspects are not as high as those of DynaEval. This supports our hypothesis in Section 1 that turn-level quality evaluation may be insufficient to assess the performance of open-domain dialogue systems.\nIn addition, dialogue aspects, including coherence, likability, informativeness and Inquisitiveness, are highly dependent on the interaction of the interlocutors. Amongst all the dialogue aspects,\nDynaEval achieves significantly higher scores in these four categories. This attributes to its incorporation of the speaker level dependency.\nTurn-level Evaluation Furthermore, it can be observed that DynaEval achieves the highest correlation in 5 out of 9 categories including the overall category. This demonstrates that DynaEval is not only useful for holistic evaluation of a dialogue, but also useful for turn level evaluation. In this sense, DynaEval serves as a better proxy to the human evaluation process (Li et al., 2019) whereby humans mainly evaluate the conversations in a holistic manner and laser-focus on the problematic turns.\nSpecifically, DynaEval performs well in turnlevel aspects, such as relevance, semantic appropriateness and correctness. These aspects highly correlate to the dialogue-level attributes, such as coherence and understanding, suggesting that the evaluation of these turn-level attributes also benefit from the explicit modeling of the speaker and utterance level interaction in a unified framework.\nError Analysis An interesting finding is that DynaEval and FED actually complement each other at both dialogue and turn level. For example, at the dialogue level, FED performs well in diversity and topic depth, but struggles with coher-\nence and consistency. DynaEval performs well in coherence and consistency, but its performance in diversity is much lower in comparison to FED. This may be because dialoGPT, the backbone of FED, was trained on a large amount of Reddit data, which contain diverse amount of topics and variation of expressions while DynaEval is trained on a single dialogue domian. Moreover, dialoGPT does not explicitly model such speaker-level interaction, but DynaEval does. Hence, DynaEval is more useful for evaluating coherence and consistency aspects of a dialogue. One way to improve DynaEval for evaluating topic depth and diversity is to pre-train on a large amount of dialogue data with a variety of topics and then fine-tune it on the target domain.\nAnother observation is that DynaEval performs significantly poorer for the fluency aspect at turnlevel than for other turn-level aspects. Additionally, GPT-2, USR and FED, which leverage pretrained language model, perform significantly better than DynaEval in this category. This may be because DynaEval directly models a dialogue at the utterance level instead of at the token level, while the other metrics consider the language modeling objective, which focuses more on the token-level dependencies rendering them effective for evaluating the naturalness of a response. A remedy to this problematic aspect of DynaEval is to introduce perturbation strategies targeting the token level, such as word drop, word shuffling and word replacement (Sinha et al., 2020; Park et al., 2021). Such strategies provide negative samples mimicking the non-sensical or non-grammatical responses produced by certain seq2seq generative models. Another simple solution is to combine DynaEval with turn-level metrics specifically designed for evaluating naturalness of dialogue responses.\nBesides the fluency aspect, DynaEval’s performance in interestingness, engagement and specificity at the turn level is not as pronounced as that of FED. This may be because purely modeling the dialogue itself is not enough for all the aspects. The model may need to incorporate external knowledge concerning a diverse range of topics to be able to reflect these attributes. The same conclusion can also be drawn from DynaEval’s relatively weaker performance in the diversity category at the dialogue level.\nLastly, DynaEval primarily targets open-domain dialogues where there is no clear or predefined task to perform. When evaluating task-oriented\ndialogues, task completion will take a more central role. Meta-information such as intents and request types are important to determine task completion and therefore, the evaluation framework will require further adaptation accounting for these information when evaluating task-oriented dialogues."
    }, {
      "heading" : "5 Conclusion & Future Work",
      "text" : "DynaEval serves as a unified framework for both turn and dialogue level evaluation in open-domain dialogue. It provides meaningful representations that incorporate information reflecting various important dialogue attributes. Its explicit modeling of speaker and utterance level interaction leveraging GCN has been proven beneficial for the evaluation task. Lastly, the error analysis in Section 4.3 sheds light on how DynaEval can be further improved. DynaEval can also be combined with the specialized turn-level metrics, such as those targeting fluency and engagement, to fully approximate the interactive human evaluation process."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work is supported by Human-Robot Interaction Phase 1 (Grant No. 19225 00054), National Research Foundation (NRF) Singapore under the National Robotics Programme; Human Robot Collaborative AI for AME (Grant No. A18A2b0046), NRF Singapore; Robert Bosch (SEA) Pte Ltd under EDB’s Industrial Postgraduate Programme – II (EDB-IPP), project title: Applied Natural Language Processing; and by the Spanish projects: AMIC (MINECO, TIN2017-85854-C4-4-R) and CAVIAR (MINECO, TEC2017-84593-C2-1-R) projects partially funded by the European Union.\nEthical Considerations & Broader Impact\nThis study conforms to the prevailing ethical guidelines. All datasets used are in the public domain. In addition, we have identified a way that DynaEval can help address the ethical concerns. By explicitly training the framework to discriminate safe dialogues from unsafe ones, it can help detect dialogues containing inappropriate sentences, such as those regarding injustice and discrimination. Such application may be useful in many real-life scenarios where the behaviors of chatbots need to be properly monitored to avoid insensitive and irresponsible comments from the chatbots."
    }, {
      "heading" : "A Additional Experimental Results",
      "text" : "A.1 Utterance-level Pooling Techniques\nTo derive the dialogue-level representation, we have adopted the mean pooling method in DynaEval. In this section, we examine the effects of different pooling methods in the dialogue-level discrimination task. Specifically, we compare the performance of mean pooling against max pooling and the concatenation of sentence vectors derived with both mean and max pooling. The performance comparison is presented in Table 4. It can be observed that the performance difference across various pooling strategies is not statistically significant.\nA.2 Dialogue-level Correlation Analysis of Turn-level Metrics\nFor each turn-level metric, we have applied four simple aggregation strategies to derive dialogue level scores from their respective constituent turn level scores: (1) Mean, (2) Sum, (3) Max and (4) Multiplication. The dialogue level correlation coefficients of USR, BERT-RUBER and GPT-2 based coherence metric are reported in Table 5, Table 6 and Table 7 correspondingly. Note that for turnlevel metrics leveraging the language model objective, we don’t consider token-level aggregation variants. Instead, we follow the same formulations in the original papers. For example, the GPT-2 based coherence metric (Pang et al., 2020) computes a turn-level score based on averaging the token-wise conditional log probabilities in the corresponding response.\nIt can be observed that all three metrics don’t perform well at dialogue level evaluation. This further validates our statement in Section 1 that turn-level quality evaluation may be insufficient to assess the performance of open-domain dialogue systems as they don’t specifically model the interaction over an entire dialogue.\nQuality Mean Sum Max Prod\nUSR\nQuality Mean Sum Max Prod\nBERT-R"
    }, {
      "heading" : "B Reproducibility",
      "text" : "B.1 Training Setup & Hyperparameters For all the experiments involving training, we run the experiments five times with different random seeds for model weights initialization to reduce the risk of randomness. The experiments are performed on a single Tesla V100 32GB GPU with a batch size of 512. The model is trained for 20 epochs and its parameters are optimized using the Adam optimizer. The average run time for each epoch is around 8 hours and 15 minutes. The initial learning rate is set to 0.002 and decays by a factor\nQuality Mean Sum Max Prod\nGPT-2\nof 0.5 per epoch. A dropout of 0.5 is also applied. For Empathetic Dialogue and DailyDialog, the context window length,M is set to 4, because these two datasets contain relatively short conversations (4.31 and 7.90 average number of utterances per dialogue respectively). A context window size of 4 ensures each utterance is connected to all the remaining utterances in most of the dialogues. The utterances may provide important contextual information to each other within a dialogue. For ConvAI2, M is set to 2 to avoid introducing too much irrelavant context information. This is because most of the conversations in ConvAI2 are about two people getting to know each other and there are frequent topic changes in the conversations. M serves as an important hyperparameter to control the influence of an utterance on the rest in a dialogue.\nFor training DynaEval, we have filtered out dialogues of which the number of utterances is less than 4 or more than 30. We hypothesize that dialogues with less than 4 utterances containing little information for modeling speaker and utterance level interaction. Moreover, there are very few dialogues with more than 30 utterances in both datasets. Including them leads to large graphs and unnecessary paddings, which slow down the training process."
    } ],
    "references" : [ {
      "title" : "Towards a human-like open-domain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu" ],
      "venue" : "arXiv preprint arXiv:2001.09977",
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics, 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "Is this dialogue coherent? learning from dialogue acts and entities",
      "author" : [ "Alessandra Cervone", "Giuseppe Riccardi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 162–174, 1st virtual meeting.",
      "citeRegEx" : "Cervone and Riccardi.,? 2020",
      "shortCiteRegEx" : "Cervone and Riccardi.",
      "year" : 2020
    }, {
      "title" : "Coherence models for dialogue",
      "author" : [ "Alessandra Cervone", "Evgeny Stepanov", "Giuseppe Riccardi." ],
      "venue" : "Proc. Interspeech 2018, pages 1011–1015.",
      "citeRegEx" : "Cervone et al\\.,? 2018",
      "shortCiteRegEx" : "Cervone et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured dialogue policy with graph neural networks",
      "author" : [ "Lu Chen", "Bowen Tan", "Sishan Long", "Kai Yu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1257–1268, Santa Fe, New Mexico, USA. Associ-",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland, USA. Association",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Spot the bot: A robust and efficient framework for the evaluation of conversational dialogue systems",
      "author" : [ "Jan Deriu", "Don Tuggener", "Pius von Däniken", "Jon Ander Campos", "Alvaro Rodrigo", "Thiziri Belkacem", "Aitor Soroa", "Eneko Agirre", "Mark Cieliebak" ],
      "venue" : null,
      "citeRegEx" : "Deriu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Deriu et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic evaluation of endto-end dialog systems with adequacy-fluency metrics",
      "author" : [ "Luis Fernando D’Haro", "Rafael E Banchs", "Chiori Hori", "Haizhou Li" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "D.Haro et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "D.Haro et al\\.",
      "year" : 2019
    }, {
      "title" : "The second conversational intelligence challenge (ConvAI2)",
      "author" : [ "Lowe" ],
      "venue" : "In The NeurIPS’18 Competition,",
      "citeRegEx" : "Lowe,? \\Q2020\\E",
      "shortCiteRegEx" : "Lowe",
      "year" : 2020
    }, {
      "title" : "Evaluating coherence in dialogue systems using entailment",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Dziri et al\\.,? 2019",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols",
      "author" : [ "Sarah E. Finch", "Jinho D. Choi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 236–",
      "citeRegEx" : "Finch and Choi.,? 2020",
      "shortCiteRegEx" : "Finch and Choi.",
      "year" : 2020
    }, {
      "title" : "A semi-automated evaluation metric for dialogue model coherence",
      "author" : [ "Sudeep Gandhe", "David Traum." ],
      "venue" : "Situated Dialog in SpeechBased Human-Computer Interaction, pages 217– 225. Springer.",
      "citeRegEx" : "Gandhe and Traum.,? 2016",
      "shortCiteRegEx" : "Gandhe and Traum.",
      "year" : 2016
    }, {
      "title" : "Approximating interactive human evaluation with self-play for open-domain dialog systems",
      "author" : [ "Asma Ghandeharioun", "Judy Hanwen Shen", "Natasha Jaques", "Craig Ferguson", "Noah Jones", "Agata Lapedriza", "Rosalind Picard." ],
      "venue" : "Advances in Neural",
      "citeRegEx" : "Ghandeharioun et al\\.,? 2019",
      "shortCiteRegEx" : "Ghandeharioun et al\\.",
      "year" : 2019
    }, {
      "title" : "Better automatic evaluation of open-domain dialogue systems with contextualized embeddings",
      "author" : [ "Sarik Ghazarian", "Johnny Wei", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Lan-",
      "citeRegEx" : "Ghazarian et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2019
    }, {
      "title" : "Predictive engagement: An efficient metric for automatic evaluation of opendomain dialogue systems",
      "author" : [ "Sarik Ghazarian", "Ralph Weischedel", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
      "citeRegEx" : "Ghazarian et al\\.,? 2020",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2020
    }, {
      "title" : "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "author" : [ "Deepanway Ghosal", "Navonil Majumder", "Soujanya Poria", "Niyati Chhaya", "Alexander Gelbukh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Ghosal et al\\.,? 2019",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2019
    }, {
      "title" : "Centering: A framework for modelling the local coherence of discourse",
      "author" : [ "Barbara J Grosz", "Aravind K Joshi", "Scott Weinstein" ],
      "venue" : null,
      "citeRegEx" : "Grosz et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Grosz et al\\.",
      "year" : 1995
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Cohesion in English",
      "author" : [ "Michael Alexander Kirkwood Halliday", "Ruqaiya Hasan." ],
      "venue" : "9. Routledge.",
      "citeRegEx" : "Halliday and Hasan.,? 2014",
      "shortCiteRegEx" : "Halliday and Hasan.",
      "year" : 2014
    }, {
      "title" : "Divergence in dialogue",
      "author" : [ "Patrick GT Healey", "Matthew Purver", "Christine Howes." ],
      "venue" : "PloS one, 9(6):e98598.",
      "citeRegEx" : "Healey et al\\.,? 2014",
      "shortCiteRegEx" : "Healey et al\\.",
      "year" : 2014
    }, {
      "title" : "Evaluating coherence in open domain conversational systems",
      "author" : [ "Ryuichiro Higashinaka", "Toyomi Meguro", "Kenji Imamura", "Hiroaki Sugiyama", "Toshiro Makino", "Yoshihiro Matsuo." ],
      "venue" : "Fifteenth Annual Conference of the International Speech Com-",
      "citeRegEx" : "Higashinaka et al\\.,? 2014",
      "shortCiteRegEx" : "Higashinaka et al\\.",
      "year" : 2014
    }, {
      "title" : "GRADE: Automatic graphenhanced coherence metric for evaluating opendomain dialogue systems",
      "author" : [ "Lishan Huang", "Zheng Ye", "Jinghui Qin", "Liang Lin", "Xiaodan Liang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Pone: A novel automatic evaluation metric for open-domain generative dialogue systems",
      "author" : [ "Tian Lan", "Xian-Ling Mao", "Wei Wei", "Xiaoyan Gao", "Heyan Huang." ],
      "venue" : "ACM Transactions on Information Systems (TOIS), 39(1):1–37.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling codeswitch languages using bilingual parallel corpus",
      "author" : [ "Grandee Lee", "Haizhou Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 860– 870, Online. Association for Computational Linguis-",
      "citeRegEx" : "Lee and Li.,? 2020",
      "shortCiteRegEx" : "Lee and Li.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons",
      "author" : [ "Margaret Li", "Jason Weston", "Stephen Roller." ],
      "venue" : "arXiv preprint arXiv:1909.03087.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Pa-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised evaluation of interactive dialog with DialoGPT",
      "author" : [ "Shikib Mehri", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225–235, 1st virtual meeting. Asso-",
      "citeRegEx" : "Mehri and Eskenazi.,? 2020a",
      "shortCiteRegEx" : "Mehri and Eskenazi.",
      "year" : 2020
    }, {
      "title" : "USR: An unsupervised and reference free evaluation metric for dialog generation",
      "author" : [ "Shikib Mehri", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681–707, Online. Association for",
      "citeRegEx" : "Mehri and Eskenazi.,? 2020b",
      "shortCiteRegEx" : "Mehri and Eskenazi.",
      "year" : 2020
    }, {
      "title" : "Dialogue coherence assessment without explicit dialogue act labels",
      "author" : [ "Mohsen Mesgar", "Sebastian Bücker", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1439–1450,",
      "citeRegEx" : "Mesgar et al\\.,? 2020",
      "shortCiteRegEx" : "Mesgar et al\\.",
      "year" : 2020
    }, {
      "title" : "Language model transformers as evaluators for open-domain dialogues",
      "author" : [ "Rostislav Nedelchev", "Jens Lehmann", "Ricardo Usbeck." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 6797–6808, Barcelona,",
      "citeRegEx" : "Nedelchev et al\\.,? 2020",
      "shortCiteRegEx" : "Nedelchev et al\\.",
      "year" : 2020
    }, {
      "title" : "I like fish, especially dolphins: Addressing contradictions in dialogue modelling",
      "author" : [ "Yixin Nie", "Mary Williamson", "Mohit Bansal", "Douwe Kiela", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:2012.13391.",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards holistic and automatic evaluation of open-domain dialogue",
      "author" : [ "Bo Pang", "Erik Nijkamp", "Wenjuan Han", "Linqi Zhou", "Yixian Liu", "Kewei Tu" ],
      "venue" : null,
      "citeRegEx" : "Pang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Generating negative samples by manipulating golden responses for unsupervised learning of a response evaluation model",
      "author" : [ "ChaeHun Park", "Eugene Jang", "Wonsuk Yang", "Jong Park." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Park et al\\.,? 2021",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2021
    }, {
      "title" : "Co-GAT: A co-interactive graph attention network for joint dialog act recognition and sentiment classification",
      "author" : [ "Libo Qin", "Zhouyang Li", "Wanxiang Che", "Minheng Ni", "Ting Liu." ],
      "venue" : "arXiv preprint arXiv:2012.13260.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards empathetic opendomain conversation models: A new benchmark and dataset",
      "author" : [ "Hannah Rashkin", "Eric Michael Smith", "Margaret Li", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Rashkin et al\\.,? 2019",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2019
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating dialogue generation systems via response selection",
      "author" : [ "Shiki Sato", "Reina Akama", "Hiroki Ouchi", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 593–599, Online.",
      "citeRegEx" : "Sato et al\\.,? 2020",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2020
    }, {
      "title" : "The graph neural network model",
      "author" : [ "Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini." ],
      "venue" : "IEEE transactions on neural networks, 20(1):61–80.",
      "citeRegEx" : "Scarselli et al\\.,? 2008",
      "shortCiteRegEx" : "Scarselli et al\\.",
      "year" : 2008
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European semantic web conference, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "What makes a good conversation? how controllable attributes affect human judgments",
      "author" : [ "Abigail See", "Stephen Roller", "Douwe Kiela", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "See et al\\.,? 2019",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning an unreferenced metric for online dialogue evaluation",
      "author" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Jasmine Wang", "Ryan Lowe", "William L. Hamilton", "Joelle Pineau." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Sinha et al\\.,? 2020",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2020
    }, {
      "title" : "Can you put it all together: Evaluating conversational agents’ ability to blend skills",
      "author" : [ "Eric Michael Smith", "Mary Williamson", "Kurt Shuster", "Jason Weston", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Smith et al\\.,? 2020",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2020
    }, {
      "title" : "Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogue natural language inference",
      "author" : [ "Sean Welleck", "Jason Weston", "Arthur Szlam", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731–3741, Florence, Italy. Association for",
      "citeRegEx" : "Welleck et al\\.,? 2019",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "Diverse and informative dialogue generation with context-specific commonsense knowledge awareness",
      "author" : [ "Sixing Wu", "Ying Li", "Dawei Zhang", "Yang Zhou", "Zhonghai Wu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Better conversations by modeling, filtering, and optimizing for coherence and diversity",
      "author" : [ "Xinnuo Xu", "Ondřej Dušek", "Ioannis Konstas", "Verena Rieser." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards coherent and engaging spoken",
      "author" : [ "Sanghyun Yi", "Rahul Goel", "Chandra Khatri", "Alessandra Cervone", "Tagyoung Chung", "Behnam Hedayatnia", "Anu Venkatesh", "Raefer Gabriel", "Dilek HakkaniTur" ],
      "venue" : null,
      "citeRegEx" : "Yi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2019
    }, {
      "title" : "2021a. Deep AM-FM: Toolkit for automatic dialogue evaluation",
      "author" : [ "Chen Zhang", "Luis Fernando D’Haro", "Rafael E Banchs", "Thomas Friedrichs", "Haizhou Li" ],
      "venue" : "In Conversational Dialogue Systems for the Next Decade,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "2021b. D-score: Holistic dialogue evaluation without reference",
      "author" : [ "Chen Zhang", "Grandee Lee", "Luis Fernando D’Haro", "Haizhou Li" ],
      "venue" : "IEEE/ACM Transactions on Audio,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Reinforcing coherence for sequence to sequence model in dialogue generation",
      "author" : [ "Hainan Zhang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Xueqi Cheng." ],
      "venue" : "IJCAI, pages 4567–4573.",
      "citeRegEx" : "Zhang et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge-enriched transformer for emotion detection in textual conversations",
      "author" : [ "Peixiang Zhong", "Di Wang", "Chunyan Miao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical intention enhanced network for automatic dialogue coherence assessment",
      "author" : [ "Yunxiao Zhou", "Man Lan", "Wenting Wang." ],
      "venue" : "2019 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 49,
      "context" : "Modern dialogue systems (Smith et al., 2020; Zhang et al., 2020; Adiwardana et al., 2020) leveraging large-scale language model pre-training (Devlin et al.",
      "startOffset" : 24,
      "endOffset" : 89
    }, {
      "referenceID" : 60,
      "context" : "Modern dialogue systems (Smith et al., 2020; Zhang et al., 2020; Adiwardana et al., 2020) leveraging large-scale language model pre-training (Devlin et al.",
      "startOffset" : 24,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Modern dialogue systems (Smith et al., 2020; Zhang et al., 2020; Adiwardana et al., 2020) leveraging large-scale language model pre-training (Devlin et al.",
      "startOffset" : 24,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", 2020) leveraging large-scale language model pre-training (Devlin et al., 2019; Radford et al., 2019) are capable of generating fluent and contextually relevant utterances.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 41,
      "context" : ", 2020) leveraging large-scale language model pre-training (Devlin et al., 2019; Radford et al., 2019) are capable of generating fluent and contextually relevant utterances.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "Yet, they still face difficulties in mimicking human conversations in the sense that they lack certain conversation-level attributes, such as coherence (Cervone et al., 2018), consistency (Welleck et al.",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 52,
      "context" : ", 2018), consistency (Welleck et al., 2019; Nie et al., 2020), diversity (Li et al.",
      "startOffset" : 21,
      "endOffset" : 61
    }, {
      "referenceID" : 36,
      "context" : ", 2018), consistency (Welleck et al., 2019; Nie et al., 2020), diversity (Li et al.",
      "startOffset" : 21,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : ", 2020), diversity (Li et al., 2016; Wu et al., 2020) and engagement (Ghandeharioun et al.",
      "startOffset" : 19,
      "endOffset" : 53
    }, {
      "referenceID" : 53,
      "context" : ", 2020), diversity (Li et al., 2016; Wu et al., 2020) and engagement (Ghandeharioun et al.",
      "startOffset" : 19,
      "endOffset" : 53
    }, {
      "referenceID" : 38,
      "context" : "com/e0397123/DynaEval Commonly used static metrics, such as BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004), correlate poorly with human judgements (Liu et al.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004), correlate poorly with human judgements (Liu et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004), correlate poorly with human judgements (Liu et al.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 30,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004), correlate poorly with human judgements (Liu et al., 2016) rendering",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "While some recent automatic dialogue evaluation metrics (Ghazarian et al., 2019; Mehri and Eskenazi, 2020b; Huang et al., 2020; Zhang et al., 2021b) demonstrate strong correlations with human",
      "startOffset" : 56,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "While some recent automatic dialogue evaluation metrics (Ghazarian et al., 2019; Mehri and Eskenazi, 2020b; Huang et al., 2020; Zhang et al., 2021b) demonstrate strong correlations with human",
      "startOffset" : 56,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "While some recent automatic dialogue evaluation metrics (Ghazarian et al., 2019; Mehri and Eskenazi, 2020b; Huang et al., 2020; Zhang et al., 2021b) demonstrate strong correlations with human",
      "startOffset" : 56,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "This is a departure from the way we frame the problem as a weakly supervised next sentence prediction (Mehri and Eskenazi, 2020b; Sato et al., 2020) or language modeling tasks (Nedelchev et al.",
      "startOffset" : 102,
      "endOffset" : 148
    }, {
      "referenceID" : 44,
      "context" : "This is a departure from the way we frame the problem as a weakly supervised next sentence prediction (Mehri and Eskenazi, 2020b; Sato et al., 2020) or language modeling tasks (Nedelchev et al.",
      "startOffset" : 102,
      "endOffset" : 148
    }, {
      "referenceID" : 35,
      "context" : ", 2020) or language modeling tasks (Nedelchev et al., 2020; Pang et al., 2020) for context-response pairs.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 37,
      "context" : ", 2020) or language modeling tasks (Nedelchev et al., 2020; Pang et al., 2020) for context-response pairs.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "2020), USR-MLM (Mehri and Eskenazi, 2020b) and GPT-2 (Radford et al.",
      "startOffset" : 15,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "2020), USR-MLM (Mehri and Eskenazi, 2020b) and GPT-2 (Radford et al., 2019) based fluency metrics (Nedelchev et al.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 50,
      "context" : "For example, RUBER (Tao et al., 2018), BERT-RUBER(Ghazarian et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : ", 2018), BERT-RUBER(Ghazarian et al., 2019) and USR-DR (Mehri and Eskenazi, 2020b) predict the relatedness between generated",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : ", 2019) and USR-DR (Mehri and Eskenazi, 2020b) predict the relatedness between generated",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Besides these two major aspects, there are many metrics for other qualities, such as adequacy (D’Haro et al., 2019; Zhang et al., 2021a), consistency (Welleck et al.",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 52,
      "context" : ", 2021a), consistency (Welleck et al., 2019; Dziri et al., 2019), engagement (Ghazarian et al.",
      "startOffset" : 22,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : ", 2021a), consistency (Welleck et al., 2019; Dziri et al., 2019), engagement (Ghazarian et al.",
      "startOffset" : 22,
      "endOffset" : 64
    }, {
      "referenceID" : 47,
      "context" : "make the assessment at the end of the conversations (See et al., 2019; Finch and Choi, 2020; Li et al., 2019; Deriu et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "make the assessment at the end of the conversations (See et al., 2019; Finch and Choi, 2020; Li et al., 2019; Deriu et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "make the assessment at the end of the conversations (See et al., 2019; Finch and Choi, 2020; Li et al., 2019; Deriu et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "make the assessment at the end of the conversations (See et al., 2019; Finch and Choi, 2020; Li et al., 2019; Deriu et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 129
    }, {
      "referenceID" : 32,
      "context" : "It has been shown to be more reliable than turn-level static evaluation (Mehri and Eskenazi, 2020a).",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 60,
      "context" : "of a system utterance in an interactive setting by computing the likelihood of a particular follow-up utterance responded by dialoGPT (Zhang et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 43,
      "context" : "dialogue evaluation, which leverages a pre-trained DistilBERT (Sanh et al., 2019) model to extract the semantic representation of dialogue turns and uses bidirectional LSTM to explicitly model the discourse structure.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "Examining a dialogue at macro level is related to discourse coherence (Halliday and Hasan, 2014; Grosz et al., 1995; Barzilay and Lapata, 2008), which considers whether a piece of text is in a consistent and logical manner, as opposed to a random collection of sentences.",
      "startOffset" : 70,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Examining a dialogue at macro level is related to discourse coherence (Halliday and Hasan, 2014; Grosz et al., 1995; Barzilay and Lapata, 2008), which considers whether a piece of text is in a consistent and logical manner, as opposed to a random collection of sentences.",
      "startOffset" : 70,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Examining a dialogue at macro level is related to discourse coherence (Halliday and Hasan, 2014; Grosz et al., 1995; Barzilay and Lapata, 2008), which considers whether a piece of text is in a consistent and logical manner, as opposed to a random collection of sentences.",
      "startOffset" : 70,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Many studies have followed the standard discourse coherence evaluation protocol (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 148
    }, {
      "referenceID" : 62,
      "context" : "Many studies have followed the standard discourse coherence evaluation protocol (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 148
    }, {
      "referenceID" : 34,
      "context" : "Many studies have followed the standard discourse coherence evaluation protocol (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : ", 2019), that is closely linked to modeling with entities and dialogue acts (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 144
    }, {
      "referenceID" : 62,
      "context" : ", 2019), that is closely linked to modeling with entities and dialogue acts (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 144
    }, {
      "referenceID" : 34,
      "context" : ", 2019), that is closely linked to modeling with entities and dialogue acts (Cervone and Riccardi, 2020; Zhou et al., 2019; Mesgar et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 144
    }, {
      "referenceID" : 51,
      "context" : ", 2020) proposes a joint framework leveraging graph attention network (Veličković et al., 2018) for both dialogue act recognition and sentiment classification.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "GNN is useful for dialogue modeling, because the relative position of target and context utterances decides how past utterances influence future utterances and vice versa (Ghosal et al., 2019).",
      "startOffset" : 171,
      "endOffset" : 192
    }, {
      "referenceID" : 31,
      "context" : "model (Liu et al., 2019) with training data of the target dialogue domain, because task-adaptive finetuning of the pre-trained language model on the target domain data benefits the final performance (Gururangan et al.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : ", 2019) with training data of the target dialogue domain, because task-adaptive finetuning of the pre-trained language model on the target domain data benefits the final performance (Gururangan et al., 2020; Lee and Li, 2020).",
      "startOffset" : 182,
      "endOffset" : 225
    }, {
      "referenceID" : 25,
      "context" : ", 2019) with training data of the target dialogue domain, because task-adaptive finetuning of the pre-trained language model on the target domain data benefits the final performance (Gururangan et al., 2020; Lee and Li, 2020).",
      "startOffset" : 182,
      "endOffset" : 225
    }, {
      "referenceID" : 61,
      "context" : "Sometimes, adding more context leads to diminishing performance gain or even negative impact (Zhong et al., 2019).",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Edge Relations Following (Ghosal et al., 2019), there are two aspects to take into account when defining the relation types.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 46,
      "context" : "neighbourhood nodes to the current node vi based on the relation-aware transformation motivated by (Schlichtkrull et al., 2018) whereby edges of different relation types are associated with different transformation matrix, W ′ θ:",
      "startOffset" : 99,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : "This strategy is motivated by (Healey et al., 2014), which adopts a ”Chance Other” method to measure how much syntactic and lexical repetition of a speaker happen by chance.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 42,
      "context" : "datasets are included in our experiments, Empathetic Dialogue (Rashkin et al., 2019), ConvAI2 PERSONACHAT (Zhang et al.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Similar to the previous works (Cervone and Riccardi, 2020; Mesgar et al., 2020), 20 perturbations are created for each dialogue w.",
      "startOffset" : 30,
      "endOffset" : 79
    }, {
      "referenceID" : 34,
      "context" : "Similar to the previous works (Cervone and Riccardi, 2020; Mesgar et al., 2020), 20 perturbations are created for each dialogue w.",
      "startOffset" : 30,
      "endOffset" : 79
    }, {
      "referenceID" : 54,
      "context" : "Baselines we compare DynaEval against three baselines: RANDOM, CoSim (Xu et al., 2018) and S-DiCoh (Mesgar et al.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 54,
      "context" : "CoSim is a common method for dialogue coherence assessment (Xu et al., 2018; Zhang et al., 2018a).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 58,
      "context" : "CoSim is a common method for dialogue coherence assessment (Xu et al., 2018; Zhang et al., 2018a).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "S-DiCoh (Mesgar et al., 2020) is a recent state-of-the-art dialogue coherence model.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : "Evaluation Dataset FED (Mehri and Eskenazi, 2020a) is a bench-marking dataset useful for both dialogue-level and turn-level evaluation.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "It contains both human-human conversations and humanchatbot conversations, which are collected by the authors of the Meena chatbot (Adiwardana et al., 2020) in an interactive setup.",
      "startOffset" : 131,
      "endOffset" : 156
    }, {
      "referenceID" : 33,
      "context" : "reference-free state-of-the-art dialogue metrics, including USR (Mehri and Eskenazi, 2020b), BERTRUBER (Ghazarian et al.",
      "startOffset" : 64,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "reference-free state-of-the-art dialogue metrics, including USR (Mehri and Eskenazi, 2020b), BERTRUBER (Ghazarian et al., 2019) (BERT-R), GPT-2 based coherence metric (Pang et al.",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 37,
      "context" : ", 2019) (BERT-R), GPT-2 based coherence metric (Pang et al., 2020) (GPT2) and FED (Mehri and Eskenazi, 2020a)6, serve as the baseline dialogue evaluators.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : ", 2020) (GPT2) and FED (Mehri and Eskenazi, 2020a)6, serve as the baseline dialogue evaluators.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "In this sense, DynaEval serves as a better proxy to the human evaluation process (Li et al., 2019) whereby humans mainly evaluate the conversations in a holistic manner and laser-focus on the problematic turns.",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 48,
      "context" : "problematic aspect of DynaEval is to introduce perturbation strategies targeting the token level, such as word drop, word shuffling and word replacement (Sinha et al., 2020; Park et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 192
    }, {
      "referenceID" : 39,
      "context" : "problematic aspect of DynaEval is to introduce perturbation strategies targeting the token level, such as word drop, word shuffling and word replacement (Sinha et al., 2020; Park et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 192
    } ],
    "year" : 2021,
    "abstractText" : "A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval1, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level.",
    "creator" : "LaTeX with hyperref"
  }
}