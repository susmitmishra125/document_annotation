{
  "name" : "2021.acl-long.254.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    "authors" : [ "Fengbin Zhu", "Wenqiang Lei", "Youcheng Huang", "Chao Wang", "Shuo Zhang", "Jiancheng Lv", "Fuli Feng", "Tat-Seng Chua" ],
    "emails" : [ "wenqianglei}@gmail.com,", "wangchao@6estates.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3277–3287\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3277"
    }, {
      "heading" : "1 Introduction",
      "text" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al.,\n∗Corresponding author\n2018; Zhang and Balog, 2019; Zhang et al., 2020). Though receiving growing interests (Das et al., 2017; Sun et al., 2019; Chen et al., 2020b, 2021), works on hybrid data comprising of unstructured text and structured or semi-structured KB/tables are rare. Recently, Chen et al. (2020b) attempt to simulate a type of hybrid data through manually linking table cells to Wiki pages via hyperlinks. However, such connection between table and text is relatively loose.\nIn the real world, a more common hybrid data form is, the table (that usually contains numbers) is more comprehensively linked to text, e.g., semantically related or complementary. Such hybrid data are very pervasive in various scenarios like scientific research papers, medical reports, financial reports, etc. The left box of Figure 1 shows a real example from some financial report, where there is a table containing row/column header and numbers inside, and also some paragraphs describing it. We call the hybrid data like this example hybrid context in QA problems, as it contains both tabular and textual content, and call the paragraphs associated paragraphs to the table. To comprehend and answer a question from such hybrid context relies on the close relation between table and paragraphs, and usually requires numerical reasoning. For example, one needs to identify “revenue from the external customers” in the describing text so as to understand the content of the table. As for “How much does the commercial cloud revenue account for the total revenue in 2019?”, one needs to get the total revenue in 2019, i.e. “125, 843 million” from the table and commercial cloud revenue, i.e. “38.1 billion”, from the text to infer the answer.\nTo stimulate progress of QA research over such hybrid data, we propose a new dataset, named TATQA (Tabular And Textual dataset for Question Answering). The hybrid contexts in TAT-QA are extracted from real-world financial reports, each\ncomposed of a table with row/col header and numbers, as well as at least two paragraphs that describe, analyse or complement the content of this table. Given hybrid contexts, we invite annotators with financial knowledge to generate questions that are useful in real-world financial analyses and provide answers accordingly. It is worth mentioning that a large portion of questions in TAT-QA demand numerical reasoning, for which derivation of the answer is also labeled to facilitate developing explainable models. In total, TAT-QA contains 16, 552 questions associated with 2, 757 hybrid contexts from 182 reports.\nWe further propose a novel TAGOP model based on TAT-QA. Taking as input the given question, table and associated paragraphs, TAGOP applies sequence tagging to extract relevant cells from the table and relevant spans from text as the evidences. Then it applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. Predicting the magnitude of a number is an important aspect when tackling hybrid data in TAT-QA, including thousand, million, billion, etc. that are often omitted or shown only in headers or associated paragraphs of the table for brevity. We term such magnitude of a number as its scale. Take Question 6 in Figure 1 as an example: “How much of the total revenue in 2018 did not come from devices?” The numerical value in the answer is obtained by subtraction: “110, 360 - 5, 134”, while the scale “million” is identified from the first-row header of the table. In TAGOP, we incorporate a multi-class classifier for scale prediction.\nWe test three types of QA models on TAT-QA,\nspecially addressing tabular, textual, and hybrid data. Our TAGOP achieves 58.0% in terms of F1, which is a 11.1% absolute increase over the best baseline model, according to our experiments on TAT-QA. It is worth noting that the results still lag far behind performance of human experts, i.e. 90.8% in F1. We can see that to tackle the QA task over the hybrid data as in TAT-QA is challenging and more effort is demanded. We expect our TAT-QA dataset and TAGOP model to serve as a benchmark and baseline respectively to contribute to the development of QA models for hybrid data, especially those requiring numerical reasoning."
    }, {
      "heading" : "2 Dataset Construction and Analysis",
      "text" : "We here explain how we construct TAT-QA and analyze its statistics to better reveal its proprieties."
    }, {
      "heading" : "2.1 Data Collection and Preprocessing",
      "text" : "In TAT-QA there are two forms of data: tables and their relevant text, which are extracted from real-world financial reports.\nIn particular, we first download about 500 financial reports released in the past two years from an online website1. We adopt the table detection model in (Li et al., 2019) to detect tables in these reports, and apply Apache PDFBox2 library to extract the table contents to be processed with our annotation tool. We only keep those tables with 3 ∼ 30 rows and 3 ∼ 6 columns. Finally, about 20, 000 candidate tables are retained, which have no standard schema and lots of numbers inside.\n1https://www.annualreports.com/ 2https://pdfbox.apache.org/\nThe corresponding reports with selected tables are also kept. Note that these candidate tables may still contain errors, such as containing too few or many rows/cols, mis-detected numbers, which will be manually picked out and deleted or fixed during the annotation process."
    }, {
      "heading" : "2.2 Dataset Annotation",
      "text" : "The annotation is done with our self-developed tool. All the annotators are with financial background knowledge. Adding Relevant Paragraphs to Tables We build valid hybrid contexts based on the original reports kept in the previous step. A valid hybrid context in TAT-QA consists of a table and at least two associated paragraphs surrounding it, as shown in the left box in Figure 1. To associate enough relevant paragraphs to a candidate table, the annotators first check whether there are ≥ 2 paragraphs around this table, and then check whether they are relevant, meaning the paragraphs should be describing, analysing or complementing the content in the table. If yes, then all the surrounding paragraphs will be associated to this table. Otherwise, the table will be skipped (discarded).3 Question-Answer Pair Creation Based on the valid hybrid contexts, the annotators are then asked to create question-answer pairs, where the questions need to be useful in real-world financial analyses. In addition, we encourage them to create questions that can be answered by people without much finance knowledge and use common words instead of the same words appeared in the hybrid context (Rajpurkar et al., 2016). Given one hybrid context, at least 6 questions are generated, including extracted and calculated questions. For extracted questions, the answers can be a single span or multiple spans from either the table or the associated paragraphs. For calculated questions, numerical reasoning is required to produce the answers, including addition, subtraction, multiplication, division, counting, comparison/sorting and their compositions. Furthermore, we particularly ask the annotators to annotate the right scale for the numerical answer when necessary. Answer Type and Derivation Annotation The answers in TAT-QA have three types: a single span or multiple spans extracted from the table or text, as well as a generated answer (usually obtained through numerical reasoning). The annotators will\n3About two thirds of candidate tables were discarded.\nalso need to label its type after they generate an answer. For generated answers, the corresponding derivations are provided to facilitate the development of explainable QA models, including two types: 1) an arithmetic expression, like (11, 386 - 10, 353)/10, 353) for Question 8 in Figure 1, which can be executed to arrive at the final answer; and 2) a set of items separated with “##”, like “device ## enterprise services” for Question 4 in Figure 1 where the count of items equals the answer. We further divide questions in TAT-QA into four kinds: Span, Spans, Arithmetic and Counting, where the latter two kinds correspond to the above two types of deviations, to help us better investigate the numerical reasoning capability of a QA model. Answer Source Annotation For each answer, annotators are required to specify the source(s) it is derived from, including Table, Text, and Table-text (both). This is to force the model to learn to aggregate information from hybrid sources to infer the answer, thus lift its generalizability. For example, to answer Question 7 in Figure 1: “How much does the commercial cloud revenue account for the total revenue in 2019?”, we can observe from the derivation that “125, 843 million” comes from the table while “38.1 billion” from text."
    }, {
      "heading" : "2.3 Quality Control",
      "text" : "To ensure the quality of annotation in TAT-QA, we apply strict quality control procedures. Competent Annotators To build TAT-QA, financial domain knowledge is necessary. Hence, we employ about 30 university students majored in finance or similar disciplines as annotators. We give all candidate annotators a minor test and only those with 95% correct rate are hired. Before starting the annotation work, we give a training session to the annotators to help them fully understand our annotation requirements and also learn the usage of our annotation system. Two-round Validation For each annotation, we ask two different verifiers to perform a two-round validation after it is submitted, including checking and approval, to ensure its quality. We have five verifiers in total, including two annotators who have good performance on this project and three graduate students with financial background. In the checking phase, a verifier checks the submitted annotation and asks the annotator to fix it if any mistake or problem is found. In the approval phase, a different verifier inspects the annotation again\nthat has been confirmed by the first verifier, and then approves it if no problem is found."
    }, {
      "heading" : "2.4 Dataset Analysis",
      "text" : "Averagely, an annotator can label two hybrid contexts per hour; the whole annotation work lasts about three months. Finally, we attain a total of 2, 757 hybrid contexts and 16, 552 corresponding question-answer pairs from 182 financial reports. The hybrid contexts are randomly split into training set (80%), development set (10%) and test set (10%); hence all questions about a particular hybrid context belong to only one of the splits. We show the basic statistics of each split in Table 1, and the question distribution regarding answer source and answer type in Table 2. In Figure 1, we give an example from TAT-QA, demonstrating the various reasoning types and percentage of each reasoning type over the whole dataset."
    }, {
      "heading" : "3 TAGOP Model",
      "text" : "We introduce a novel QA model, named TAGOP, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020). This step is analogy to slot filling or schema linking, whose effectiveness has been demonstrated in dialogue systems (Lei et al., 2018; Jin et al., 2018) and semantic parsing (Lei et al., 2020). And then TAGOP performs symbolic reasoning over them with a set of aggregation OPerators to arrive at the final answer. The overall architecture is illustrated in Figure 2."
    }, {
      "heading" : "3.1 Sequence Tagging",
      "text" : "Given a question, TAGOP first extracts supporting evidences from its hybrid context (i.e. the table and associated paragraphs) via sequence tagging with the Inside–Outside tagging (IO) approach (Ramshaw and Marcus, 1995). In particular, it assigns each token either I or O label and takes\nthose tagged with I as the supporting evidences for producing the answer. The given question, flattened table by row (Herzig et al., 2020) and associated paragraphs are input sequentially to a transformerbased encoder like RoBERTa (Liu et al., 2019), as shown in the bottom part of Figure 2, to obtain corresponding representations. Each sub-token is tagged independently, and the corresponding cell in the table or word in the paragraph would be regarded as positive if any of its sub-tokens is tagged with I. For the paragraphs, the continuous words that are predicted as positive are combined as a span. During testing, all positive cells and spans are taken as the supporting evidences. Formally, for each sub-token t in the paragraph, the probability of the tag is computed as\np tag t = softmax(FFN(ht)) (1)\nwhere FFN is a two-layer feed-forward network with GELU (Hendrycks and Gimpel, 2016) activation and ht is the representation of sub-token t."
    }, {
      "heading" : "3.2 Aggregation Operator",
      "text" : "Next, we perform symbolic reasoning over obtained evidences to infer the final answer, for which we apply an aggregation operator. In our TAGOP, there are ten types of aggregation operators. For each input question, an operator classifier is applied to decide which operator the evidences would go through; for some operators sensitive to the order of input numbers, an auxiliary number order classifier is used. The aggregation operators are explained as below, covering most reasoning types as listed in Figure 1.\n• Span-in-text: To select the span with the highest probability from predicted candidate spans. The probability of a span is the highest probability of all its sub-tokens tagged I. • Cell-in-table: To select the cell with the highest\nprobability from predicted candidate cells. The probability of a cell is the highest probability of all its sub-tokens tagged I.\n• Spans: To select all the predicted cell and span candidates; • Sum: To sum all predicted cells and spans purely\nconsisting of numbers; • Count: To count all predicted cells and spans; • Average: To average over all the predicted cells\nand spans purely consisting of numbers; • Multiplication: To multiply all predicted cells\nand spans purely consisting of numbers; • Division: To first rank all the predicted cells\nand spans purely consisting of numbers based on their probabilities, and then apply division calculation to top-two; • Difference: To first rank all predicted numerical\ncells and spans based on their probabilities, and then apply subtraction calculation to top-two. • Change ratio: For the top-two values after rank-\ning all predicted numerical cells and spans based on their probabilities, compute the change ratio of the first value compared to the second one.\nOperator Classifier To predict the right aggregation operator, a multi-class classifier is developed. In particular, we take the vector of [CLS] as input to compute the probability:\npop = softmax(FFN([CLS]) (2)\nwhere FFN denotes a two-layer feed-forward network with the GELU activation. Number Order Classifier For operators of Difference, Division and Change ratio, the order of the input two numbers matters in the final result. Hence we additionally append a number order classifier\nafter them, formulated as\nporder = softmax(FFN(avg(ht1, ht2)) (3)\nwhere FFN denotes a two-layer feed-forward network with the GELU activation, ht1, ht2 are representations of the top two tokens according to probability, and “avg” means average. For a token, its probability is the highest probability of all its sub-tokens tagged I, and its representation is the average over those of its sub-tokens."
    }, {
      "heading" : "3.3 Scale Prediction",
      "text" : "Till now we have attained the string or numerical value to be contained in the final answer. However, a right prediction of a numerical answer should not only include the right number but also the correct scale. This is a unique challenge over TATQA and very pervasive in the context of finance. We develop a multi-class classifier to predict the scale. Generally, the scale in TAT-QA may be None, Thousand, Million, Billion, and Percent. Taking as input the concatenated representation of [CLS], the table and paragraphs sequentially, the multi-class classifier computes the probability of the scale as\npscale = softmax(FFN([[CLS];htab;hp]) (4)\nwhere htab and hp are the representations of the table and the paragraphs respectively, which are obtained by applying an average pooling over the representations of their corresponding tokens,“;” denotes concatenation, and FFN denotes a two-layer feed-forward network with the GELU activation.\nAfter obtaining the scale, the numerical or string prediction is multiplied or concatenated with the\ncorresponding scale as the final prediction to compare with the ground-truth answer respectively."
    }, {
      "heading" : "3.4 Training",
      "text" : "To optimize TAGOP, the overall loss is the sum of the loss of the above four classification tasks:\nL = NLL(log(Ptag),Gtag) + NLL(log(Pop),Gop) +\nNLL(log(Pscale),Gscale) +\nNLL(log(Porder),Gorder)\n(5)\nwhere NLL(·) is the negative log-likelihood loss, Gtag and Gop come from the supporting evidences which are extracted from the annotated answer and derivation. We locate the evidence in the table first if it is among the answer sources, and otherwise in its associated paragraphs. Note we only keep the first found if an evidence appears multiple times in the hybrid context. Gscale uses the annotated scale of the answer; Gorder is needed when the groundtruth operator is one of Difference, Division and Change ratio, which is obtained by mapping the two operands extracted from their corresponding ground-truth deviation in the input sequence. If their order is the same as that in the input sequence, Gorder = 0; otherwise it is 1."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "4.1 Baselines",
      "text" : "Textual QA Models We adopt two reading comprehension (RC) models as baselines over textual data: BERT-RC (Devlin et al., 2018), which is a SQuAD-style RC model; and NumNet+ V2 4 (Ran et al., 2019), which achieves promising performance on DROP that requires numerical reasoning over textual data. We adapt them to our TAT-QA as follows. We convert the table to a sequence by row, also as input to the models, followed by tokens from the paragraphs. Besides, we add a multi-class classifier, exactly as in our TAGOP, to enable the two models to predict the scale based on Eq. (4). Tabular QA Model We employ TaPas for WikiTableQuestion (WTQ) (Herzig et al., 2020) as a baseline over tabular data. TaPas is pretrained over large-scale tables and associated text from Wikipedia jointly for table parsing. To train it, we heuristically locate the evidence in the table with the annotated answer or derivation, which is the\n4https://github.com/llamazing/numnet plus\nfirst matched one if a same value appears multiple times. In addition, we remove the “numerical rank id” feature in its embedding layer, which ranks all values per numerical column in the table but does not make sense in TAT-QA. Similar to above textual QA setting, we add an additional multi-class classifier to predict the scale as in Eq. (4). Hybrid QA Model We adopt HyBrider (Chen et al., 2020b) as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia. We use the code released in the original paper5, but adapt it to TAT-QA. Concretely, each cell in the table of TAT-QA is regarded as “linked” with associated paragraphs of this table, like hyperlinks in the original paper, and we only use its cell matching mechanism to link the question with the table cells in its linking stage. The selected cells and paragraphs are fed into the RC model in the last stage to infer the answer. For ease of training on TAT-QA, we also omit the prediction of the scale, i.e. we regard the predicted scale by this model as always correct."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "We adopt the popular Exact Match (EM) and numeracy-focused F1 score (Dua et al., 2019) to measure model performance on TAT-QA. However, the original implementation of both metrics is insensitive to whether a value is positive or negative in the answer as the minus is omitted in evaluation. Since this issue is crucial for correctly interpreting numerical values, especially in the finance domain, we keep the plus-minus of a value when calculating them. In addition, the numeracy-focused F1 score is set to 0 unless the predicted number multiplied by predicted scale equals exactly the ground truth."
    }, {
      "heading" : "4.3 Results and Analysis",
      "text" : "In the following, we report our experimental results on dev and test sets of TAT-QA. Comparison with Baselines We first compare our TAGOP with three types of previous QA models as described in Section 4.1. The results are summarized in Table 3. It can be seen that our model is always superior to other baselines in terms of both metrics, with very large margins over the second best, namely 50.1/58.0 vs. 37.0/46.9 in EM/F1 on test set of TAT-QA respectively. This well reveals the effectiveness of our method that reasons over both tabular and textual data involving lots\n5https://github.com/wenhuchen/HybridQA\nof numerical contents. For two textual QA baselines, NumNet+ V2 performs better than BERT-RC, which is possibly attributed to the stronger capability of numerical reasoning of the latter, but it is still worse than our method. The tabular QA baseline Tapas for WTQ is trained with only tabular data in TAT-QA, showing very limited capability to process hybrid data, as can be seen from its performance. The HyBrider is the worst among all baseline models, because it is designed for HybridQA (Chen et al., 2020b) which does not focus on the comprehensive interdependence of table and paragraphs, nor numerical reasoning.\nHowever, all the models perform significantly worse than human performance6, indicating TATQA is challenging to current QA models and more efforts on hybrid QA are demanded. Answer Type and Source Analysis Furthermore, we analyze detailed performance of TAGOP w.r.t answer type and source in Table 4. It can be seen that TAGOP performs better on the questions whose answers rely on the tables compared to those from the text. This is probably because table cells have clearer boundaries than text spans to the model, thus it is relatively easy for the model to extract supporting evidences from the tables leveraging sequence tagging techniques. In addition, TAGOP performs relatively worse on arithmetic questions compared with other types. This may be because the calculations for arithmetic questions are diverse and harder than other types, indicating the challenge of TAT-QA, especially for the requirement of numerical reasoning. Results of TAGOP with Different Operators We here investigate the contributions of the ten aggregation operators to the final performance of TAGOP. As shown in Table 5, we devise nine variants of the full model of TAGOP; based on the variant of TAGOP with only one operator (e.g. Span-in-text), for each of other variants, we add one more operator back. As can be seen from the table, all added operators can benefit the model performance. Furthermore, we find that some operators like Spanin-text, Cell-in-table, Difference and Average make\n6The human performance is evaluated by asking annotators to answer 50 randomly sampled hybrid contexts (containing 301 questions) from our test set. Note the human performance is still not 100% correct because our questions require relatively heavy cognitive load like tedious numerical calculations. Comparing human performance of F1 in SQUAD (Rajpurkar et al., 2016) (86.8%) and DROP (Dua et al., 2019)) (96.4%), the score (90.8%) in our dataset already indicates a good quality and annotation consistency in our dataset.\nmore contributions than others. In comparison, Sum and Multiplication bring little gain or even decline. After analysis, we find this is because the instances of Sum or Multiplication are minor in our test set, which are easily influenced by randomness. Error Analysis We further investigate our TAGOP by analysing error cases. We randomly sample 100 error instances from the test set, and classify them into five categories as shown in Table 6, each with an example: (1) Wrong Evidence (55%), meaning the model obtained wrong supporting evidence from the hybrid context; (2) Missing\nEvidence (29%), meaning the model failed to extract the supporting evidence for the answer; (3) Wrong Calculation (9%), meaning the model failed to compute the answer with the correct supporting evidence; (4) Unsupported Calculation (4%), meaning the ten operators defined cannot support this calculation; (5) Scale Error (3%), meaning the model failed to predict the scale of the numerical value in an answer.\nWe can then observe about 84% error is caused by the failure to extract the supporting evidence from the table and paragraphs given a question. This demonstrates more efforts are needed to strengthen the model’s capability of precisely aggregating information from hybrid contexts.\nAfter instance-level analysis, we find another interesting error resource is the dependence on domain knowledge. While we encourage annotators to create questions answerable by humans without much finance knowledge, we still find domain knowledge is required for some questions. For example, given the question “What is the gross profit margin of the company in 2015?”, the model needs to extract the gross profit and revenue from the hybrid context and compute the answer according to the finance formula (“gross profit margin = gross profit / revenue”). How to integrate such finance knowledge into QA models to answer questions in TAT-QA still needs further exploration."
    }, {
      "heading" : "5 Related Work",
      "text" : "QA Datasets Currently, there are many datasets for QA tasks, focusing on text, or KB/table. Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), etc. Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.g. multihop reasoning (Yang et al., 2018; Welbl et al., 2018). DROP (Dua et al., 2019) is built to develop numerical reasoning capability of QA models, which in this sense is similar to TAT-QA, but only focuses on textual data. KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al., 2017; Yu et al., 2018). Comparably, QA over hybrid data receives limited efforts, focusing on mixture of KB/tables and text. HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks. Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering (Dua et al., 2019; Ran et al., 2019; Andor et al., 2019; Chen et al., 2020a; Pasupat and Liang, 2015; Herzig et al., 2020; Yin et al., 2020; Zhang and Balog, 2020) and arithmetic word problems (Kushman et al., 2014; Mitra and Baral, 2016; Huang et al., 2017; Ling et al., 2017). To our best knowledge, no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a new challenging QA dataset TATQA, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. To answer questions in TAT-QA, the close relation between table and paragraphs and numerical reasoning are required. We also propose a baseline model TAGOP based on TAT-QA, aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer. Experiments show TATQA dataset is very challenging and more effort is demanded for tackling QA tasks over hybrid data. We expect our TAT-QA dataset and TAGOP model would serve as a benchmark and baseline respectively to help build more advanced QA models,\nfacilitating the development of QA technologies to address more complex and realistic hybrid data, especially those requiring numerical reasoning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors gratefully acknowledge Zhuyun Dai for giving valuable suggestions on this study, Xinnan Zhang for developing the data annotation tool, and Tong Ye and Ming Wei Chan for their work on checking the annotation quality. Our thanks also go to all the anonymous reviewers for their positive feedback. This research is supported by the NExT Research Centre, Singapore."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Table Analysis\nTo maintain the semi-structured nature of financial tables, we almost keep the same table structure in TAT-QA as that in the original financial reports. We sample 100 hybrid contexts from the training set to conduct a manual evaluation to assess the complexity of the table structures. Specifically, we analyze the distribution w.r.t. the number of row headers, as shown in Table 7. It can be seen that around 79% of the tables have two or more rowheaders, indicating large difficulty in interpreting financial tables. In addition, we have also found that all sampled tables all have one column header.\nA.2 Operator Classifier\nWe present the proportion of questions that should go through each aggregation operator (ground truth), as well as the performance of our operator classifier on dev and test set in Table 8.\nA.3 Scale Prediction We report the proportion of the ground truth scale in an answer and also the performance of our scale predictor on dev and test set in Table 9."
    } ],
    "references" : [ {
      "title" : "Giving BERT a calculator: Finding operations and arguments with reading comprehension",
      "author" : [ "Daniel Andor", "Luheng He", "Kenton Lee", "Emily Pitler." ],
      "venue" : "EMNLP-IJCNLP, pages 5947–5952. ACL.",
      "citeRegEx" : "Andor et al\\.,? 2019",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic parsing on Freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Wash-",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Question directed graph attention network for numerical reasoning over text",
      "author" : [ "Kunlong Chen", "Weidi Xu", "Xingyi Cheng", "Zou Xiaochuan", "Yuyu Zhang", "Le Song", "Taifeng Wang", "Yuan Qi", "Wei Chu." ],
      "venue" : "EMNLP-IJCNLP, pages 6759–6768.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Open question answering over tables and text",
      "author" : [ "Wenhu Chen", "Ming-Wei Chang", "Eva Schlinger", "William Yang Wang", "William W. Cohen." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "HybridQA: A dataset of multi-hop question answering over tabular and textual data",
      "author" : [ "Wenhu Chen", "Hanwen Zha", "Zhiyu Chen", "Wenhan Xiong", "Hong Wang", "William Yang Wang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP,",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Question answering on knowledge bases and text using universal schema and memory networks",
      "author" : [ "Rajarshi Das", "Manzil Zaheer", "Siva Reddy", "Andrew McCallum." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "CoRR, abs/1606.08415.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2016",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Process-",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "TaPas: Weakly supervised table parsing via pre-training",
      "author" : [ "Jonathan Herzig", "Pawel Krzysztof Nowak", "Thomas Müller", "Francesco Piccinno", "Julian Eisenschlos." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Herzig et al\\.,? 2020",
      "shortCiteRegEx" : "Herzig et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning fine-grained expressions to solve math word problems",
      "author" : [ "Danqing Huang", "Shuming Shi", "Chin-Yew Lin", "Jian Yin." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 805–814. ACL.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Explicit state tracking with semisupervisionfor neural dialogue generation",
      "author" : [ "Xisen Jin", "Wenqiang Lei", "Zhaochun Ren", "Hongshen Chen", "Shangsong Liang", "Yihong Zhao", "Dawei Yin." ],
      "venue" : "Proceedings of the 27th ACM International Conference",
      "citeRegEx" : "Jin et al\\.,? 2018",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 271–281. ACL.",
      "citeRegEx" : "Kushman et al\\.,? 2014",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Re-examining the role of schema linking in textto-sql",
      "author" : [ "Wenqiang Lei", "Weixin Wang", "Zhixin Ma", "Tian Gan", "Wei Lu", "Min-Yen Kan", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Molweni: A challenge multiparty dialogues-based machine reading comprehension dataset with discourse structure",
      "author" : [ "Jiaqi Li", "Ming Liu", "Min-Yen Kan", "Zihao Zheng", "Zekun Wang", "Wenqiang Lei", "Ting Liu", "Bing Qin." ],
      "venue" : "CoRR, abs/2004.05080.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Tablebank: A benchmark dataset for table detection and recognition",
      "author" : [ "Minghao Li", "Lei Cui", "Shaohan Huang", "Furu Wei", "Ming Zhou", "Zhoujun Li" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Dataset and neural recurrent sequence labeling model for opendomain factoid question answering",
      "author" : [ "Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "author" : [ "Wang Ling", "Dani Yogatama", "Chris Dyer", "Phil Blunsom." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ling et al\\.,? 2017",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to use formulas to solve simple arithmetic problems",
      "author" : [ "Arindam Mitra", "Chitta Baral." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2144– 2153. ACL.",
      "citeRegEx" : "Mitra and Baral.,? 2016",
      "shortCiteRegEx" : "Mitra and Baral.",
      "year" : 2016
    }, {
      "title" : "Largescale question tagging via joint question-topic embedding learning",
      "author" : [ "Liqiang Nie", "Yongqi Li", "Fuli Feng", "Xuemeng Song", "Meng Wang", "Yinglong Wang." ],
      "venue" : "ACM Transactions on Information Systems (TOIS), 38(2):1–23.",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
      "citeRegEx" : "Pasupat and Liang.,? 2015",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLPIJCNLP, pages 2383–2392. ACL.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Text chunking using transformation-based learning",
      "author" : [ "Lance Ramshaw", "Mitch Marcus." ],
      "venue" : "Third Workshop on Very Large Corpora.",
      "citeRegEx" : "Ramshaw and Marcus.,? 1995",
      "shortCiteRegEx" : "Ramshaw and Marcus.",
      "year" : 1995
    }, {
      "title" : "NumNet: Machine reading comprehension with numerical reasoning",
      "author" : [ "Qiu Ran", "Yankai Lin", "Peng Li", "Jie Zhou", "Zhiyuan Liu." ],
      "venue" : "EMNLP-IJCNLP, pages 2474–2484.",
      "citeRegEx" : "Ran et al\\.,? 2019",
      "shortCiteRegEx" : "Ran et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple and effective model for answering multi-span questions",
      "author" : [ "Elad Segal", "Avia Efrat", "Mor Shoham", "Amir Globerson", "Jonathan Berant." ],
      "venue" : "EMNLP-IJCNLP, pages 3074–3080. ACL.",
      "citeRegEx" : "Segal et al\\.,? 2020",
      "shortCiteRegEx" : "Segal et al\\.",
      "year" : 2020
    }, {
      "title" : "PullNet: Open domain question answering with iterative retrieval on knowledge bases and text",
      "author" : [ "Haitian Sun", "Tania Bedrax-Weiss", "William Cohen." ],
      "venue" : "EMNLP-IJCNLP, pages 2380–2390. ACL.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Table cell search for question answering",
      "author" : [ "Huan Sun", "Hao Ma", "Xiaodong He", "Wen-tau Yih", "Yu Su", "Xifeng Yan." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page 771–782. International World Wide Web Con-",
      "citeRegEx" : "Sun et al\\.,? 2016",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, pages 287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empiri-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Yih et al\\.,? 2015",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2015
    }, {
      "title" : "TaBERT: Pretraining for joint understanding of textual and tabular data",
      "author" : [ "Pengcheng Yin", "Graham Neubig", "Wen-tau Yih", "Sebastian Riedel." ],
      "venue" : "ACL, pages 8413–8426. ACL.",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
      "author" : [ "Tao Yu", "Rui Zhang", "Kai Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Li", "Qingning Yao", "Shanelle Roman" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Autocompletion for data cells in relational tables",
      "author" : [ "Shuo Zhang", "Krisztian Balog." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM ’19, pages 761–770.",
      "citeRegEx" : "Zhang and Balog.,? 2019",
      "shortCiteRegEx" : "Zhang and Balog.",
      "year" : 2019
    }, {
      "title" : "Web table extraction, retrieval, and augmentation: A survey",
      "author" : [ "Shuo Zhang", "Krisztian Balog." ],
      "venue" : "ACM Trans. Intell. Syst. Technol., 11(2):13:1–13:35.",
      "citeRegEx" : "Zhang and Balog.,? 2020",
      "shortCiteRegEx" : "Zhang and Balog.",
      "year" : 2020
    }, {
      "title" : "Summarizing and exploring tabular data in conversational search",
      "author" : [ "Shuo Zhang", "Zhuyun Dai", "Krisztian Balog", "Jamie Callan." ],
      "venue" : "SIGIR ’20, pages 1537–1540.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1709.00103.",
      "citeRegEx" : "Zhong et al\\.,? 2017",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "Retrieving and reading: A comprehensive survey on open-domain question answering",
      "author" : [ "Fengbin Zhu", "Wenqiang Lei", "Chao Wang", "Jianming Zheng", "Soujanya Poria", "Tat-Seng Chua." ],
      "venue" : "CoRR, abs/2101.00774.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al.",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al.",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 31,
      "context" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al.",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al.",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "Existing QA systems largely focus on only unstructured text (Hermann et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019; Yang et al., 2018; Li et al., 2020; Nie et al., 2020), structured knowledge base (KB) (Berant et al.",
      "startOffset" : 60,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : ", 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 32,
      "context" : ", 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : ", 2020), structured knowledge base (KB) (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018), or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Though receiving growing interests (Das et al., 2017; Sun et al., 2019; Chen et al., 2020b, 2021), works on hybrid data comprising of unstructured text and structured or semi-structured KB/tables are rare.",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "Though receiving growing interests (Das et al., 2017; Sun et al., 2019; Chen et al., 2020b, 2021), works on hybrid data comprising of unstructured text and structured or semi-structured KB/tables are rare.",
      "startOffset" : 35,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "We adopt the table detection model in (Li et al., 2019) to detect tables in these reports, and apply Apache PDFBox2 library to extract the table contents to be processed with our annotation tool.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 23,
      "context" : "In addition, we encourage them to create questions that can be answered by people without much finance knowledge and use common words instead of the same words appeared in the hybrid context (Rajpurkar et al., 2016).",
      "startOffset" : 191,
      "endOffset" : 215
    }, {
      "referenceID" : 17,
      "context" : "We introduce a novel QA model, named TAGOP, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 224
    }, {
      "referenceID" : 28,
      "context" : "We introduce a novel QA model, named TAGOP, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 224
    }, {
      "referenceID" : 26,
      "context" : "We introduce a novel QA model, named TAGOP, which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by (Li et al., 2016; Sun et al., 2016; Segal et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : "This step is analogy to slot filling or schema linking, whose effectiveness has been demonstrated in dialogue systems (Lei et al., 2018; Jin et al., 2018) and semantic parsing (Lei et al.",
      "startOffset" : 118,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "This step is analogy to slot filling or schema linking, whose effectiveness has been demonstrated in dialogue systems (Lei et al., 2018; Jin et al., 2018) and semantic parsing (Lei et al.",
      "startOffset" : 118,
      "endOffset" : 154
    }, {
      "referenceID" : 24,
      "context" : "the table and associated paragraphs) via sequence tagging with the Inside–Outside tagging (IO) approach (Ramshaw and Marcus, 1995).",
      "startOffset" : 104,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "The given question, flattened table by row (Herzig et al., 2020) and associated paragraphs are input sequentially to a transformerbased encoder like RoBERTa (Liu et al.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : ", 2020) and associated paragraphs are input sequentially to a transformerbased encoder like RoBERTa (Liu et al., 2019), as shown in the bottom part of Figure 2, to obtain corresponding representations.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "where FFN is a two-layer feed-forward network with GELU (Hendrycks and Gimpel, 2016) activation and ht is the representation of sub-token t.",
      "startOffset" : 56,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "Textual QA Models We adopt two reading comprehension (RC) models as baselines over textual data: BERT-RC (Devlin et al., 2018), which is a SQuAD-style RC model; and NumNet+ V2 4 (Ran et al.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : ", 2018), which is a SQuAD-style RC model; and NumNet+ V2 4 (Ran et al., 2019), which achieves promising performance on DROP that requires numerical reasoning over textual data.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Tabular QA Model We employ TaPas for WikiTableQuestion (WTQ) (Herzig et al., 2020) as a baseline over tabular data.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Hybrid QA Model We adopt HyBrider (Chen et al., 2020b) as our baseline over hybrid data, which tackles tabular and textual data from Wikipedia.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "The HyBrider is the worst among all baseline models, because it is designed for HybridQA (Chen et al., 2020b) which does not focus on the comprehensive interdependence of table and paragraphs, nor numerical reasoning.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "Comparing human performance of F1 in SQUAD (Rajpurkar et al., 2016) (86.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "Textual ones include CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 39,
      "context" : "Recently deep reasoning over textual data has gained increasing attention (Zhu et al., 2021), e.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : "KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "KB/Tabular QA aims to automatically answer questions via wellstructured KB (Berant et al., 2013; Talmor and Berant, 2018; Yih et al., 2015) or semi-structured tables (Pasupat and Liang, 2015; Zhong et al.",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "HybridQA (Chen et al., 2020b) is one existing hybrid dataset for QA tasks, where the context is a table connected with Wiki pages via hyperlinks.",
      "startOffset" : 9,
      "endOffset" : 29
    } ],
    "year" : 2021,
    "abstractText" : "Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and their compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% in F1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind the performance of human expert, i.e. 90.8% in F1. It demonstrates that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid data. Our dataset is publicly available for noncommercial use at https://nextplusplus. github.io/TAT-QA/.",
    "creator" : "LaTeX with hyperref"
  }
}