{
  "name" : "2021.acl-long.226.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lightweight Cross-Lingual Sentence Representation Learning",
    "authors" : [ "Zhuoyuan Mao", "Prakhar Gupta", "Chenhui Chu", "Martin Jaggi", "Sadao Kurohashi" ],
    "emails" : [ "kuro}@nlp.ist.i.kyoto-u.ac.jp", "martin.jaggi}@epfl.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2902–2913\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2902"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-lingual sentence representation models (Schwenk and Douze, 2017; España-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li\n1https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec\nand Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages.\nThe above-mentioned models can be categorized into two classes. On one hand, global fine-tuning methods like mBERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices.\nIn this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our model uses significantly less number of parameters compared to both global fine-tuning methods like mBERT, and fixeddimensional representation methods like LASER,\nand T-LASER (Li and Mak, 2020) (see Table 1). Given a fixed training-set and model architecture, the robustness of the sentence representation is dependent on the training task. It is much more difficult for a lightweight model to learn robust representations merely with existing generative tasks (see Section 2 and Section 4.5), which could be attributed to its smaller size. In order to ameliorate this problem, we redesign a crosslingual language model by combining the singleword masked language model (SMLM) with crosslingual token-level reconstruction (XTR). Furthermore, we introduce two contrastive learning methods as auxiliary tasks to compensate for the learning bottleneck of lightweight transformer for generative tasks. Following the state-of-the-art fixeddimensional model LASER, we proceed to learn cross-lingual sentence representations from parallel sentences, where we employ 2-layer dualtransformer encoders to shrink the model architecture. By introducing the above-stated training tasks, we establish a computationally-lite framework for training cross-lingual sentence models.\nWe evaluate the learned sentence representations on cross-lingual tasks including multilingual document classification (MLDoc) (Schwenk and Li, 2018) and XSR. Our results confirm the ability of our lightweight model to yield robust sentence representations. We also do a systematic study on the performance of our model in an ablative manner. The contributions of this work can be summarized as follows:\n• We implement fixed-dimensional crosslingual sentence representation learning in a lightweight model, achieving improved training efficiency and competitive performance of the learned sentence representations.\n• Our proposed novel generative and contrastive tasks allow cross-lingual sentence representa-\ntion efficiently trainable by the lightweight model. The contribution from each task is empirically analyzed."
    }, {
      "heading" : "2 Related Work",
      "text" : "A majority of training tasks for learning fixeddimensional cross-lingual sentence representations can be ascribed to one of the following 2 categories: generative or contrastive. In this section, we revisit the previous work in these 2 categories, which is crucial for designing a cross-lingual representation model. Generative Tasks. Generative tasks measure a generative probability between predicted tokens and real tokens by training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; España-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b) benefits from a massively multilingual machine translation task covering 93 languages. In our work, we revisit the BERT-style training tasks and introduce a novel\ngenerative loss enhanced by KL-Divergence based token distribution prediction. Our proposed generative task performs effectively for the lightweight dual-transformer framework while other generative tasks should be implemented via a large-capacity model. Contrastive Tasks. Contrastive tasks measure (contrast) the similarities of sample pairs in the representation space. Negative sampling, which is a typical feature of the contrastive methods is first introduced in the work of word representation learning (Mikolov et al., 2013). Subsequently, contrastive tasks gradually emerged in many NLP tasks in various ways: negative sampling in knowledge graph embedding learning (Bordes et al., 2013; Wang et al., 2014), next sentence prediction in BERT (Devlin et al., 2019), token-level discrimination in ELECTRA (Clark et al., 2020), sentence-level discrimination in DeCLUTR (Giorgi et al., 2020), and hierarchical contrastive learning in HICTL (Wei et al., 2020). For the cross-lingual sentence representation training, typical ones include using correct and wrong translation pairs introduced by Guo et al. (2018); Yang et al. (2019); Chidambaram et al. (2019); Feng et al. (2020) or utilizing similarities between sentence pairs by introducing a regularization term (Yu et al., 2018). As another advantage, contrastive methods have proven to be more efficient than generative methods (Clark et al., 2020). Inspired by previous work, for our lightweight model, we propose a robust sentence-level contrastive task by leveraging similarity relationships arising from translation pairs."
    }, {
      "heading" : "3 Methodology",
      "text" : "We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework. Concerning the training tasks, we propose a novel cross-lingual language model, which combines SMLM and XTR. Moreover, we introduce two sentence-level self-supervised learning tasks (sentence alignment and sentence similarity losses) to leverage robust parallel level supervision to better conduct the cross-lingual sentence representation space alignment."
    }, {
      "heading" : "3.1 Architecture",
      "text" : "We employ the dual transformer sharing parameters without any decoder as the basic unit to encode parallel sentences respectively, to avoid the loss in efficiency caused by the presence of a decoder.\nUnlike XLM (Conneau and Lample, 2019), we utilize a dual model architecture rather than a single transformer to encode sentence pairs, because it can force the encoder to capture more cross-lingual characteristics (Reimers and Gurevych, 2019; Feng et al., 2020). Moreover, we decrease the number of layers and embedding dimension to accelerate the training phase, as shown in Table 1.\nThe architecture of the proposed method is illustrated in Figure 1 (left). We build sentence representations on the top of 2-layer transformer (Vaswani et al., 2017) encoders by a mean-pooling operation from the final states of all the positions within a sentence. Pre-trained sentence representations for downstream tasks are denoted by u and v, which are used to compute the loss for the sentencelevel contrastive task. Moreover, we add a fullyconnected layer before computing the loss of the cross-lingual language model inspired by Chen et al. (2020). This linear layer can enhance our lightweight model by a nontrivial margin, because the hidden state for computing loss for the generative task is far different from the sentence presentation we aim to train. Two transformer encoders and linear layers share parameters, which has been proved effective and necessary for cross-lingual representation learning (Conneau et al., 2020b)."
    }, {
      "heading" : "3.2 Generative Task",
      "text" : "SMLM. SMLM is proposed by Sabet et al. (2019), which is a variant of the standard MLM in BERT (Devlin et al., 2019). SMLM can enforce the monolingual performance, because the prediction of a number of masked tokens in MLM is too complicated for the shallow transformer encoder to learn.2 Inspired by this, we implement SMLM by a dual transformer architecture. The transformer encoder for language l1 predicts a masked token in a sentence in l1 as the monolingual loss. The language l2 encoder sharing all the parameters with l1 encoder predicts the same masked token by the corresponding sentence (translation in l2) as the cross-lingual loss, as shown in Figure 1 (top right). Specifically, for a parallel corpus C and language l1 and l2, the loss of SMLM computed from l1 encoder El1 and\n2A detailed comparison between SMLM and MLM under our lightweight model setting is conducted (see Section 4.5).\nl2 encoder El2 is formulated as: LSMLM = ∑ S∈C\nl,l′∈{l1,l2} l 6=l′\n{ − log ( P (wt|Sl\\{wt};θ) ) − log (P (wt|Sl′ ;θ))\n} (1)\nwhere wt is the word to be predicted, Sl1\\{wt} is a sentence in which wt is masked, S = (Sl1 , Sl2) denotes a parallel sentence pair, θ represents the parameters to be trained in El1 and El2 , and the classification probability P is computed by Softmax on the top of the embedding layer. XTR. Inspired by LASER, we also use a reconstruction loss. However, introducing a decoder to implement the translation loss like LASER will increase the computational overhead associated with our model, which contradicts with our objective to design a computationally-lite model architecture.\nTo implement the reconstruction loss with just the encoder, we propose a XTR loss by which we jointly enforce the encoder to reconstruct the word distribution of corresponding target sentence as shown by q in Figure 1 (top right). Specifically, we utilize the following KL-Divergence based formulation as the training loss:\nLXMLM = ∑\nS∈C l,l′∈{l1,l2}\nl 6=l′\n{ −DKL ( p (hSl ;θ) ‖ q ( wSl′ )) −DKL ( p ( hSl′ ;θ ) ‖ q (wSl)\n) } (2)\nwhere DKL denotes KL-Divergence based loss, p (hSl ;θ) represents the hidden state on the top of encoderEl as shown in Figure 1 (left) under the input Sl, and wSl indicates the set that contains all the tokens in Sl. We utilize discrete uniform distribution for the tokens in target language to define q for wSl . Specifically, q (wSl) is defined as:\nq (wi) =  Nwi ‖Sl′‖ , wi ∈ Sl′\n0, wi /∈ Sl′ (3)\nwhere Nwi indicates the number of words wi in sentence Sl′ and ‖Sl′‖ indicates the length of Sl′ .3 Unified Generative Task (UGT). Finally, we unify SMLM (Eq. (1)) and XTR (Eq. (2)) by redefining the label distribution q (wSl) for KLDivergence based loss. As shown in Figure 1 (top\n3We set all the Nwi to be 1 in the current implementation. Word frequency will be taken into consideration for the generative task in future work.\nright), the model is forced to learn under the supervision of a biased cross-lingual probability distribution of tokens. It is formulated the same as Eq. (3) if the token wt is masked from Sl′ , else if wt is masked within Sl:\nq (wi) =  Nwi 2 ‖Sl′‖ , wi ∈ Sl′ 1/2, wi = wt\n0, others\n(4)"
    }, {
      "heading" : "3.3 Sentence-Level Contrastive Task",
      "text" : "Meanwhile, as shown in Figure 1 (bottom right), we introduce two auxiliary similarity-based training tasks to strengthen sentence-level supervision. We construct these two assisting tasks on the basis of mean pooled sentence representations, aiming to capture sentence similarity information across languages.\nInspired by Guo et al. (2018); Yang et al. (2019); Feng et al. (2020), we propose a sentence alignment loss. The sentence alignment loss aims to force the transformer model to recognize the sentence pair, where one sentence is the translation of the other. One positive and other negative samples contribute to the gradient update in a single batch, which provides contrastive training patterns for the model training. For contrastively discriminating positive and negative samples, we use (batchsize− 1)× 2 negative samples.4 This indicates all the sentences within a batch except the positive one will be negative samples.\nMore precisely, assuming the mean pooled sentence representations of Sl1 and Sl2 are u(Sl1) and v(Sl2). Assume that Bi is a specific batch of several paired sentences, uij and vij respectively indicate the representation of j-th sentence S(j) = (S\n(j) l1 , S (j) l2\n) in language l1 and l2 within batch Bi. Note that the masked token wt is omitted in the following equations. The above-proposed in-batch sentence alignment loss to align sentence pairs is defined as:\nLalign = − ∑ i ∑ j (log exp (u>ijvij)∑ k exp (u > ijvik)\n+ log exp (u>ijvij)∑ k exp (u > ikvij) )\n(5)\n4For each language, there are batchsize − 1 negative samples. Note that this contrastive task is different from those in Yang et al. (2019) and Feng et al. (2020), where they utilize cosine similarity while we directly use the inner product to accelerate the model.\nwhere S(k), S(j) ∈ Bi. We further introduce a sentence similarity loss to better align similarities for all the sentence pairs throughout a batch. By constructing these similarity-based sentence-level contrastive tasks, we hope that it can force the sentence representations to be competent for sentence-level alignment downstream tasks. Specifically, in-batch sentence similarity loss, Lsim is formulated as:\nLsim = − ∑ i ∑ j log cos\n{ π\n2 ( exp (u>ij1uij2)∑ k exp (u > ij1 uik)\n− exp (v>ij1vij2)∑ k exp (v > ij1 vik) ) } (6)\nwhere S(k), S(j) ∈ Bi.5 In summary, Eq. (5) optimizes a loss for the contrastive task by discriminating correct translation from others for a given sentence, as shown in Figure 1 (Lalign in bottom right). Eq. (6) aligns the cross similarities between every sentence pairs within a batch, as shown in Figure 1 (Lsim in bottom right). The similarity score matrix generated by the inner product between sentence pairs in a batch will be trained to be a symmetrical matrix with diagonal elements approximate to 1 after the Softmax operation."
    }, {
      "heading" : "3.4 Weighted Loss for Generative and Contrastive Tasks",
      "text" : "We jointly minimize the loss of the generative task and two auxiliary contrastive tasks with the weight combination of (1, 2, 2):6\nL(ω0, ω1, ω2) = LXMLM + 2Lalign + 2Lsim (7)\nwhere LXMLM denotes the loss of Eq. (2) and the label distribution for KL-Divergence based loss is the unified reconstruction distribution formulated by Eq. (4). Lalign and Lsim represent the losses in Eq. (5) and Eq. (6), respectively.\n5With regard to Eq. 6, log cos is employed for implementing a regression loss because we focused on the hidden states after Softmax that indicate the probabilities. We will consider using MSE loss on the states before Softmax in future exploration.\n6We assign a bigger weight for contrastive tasks according to the task discrepency between the generative task and contrastive tasks introduced by sentence pair similarities."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate our cross-lingual sentence representation models by cross-lingual document classification and bitext mining for these 2 main downstream tasks belong to 2 groups: unrelated and related to the training task. For the former, we select MLDoc (Schwenk and Li, 2018) to evaluate the classifier transfer ability of the cross-lingual model, while for the latter we conduct sentence retrieval on another parallel dataset Europarl7 to evaluate the performance of our models."
    }, {
      "heading" : "4.1 Configuration Details",
      "text" : "We build our PyTorch implementation on top of HuggingFace’s Transformers library (Wolf et al., 2020). Training data is composed of the ParaCrawl8 (Bañón et al., 2020) v5.0 datasets for each language pair. We experiment on English– French, English–German, English–Spanish and English–Italian. We filter the parallel corpus for each language pair by removing sentences that cover tokens out of 2 languages. Raw and filtered number of the parallel sentences for each pair are shown in Table 2. 10,000 sentences are selected for validation on each language pair. We tokenize sentences by SentencePiece9 (Kudo, 2018) and build a shared vocabulary with the size of 50k for each language pair.\nFor each encoder, we use the transformer architecture with 2 hidden layers, 8 attention heads, hidden size of 512 and filter size of 1,024, and the parameters of two encoders are shared with each other. The sentence representations generated are 512 dimensional. For the training phase, it minimizes the weighted losses for our proposed crosslingual language model jointly with 2 auxiliary tasks. We train 12 epochs for each language pair (30 epochs for English-Italian because of nearly half number of parallel sentences) with the Adam\n7https://www.statmt.org/europarl/ 8http://opus.nlpl.eu/ParaCrawl-v5.php 9https://github.com/google/\nsentencepiece\noptimizer, learning rate of 0.001 with warm-up strategy for 3 epochs (6 epochs for English-Italian) and dropout-probability of 0.1 on a single TITAN X Pascal GPU with the batch size of 128 paired sentences. Training loss for each language pair can converge within 10 GPU (12GB)×days, which is far more efficient than most cross-lingual sentence representation learning methods.10"
    }, {
      "heading" : "4.2 Baselines",
      "text" : "For evaluation on the MLDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019).\nFor the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines.\nNote that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours.\nWe also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations."
    }, {
      "heading" : "4.3 MLDoc: Zero-shot Cross-lingual Document Classification",
      "text" : "The MLDoc task, which consists of news documents given in 8 different languages, is a benchmark to evaluate cross-lingual sentence representations. We conduct our evaluations in a zeroshot scenario: we train and validate a new linear\n10Note that it is impractical to compare the efficiency with LASER, which is trained by 80 V100 GPU×days due to different training data settings. However, it is obvious that our lightweight model is significantly more efficient than the 5-layer LSTM-based encoder-decoder model structure of LASER, because of the parallel computing nature of the transformer encoder (Vaswani et al., 2017) of our model without any decoder.\nclassifier on the top of the pre-trained sentence representations in the source language, and then evaluate the classifier on the test set for the target language. We implement the evaluation by facebook’s MLDoc library.11 As shown in Table 3, our lightweight transformer model obtains the best results for most language pairs compared with previous fixed-dimensional word and sentence representation learning methods. Our methods yield only slightly worse performance even when compared with the state-of-the-art global fine-tuning style method, MultiFit (Eisenschlos et al., 2019), on this task. This is because the entire model will be updated in the fine-tuning phase, which indicates more parameters will be task-specific after fine-tuning. For fixed-dimensional methods, just an\n11https://github.com/facebookresearch/ MLDoc\nadditional dense layer will be trained, which leads to their higher efficiency."
    }, {
      "heading" : "4.4 XSR: Cross-lingual Sentence Retrieval",
      "text" : "We also conduct an evaluation to gauge the quality of our cross-lingual sentence representations on the bitext mining task, which is identical to some components of the training task. Specifically, given 2,000 sentences in the source language, we conduct the corresponding sentence retrieval from 200K sentences in the target language. P@1 scores of our lightweight models and previous bilingual representation methods calculated by Artetxe and Schwenk (2019a) are reported. As shown in Table 4, we observe that our lightweight models outperform the bilingual pooling-based representation learning methods by a significant margin, which reflects the basic ability of the contextualized rep-\nresentations generated by our lightweight models. However, our lightweight models underperform LASER, which can be attributed to our lightweight capacities and bilingual settings. Note that LASER uses significantly larger multilingual training data (see Section 4.2)."
    }, {
      "heading" : "4.5 Analyses",
      "text" : "We perform ablation experiments to confirm the efficiency and the effectiveness of each training task for our models. Analyses for other hyperparameter configurations of batch size, sentence representation dimension, and training corpus size are presented in Appendix A. Relation among Number of Layers, Efficiency, and Performances. We report the efficiency statistics and performances of our proposed methods trained by different layer number settings. As shown in Table 5, we observe a linear increase of memory occupation and training time per 10,000 training steps by increasing the number of transformer encoder layers. Specifically, a 6-layer transformer encoder occupies nearly 2.5 times memory and costs 1.8 times training time compared to our\n2-layer model. Therefore, given the same memory occupation (by adjusting the batch size), theoretically our lightweight model can be implemented over 4 times (≈ 2.5× 1.8) faster than the 6-layer model. Concerning the respective performances on MLDoc and XSR, we see that lightweight model with 2 transformer layers obtains the peak performance on MLdoc, and the performances decrease when we add more layers. This indicates that the 2-layer transformer encoder is an ideal structure for our proposed training tasks on the document classification task. On the other hand, performances on XSR keep increasing gradually with more layers, where the 1-layer model can even yield decent performance on this task.\nOur proposed training tasks perform well from the 2-layer model, while 6 layers are required for standard MLM and 5 LSTM layers are required for LASER. This is why we use 2-layer as the basic unit for our model.\nEffectiveness of Different Generative Tasks. We report the results with different generative tasks in Table 6. We observe that XTR outperforms other generative tasks by a significant margin on both\nMLDoc and XSR downstream tasks. XTR yields further improvements when unified with SMLM, which is introduced as the generative task in our model. This demonstrates the necessity of a welldesigned generative task for the lightweight dualtransformer architecture. Effectiveness of the Contrastive Tasks. In Table 7, we study the contribution of the sentencelevel contrastive tasks. We observe that a higher performance on MLDoc is yielded by the vanilla model while more sentence-level contrastive tasks improve the performance on XSR. This can be attributed to the similar nature between the supervision provided by sentence-level contrastive tasks and XSR process. In other words, contrastive-style tasks have a detrimental effect on the document classification downstream task. In future work, we will explore how to train a balanced sentence representation model with contrastive tasks."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we presented a lightweight dualtransformer based cross-lingual sentence representation learning method. For the fixed 2-layer dualtransformer framework, we explored several generative and contrastive tasks to ensure the sentence representation quality and facilitate the improvement of the training efficiency. In spite of the lightweight model capacity, we reported substantial improvements on MLDoc compared to fixeddimensional representation methods and we obtained comparable results on XSR. In the future, we plan to verify whether our proposed methods can be combined with knowledge distillation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank all the reviewers for their valuable comments and suggestions to improve this paper. This work was partially supported by Grantin-Aid for Young Scientists #19K20343, JSPS."
    }, {
      "heading" : "A Appendices",
      "text" : "Comparisons with Reimers and Gurevych (2020). Reimers and Gurevych (2020) use the knowledge distillation to train multilingual sentence embeddings where pre-trained encoders are utilized to initialize the teacher and student model, which is a kind of the global fine-tuning style methods finetuned by parallel sentences. As the results on MLDoc and XSR shown in Table 8, their multilingual\nrepresentations yield good performance on bitext mining but perform poorly on classification tasks. This demonstrates the importance of exploring taskagnostic multilingual sentence representations like LASER and ours. Batch Size. We investigate the effect of the batch size for contrastive tasks, where different batch sizes indicate the discrepancy of the negative sample numbers. As shown in Table 9, larger batch harms the lightweight model based sentence representation learning and 128 is reported as the best batch size setting for our lightweight model. Furthermore, batch size of 128 allows the training to be assigned on 12GB GPU card while a larger batch size requires more GPU memory. Corpus Size. We show the impact of the size of the parallel corpus on English-French in Table 10. For MLDoc, we observe higher accuracy on larger corpus while for XSR, a small fraction of the large corpus suffices to yield effective results. This indicates\nen-fr.m en-de.m en-es.m en-it.m en-fr.x en-de.x en-es.x en-it.x\nLanguage Pair\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nA cc\nu ra\ncy\n256→ 512→ 256← 512←\nFigure 2: Performance of different representation dimensions on MLDoc (.m) and XSR (.x). Arrows denotes direction of zero-shot setting.\nthat more parallel data improves the performance on MLDoc. Sentence Representation Dimension. In Figure 2, we present the effect of the sentence representation dimension. 512-dimensional sentence representations significantly outperform 256- dimensional ones in our lightweight model. Moreover, representation size of 512 yields better performance without increasing the training time."
    } ],
    "references" : [ {
      "title" : "Marginbased parallel corpus mining with multilingual sentence embeddings",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3197–3203, Florence, Italy. Asso-",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019a",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019b",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "ParaCrawl: Web-scale acquisition of parallel corpora",
      "author" : [ "Sarrı́as", "Marek Strelec", "Brian Thompson", "William Waites", "Dion Wiggins", "Jaume Zaragoza" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Sarrı́as et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sarrı́as et al\\.",
      "year" : 2020
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcı́aDurán", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning cross-lingual sentence representations via a multi-task dual-encoder model",
      "author" : [ "Muthu Chidambaram", "Yinfei Yang", "Daniel Cer", "Steve Yuan", "Yunhsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of the 4th Workshop on Represen-",
      "citeRegEx" : "Chidambaram et al\\.,? 2019",
      "shortCiteRegEx" : "Chidambaram et al\\.",
      "year" : 2019
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Emerging cross-lingual structure in pretrained language models",
      "author" : [ "Alexis Conneau", "Shijie Wu", "Haoran Li", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Trans-gram, fast cross-lingual word-embeddings",
      "author" : [ "Jocelyn Coulmance", "Jean-Marc Marty", "Guillaume Wenzek", "Amine Benhalloum." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1109–",
      "citeRegEx" : "Coulmance et al\\.,? 2015",
      "shortCiteRegEx" : "Coulmance et al\\.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "MultiFiT: Efficient multi-lingual language model fine-tuning",
      "author" : [ "Julian Eisenschlos", "Sebastian Ruder", "Piotr Czapla", "Marcin Kadras", "Sylvain Gugger", "Jeremy Howard." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Eisenschlos et al\\.,? 2019",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical analysis of nmt-derived interlingual embeddings and their use in parallel sentence identification",
      "author" : [ "Cristina España-Bonet", "Ádám Csaba Varga", "Alberto Barrón-Cedeño", "Josef van Genabith." ],
      "venue" : "IEEE J. Sel. Top. Signal Process., 11(8):1340–",
      "citeRegEx" : "España.Bonet et al\\.,? 2017",
      "shortCiteRegEx" : "España.Bonet et al\\.",
      "year" : 2017
    }, {
      "title" : "Languageagnostic BERT sentence embedding",
      "author" : [ "Fangxiaoyu Feng", "Yinfei Yang", "Daniel Cer", "Naveen Arivazhagan", "Wei Wang." ],
      "venue" : "CoRR, abs/2007.01852.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M. Giorgi", "Osvald Nitski", "Gary D. Bader", "Bo Wang." ],
      "venue" : "CoRR, abs/2006.03659.",
      "citeRegEx" : "Giorgi et al\\.,? 2020",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective parallel corpus mining using bilingual sentence embeddings",
      "author" : [ "Mandy Guo", "Qinlan Shen", "Yinfei Yang", "Heming Ge", "Daniel Cer", "Gustavo Hernandez Abrego", "Keith Stevens", "Noah Constant", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "TinyBERT: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning bilingual sentence embeddings via autoencoding and computing similarities with a multilayer perceptron",
      "author" : [ "Yunsu Kim", "Hendrik Rosendahl", "Nick Rossenbach", "Jan Rosendahl", "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75, Mel-",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer based multilingual document embedding model",
      "author" : [ "Wei Li", "Brian Mak." ],
      "venue" : "CoRR, abs/2008.08567.",
      "citeRegEx" : "Li and Mak.,? 2020",
      "shortCiteRegEx" : "Li and Mak.",
      "year" : 2020
    }, {
      "title" : "Bilingual word representations with monolingual quality in mind",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 151–159, Denver, Col-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512–4525,",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "Explicit cross-lingual pre-training for unsupervised machine translation",
      "author" : [ "Shuo Ren", "Yu Wu", "Shujie Liu", "Ming Zhou", "Shuai Ma." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust crosslingual embeddings from parallel sentences",
      "author" : [ "Ali Sabet", "Prakhar Gupta", "Jean-Baptiste Cordonnier", "Robert West", "Martin Jaggi." ],
      "venue" : "CoRR, abs/1912.12481.",
      "citeRegEx" : "Sabet et al\\.,? 2019",
      "shortCiteRegEx" : "Sabet et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "CoRR, abs/1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning joint multilingual sentence representations with neural machine translation",
      "author" : [ "Holger Schwenk", "Matthijs Douze." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 157–167, Vancouver, Canada. Association for",
      "citeRegEx" : "Schwenk and Douze.,? 2017",
      "shortCiteRegEx" : "Schwenk and Douze.",
      "year" : 2017
    }, {
      "title" : "A corpus for multilingual document classification in eight languages",
      "author" : [ "Holger Schwenk", "Xian Li." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Re-",
      "citeRegEx" : "Schwenk and Li.,? 2018",
      "shortCiteRegEx" : "Schwenk and Li.",
      "year" : 2018
    }, {
      "title" : "MobileBERT: a compact task-agnostic BERT for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen." ],
      "venue" : "Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada,",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "On learning universal representations across languages",
      "author" : [ "Xiangpeng Wei", "Yue Hu", "Rongxiang Weng", "Luxi Xing", "Heng Yu", "Weihua Luo." ],
      "venue" : "CoRR, abs/2007.15960.",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving multilingual sentence embedding using bidirectional dual encoder with additive margin",
      "author" : [ "Yinfei Yang", "Gustavo Hernández Ábrego", "Steve Yuan", "Mandy Guo", "Qinlan Shen", "Daniel Cer", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual seq2seq training with similarity loss for cross-lingual document classification",
      "author" : [ "Katherine Yu", "Haoran Li", "Barlas Oguz." ],
      "venue" : "Proceedings of The Third Workshop on Representation Learning for NLP, pages 175–179, Melbourne, Aus-",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Large-scale models for learning fixeddimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks.",
      "startOffset" : 99,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "methods like mBERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and XLM (Conneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "els have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : ", 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : ", 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : ", 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 120
    }, {
      "referenceID" : 31,
      "context" : ", 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "mBERT (Devlin et al., 2019) Transformer 768 3,072 12 12 N/A 110M LASER (Artetxe and Schwenk, 2019b) Bi-LSTM 512×2 N/A N/A 5 5 154M",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : ", 2019) Transformer 768 3,072 12 12 N/A 110M LASER (Artetxe and Schwenk, 2019b) Bi-LSTM 512×2 N/A N/A 5 5 154M",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "T-LASER (Li and Mak, 2020) Transformer 1,024 4,096 16 6 1 246M",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 30,
      "context" : "We evaluate the learned sentence representations on cross-lingual tasks including multilingual document classification (MLDoc) (Schwenk and Li, 2018) and XSR.",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sen-",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : ", 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al.",
      "startOffset" : 62,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : ", 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al.",
      "startOffset" : 62,
      "endOffset" : 111
    }, {
      "referenceID" : 26,
      "context" : ", 2020a) or explicitly predicting the translated token (Ren et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : "The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019).",
      "startOffset" : 166,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "Sequence to sequence methods (Schwenk and Douze, 2017; España-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence.",
      "startOffset" : 29,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "Sequence to sequence methods (Schwenk and Douze, 2017; España-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence.",
      "startOffset" : 29,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Sequence to sequence methods (Schwenk and Douze, 2017; España-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence.",
      "startOffset" : 29,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "Sequence to sequence methods (Schwenk and Douze, 2017; España-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence.",
      "startOffset" : 29,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b) benefits from a massively multilingual machine translation task covering 93 languages.",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Negative sampling, which is a typical feature of the contrastive methods is first introduced in the work of word representation learning (Mikolov et al., 2013).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "edge graph embedding learning (Bordes et al., 2013; Wang et al., 2014), next sentence prediction in BERT (Devlin et al.",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 33,
      "context" : "edge graph embedding learning (Bordes et al., 2013; Wang et al., 2014), next sentence prediction in BERT (Devlin et al.",
      "startOffset" : 30,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : ", 2014), next sentence prediction in BERT (Devlin et al., 2019), token-level discrimination in ELECTRA (Clark et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : ", 2019), token-level discrimination in ELECTRA (Clark et al., 2020), sentence-level discrimination in DeCLUTR (Giorgi et al.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : ", 2020), sentence-level discrimination in DeCLUTR (Giorgi et al., 2020), and hierarchical contrastive learning",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : "utilizing similarities between sentence pairs by introducing a regularization term (Yu et al., 2018).",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "As another advantage, contrastive methods have proven to be more efficient than generative methods (Clark et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Unlike XLM (Conneau and Lample, 2019), we utilize a dual model architecture rather than a single transformer to encode sentence pairs, because it can force the encoder to capture more cross-lingual characteristics (Reimers and Gurevych, 2019; Feng et al.",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "Unlike XLM (Conneau and Lample, 2019), we utilize a dual model architecture rather than a single transformer to encode sentence pairs, because it can force the encoder to capture more cross-lingual characteristics (Reimers and Gurevych, 2019; Feng et al., 2020).",
      "startOffset" : 214,
      "endOffset" : 261
    }, {
      "referenceID" : 14,
      "context" : "Unlike XLM (Conneau and Lample, 2019), we utilize a dual model architecture rather than a single transformer to encode sentence pairs, because it can force the encoder to capture more cross-lingual characteristics (Reimers and Gurevych, 2019; Feng et al., 2020).",
      "startOffset" : 214,
      "endOffset" : 261
    }, {
      "referenceID" : 32,
      "context" : "We build sentence representations on the top of 2-layer transformer (Vaswani et al., 2017) encoders by a mean-pooling operation from the final states of all the positions within a sentence.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "and linear layers share parameters, which has been proved effective and necessary for cross-lingual representation learning (Conneau et al., 2020b).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "(2019), which is a variant of the standard MLM in BERT (Devlin et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "For the former, we select MLDoc (Schwenk and Li, 2018) to evaluate the classifier transfer ability of the cross-lingual model, while for the latter we conduct sentence retrieval on another parallel dataset Europarl7 to evaluate the performance of our models.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "We tokenize sentences by SentencePiece9 (Kudo, 2018) and build a shared vocabulary with the size of 50k for each language pair.",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 30,
      "context" : "For evaluation on the MLDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al.",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 28,
      "context" : "For evaluation on the MLDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al.",
      "startOffset" : 171,
      "endOffset" : 190
    }, {
      "referenceID" : 37,
      "context" : ", 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and",
      "startOffset" : 78,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, Multi-",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : ", 2015) & BiSent2Vec (Sabet et al., 2019), and multilin-",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "gual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : ", 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations.",
      "startOffset" : 43,
      "endOffset" : 71
    }, {
      "referenceID" : 32,
      "context" : "However, it is obvious that our lightweight model is significantly more efficient than the 5-layer LSTM-based encoder-decoder model structure of LASER, because of the parallel computing nature of the transformer encoder (Vaswani et al., 2017) of our model without any decoder.",
      "startOffset" : 220,
      "endOffset" : 242
    }, {
      "referenceID" : 12,
      "context" : "Our methods yield only slightly worse performance even when compared with the state-of-the-art global fine-tuning style method, MultiFit (Eisenschlos et al., 2019), on this task.",
      "startOffset" : 137,
      "endOffset" : 163
    } ],
    "year" : 2021,
    "abstractText" : "Large-scale models for learning fixeddimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1",
    "creator" : "LaTeX with hyperref"
  }
}