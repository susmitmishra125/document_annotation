{
  "name" : "2021.acl-long.563.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Discriminative Reranking for Neural Machine Translation",
    "authors" : [ "Ann Lee", "Michael Auli", "Marc’Aurelio Ranzato" ],
    "emails" : [ "annl@fb.com", "michaelauli@fb.com", "ranzato@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7250–7264\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7250"
    }, {
      "heading" : "1 Introduction",
      "text" : "Reranking models take a number of different output hypotheses generated by a baseline model and select one hypothesis based on more powerful features. Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al., 2004; Shen et al., 2004).\nTraditional statistical models (SMT) based on n-gram counts made very strong independence assumptions where features would only capture very local context information to avoid sparsity and poor generalization. A large n-best list produced by these models would then be passed to a discriminatively trained reranker which leverages features engineered to capture more global context (Och et al., 2004) yielding significant improvements to the quality of the translations.\nOn the other hand, modern neural models (NMT) make much weaker independence assumptions because predictions of standard sequenceto-sequence models depend on the entire source sentence as well as the target prefix generated. However, reranking may still be beneficial for two reasons: First, NMT systems are subject to exposure bias (Ranzato et al., 2016), i.e., models are never exposed to their own generations at training time, while a reranking model has been trained on model outputs. Second, beam search with autoregressive models uses the chain rule to sum individual token-level probabilities to obtain a target sequence probability. However, individual probabilities are based on a limited amount of target context, while a reranking model can condition on the entire target context. Indeed, recent generative reranking approaches applied to NMT, such as Noisy-Channel Decoding (NCD, Yee et al. 2019) which leverages a pre-trained language model and a backward model, show strong improvements over beam search outputs, as demonstrated in recent WMT evaluations (Ng et al., 2019).\nIn this paper, we explore whether training large transformer models using the reranking objective can further improve performance. Our model, dubbed DrNMT, takes as input the entire source sentence and an n-best list of output hypotheses to predict a distribution of sentence-level evaluation scores, such as BLEU.1 This setup is similar to earlier work with SMT, except that the baseline model is an NMT model and the reranker is a big transformer architecture as opposed to a log-linear model on top of discrete or human engineered features.\nUnfortunately, optimizing for the task of interest does not always lead to better performance. Overfitting to the training set is a potential concern, as the\n1Our approach is general and enables optimizing any userspecified metric, or combinations thereof.\nreranker has hundreds of millions of parameters yet it receives only one gradient and weight update per source/target sentence pair as opposed to one per token as for standard NMT models. In our work, we mitigate overfitting in two ways. First, we leverage the success of pre-training by finetuning masked language models (MLM; Devlin et al. 2019) which initializes the model with features trained on much more training data. Second, we augment the original dataset with back-translated data (BT; Sennrich et al. 2016).\nExperiments show that DrNMT can match the performance of a strong NCD baseline and that their combination leads to further improvements as measured by BLEU, TER and also human evaluation."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our method is inspired by the seminal work of Shen et al. (2004) and Och et al. (2004) who introduced and popularized discriminative reranking to SMT. Besides using a weaker MT system to generate the n-best list, these works relied on a linear discriminator trained on human-designed features as opposed to a transformer taking the raw source sentence and hypothesis.\nMost work using NMT has focused on generative reranking methods (Liu et al., 2018; Imamura and Sumita, 2017; Wang et al., 2017), where the reranker’s parameters are optimized using a criterion which is different from the metric of interest. For instance, Yu et al. (2017); Yee et al. (2019) perform noisy-channel decoding where hypotheses are scored by linearly combining the output of the forward model, a target-side language model and a backward model which scores the source sentence given the hypothesis. These methods have shown remarkable improvements over the output of beam decoding, despite not being trained for the reranking task (except for the two or three hyperparameters of the linear combination of scores which are tuned on a validation set). Another approach belonging to this class of methods is the one proposed by Salazar et al. (2019), which employs the scores from a masked language model (MLM). While this method employs a transformer architecture, it is still not trained for the task of interest.\nTo the best of our knowledge, there is only concurrent work by Naskar et al. (2020) which attempts at training discriminatively a reranker for\nNMT. They use a pair-wise margin loss on hypotheses sampled from the NMT, while we learn to rank the full n-best list produced by beam. Their experiments also show that the reranker performs better when directly conditioned on the source sentence. However, they do not compare nor combine their method with NCD like we do. Both their work and our work are however an extension of Deng et al. (2020), who proposed to train a discriminator to improve neural language modeling.\nThere is also a large body of literature on different ways to combine SMT and NMT by using one to rerank the other, since SMT is generally better at adequacy while NMT is better at fluency. For instance, Auli and Gao (2014) uses an RNN discriminator to rerank the n-best list produced by a phrase-based SMT. Instead, Ehara (2017) does the opposite, using an SMT discriminator to rerank an n-best list produced by an NMT.\nFinally, our work is also related to recent attempts at using adversarial training to improve MT (Wu et al., 2018; Zhang et al., 2018). Unlike these approaches our method is much simpler because we do not update the parameters of the MT system generating the hypotheses. Moreover, our discriminator is trained to predict the distribution of desired metric and it is used at decoding time to rerank, while GAN-based MT would only retain the generator."
    }, {
      "heading" : "3 Model",
      "text" : "Given a source sentence x, an NMT model generates a set of hypotheses U(x) = {u1, u2, ..., un} in the target language. The goal of this work is to learn a reranker that produces higher scores for hypotheses of better quality, as defined in terms of a user-specified metric µ(u, r) such as BLEU (Papineni et al., 2002a), where quality is measured with respect to a reference r.\nAs illustrated in Figure 1, our reranker is a transformer architecture which takes as input the concatenation of the source sentence x and hypothesis u ∈ U(x). The architecture includes also position embeddings and language embeddings, to help the model represent tokens that are shared between the two languages (Conneau and Lample, 2019). The final hidden state corresponding to the start of sentence token (〈s〉) serves as the joint representation for (x, u); let us denote this feature vector as z ∈ Rd. The reranker associates a scalar score o ∈ R to (x, u) by applying a one hidden layer\nneural network with d tanh hidden units to z, as default in the design of the “classification head” of RoBERTa (Liu et al., 2019). The parameters of the reranker are denoted by θ and include the parameters of the transformer, all the embeddings and also the top projection block mapping the feature vector to the scalar score. Each hypothesis ui in the set U(x) is therefore processed independently and yields a score oi."
    }, {
      "heading" : "4 Training and Inference",
      "text" : "We train the reranker discriminatively, hence the name DrNMT for Discriminative Reranker for NMT, by minimizing the KL-divergence between the target distribution and the model output distribution, DKL(pT ||pM ) (Cao et al., 2007). For each x, the model output distribution is a softmax over all n hypotheses in the n-best list:\npM (ui|x; θ) = exp(oi(ui|x; θ))∑n j=1 exp(oj(uj |x; θ)) , (1)\nwhere we made explicit that the score oj is conditioned on the input x and parameter vector θ. Notice that we do not enforce any additional factorization. In particular, we do not assume that the score is computed auto-regressively.\nThe target distribution is defined as a normalized distribution of the end metric µ(ui, r) which we assume to improve as it takes on larger values:\npT (ui) = exp(µ(ui, r)/T )∑n j=1 exp(µ(uj , r)/T ) , (2)\nwhere T is the temperature to control the smoothness of the distribution. In practice, we apply a minmax normalization on µ. We subtract each value by the minimum in the hypothesis set, and divide\nthe result by the difference between the maximum and the minimum value, so that the best hypothesis scores 1 and the worst 0. This helps the optimization as it reduces the variance of the gradients, as pointed out by Edunov et al. (2018).\nThe parameters of DrNMT are then learned by minimizing the KL divergence over the training dataset. For a given training example, we have:\nL(θ) = − n∑\nj=1\npT (uj) log pM (uj |x; θ). (3)\nWe minimize this loss over the training set by stochastic gradient descent using standard backpropagation of the error, since all terms are differentiable. In order to alleviate overfitting, we employ dropout regularization (Srivastava et al., 2014), we pre-train the model (Conneau et al., 2019) and we also perform data augmentation by training on back-translated data (BT) (Sennrich et al., 2016). See §5.3 for details.\nAt test time, generation proceeds by first having the NMT generate the n-best list, and then by applying the reranker to select the best hypothesis. Since the score of the forward model is also available, unless otherwise specified we rerank using a weighted combination of both; this is dubbed as DrNMT. In the experiments we also report results by adding all the other scores from NCD, namely the backward model score and the language model score. We denote this variant by ”DrNMT + NCD”. Whenever we combine scores from various models we tune the additional hyper-parameters controlling the weighted combination by random search on the validation set (Yee et al., 2019)."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "In this section we describe the datasets, baselines and model details."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We experiment on four language pairs: GermanEnglish (De-En), English-German (En-De), English-Tamil (En-Ta) and Russian-English (Ru-En). For training on De-En and En-De, we use NewsCommentary from WMT’19 (Barrault et al., 2019) and NewsCrawl2018 for the parallel dataset and target side monolingual data, respectively. We validate on newstest2014 and newstest2015, and test on newstest2016, 2017, 2018 and 2019. For En-Ta, we use all bitext and monolingual data shared by the WMT’20 news translation task for training, and the officially released development and test sets for validation and testing purposes. For Ru-En, we use all the parallel data from WMT’19 (Barrault et al., 2019) and NewsCrawl2018 as the monolingual dataset for training, validate on newstest2015 and 2016, and test on newstest 2017, 2018 and 2019.\nWe follow the steps in Ng et al. (2019) for data preprocessing, including sentence deduplication, language identification filtering on all bitext and monolingual data (Joulin et al., 2017) and indomain filtering (Moore and Lewis, 2010) on Tamil CommonCrawl data. Table 1 shows the resulting size of each dataset. For the base NMT models, we learn 30K byte-pair encoding (BPE) units for De-En and En-De, 20K BPE units for En-Ta and 24K BPE units for Ru-En separately, using the sentencepiece toolkit (Kudo and Richardson, 2018). All systems are evaluated using SACREBLEU (Post, 2018)."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "We use the Transformer (Vaswani et al., 2017) architecture and train MT models using bitext data only. These are the models that generate the n-best list, and which serve also as a lower bound for the performance of DrNMT. BT data is generated from beam decoding with beam size equal to 5. Since the bitext data of En-Ta originates from seven different sources, we prepend dataset tags to each source sentence to indicate the origin (Kobus et al., 2017). We do not prepend any tags on the validation and test sets when decoding, as this choice worked best during cross-validation. In general and for each language pair, we tune the model architecture and\nall hyper-parameters on the validation set. In addition to beam decoding, we consider two reranking baselines. First, we consider the method recently introduced by Salazar et al. (2019). In its simplest formulation, this takes a pre-trained masked language model (MLM) on the target side, and iteratively masks one word of the hypothesis at the time and aggregates the corresponding scores to yield a score for the whole hypothesis. Then, this score is combined with the score of the forward model to rerank the n-best list; this is dubbed as “fw + MLM”. We also have a version of MLM which is tuned on our target side monolingual dataset; we dub this “fw + MLM-ft”.\nFinally, we consider reranking using noisy channel decoding (NCD; Yee et al. 2019). NCD reranks by taking a weighted combination of three scores: the forward model score, the score of a target-side language model (LM), and the score of a backward model. A length penalty is then applied on the combined score. The weights and the length penalty are tuned on the validation set via random search. All LMs are transformers with 16 blocks, 16 attention heads and embedding size 1024. They are trained on the target side monolingual data only.\n5.3 Setting Up DrNMT\nWe use XLM-RBase2 (Conneau et al., 2019), a transformer-based multilingual MLM trained on more than 2.5T of of filtered CommonCrawl data in 100 languages, including En, De, Ta and Ru, as the pre-trained model for DrNMT. The same model is also used in the MLM baseline described in §5.2. The XLM-RBase model consists of 12 transformer blocks, 12 attention heads, embedding size 768 (270M params) and has a vocabulary size of 250K BPE units. As each training sample of XLM-R only contained one single language, we further enhance the model with two language embeddings,\n2https://github.com/pytorch/fairseq/ tree/master/examples/xlmr\ninitialized from random, to indicate the source and target languages for the reranker.\nWe perform beam decoding on both bitext and BT data using the baseline MT models to generate n-best lists with 50 hypotheses. We combine n-best lists from both bitext and BT as training data for the rerankers for De-En, En-De and En-Ta, and use only BT data for Ru-En. We train DrNMT with batch size 512, use Adam (Kingma and Ba, 2015) and early-stop when the validation performance does not improve after 12K parameter updates. All hyper-parameters, including learning rate, number of warmup steps, dropout rate, etc., are tuned on the validation set. All models are implemented and trained using fairseq (Ott et al., 2019)3."
    }, {
      "heading" : "6 Results",
      "text" : "In this section we report the main findings of our work. When optimizing for BLEU as metric, the performance of DrNMT and baselines for De-En, En-De, En-Ta and Ru-En is summarized in Table 2. The findings are similar across the four language directions. We therefore focus the discussion on the De-En test set results.\nFirst, we notice that all methods improve over the beam search output with gains ranging from 1.0 to 4.1 BLEU. However, there may be still room for improvement as the oracle performance suggests. The oracle is computed by selecting the best hypotheses based on BLEU with respect to the human reference. Of course, the oracle may be not achievable because of uncertainty in the translation task.\n3Code for reproducing the results can be found at: https://github.com/pytorch/fairseq/ tree/master/examples/discriminative_ reranking_nmt\nSecond, Salazar et al. (2019)’s method, particularly the version fine-tuned on the in-domain training dataset, improves upon beam by 1.1 BLEU points. However, the improvement over beam is not as large as with NCD, which improves upon beam by 3.2 BLEU points, suggesting that among the non-discriminative reranking methods NCD performs the best.\nThird, DrNMT performs on par (En-Ta, En-De and Ru-En) or better (De-En) than NCD, showing that discriminative reranking can be very competitive. Note, that the reranker requires only one additional forward pass through the hypotheses generated by beam, while NCD requires two forward passes (one for the LM and one for the backward MT model). Therefore, our reranker works at least as well as NCD while requiring roughly half of the compute.\nFourth, the discriminative reranker and NCD are complementary to each other, since combining both achieves the best performance overall across the three language directions, with gains between 0.9 BLEU (De-En) and 0.2 (En-Ta) compared to NCD, and an overall gain between 4.1 BLEU (De-En) and 0.5 (En-Ta) compared to the beam baseline.\nFifth, the gain brought by discriminative reranking can be better appreciated by comparing ”fw + LM” and DrNMT, as the major difference between the two approaches is the objective function used for training them (generative language modeling instead of prediction of the distribution of BLEU scores). We can see that in all cases, discriminative reranking yields better translations, with gains between 0.2 and 2.3 BLEU points depending on the language direction.\nFinally, we notice that En-Ta is a difficult lan-\nguage pair, in which the baseline NMT is weak and none of the reranking approaches work nearly as well as in the other language directions. The difference between validation and test BLEU scores suggests also a certain degree of overfitting to the validation set. Despite this, our reranker still yields the largest improvement over beam. Appendix B shows similar trends when test performance is measured in terms of translation error rate (TER) (Snover et al., 2006), showing that DrNMT is not particularly overfitting to the training metric.\nHuman evaluation: We randomly sample 750 sentences from the De-En test sets and collect human ratings. We perform A/B testing, where a rater can see the source sentence together with translated sentences from two systems. We conduct two rounds of human evaluation by comparing the proposed ”DrNMT + NCD” vs. ”beam”, and ”DrNMT + NCD” vs. ”NCD”. For each sentence, we collect three ratings (between 0 to 100) and average the scores, treating sentences with a score difference less than 5 as equally good. Out of the 750 sentences, our proposed method generates better translation than beam on 149 sentences and is worse on 82 sentences, and it performs better than NCD on 123 sentences and worse on 108 sentences, corroborating the gains observed when measuring with BLEU.\nNext we show that DrNMTworks with other userspecified metrics, study how performance varies with the number of hypotheses and perform several ablation studies to better understand its critical components."
    }, {
      "heading" : "6.1 Optimizing for a Different Metric",
      "text" : "In order to validate the generality of DrNMT, we consider as metric µ the opposite of TER, so that larger values indicate better translation quality.\nTable 3 shows validation and test performance\nin terms of both BLEU and TER when optimizing for either one of the two metrics. While the two metrics are correlated, the best results are achieved when optimizing for the metric used at test time."
    }, {
      "heading" : "6.2 Varying the Number of Hypotheses",
      "text" : "We examine the effect of training the reranker with different sizes of the n-best list, U(x). Even though we fix the n-best list size at training time, we can apply the reranker on n-best lists of different sizes at test time. Figure 2 shows the performance of DrNMT on De-En validation sets from four rerankers trained with 5, 10, 20 and 50 hypotheses, respectively.\nAs the size of the n-best list during test time increases, the performance of all rerankers and NCD improve. On the other hand, the performance of beam decoding starts to saturate early at beam size 10. A reranker trained with 50 hypotheses gives a 1.4 BLEU improvement over beam decoding when beam size is only 5 at test time, and the improvement increases to 3.4 BLEU as we increase the beam size to 200 at test time. DrNMT consistently perform better than or equally well as NCD in all training and testing scenarios.\nInterestingly, a reranker trained with more hypotheses performs better than one trained with fewer hypotheses, regardless of the beam size used at test time. For instance, when the beam size is 20 at test time, the reranker trained with beam 50 improves over beam by 2.3 BLEU points, while the one which was trained with 20 like at test time, improves by 2.2 BLEU points.\nTo our surprise, a reranker trained with only 5 hypotheses can still yield a 3.2 BLEU gain compared with beam decoding when used to rerank 200 hypotheses during test time, indicating that the reranker suffers little from the mismatch between training and testing conditions. As a result, depending on available compute resources, one can decide to set the number of hypotheses to the largest value possible to get better test time performance with larger n-best lists, while being robust to the particular choice used at training time."
    }, {
      "heading" : "6.3 Ablation Study",
      "text" : "We report an ablation study by probing all major design choices made. We train DrNMT by optimizing BLEU and evaluate it on the validation set of the De-En task using 50 hypotheses both at training and test time. Table 4 summarizes all the results.\nPre-training: We investigate the importance of pre-training by comparing with a reranker of the same size initialized with random weights. Table 4 shows that a randomly initialized reranker performs significantly less well, with a decrease of 0.8 BLEU. In addition to lower performance, a randomly initialized reranker also trains more slowly, by requiring 1.6× more weight updates compared to the pre-trained reranker to converge. This corroborates our choice to pre-train, as the reranking task is fairly related to the pre-training task and we lack sufficient labeled data to train such a large model from scratch. Notice that our pre-trained reranker trains for at most two passes over the data before starting to overfit to its training set.\nSource sentence: When comparing “fw + LM” against DrNMT to assess the impact of training discriminatively, we did not take into account a confounding factor which is the fact that the LM\ndoes not attend over the source sentence. Indeed, Salazar et al. (2019) score hypotheses without taking into account the source sentence. What is the gain brought by considering also the source sentence? To answer this question we compare our reranker with a reranker that takes as input only the hypotheses. As shown in Table 4, including the source sentences achieves a small gain of 0.2 BLEU.\nNormalization: We apply minmax normalization and set T = 0.5 when computing the target distribution in the training objective, so that for every source sentence, the range of the BLEU scores of its hypotheses is between 0 and 2. This choice yields a 0.4 BLEU improvement compared to a reranker trained with the raw BLEU scores.\nTraining data: So far we’ve been training the reranker with both bitext and BT data. In Table 4, we see that training the reranker with only bitext data deteriorates the model’s performance by 2 BLEU points. The model starts overfitting after 15 passes over the small bitext (around 9,000 parameter updates). Incorporating the BT data helps alleviate this issue. The model achieves the best validation performance after 1.9 passes over the combination of bitext and BT data (around 63,000 parameter updates).\nModel size: We explore building the reranker using only the first few layers of the XLM-RBase model. Since beam hypotheses often differ only locally on isolated phrases, one may wonder whether more local features, as those produced by a shallower reranker may work better. Moreover, reducing the model capacity may help preventing overfitting. Compared with either only three or six transformer blocks, Table 4 shows that deeper and bigger models work better, despite being more prone to overfitting and despite capturing more global information about their input."
    }, {
      "heading" : "6.4 Other Training and Model Variations",
      "text" : "We conclude our empirical evaluation by investigating how reranking works on top of baseline NMT models trained with back-translation, and by reporting two variations of model architectures. As before, we report results on the validation set of the De-En task with n-best list of size 50, using BLEU as metric.\nMT trained with bitext+BT: Would the gains brought by the reranker carry over when this is ap-\nplied on the n-best list produced by a baseline NMT model trained with back-translation? As shown in Table 2 the beam baseline on validation was at 24.7 BLEU, while if we train the NMT by adding backtranslated data, BLEU increases to 31.6 (Table 5). In this case, we train the reranker using hypotheses generated by the more powerful NMT model trained with back-translated data. From Table 5, we can see that DrNMT gives 1.5 BLEU improvement over the beam decoding baseline, and combining NCD and reranker gives an additional gain of 0.5 BLEU, which is less than what we reported in Table 2 but still confirming the overall finding of discriminative reranker and NCD performing similarly while being complementary to each other.\nCausal vs. bidirectional: As the complete hypothesis is available during reranking, the architecture of our reranker is bidirectional as it conditions on the whole sentence. This contrasts with how the baseline NMT model generates hypotheses and how it scores them with beam which leverages an auto-regressive decomposition. Here we explore the importance of joint modeling and consider an alternative reranker which consists of an encoder and a causal decoder, and which is therefore initialized from the base NMT generating the n-best list. Given a source sentence and a hypothesis as input, the output of the decoder is a T × d matrix (notice that hidden states are causal), where T is the number of tokens of the hypothesis, and d is the hidden dimension. We average the output across position to obtain a d-dimensional representation and apply the same one-hidden layer neural network to obtain a reranking score. Table 6 shows that our bidirectional architecture outperforms the causal architecture by 0.8 BLEU.\nSet reranker: While our training objective considers the full set of hypotheses of each source\nsentence, the reranker scores each pair of (x, ui) in isolation; it never compares hypotheses directly. We therefore explore an architecture that computes cross-hypothesis features. In the original reranker architecture, the model produces a ddimensional representation for each (x, ui). We add another transformer block that computes selfattention across the set of n representations for {(x, u)|u ∈ U(x)}. We then apply the one hidden layer projection block to map each d dimensional vector to a single score as before, yielding n scores for reranking. This design enables the model to have set-level information during reranking, and thus the scoring has to be performed on the full set at once. Table 7 shows that these two model variants perform the same, suggesting that set level representations may need to be captured at a lower layer of the transformer. We leave this avenue of exploration for future work."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Reranking is effective for both SMT and NMT. Inspired by work done almost two decades ago (Shen et al., 2004; Och, 2003), we studied discriminative reranking for NMT and found that it performs at least as well as the strongest generative reranking method we are aware of, namely noisy channel decoding (NCD) (Yee et al., 2019) - as long as care is taken to alleviate overfitting.\nThere is a subtle trade-off between improvements stemming from optimizing the end metric and addressing exposure bias on the one hand, and poor generalization and sample inefficiency of discriminative training on the other hand. In this study we regularize the reranker by using dropout, by pre-training on large corpora and by performing data augmentation.\nEmpirically, we found that NCD and our discrim-\ninative reranker are complementary to each other, yielding sizeable improvements over each other and the beam baseline. Our reranker is computationally less demanding than NCD, since it consists of a single model while NCD requires scoring using two additional models. Our reranker is also robust to the choice of the size of the n-best list and other hyper-parameters settings.\nIn the future we plan to investigate better ways to alleviate sample inefficiency, as well as to design more effective architectures to score at the set level."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Peng-Jen Chen for his guidance on training NMT systems, and Sergey Edunov for his advice on setting up the human evaluation."
    }, {
      "heading" : "A Training details",
      "text" : "A.1 MT model We build the baseline MT models in Table 2 following the Transformer big architecture (Vaswani et al., 2017) with 6 layers, embedding size 1024 and 16 attention heads. Table 8 shows the additional hyper-parameters that we tune on the validation set for the best performing models of each language direction. We use Adam with β1 = 0.9, β2 = 0.98, = 0.00000001, and apply an inverse square root learning rate schedule with 4000 warmup steps. We train for 200 epochs for De-En, En-De and EnTa, and 100K updates for Ru-En, and select the best checkpoint based on validation loss.\nA.2 LM For all LMs, we use 16 transformer layers, embedding size 1024, feed-forward network embedding size 4096 and 16 attention heads. We optimize with NAG with learning rate 0.0001 and a cosine learning rate schedule with 16K warmup steps. All models are trained on 32 GPUs for a maximum of 984K steps, and the best checkpoint is selected based on validation loss.\nA.3 DrNMT We train DrNMT using Adam with β1 = 0.9, β2 = 0.98, = 0.000001, and apply a polynomial learning rate decay schedule with 8000 warmup steps for De-En, En-Ta, and Ru-En, and 16K warmup steps for En-De. We use a learning rate of 0.00005 and dropout 0.2 for De-En, En-Ta, and Ru-En, and a learning rate of 0.00001 and dropout 0.1 for EnDe."
    }, {
      "heading" : "B TER results",
      "text" : "Table 9 summarizes the average validation and test TER (Snover et al., 2006) of DrNMT trained with BLEU (Papineni et al., 2002b) scores. Table 10, Table 11, and Table 12 show TER of each validation and test set for De-En, En-De and Ru-En, respectively. Note that for En-Ta we only have one validation and one test set."
    }, {
      "heading" : "C BLEU results",
      "text" : "Table 13, Table 14, and Table 15 show the performance of DrNMT, trained and evaluated on BLEU, on each validation and test set for De-En, En-De and Ru-En, respectively. The average validation and test BLEU scores of each language pair are reported in the main paper in Table 2."
    }, {
      "heading" : "D Examples",
      "text" : "Table 16 and Table 17 show examples of translation from NCD and DrNMT + NCD."
    } ],
    "references" : [ {
      "title" : "Decoder integration and expected BLEU training for recurrent neural network language models",
      "author" : [ "Michael Auli", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Auli and Gao.,? 2014",
      "shortCiteRegEx" : "Auli and Gao.",
      "year" : 2014
    }, {
      "title" : "Findings of the 2019 conference on machine translation (WMT19)",
      "author" : [ "Loı̈c Barrault", "Ondřej Bojar", "Marta R Costa-Jussà", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Philipp Koehn", "Shervin Malmasi" ],
      "venue" : null,
      "citeRegEx" : "Barrault et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Barrault et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to rank: from pairwise approach to listwise approach",
      "author" : [ "Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li." ],
      "venue" : "Proceedings of the 24th international conference on Machine learning, pages 129–136.",
      "citeRegEx" : "Cao et al\\.,? 2007",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2007
    }, {
      "title" : "Coarseto-fine n-best parsing and MaxEnt discriminative reranking",
      "author" : [ "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180.",
      "citeRegEx" : "Charniak and Johnson.,? 2005",
      "shortCiteRegEx" : "Charniak and Johnson.",
      "year" : 2005
    }, {
      "title" : "Discriminative reranking for natural language parsing",
      "author" : [ "Michael Collins", "Terry Koo." ],
      "venue" : "Computational Linguistics.",
      "citeRegEx" : "Collins and Koo.,? 2005",
      "shortCiteRegEx" : "Collins and Koo.",
      "year" : 2005
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Residual energybased models for text generation",
      "author" : [ "Yuntian Deng", "Anton Bakhtin", "Myle Ott", "Arthur Szlam", "Marc’Aurelio Ranzato" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Deng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Classical structured prediction losses for sequence to sequence learning",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Edunov et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "SMT reranked NMT",
      "author" : [ "Terumasa Ehara." ],
      "venue" : "Workshop on Asian Translation.",
      "citeRegEx" : "Ehara.,? 2017",
      "shortCiteRegEx" : "Ehara.",
      "year" : 2017
    }, {
      "title" : "Ensemble and reranking: Using multiple models in the NICT-2 neural machine translation system at WAT2017",
      "author" : [ "Kenji Imamura", "Eiichiro Sumita." ],
      "venue" : "Workshop on Asian Translation.",
      "citeRegEx" : "Imamura and Sumita.,? 2017",
      "shortCiteRegEx" : "Imamura and Sumita.",
      "year" : 2017
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Édouard Grave", "Piotr Bojanowski", "Tomáš Mikolov." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Domain control for neural machine translation",
      "author" : [ "Catherine Kobus", "Josep M Crego", "Jean Senellart." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 372–378.",
      "citeRegEx" : "Kobus et al\\.,? 2017",
      "shortCiteRegEx" : "Kobus et al\\.",
      "year" : 2017
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A comparable study on model averaging, ensembling and reranking in NMT",
      "author" : [ "Yuchen Liu", "Long Zhou", "Yining Wang", "Yang Zhao", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "International Conference on Natural Language Processing and Chinese Comput-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C Moore", "William Lewis." ],
      "venue" : "Proceedings of the Association for Computational Linguistics 2010 Conference Short Papers, pages 220– 224.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "Energy-based reranking: Improving neural machine translation using energy-based models",
      "author" : [ "Subhajit Naskar", "Amirmohammad Rooshenas", "Simeng Sun", "Mohit Iyyer", "Andrew McCallum." ],
      "venue" : "arXiv:2009.13267.",
      "citeRegEx" : "Naskar et al\\.,? 2020",
      "shortCiteRegEx" : "Naskar et al\\.",
      "year" : 2020
    }, {
      "title" : "Facebook FAIR’s WMT19 news translation task submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan. Association for Computational Linguistics.",
      "citeRegEx" : "Och.,? 2003",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "A smorgasbord of features for statistical machine translation",
      "author" : [ "Franz Josef Och", "Daniel Gildea", "Sanjeev Khudanpur", "Anoop Sarkar", "Kenji Yamada", "Alexander Fraser", "Shankar Kumar", "Libin Shen", "David A Smith", "Katherine Eng" ],
      "venue" : null,
      "citeRegEx" : "Och et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Och et al\\.",
      "year" : 2004
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002a",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002b",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q Nguyen", "Katrin Kirchhoff." ],
      "venue" : "arXiv preprint arXiv:1910.14659.",
      "citeRegEx" : "Salazar et al\\.,? 2019",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of Association for Machine Translation in the Americas.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15(56):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Sogou neural machine translation systems for wmt17",
      "author" : [ "Yuguang Wang", "Shanbo Cheng", "Liyang Jiang", "Jiajun Yang", "Wei Chen", "Muze Li", "Lin Shi", "Yanfeng Wang", "Hongtao Yang." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation,",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial neural machine translation",
      "author" : [ "Lijun Wu", "Yingce Xia", "Li Zhao", "Fei Tian", "Tao Qin", "Jianhuang Lai", "Tie-Yan Liu." ],
      "venue" : "arXiv:1704.06933.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Simple and effective noisy channel modeling for neural machine translation",
      "author" : [ "Kyra Yee", "Yann Dauphin", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Yee et al\\.,? 2019",
      "shortCiteRegEx" : "Yee et al\\.",
      "year" : 2019
    }, {
      "title" : "The neural noisy channel",
      "author" : [ "Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomás Kociský." ],
      "venue" : "International Conference on Learning Representations, ICLR 2017.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional generative adversarial networks for neural machine translation",
      "author" : [ "Zhirui Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Enhong Chen." ],
      "venue" : "Proceedings of the 22nd conference on computational natural language learning, pages 190–199.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al.",
      "startOffset" : 127,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al.",
      "startOffset" : 127,
      "endOffset" : 178
    }, {
      "referenceID" : 22,
      "context" : "Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al., 2004; Shen et al., 2004).",
      "startOffset" : 215,
      "endOffset" : 252
    }, {
      "referenceID" : 30,
      "context" : "Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al., 2004; Shen et al., 2004).",
      "startOffset" : 215,
      "endOffset" : 252
    }, {
      "referenceID" : 22,
      "context" : "A large n-best list produced by these models would then be passed to a discriminatively trained reranker which leverages features engineered to capture more global context (Och et al., 2004) yielding significant improvements to the quality of the translations.",
      "startOffset" : 172,
      "endOffset" : 190
    }, {
      "referenceID" : 27,
      "context" : "However, reranking may still be beneficial for two reasons: First, NMT systems are subject to exposure bias (Ranzato et al., 2016), i.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "2019) which leverages a pre-trained language model and a backward model, show strong improvements over beam search outputs, as demonstrated in recent WMT evaluations (Ng et al., 2019).",
      "startOffset" : 166,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "First, we leverage the success of pre-training by finetuning masked language models (MLM; Devlin et al. 2019) which initializes the model with features trained on much more training data.",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "Second, we augment the original dataset with back-translated data (BT; Sennrich et al. 2016).",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Most work using NMT has focused on generative reranking methods (Liu et al., 2018; Imamura and Sumita, 2017; Wang et al., 2017), where the reranker’s parameters are optimized using a criterion which is different from the metric of interest.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "Most work using NMT has focused on generative reranking methods (Liu et al., 2018; Imamura and Sumita, 2017; Wang et al., 2017), where the reranker’s parameters are optimized using a criterion which is different from the metric of interest.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "Most work using NMT has focused on generative reranking methods (Liu et al., 2018; Imamura and Sumita, 2017; Wang et al., 2017), where the reranker’s parameters are optimized using a criterion which is different from the metric of interest.",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 35,
      "context" : "Finally, our work is also related to recent attempts at using adversarial training to improve MT (Wu et al., 2018; Zhang et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "Finally, our work is also related to recent attempts at using adversarial training to improve MT (Wu et al., 2018; Zhang et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "The goal of this work is to learn a reranker that produces higher scores for hypotheses of better quality, as defined in terms of a user-specified metric μ(u, r) such as BLEU (Papineni et al., 2002a), where quality is measured with respect to a reference r.",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 6,
      "context" : "The architecture includes also position embeddings and language embeddings, to help the model represent tokens that are shared between the two languages (Conneau and Lample, 2019).",
      "startOffset" : 153,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "neural network with d tanh hidden units to z, as default in the design of the “classification head” of RoBERTa (Liu et al., 2019).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "We train the reranker discriminatively, hence the name DrNMT for Discriminative Reranker for NMT, by minimizing the KL-divergence between the target distribution and the model output distribution, DKL(pT ||pM ) (Cao et al., 2007).",
      "startOffset" : 211,
      "endOffset" : 229
    }, {
      "referenceID" : 32,
      "context" : "In order to alleviate overfitting, we employ dropout regularization (Srivastava et al., 2014), we pre-train the model (Conneau et al.",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : ", 2014), we pre-train the model (Conneau et al., 2019) and we also perform data augmentation by training on back-translated data (BT) (Sennrich et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and we also perform data augmentation by training on back-translated data (BT) (Sennrich et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 36,
      "context" : "Whenever we combine scores from various models we tune the additional hyper-parameters controlling the weighted combination by random search on the validation set (Yee et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "For training on De-En and En-De, we use NewsCommentary from WMT’19 (Barrault et al., 2019) and NewsCrawl2018 for the parallel dataset and target side monolingual data, respectively.",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "For Ru-En, we use all the parallel data from WMT’19 (Barrault et al., 2019) and NewsCrawl2018 as the monolingual dataset for training, validate on newstest2015 and 2016, and test on newstest 2017, 2018 and 2019.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "(2019) for data preprocessing, including sentence deduplication, language identification filtering on all bitext and monolingual data (Joulin et al., 2017) and indomain filtering (Moore and Lewis, 2010) on Tamil CommonCrawl data.",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : ", 2017) and indomain filtering (Moore and Lewis, 2010) on Tamil CommonCrawl data.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "For the base NMT models, we learn 30K byte-pair encoding (BPE) units for De-En and En-De, 20K BPE units for En-Ta and 24K BPE units for Ru-En separately, using the sentencepiece toolkit (Kudo and Richardson, 2018).",
      "startOffset" : 186,
      "endOffset" : 213
    }, {
      "referenceID" : 26,
      "context" : "All systems are evaluated using SACREBLEU (Post, 2018).",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "We use the Transformer (Vaswani et al., 2017) architecture and train MT models using bitext data only.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "Since the bitext data of En-Ta originates from seven different sources, we prepend dataset tags to each source sentence to indicate the origin (Kobus et al., 2017).",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 36,
      "context" : "Finally, we consider reranking using noisy channel decoding (NCD; Yee et al. 2019).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "We use XLM-RBase(2) (Conneau et al., 2019), a transformer-based multilingual MLM trained on more than 2.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "We train DrNMT with batch size 512, use Adam (Kingma and Ba, 2015) and early-stop when the validation performance does not improve after 12K parameter updates.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 23,
      "context" : "All models are implemented and trained using fairseq (Ott et al., 2019)3.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 31,
      "context" : "Appendix B shows similar trends when test performance is measured in terms of translation error rate (TER) (Snover et al., 2006), showing that DrNMT is not particularly overfitting to the training metric.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 30,
      "context" : "Inspired by work done almost two decades ago (Shen et al., 2004; Och, 2003), we studied discriminative reranking for NMT and found that it performs at least as well as the strongest generative reranking method we are aware of, namely noisy channel decoding (NCD) (Yee et al.",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Inspired by work done almost two decades ago (Shen et al., 2004; Och, 2003), we studied discriminative reranking for NMT and found that it performs at least as well as the strongest generative reranking method we are aware of, namely noisy channel decoding (NCD) (Yee et al.",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 36,
      "context" : ", 2004; Och, 2003), we studied discriminative reranking for NMT and found that it performs at least as well as the strongest generative reranking method we are aware of, namely noisy channel decoding (NCD) (Yee et al., 2019) - as long as care is taken to alleviate overfitting.",
      "startOffset" : 206,
      "endOffset" : 224
    } ],
    "year" : 2021,
    "abstractText" : "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.",
    "creator" : "LaTeX with hyperref"
  }
}