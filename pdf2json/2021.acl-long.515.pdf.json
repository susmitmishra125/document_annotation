{
  "name" : "2021.acl-long.515.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates",
    "authors" : [ "Yuqing Xie", "Yi-An Lai", "Yuanjun Xiong", "Yi Zhang", "Stefano Soatto" ],
    "emails" : [ "yuqing.xie@uwaterloo.ca", "yianl@amazon.com", "yuanjx@amazon.com", "yizhngn@amazon.com", "soattos@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6589–6602\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6589"
    }, {
      "heading" : "1 Introduction",
      "text" : "Regression-free model update is a desirable system property which guarantees interoperability of a new system with a legacy version, also known as backward compatibility. Regression occurs when the newly updated system stops functioning as intended.\nAs advances in deep learning spark industrial applications in AI areas such as natural language processing, the long-term maintenance of such systems is becoming ever more challenging. While models with complex neural architectures and huge parameter space continue to reach higher accuracy, the lack of interpretability and functional decomposibility in these models make it infeasible to apply traditional software regression testing\n∗ Work done while at Amazon AWS AI. 1Here regression refers to bugs in software testing instead\nof the statistical estimation method.\nmethods such as unit tests. As result, validating and mitigating regressions during model update is often a long and painful engineering process, which often over-shadows the benefits of a new model.\nThe model regression issue in deep learning first comes into sight in Shen et al. (2020), where they inspect compatible representation learning for image retrieval. Yan et al. (2020) proposed the positive-congruent training (PCT) for image classification that minimizes prediction errors and model regression at the same time. To our best knowledge, the model update regression has not been studied on NLP tasks.\nFollowing Yan et al. (2020), in this work we measure the model update regression in NLP by negative flips. In Figure 1, we demonstrate prediction flip scenarios. Negative flips are shown in the upper-right quadrant where the old model makes correct predictions and the new model predictions are wrong. As we will show in Section 2, regression are prevalent in NLU model updates even with the slightest changes in the new model training process.\nTo develop a model with minimum regression, we first formulate the learning task into a constrained optimization problem by taking the regression-free conditions as constraints. We apply the Lagrangian relaxation to bring the regressionfree constraint into the optimization objective as an\nadditional penalty loss, and provide approximate solution via knowledge distillation. Yan et al. (2020) also observed that model ensemble can also reduce negative flips without explicit input from the old model. We evaluate both distillation and ensemble based methods on a diverse set of NLP tasks.\nTo further understand how the above methods contribute to reducing it, we utilize CHECKLIST (Ribeiro et al., 2020) to quantify linguistic behavioral changes before and after applying proposed methods. We find that regressions are prevalent in NLP tasks, and their distribution correlates with different linguistic phenomena.\nOur main contributions are as follows:\n• We provide empirical evidence to show that the model update regression occurs across text classification tasks in NLP;\n• We formulate the regression-free model updates into a constrained optimization problem, and further reduce into a relaxed form which can be approximately optimized through knowledge distillation training method;\n• We also explore the model ensemble as another method to reduce regression, and analyzed its efficacy;\n• We analyze the source of the regressions in NLP tasks through linguistic behavioural testing, compare reduction in both distillation and ensemble methods."
    }, {
      "heading" : "2 Measuring Regression in NLP Model Update",
      "text" : "In this section, we first formulate the measure of model update regression on classification tasks. Then we benchmark on GLUE tasks (Wang et al., 2018) and show that there is a prevalent presence of regression when updating models in NLP."
    }, {
      "heading" : "2.1 Regression Measurement on Classification Tasks",
      "text" : "Similar to software regression testing, we need to collect a group of test cases when measuring regression. We start from a regression set: Dreg = {xi, yi}Ni=1, yi ∈ {l1, l2, ..., lC}, where li is the i-th label and C is the number of classes. In practice, we can use the development set or compile a collection of critical use cases as Dreg.\nIn a classification task, given a input xi, a neural network model f , parameterized by φ,\napproximates the posterior probabilistic distribution p(yi|xi) over all possible labels: ~fφ(xi) = (pφ(y = l1|xi), ..., pφ(y = lC |xi))>. To simplify, we denote the final prediction of a model to be fφ(x) = argmaxlj pφ(lj |x).\nThe regressionRNF between two models fφold and fφnew on Dreg can be defined as the portion of negative flip cases:\nRNF (Dreg, ~fφold , ~fφnew)\n= |{x|fφold = y, fφnew 6= y}|\n|Dreg| .\nWe use negative flip RNF as our regression measurement for classification tasks. LowerRNF for a new models means better compatibility with the old model."
    }, {
      "heading" : "2.2 Benchmark Severity of Regression",
      "text" : "The success of Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) have made pretraining then fine-tuning a standard paradigm in NLP systems. When updating these systems, differences can come from various aspects:\n• Changes in the fine-tuning hyperparameters (e.g. random seed, learning rate schedule, epoch, etc.)\n• Changes in model size or architecture (e.g. from BERTbase to BERTlarge)\n• Changes in pre-training procedure or objective (e.g. BERT to ROBERTA (Liu et al., 2020), to BERTwhole-word-masking or to ELECTRA (Clark et al., 2020))\n• Changes in pre-trained model architecture (e.g. BERT to ALBERT (Lan et al., 2020))\nWhile accuracy or efficient improvements are strong motivations for these model updates, they could also introduce behavioral incongruence when compared to the previous model. To benchmark the severity of regression, we apply a general setup: Fine-tune various pre-trained language models (LM) on GLUE and calculateRNF when updating from BERTbase to other LMs . We use dev sets as Dreg. Results in Table 1 show that:\n1. Model update regression is prevalent on NLU tasks. A minimum of 1.98%RNF is observed across diverse classification tasks and model update scenarios, while the average accuracy gain is only 1.4%.\n2. Minor changes such as random seeds can introduce significant regression. Shown in\n→BERTbase, even when we only alter the initialization random seed, this can lead to up to 3.56% negative flip.\n3. Negative flip rates are often much higher than the accuracy gains. When updating to BERTlarge on QQP, RNF is about 8X the accuracy gain. This implies reducing error rate alone does not ensure the decrease in regression.\n4. Pre-training objective or architecture updates often lead to higher regressions than those caused by model size or random seeds. The regressions are higher when updating to ALBERTA, compared with updating to a larger model BERTlarge or a different random seed. This implies systematic regression could be introduced if the backbone models are different."
    }, {
      "heading" : "3 Reducing Regression in Model Update",
      "text" : "In this section, we first formulate regressionfree model update as a constrained optimization problem, then further reducing it to a joint optimization objective combining the training loss on the original task and a distillation loss with respect to the old model’s behavior.\nUnlike typical optimizations in neural model training where we minimizes a loss function on a training set, the regression-free model update requires the model to learn the target task as well as comply with conditions posed by the old model.\n2Full results on GLUE can be found in Appendix A\nWe can cast the regression-free model update as a constrained optimization problem by writing down the classification loss as the optimization objective and the regression-free conditions as constraints:\nmin φnew ∑ x∈Dtrain LCE(x, φnew)\ns.t.RNF (fφold , fφnew ,Dreg) = 0. (1)\nwhere Dtrain,Dreg represent the training and regression sets, respectively.\nThe constraint in Equation 1 asks for zero regression on Dreg. It would be difficult to ensure the constraint is satisfied along the model training. We instead relax the hard constraint into a soft inequality condition that allows the regression measure to be less than a constant C:\nmin φnew ∑ x∈Dtrain LCE(x, φnew)\ns.t. C −RNF (fφold , fφnew ,Dreg) ≥ 0. (2)\nTraining a model directly with the regressionfree constraint still remains difficult in that signals from old predictions are sparse and RNF is nondifferentiable. Here, we propose two proxies of RNF to measure regression in continuous space. Proxy from Prediction Probabilities. We use the KL divergence between the predicted probabilities of both models as one soft regression measure:\nRKL-div(fφold , fφnew ,Dreg) = ∑\nx∈Dreg\nDKL(pφold(y|x)||pφnew(y|x)). (3)\nProxy from Deep Representations. We can also use the l2 distance between models’ sentence representations, e.g. [CLS] embedding in BERT as another soft regression measure:\nRl2(fφold , fφnew ,Dreg) = ∑\nx∈Dreg\nl2(~fφold(x), ~fφnew(x)).\n(4)\nA linear projection is used to align the representations if they initially lie in different spaces.\nReduce to Knowledge Distillation. Finally, we apply the Lagrangian relaxation to bring the regression-free constraint into the optimization objective as an additional penalty loss:\nmin φnew ∑ x∈Dtrain LCE(x, φnew)\n− α ∗ (C −Rsoft(fφold , fφnew ,Dreg)), (5)\nwhere α is a positive penalty scaling parameter and Rsoft can be chosen from RKL-div or Rl2 . Then, the above optimization problem can be cast into a joint learning of the original target task and knowledge distillation from the old model. The distillation loss acts as a surrogate of the model update regression measure. The joint learning process minimizes this term as an approximation of minimizing the overall model update regression."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Implementation Details",
      "text" : "Since we usually update models from elementary ones to improved ones, in the experiments we take origin BERTbase (12-layer, 768-hidden, 12- heads, 110M parameters) (Devlin et al., 2019) as the old model’s backbone and update it to a homogeneous model, e.g. BERTbase with different fine-tuning random seeds or parameters, or a heterogeneous models with improvements such as BERTlarge (24-layer, 1024-hidden, 16- heads, 340M parameters). We fine-tune the pretrained LMs without any constraint as our baselines. We use the GLUE datasets to benchmark the effectiveness of proposed techniques. Details of each GLUE task can be found in Appendix D. For investigative experiments, we use the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), a paraphrase identification dataset that aims to classify whether two sentences are the paraphrase of each other. Pre-trained\nmodel artifacts and the GLUE dataset processing procedures are brought from Hugging Face3 and experiments are done in PyTorch (Paszke et al., 2019) with Tesla V100 GPUs. Cross-entropy is used for fine-tuning on target tasks with batch size 16 for 4 to 6 epochs. The learning rate is searched among 2e−5, 3e−5 and 5e−5.\nDuring joint training of classification and knowledge distillation, we take the fine-tuned old models as the teacher, and distill with batch size 16 for 6 to 8 epochs. We set Dreg = Dtrain when training models with the constraint and use Dreg = Ddev for reporting results. To encourage constraint satisfaction and reduce regression, we only include the distillation penalty into our loss on the examples where the current model makes negative flips."
    }, {
      "heading" : "4.2 Ensemble",
      "text" : "Yan et al. (2020) reported an intriguing finding on image classification tasks that model ensemble can reduce model update regressions without explicit regularization from the old model. This was attributed to the reduction of variance in ensemble model predictions, making it less prone to overfitting and indirectly reducing regressions. Here we include model ensemble as an alternative approach to reduce regression, with further analysis on how ensemble reduces regression in Section 5.1."
    }, {
      "heading" : "4.3 Main Results",
      "text" : "Table 2 shows the efficacy of distillation method and model ensemble on reducing NLP classification task model update regressions. On average, the distillation method reduces RNF by 30.6% and 36.3% while the ensemble method reduces RNF by 55.9% and 20.6% when updating to BERTbase and to BERTlarge, respectively. Both distillation and ensemble methods can significantly bring down negative flips across GLUE tasks compared with the baselines. The ensemble seems to work better when the old and new models share the same underlying pre-trained LM. In the update BERTbase→BERTbase, the ensemble method outperforms the distillation on reducing the regression. On the other hand, the distillation method seems to be more effective on reducing regression under the heterogeneous model update setting. In the update BERTbase →BERTlarge, distillation reduce more regression, with especially\n3https://huggingface.co\nlarge reductions on small datasets such as CoLA and SST-2. We hypothesize that it’s because the ensemble focuses on reducing the variance in model predictions, while distillation enables the explicit alignment in either probability distribution or representation space between the old and the new model. When the new model is very different from the old one, it can implicitly align new model’s behavior with the old one."
    }, {
      "heading" : "4.4 Variants in Distillation Objective",
      "text" : "As introduced in Section 3, we can have several variants of distillation loss to be used to constrain new model training on the old model. We explore and benchmark the following variants on the MRPC task:\n• Distillation - RKL-div, Logits calculates the distillation loss as the KL divergence between the two Bernoulli distributions set by the old and new model prediction probabilities;\n• Distillation - Rl2 , [CLS] uses the [CLS] token embedding from the final layer as sentence representations and calculates the distillation loss as the Euclidean distance between the two vectors;\n• Distillation - Rl2 , All [CLS] also calculates the Euclidean distance between the old and new sentence representation vectors, but with concatenated [CLS] token embeddings from all layers instead of the final layer.\nPre-trained models could have different layers. For BERTbase →BERTlarge in the All [CLS] setup, we align representations from BERTlarge’s even layers with the corresponding BERTbase layers, e.g. 14-th layer in BERTlarge is aligned with 7-th in BERTbase.\nTable 3 shows the results. In the homogeneous setup, the most effective variant is to align the prediction probabilities via RKL-div, where it achieves up to 58% RNF reduction, i.e. from 4.17% to 1.72%. For Rl2 setup, aligning at all layers can further reduceRNF compared with only aligning at the final layer. This implies a deeper alignment can help the new model more effectively learn to behave similarly as the old one when fine-tuning the same architecture with a different random seed.\nIn the heterogeneous setup, Rl2 , [CLS] works the best for BERTlarge that achieves 62% RNF reduction, with RKL-div having a comparable performance. Overall,RKL-div produces consistent regression reductions across different setups, which we pick it as our default setting in the distillation method.\nFrom Table 3, we can also observe that the deeper alignment seems to hurt RNF in the heterogeneous update setup. The reason might be that differences between pre-trained models are too significant. The distillation with simple all-layer alignment could mess up pre-trained representations rather than effectively encourage\nnew models to learn where the old model performs well.\nAnother interesting finding is that the simple model ensemble is a competitive solution comparing to the distillation. In the BERTbase →BERTbase setup, the ensemble even outperforms all the other distillation variants. This is indeed a bit counter-intuitive as the distillation explicitly encourages the new model to pick up old models’ correct predictions while the ensemble does not involve the old model in the process. We conduct deeper analysis trying to understand on which aspects that these methods work to reduce the regression in the next section."
    }, {
      "heading" : "5 Analyzing Regression in Model Updates",
      "text" : "In this section, we first analyze the model ensemble and present our hypothesis on how it reduces regression. Next, we conduct behavioral testing across diverse linguistic phenomena to see where the reduced and remaining regressions reside."
    }, {
      "heading" : "5.1 Analysis of Updating to Model Ensemble",
      "text" : "Similar to the findings of Yan et al. (2020), we observe in Table 2 and 3 that a simple ensemble of models trained with different random initialization before finetuning can reduce regression in some cases. We fine-tune BERTbase on MRPC with 20 random seeds as our old base models, and another 20 seeds as our new single models, and another 100 seeds for building 20 ensemble models. Next, we calculateRNF on the dev set in each model update setup, i.e. 400 update pairs. Figure 2 plots their model update regression RNF distributions. We observe that the ensemble can not only bring down RNF but also reduce its variance.\nFrom Figure 2, we conjecture that each single\nmodel could learn a subset of all possible patterns in the data to achieve comparable accuracy on the task. Models fine-tuned with different seeds could rely on different sets of patterns for predictions, leading to behavioral difference and regression. On the other hand, ensemble aggregates distinct and complementary behaviors from individual models, leading to less eccentric behavior and increased compatible with individual models on average. In a parallel work, Zeyuan and Li (2020) provides a theoretical framework of how ensemble works from the multi-view perspective. They show that single models can pick up multiple but different views of the data, and the ensemble naturally collects more view features, leading to a higher accuracy. Our hypothesis concurs with their findings.\nHowever, ensemble is not required to achieve moderate model behavior. To verify this, we designed the following simple model selection\nprocedure. We first train 20 new single models, among which we compute for each model the average RNF on the first half of dev set when updating from the other 19 models. We then select the single model with the lowest averageRNF as the centric. Results in Table 4 show the accuracy andRNF on the second half of the dev set. Indeed the single centric model achieves substantial reduction inRNF comparable to model ensemble. We further plot all the BERTbase models based on their class predictions down-projected by PCA (Hotelling, 1933). Figure 3 shows that single models tend to spread while ensembles are more concentrated and close together. We can also see that the centric indeed sits near the center of single model cluster. In essence, the centric model is a single model that requires much less compute resource than the ensemble model during inference, yet can achieve comparable performance and reductions in regression."
    }, {
      "heading" : "5.2 Analyzing Regression with Linguistic Behavioral Testing",
      "text" : "To further understand where the regression happens and how the above methods contribute to reducing regression, we conduct qualitative analysis across diverse linguistic phenomena. More precisely, we leverage the CHECKLIST (Ribeiro et al., 2020) behavioral testing and construct regression sets for relevant linguistic capabilities and tests based on perturbations and provided templates. For example, to test the capability of dealing with lexical taxonomy in the paraphrase detection task, we replace adjectives in one sentence with their synonyms with the label unchanged and expect the model can still predict correctly. We manually set the templates, apply CHECKLIST to automatically generate testing sentence pairs, and calculate RNF for each linguistic test. Detailed linguistic behavioral testing setups with examples can be found in Appendix C.\nTable 5 shows the linguistic behavioral testing results when updating from BERTbase - 1 Seed to BERTlarge. Each row denotes one specific behavioral test and 500 cases are sampled in each test. We focus on negative flips where the new model fails the test while the old model passes. We can observe that the vanilla finetuned BERTlarge has significant regressions on switching with synonyms, asymmetric ordering, and active-passive swap related to people names (see Appendix C). Also, we observe that models tend to either fail or pass almost all cases in a test, which leads to high variances inRNF . This implies that models fine-tuned with different seeds can have different behavioral patterns, which could be one source of regression.\nFurthermore, Table 5 shows that the distillation can effectively reduce regressions across almost all types of behavioral tests. This demonstrates that minimizing the surrogate regression measure, formulated as a knowledge distillation objective, reduces the regression through actually aligning new model’s behavior with the old model.\nFor the ensemble, although it can reduce significant regressions in the benchmark, we observe that it can only improve the model update compatibility on a handful of capabilities. We hypothesize that the ensemble mostly improves the compatibility with the underlying constituent models. Without an explicit alignment, it cannot proactively reduce the regression on certain behavior tests when updating\nfrom other distinct models."
    }, {
      "heading" : "6 Related work",
      "text" : ""
    }, {
      "heading" : "6.1 Model Update Regression and Solutions",
      "text" : "The backward compatibility representation learning first comes into sight in Shen et al. (2020) on learning inter-operabile visual embeddings for image retrieval tasks. Later, Yan et al. (2020) formalize the model update regression problem in machine learning and explore solutions on image classification tasks. They suggest negative flip (NF) as the empirical measurement of regression and propose a specialized knowledge distillation loss (Hinton et al., 2015) as a surrogate of regression for joint optimizations. Our work investigates the model update regression in NLP classification tasks, which involve discrete signals and rich linguistic structures. We formulate our solutions from the perspective of constraint satisfaction and verify their efficacy on scenarios including update to distinct architectures."
    }, {
      "heading" : "6.2 Transfer Learning, Lifelong Learning and Concept Drifting",
      "text" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020). Our work follows this transfer learning paradigm but our main focus is to investigate the regression phenomenon when updating backbone pre-trained models. Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017;\nYoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al., 2017; Chaudhry et al., 2018; Prabhu et al., 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution. The model update regression problem differs in that models are trained on the same task and dataset, but we update from one model to another."
    }, {
      "heading" : "6.3 Behavioral Testing of NLP Models",
      "text" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020). In particular, CHECKLIST (Ribeiro et al., 2020) leverages and expands those techniques to efficiently evaluate a wide range of linguistic behavioral capabilities of NLP models. Our work applies CHECKLIST to inspect where the model update regressions come from and on which linguistic phenomena our proposed solutions help to reduce regressions."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we investigated the regression in NLP model updates on classification tasks and show that it has a prevalent presence across tasks and models. We formulated the regression-free model update problem as a constrained optimization problem and reduce it into a joint learning objective on target\ntask while distilling from the old model. Together with the ensemble, these methods can cut down the regression by 60% at best. Experiments on the GLUE benchmark showed that ensemble can be effective in reducing the regression when updating to homogeneous models. On the other hand, knowledge distillation produced more significant regression reductions under the heterogeneous setting. Through linguistic behavioral testing we showed that distillation can reduce the regression across a wider range of linguistic phenomena than ensemble method. While the regression reduction achieved by the discussed methods are promising, they are far from reaching regression-free. We leave the design of more advanced regressionreduction methods as future works."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to acknowledge the AWS AI team for inspiring discussions, honest feedback, and full support. We are also very grateful to the reviewers for insightful comments and helpful suggestions."
    }, {
      "heading" : "A Full Results of Regression Between SOTA Model Updates",
      "text" : "Due to the page limitation, we present the full regression update comparison between commonly used pre-trained model pairs (Devlin et al., 2019; Liu et al., 2020; Lan et al., 2020; Clark et al., 2020) in Table 6.\nWe show regression in model updates from BERTbase to the other common used pre-trained models; we also show the regression in updates from BERTlarge to BERTlarge−whole−word−masking and updating from ROBERTAbase to ELECTRAbase.\nOther than the universal presence of the regression, Table 6 shows that:\n1. The more difference in pre-training method, the higher regression can be observed. From the results of updating from BERTbase experiment group we can have this conclusion: The updating factors can increase regression in ascending order: hyperparameters (to BERTbase), pre-train settings(to ROBERTAbase), model size (to BERTlarge), pre-train objection (to ELECTRAbase), model structure(to ALBERTbase)\n2. Similar updating factor results in similar level of regression. Updating from BERTlarge to BERTlarge−wwm have similar regression level as in updating from BERTbase to ELECTRAbase, both are updating in the pre-training objective. Similarly, Updating from ROBERTAbase to ELECTRAbase have similar regression as in updating from BERTbase to ELECTRAbase."
    }, {
      "heading" : "B Selection of Regression Set During Training",
      "text" : "Here, we explore the difference of regressing set selection during knowledge distillation.\nFor the regression set used in the model training process, we propose several options:\n1. Take the entire training set as our the regression set in training Dreg = Dtrain 2. Training examples where the old model makes correct predictions Dreg = Dcorrect 3. Training examples where the old model gets a higher predict probability on the groundtruth class than the new model Dreg =\nDbetter, equivalent to adjusting α dynamically according to the performance of the two models, we set α to zero when pφold(y|x) < pφnew(y|x) 4. Extra data from other tasks Dreg = Dextra 5. User-provided regression set, which includes\nexamples with high-stakes Dreg = Duser We experiment with all options except for the userprovided regression set, see Table 7.\nDynamically adapting the regression set according to the current performance of the new model in Distillation (RKL-div,Dbetter) offers the most reduction in the regression without sacrificing the accuracy. We conjecture that it’s because we apply the soft regression-free constraint loss precisely on examples where the new model’s performance is behind."
    }, {
      "heading" : "C Linguistic Behaviour Test settings",
      "text" : "In the linguistic behaviour tests, we go through a variety range of linguistic aspects and design test examples following CheckList(Ribeiro et al., 2020).\nIn Table 8 we show the tests for linguistic behaviour tests. Please find the example test cases in the third column for each testing."
    }, {
      "heading" : "D GLUE Details",
      "text" : "The GLUE datasets are described as follows(Jiao et al., 2020): MNLI. Multi-Genre Natural Language Inference is a large-scale, crowd-sourced entailment classification task (Williams et al., 2018). Given a pair of 〈premise, hypothesis〉, the goal is to predict whether the hypothesis is an entailment, contradiction, or neutral with respect to the premise. QQP. Quora Question Pairs is a collection of question pairs from the website Quora. The task is to determine whether two questions are semantically equivalent (Chen et al., 2018). QNLI. Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary sentence pair classification task by Wang et al. (2018). Given a pair 〈question, context〉. The task is to determine whether the context contains the answer to the question. SST-2. The Stanford Sentiment Treebank is a binary single-sentence classification task, where\nthe goal is to predict the sentiment of movie reviews (Socher et al., 2013). CoLA. The Corpus of Linguistic Acceptability is a task to predict whether an English sentence is a grammatically correct one (Warstadt et al., 2019). STS-B. The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and many other domains (Cer et al., 2017). The task aims to evaluate how similar two pieces of texts are by a score from 1 to 5. MRPC. Microsoft Research Paraphrase Corpus is\na paraphrase identification dataset where systems aim to identify if two sentences are paraphrases of each other (Dolan and Brockett, 2005). RTE. Recognizing Textual Entailment is a binary entailment task with a small training dataset (Bentivogli et al., 2009)."
    } ],
    "references" : [ {
      "title" : "Synthetic and natural noise both break neural machine translation",
      "author" : [ "Yonatan Belinkov", "Yonatan Bisk." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Belinkov and Bisk.,? 2018",
      "shortCiteRegEx" : "Belinkov and Bisk.",
      "year" : 2018
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo", "Bernardo Magnini." ],
      "venue" : "Proceedings of TAC.",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Riemannian walk for incremental learning: Understanding forgetting and intransigence",
      "author" : [ "Arslan Chaudhry", "Puneet K Dokania", "Thalaiyasingam Ajanthan", "Philip HS Torr." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Chaudhry et al\\.,? 2018",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2018
    }, {
      "title" : "Lifelong language knowledge distillation",
      "author" : [ "Yung-Sung Chuang", "Shang-Yu Su", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2914–2924, Online. Association",
      "citeRegEx" : "Chuang et al\\.,? 2020",
      "shortCiteRegEx" : "Chuang et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual language model pretraining",
      "author" : [ "Alexis CONNEAU", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "CONNEAU and Lample.,? 2019",
      "shortCiteRegEx" : "CONNEAU and Lample.",
      "year" : 2019
    }, {
      "title" : "A continual learning survey: Defying forgetting in classification tasks",
      "author" : [ "Matthias Delange", "Rahaf Aljundi", "Marc Masana", "Sarah Parisot", "Xu Jia", "Ales Leonardis", "Greg Slabaugh", "Tinne Tuytelaars." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine",
      "citeRegEx" : "Delange et al\\.,? 2021",
      "shortCiteRegEx" : "Delange et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Analysis of a complex of statistical variables into principal components",
      "author" : [ "Harold Hotelling." ],
      "venue" : "Journal of educational psychology, 24(6):417.",
      "citeRegEx" : "Hotelling.,? 1933",
      "shortCiteRegEx" : "Hotelling.",
      "year" : 1933
    }, {
      "title" : "Chapter 1 - introduction",
      "author" : [ "I. Gama J. Žliobaitė", "M. Pechenizkiy" ],
      "venue" : "Stefanowski J. Japkowicz N., editor, An Overview of Concept Drift Applications., volume 16 of Big Data Analysis: New Algorithms for a New Society. Studies in Big Data. Springer.",
      "citeRegEx" : "Žliobaitė and Pechenizkiy,? 2016",
      "shortCiteRegEx" : "Žliobaitė and Pechenizkiy",
      "year" : 2016
    }, {
      "title" : "Tiny{bert}: Distilling {bert} for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu" ],
      "venue" : null,
      "citeRegEx" : "Jiao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Meta-learning, model selection, and example selection in machine learning domains with concept drift",
      "author" : [ "Ralf Klinkenberg" ],
      "venue" : null,
      "citeRegEx" : "Klinkenberg.,? \\Q2005\\E",
      "shortCiteRegEx" : "Klinkenberg.",
      "year" : 2005
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Gradient episodic memory for continual learning",
      "author" : [ "David Lopez-Paz", "Marc’Aurelio Ranzato" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Lopez.Paz and Ranzato.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lopez.Paz and Ranzato.",
      "year" : 2017
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Perturbation sensitivity analysis to detect unintended model biases",
      "author" : [ "Vinodkumar Prabhakaran", "Ben Hutchinson", "Margaret Mitchell." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Prabhakaran et al\\.,? 2019",
      "shortCiteRegEx" : "Prabhakaran et al\\.",
      "year" : 2019
    }, {
      "title" : "Gdumb: A simple approach that questions our progress in continual learning",
      "author" : [ "Ameya Prabhu", "Philip HS Torr", "Puneet K Dokania." ],
      "venue" : "European Conference on Computer Vision, pages 524–540. Springer.",
      "citeRegEx" : "Prabhu et al\\.,? 2020",
      "shortCiteRegEx" : "Prabhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "icarl: Incremental classifier and representation learning",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Alexander Kolesnikov", "Georg Sperl", "Christoph H Lampert." ],
      "venue" : "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010.",
      "citeRegEx" : "Rebuffi et al\\.,? 2017",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2017
    }, {
      "title" : "Semantically equivalent adversarial rules for debugging nlp models",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Ribeiro et al\\.,? 2018",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond incremental processing: Tracking concept drift",
      "author" : [ "Jeffrey C. Schlimmer", "Richard H. Granger." ],
      "venue" : "Proceedings of the Fifth AAAI National Conference on Artificial Intelligence, AAAI’86, page 502–507. AAAI Press.",
      "citeRegEx" : "Schlimmer and Granger.,? 1986",
      "shortCiteRegEx" : "Schlimmer and Granger.",
      "year" : 1986
    }, {
      "title" : "Towards backward-compatible representation learning",
      "author" : [ "Yantao Shen", "Yuanjun Xiong", "Wei Xia", "Stefano Soatto." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Lamol: Language modeling for lifelong language learning",
      "author" : [ "Fan-Keng Sun", "Cheng-Hao Ho", "Hung-Yi Lee." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "olmpics-on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743– 758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "The problem of concept drift: definitions and related work",
      "author" : [ "A. Tsymbal" ],
      "venue" : null,
      "citeRegEx" : "Tsymbal.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tsymbal.",
      "year" : 2004
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Errudite: Scalable, reproducible, and testable error analysis",
      "author" : [ "Tongshuang Wu", "Marco Tulio Ribeiro", "Jeffrey Heer", "Daniel S Weld." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 747–763.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Positive-congruent training: Towards regression-free model updates",
      "author" : [ "Sijie Yan", "Yuanjun Xiong", "Kaustav Kundu", "Shuo Yang", "Siqi Deng", "Meng Wang", "Wei Xia", "Stefano Soatto." ],
      "venue" : "arXiv preprint arXiv:2011.09161.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Lifelong learning with dynamically expandable networks",
      "author" : [ "Jaehong Yoon", "Eunho Yang", "Jeongtae Lee", "Sung Ju Hwang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Yoon et al\\.,? 2018",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards understanding ensemble, knowledge distillation and self-distillation in deep learning",
      "author" : [ "Zhu Zeyuan", "Yuanzhi Li." ],
      "venue" : "arXiv preprint arXiv:2012.09816.",
      "citeRegEx" : "Zeyuan and Li.,? 2020",
      "shortCiteRegEx" : "Zeyuan and Li.",
      "year" : 2020
    }, {
      "title" : "Given a pair 〈question, context〉. The task is to determine whether the context contains the answer to the question. SST-2",
      "author" : [ "Wang" ],
      "venue" : "The Stanford Sentiment Treebank",
      "citeRegEx" : "Wang,? \\Q2018\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "To further understand how the above methods contribute to reducing it, we utilize CHECKLIST (Ribeiro et al., 2020) to quantify linguistic behavioral changes before and after applying proposed methods.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "Then we benchmark on GLUE tasks (Wang et al., 2018) and show that there is a prevalent presence of regression when updating models in NLP.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : "The success of Transformer (Vaswani et al., 2017) and BERT (Devlin et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : ", 2017) and BERT (Devlin et al., 2019) have made pretraining then fine-tuning a standard paradigm in NLP systems.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "BERT to ROBERTA (Liu et al., 2020), to BERTwhole-word-masking or to ELECTRA (Clark et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : ", 2020), to BERTwhole-word-masking or to ELECTRA (Clark et al., 2020))",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Since we usually update models from elementary ones to improved ones, in the experiments we take origin BERTbase (12-layer, 768-hidden, 12heads, 110M parameters) (Devlin et al., 2019) as the old model’s backbone and update it to a homogeneous model, e.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "For investigative experiments, we use the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005), a paraphrase identification dataset that aims to classify whether two sentences are the paraphrase of each other.",
      "startOffset" : 86,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : "Due to page limitation, we only show the matched results on MNLI (Williams et al., 2018).",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "We further plot all the BERTbase models based on their class predictions down-projected by PCA (Hotelling, 1933).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "More precisely, we leverage the CHECKLIST (Ribeiro et al., 2020) behavioral testing and construct regression sets for relevant linguistic capabilities and tests based on perturbations and provided templates.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "They suggest negative flip (NF) as the empirical measurement of regression and propose a specialized knowledge distillation loss (Hinton et al., 2015) as a surrogate of regression for joint optimizations.",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 241
    }, {
      "referenceID" : 15,
      "context" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 241
    }, {
      "referenceID" : 6,
      "context" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 241
    }, {
      "referenceID" : 5,
      "context" : "Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 241
    }, {
      "referenceID" : 17,
      "context" : "Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al.",
      "startOffset" : 56,
      "endOffset" : 165
    }, {
      "referenceID" : 37,
      "context" : "Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al.",
      "startOffset" : 56,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al.",
      "startOffset" : 56,
      "endOffset" : 165
    }, {
      "referenceID" : 28,
      "context" : "Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al.",
      "startOffset" : 56,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al.",
      "startOffset" : 56,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : ", 2020), incremental learning (Rebuffi et al., 2017; Chaudhry et al., 2018; Prabhu et al., 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I.",
      "startOffset" : 30,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : ", 2020), incremental learning (Rebuffi et al., 2017; Chaudhry et al., 2018; Prabhu et al., 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I.",
      "startOffset" : 30,
      "endOffset" : 96
    }, {
      "referenceID" : 20,
      "context" : ", 2020), incremental learning (Rebuffi et al., 2017; Chaudhry et al., 2018; Prabhu et al., 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I.",
      "startOffset" : 30,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : ", 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution.",
      "startOffset" : 29,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : ", 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution.",
      "startOffset" : 29,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : ", 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobaitė I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution.",
      "startOffset" : 29,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 245
    }, {
      "referenceID" : 23,
      "context" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 245
    }, {
      "referenceID" : 19,
      "context" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 245
    }, {
      "referenceID" : 35,
      "context" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 245
    }, {
      "referenceID" : 29,
      "context" : "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 245
    }, {
      "referenceID" : 24,
      "context" : "In particular, CHECKLIST (Ribeiro et al., 2020) leverages and expands those techniques to efficiently evaluate a wide range of linguistic behavioral capabilities of NLP models.",
      "startOffset" : 25,
      "endOffset" : 47
    } ],
    "year" : 2021,
    "abstractText" : "Behavior of deep neural networks can be inconsistent between different versions. Regressions1during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CHECKLIST behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.",
    "creator" : "LaTeX with hyperref"
  }
}