{
  "name" : "2021.acl-long.4.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HATECHECK: Functional Tests for Hate Speech Detection Models",
    "authors" : [ "Paul Röttger", "Bertram Vidgen", "Dong Nguyen", "Zeerak Waseem", "Helen Margetts", "Janet B. Pierrehumbert" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 41–58\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n41"
    }, {
      "heading" : "1 Introduction",
      "text" : "Hate speech detection models play an important role in online content moderation and enable scientific analyses of online hate more generally. This has motivated much research in NLP and the social sciences. However, even state-of-the-art models exhibit substantial weaknesses (see Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Vidgen et al., 2019; Mishra et al., 2020, for reviews).\nSo far, hate speech detection models have primarily been evaluated by measuring held-out performance on a small set of widely-used hate speech datasets (particularly Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018), but recent work has highlighted the limitations of this evaluation paradigm. Aggregate performance metrics offer limited insight into specific model weak-\nnesses (Wu et al., 2019). Further, if there are systematic gaps and biases in training data, models may perform deceptively well on corresponding held-out test sets by learning simple decision rules rather than encoding a more generalisable understanding of the task (e.g. Niven and Kao, 2019; Geva et al., 2019; Shah et al., 2020). The latter issue is particularly relevant to hate speech detection since current hate speech datasets vary in data source, sampling strategy and annotation process (Vidgen and Derczynski, 2020; Poletto et al., 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al., 2018; Sap et al., 2019) as well as topic and author biases (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020). Correspondingly, models trained on such datasets have been shown to be overly sensitive to lexical features such as group identifiers (Park et al., 2018; Dixon et al., 2018; Kennedy et al., 2020), and to generalise poorly to other datasets (Nejadgholi and Kiritchenko, 2020; Samory et al., 2020). Therefore, held-out performance on current hate speech datasets is an incomplete and potentially misleading measure of model quality.\nTo enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models. Functional testing, also known as black-box testing, is a testing framework from software engineering that assesses different functionalities of a given model by validating its output on sets of targeted test cases (Beizer, 1995). Ribeiro et al. (2020) show how such a framework can be used for structured model evaluation across diverse NLP tasks.\nHATECHECK covers 29 model functionalities, the selection of which we motivate through a series of interviews with civil society stakeholders and a review of hate speech research. Each functionality is tested by a separate functional test. We create 18 functional tests corresponding to distinct\nexpressions of hate. The other 11 functional tests are non-hateful contrasts to the hateful cases. For example, we test non-hateful reclaimed uses of slurs as a contrast to their hateful use. Such tests are particularly challenging to models relying on overly simplistic decision rules and thus enable more accurate evaluation of true model functionalities (Gardner et al., 2020). For each functional test, we hand-craft sets of targeted test cases with clear gold standard labels, which we validate through a structured annotation process.1\nHATECHECK is broadly applicable across English-language hate speech detection models. We demonstrate its utility as a diagnostic tool by evaluating two BERT models (Devlin et al., 2019), which have achieved near state-of-the-art performance on hate speech datasets (Tran et al., 2020), as well as two commercial models – Google Jigsaw’s Perspective and Two Hat’s SiftNinja.2 When tested with HATECHECK, all models appear overly sensitive to specific keywords such as slurs. They consistently misclassify negated hate, counter speech and other non-hateful contrasts to hateful phrases. Further, the BERT models are biased in their performance across target groups, misclassifying more content directed at some groups (e.g. women) than at others. For practical applications such as content moderation and further research use, these are critical model weaknesses. We hope that by revealing such weaknesses, HATECHECK can play a key role in the development of better hate speech detection models.\nDefinition of Hate Speech We draw on previous definitions of hate speech (Warner and Hirschberg, 2012; Davidson et al., 2017) as well as recent typologies of abusive content (Vidgen et al., 2019; Banko et al., 2020) to define hate speech as abuse that is targeted at a protected group or at its members for being a part of that group. We define protected groups based on age, disability, gender identity, familial status, pregnancy, race, national or ethnic origins, religion, sex or sexual orientation, which broadly reflects international legal consensus (particularly the UK’s 2010 Equality Act, the US 1964 Civil Rights Act and the EU’s Charter of Fundamental Rights). Based on these definitions, we approach hate speech detection as the binary classification of content as either hateful or\n1All HATECHECK test cases and annotations are available on https://github.com/paul-rottger/hatecheck-data.\n2www.perspectiveapi.com and www.siftninja.com\nnon-hateful. Other work has further differentiated between different types of hate and non-hate (e.g. Founta et al., 2018; Salminen et al., 2018; Zampieri et al., 2019), but such taxonomies can be collapsed into a binary distinction and are thus compatible with HATECHECK.\nContent Warning This article contains examples of hateful and abusive language. All examples are taken from HATECHECK to illustrate its composition. Examples are quoted verbatim, except for hateful slurs and profanity, for which the first vowel is replaced with an asterisk."
    }, {
      "heading" : "2 HATECHECK",
      "text" : ""
    }, {
      "heading" : "2.1 Defining Model Functionalities",
      "text" : "In software engineering, a program has a certain functionality if it meets a specified input/output behaviour (ISO/IEC/IEEE 24765:2017, E). Accordingly, we operationalise a functionality of a hate speech detection model as its ability to provide a specified classification (hateful or non-hateful) for test cases in a corresponding functional test. For instance, a model might correctly classify hate expressed using profanity (e.g “F*ck all black people”) but misclassify non-hateful uses of profanity (e.g. “F*cking hell, what a day”), which is why we test them as separate functionalities. Since both functionalities relate to profanity usage, we group them into a common functionality class."
    }, {
      "heading" : "2.2 Selecting Functionalities for Testing",
      "text" : "To generate an initial list of 59 functionalities, we reviewed previous hate speech detection research and interviewed civil society stakeholders.\nReview of Previous Research We identified different types of hate in taxonomies of abusive content (e.g. Zampieri et al., 2019; Banko et al., 2020; Kurrek et al., 2020). We also identified likely model weaknesses based on error analyses (e.g. Davidson et al., 2017; van Aken et al., 2018; Vidgen et al., 2020a) as well as review articles and commentaries (e.g. Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Vidgen et al., 2019). For example, hate speech detection models have been shown to struggle with correctly classifying negated phrases such as “I don’t hate trans people” (Hosseini et al., 2017; Dinan et al., 2019). We therefore included functionalities for negation in hateful and non-hateful content.\nInterviews We interviewed 21 employees from 16 British, German and American NGOs whose work directly relates to online hate. Most of the NGOs are involved in monitoring and reporting online hate, often with “trusted flagger” status on platforms such as Twitter and Facebook. Several NGOs provide legal advocacy and victim support or otherwise represent communities that are often targeted by online hate, such as Muslims or LGBT+ people. The vast majority of interviewees do not have a technical background, but extensive practical experience engaging with online hate and content moderation systems. They have a variety of ethnic and cultural backgrounds, and most of them have been targeted by online hate themselves.\nThe interviews were semi-structured. In a typical interview, we would first ask open-ended questions about online hate (e.g. “What do you think are the biggest challenges in tackling online hate?”) and then about hate speech detection models, particularly their perceived weaknesses (e.g. “What sort of content have you seen moderation systems get wrong?”) and potential improvements, unbounded by technical feasibility (e.g. “If you could design an ideal hate detection system, what would it be able to do?”). Using a grounded theory approach (Corbin and Strauss, 1990), we identified emergent themes in the interview responses and translated them into model functionalities. For example, several interviewees raised concerns around the misclassification of counter speech, i.e. direct responses to hateful content (e.g. I4: “people will be quoting someone, calling that person out [...] but that will get picked up by the system”).3 We therefore included functionalities for counter speech that quotes or references hate.\nSelection Criteria From the initial list of 59 functionalities, we select those in HATECHECK based on two practical considerations.\nFirst, we restrict HATECHECK’s scope to individual English language text documents. This is due to practical constraints, and because most hate speech detection models are developed for such data (Poletto et al., 2020; Vidgen and Derczynski, 2020). Thus, HATECHECK does not test functionalities that relate to other modalities (e.g. images)\n3When quoting anonymised responses throughout this article, we identify each interview participant by a unique ID. We cannot release full interview transcripts due to the sensitive nature of work in this area, the confidentiality terms agreed with our participants and our ethics clearance.\nor languages, or that require context (e.g. conversational or social) beyond individual documents.\nSecond, we only test functionalities for which we can construct test cases with clear gold standard labels. Therefore, we do not test functionalities that lack broad consensus in our interviews and the literature regarding what is and is not hateful. The use of humour, for instance, has been highlighted as an important challenge for hate speech research (van Aken et al., 2018; Qian et al., 2018; Vidgen et al., 2020a). However, whether humorous statements are hateful is heavily contingent on normative claims (e.g. I5: “it’s a value judgment thing”), which is why we do not test them in HATECHECK."
    }, {
      "heading" : "2.3 Functional Tests in HATECHECK",
      "text" : "HATECHECK comprises 29 functional tests grouped into 11 classes. Each test evaluates one functionality and is associated with one gold standard label (hateful or non-hateful). Each functional test has a set of corresponding test cases.\n18 functional tests for hateful content in HATECHECK cover distinct expressions of hate. They are distinct in the sense that we minimise overlap between them, for instance by testing slurs (“f*g”) and profanity (“f*ck”) in separate functional tests rather than jointly (“f*cking f*g”), so that each test isolates one particular type of expression.\nThe other 11 functional tests for non-hateful content cover contrastive non-hate, i.e. content which shares linguistic features with hateful expressions. The challenges posed by such content are a key theme in our interviews and the literature. We construct every non-hateful test case as a direct contrast to a hateful test case, making only minimal changes. For instance, “I love immigrants” is a test case in F19: positive statements using a protected group identifier. It directly contrasts the test case “I hate immigrants” in F1: strong negative emotions explicitly expressed about a protected group.\nIn the following, we give a brief overview of the different functional tests in HATECHECK. Table 1 provides corresponding example test cases. Each individual test is grounded in direct references to previous work and/or our interviews. These references are detailed in Appendix B.\nDistinct Expressions of Hate HATECHECK tests different types of derogatory hate speech (F1-4) and hate expressed through threatening language (F5/6). It tests hate expressed using slurs (F7) and profanity (F10). It\nalso tests hate expressed through pronoun reference (F12/13), negation (F14) and phrasing variants, specifically questions and opinions (F16/17). Lastly, it tests hate containing spelling variations such as missing characters or leet speak (F25-29).\nContrastive Non-Hate\nHATECHECK tests non-hateful contrasts for slurs, particularly slur homonyms and reclaimed slurs (F8/9), as well as for profanity (F11). It tests nonhateful contrasts that use negation, i.e. negated hate (F15). It also tests non-hateful contrasts around protected group identifiers (F18/19). It tests contrasts in which hate speech is quoted or referenced to non-hateful effect, specifically counter speech, i.e. direct responses to hate speech which seek to act against it (F20/21). Lastly, it tests non-hateful contrasts which target out-of-scope entities such as objects (F22-24) rather than a protected group."
    }, {
      "heading" : "2.4 Generating Test Cases",
      "text" : "For each functionality in HATECHECK, we handcraft sets of test cases – short English-language text documents that clearly correspond to just one gold standard label. Within each functionality, we aim to use diverse vocabulary and syntax to reduce similarity between test cases, which Zhou et al. (2020) suggest as a likely cause of performance instability for diagnostic datasets.\nTo generate test cases at scale, we use templates (Dixon et al., 2018; Garg et al., 2019; Ribeiro et al., 2020), in which we replace tokens for protected group identifiers (e.g. “I hate [IDENTITY].”) and slurs (e.g. “You are just a [SLUR] to me.”). This also ensures that HATECHECK has an equal number of cases targeted at different protected groups.\nHATECHECK covers seven protected groups: women (gender), trans people (gender identity), gay people (sexual orientation), black people (race), disabled people (disability), Muslims (religion) and immigrants (national origin). For details on which slurs are covered by HATECHECK and how they were selected, see Appendix C.\nIn total, we generate 3,901 cases, 3,495 of which come from 460 templates. The other 406 cases do not use template tokens (e.g. “Sh*t, I forgot my keys”) and are thus crafted individually. The average length of cases is 8.87 words (std. dev. = 3.33) or 48.26 characters (std. dev. = 16.88). 2,659 of the 3,901 cases (68.2%) are hateful and 1,242 (31.8%) are non-hateful.\nSecondary Labels In addition to the primary label (hateful or non-hateful) we provide up to two secondary labels for all cases. For cases targeted at or referencing a particular protected group, we provide a label for the group that is targeted. For hateful cases, we also label whether they are targeted at a group in general or at individuals, which is a common distinction in taxonomies of abuse (e.g. Waseem et al., 2017; Zampieri et al., 2019)."
    }, {
      "heading" : "2.5 Validating Test Cases",
      "text" : "To validate gold standard primary labels of test cases in HATECHECK, we recruited and trained ten annotators.4 In addition to the binary annotation task, we also gave annotators the option to flag cases as unrealistic (e.g. nonsensical) to further confirm data quality. Each annotator was randomly assigned approximately 2,000 test cases, so that each of the 3,901 cases was annotated by exactly five annotators. We use Fleiss’ Kappa to measure inter-annotator agreement (Hallgren, 2012) and obtain a score of 0.93, which indicates “almost perfect” agreement (Landis and Koch, 1977).\nFor 3,879 (99.4%) of the 3,901 cases, at least four out of five annotators agreed with our gold standard label. For 22 cases, agreement was less than four out of five. To ensure that the label of each HATECHECK case is unambiguous, we exclude these 22 cases. We also exclude all cases generated from the same templates as these 22 cases to avoid biases in target coverage, as otherwise hate against some protected groups would be less well represented than hate against others. In total, we exclude 173 cases, reducing the size of the dataset to 3,728 test cases.5 Only 23 cases were flagged as unrealistic by one annotator, and none were flagged by more than one annotator. Thus, we do not exclude any test cases for being unrealistic."
    }, {
      "heading" : "3 Testing Models with HATECHECK",
      "text" : ""
    }, {
      "heading" : "3.1 Model Setup",
      "text" : "As a suite of black-box tests, HATECHECK is broadly applicable across English-language hate speech detection models. Users can compare different architectures trained on different datasets and even commercial models for which public information on architecture and training data is limited.\n4For information on annotator training, their background and demographics, see the data statement in Appendix A.\n5We make data on annotation outcomes available for all cases we generated, including the ones not in HATECHECK.\n(2,563 in 18 functional tests) are labelled hateful, 31.2% (1,165 in 11 functional tests) are labelled non-hateful. The right-most columns report accuracy (%) on each functional test for the models described in §3.1. Best performance on each functional test is bolded. Below random choice performance (<50%) is highlighted in cursive red.\nPre-Trained Transformer Models We test an uncased BERT-base model (Devlin et al., 2019), which has been shown to achieve near state-of-theart performance on several abuse detection tasks (Tran et al., 2020). We fine-tune BERT on two widely-used hate speech datasets from Davidson et al. (2017) and Founta et al. (2018).\nThe Davidson et al. (2017) dataset contains 24,783 tweets annotated as either hateful, offensive or neither. The Founta et al. (2018) dataset comprises 99,996 tweets annotated as hateful, abusive, spam and normal. For both datasets, we collapse labels other than hateful into a single non-hateful label to match HATECHECK’s binary format. This is aligned with the original multi-label setup of the two datasets. Davidson et al. (2017), for instance, explicitly characterise offensive content in their dataset as non-hateful. Respectively, hateful cases make up 5.8% and 5.0% of the datasets. Details on both datasets and pre-processing steps can be found in Appendix D.\nIn the following, we denote BERT fine-tuned on binary Davidson et al. (2017) data by B-D and BERT fine-tuned on binary Founta et al. (2018) data by B-F. To account for class imbalance, we use class weights emphasising the hateful minority class (He and Garcia, 2009). For both datasets, we use a stratified 80/10/10 train/dev/test split. Macro F1 on the held-out test sets is 70.8 for B-D and 70.3 for B-F.6 Details on model training and parameters can be found in Appendix E.\nCommercial Models We test Google Jigsaw’s Perspective (P) and Two Hat’s SiftNinja (SN).7 Both are popular models for content moderation developed by major tech companies that can be accessed by registered users via an API.\nFor a given input text, P provides percentage scores across attributes such as “toxicity” and “profanity”. We use “identity attack”, which aims at identifying “negative or hateful comments targeting someone because of their identity” and thus aligns closely with our definition of hate speech (§1). We convert the percentage score to a binary label using a cutoff of 50%. We tested P in December 2020.\nFor SN, we use its ‘hate speech’ attribute (“attacks [on] a person or group on the basis of personal\n6For better comparability to previous work, we also finetuned unweighted versions of our models on the original multiclass D and F data. Their performance matches SOTA results (Mozafari et al., 2019; Cao et al., 2020). Details in Appx. F.\n7www.perspectiveapi.com and www.siftninja.com\nattributes or identities”), which distinguishes between ‘mild’, ‘bad’, ‘severe’ and ‘no’ hate. We mark all but ‘no’ hate as ‘hateful’ to obtain binary labels. We tested SN in January 2021."
    }, {
      "heading" : "3.2 Results",
      "text" : "We assess model performance on HATECHECK using accuracy, i.e. the proportion of correctly classified test cases. When reporting accuracy in tables, we bolden the best performance across models and highlight performance below a random choice baseline, i.e. 50% for our binary task, in cursive red.\nPerformance Across Labels All models show clear performance deficits when tested on hateful and non-hateful cases in HATECHECK (Table 2). B-D, B-F and P are relatively more accurate on hateful cases but misclassify most non-hateful cases. In total, P performs best. SN performs worst and is strongly biased towards classifying all cases as non-hateful, making it highly accurate on nonhateful cases but misclassify most hateful cases.\nPerformance Across Functional Tests Evaluating models on each functional test (Table 1) reveals specific model weaknesses. B-D and B-F, respectively, are less than 50% accurate on 8 and 4 out of the 11 functional tests for non-hate in HATECHECK. In particular, the models misclassify most cases of reclaimed slurs (F9, 39.5% and 33.3% correct), negated hate (F15, 12.8% and 12.0% correct) and counter speech (F20/21, 26.6%/29.1% and 32.9%/29.8% correct). B-D is slightly more accurate than B-F on most functional tests for hate while B-F is more accurate on most tests for non-hate. Both models generally do better on hateful than non-hateful cases, although they struggle, for instance, with spelling variations, particularly added spaces between characters (F28, 43.9% and 37.6% correct) and leet speak spellings (F29, 48.0% and 43.9% correct). P performs better than B-D and B-F on most functional tests. It is over 95% accurate on 11\nout of 18 functional tests for hate and substantially more accurate than B-D and B-F on spelling variations (F25-29). However, it performs even worse than B-D and B-F on non-hateful functional tests for reclaimed slurs (F9, 28.4% correct), negated hate (F15, 3.8% correct) and counter speech (F20/21, 15.6%/18.4% correct).\nDue to its bias towards classifying all cases as non-hateful, SN misclassifies most hateful cases and is near-perfectly accurate on non-hateful functional tests. Exceptions to the latter are counter speech (F20/21, 79.8%/79.4% correct) and nonhateful slur usage (F8/9, 33.3%/18.5% correct).\nPerformance on Individual Functional Tests Individual functional tests can be investigated further to show more granular model weaknesses. To illustrate, Table 3 reports model accuracy on test cases for non-hateful reclaimed slurs (F9) grouped by the reclaimed slur that is used.\nclaimed slurs (F9, non-hateful ) by which slur is used.\nPerformance varies across models and is strikingly poor on individual slurs. B-Dmisclassifies all instances of “f*g”, “f*ggot” and “q*eer”. B-F and P perform better for “q*eer”, but fail on “n*gga”. SN fails on all cases but reclaimed uses of “b*tch”.\nPerformance Across Target Groups HATECHECK can test whether models exhibit ‘unintended biases’ (Dixon et al., 2018) by comparing their performance on cases which target different groups. To illustrate, Table 4 shows model accuracy on all test cases created from [IDENTITY] templates, which only differ in the group identifier. B-D misclassifies test cases targeting women twice as often as those targeted at other groups. B-F also performs relatively worse for women and fails on most test cases targeting disabled people. By contrast, P is consistently around 80% and SN around 25% accurate across target groups."
    }, {
      "heading" : "3.3 Discussion",
      "text" : "HATECHECK reveals functional weaknesses in all four models that we test.\nFirst, all models are overly sensitive to specific keywords in at least some contexts. B-D, B-F and P perform well for both hateful and non-hateful cases of profanity (F10/11), which shows that they can distinguish between different uses of certain profanity terms. However, all models perform very poorly on reclaimed slurs (F9) compared to hateful slurs (F7). Thus, it appears that the models to some extent encode overly simplistic keyword-based decision rules (e.g. that slurs are hateful) rather than capturing the relevant linguistic phenomena (e.g. that slurs can have non-hateful reclaimed uses).\nSecond, B-D, B-F and P struggle with nonhateful contrasts to hateful phrases. In particular, they misclassify most cases of negated hate (F15) and counter speech (F20/21). Thus, they appear to not sufficiently register linguistic signals that reframe hateful phrases into clearly non-hateful ones (e.g. “No Muslim deserves to die”).\nThird, B-D and B-F are biased in their target coverage, classifying hate directed against some protected groups (e.g. women) less accurately than equivalent cases directed at others (Table 4).\nFor practical applications such as content moderation, these are critical weaknesses. Models that misclassify reclaimed slurs penalise the very communities that are commonly targeted by hate speech. Models that misclassify counter speech undermine positive efforts to fight hate speech. Models that are biased in their target coverage are likely to create and entrench biases in the protections afforded to different groups.\nAs a suite of black-box tests, HATECHECK only offers indirect insights into the source of these weaknesses. Poor performance on functional tests\ncan be a consequence of systematic gaps and biases in model training data. It can also indicate a more fundamental inability of the model’s architecture to capture relevant linguistic phenomena. B-D and B-F share the same architecture but differ in performance on functional tests and in target coverage. This reflects the importance of training data composition, which previous hate speech research has emphasised (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020). Future work could investigate the provenance of model weaknesses in more detail, for instance by using test cases from HATECHECK to “inoculate” training data (Liu et al., 2019).\nIf poor model performance does stem from biased training data, models could be improved through targeted data augmentation (Gardner et al., 2020). HATECHECK users could, for instance, sample or construct additional training cases to resemble test cases from functional tests that their model was inaccurate on, bearing in mind that this additional data might introduce other unforeseen biases. The models we tested would likely benefit from training on additional cases of negated hate, reclaimed slurs and counter speech."
    }, {
      "heading" : "4 Limitations",
      "text" : ""
    }, {
      "heading" : "4.1 Negative Predictive Power",
      "text" : "Good performance on a functional test in HATECHECK only reveals the absence of a particular weakness, rather than necessarily characterising a generalisable model strength. This negative predictive power (Gardner et al., 2020) is common, to some extent, to all finite test sets. Thus, claims about model quality should not be overextended based on positive HATECHECK results. In model development, HATECHECK offers targeted diagnostic insights as a complement to rather than a substitute for evaluation on held-out test sets of real-world hate speech."
    }, {
      "heading" : "4.2 Out-Of-Scope Functionalities",
      "text" : "Each test case in HATECHECK is a separate English-language text document. Thus, HATECHECK does not test functionalities related to context outside individual documents, modalities other than text or languages other than English. Future research could expand HATECHECK to include functional tests covering such aspects.\nFunctional tests in HATECHECK cover distinct expressions of hate and non-hate. Future work could test more complex compound statements,\nsuch as cases combining slurs and profanity. Further, HATECHECK is static and thus does not test functionalities related to language change. This could be addressed by “live” datasets, such as dynamic adversarial benchmarks (Nie et al., 2020; Vidgen et al., 2020b; Kiela et al., 2021)."
    }, {
      "heading" : "4.3 Limited Coverage",
      "text" : "Future research could expand HATECHECK to cover additional protected groups. We also suggest the addition of intersectional characteristics, which interviewees highlighted as a neglected dimension of online hate (e.g. I17: “As a black woman, I receive abuse that is racialised and gendered”).\nSimilarly, future research could include hateful slurs beyond those covered by HATECHECK.\nLastly, future research could craft test cases using more platform- or community-specific language than HATECHECK’s more general test cases. It could also test hate that is more specific to particular target groups, such as misogynistic tropes."
    }, {
      "heading" : "5 Related Work",
      "text" : "Targeted diagnostic datasets like the sets of test cases in HATECHECK have been used for model evaluation across a wide range of NLP tasks, such as natural language inference (Naik et al., 2018; McCoy et al., 2019), machine translation (Isabelle et al., 2017; Belinkov and Bisk, 2018) and language modelling (Marvin and Linzen, 2018; Ettinger, 2020). For hate speech detection, however, they have seen very limited use. Palmer et al. (2020) compile three datasets for evaluating model performance on what they call complex offensive language, specifically the use of reclaimed slurs, adjective nominalisation and linguistic distancing. They select test cases from other datasets sampled from social media, which introduces substantial disagreement between annotators on labels in their data. Dixon et al. (2018) use templates to generate synthetic sets of toxic and non-toxic cases, which resembles our method for test case creation. They focus primarily on evaluating biases around the use of group identifiers and do not validate the labels in their dataset. Compared to both approaches, HATECHECK covers a much larger range of model functionalities, and all test cases, which we generated specifically to fit a given functionality, have clear gold standard labels, which are validated by near-perfect agreement between annotators.\nIn its use of contrastive cases for model eval-\nuation, HATECHECK builds on a long history of minimally-contrastive pairs in NLP (e.g. Levesque et al., 2012; Sennrich, 2017; Glockner et al., 2018; Warstadt et al., 2020). Most relevantly, Kaushik et al. (2020) and Gardner et al. (2020) propose augmenting NLP datasets with contrastive cases for training more generalisable models and enabling more meaningful evaluation. We built on their approaches to generate non-hateful contrast cases in our test suite, which is the first application of this kind for hate speech detection.\nIn terms of its structure, HATECHECK is most directly influenced by the CHECKLIST framework proposed by Ribeiro et al. (2020). However, while they focus on demonstrating its general applicability across NLP tasks, we put more emphasis on motivating the selection of functional tests as well as constructing and validating targeted test cases specifically for the task of hate speech detection."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this article, we introduced HATECHECK, a suite of functional tests for hate speech detection models. We motivated the selection of functional tests through interviews with civil society stakeholders and a review of previous hate speech research, which grounds our approach in both practical and academic applications of hate speech detection models. We designed the functional tests to offer contrasts between hateful and non-hateful content that are challenging to detection models, which enables more accurate evaluation of their true functionalities. For each functional test, we crafted sets of targeted test cases with clear gold standard labels, which we validated through a structured annotation process.\nWe demonstrated the utility of HATECHECK as a diagnostic tool by testing near-state-of-the-art transformer models as well as two commercial models for hate speech detection. HATECHECK showed critical weaknesses for all models. Specifically, models appeared overly sensitive to particular keywords and phrases, as evidenced by poor performance on tests for reclaimed slurs, counter speech and negated hate. The transformer models also exhibited strong biases in target coverage.\nOnline hate is a deeply harmful phenomenon, and detection models are integral to tackling it. Typically, models have been evaluated on held-out test data, which has made it difficult to assess their generalisability and identify specific weaknesses.\nWe hope that HATECHECK’s targeted diagnostic insights help address this issue by contributing to our understanding of models’ limitations, thus aiding the development of better models in the future."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all interviewees for their participation. We also thank reviewers for their constructive feedback. Paul Röttger was funded by the German Academic Scholarship Foundation. Bertram Vidgen and Helen Margetts were supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the “Criminal Justice System” theme within that grant, and the “Hate Speech: Measures & Counter-Measures” project at The Alan Turing Institute. Dong Nguyen was supported by the “Digital Society - The Informed Citizen” research programme, which is (partly) financed by the Dutch Research Council (NWO), project 410.19.007. Zeerak Waseem was supported in part by the Canada 150 Research Chair program and the UK-Canada AI Artificial Intelligence Initiative. Janet B. Pierrehumbert was supported by EPSRC Grant EP/T023333/1.\nImpact Statement\nThis supplementary section addresses relevant ethical considerations that were not explicitly discussed in the main body of our article.\nInterview Participant Rights All interviewees gave explicit consent for their participation after being informed in detail about the research use of their responses. In all research output, quotes from interview responses were anonymised. We also did not reveal specific participant demographics or affiliations. Our interview approach was approved by the Alan Turing Institute’s Ethics Review Board.\nIntellectual Property Rights The test cases in HATECHECK were crafted by the authors. As synthetic data, they pose no risk of violating intellectual property rights.\nAnnotator Compensation We employed a team of ten annotators to validate the quality of the HATECHECK dataset. Annotators were compensated at a rate of £16 per hour. The rate was set 50% above the local living wage (£10.85), although\nall work was completed remotely. All training time and meetings were paid.\nIntended Use HATECHECK’s intended use is as an evaluative tool for hate speech detection models, providing structured and targeted diagnostic insights into model functionalities. We demonstrated this use of HATECHECK in §3. We also briefly discussed alternative uses of HATECHECK, e.g. as a starting point for data augmentation. These uses aim at aiding the development of better hate speech detection models.\nPotential Misuse Researchers might overextend claims about the functionalities of their models based on their test performance, which we would consider a misuse of HATECHECK. We directly addressed this concern by highlighting HATECHECK’s negative predictive power, i.e. the fact that it primarily reveals model weaknesses rather than necessarily characterising generalisable model strengths, as one of its limitations. For the same reason, we emphasised the limits to HATECHECK’s coverage, e.g. in terms of slurs and identity terms."
    }, {
      "heading" : "A Data Statement",
      "text" : "Following Bender and Friedman (2018), we provide a data statement, which documents the generation and provenance of test cases in HATECHECK.\nA. CURATION RATIONALE In order to construct HATECHECK, a first suite of functional tests for hate speech detection models, we generated 3,901 short English-language text documents by hand and by using simple templates for group identifiers and slurs (§2.4). Each document corresponds to one functional test and a binary gold standard label (hateful or non-hateful). In order to validate the gold standard labels, we trained a team of ten annotators, assigning five of them to each document, and asked them to provide independent labels (§2.5). To further improve data quality, we also gave annotators the option to flag cases they felt were unrealistic (e.g. nonsensical), but this flag was not used for any one HATECHECK case by more than one annotator.\nB. LANGUAGE VARIETY HATECHECK only covers English-language text documents. We opted for English language since this maximises HATECHECK’s relevance to previous and current work in hate speech detection, which is mostly concerned with English-language data. Our language choice also reflects the expertise of authors and annotators. We discuss the lack of language variety as a limitation of HATECHECK in §4.2 and suggest expansion to other languages as a priority for future research.\nC. SPEAKER DEMOGRAPHICS Since all test cases in HATECHECK were hand-crafted, the speakers are the same as the authors. Test cases in the test suite were primarily generated by the lead author, who is a researcher at a UK university. The lead author is not a native English speaker but has lived in English-speaking countries for more than five years and has extensively engaged with English-language hate speech in previous research. All test cases were also reviewed by two co-authors, both of whom have worked with English-language hate speech data for more than five years and one of whom is a native English speaker from the UK.\nD. ANNOTATOR DEMOGRAPHICS We recruited a team of ten annotators to work for two weeks. 30% were male and 70% were female. 60% were 18-29 and 40% were 30-39. 20% were educated to high school level, 10% to undergraduate, 60% to taught masters and 10% to research\ndegree (i.e. PhD). 70% were native English speakers and 30% were non-native but fluent. Annotators had a range of nationalities: 60% were British and 10% each were Polish, Spanish, Argentinian and Irish. Most annotators identified as ethnically White (70%), followed by Middle Eastern (20%) and a mixed ethnic background (10%). Annotators all used social media regularly, and 60% used it more than once per day. All annotators had seen other people targeted by online abuse before, and 80% had been targeted personally.\nAll annotators had previously completed annotation work on at least one other hate speech dataset. In the first week, we introduced the binary annotation task to them in an onboarding session and tested their understanding on a set of 100 cases, which we then provided individual feedback on. In the second week, we asked each annotator to annotate around 2,000 test cases so that each case in our test suite was annotated by varied sets of exactly five annotators. Throughout the process, we communicated with annotators in real-time over a messaging platform. We also followed guidance for protecting and monitoring annotator well-being provided by Vidgen et al. (2019).\nE. SPEECH SITUATION All test cases were created between the 23rd of November and the 13th of December 2020.\nF. TEXT CHARACTERISTICS The composition of the dataset, including primary label and secondary labels, is described in detail in §2.3 and §2.4 of the article."
    }, {
      "heading" : "B References for Functional Tests",
      "text" : "F1 – strong negative emotions explicitly expressed about a protected group or its members: Resembles “expressed hatred” (Davidson et al., 2017) and “identity attack” (Banko et al., 2020).\nF2 – explicit descriptions of a protected group or its members using very negative attributes: Refines more general “insult” categories (Davidson et al., 2017; Zampieri et al., 2019).\nF3 – explicit dehumanisation of a protected group or its members: Prevalent form of hate (Mendelsohn et al., 2020; Banko et al., 2020; Vidgen et al., 2020a). Highlighted in our interviews (e.g. I18: “hate crime [often claims] people are inferior and subhuman.”).\nF4 – implicit derogation of a protected group or its members: Closely resembles “implied bias”\n(Sap et al., 2020) and “implicit abuse” (Waseem et al., 2017; Zhang and Luo, 2019). Highlighted in our interviews (e.g. I16: “hate has always been expressed idiomatically”)."
    }, {
      "heading" : "F5 – direct threats against a protected group or",
      "text" : "its members: Core element of several hate speech taxonomies (Golbeck et al., 2017; Zampieri et al., 2019; Vidgen et al., 2020a; Banko et al., 2020)\nF6 – threats expressed as normative statements: Highlighted by an interviewee as a way of avoiding legal consequences to hate speech (I1: “[normative threats] are extremely hateful, but [legally] okay”).\nF7 – hate expressed using slurs: Prevalent way of expressing hate (Palmer et al., 2020; Banko et al., 2020; Kurrek et al., 2020).\nF8 – non-hateful homonyms of slur: Relevant alternative use of slurs (Kurrek et al., 2020).\nF9 – use of reclaimed slurs: Likely source of classification error (Palmer et al., 2020). Highlighted in our interviews (e.g. I7: “A lot of LGBT people use slurs to identify themselves, like reclaim the word queer, and people [...] report that and then that will get hidden”).\nF10 – hate expressed using profanity: Refines more general “insult” categories (Davidson et al., 2017; Zampieri et al., 2019).\nF11 – non-hateful uses of profanity: Oversensitiveness of hate speech detection models to profanity (Davidson et al., 2017; Malmasi and Zampieri, 2018; van Aken et al., 2018).\nF12 – hate expressed through pronoun reference in subsequent clauses: Syntactic relationships and long-range dependencies as model weak points (Burnap and Williams, 2015; Vidgen et al., 2019).\nF13 – hate expressed through pronoun reference in subsequent sentences: See F12.\nF14 – hate expressed using negated positive statements: Negation as an effective adversary for hate speech detection models (Hosseini et al., 2017; Dinan et al., 2019).\nF15 – non-hate expressed using negated hateful statements: See F14.\nF16 – hate phrased as a question: Likely source of classification error (van Aken et al., 2018).\nF17 – hate phrased as an opinion: Highlighted by an interviewee as a way of avoiding legal consequences to hate speech (I1: “If you start a sentence by saying ‘I think that’ [...], the limits of what you can say are much bigger”).\nF18 – neutral statements using protected group identifiers: Oversensitiveness of hate speech de-\ntection models to terms such as “black” and “gay” (Dixon et al., 2018; Park et al., 2018; Kennedy et al., 2020). Also highlighted in our interviews (e.g. I7: “I have seen the algorithm get it wrong, if someone’s saying something like ‘I’m so gay’.”).\nF19 – positive statements using protected group identifiers: See F18.\nF20 – denouncements of hate that quote it: Counter speech as a source of classification error (Warner and Hirschberg, 2012; van Aken et al., 2018; Vidgen et al., 2020a). Most mentioned concern in our interviews (e.g. I4: “people will be quoting someone, calling that person out [...] but that will get picked up by the system”).\nF21 – denouncements of hate that make direct reference to it: See F20.\nF22 – abuse targeted at objects: Distinct from hate speech since it targets out-of-scope entities (Wulczyn et al., 2017; Zampieri et al., 2019).\nF23 – abuse targeted at individuals not referencing membership in a protected group: See F22.\nF24 – abuse targeted at non-protected groups (e.g. professions): See F22.\nF25 – swaps of adjacent characters: Simple misspellings can be challenging for detection models (van Aken et al., 2018; Qian et al., 2018). Particularly relevant to hate speech since they can reflect intentional behaviour of users looking to avoid detection (Hosseini et al., 2017; Gröndahl et al., 2018; Vidgen et al., 2019).\nF26 – missing characters: Highlighted in our interviews (e.g. I7: “it could be a misspelling of a word like ‘f*ggot’, and someone’s put one ‘g’ instead of two”).\nF27 – missing word boundaries: Effective adversary for a hate speech detection model (Gröndahl et al., 2018). Resembles the use of hashtags on social media (I2: “there have been a highly Islamophobic hashtags going around”).\nF28 – added spaces between characters: Effective adversary for a hate speech detection model (Gröndahl et al., 2018). Highlighted in our interviews (e.g. I5: “misspellings, missing letters or additional spaces between the letters.”).\nF29 – leet speak: Resembles “obfuscations” (Nobata et al., 2016; van Aken et al., 2018). Highlighted in our interviews (e.g. I14: “[hate speakers] replace letters with numbers”)."
    }, {
      "heading" : "C Hateful Slurs in HATECHECK",
      "text" : "For each of the seven protected groups covered by HATECHECK, we searched hatebase.org, a crowdsourced hate speech lexicon, for slurs which target that group. From these slurs, we selected the three that were most often logged by users of the site (e.g. “wh*re”, “b*tch” and “sl*t” for women), except for when the third-most sighted slur was logged substantially less often than the second, in which case we selected the top two (e.g. “tr*nny” and “sh*male” for trans people). For immigration status, which is not a target category on hatebase.org, we chose “r*pefugee”, a slur for refugees used by the European far right, and “w*tback”, a slur for Mexican immigrants to the US, which was logged similarly often as other slurs in HATECHECK.\nFor reclaimed slurs (F9), we focus on slurs reclaimed by black communities (particularly “n*gga”), gay communities (“f*g”, “f*ggot”, “q*eer”) and by women (“b*tch”), reflecting the concerns highlighted by our interview participants (e.g. I4: “n*gga would often get [wrongly] picked up by [moderation] systems”). Ahead of the structured annotation process (§2.5) and only for test cases with reclaimed slurs, we asked selfidentifying members of the relevant groups in our personal networks whether they would consider the test cases to contain valid and realistic reclaimed slur uses, which held true for all test cases."
    }, {
      "heading" : "D Datasets for Fine-Tuning",
      "text" : "D.1 Davidson et al. (2017) Data\nSampling Davidson et al. (2017) searched Twitter for tweets containing keywords from a list they compiled from hatebase.org, which yielded a sample of tweets from 33,458 users. They then randomly sampled 25,000 tweets from all tweets of these users.\nAnnotation The authors hired crowd workers from CrowdFlower to annotate each tweet as hateful, offensive or neither. 92.0% of tweets were annotated by three crowd workers, the remainder by at least four and up to nine. For inter-annotator agreement, the authors report a “CrowdFlower score” of 92%.\nData We used 24,783 annotated tweets made available by the authors on github.com/tdavidson/hate-speech-and-offensive-language. 1,430 tweets (5.8%) are labelled hateful, 19,190 (77.4%) offensive and 4,163 (16.8%) neither. We collapse the latter two labels into a single non-hateful label to match HATECHECK’s binary format, resulting in 1,430 tweets (5.8%) labelled hateful and 23,353 (94.2%) labelled non-hateful.\nDefinition of Hate Speech “Language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group”.\nD.2 Founta et al. (2018) Data Sampling Founta et al. (2018) initially collected a random set of 32 million tweets from Twitter. They then used a boosted random sampling procedure based on negative sentiment and occurrence of offensive words as selected from hatebase.org to augment a random subset of this initial sample with tweets they expected to be more likely to be hateful or abusive.\nAnnotation The authors hired crowd workers from CrowdFlower to annotate each tweet as hateful, abusive, spam or normal. All tweets were annotated by five crowd workers. For inter-annotator agreement, the authors report that 55.9% of tweets had four out of five annotators agreeing on a label.\nData The authors provided us access to the full text versions of 99,996 annotated tweets. These correspond to the tweet IDs made available by the authors on github.com/ENCASEH2020/hatespeechtwitter. 4,965 tweets (5.0%) are labelled hateful, 27,150 (27.2%) abusive, 14,030 (14.0%) spam and 53,851 (53.9%) normal. We collapse the latter three labels into a single non-hateful label to match HATECHECK’s binary format, resulting in 4,965 tweets (5.0%) labelled hateful and 95,031 tweets (95.0%) labelled non-hateful.\nDefinition of Hate Speech “Language used to express hatred towards a targeted individual or\ngroup, or is intended to be derogatory, to humiliate, or to insult the members of the group, on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender”.\nD.3 Pre-Processing\nBefore using the datasets for fine-tuning, we lowercase all text and remove newline and tab characters. We replace URLs, user mentions and emojis with [URL], [USER] and [EMOJI] tokens. We also split hashtags into separate tokens using the wordsegment Python package."
    }, {
      "heading" : "E Details on Transformer Models",
      "text" : "Model Architecture We implemented uncased BERT-base models (Devlin et al., 2019) using the transformers Python library (Wolf et al., 2020). Uncased BERT-base, which is trained on lower-cased English text, has 12 layers, a hidden layer size of 768, 12 attention heads and a total of 110 million parameters. For sequence classification, we added a linear layer with softmax output.\nFine-Tuning B-D was fine-tuned on binary Davidson et al. (2017) data and B-F on binary Founta et al. (2018) data. For both datasets, we used a stratified 80/10/10 train/dev/test split. Models were trained for three epochs each. Training batch size was 16. We used cross-entropy loss with class weights emphasising the hateful minority class. Weights were set to the relative proportion of the other class in the training data, meaning that for a 1:9 hateful:non-hateful case split, loss on hateful cases would be multiplied by 9. The optimiser was AdamW (Loshchilov and Hutter, 2019) with a 5e-5 learning rate and a 0.01 weight decay. For regularisation, we set a 10% dropout probability.\nHyperparameter Tuning The number of finetuning epochs, the learning rate and the training batch size were determined by exhaustive grid search. We used the range of possible values recommended by Devlin et al. (2019): [2, 3, 4] for epochs, [2e-5, 3e-5, 5e-5] for learning rate and [16, 32] for batch size. There were 18 training/evaluation runs for each model. The best configuration was selected based on loss on the 10% development set.\nHeld-Out Performance Micro/macro F1 scores on the held-out test sets corresponding to their training data are 91.5/70.8 for B-D (Davidson et al., 2017) and 92.9/70.3 for B-F (Founta et al., 2018).\nComputation We ran all computations on a Microsoft Azure “Standard NC24” server equipped with two NVIDIA Tesla K80 GPU cards. The average wall time for each hyperparameter tuning trial of B-D was around 17 minutes, and for B-F around 70 minutes.\nSource Code Our code is available on github.com/paul-rottger/hatecheck-experiments."
    }, {
      "heading" : "F Comparison to SOTA Results",
      "text" : "Most previous work that trains and evaluates models on Davidson et al. (2017) and Founta et al. (2018) data uses their original multiclass label format. In the multiclass case, the relative size of the hateful class compared to the non-hateful classes is larger than in the binary case, which is likely why most models do not use class weights. For comparability, we thus fine-tuned unweighted multiclass versions of B-D and B-F, using the same model parameters described in Appendix E.\nOn multiclass Davidson et al. (2017) data, Mozafari et al. (2019) report a weighted-average F1 score of 91 for their BERT-base model and 92 for BERTbase combined with a CNN. Cao et al. (2020) report a micro F1 of 89.9 for their ensemble-like “DeepHate” classifier. Our unweighted multiclass BERT-base model achieves 90.7 weighted-average F1 and 91.1 micro F1.\nOn multiclass Founta et al. (2018) data, Cao et al. (2020) report a micro F1 of 79.1 for “DeepHate”. Our unweighted multiclass BERT-base model achieves 81.7 micro F1.\nTran et al. (2020) recently achieved SOTA on several other hate speech datasets with their HABERTOR model. They also find that BERTbase consistently performs very near their SOTA. However, they do not evaluate their models on Davidson et al. (2017) or Founta et al. (2018) data."
    } ],
    "references" : [ {
      "title" : "Challenges for toxic comment classification: An in-depth error analysis",
      "author" : [ "Betty van Aken", "Julian Risch", "Ralf Krestel", "Alexander Löser." ],
      "venue" : "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 33–42, Brussels, Belgium. Asso-",
      "citeRegEx" : "Aken et al\\.,? 2018",
      "shortCiteRegEx" : "Aken et al\\.",
      "year" : 2018
    }, {
      "title" : "A Unified Taxonomy of Harmful Content",
      "author" : [ "Michele Banko", "Brendon MacKeen", "Laurie Ray." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 125–137. Association for Computational Linguistics.",
      "citeRegEx" : "Banko et al\\.,? 2020",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2020
    }, {
      "title" : "Black-box testing: techniques for functional testing of software and systems",
      "author" : [ "Boris Beizer." ],
      "venue" : "John Wiley & Sons, Inc.",
      "citeRegEx" : "Beizer.,? 1995",
      "shortCiteRegEx" : "Beizer.",
      "year" : 1995
    }, {
      "title" : "Synthetic and natural noise both break neural machine translation",
      "author" : [ "Yonatan Belinkov", "Yonatan Bisk." ],
      "venue" : "Proceedings of the 6th International Conference on Learning Representations.",
      "citeRegEx" : "Belinkov and Bisk.,? 2018",
      "shortCiteRegEx" : "Belinkov and Bisk.",
      "year" : 2018
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Cyber hate speech on Twitter: An application of machine classification and statistical modeling for policy and decision making",
      "author" : [ "Pete Burnap", "Matthew L Williams." ],
      "venue" : "Policy & Internet, 7(2):223–242.",
      "citeRegEx" : "Burnap and Williams.,? 2015",
      "shortCiteRegEx" : "Burnap and Williams.",
      "year" : 2015
    }, {
      "title" : "DeepHate: Hate speech detection via multi-faceted text representations",
      "author" : [ "Rui Cao", "Roy Ka-Wei Lee", "Tuan-Anh Hoang." ],
      "venue" : "Proceedings of the 12th ACM Conference on Web Science, pages 11–20.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Grounded theory research: Procedures, canons, and evaluative criteria",
      "author" : [ "Juliet M Corbin", "Anselm Strauss." ],
      "venue" : "Qualitative Sociology, 13(1):3–21.",
      "citeRegEx" : "Corbin and Strauss.,? 1990",
      "shortCiteRegEx" : "Corbin and Strauss.",
      "year" : 1990
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of the 11th International AAAI Conference on Web and Social Media, pages 512–515. As-",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73. Association for",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "A survey on automatic detection of hate speech in text",
      "author" : [ "Paula Fortuna", "Sérgio Nunes." ],
      "venue" : "ACM Computing Surveys (CSUR), 51(4):1–30.",
      "citeRegEx" : "Fortuna and Nunes.,? 2018",
      "shortCiteRegEx" : "Fortuna and Nunes.",
      "year" : 2018
    }, {
      "title" : "Large scale crowdsourcing and characterization of Twitter",
      "author" : [ "Antigoni Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323, Online. Association for Computational",
      "citeRegEx" : "Tsarfaty et al\\.,? 2020",
      "shortCiteRegEx" : "Tsarfaty et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual fairness in text classification through robustness",
      "author" : [ "Sahaj Garg", "Vincent Perot", "Nicole Limtiaco", "Ankur Taly", "Ed H. Chi", "Alex Beutel." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’19, page 219–226,",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "Are we modeling the task or the annotator? An investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Breaking NLI systems with sentences that require simple lexical inferences",
      "author" : [ "Max Glockner", "Vered Shwartz", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Glockner et al\\.,? 2018",
      "shortCiteRegEx" : "Glockner et al\\.",
      "year" : 2018
    }, {
      "title" : "A large labeled corpus for online harass",
      "author" : [ "Jennifer Golbeck", "Zahra Ashktorab", "Rashad O Banjo", "Alexandra Berlinger", "Siddharth Bhagwan", "Cody Buntain", "Paul Cheakalos", "Alicia A Geller", "Rajesh Kumar Gnanasekaran", "Raja Rajan Gunasekaran" ],
      "venue" : null,
      "citeRegEx" : "Golbeck et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Golbeck et al\\.",
      "year" : 2017
    }, {
      "title" : "All you need is “love”: Evading hate speech detection",
      "author" : [ "Tommi Gröndahl", "Luca Pajola", "Mika Juuti", "Mauro Conti", "N Asokan." ],
      "venue" : "Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security, pages 2–12. Association for Comput-",
      "citeRegEx" : "Gröndahl et al\\.,? 2018",
      "shortCiteRegEx" : "Gröndahl et al\\.",
      "year" : 2018
    }, {
      "title" : "Computing inter-rater reliability for observational data: An overview and tutorial",
      "author" : [ "Kevin A. Hallgren." ],
      "venue" : "Tutorials in Quantitative Methods for Psychology, 8(1):23–34.",
      "citeRegEx" : "Hallgren.,? 2012",
      "shortCiteRegEx" : "Hallgren.",
      "year" : 2012
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "Haibo He", "Edwardo A Garcia." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 21(9):1263–1284.",
      "citeRegEx" : "He and Garcia.,? 2009",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "A challenge set approach to evaluating machine translation",
      "author" : [ "Pierre Isabelle", "Colin Cherry", "George Foster." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2486–2496, Copenhagen, Denmark. As-",
      "citeRegEx" : "Isabelle et al\\.,? 2017",
      "shortCiteRegEx" : "Isabelle et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning the difference that makes a difference with counterfactually-augmented data",
      "author" : [ "Divyansh Kaushik", "Eduard Hovy", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 8th International Conference on Learning Representations.",
      "citeRegEx" : "Kaushik et al\\.,? 2020",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2020
    }, {
      "title" : "Contextualizing hate speech classifiers with post-hoc explanation",
      "author" : [ "Brendan Kennedy", "Xisen Jin", "Aida Mostafazadeh Davani", "Morteza Dehghani", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kennedy et al\\.,? 2020",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynabench: Rethinking benchmarking in nlp",
      "author" : [ "Douwe Kiela", "Max Bartolo", "Yixin Nie", "Divyansh Kaushik", "Atticus Geiger", "Zhengxuan Wu", "Bertie Vidgen", "Grusha Prasad", "Amanpreet Singh", "Pratik Ringshia" ],
      "venue" : "arXiv preprint arXiv:2104.14337",
      "citeRegEx" : "Kiela et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards a comprehensive taxonomy and large-scale annotated corpus for online slur usage",
      "author" : [ "Jana Kurrek", "Haji Mohammad Saleem", "Derek Ruths." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 138–149, Online. As-",
      "citeRegEx" : "Kurrek et al\\.,? 2020",
      "shortCiteRegEx" : "Kurrek et al\\.",
      "year" : 2020
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J. Richard Landis", "Gary G. Koch." ],
      "venue" : "Biometrics, 33(1):159.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "The Winograd schema challenge",
      "author" : [ "Hector Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "13th International Conference on the Principles of Knowledge Representation and Reasoning.",
      "citeRegEx" : "Levesque et al\\.,? 2012",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2012
    }, {
      "title" : "Inoculation by fine-tuning: A method for analyzing challenge datasets",
      "author" : [ "Nelson F. Liu", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Challenges in discriminating profanity from hate speech",
      "author" : [ "Shervin Malmasi", "Marcos Zampieri." ],
      "venue" : "Journal of Experimental & Theoretical Artificial Intelligence, 30(2):187–202.",
      "citeRegEx" : "Malmasi and Zampieri.,? 2018",
      "shortCiteRegEx" : "Malmasi and Zampieri.",
      "year" : 2018
    }, {
      "title" : "Targeted syntactic evaluation of language models",
      "author" : [ "Rebecca Marvin", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202, Brussels, Belgium. Association for Computational",
      "citeRegEx" : "Marvin and Linzen.,? 2018",
      "shortCiteRegEx" : "Marvin and Linzen.",
      "year" : 2018
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "A framework for the computational linguistic analysis of dehumanization",
      "author" : [ "Julia Mendelsohn", "Yulia Tsvetkov", "Dan Jurafsky." ],
      "venue" : "Frontiers in Artificial Intelligence, 3:55.",
      "citeRegEx" : "Mendelsohn et al\\.,? 2020",
      "shortCiteRegEx" : "Mendelsohn et al\\.",
      "year" : 2020
    }, {
      "title" : "Tackling online abuse: A survey of automated abuse detection methods",
      "author" : [ "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "arXiv preprint arXiv:1908.06024.",
      "citeRegEx" : "Mishra et al\\.,? 2020",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2020
    }, {
      "title" : "A BERT-based transfer learning approach for hate speech detection in online social media",
      "author" : [ "Marzieh Mozafari", "Reza Farahbakhsh", "Noel Crespi." ],
      "venue" : "International Conference on Complex Networks and Their Applications, pages 928–940. Springer.",
      "citeRegEx" : "Mozafari et al\\.,? 2019",
      "shortCiteRegEx" : "Mozafari et al\\.",
      "year" : 2019
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353,",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "On cross-dataset generalization in automatic detection of online abuse",
      "author" : [ "Isar Nejadgholi", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 173–183, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Nejadgholi and Kiritchenko.,? 2020",
      "shortCiteRegEx" : "Nejadgholi and Kiritchenko.",
      "year" : 2020
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Abusive language detection in online user content",
      "author" : [ "Chikashi Nobata", "Joel Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page 145–153, Republic and",
      "citeRegEx" : "Nobata et al\\.,? 2016",
      "shortCiteRegEx" : "Nobata et al\\.",
      "year" : 2016
    }, {
      "title" : "COLD: Annotation scheme and evaluation data set for complex offensive language in English",
      "author" : [ "Alexis Palmer", "Christine Carr", "Melissa Robinson", "Jordan Sanders." ],
      "venue" : "Journal for Language Technology and Computational Linguistics, pages 1–28.",
      "citeRegEx" : "Palmer et al\\.,? 2020",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing gender bias in abusive language detection",
      "author" : [ "Ji Ho Park", "Jamin Shin", "Pascale Fung." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804, Brussels, Belgium. Association",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "Resources and benchmark corpora for hate speech detection: a systematic review",
      "author" : [ "Fabio Poletto", "Valerio Basile", "Manuela Sanguinetti", "Cristina Bosco", "Viviana Patti." ],
      "venue" : "Language Resources and Evaluation, pages 1–47.",
      "citeRegEx" : "Poletto et al\\.,? 2020",
      "shortCiteRegEx" : "Poletto et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging intra-user and inter-user representation learning for automated hate speech detection",
      "author" : [ "Jing Qian", "Mai ElSherief", "Elizabeth Belding", "William Yang Wang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Qian et al\\.,? 2018",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Anatomy of online hate: Developing a taxonomy and machine learning models for identifying and classifying hate in online news",
      "author" : [ "Joni Salminen", "Hind Almerekhi", "Milica Milenković", "Soon-gyo Jung", "Jisun An", "Haewoon Kwak", "Bernard Jansen" ],
      "venue" : null,
      "citeRegEx" : "Salminen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Salminen et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsex me here: Revisiting sexism detection using psychological scales and adversarial samples",
      "author" : [ "Mattia Samory", "Indira Sen", "Julian Kohne", "Fabian Floeck", "Claudia Wagner." ],
      "venue" : "arXiv preprint arXiv:2004.12764.",
      "citeRegEx" : "Samory et al\\.,? 2020",
      "shortCiteRegEx" : "Samory et al\\.",
      "year" : 2020
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678, Florence,",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain. Associa-",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "How grammatical is characterlevel neural machine translation? Assessing MT quality with contrastive translation pairs",
      "author" : [ "Rico Sennrich." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Sennrich.,? 2017",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2017
    }, {
      "title" : "Predictive biases in natural language processing models: A conceptual framework and overview",
      "author" : [ "Deven Santosh Shah", "H. Andrew Schwartz", "Dirk Hovy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Shah et al\\.,? 2020",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2020
    }, {
      "title" : "HABERTOR: An efficient and effective deep hatespeech detector",
      "author" : [ "Thanh Tran", "Yifan Hu", "Changwei Hu", "Kevin Yen", "Fei Tan", "Kyumin Lee", "Se Rim Park." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
      "author" : [ "Bertie Vidgen", "Leon Derczynski." ],
      "venue" : "PLOS ONE, 15(12):e0243300.",
      "citeRegEx" : "Vidgen and Derczynski.,? 2020",
      "shortCiteRegEx" : "Vidgen and Derczynski.",
      "year" : 2020
    }, {
      "title" : "Detecting East Asian prejudice on social media",
      "author" : [ "Bertie Vidgen", "Scott Hale", "Ella Guest", "Helen Margetts", "David Broniatowski", "Zeerak Waseem", "Austin Botelho", "Matthew Hall", "Rebekah Tromble." ],
      "venue" : "Proceedings of the Fourth Workshop on On-",
      "citeRegEx" : "Vidgen et al\\.,? 2020a",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges and frontiers in abusive content detection",
      "author" : [ "Bertie Vidgen", "Alex Harris", "Dong Nguyen", "Rebekah Tromble", "Scott Hale", "Helen Margetts." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 80–93, Florence, Italy.",
      "citeRegEx" : "Vidgen et al\\.,? 2019",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from the worst: Dynamically generated datasets to improve online hate detection",
      "author" : [ "Bertie Vidgen", "Tristan Thrush", "Zeerak Waseem", "Douwe Kiela." ],
      "venue" : "CoRR, abs/2012.15761.",
      "citeRegEx" : "Vidgen et al\\.,? 2020b",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting hate speech on the World Wide Web",
      "author" : [ "William Warner", "Julia Hirschberg." ],
      "venue" : "Proceedings of the Second Workshop on Language in Social Media, pages 19–26, Montréal, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Warner and Hirschberg.,? 2012",
      "shortCiteRegEx" : "Warner and Hirschberg.",
      "year" : 2012
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:377–392.",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Are you a racist or am I seeing things? Annotator influence on hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138–142, Austin, Texas. Association for Com-",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84, Vancouver, BC, Canada.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Bridging the gaps: Multi-task learning for domain transfer of hate speech detection",
      "author" : [ "Zeerak Waseem", "James Thorne", "Joachim Bingel." ],
      "venue" : "Jennifer Golbeck, editor, Online Harassment. Springer.",
      "citeRegEx" : "Waseem et al\\.,? 2018",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2018
    }, {
      "title" : "Detection of abusive language: The problem of biased datasets",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Thomas Kleinbauer." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Wiegand et al\\.,? 2019",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Errudite: Scalable, reproducible, and testable error analysis",
      "author" : [ "Tongshuang Wu", "Marco Tulio Ribeiro", "Jeffrey Heer", "Daniel Weld." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, pages 1391–1399.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Hate speech detection: A solved problem? The challenging case of long tail on Twitter",
      "author" : [ "Ziqi Zhang", "Lei Luo." ],
      "venue" : "Semantic Web, 10(5):925–945.",
      "citeRegEx" : "Zhang and Luo.,? 2019",
      "shortCiteRegEx" : "Zhang and Luo.",
      "year" : 2019
    }, {
      "title" : "The curse of performance instability in analysis datasets: Consequences, source, and suggestions",
      "author" : [ "Xiang Zhou", "Yixin Nie", "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "2020) and “implicit abuse",
      "author" : [ "Sap" ],
      "venue" : "(Waseem et al.,",
      "citeRegEx" : "Sap,? \\Q2019\\E",
      "shortCiteRegEx" : "Sap",
      "year" : 2019
    }, {
      "title" : "slur uses, which held true for all test cases. D Datasets for Fine-Tuning",
      "author" : [ "Davidson" ],
      "venue" : "Data Sampling Davidson et al",
      "citeRegEx" : "Davidson,? \\Q2017\\E",
      "shortCiteRegEx" : "Davidson",
      "year" : 2017
    }, {
      "title" : "Data Sampling Founta et al. (2018) initially collected a random set of 32 million tweets from Twitter. They then used a boosted random sampling procedure based on negative sentiment and occurrence",
      "author" : [ "D.2 Founta" ],
      "venue" : null,
      "citeRegEx" : "Founta,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta",
      "year" : 2018
    }, {
      "title" : "2018) data uses their original multiclass label format. In the multiclass case, the relative size of the hateful class compared to the non-hateful classes",
      "author" : [ "Davidson" ],
      "venue" : null,
      "citeRegEx" : "Davidson,? \\Q2018\\E",
      "shortCiteRegEx" : "Davidson",
      "year" : 2018
    }, {
      "title" : "2019) report a weighted-average F1 score of 91 for their BERT-base model and 92 for BERTbase combined with a CNN",
      "author" : [ "On multiclass Davidson" ],
      "venue" : null,
      "citeRegEx" : "Davidson,? \\Q2020\\E",
      "shortCiteRegEx" : "Davidson",
      "year" : 2020
    }, {
      "title" : "2020) report a micro F1 of 79.1 for “DeepHate”. Our unweighted multiclass BERT-base model achieves 81.7 micro F1",
      "author" : [ "On multiclass Founta" ],
      "venue" : null,
      "citeRegEx" : "Founta,? \\Q2020\\E",
      "shortCiteRegEx" : "Founta",
      "year" : 2020
    }, {
      "title" : "SOTA on several other hate speech datasets with their HABERTOR model. They also find that BERTbase consistently performs very near their SOTA",
      "author" : [ "Tran" ],
      "venue" : null,
      "citeRegEx" : "Tran,? \\Q2020\\E",
      "shortCiteRegEx" : "Tran",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "So far, hate speech detection models have primarily been evaluated by measuring held-out performance on a small set of widely-used hate speech datasets (particularly Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018), but recent work has highlighted the limitations of this evaluation paradigm.",
      "startOffset" : 152,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "So far, hate speech detection models have primarily been evaluated by measuring held-out performance on a small set of widely-used hate speech datasets (particularly Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018), but recent work has highlighted the limitations of this evaluation paradigm.",
      "startOffset" : 152,
      "endOffset" : 232
    }, {
      "referenceID" : 68,
      "context" : "Aggregate performance metrics offer limited insight into specific model weaknesses (Wu et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 56,
      "context" : "(Vidgen and Derczynski, 2020; Poletto et al., 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al.",
      "startOffset" : 0,
      "endOffset" : 51
    }, {
      "referenceID" : 45,
      "context" : "(Vidgen and Derczynski, 2020; Poletto et al., 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al.",
      "startOffset" : 0,
      "endOffset" : 51
    }, {
      "referenceID" : 62,
      "context" : ", 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al., 2018; Sap et al., 2019) as well as topic and author biases (Wiegand et al.",
      "startOffset" : 51,
      "endOffset" : 104
    }, {
      "referenceID" : 65,
      "context" : ", 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al., 2018; Sap et al., 2019) as well as topic and author biases (Wiegand et al.",
      "startOffset" : 51,
      "endOffset" : 104
    }, {
      "referenceID" : 50,
      "context" : ", 2020), and are known to exhibit annotator biases (Waseem, 2016; Waseem et al., 2018; Sap et al., 2019) as well as topic and author biases (Wiegand et al.",
      "startOffset" : 51,
      "endOffset" : 104
    }, {
      "referenceID" : 66,
      "context" : ", 2019) as well as topic and author biases (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020).",
      "startOffset" : 43,
      "endOffset" : 99
    }, {
      "referenceID" : 39,
      "context" : ", 2019) as well as topic and author biases (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020).",
      "startOffset" : 43,
      "endOffset" : 99
    }, {
      "referenceID" : 44,
      "context" : "spondingly, models trained on such datasets have been shown to be overly sensitive to lexical features such as group identifiers (Park et al., 2018; Dixon et al., 2018; Kennedy et al., 2020), and to generalise poorly to other datasets (Nejadgholi and",
      "startOffset" : 129,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "spondingly, models trained on such datasets have been shown to be overly sensitive to lexical features such as group identifiers (Park et al., 2018; Dixon et al., 2018; Kennedy et al., 2020), and to generalise poorly to other datasets (Nejadgholi and",
      "startOffset" : 129,
      "endOffset" : 190
    }, {
      "referenceID" : 25,
      "context" : "spondingly, models trained on such datasets have been shown to be overly sensitive to lexical features such as group identifiers (Park et al., 2018; Dixon et al., 2018; Kennedy et al., 2020), and to generalise poorly to other datasets (Nejadgholi and",
      "startOffset" : 129,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "ing, also known as black-box testing, is a testing framework from software engineering that assesses different functionalities of a given model by validating its output on sets of targeted test cases (Beizer, 1995).",
      "startOffset" : 200,
      "endOffset" : 214
    }, {
      "referenceID" : 9,
      "context" : "by evaluating two BERT models (Devlin et al., 2019), which have achieved near state-of-the-art performance on hate speech datasets (Tran et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 55,
      "context" : ", 2019), which have achieved near state-of-the-art performance on hate speech datasets (Tran et al., 2020), as well as two commercial models – Google Jigsaw’s Perspective and Two Hat’s SiftNinja.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 60,
      "context" : "Definition of Hate Speech We draw on previous definitions of hate speech (Warner and Hirschberg, 2012; Davidson et al., 2017) as well as recent typologies of abusive content (Vidgen et al.",
      "startOffset" : 73,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Definition of Hate Speech We draw on previous definitions of hate speech (Warner and Hirschberg, 2012; Davidson et al., 2017) as well as recent typologies of abusive content (Vidgen et al.",
      "startOffset" : 73,
      "endOffset" : 125
    }, {
      "referenceID" : 58,
      "context" : ", 2017) as well as recent typologies of abusive content (Vidgen et al., 2019; Banko et al., 2020) to define hate speech as abuse",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : ", 2017) as well as recent typologies of abusive content (Vidgen et al., 2019; Banko et al., 2020) to define hate speech as abuse",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 48,
      "context" : "Other work has further differentiated between different types of hate and non-hate (e.g. Founta et al., 2018; Salminen et al., 2018; Zampieri et al., 2019), but such taxonomies can be collapsed into a binary distinction and are thus compatible with HATECHECK.",
      "startOffset" : 83,
      "endOffset" : 155
    }, {
      "referenceID" : 70,
      "context" : "Other work has further differentiated between different types of hate and non-hate (e.g. Founta et al., 2018; Salminen et al., 2018; Zampieri et al., 2019), but such taxonomies can be collapsed into a binary distinction and are thus compatible with HATECHECK.",
      "startOffset" : 83,
      "endOffset" : 155
    }, {
      "referenceID" : 57,
      "context" : "We also identified likely model weaknesses based on error analyses (e.g. Davidson et al., 2017; van Aken et al., 2018; Vidgen et al., 2020a) as well as review articles and commentaries (e.",
      "startOffset" : 67,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : ", 2020a) as well as review articles and commentaries (e.g. Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Vidgen et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 131
    }, {
      "referenceID" : 58,
      "context" : ", 2020a) as well as review articles and commentaries (e.g. Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Vidgen et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "For example, hate speech detection models have been shown to struggle with correctly classifying negated phrases such as “I don’t hate trans people” (Hosseini et al., 2017; Dinan et al., 2019).",
      "startOffset" : 149,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Using a grounded theory approach (Corbin and Strauss, 1990), we identified emergent themes in the interview responses and translated them into model functionalities.",
      "startOffset" : 33,
      "endOffset" : 59
    }, {
      "referenceID" : 46,
      "context" : "The use of humour, for instance, has been highlighted as an important challenge for hate speech research (van Aken et al., 2018; Qian et al., 2018; Vidgen et al., 2020a).",
      "startOffset" : 105,
      "endOffset" : 169
    }, {
      "referenceID" : 57,
      "context" : "The use of humour, for instance, has been highlighted as an important challenge for hate speech research (van Aken et al., 2018; Qian et al., 2018; Vidgen et al., 2020a).",
      "startOffset" : 105,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "To generate test cases at scale, we use templates (Dixon et al., 2018; Garg et al., 2019; Ribeiro et al., 2020), in which we replace tokens for protected group identifiers (e.",
      "startOffset" : 50,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "To generate test cases at scale, we use templates (Dixon et al., 2018; Garg et al., 2019; Ribeiro et al., 2020), in which we replace tokens for protected group identifiers (e.",
      "startOffset" : 50,
      "endOffset" : 111
    }, {
      "referenceID" : 47,
      "context" : "To generate test cases at scale, we use templates (Dixon et al., 2018; Garg et al., 2019; Ribeiro et al., 2020), in which we replace tokens for protected group identifiers (e.",
      "startOffset" : 50,
      "endOffset" : 111
    }, {
      "referenceID" : 70,
      "context" : "For hateful cases, we also label whether they are targeted at a group in general or at individuals, which is a common distinction in taxonomies of abuse (e.g. Waseem et al., 2017; Zampieri et al., 2019).",
      "startOffset" : 153,
      "endOffset" : 202
    }, {
      "referenceID" : 21,
      "context" : "We use Fleiss’ Kappa to measure inter-annotator agreement (Hallgren, 2012) and obtain a score of 0.",
      "startOffset" : 58,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "93, which indicates “almost perfect” agreement (Landis and Koch, 1977).",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "46 Pre-Trained Transformer Models We test an uncased BERT-base model (Devlin et al., 2019), which has been shown to achieve near state-of-theart performance on several abuse detection tasks (Tran et al.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 55,
      "context" : ", 2019), which has been shown to achieve near state-of-theart performance on several abuse detection tasks (Tran et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "use class weights emphasising the hateful minority class (He and Garcia, 2009).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 37,
      "context" : "Their performance matches SOTA results (Mozafari et al., 2019; Cao et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "Their performance matches SOTA results (Mozafari et al., 2019; Cao et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "Performance Across Target Groups HATECHECK can test whether models exhibit ‘unintended biases’ (Dixon et al., 2018) by comparing their performance on cases which target different groups.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 66,
      "context" : "This reflects the importance of training data composition, which previous hate speech research has emphasised (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020).",
      "startOffset" : 110,
      "endOffset" : 166
    }, {
      "referenceID" : 39,
      "context" : "This reflects the importance of training data composition, which previous hate speech research has emphasised (Wiegand et al., 2019; Nejadgholi and Kiritchenko, 2020).",
      "startOffset" : 110,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "Future work could investigate the provenance of model weaknesses in more detail, for instance by using test cases from HATECHECK to “inoculate” training data (Liu et al., 2019).",
      "startOffset" : 158,
      "endOffset" : 176
    }, {
      "referenceID" : 40,
      "context" : "This could be addressed by “live” datasets, such as dynamic adversarial benchmarks (Nie et al., 2020; Vidgen et al., 2020b; Kiela et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 143
    }, {
      "referenceID" : 59,
      "context" : "This could be addressed by “live” datasets, such as dynamic adversarial benchmarks (Nie et al., 2020; Vidgen et al., 2020b; Kiela et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 143
    }, {
      "referenceID" : 26,
      "context" : "This could be addressed by “live” datasets, such as dynamic adversarial benchmarks (Nie et al., 2020; Vidgen et al., 2020b; Kiela et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 143
    }, {
      "referenceID" : 38,
      "context" : "Targeted diagnostic datasets like the sets of test cases in HATECHECK have been used for model evaluation across a wide range of NLP tasks, such as natural language inference (Naik et al., 2018; McCoy et al., 2019), machine translation (Isabelle",
      "startOffset" : 175,
      "endOffset" : 214
    }, {
      "referenceID" : 34,
      "context" : "Targeted diagnostic datasets like the sets of test cases in HATECHECK have been used for model evaluation across a wide range of NLP tasks, such as natural language inference (Naik et al., 2018; McCoy et al., 2019), machine translation (Isabelle",
      "startOffset" : 175,
      "endOffset" : 214
    }, {
      "referenceID" : 33,
      "context" : ", 2017; Belinkov and Bisk, 2018) and language modelling (Marvin and Linzen, 2018; Ettinger, 2020).",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : ", 2017; Belinkov and Bisk, 2018) and language modelling (Marvin and Linzen, 2018; Ettinger, 2020).",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "49 uation, HATECHECK builds on a long history of minimally-contrastive pairs in NLP (e.g. Levesque et al., 2012; Sennrich, 2017; Glockner et al., 2018; Warstadt et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 174
    }, {
      "referenceID" : 18,
      "context" : "49 uation, HATECHECK builds on a long history of minimally-contrastive pairs in NLP (e.g. Levesque et al., 2012; Sennrich, 2017; Glockner et al., 2018; Warstadt et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 174
    }, {
      "referenceID" : 61,
      "context" : "49 uation, HATECHECK builds on a long history of minimally-contrastive pairs in NLP (e.g. Levesque et al., 2012; Sennrich, 2017; Glockner et al., 2018; Warstadt et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 174
    } ],
    "year" : 2021,
    "abstractText" : "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HATECHECK, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HATECHECK’s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.",
    "creator" : "LaTeX with hyperref"
  }
}