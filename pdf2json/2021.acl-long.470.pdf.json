{
  "name" : "2021.acl-long.470.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Long-Span Summarization via Local Attention and Content Selection",
    "authors" : [ "Potsawee Manakul", "Mark J. F. Gales" ],
    "emails" : [ "pm574@cam.ac.uk,", "mjfg@eng.cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6026–6041\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6026"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer-based models (Vaswani et al., 2017) are ubiquitously state-of-art across many natural language processing (NLP) tasks, including summarization. To achieve the best results, the community has trained ever larger transformer models on larger amount of data, and/or more task-specific optimization objectives (Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020). In long document summarization, the input\n1Our code is available at https://github.com/ potsawee/longsum0.\nsequences could be more than an order of magnitude longer than the limits of these transformer models. Although the limits can be extended, training large transformer models on long sequences is expensive and may not be possible on a standard GPU card because of the self-attention mechanism that grows quadratically with sequence length.\nTo tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020). However, pre-trained weights of the modified models are not readily available. In contrast, standard models such as BERT (Devlin et al., 2019) or BART (Lewis et al., 2020) have been trained on various target tasks, including text summarization (Liu and Lapata, 2019b). This allows practitioners to achieve good performance with less training time. Thus, we are interested in exploiting pretrained models for long-span summarization tasks.\nWe study a range of design configurations empirically and theoretically in regards to memory and compute requirements as well as their performance. We propose that long-span dependencies can be handled by two complementary methods. Firstly, inspired by modified self-attention transformers, we exploit standard transformer models by constraining attention mechanism to be local, allowing longer input spans during training. Secondly, because abstractive summarization systems perform content selection implicitly (Nallapati et al., 2016; Lebanoff et al., 2020), to reduce memory and compute requirements an alternative method is to perform content selection explicitly before the abstractive stage. We study content selection during two phases: training time and test time. At training time, we investigate methods to select data for training fixed-span abstractive models. At test\ntime, we extend existing model-based selection methods, and we propose a multitask content selection method that ranks sentences through extractive labelling based module (Cheng and Lapata, 2016) and attention based module (See et al., 2017). Ultimately, we explore the combined approach, consisting of local self-attention transformer and content selection for long-document summarization.\nWe conduct our experiments using a number of design configurations on the Spotify opendomain Podcast summarization dataset (Clifton et al., 2020). This dataset is challenging not only because of its long-span nature, but also because transcribed spoken utterances typically have lower information density (Li et al., 2019; Manakul et al., 2020). Furthermore, we carry out experiments on arXiv and PubMed datasets (Cohan et al., 2018) to further demonstrate and verify the effectiveness of our approach as well as making comparisons to existing approaches. We highlight the strengths and weaknesses of our approach in different resources and tasks. The main contributions of this paper are:\n• On local self-attention, we show how to exploit a standard transformer model for longspan summarization, and we show good design considerations based on empirical results.\n• On content selection, we demonstrate the best selection method at training time, and we propose a multitask content selection (MCS) method outperforming baselines at test time.\n• Our work has set new state-of-the-art results on Spotify Podcast, arXiv and PubMed datasets in the ROUGE scores. Furthermore, with a small-scale GPU card, our approach achieves comparable or superior performance to previous state-of-the-art systems."
    }, {
      "heading" : "2 Related Work",
      "text" : "Efficient Transformers. Pre-trained transformer models have shown success and become the starting point for various NLP problems such as BERT (Devlin et al., 2019) in contextual representation, GPT2 in text generation (Radford et al., 2019), or BART in seq2seq tasks (Lewis et al., 2020). However, the memory and time requirements for transformer models grow quadratically with the sequence length, and for long-span tasks this quickly leads to GPU running out of memory in training. To mitigate the quadratic nature, a wide range of modified architectures have recently been proposed\n(Tay et al., 2021). They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al., 2020; Tay et al., 2020a), low-rank matrix approximation (Wang et al., 2020), or kernel method (Choromanski et al., 2021). Alternatively, it has been shown that some attention heads are redundant and can be pruned to reduce model size (Voita et al., 2019; Michel et al., 2019). Knowledge distillation reduces memory and compute by compressing a large model to a smaller one (Hinton et al., 2015; Sanh et al., 2019). In contrast, we focus on the dependencies of long input and target sequences in encoder-decoder architectures, and we exploit publicly available transformer models with summarization weights to long-span summarization tasks.\nLong-span Summarization. Efficient transformer architectures have been applied to summarize long documents such as BigBird (Zaheer et al., 2020), and Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020), which has recently been revised parallel to this work.2 Hierarchical transformer architectures have been applied to multi-document summarization (Liu and Lapata, 2019a), and extractive news and table-to-text summarization (Zhang et al., 2019; Narayan et al., 2020). Hierarchical attention RNN system has been applied to summarize long articles (Cohan et al., 2018).\nAlternatively, earlier methods show that good content selection helps abstractive news summarization systems (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018). Hybrid systems that select sentences and generate an abstractive summary have been proposed such as extractive system + TLM for scientific articles (Pilault et al., 2020), simple selection + BART for podcasts (Manakul and Gales, 2020; Song et al., 2020), and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (He et al., 2020; Dou et al., 2021).\nOther work includes dividing the source and target into multiple smaller pairs to train abstractive summarizers (Gidiotis and Tsoumakas, 2020). Extractive methods with and without redundancy reduction techniques for long-span summarization have been studied (Xiao and Carenini, 2019, 2020).\n2On the self-attention aspect, we believe this system is the most comparable to ours (see comparisons in Sec. 6.2)."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "Spotify Podcast.3 The dataset consists of ASR transcripts with human descriptions as summaries (Clifton et al., 2020). We follow the data processing at TREC2020 (Jones et al., 2020) in removing bad transcript-summary pairs from a total of 105,360+1,027 episodes, resulting in train/valid/test splits of 60,415/2,189/1,027 episodes the same as Manakul and Gales (2020).\narXiv and PubMed. Popular long document summarization datasets consist of academic articles with abstracts as summaries (Cohan et al., 2018) and train/valid/test splits of 203,037/6,436/6,440 for arXiv and 119,924/6,633/6,658 for PubMed."
    }, {
      "heading" : "3.2 Models",
      "text" : "BART and LoBART. We use the publicly released BART model (Lewis et al., 2020) fine-tuned on CNNDM (Hermann et al., 2015).4 Following the local window attention in Sparse Transformer (Child et al., 2019) and Longformer (Beltagy et al., 2020), we modify the self-attention mechanism in the encoder to local self-attention (see Figure 2), and we refer to this local self-attention BART as LoBART. It has the same architecture as BART, e.g. the number of parameters, except that we extend positional embedding beyond 1,024 by copying BART’s positional embedding with flipping to allow a smoother transition. See details in Appendix B.1.\n3https://podcastsdataset.byspotify.com 4https://huggingface.co/facebook/\nbart-large-cnn\nHierarchical RNN. The content selection model is based on a hierarchical encoder-decoder architecture that has been shown effective on meeting and long document summarization (Cohan et al., 2018; Zhao et al., 2019; Li et al., 2019). The model consists of word-level and sentence-level GRUs (Cho et al., 2014). We add a linear layer on top of the sentence-level GRU to perform extractive labelling. The sentence-level attention mechanism and extractive labelling modules form our multitask content selection (MCS). More details in Section 5.2.\nWe provide the full details about our implementation, model parameters, hyperparameters, optimizer, and training configurations in Appendix B."
    }, {
      "heading" : "4 Longer Span via Local Self-Attention",
      "text" : "It has been known that memory and compute complexity of transformers is quadratic with the sequence length. However, in encoder-decoder architectures, the exact dependencies on input length N , target length M , and batch size B are less understood. This is particularly important in long-span seq2seq tasks because large memory or compute requirement could make training impractical. Thus, this work studies these dependencies, and shows the trade-off between the size of input span and the size of attention span in local self-attention."
    }, {
      "heading" : "4.1 Memory Analysis and LoBART Design",
      "text" : "Firstly, through a regression analysis for an encoder-decoder architecture such as BART, the\nmemory required in training is:\ncb1 +B(c b 2M + c b 3N + c b 4MN + c b 5M 2 + cb6N 2)\nThe term cb1 depends on only the model size and optimizer, and it is constant (theoretical calculation provided in Appendix A). The remaining terms are activation memory associated with the activation outputs cached for backpropagation, and they grow with N , M , and B. Table 2 shows systemindependent5 regression results for the memory in training BART. It is apparent that as N grows the dominant term is cb6N\n2, which is associated with the encoder self-attention. Thus, this motivates us to modify self-attention only on the encoder side.\nBy introducing local self-attention of width W , the memory in training LoBART becomes:\ncl1 +B(c l 2M + c l 3N + c l 4MN + c l 5M 2 + cl6NW )\nFor large N , the memory is now dominated by cl6NW . The coefficient c l 6 ≈ 1.72cb6, suggesting thatW should be at most 0.58N to reduce memory. We provide more details about the exact theoretical calculation for model and optimizer memory as well as time complexity in Appendix A.\nThe memory for training BART/LoBART in Figure 3 enables us to choose an operating point. Additionally, other complementary techniques for reducing memory in training include: (i) gradientcheckpoint where a subset of intermediate values in the computation graph are cached, and the rest are re-computed during backpropagation (Chen et al., 2016), but this requires changes to optimization and leads to longer training time; (ii) half/mixedprecision training (Micikevicius et al., 2018) that would almost halve y-axis in Figure 3, but this requires changes to the model precision and may result in lower performance; (iii) model parallelism with micro-batching (Huang et al., 2019), but this method requires multiple accelerators."
    }, {
      "heading" : "4.2 BART and LoBART",
      "text" : "We study the characteristics of the full selfattention in BART by defining the mean attention\n5system-independent across hardware and machines; albeit implementation-dependent. This analysis is based on widely used PyTorch and Huggingface implementation.\ndistance in a particular layer and head as follows:\nD = 1\nN N∑ i=1  N∑ j=1 αi,j × |i− j|  (1) where αi,j is the attention weight of position i attending to position j ( ∑N j=1 αi,j = 1). This measure corresponds to the average distance of self-attention. If the attention weight is uniform, DU = N2−1 3N . For N = 1024, DU = 341. In Figure 4, our results show that most layers have a shorter mean distance than DU , supporting that the information is more localized. The mean distances of differently initialized BART models computed on the podcast data also show that the attention mechanism is learned during pre-training stage as there is little variation after the pre-training stage. As illustrated in Figure 4, the average attention distance D of the BART model is around 250-350 tokens. This suggests the window size W should be designed to be above 700, allowing half local attention window W/2 be greater than 250-350 to effectively match BART and to exploit transfer learning more efficiently.\nSubsequently, we train different configurations of BART/LoBART models up to our GPU memory limit of 32GiB. The results in Table 3 show that: (i) expanding the model to accommodate longer input spans improve over the baseline BART(1k) as opposed to Manakul and Gales (2020) that trained longer-span models by freezing bottom layers and did not show any improvement over their baseline; (ii) Although LoBART(8k) with W=512 can process longer input spans than LoBART(4k) with W=1024, it performs worse and we suggest that this is because LoBART(8k)’s window is too small,\ne.g. <700, to utilize transfer learning efficiently and its effective receptive field is also smaller."
    }, {
      "heading" : "5 Longer Span via Content Selection",
      "text" : "Some input sequences still exceed LoBART’s longer fixed-span limit. Further extending the input span would lead to a small local attention span, a diminishing improvement, or GPU running out of memory. Alternatively, it has been shown that a better content selection improves abstractive summarization in news (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018), multi documents (Liu and Lapata, 2019a; Liu et al., 2018), and scientific articles (Pilault et al., 2020). Thus, we propose to tackle the excess length by content selection. Here, we distinguish between two phases of content selection: training time and test time."
    }, {
      "heading" : "5.1 Training-time Content Selection",
      "text" : "During training, ground-truth targets are available. We categorize selection methods in this phase into two types: ground-truth based (model-free), which is also referred to as oracle; and model-based. Ground-truth based methods cannot be used at test time, while model-based methods can be applied at both phases. Although model-based methods do not rely on ground-truth targets, they have the advantage of matching in training and test phases. Existing oracle methods include using ROUGE-2 recall (Liu et al., 2018) or the average of ROUGE-1,2,L recall (Pilault et al., 2020). We discuss model-based methods in Section 5.2, where we propose the MCS method. Let the subscript (i, j) denote the position of the j-th word in the i-th input sentence, the full input X = {x1, ...,xi, ...,xN1} = [x1,1, x1,2, x1,J1︸ ︷︷ ︸\nsent 1\n, ..., xi,1, xi,Ji︸ ︷︷ ︸ sent i , ..., xN1,1, xN1,JN1︸ ︷︷ ︸ sent N1 ].\nContent selection re-ranks, truncates, and sorts X to get Xcs for training BART/LoBART as follows:\nX̄ = {xr1 ,xr2 ,xr3 , ...,xrR} (2) Xcs = SortOrig(TruncateN(X̄)) (3)\nwhere ri is the index of the sentence of rank i, the TruncateN operation filters X̄ such that the total of number of words is less than N , and SortOrig retains the original sentence order. The following ranking methods are considered:\n• Truncation (TRC): rk = k.\n• Model-based: Given the score f of model φ, rk = {i ∈ N1 : fφ(i|X) is ranked k-th}\n• Oracle (ORC): Given the ground-truth summary y and similarity measure d, rk = {i ∈ N1 : d(xi,y) is ranked k-th}\nIn this work, we use ROUGE-2 recall as the similarity measure d. For the ORC method, first, we retain only sentences with positive d, leading to R ≤ N1. We found that the number of sentences with positive d is low at 21.3% of the total number of sentences in average on podcast data. This corresponds to 56% of training instances being shorter than BART input span of 1024.6 This no-padding oracle method (ORCno-pad) is highly aggressive, potentially preventing the downstream summarizer\n6We refer to this percentage as %AgORCno-pad (the percentage of inputs aggressively extracted by the oracle method).\nfrom learning complex abstraction. Hence, we propose variants of oracle methods to extend the ORCno-pad-selected input to the max input span N :\n• ORCpad-lead: Pad by leading unselected sentences and keep the original sentence order.\n• ORCpad-rand: Pad by random unselected sentences and keep the original sentence order.\nIn Figure 5, since any oracle method is considered cheating at test time, the best performance is obtained by MCS (in blue), and the upper bound performance is obtained by optimal oracle method (in green). The results show that although ORCno-pad yields the highest upper bound, the abstractive model in fact does not learn how to perform abstraction. For instance, with TRC or MCS at test time, ORCno-pad yields the lowest performance level. The best way to fine-tune the abstractive model shown in Figure 5 is using ORCpad-rand. Compared to ORCpad-lead, ORCpad-rand is better as it introduces more diversity to the abstractive model. Compared to the model-based method, ORCpad-rand is also computationally less expensive.\nIn addition, Table 5 shows that when there is no content selection at test time (i.e. TRC applied), LoBART(4k) and LoBART(8k) benefit from ORCpad-rand, whereas BART(1k) does not. This is because in the 1k setting, content selection is more aggressive; as a result, the large mismatch between training and test leads to a poor result. Thus, we suggest that the best content selection during training is ORCpad-rand given that content selection will be used at test time, or model’s input span is long."
    }, {
      "heading" : "5.2 Multitask Content Selection (MCS)",
      "text" : "To process long input sequences entirely, we consider RNN, whose memory requirement grows lin-\nearly with the sequence length, and hierarchical architectures which have been shown effective for long seq2seq tasks (Cohan et al., 2018; Li et al., 2019). In this work, the hierarchical RNN model described in Section 3.2 has memory requirement given the target length of 144 during training of 0.83+B(3.96×10−5+3.33×10−5N2)N1,7 where N1 is #sentences, and N2 is the maximum number of words in a sentence, and B is batch size. By setting N1=1000 and N2=50, only 2% of podcast data exceeds this limit, while taking GPU memory to only 2.53GiB for B=1. Thus, this shows that this model can cover long sequences.\nPrevious model-based methods treat content selection as extractive labelling and create labels heuristically (Pilault et al., 2020), or using encoderdecoder attention mechanism (Manakul and Gales, 2020). To utilize both of these in one framework, we propose a Multitask Content Selection (MCS) method where we train the hierarchical encoderdecoder with attention mechanism and a classification layer on top of the encoder (described in Section 3.2). First, the model is trained on seq2seq abstractive summarization objective:\nLseq2seq = − M∑\nm=1\nlogP (ym|y<m,X) (4)\nSecond, we create binary labels as follows: for sentence i, the label zi is 1 if d(xi,y) > 0; else zi is 0, and d is the ROUGE-2 recall measure. The extractive labelling task objective is:\nLlabel = − ∑N1 i=1 (zi log ẑi + (1− zi) log(1− ẑi)) (5)\nẑi = sigmoid(WTclshi + bcls) (6)\nwhere hi is the sentence-level encoder output associated with sentence i, and Wcls,bcls are the parameters of the classification layer. Thus, the MCS training loss is defined as follows:\nLMCS = γLlabel + (1− γ)Lseq2seq (7)\nAt inference stage, there are two modes: (i) standard abstractive summary generation, e.g. via beam search decoding; (ii) ranking input sentences via labelling score and seq2seq attention score. The latter is how we use MCS during inference.8 For sentence i, the scores are:\nscorei,(label) = ẑi, scorei,(seq2seq) = ∑M m=1 α s m,i\n(8) 7Obtained by least-squares regression with 20 samples. 8In practice, we run beam search decoding of width 4,\nand we obtain the attention score from the top beam.\nwhere αsm,i is the sentence-level attention weight at decoder step m over input sentence i. Since the scores are on different scales, rather than using the scores defined in Eq. 8, we simply rank the scores, and then normalize the score ranks into the range 0.0 to 1.0. Let nscore denote the normalized ranking score, the MCS inference score is:\nfφ(i|X) = nscorei,(label) + nscorei,(seq2seq) (9)\nIn our preliminary experiments, we vary the amount of selected sentences from the limit of BART/LoBART to a few sentences, and we found that more aggressive selection at test time degrades the performance. Therefore, our MCS selects input sentences up to the limit of BART/LoBART.\nBy setting γ=0.0, our method is comparable to the attention-based method in Manakul and Gales (2020). By setting γ=1.0, our method is similar to the extractive models in Hsu et al. (2018); Pilault et al. (2020). In Table 4, we show that when coupled with BART, MCS yields better summarization performance than both Attn-only and Ext-only baselines. MCS also achieves higher recall rate of sentences with d(xi,y) > 0 than the two baselines."
    }, {
      "heading" : "6 Combined Approach",
      "text" : ""
    }, {
      "heading" : "6.1 Spotify Podcast results",
      "text" : "In Table 5, a performance gain is obtained in all settings by adding MCS. By comparing different configurations with MCS, it can be seen that the gain from MCS in LoBART(8k) system is the lowest. This is because the average length is 5,727, meaning that many Podcasts inputs to LoBART(8k) do not benefit from content selection.\nCUED-filt, the best single-model system in Manakul and Gales (2020), uses an attention-based content selection at both training and test time, and it is combined with fine-tuned vanilla BART. Our approach outperforms CUED-filt by improved content selection at both training time and test time as\ndemonstrated by BART(1k)-ORC+MCS. Additionally, local self-attention allows training on longer sequences, and our LoBART(4k)-ORC+MCS system has yielded the best results. Lastly, even though LoBART(8k) requires more resource to train, it does not perform as well as LoBART(4k) due to its smaller attention window, and it also has a lower improvement when adding MCS."
    }, {
      "heading" : "6.2 ArXiv and PubMed results",
      "text" : "To verify the effectiveness of our systems, we re-train BART(1k) and LoBART(4k) on arXiv and PubMed datasets. Our training is different from Ext+TLM (Pilault et al., 2020) where their abstractive models are trained using inputs extracted from top two sentences in ROUGE recall for each target sentence without padding, similar to ORCno-pad. Although in 1k setting, ORCno-pad yields %AgORCno-pad (defined in Section 5.1) of only 2.8% on arXiv (12% on PubMed), in 4k setting this is 39% on arXiv (71% on PubMed). Based on the best configurations on podcast data, we train BART(1k) and LoBART(4k) using TRC or ORCpad-rand content selection, and we train the hierarchical model on arXiv/PubMed for MCS.\nArXiv. In Table 6, both BART(1k)+MCS and LoBART(4k)+MCS outperform all existing systems. To better understand the advantages of our approach, the following systems are compared:\nCTRLsum versus our BART(1k) baseline; LED and BigBird versus our LoBART(4k) system.\nCTRLsum extends BART by conditioning it with extracted keywords v using a BERT-based model, e.g. p(y|X,v). Their BERT-based model uses sliding window allowing it to extract v in long sequences, but their BART is still limited to the first 1,024 tokens. As a result, it performs better than BART(1k), but worse than BART(1k)+MCS.\nLoBART(4k) has a similar architecture to LED(4k) without the global attention pattern for special tokens. Instead, our LoBART(4k) benefits from knowledge transferred from CNNDM and the ORCpad-rand training-time content selection, which yields a larger gain when MCS is applied, i.e. the system trained with truncated data has a smaller gain when MCS is applied. Transfer learning comparison and additional results on the impact of ORCpad-rand are provided in Appendix C.\nCompared to BigBird, LoBART(4k) has a longer input span, e.g. 3,072 vs. 4,096. However, BigBird benefits from utilizing more recent summarization specific pre-training Pegasus (Zhang et al., 2020) which is better than our transfer learning. BigBird incorporates a global attention pattern similar to LED, and it also has a random attention pattern. Hence, LoBART without MCS performs worse.\nUltimately, we show that adding MCS to either BART(1k) or LoBART(4k) yields a significant improvement, resulting in state-of-the-art results in both settings. Moreover, although the gain from adding MCS is comparable to the gain observed in extending LED(4k) to LED(16k), the content selection method adds less training cost.\nPubMed. Similarly, LoBART(4k)+MCS achieves state-of-the-art results shown in Table 6. In contrast to the arXiv results, BART(1k)+MCS does not outperform LoBART(4k) nor BigBird, and the gain from MCS is not as high in both 1k and 4k settings."
    }, {
      "heading" : "6.3 Local Attention v.s. MCS.",
      "text" : "Local attention yields better performance on PubMed, while MCS yields better performance on arXiv. To understand this discrepancy, a finegrained analysis is conducted.\nIn Figure 6, we partition the test sets by input lengths, and we evaluate the performance improvement in each partition with respect to the BART(1k) baseline.9 The results illustrate that as the input length N increases:\n• The improvement of systems with MCS increases and subsequently plateaus out.\n• The improvement of systems without MCS decreases once the input exceeds the length limit but then plateaus, suggesting that fixedspan systems without content selection perform worse once the maximum fixed-span is reached. For instance, below 4,000 input words, LoBART(4k) without MCS performs better than BART(1k)+MCS on both datasets.\nTherefore, our MCS method is more effective on arXiv compared to PubMed because the average length of PubMed documents is more than twice shorter than the average length of arXiv documents."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We study two methods for long-span summarization tasks. First, on local self-attention transformers, we present the design considerations for local self-attention BART, and we investigate the feasibility and performance of different network configurations. Second, on content selection, we distinguish between training time and test time methods, and we provide a good practice for both phases. At training time, we show that the oracle method with random sentences padded (ORCpad-rand) yields the best results. At test time, we propose multitask content selection (MCS) that shows an improvement over baselines. We demonstrate that content selection is essential, in particular for longer documents such as the articles in the arXiv dataset. Our BART(1k)+MCS outperforms the current best systems on Podcast and arXiv datasets, and this system does not require a large-scale accelerator in training. Ultimately, by combining local self-attention technique with MCS, our LoBART(4k)+MCS system has set new state-of-the-art results in terms of ROUGE scores in all three long-span summarization tasks. Future work will focus on training our LoBART+MCS system in an end-to-end fashion.\n9For arXiv/PubMed, each test set consists of over 6,000 instances, while Podcast test set has only 1,027 instances. The same analysis is conducted on Podcast, but the results are noisy due to the smaller size of its test set (see Appendix C)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This paper reports on research supported by ALTA institute, Cambridge Assessment English, University of Cambridge, and Cambridge International & St John’s College Scholarship. Thanks to Yiting Lu, Qingyun Dou, Xixin Wu, Raf Czlonka, and Kate Knill for interesting discussions and computing resource support. Thanks to the anonymous reviewers for their helpful comments."
    }, {
      "heading" : "A Detailed Memory & Time Analysis",
      "text" : "Our memory analysis is system-independent, albeit implementation-dependent. We carry out the experiments using PyTorch version 1.2.0. We use pytorch_memlab10 to compute GPU memory during forward and backward passes. Our notation is: input length N , target length M , local self-attention width W , and batch size B."
    }, {
      "heading" : "A.1 BART Memory",
      "text" : "We collect 30 samples, spanning N ∈ [64, 3000] and M ∈ [36, 576] using batch size of 1. Our least-squared regression of the memory equation memory = cb1+B(c b 2M+c b 3N+c b 4MN+c b 5M\n2+ cb6N\n2) yields R2 = 1,RMSE = 0.026, and the coefficients are: cb1 = 6.054, c b 2 = 1.594 × 10−3, cb3 = 8.192 × 10−4, cb4 = 1.418 × 10−6, cb5 = 1.077× 10−6, cb6 = 1.456× 10−6."
    }, {
      "heading" : "Model and Optimizer",
      "text" : "The constant term cb1 = 6.054 GiB is independent of batch size, system, or implementation (given the same floating-point precision). This term comprises model and optimizer memory as follows (in 32-bit floating point, 1 variable takes 4 bytes):\n1. Model Parameter: BART has 406,290,432 parameters, yielding 406290432× 4 = 1.625× 109bytes = 1.51 GiB.\n2. Model Gradient: Each parameter has one corresponding gradient variable, e.g. .grad in PyTorch. Thus, this also occupies 1.51 GiB.\n3. Optimizer: Adam optimizer (Kingma and Ba, 2015) stores first moment and second moment for each and every model parameters, hence, taking 3.02 GiB."
    }, {
      "heading" : "Activation",
      "text" : "The terms corresponding to cb2, ..., c b 6 are associated with activation buffers cached for computing gradients in backpropagation. These terms grow linearly with batch size. The dominant term cb6N\n2B grows quadratically with the input length N , motivating encoder’s local self-attention design.\nChen et al. (2016) proposes a method to save the activation memory by only caching buffers of a subset of layers, and re-computing the rest dynamically during backpropagation. This results in repeated computations and more training time.\n10https://github.com/Stonesjtu/ pytorch_memlab"
    }, {
      "heading" : "A.2 LoBART Memory",
      "text" : "We collect 36 samples, spanning N ∈ [512, 4096], M ∈ [100, 400], and W ∈ [32, 512] using batch size of 1. Our least-squared regression of the memory equation memory = cl1 + B(c l 2M + c l 3N + cl4MN + c l 5M\n2 + cl6NW ) yields RMSE = 0.010, and the coefficients are: cl1 = 6.104, c l 2 = 1.443× 10−3, cl3 = 1.032 × 10−3, cl4 = 1.487 × 10−6, cl5 = 1.277×10−6, cl6 = 2.503×10−6. The model and optimizer memory is similar to the analysis for BART. The activation memory is now dominated by cl6NW ×B, where cl6 = 1.72cb6. Thus, we highlight that once W > 0.58N , LoBART no longer reduces memory. Note that we also tried incorporating the terms N2 and W in the least-squared regression analysis, but their resulting coefficients are small, making both terms negligible. This is expected as quadratic self-attention is replaced by local attention of width W , and the width W only determines the receptive field of each and every position in N , resulting in the NW term."
    }, {
      "heading" : "A.3 Time: BART & LoBART",
      "text" : "Unlike memory, time requirement is both system and implementation dependent. In this analysis, we show the results on our infrastructure consisting of a 32 GiB V100 GPU and 32-core Intel Xeon 4215R CPU (3.20GHz). We compute the time required for 50 forward and backward passes in 12 settings for each model configuration. Similar to the memory analysis, we perform least-squared regression where the results are shown in Figure 7. It can be seen that although LoBART reduces memory requirement, when it comes to time requirement, LoBART is only comparable to BART. This is due to the implementation of local self-attention that involves more processes such as chunking."
    }, {
      "heading" : "1000 2000 3000 4000 5000 6000 7000 8000 9000",
      "text" : "B Implementation Details"
    }, {
      "heading" : "B.1 Models",
      "text" : ""
    }, {
      "heading" : "BART & LoBART.",
      "text" : "We use publicly released BART-large.11 For LoBART, our local self-attention is based on HuggingFace’s implementation (Wolf et al., 2020).12 The number of parameters in BART is 406M.\nThe positional embedding of LoBART beyond 1,024 is created by copying BART’s positional embedding with flipping to allow a smoother transition as shown in Figure 8, and the number of parameters in LoBART(nk) is 406M + 50,264×(n-1)×1,024."
    }, {
      "heading" : "Hierarchical RNN.",
      "text" : "The encoder consists of word-level and sentencelevel bidirectional GRUs. The word-level GRU takes embedding vector ei,j of word i in sentence j, and outputs forward representation h(f)i,j and backward representation h(b)i,j . The sentence-level GRU takes concatenated vector [h(f)Nj ,j ;h (b) 1,j ], and outputs sentence representation hj . The decoder consists of a unidirectional GRU. Each of the encoder GRUs has 2 layers with a dropout layer (p=0.1), and the decoder GRU has 1 layer. There are word-level and sentence-level attention mechanisms connecting the encoder and decoder. The classification head is a single-layer feedforward layer. The dimension of embedding space is 256, and the hidden size is 512. The number of parameters is 52M."
    }, {
      "heading" : "B.2 Training & Inference Hyperparameters",
      "text" : "We process data using the same byte-pair-encoding tokenizer as the BART-large tokenizer, and we use NLTK tokenizer for sentence splitting. We use 32- bit precision training. We stop training when the loss on the validation set stop improving for 3 times. For example, the training steps are approximately:\n11https://huggingface.co/facebook/ bart-large-cnn\n12https://huggingface.co/transformers/\n180k for Podcast; 240k for arXiv; 160k for PubMed. We report the validation performance when training is stopped in Table 10. Adam optimizer is used for all experiments with learning rate:"
    }, {
      "heading" : "B.3 Evaluation",
      "text" : "Our ROUGE (Lin, 2004) scoring tool is pyrouge, which is a wrapper for perl script.13\n13https://pypi.org/project/pyrouge/"
    }, {
      "heading" : "C Additional Results",
      "text" : ""
    }, {
      "heading" : "Losses on Validation Sets",
      "text" : "In Table 10, we show the standard cross entropy losses on validation sets of our BART/LoBART."
    }, {
      "heading" : "BART and LoBART on arXiv/PubMed",
      "text" : "In Table 11, we provide configurations in addition to Table 6. These results (as well as Podcast results in Table 5) show that: in all settings, applying MCS at test time yields a performance gain; and with ORC applied at training, a larger gain is observed."
    }, {
      "heading" : "Transfer Learning from CNN/DailyMail",
      "text" : "In Table 12, we show the impact of transfer learning on fine-tuning BART to Podcast. In Table 13, LED(4k) should be very close to LoBART(4k)TRC-BART-large, we believe that the performance difference is due to the stochastic nature of training.\nNevertheless, our experiments are carried out using the same training setting, e.g. hyperparameters, optimizer, etc. Thus, based on the results, we believe that there is an observable improvement due to transfer learning from CNNDM.\nFine-grained analysis on Podcast test set"
    } ],
    "references" : [ {
      "title" : "ETC: Encoding long and structured inputs in transformers",
      "author" : [ "Joshua Ainslie", "Santiago Ontanon", "Chris Alberti", "Vaclav Cvicek", "Zachary Fisher", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai", "Qifan Wang", "Li Yang." ],
      "venue" : "Proceedings of the 2020 Con-",
      "citeRegEx" : "Ainslie et al\\.,? 2020",
      "shortCiteRegEx" : "Ainslie et al\\.",
      "year" : 2020
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Training deep nets with sublinear memory cost",
      "author" : [ "Tianqi Chen", "Bing Xu", "Chiyuan Zhang", "Carlos Guestrin." ],
      "venue" : "arXiv preprint arXiv:1604.06174.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–686, Mel-",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Neural summarization by extracting sentences and words",
      "author" : [ "Jianpeng Cheng", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 484–494, Berlin, Germany. As-",
      "citeRegEx" : "Cheng and Lapata.,? 2016",
      "shortCiteRegEx" : "Cheng and Lapata.",
      "year" : 2016
    }, {
      "title" : "Generating long sequences with sparse transformers",
      "author" : [ "Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever." ],
      "venue" : "arXiv preprint arXiv:1904.10509.",
      "citeRegEx" : "Child et al\\.,? 2019",
      "shortCiteRegEx" : "Child et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "100,000 podcasts: A spoken English document corpus",
      "author" : [ "Ann Clifton", "Sravana Reddy", "Yongze Yu", "Aasish Pappu", "Rezvaneh Rezapour", "Hamed Bonab", "Maria Eskevich", "Gareth Jones", "Jussi Karlgren", "Ben Carterette", "Rosie Jones." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Clifton et al\\.,? 2020",
      "shortCiteRegEx" : "Clifton et al\\.",
      "year" : 2020
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformer-XL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "GSum: A general framework for guided neural abstractive summarization",
      "author" : [ "Zi-Yi Dou", "Pengfei Liu", "Hiroaki Hayashi", "Zhengbao Jiang", "Graham Neubig." ],
      "venue" : "Proceedings of the 2021 Conference of",
      "citeRegEx" : "Dou et al\\.,? 2021",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2021
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium. Association",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "A divide-and-conquer approach to the summarization of long documents",
      "author" : [ "Alexios Gidiotis", "Grigorios Tsoumakas." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029– 3040.",
      "citeRegEx" : "Gidiotis and Tsoumakas.,? 2020",
      "shortCiteRegEx" : "Gidiotis and Tsoumakas.",
      "year" : 2020
    }, {
      "title" : "CTRLsum: Towards generic controllable text summarization",
      "author" : [ "Junxian He", "Wojciech Kryściński", "Bryan McCann", "Nazneen Rajani", "Caiming Xiong." ],
      "venue" : "arXiv preprint arXiv:2012.04281.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified model for extractive and abstractive summarization using inconsistency loss",
      "author" : [ "Wan-Ting Hsu", "Chieh-Kai Lin", "Ming-Ying Lee", "Kerui Min", "Jing Tang", "Min Sun." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Hsu et al\\.,? 2018",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2018
    }, {
      "title" : "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "author" : [ "Yanping Huang", "Youlong Cheng", "Ankur Bapna", "Orhan Firat", "Dehao Chen", "Mia Xu Chen", "HyoukJoong Lee", "Jiquan Ngiam", "Quoc V. Le", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "In",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Trec 2020 podcasts track overview",
      "author" : [ "Rosie Jones", "Ben Carterette", "Ann Clifton", "Maria Eskevich", "Gareth J.F. Jones", "Jussi Karlgren", "Aasish Pappu", "Sravana Reddy", "Yongze Yu." ],
      "venue" : "The 29th Text Retrieval Conference (TREC) notebook.",
      "citeRegEx" : "Jones et al\\.,? 2020",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "A cascade approach to neural abstractive summarization with content selection and fusion",
      "author" : [ "Logan Lebanoff", "Franck Dernoncourt", "Doo Soon Kim", "Walter Chang", "Fei Liu." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associ-",
      "citeRegEx" : "Lebanoff et al\\.,? 2020",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Keep meeting summaries on topic: Abstractive multi-modal meeting summarization",
      "author" : [ "Manling Li", "Lingyu Zhang", "Heng Ji", "Richard J. Radke." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J. Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancou-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070–5081, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Liu and Lapata.,? 2019a",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019b",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "CUED speech at TREC 2020 podcast summarisation track",
      "author" : [ "Potsawee Manakul", "Mark Gales." ],
      "venue" : "arXiv preprint arXiv:2012.02535.",
      "citeRegEx" : "Manakul and Gales.,? 2020",
      "shortCiteRegEx" : "Manakul and Gales.",
      "year" : 2020
    }, {
      "title" : "Abstractive Spoken Document Summarization Using Hierarchical Model with Multi-Stage Attention Diversity Optimization",
      "author" : [ "Potsawee Manakul", "Mark J.F. Gales", "Linlin Wang." ],
      "venue" : "Proc. Interspeech 2020, pages 4248–4252.",
      "citeRegEx" : "Manakul et al\\.,? 2020",
      "shortCiteRegEx" : "Manakul et al\\.",
      "year" : 2020
    }, {
      "title" : "Are sixteen heads really better than one? In Advances in Neural Information Processing Systems",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : "Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixed precision training",
      "author" : [ "Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory F. Diamos", "Erich Elsen", "David Garcı́a", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu" ],
      "venue" : null,
      "citeRegEx" : "Micikevicius et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Micikevicius et al\\.",
      "year" : 2018
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Çağlar Gulçehre", "Bing Xiang." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Stepwise extractive summarization and planning with structured transformers",
      "author" : [ "Shashi Narayan", "Joshua Maynez", "Jakub Adamek", "Daniele Pighin", "Blaz Bratanic", "Ryan McDonald." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Narayan et al\\.,? 2020",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2020
    }, {
      "title" : "Image transformer",
      "author" : [ "Niki Parmar", "Ashish Vaswani", "Jakob Uszkoreit", "Lukasz Kaiser", "Noam Shazeer", "Alexander Ku", "Dustin Tran." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stock-",
      "citeRegEx" : "Parmar et al\\.,? 2018",
      "shortCiteRegEx" : "Parmar et al\\.",
      "year" : 2018
    }, {
      "title" : "On extractive and abstractive neural document summarization with transformer language models",
      "author" : [ "Jonathan Pilault", "Raymond Li", "Sandeep Subramanian", "Chris Pal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Pilault et al\\.,? 2020",
      "shortCiteRegEx" : "Pilault et al\\.",
      "year" : 2020
    }, {
      "title" : "Blockwise selfattention for long document understanding",
      "author" : [ "Jiezhong Qiu", "Hao Ma", "Omer Levy", "Wen-tau Yih", "Sinong Wang", "Jie Tang." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic summarization of open-domain podcast episodes",
      "author" : [ "Kaiqiang Song", "Chen Li", "Xiaoyang Wang", "Dong Yu", "Fei Liu." ],
      "venue" : "arXiv preprint arXiv:2011.04132.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Sparse sinkhorn attention",
      "author" : [ "Yi Tay", "Dara Bahri", "Liu Yang", "Donald Metzler", "Da-Cheng Juan." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Ma-",
      "citeRegEx" : "Tay et al\\.,? 2020a",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Long range arena : A benchmark for efficient transformers",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Samira Abnar", "Yikang Shen", "Dara Bahri", "Philip Pham", "Jinfeng Rao", "Liu Yang", "Sebastian Ruder", "Donald Metzler." ],
      "venue" : "International Conference on Learning",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "arXiv preprint arXiv:2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020b",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Linformer: Selfattention with linear complexity",
      "author" : [ "Sinong Wang", "Belinda Li", "Madian Khabsa", "Han Fang", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2006.04768.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Extractive summarization of long documents by combining global and local context",
      "author" : [ "Wen Xiao", "Giuseppe Carenini." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Xiao and Carenini.,? 2019",
      "shortCiteRegEx" : "Xiao and Carenini.",
      "year" : 2019
    }, {
      "title" : "Systematically exploring redundancy reduction in summarizing long documents",
      "author" : [ "Wen Xiao", "Giuseppe Carenini." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th In-",
      "citeRegEx" : "Xiao and Carenini.,? 2020",
      "shortCiteRegEx" : "Xiao and Carenini.",
      "year" : 2020
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu." ],
      "venue" : "International Conference on Machine Learning, pages 11328–11339. PMLR.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "author" : [ "Xingxing Zhang", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive meeting summarization via hierarchical adaptive segmental network learning",
      "author" : [ "Zhou Zhao", "Haojie Pan", "Changjie Fan", "Yan Liu", "Linlin Li", "Min Yang." ],
      "venue" : "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17,",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "2016) proposes a method to save the activation memory by only caching buffers of a subset of layers, and re-computing the rest dynamically during backpropagation. This results in repeated computations and more training time",
      "author" : [ "Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2016
    }, {
      "title" : "The impact of transfer learning on initializing LoBART",
      "author" : [ "Beltagy" ],
      "venue" : "C Additional Results",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 47,
      "context" : "Transformer-based models (Vaswani et al., 2017) are ubiquitously state-of-art across many natural language processing (NLP) tasks, including summarization.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "To achieve the best results, the community has trained ever larger transformer models on larger amount of data, and/or more task-specific optimization objectives (Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 244
    }, {
      "referenceID" : 40,
      "context" : "To achieve the best results, the community has trained ever larger transformer models on larger amount of data, and/or more task-specific optimization objectives (Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 244
    }, {
      "referenceID" : 24,
      "context" : "To achieve the best results, the community has trained ever larger transformer models on larger amount of data, and/or more task-specific optimization objectives (Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 244
    }, {
      "referenceID" : 46,
      "context" : "To tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 304
    }, {
      "referenceID" : 22,
      "context" : "To tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 304
    }, {
      "referenceID" : 6,
      "context" : "To tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 304
    }, {
      "referenceID" : 1,
      "context" : "To tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 304
    }, {
      "referenceID" : 0,
      "context" : "To tackle the quadratic characteristic, recent works have modified self-attention mechanism and proposed variants of the transformer such that the quadratic complexity is reduced (Tay et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 179,
      "endOffset" : 304
    }, {
      "referenceID" : 11,
      "context" : "In contrast, standard models such as BERT (Devlin et al., 2019) or BART (Lewis et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : ", 2019) or BART (Lewis et al., 2020) have been trained on various target tasks, including text summarization (Liu and Lapata, 2019b).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 29,
      "context" : ", 2020) have been trained on various target tasks, including text summarization (Liu and Lapata, 2019b).",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "Secondly, because abstractive summarization systems perform content selection implicitly (Nallapati et al., 2016; Lebanoff et al., 2020), to reduce memory and compute requirements an alternative method is to perform content selection explicitly before the abstractive stage.",
      "startOffset" : 89,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "Secondly, because abstractive summarization systems perform content selection implicitly (Nallapati et al., 2016; Lebanoff et al., 2020), to reduce memory and compute requirements an alternative method is to perform content selection explicitly before the abstractive stage.",
      "startOffset" : 89,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "6027 time, we extend existing model-based selection methods, and we propose a multitask content selection method that ranks sentences through extractive labelling based module (Cheng and Lapata, 2016) and attention based module (See et al.",
      "startOffset" : 176,
      "endOffset" : 200
    }, {
      "referenceID" : 42,
      "context" : "6027 time, we extend existing model-based selection methods, and we propose a multitask content selection method that ranks sentences through extractive labelling based module (Cheng and Lapata, 2016) and attention based module (See et al., 2017).",
      "startOffset" : 228,
      "endOffset" : 246
    }, {
      "referenceID" : 8,
      "context" : "We conduct our experiments using a number of design configurations on the Spotify opendomain Podcast summarization dataset (Clifton et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "This dataset is challenging not only because of its long-span nature, but also because transcribed spoken utterances typically have lower information density (Li et al., 2019; Manakul et al., 2020).",
      "startOffset" : 158,
      "endOffset" : 197
    }, {
      "referenceID" : 31,
      "context" : "This dataset is challenging not only because of its long-span nature, but also because transcribed spoken utterances typically have lower information density (Li et al., 2019; Manakul et al., 2020).",
      "startOffset" : 158,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, we carry out experiments on arXiv and PubMed datasets (Cohan et al., 2018) to further demonstrate and verify the effectiveness of our approach as well as making comparisons to existing approaches.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Pre-trained transformer models have shown success and become the starting point for various NLP problems such as BERT (Devlin et al., 2019) in contextual representation, GPT2 in text generation (Radford et al.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : ", 2019) in contextual representation, GPT2 in text generation (Radford et al., 2019), or BART in seq2seq tasks (Lewis et al.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : ", 2019), or BART in seq2seq tasks (Lewis et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 45,
      "context" : "To mitigate the quadratic nature, a wide range of modified architectures have recently been proposed (Tay et al., 2021).",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 36,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 10,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 6,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 38,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 1,
      "context" : "They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (Parmar et al., 2018; Dai et al., 2019; Child et al., 2019; Qiu et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020), learnable patterns (Kitaev et al.",
      "startOffset" : 108,
      "endOffset" : 250
    }, {
      "referenceID" : 22,
      "context" : ", 2020), learnable patterns (Kitaev et al., 2020; Tay et al., 2020a), low-rank matrix approximation (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 44,
      "context" : ", 2020), learnable patterns (Kitaev et al., 2020; Tay et al., 2020a), low-rank matrix approximation (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 49,
      "context" : ", 2020a), low-rank matrix approximation (Wang et al., 2020), or kernel method (Choromanski et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 48,
      "context" : "Alternatively, it has been shown that some attention heads are redundant and can be pruned to reduce model size (Voita et al., 2019; Michel et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 153
    }, {
      "referenceID" : 32,
      "context" : "Alternatively, it has been shown that some attention heads are redundant and can be pruned to reduce model size (Voita et al., 2019; Michel et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "Knowledge distillation reduces memory and compute by compressing a large model to a smaller one (Hinton et al., 2015; Sanh et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "Knowledge distillation reduces memory and compute by compressing a large model to a smaller one (Hinton et al., 2015; Sanh et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : ", 2020), and Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020), which has recently been revised parallel to this work.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "marization (Liu and Lapata, 2019a), and extractive news and table-to-text summarization (Zhang et al.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 54,
      "context" : "marization (Liu and Lapata, 2019a), and extractive news and table-to-text summarization (Zhang et al., 2019; Narayan et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 35,
      "context" : "marization (Liu and Lapata, 2019a), and extractive news and table-to-text summarization (Zhang et al., 2019; Narayan et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "Hierarchical attention RNN system has been applied to summarize long articles (Cohan et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Alternatively, earlier methods show that good content selection helps abstractive news summarization systems (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 173
    }, {
      "referenceID" : 13,
      "context" : "Alternatively, earlier methods show that good content selection helps abstractive news summarization systems (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 173
    }, {
      "referenceID" : 18,
      "context" : "Alternatively, earlier methods show that good content selection helps abstractive news summarization systems (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 173
    }, {
      "referenceID" : 37,
      "context" : "Hybrid systems that select sentences and generate an abstractive summary have been proposed such as extractive system + TLM for scientific articles (Pilault et al., 2020), simple selection + BART for podcasts (Manakul and Gales, 2020; Song et al.",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 30,
      "context" : ", 2020), simple selection + BART for podcasts (Manakul and Gales, 2020; Song et al., 2020), and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (He et al.",
      "startOffset" : 46,
      "endOffset" : 90
    }, {
      "referenceID" : 43,
      "context" : ", 2020), simple selection + BART for podcasts (Manakul and Gales, 2020; Song et al., 2020), and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (He et al.",
      "startOffset" : 46,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : ", 2020), and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (He et al., 2020; Dou et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : ", 2020), and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (He et al., 2020; Dou et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "Other work includes dividing the source and target into multiple smaller pairs to train abstractive summarizers (Gidiotis and Tsoumakas, 2020).",
      "startOffset" : 112,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "3 The dataset consists of ASR transcripts with human descriptions as summaries (Clifton et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "We follow the data processing at TREC2020 (Jones et al., 2020) in removing bad transcript-summary pairs from a total of 105,360+1,027 episodes, resulting",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Popular long document summarization datasets consist of academic articles with abstracts as summaries (Cohan et al., 2018) and train/valid/test splits of 203,037/6,436/6,440 for arXiv and 119,924/6,633/6,658 for PubMed.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "We use the publicly released BART model (Lewis et al., 2020) fine-tuned on CNNDM (Hermann et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "4 Following the local window attention in Sparse Transformer (Child et al., 2019) and Longformer (Beltagy et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : ", 2019) and Longformer (Beltagy et al., 2020), we modify the self-attention mechanism in the encoder to local self-attention (see Figure 2), and we refer to this local self-attention BART as LoBART.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "ture that has been shown effective on meeting and long document summarization (Cohan et al., 2018; Zhao et al., 2019; Li et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 134
    }, {
      "referenceID" : 55,
      "context" : "ture that has been shown effective on meeting and long document summarization (Cohan et al., 2018; Zhao et al., 2019; Li et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "ture that has been shown effective on meeting and long document summarization (Cohan et al., 2018; Zhao et al., 2019; Li et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "The model consists of word-level and sentence-level GRUs (Cho et al., 2014).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Additionally, other complementary techniques for reducing memory in training include: (i) gradientcheckpoint where a subset of intermediate values in the computation graph are cached, and the rest are re-computed during backpropagation (Chen et al., 2016), but this requires changes to optimization and leads to longer training time; (ii) half/mixedprecision training (Micikevicius et al.",
      "startOffset" : 236,
      "endOffset" : 255
    }, {
      "referenceID" : 33,
      "context" : ", 2016), but this requires changes to optimization and leads to longer training time; (ii) half/mixedprecision training (Micikevicius et al., 2018) that would almost halve y-axis in Figure 3, but this requires changes to the model precision and may result in lower performance; (iii) model parallelism with micro-batching (Huang et al.",
      "startOffset" : 120,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : ", 2018) that would almost halve y-axis in Figure 3, but this requires changes to the model precision and may result in lower performance; (iii) model parallelism with micro-batching (Huang et al., 2019), but this method requires multiple accelerators.",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "Alternatively, it has been shown that a better content selection improves abstractive summarization in news (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018), multi documents (Liu and Lapata, 2019a; Liu et al.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Alternatively, it has been shown that a better content selection improves abstractive summarization in news (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018), multi documents (Liu and Lapata, 2019a; Liu et al.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "Alternatively, it has been shown that a better content selection improves abstractive summarization in news (Chen and Bansal, 2018; Gehrmann et al., 2018; Hsu et al., 2018), multi documents (Liu and Lapata, 2019a; Liu et al.",
      "startOffset" : 108,
      "endOffset" : 172
    }, {
      "referenceID" : 28,
      "context" : ", 2018), multi documents (Liu and Lapata, 2019a; Liu et al., 2018), and scientific articles (Pilault et al.",
      "startOffset" : 25,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : ", 2018), multi documents (Liu and Lapata, 2019a; Liu et al., 2018), and scientific articles (Pilault et al.",
      "startOffset" : 25,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "Existing oracle methods include using ROUGE-2 recall (Liu et al., 2018) or the average of ROUGE-1,2,L recall (Pilault et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 37,
      "context" : ", 2018) or the average of ROUGE-1,2,L recall (Pilault et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "To process long input sequences entirely, we consider RNN, whose memory requirement grows linearly with the sequence length, and hierarchical architectures which have been shown effective for long seq2seq tasks (Cohan et al., 2018; Li et al., 2019).",
      "startOffset" : 211,
      "endOffset" : 248
    }, {
      "referenceID" : 25,
      "context" : "To process long input sequences entirely, we consider RNN, whose memory requirement grows linearly with the sequence length, and hierarchical architectures which have been shown effective for long seq2seq tasks (Cohan et al., 2018; Li et al., 2019).",
      "startOffset" : 211,
      "endOffset" : 248
    }, {
      "referenceID" : 37,
      "context" : "Previous model-based methods treat content selection as extractive labelling and create labels heuristically (Pilault et al., 2020), or using encoderdecoder attention mechanism (Manakul and Gales, 2020).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 30,
      "context" : ", 2020), or using encoderdecoder attention mechanism (Manakul and Gales, 2020).",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 37,
      "context" : "Our training is different from Ext+TLM (Pilault et al., 2020) where their abstractive models are trained using inputs extracted from top two sentences in ROUGE recall for each target sentence without padding, similar to ORCno-pad.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "Pr ev io us W or k Abs Discourse-Aware (Cohan et al., 2018) 35.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 53,
      "context" : "However, BigBird benefits from utilizing more recent summarization specific pre-training Pegasus (Zhang et al., 2020) which is better than our transfer learning.",
      "startOffset" : 97,
      "endOffset" : 117
    } ],
    "year" : 2021,
    "abstractText" : "Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pretrained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.1",
    "creator" : "LaTeX with hyperref"
  }
}