{
  "name" : "2021.acl-long.168.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach",
    "authors" : [ "Lu Cheng", "Ahmadreza Mosallanezhad", "Yasin N. Silva", "Deborah L. Hall", "Huan Liu" ],
    "emails" : [ "huanliu}@asu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2158–2168\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2158"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cyberbullying has become a prevalent adverse behavior in online social interactions. Recent findings indicate that over 35% of young people have been victims of cyberbullying and roughly 15% have admitted to cyberbullying others (Hinduja and Patchin, 2020; Kim et al., 2021). The detrimental consequences of cyberbullying have motivated considerable efforts in various fields to combat cyberbullying. For example, in computational studies of cyberbullying detection – which have been largely aimed at classifying text posted on social media\n∗Equal contribution\nplatforms with machine learning and natural language processing (NLP) – the primary goal is to improve the overall accuracy and speediness of detection. Partly due to an increased awareness of the repetitive nature of cyberbullying behavior, a number of recent efforts in cyberbullying detection have shifted in focus from classification of a single text to detection in a social media session. A session typically consists of an image/video with a caption, a sequence of comments, and other social content, e.g., number of likes.\nThe promising results, nevertheless, may come from a deeply biased model that captures, uses, and even amplifies the unintended biases embedded in social media data (Zhang et al., 2020). That is, because humans are biased, human-generated language corpora can introduce human social prejudices into model training processes (Caliskan et al., 2017). Evidence of such bias has been found in toxicity detection (Zhang et al., 2020) and hate speech detection (Davidson et al., 2019), revealing that tweets in African-American Vernacular English (AAVE) are more likely to be classified as abusive or offensive. Similarly, a cyberbullying classifier may simply take advantage of sensitive triggers, e.g., demographic-identity information (e.g.,\n“gay”) and offensive terms (“stupid,” “ni***r”), to make decisions. Indeed, we find that in the Instagram data for benchmarking cyberbullying detection released by (Hosseinmardi et al., 2015), 68.4% of sessions containing the word “gay” are labeled as bullying, 89.4% of sessions containing the word “ni***r,” and 64.3% of sessions containing the word “Mexican”. In Figure 1, we showcase differences in the performance of a standard hierarchical attention network (HAN) (Yang et al., 2016) – a commonly used model for session-based cyberbullying detection – and a HAN that was debiased using our proposed strategy in sessions with and without sensitive triggers using the benchmark Instagram data. Specifically, the x-axis represents the probability of the classifier predicting a session as bullying, i.e., the decision scores F : p(label = bully|Z). The y-axis represents the conditional probability densities of the decision scores, i.e., p(F|Z). Figure 1(a) shows that the densities are dependent on Z and the dependencies are largely reduced by our mitigation strategy, as depicted in Figure 1(b).\nThis paper aims to mitigate the unintended bias in cyberbullying detection in social media sessions. Our task poses multi-faceted challenges that render recent model-agnostic research in fair text classification – especially, data manipulation methods (Dixon et al., 2018; Sun et al., 2019) – inapplicable. First, in contrast to a single text (e.g., a tweet), social media sessions with a sequence of comments contain rich contextual information. Bias mitigation cannot be defined without context (Lee et al., 2020). The axiomatic and absolute definitions may render current interventions (e.g., gender-swapping) ineffective and may even misguide cyberbullying classifiers. Second, session-based cyberbullying detection is a sequential decision-making process rather than a one-off operation. Therefore, current decisions made by a cyberbullying classifier can influence its future predictions and debiasing strategies. Third, these data manipulation methods are impractical in our task due to the need for extra data annotation, which is especially time-consuming for sequential social media data with rich context. In addition, these methods consider fairness through a differentiable loss function that may not directly incorporate specific fairness goals or measures.\nTo address these challenges, we propose a context-aware and model-agnostic debiasing training framework for cyberbullying detection. It does\nnot require additional resources, apart from a predefined set of sensitive triggers. In particular, drawing from recent advances in reinforcement learning (RL), we consider a classifier as an agent that interacts with the environment to accumulate experience in cyberbullying detection and bias mitigation. At each timestep, the agent makes decisions based on all comments observed up to that point in time and is updated by the collected feedback. Empirical evaluations on two real-world datasets show that the proposed debiasing framework can effectively mitigate the unintended biases while improving the performance of cyberbullying detection."
    }, {
      "heading" : "2 Related Work",
      "text" : "Cyberbullying Detection. The growing prevalence of social networking sites and convenient access to digital devices and the internet have substantially expedited information-sharing processes. A byproduct of this, however, has been the increased vulnerability of young people, in particular, to one of the most serious online risks – cyberbullying. To help combat cyberbullying, researchers have used various techniques in machine learning and NLP to automate the process of cyberbullying detection. This is also evidenced by a number of recent competitions and workshops for related tasks such as detection of hate speech against immigrants and women (Basile et al., 2019), offensive language identification (Zampieri et al., 2020), and toxic spans detection (Pavlopoulos et al., 2021).\nEarly works simplified the task as text classification, the input of which are content-based features (e.g., cyberbullying keywords) extracted from a single text (e.g., a tweet) and labels denoting whether the text is relevant to cyberbullying, see, e.g., (Dinakar et al., 2011; Xu et al., 2012). To better leverage the rich information included in social media data, many studies proposed to augment textual features with emotion/sentiment (Dani et al., 2017), social network information such as relational centrality and ego networks (Squicciarini et al., 2015; Huang et al., 2014), and other multi-modal information such as location and time (Cheng et al., 2019b). Extensive experimental results revealed that the improvement of these approaches is significant.\nFrom the data perspective, research in cyberbullying detection has shifted from modeling a single text to multi-modal data and social media sessions. Underpinning these transitions is an increased recognition of two distinct characteris-\ntics of cyberbullying behavior – repetitiveness and power imbalance (Smith et al., 2008). To address these characteristics, studies such as (Cheng et al., 2019a, 2021) proposed to model the structure of a session and temporal dynamics among the comments using HAN. Yet, whereas numerous studies have focused on achieving better prediction performance, these approaches tend to carry or reinforce the unintended social biases in the datasets (Gencoglu, 2020). Our work thus complements earlier research by examining and mitigating unintended bias in cyberbullying detection models.\nFairness in NLP. Humans are inherently biased, and many studies have revealed human biases and discrimination in natural language (Garg et al., 2018; Jentzsch et al., 2019). Evidence has, for instance, emerged in biased pre-trained word embeddings and semantics derived from language corpora. However, in the field of NLP, the question of how to alleviate bias and promote fairness has only more recently begun to be addressed. Using text classification tasks as an example, one predominant method to make the classifiers fairer is to balance training data in a statistical sense. In particular, one can augment original data with external labeled data (Dixon et al., 2018). Similar methods include data oversampling/downsampling, sample weighting (Zhang et al., 2020), and identity term swapping (Park et al., 2018). Dixon et al. (Dixon et al., 2018) added non-toxic samples containing identity terms from Wikipedia articles into training data. A similar strategy was used in (Nozza et al., 2019) for misogyny detection. Badjatiya et al. (Badjatiya et al., 2019) proposed to replace sensitive words with neutral words or tokens.\nThis balancing strategy, while convenient and easy to implement, is not compatible with sessionbased cyberbullying detection. First, practical considerations impede us from providing additional labeled data with specific sensitive triggers. Data labeling for session-based cyberbullying detection is especially time-consuming and labor-intensive, given that it requires carefully examining a media object and all associated comments in a social media session. Second, because there are potentially many words or tokens sensitive to cyberbullying, identity term swapping is almost impossible. Third, social media sessions contain sequences of comments that provide contextual information important for both cyberbullying detection and bias mitigation. Simple data augmentation can result\nin the significant loss of such information. Lastly, balancing can introduce additional calibration parameters that can impair classification performance and bias mitigation (Gencoglu, 2020)."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "Cyberbullying is often characterized as a repeated rather than a one-off behavior (Smith et al., 2008). This unique trait has motivated research that focuses on the detection of cyberbullying in entire social media sessions. In contrast to a single text, e.g., a Facebook comment or a tweet, a social media session is typically composed of an initial post (e.g., an image with a caption), a sequence of comments from different users, timestamps, spatial location, user profile information, and other social content such as number of likes (Cheng et al., 2020). Session-based cyberbullying detection presents a number of characteristics such as multi-modality and user interaction (Cheng et al., 2020). In this work, because our goal is to mitigate bias in natural language, we focus on text (i.e., a sequence of comments) in a social media session. We formally define session-based cyberbullying detection as follows:\nDefinition (Cyberbullying Detection in a Social Media Session). We consider a corpus ofN social media sessions C = {∫1, ∫2, ..., ∫N}, in which each session consists of a sequence of comments denoted as {c1, ..., cC}. A session is labeled as either y = 1 denoting a bullying session or y = 0 denoting a non-bullying session. LetD be the dimension of extracted textual features (e.g., Bag of Words) xi for ci. Session-based cyberbullying detection aims to learn a binary classifier using a sequence of textual data to identify if a social media session is a cyberbullying instance:\nF : {x1, ...,xC} ∈ RD → {0, 1}. (1)"
    }, {
      "heading" : "4 Proposed Method",
      "text" : "An unbiased model for cyberbullying detection makes decisions based on the semantics in a social media session instead of sensitive triggers potentially related to cyberbullying, such as “gay,” “black,” or “fat.” In the presence of unintended bias, a model may present high performance for sessions with these sensitive triggers without knowing their semantics (Dixon et al., 2018). In this section, we first discuss how to define and assess bias in the context of session-based cyberbullying detection.\nWe then present the details of our bias mitigation strategy."
    }, {
      "heading" : "4.1 Assessing Bias",
      "text" : "Bias in a text classification model can be assessed by the False Negative Equality Difference (FNED) and False Positive Equality Difference (FPED) metrics, as used in previous studies such as (Zhang et al., 2020; Gencoglu, 2020; Huang et al., 2020). They are a relaxation of Equalized Odds (Borkan et al., 2019) and defined as\nFNED = ∑ z |FNRz − FNRoverall|, (2)\nFPED = ∑ z |FPRz − FPRoverall|, (3)\nwhere z denotes cyberbullying-sensitive triggers, such as “gay,” “black,” and “Mexican.” The complete list of sensitive triggers can be found in Appendix A. FNRoverall and FPRoverall denote the False Negative Rate and False Positive Rate over the entire training dataset. Similarly, FNRz and FPRz are calculated over the subset of the data containing the sensitive triggers. An unbiased cyberbullying model meets the following condition:\nP (Ŷ |Z) = P (Ŷ ), (4) where Ŷ stands for the predicted label. By Equation 4, we imply that Ŷ is independent of the cyberbullying-sensitive triggers Z –that is, a debiased model performs similarly for sessions with and without Z.\nNote that the widely-used non-discrimination evaluation sets – Identity Phrase Templates Test Sets (IPTTS) (Dixon et al., 2018) – are not applicable to our task. IPTTS are generated by predefined templates with slots for specific terms, e.g., “I am a boy” and “I am a girl.” They only include examples for single text, whereas a social media session includes a sequence of comments. As we will show in subsection 5.1, the average number of comments in the Instagram dataset is 72, which can pose great challenges for generating synthetic social media sessions and the labeling process."
    }, {
      "heading" : "4.2 Mitigating Bias",
      "text" : "Essentially, a debiasing session-based cyberbullying detection is a sequential decision-making process where decisions are updated periodically to assure high performance. In this debiasing framework, comments arrive and are observed sequentially. At each timestep, two decisions are made\nbased on the feedback from past decisions: (1) predicting whether a session is bullying and (2) gauging the performance differences between sessions with and without sensitive triggers. Our debiasing strategy is built on the recent results of RL (Shi et al., 2018; Zou et al., 2019; Mosallanezhad et al., 2019), particularly, the sequential Markov Decision Process (MDP). In this approach, an agent A interacts with an environment over discrete time steps t: the agent selects action at in response to state st. at causes the environment to change its state from st to st+1 and returns a reward rt+1. Therefore, each interaction between the agent and the environment creates an experience tuple Mt = (st, at, st+1, rt+1). The experience tuple is used to train the agent A through different interactions with the environment. The agent’s goal is to excel at a specific task, such as generating text (Shi et al., 2018) or summarizing text (Keneshloo et al., 2019).\nIn this work, we leverage techniques in RL to alleviate the unintended bias when classifying social media sessions into bullying or non-bullying based on user comments. In particular, we consider a standard classifier F (e.g., HAN) as an RL agent and a sequence of comments observed at time {1, 2, ..., t} as state st. The agent selects an action at ∈ {non-bullying, bullying} according to a policy function π(st). π(st) indicates the probability distribution of actions a in response to state st, whereas π(st, at) shows the probability of choosing action at in response to state st. The action can\nbe interpreted as the predicted label ŷ using the input comments. The reward rt+1 is then calculated for the state-action set (st, at) and the cumulative discounted sum of rewards Gt is used to optimize the policy function π(st).\nBelow, we provide details of the (1) environment, (2) states, (3) actions, and (4) the reward function for the proposed debiasing approach.\n• Environment is a session comments loader. At each episode, the environment chooses a single session and returns its first t comments as state st. As such, states are independent from the agent’s actions, as they do not affect the next state. When it reaches the maximum number of comments of the selected session C, the process is terminated.\n• State st is a sequence of comments in a social media session posted by various users from time 1 through time t.\n• Action at determines a session to be bullying or not, given the input comments or state st:\nat ∈ {bullying, non-bullying}. (5)\n• Reward function R is used to optimize the policy function π(st, at). It is defined based on how successfully the agent predicts the label for the input state st and how much bias the classifier currently has. We define the bias of a classifier as the harmonic mean of FPED and FNED characterized by the sensitive triggers in cyberbullying. In a debiased classifier, we expect both FPED and FNED to be close to zero. We define the reward function R as\nR = −lF − β × 2× FPED× FNED\nFPED + FNED , (6)\nwhere l indicates the prediction error of the classifier and β balances between prediction and the debiasing effect of F . The reward function is calculated based on all sessions in the environment, evaluating the performance and bias of the classifier."
    }, {
      "heading" : "4.3 Optimization Algorithm",
      "text" : "Given the environment, state, actions, and the reward function, we aim to learn the optimal action selection strategy π(st, at). At each timestep t, the agent classifies a session with t comments and the reward rt+1 is calculated using Equation 6, according to the agent’s action at and state st. The goal\nAlgorithm 1 The Optimization Algorithm Require: The dataset {x, z, y}, initialized\nπθ(s0, a0), discount rate γ, balancing weight β, learning rate lr, number of episode E. 1: while Episode e < E do 2: Initialize st,M 3: for t ∈ {0, 1, ..., C} do 4: A selects action at according to distribution π(st) 5: M ←M + (st, at, rt+1, st+1) 6: st ← st+1 7: for each timestep t, reward in Mt do 8: Gt ← ∑t i=1 γ\niri+1 9: end for\n10: Calculate mean policy loss for all timesteps according to Equation 8. 11: Update the policy according to Equation 7. 12: end for 13: end while\nof the agent is to maximize its reward according to Equation 6. We use the policy gradient algorithm – REINFORCE (Sutton et al., 1999) – to train the agent. As such, the agent has similar properties to a classifier and the classifier’s output distribution can be mapped to the agent’s policy function π(st, at). We use the following function to update the agent:\n∆θ = lr∇θL(θ), (7)\nwhere lr denotes the learning rate, θ is the parameter w.r.t. the policy function πθ(st, at), and L(θ) indicates the policy loss:\nL(θ) = log(πθ(st, at) ·Gt), (8)\nwhere Gt = ∑t i=1 γ iri+1 is the cumulative sum of rewards with discount rate γ. The pseudo-code for the optimization algorithm can be seen in Algorithm 1."
    }, {
      "heading" : "5 Evaluation",
      "text" : "In this section, we conduct both quantitative and qualitative evaluations to examine the efficacy of our debiasing strategy.1 In particular, we show that our method can effectively mitigate the impacts of unintended data biases without impairing the model’s prediction performance by answering:\n1The source code is publicly available at https://github.com/GitHubLuCheng/MitigateBiasSessionCB\n(1) Can we mitigate the unintended bias of machine learning models for detecting cyberbullying sessions by leveraging techniques in RL? (2) If so, will this debiasing strategy impair the cyberbullying detection performance? and (3) If ‘no’ to (2), what is the source of gain?"
    }, {
      "heading" : "5.1 Data.",
      "text" : "Two benchmark datasets for cyberbullying detection – Instagram (Hosseinmardi et al., 2015) and Vine (Rafiq et al., 2015) – are used for empirical evaluation. The number of sessions in Instagram and Vine is 2,218 and 970, respectively. Both datasets were crawled using a snowball sampling method and manually annotated via the crowdsourcing platform CrowdFlower.2 Sessions containing less than 15 comments were removed to ensure data annotation quality. Annotators were asked to examine the image/video, associated caption, and all of the comments in a session before making the final decisions. Instagram: Instagram3 is a social networking site ranked as one of the top five networks with the highest percentage of users reporting experiences of cyberbullying (the Label Anti Bullying Charity, 2013). Each social media session consists of image content, a corresponding caption, and a sequence of comments in temporal order. In total, this dataset is composed of 2,218 sessions, with an average number of 72 comments in each session. Vine: Vine4 was a mobile application that allowed users to upload and comment on six-second looping videos. Each social media session consists of video content, the corresponding caption, and a sequence of comments in temporal order. This dataset contains 970 sessions and each session contains, on average, 81 comments."
    }, {
      "heading" : "5.2 Experimental Setup",
      "text" : "For social media sessions, standard fairness methods, such as identity swapping and data supplementation, are not applicable. We compare our approach with commonly used machine learning\n2https://www.figure-eight.com/ 3https://www.instagram.com/ 4https://vine.co/. It was shut down in 2017.\nmodels for classification with sequential text data, including HAN, Convolutional Neural Network (CNN), and Gated Recurrent Unit (GRU), as well as a recent model proposed for session-based cyberbullying detection – HANCD (Cheng et al., 2019a). HANCD leverages multi-task learning to jointly model the hierarchical structure of a social media session and the temporal dynamics of its sequence of comments to improve the performance of cyberbullying detection.\nWe also include the state-of-the-art model Constrained (Gencoglu, 2020) that imposes two fairness constraints on cyberbullying detection to mitigate biases. In our implementation, we use the HANCD classifier as the cyberbullying model in Constrained for a fair comparison. The parameter w.r.t. the fairness constraints is set to 0.005, as suggested. Both HAN and HANCD use GRU to extract the context of the input data. We use 1-layer GRUs with a hidden size of 100 and 200 neurons for word and comment attention networks, respectively. As our approach is model-agnostic, for each standard machine learning model, there is a corresponding debiased counterpart.\nFor the proposed method, lF in the reward function (Equation 6) is computed as the cross entropy loss between the true label y and the predicted probability p:\nlF = − 1\n2 2∑ i=1 yi log(pi)+(1−yi) log(1−pi). (9)\nIn Algorithm 1, the classifier F is pre-trained for 5 iterations using loss function lF , learning rate 3e− 3, and the Adam optimizer (Kingma and Ba, 2014). F is then placed in the RL setting discussed in subsection 4.2. We apply the REINFORCE method with E = 500 episodes, learning rate 1e− 5, β = 1.0, and γ = 0.5 using the Adam optimizer to further update the classifier.\nEvaluations focus on both the prediction accuracy and the debiasing effect of a model. For prediction performance, we adopt standard metrics for binary classification, including Precision, Recall, F1, and AUC scores. Following (Zhang et al., 2020; Gencoglu, 2020), we use FPED, FNED, and total bias (FPED+FNED) to evaluate how biased a model is w.r.t. sessions with and without sensitive triggers. Lower scores indicate less bias. For all models, pre-trained GloVe word embeddings (Pennington et al., 2014) and 10-fold cross validation with 80/20 split are used for fair comparison.\nz |FNRz − FNRoverall| and FPED =∑\nz |FPRz − FPRoverall|. Values closer to 0 indicate better equity. Best viewed in color.\nFurthermore, we perform McNemar’s test to examine whether a statistically significant difference between baseline and debiased models exists in terms of cyberbullying classification accuracy and equity. The best results are highlighted in bold font.\n5.3 Can we mitigate unintended bias?\nIn this section, we show experimental results to answer the first question: “Can the proposed framework mitigate unintended bias?” As expected, the proposed RL framework can effectively mitigate the impact of the unintended bias embedded in the datasets for cyberbullying detection. We report results for both Instagram and Vine in Table 2. “De-” denotes a debiased model, e.g., De-HAN is a HAN debiased by the proposed RL framework. “Total” stands for the total bias (FPED+FNED). All McNemar’s tests resulted in statistical significance with p-values < 0.05.\nWe observe the following: (1) Compared to the\nstandard classifiers, the debiased counterparts significantly improve FNED and FPED scores, indicating that our proposed debiasing strategy can mitigate the unintended bias in data used for predicting cyberbullying sessions, regardless of the dataset or machine learning model. For example, when tested on Instagram with the HAN model, our debiasing method can decrease FPED, FNED, and total bias by 95.7%, 56.7%, and 57.0%, respectively. For Vine, the improvement with HAN is 71.4%, 3.3%, and 50.5%, respectively. (2) Total biases of standard classifiers come from both the FPRs and FNRs for the Instagram experiments, while the main contributor of biases is the FPRs for the Vine experiments. Our approach mitigates total bias in both scenarios. (3) Our debiasing strategy based on RL techniques is also more effective than the fairness constraints proposed in (Gencoglu, 2020), as indicated by the decreased total biases for both Instagram and Vine. By comparing HANCD, Constrained, and De-HANCD, we see that Constrained decreases FPED by sacrificing FNED, while DeHANCD can decrease both.\nIn addition to the quantitative results, we provide qualitative analyses by visualizing FPED and FNED of both the standard and debiased HANCD models. In an experiment with Instagram for sessions containing ten sensitive triggers, as illustrated in Figure 3, we can observe that compared to De-HANCD, HANCD is more biased toward some sensitive triggers, such as “fat” and “stupid.” Demographic-identity related bias is also detected in HANCD. For example, sessions containing identity terms including “ne**o,” “gay,” and “ni**a” are more likely to be falsely identified as “bullying,” as indicated by FPED. By contrast, De-HANCD mitigates various types of unintended biases and has more consistent performance across all of the sensitive triggers."
    }, {
      "heading" : "5.4 Is there a trade-off between accuracy and bias mitigation?",
      "text" : "A dilemma often faced by researchers studying bias and fairness in machine learning is the trade-off between fairness and efficiency (Bertsimas et al., 2012). Under this trade-off theory, forcing cyberbullying classifiers to follow the proposed debiasing strategy would invariably decrease the accuracy. This section shows that, somewhat counterintuitively, our approach can outperform biased models w.r.t. overall cyberbullying detection ac-\ncuracy, while also decreasing unintended biases in the data.\nResults are presented in Tables 3-4. We see that the proposed debiasing strategy can both alleviate the bias and retain high prediction accuracy. For instance, for Instagram, our approach achieves the highest AUC and F1 score of all evaluated models. For Vine, the improvement of De-HAN over HAN is 9.8% and 41.9% for AUC and F1 score, respectively. The improvement over Constrained is 15.8% and 15.4%, respectively. Biased models present much lower Precision than Recall for Vine. This result is in line with the findings in Table 2, where we observe that the larger bias component is associated with FPRs in Vine. This indicates that when the sample size is small, these models overfit to sensitive triggers for detecting bullying instances. The debiasing strategy effectively reduces models’ reliance on those terms and utilizes contextual information for prediction."
    }, {
      "heading" : "5.5 What is the source of gain?",
      "text" : "What is the ingredient that enables our approach to achieve both the lowest bias and highest\naccuracy? This non-compromising approach may be attributed to the proposed RL framework that effectively captures contextual information. In this section, we examine the impact of parameter β in Equation 6 by varying β ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}. We show performance w.r.t. bias mitigation (total bias) and cyberbullying detection (F1 score) in Figure 4.\nThe results clearly show the efficacy of the proposed RL framework for bias mitigation. In particular, as we increase β, the RL agent puts more effort toward alleviating biases by minimizing both FPED and FNED simultaneously. Moreover, by interacting with the environment, the RL agent also leverages contextual information in order to minimize the prediction error and receive a larger reward. As a result, the RL agent largely reduces biases while improving the prediction accuracy, as shown by the slight increase in detection performance of the classifier in Figure 4b."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work, we examined unintended biases in datasets for session-based cyberbullying detection. In contrast to conventional data for bias mitigation in text classification, social media sessions consist of a sequence of comments with rich contextual\ninformation. To alleviate these unintended biases, we propose an effective debiasing strategy by leveraging techniques in RL. Our approach is contextaware, model-agnostic, and does not require additional resources or annotations aside from a predefined set of potentially sensitive triggers related to cyberbullying. Empirical evaluations demonstrated that our approach can mitigate unintended bias in the data without impairing a model’s prediction accuracy.\nOther types of decisions in sequential decisionmaking processes can impact the underlying user population, thereby influencing future comments generated by users. Future research can be directed towards studying the long-term impact of the debiasing strategy, as well as investigating different types of biases in session-based cyberbullying detection, such as gender bias, racial bias, and language bias. Our approach can also benefit from integrating previous studies that use data augmentation or swapping methods to counteract bias. Due to the challenges of data collection and labeling, validating our approach on datasets across different social media platforms is also an important avenue for future work.\nEthics Statement\nThis work seeks to advance collaborative research efforts aimed at mitigating bias in session-based cyberbullying detection, a topic that has yet to be studied extensively. Here, we provide preliminary solutions, but more work is needed to elucidate ways to build debiased and effective models. While all data used in this study are publicly available, we are committed to securing the privacy of the individuals in our datasets. To this end, we automatically replaced user names with ordered indices in our analysis. The insulting or offensive terms and the figures used in this paper are for illustrative purposes only and do not represent the views or ethical attitudes of the authors."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This material is based upon work supported by the National Science Foundation (NSF) Grants 1719722 and 2036127."
    }, {
      "heading" : "A Sensitive Triggers for Debiasing Cyberbullying Detection",
      "text" : "We adapted the list of cyberbullying keywords suggested in the psychology literature (Ortony et al., 1987; Squicciarini et al., 2015) to curate the list of sensitive triggers used in bias mitigation for cyberbullying detection: nerd, gay, loser, freak, emo, whale, pig, fat, poser, whore, die, suck, slut, afraid, pussy, cunt, kill, dick, bitch, black, ni***r, ne**o, ni**a, Mexican, redneck, retard, shit, ass, stupid, ugly, slave, fuck, pathetic, homo."
    } ],
    "references" : [ {
      "title" : "Stereotypical bias removal for hate",
      "author" : [ "Pinkesh Badjatiya", "Manish Gupta", "Vasudeva Varma" ],
      "venue" : null,
      "citeRegEx" : "Badjatiya et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Badjatiya et al\\.",
      "year" : 2019
    }, {
      "title" : "Semeval-2019 task 5: Multilingual detection of hate speech against immi",
      "author" : [ "Valerio Basile", "Cristina Bosco", "Elisabetta Fersini", "Nozza Debora", "Viviana Patti", "Francisco Manuel Rangel Pardo", "Paolo Rosso", "Manuela Sanguinetti" ],
      "venue" : null,
      "citeRegEx" : "Basile et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2019
    }, {
      "title" : "On the efficiency-fairness trade-off",
      "author" : [ "Dimitris Bertsimas", "Vivek F Farias", "Nikolaos Trichakis." ],
      "venue" : "Management Science, 58(12):2234–2250.",
      "citeRegEx" : "Bertsimas et al\\.,? 2012",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 2012
    }, {
      "title" : "Nuanced metrics for measuring unintended bias with real data for text classification",
      "author" : [ "Daniel Borkan", "Lucas Dixon", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Companion proceedings of the 2019 world wide web conference, pages 491–500.",
      "citeRegEx" : "Borkan et al\\.,? 2019",
      "shortCiteRegEx" : "Borkan et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like biases",
      "author" : [ "Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan." ],
      "venue" : "Science, 356(6334):183–186.",
      "citeRegEx" : "Caliskan et al\\.,? 2017",
      "shortCiteRegEx" : "Caliskan et al\\.",
      "year" : 2017
    }, {
      "title" : "Hierarchical attention networks for cyberbullying detection on the instagram social network",
      "author" : [ "Lu Cheng", "Ruocheng Guo", "Yasin Silva", "Deborah Hall", "Huan Liu." ],
      "venue" : "Proceedings of the 2019 SIAM International Conference on Data Mining, pages 235–",
      "citeRegEx" : "Cheng et al\\.,? 2019a",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling temporal patterns of cyberbullying detection with hierarchical attention networks",
      "author" : [ "Lu Cheng", "Ruocheng Guo", "Yasin N Silva", "Deborah Hall", "Huan Liu." ],
      "venue" : "ACM/IMS Transactions on Data Science, 2(2):1–23.",
      "citeRegEx" : "Cheng et al\\.,? 2021",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Xbully: Cyberbullying detection within a multi-modal context",
      "author" : [ "Lu Cheng", "Jundong Li", "Yasin N Silva", "Deborah L Hall", "Huan Liu." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 339–347.",
      "citeRegEx" : "Cheng et al\\.,? 2019b",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Session-based cyberbullying detection: Problems and challenges",
      "author" : [ "Lu Cheng", "Yasin N Silva", "Deborah Hall", "Huan Liu." ],
      "venue" : "IEEE Internet Computing.",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentiment informed cyberbullying detection in social media",
      "author" : [ "Harsh Dani", "Jundong Li", "Huan Liu." ],
      "venue" : "Joint European conference on machine learning and knowledge discovery in databases, pages 52–67. Springer.",
      "citeRegEx" : "Dani et al\\.,? 2017",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2017
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "arXiv preprint arXiv:1905.12516.",
      "citeRegEx" : "Davidson et al\\.,? 2019",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling the detection of textual cyberbullying",
      "author" : [ "Karthik Dinakar", "Roi Reichart", "Henry Lieberman." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 5.",
      "citeRegEx" : "Dinakar et al\\.,? 2011",
      "shortCiteRegEx" : "Dinakar et al\\.",
      "year" : 2011
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73.",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "author" : [ "Nikhil Garg", "Londa Schiebinger", "Dan Jurafsky", "James Zou." ],
      "venue" : "Proceedings of the National Academy of Sciences, 115(16):E3635–E3644.",
      "citeRegEx" : "Garg et al\\.,? 2018",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2018
    }, {
      "title" : "Cyberbullying detection with fairness constraints",
      "author" : [ "Oguzhan Gencoglu." ],
      "venue" : "IEEE Internet Computing.",
      "citeRegEx" : "Gencoglu.,? 2020",
      "shortCiteRegEx" : "Gencoglu.",
      "year" : 2020
    }, {
      "title" : "Cyberbullying fact sheet: Identification, prevention, and response",
      "author" : [ "Sameer Hinduja", "Justin W Patchin." ],
      "venue" : "Cyberbullying Research Center. Retrieved January, 30:2011.",
      "citeRegEx" : "Hinduja and Patchin.,? 2020",
      "shortCiteRegEx" : "Hinduja and Patchin.",
      "year" : 2020
    }, {
      "title" : "Detection of cyberbullying incidents on the instagram social network",
      "author" : [ "Homa Hosseinmardi", "Sabrina Arredondo Mattson", "Rahat Ibn Rafiq", "Richard Han", "Qin Lv", "Shivakant Mishra." ],
      "venue" : "arXiv preprint arXiv:1503.03909.",
      "citeRegEx" : "Hosseinmardi et al\\.,? 2015",
      "shortCiteRegEx" : "Hosseinmardi et al\\.",
      "year" : 2015
    }, {
      "title" : "Cyber bullying detection using social and textual analysis",
      "author" : [ "Qianjia Huang", "Vivek Kumar Singh", "Pradeep Kumar Atrey." ],
      "venue" : "Proceedings of the 3rd International Workshop on Socially-Aware Multimedia, pages 3–6.",
      "citeRegEx" : "Huang et al\\.,? 2014",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual twitter corpus and baselines for evaluating demographic bias in hate speech recognition",
      "author" : [ "Xiaolei Huang", "Linzi Xing", "Franck Dernoncourt", "Michael J Paul." ],
      "venue" : "arXiv preprint arXiv:2002.10361.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantics derived automatically from language corpora contain human-like moral choices",
      "author" : [ "Sophie Jentzsch", "Patrick Schramowski", "Constantin Rothkopf", "Kristian Kersting." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and",
      "citeRegEx" : "Jentzsch et al\\.,? 2019",
      "shortCiteRegEx" : "Jentzsch et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep transfer reinforcement learning for text summarization",
      "author" : [ "Yaser Keneshloo", "Naren Ramakrishnan", "Chandan K Reddy." ],
      "venue" : "Proceedings of the 2019 SIAM International Conference on Data Mining, pages 675–683. SIAM.",
      "citeRegEx" : "Keneshloo et al\\.,? 2019",
      "shortCiteRegEx" : "Keneshloo et al\\.",
      "year" : 2019
    }, {
      "title" : "You Don’t Know How I Feel:Insider-Outsider Perspective Gaps in Cyberbullying Risk Detection",
      "author" : [ "Seunghyun Kim", "Afsaneh Razi", "Gianluca Stringhini", "Pamela Wisniewski", "Munmun De Choudhury." ],
      "venue" : "page 13.",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Ditch the label anti bullying charity: The annual cyberbullying survey 2013",
      "author" : [ "Ditch the Label Anti Bullying Charity." ],
      "venue" : "https: //www.ditchthelabel.org/wp-content/ uploads/2016/07/cyberbullying2013.pdf.",
      "citeRegEx" : "Charity.,? 2013",
      "shortCiteRegEx" : "Charity.",
      "year" : 2013
    }, {
      "title" : "From fairness metrics to key ethics indicators (keis): a context-aware approach to algorithmic ethics in an unequal society",
      "author" : [ "Michelle Seng Ah Lee", "Luciano Floridi", "Jatinder Singh." ],
      "venue" : "Available at SSRN.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep reinforcement learning-based text anonymization against private-attribute inference",
      "author" : [ "Ahmadreza Mosallanezhad", "Ghazaleh Beigi", "Huan Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Mosallanezhad et al\\.,? 2019",
      "shortCiteRegEx" : "Mosallanezhad et al\\.",
      "year" : 2019
    }, {
      "title" : "Unintended bias in misogyny detection",
      "author" : [ "Debora Nozza", "Claudia Volpetti", "Elisabetta Fersini." ],
      "venue" : "IEEE/WIC/ACM International Conference on Web Intelligence, pages 149–155.",
      "citeRegEx" : "Nozza et al\\.,? 2019",
      "shortCiteRegEx" : "Nozza et al\\.",
      "year" : 2019
    }, {
      "title" : "The referential structure of the affective lexicon",
      "author" : [ "Andrew Ortony", "Gerald L Clore", "Mark A Foss." ],
      "venue" : "Cognitive science, 11(3):341–364.",
      "citeRegEx" : "Ortony et al\\.,? 1987",
      "shortCiteRegEx" : "Ortony et al\\.",
      "year" : 1987
    }, {
      "title" : "Reducing gender bias in abusive language detection",
      "author" : [ "Ji Ho Park", "Jamin Shin", "Pascale Fung." ],
      "venue" : "arXiv preprint arXiv:1808.07231.",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "Semeval 2021 task 5: Toxic spans detection",
      "author" : [ "John Pavlopoulos", "Ion Androutsopoulos", "Jeffrey Sorensen", "Léo Laugier." ],
      "venue" : "15th International Workshop on Semantic Evaluation.",
      "citeRegEx" : "Pavlopoulos et al\\.,? 2021",
      "shortCiteRegEx" : "Pavlopoulos et al\\.",
      "year" : 2021
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Careful what you share in six seconds: Detecting cyberbullying instances in vine",
      "author" : [ "Rahat Ibn Rafiq", "Homa Hosseinmardi", "Richard Han", "Qin Lv", "Shivakant Mishra", "Sabrina Arredondo Mattson." ],
      "venue" : "ASONAM 2015, pages 617–622. IEEE.",
      "citeRegEx" : "Rafiq et al\\.,? 2015",
      "shortCiteRegEx" : "Rafiq et al\\.",
      "year" : 2015
    }, {
      "title" : "Toward diverse text generation with inverse reinforcement learning",
      "author" : [ "Zhan Shi", "Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1804.11258.",
      "citeRegEx" : "Shi et al\\.,? 2018",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2018
    }, {
      "title" : "Cyberbullying: Its nature and impact in secondary school pupils",
      "author" : [ "Peter K Smith", "Jess Mahdavi", "Manuel Carvalho", "Sonja Fisher", "Shanette Russell", "Neil Tippett." ],
      "venue" : "Journal of child psychology and psychiatry, 49(4):376–385.",
      "citeRegEx" : "Smith et al\\.,? 2008",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2008
    }, {
      "title" : "Identification and characterization of cyberbullying dynamics in an online social network",
      "author" : [ "Anna Squicciarini", "Sarah Rajtmajer", "Y Liu", "Christopher Griffin." ],
      "venue" : "Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Net-",
      "citeRegEx" : "Squicciarini et al\\.,? 2015",
      "shortCiteRegEx" : "Squicciarini et al\\.",
      "year" : 2015
    }, {
      "title" : "Mitigating gender bias in natural language processing: Literature review",
      "author" : [ "Wang." ],
      "venue" : "arXiv preprint arXiv:1906.08976.",
      "citeRegEx" : "Wang.,? 2019",
      "shortCiteRegEx" : "Wang.",
      "year" : 2019
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In NIPs,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning from bullying traces in social media",
      "author" : [ "Jun-Ming Xu", "Kwang-Sung Jun", "Xiaojin Zhu", "Amy Bellmore." ],
      "venue" : "Proceedings of the 2012 conference of the North American chapter of the association for computational linguistics: Human language",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 conference of the North American chapter of the association for computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval",
      "author" : [ "Marcos Zampieri", "Preslav Nakov", "Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Hamdy Mubarak", "Leon Derczynski", "Zeses Pitenis", "Çağrı Çöltekin" ],
      "venue" : null,
      "citeRegEx" : "Zampieri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2020
    }, {
      "title" : "Demographics should not be the reason of toxicity: Mitigating discrimination in text classifications with instance weighting",
      "author" : [ "Guanhua Zhang", "Bing Bai", "Junqi Zhang", "Kun Bai", "Conghui Zhu", "Tiejun Zhao." ],
      "venue" : "arXiv preprint arXiv:2004.14088.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "A reinforced generation of adversarial examples for neural machine translation",
      "author" : [ "Wei Zou", "Shujian Huang", "Jun Xie", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1911.03677.",
      "citeRegEx" : "Zou et al\\.,? 2019",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Recent findings indicate that over 35% of young people have been victims of cyberbullying and roughly 15% have admitted to cyberbullying others (Hinduja and Patchin, 2020; Kim et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 189
    }, {
      "referenceID" : 21,
      "context" : "Recent findings indicate that over 35% of young people have been victims of cyberbullying and roughly 15% have admitted to cyberbullying others (Hinduja and Patchin, 2020; Kim et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "Figure 1: Conditional probability densities of standard HAN and debiased HAN on sessions with and without sensitive triggers z in the Instagram dataset released by (Hosseinmardi et al., 2015).",
      "startOffset" : 164,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "The promising results, nevertheless, may come from a deeply biased model that captures, uses, and even amplifies the unintended biases embedded in social media data (Zhang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "That is, because humans are biased, human-generated language corpora can introduce human social prejudices into model training processes (Caliskan et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 40,
      "context" : "Evidence of such bias has been found in toxicity detection (Zhang et al., 2020) and hate speech detection (Davidson et al.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and hate speech detection (Davidson et al., 2019), revealing that tweets in African-American Vernacular English (AAVE) are more likely to be classified as abusive or offensive.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "Indeed, we find that in the Instagram data for benchmarking cyberbullying detection released by (Hosseinmardi et al., 2015), 68.",
      "startOffset" : 96,
      "endOffset" : 123
    }, {
      "referenceID" : 38,
      "context" : "In Figure 1, we showcase differences in the performance of a standard hierarchical attention network (HAN) (Yang et al., 2016) – a commonly used model for session-based cyberbullying detection – and a HAN that was debiased using our proposed strategy in sessions with and without sensitive triggers using the benchmark Instagram data.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Our task poses multi-faceted challenges that render recent model-agnostic research in fair text classification – especially, data manipulation methods (Dixon et al., 2018; Sun et al., 2019) – inapplicable.",
      "startOffset" : 151,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "Bias mitigation cannot be defined without context (Lee et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "women (Basile et al., 2019), offensive language identification (Zampieri et al.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 39,
      "context" : ", 2019), offensive language identification (Zampieri et al., 2020), and toxic spans detection (Pavlopoulos et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : ", 2020), and toxic spans detection (Pavlopoulos et al., 2021).",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "To better leverage the rich information included in social media data, many studies proposed to augment textual features with emotion/sentiment (Dani et al., 2017), social network information such as relational centrality and ego networks (Squicciarini et al.",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 34,
      "context" : ", 2017), social network information such as relational centrality and ego networks (Squicciarini et al., 2015; Huang et al., 2014), and other multi-modal information such as location and time (Cheng et al.",
      "startOffset" : 83,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : ", 2017), social network information such as relational centrality and ego networks (Squicciarini et al., 2015; Huang et al., 2014), and other multi-modal information such as location and time (Cheng et al.",
      "startOffset" : 83,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : ", 2014), and other multi-modal information such as location and time (Cheng et al., 2019b).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : "2160 tics of cyberbullying behavior – repetitiveness and power imbalance (Smith et al., 2008).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Yet, whereas numerous studies have focused on achieving better prediction performance, these approaches tend to carry or reinforce the unintended social biases in the datasets (Gencoglu, 2020).",
      "startOffset" : 176,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "Humans are inherently biased, and many studies have revealed human biases and discrimination in natural language (Garg et al., 2018; Jentzsch et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "Humans are inherently biased, and many studies have revealed human biases and discrimination in natural language (Garg et al., 2018; Jentzsch et al., 2019).",
      "startOffset" : 113,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "In particular, one can augment original data with external labeled data (Dixon et al., 2018).",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 40,
      "context" : "Similar methods include data oversampling/downsampling, sample weighting (Zhang et al., 2020), and identity term swapping (Park et al.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : ", 2020), and identity term swapping (Park et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "(Dixon et al., 2018) added non-toxic samples containing identity terms from Wikipedia articles into training data.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : "A similar strategy was used in (Nozza et al., 2019) for misogyny detection.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "(Badjatiya et al., 2019) proposed to replace sensitive words with neutral words or tokens.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Lastly, balancing can introduce additional calibration parameters that can impair classification performance and bias mitigation (Gencoglu, 2020).",
      "startOffset" : 129,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : "Cyberbullying is often characterized as a repeated rather than a one-off behavior (Smith et al., 2008).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : ", an image with a caption), a sequence of comments from different users, timestamps, spatial location, user profile information, and other social content such as number of likes (Cheng et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "Session-based cyberbullying detection presents a number of characteristics such as multi-modality and user interaction (Cheng et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "” In the presence of unintended bias, a model may present high performance for sessions with these sensitive triggers without knowing their semantics (Dixon et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : "Bias in a text classification model can be assessed by the False Negative Equality Difference (FNED) and False Positive Equality Difference (FPED) metrics, as used in previous studies such as (Zhang et al., 2020; Gencoglu, 2020; Huang et al., 2020).",
      "startOffset" : 192,
      "endOffset" : 248
    }, {
      "referenceID" : 14,
      "context" : "Bias in a text classification model can be assessed by the False Negative Equality Difference (FNED) and False Positive Equality Difference (FPED) metrics, as used in previous studies such as (Zhang et al., 2020; Gencoglu, 2020; Huang et al., 2020).",
      "startOffset" : 192,
      "endOffset" : 248
    }, {
      "referenceID" : 18,
      "context" : "Bias in a text classification model can be assessed by the False Negative Equality Difference (FNED) and False Positive Equality Difference (FPED) metrics, as used in previous studies such as (Zhang et al., 2020; Gencoglu, 2020; Huang et al., 2020).",
      "startOffset" : 192,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "They are a relaxation of Equalized Odds (Borkan et al., 2019) and defined as",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Note that the widely-used non-discrimination evaluation sets – Identity Phrase Templates Test Sets (IPTTS) (Dixon et al., 2018) – are not applicable to our task.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "of RL (Shi et al., 2018; Zou et al., 2019; Mosallanezhad et al., 2019), particularly, the sequential Markov Decision Process (MDP).",
      "startOffset" : 6,
      "endOffset" : 70
    }, {
      "referenceID" : 41,
      "context" : "of RL (Shi et al., 2018; Zou et al., 2019; Mosallanezhad et al., 2019), particularly, the sequential Markov Decision Process (MDP).",
      "startOffset" : 6,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "of RL (Shi et al., 2018; Zou et al., 2019; Mosallanezhad et al., 2019), particularly, the sequential Markov Decision Process (MDP).",
      "startOffset" : 6,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "The agent’s goal is to excel at a specific task, such as generating text (Shi et al., 2018) or summarizing text (Keneshloo et al.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "We use the policy gradient algorithm – REINFORCE (Sutton et al., 1999) – to train the agent.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "Two benchmark datasets for cyberbullying detection – Instagram (Hosseinmardi et al., 2015) and Vine (Rafiq et al.",
      "startOffset" : 63,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : ", 2015) and Vine (Rafiq et al., 2015) – are used for empirical evaluation.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "models for classification with sequential text data, including HAN, Convolutional Neural Network (CNN), and Gated Recurrent Unit (GRU), as well as a recent model proposed for session-based cyberbullying detection – HANCD (Cheng et al., 2019a).",
      "startOffset" : 221,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "We also include the state-of-the-art model Constrained (Gencoglu, 2020) that imposes two fairness constraints on cyberbullying detection to mitigate biases.",
      "startOffset" : 55,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "In Algorithm 1, the classifier F is pre-trained for 5 iterations using loss function lF , learning rate 3e− 3, and the Adam optimizer (Kingma and Ba, 2014).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 40,
      "context" : "Following (Zhang et al., 2020; Gencoglu, 2020), we use FPED, FNED, and total bias (FPED+FNED) to evaluate how biased a model is w.",
      "startOffset" : 10,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Following (Zhang et al., 2020; Gencoglu, 2020), we use FPED, FNED, and total bias (FPED+FNED) to evaluate how biased a model is w.",
      "startOffset" : 10,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : "For all models, pre-trained GloVe word embeddings (Pennington et al., 2014) and 10-fold cross validation with 80/20 split are used for fair comparison.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "fairness constraints proposed in (Gencoglu, 2020), as indicated by the decreased total biases for both Instagram and Vine.",
      "startOffset" : 33,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "A dilemma often faced by researchers studying bias and fairness in machine learning is the trade-off between fairness and efficiency (Bertsimas et al., 2012).",
      "startOffset" : 133,
      "endOffset" : 157
    } ],
    "year" : 2021,
    "abstractText" : "The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographicidentity terms (e.g., “gay” or “black”) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram). We then propose a contextaware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.",
    "creator" : "LaTeX with hyperref"
  }
}