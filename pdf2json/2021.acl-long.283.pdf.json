{
  "name" : "2021.acl-long.283.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations",
    "authors" : [ "Weixin Liang", "Kai-Hui Liang" ],
    "emails" : [ "wxliang@stanford.edu", "kaihui.liang@columbia.edu", "zy2461@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3652–3665\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3652"
    }, {
      "heading" : "1 Introduction",
      "text" : "Evaluation metrics heavily influence a field’s research direction. The ultimate goal of open-domain dialog systems is to provide an enjoyable experience to users. Previous research mainly focuses on optimizing automatic dialog evaluation metrics such as BLEU, which models the distance between the system responses and a limited number of references available. However, it has been shown that these metrics correlate poorly with human judgments (Liu et al., 2016).\nOpen-domain dialog system evaluation has long been one of the most difficult challenges in the dialog community for several reasons: (1) The goal of\n1Equal Contribution.\ndialog evaluation should be to evaluate users’ conversational experience. Existing automatic evaluation metrics such as BLEU are mostly constrained to a static corpus, and do not capture the user experience in a realistic interactive setting. (2) Currently, self-reported user ratings are widely used to evaluate open-domain dialogs. However, self-reported ratings suffer from bias and variance among different users (Liang et al., 2020e). Although we could tell which dialog system is better by running statistical tests on a large number of noisy ratings, it is challenging to locate dialogs with bad performance reliably. Only by identifying these bad dialogs effectively can we correct errors in these samples to improve dialog system quality.\nUser engagement has been recognized as one of the essential metrics for open-domain dialog evaluation (Ram et al., 2018). Previous research also confirms that incorporating user engagement as real-time feedback benefits dialog policy learning (Yu et al., 2016). One of the most costly bottlenecks of learning to detect user disengagement is to annotate many turn-level user engagement labels (Ghazarian et al., 2020). In addition, the data annotation process becomes more expensive and challenging for privacy-sensitive dialog corpora, due to the privacy concerns in crowdsourcing (Xia and McKernan, 2020).\nTo improve annotation efficiency, we reframe the training data annotation process as a denoising problem. Specifically, instead of manually labeling each training datum, we automatically label the training samples with a set of labeling heuristics. The heuristic functions primarily consist of regular expressions (Regexes) and incorporate open-sourced natural language understanding (NLU) services. Since the automatically generated labels might contain noise, we then denoise the labeled data using the Shapley algorithm (Jia et al., 2019a,b). We use the Shapley algorithm to\nquantify the contribution of each training datum, so that we can identify the noisy data points with negative contribution and then correct their labels. Our experiments show that HERALD achieves 86% accuracy in user disengagement detection in two dialog corpora.\nOur proposed framework HERALD is conceptually simple and suitable for a wide range of application scenarios: First, since our model could detect user engagement in real-time (i.e., after each user utterance), our model could be plugged into existing dialog systems as a real-time user experience monitor module. In this way, dialog systems could detect and react to user’s disengagement in both open-domain dialogs (Yu et al., 2016) and taskoriented dialogs (Yu et al., 2017). During training, our model could also be used as real-time feedback to benefit dialog policy learning (Yi et al., 2019). Second, HERALD could quantify user engagement and be used as an automatic dialog evaluation metric. It could locate dialogs with poor user experience reliably to improve dialog system quality (Ghazarian et al., 2020; Choi et al., 2019). Third, user engagement is an essential objective of dialog systems, but few dialog datasets with user engagement ratings are available. Our heuristic functions, combined with the proposed workflow, can be readily deployed to annotate new dialog datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Open-Domain Dialog System Evaluation",
      "text" : "Open-domain dialog system evaluation is a longlasting challenge. It has been shown that existing automatic dialog evaluation metrics correlate poorly with human judgments (Liu et al., 2016; Lowe et al., 2017; Novikova et al., 2017). A wellknown reason is that these automatic dialog evaluation metrics rely on modeling the distance between the generated response and a limited number of references available. The fundamental gap between the open-ended nature of the conversations and the limited references (Gupta et al., 2019) is not addressed in methods that are lexical-level based (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al., 2014), perplexity based (Adiwardana et al., 2020), or learning based (Tao et al., 2018; Lowe et al., 2017). Mehri and Eskénazi (2020) simulate user response using DialogGPT and evaluate the probability of user complaint.\nGiven the limitations above, self-reported user ratings are widely used to evaluate open-domain dialogs. However, self-reported ratings suffer from bias and variance among different users (Venkatesh et al., 2018). Denoising human ratings is still an open research problem (Liang et al., 2020e; Li et al., 2019)."
    }, {
      "heading" : "2.2 User Engagement in Dialogs",
      "text" : "User engagement is commonly defined as the user’s willingness to continue conversing with the dialog system (Yu et al., 2016, 2017). Existing work on measuring user engagement primarily resorts to human rating (Yi et al., 2019; Hancock et al., 2019), or proxy metrics. Example proxy metrics include conversation length like number of dialog turns (Venkatesh et al., 2018; Ram et al., 2018), and conversational breadth like topical diversity (Guo et al., 2018). Sporadic attempts have been made to detecting user disengagement in dialogs (Yu et al., 2004; Ghazarian et al., 2020; Choi et al., 2019). A major bottleneck of these methods is that they require hand-labeling many dialog samples for individual datasets. Although Liang et al. (2020e) denoise user self-reported ratings with the Shapley algorithm for dialog system evaluation, their method cannot be directly applied to dialogs without user ratings as in our setting. Our work is focusing on the problem that it is expensive and difficult to obtain user ratings. The core insight of our work is to reframe the training data annotation process as a process of denoising labels created by heuristic functions pre-defined. To the best of our knowledge, we are the first to combine automatic data labeling with the Shapley algorithm to perform dialog evaluation. Our method could potentially generalize to other classification tasks if different weak labelers are provided."
    }, {
      "heading" : "2.3 Learning from Weak Supervision",
      "text" : "Learning from weak supervision reduces annotation costs by utilizing noisy but cost-efficient labels (Ratner et al., 2020, 2016; Liang et al., 2020e). One of the most popular forms of weak supervision is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels for relationship extraction tasks (Bunescu and Mooney, 2007; Mintz et al., 2009; Hancock et al., 2018). Other applications of weak supervision to scene graph prediction (Krishna et al., 2019), intent classification (Mallinar et al., 2019), and medical imag-\ning (Varma et al., 2017) have observed similar benefits in annotation efficiency. Unlike the existing work, we leverage weak supervision to improve annotation efficiency for detecting user disengagement in social conversations."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "We defined engagement as the degree to which users are willing to continue conversing with the dialog system Yu et al. (2016, 2017). We focus on identifying the dialog turns with “disengaged” user response, since they usually indicate poor conversation experience. We formulate the user engagement prediction as a binary classification problem: Our goal is to learn a parameterized user engagement predictor Mθ that, given a dialog turn (along with its dialog context) x ∈ X, predicts the turn-level user engagement label y ∈ Y = {0, 1}, where label y = 1 means “disengaged” and y = 0 means “engaged”. We start from an unlabeled train set Dtrain = {xi}Ntrain1 without any label yi. The test set Dtest = {(xi, yi)}Ntest1 contains the ground-truth label yi. The development set Ddev has a similar structure as the test set Dtest but the development set can be much smaller than a train set (i.e., Ndev Ntrain), making it economical to obtain. Following the general architecture of neural classifiers, we formulate our model Mθ = M(φ, f ) = f (φ(x)): Here BERT (Devlin et al., 2019)-based φ is a text encoder that maps each dialog turn x to a feature space φ(x) ∈ Rd. f is the final linear layer with softmax activation."
    }, {
      "heading" : "4 Data",
      "text" : "To ensure our framework is generalized to various corpora, we investigate multiple open-domain dialog datasets ranging from ASR-based (Gunrock (Liang et al., 2020a)) to text-based (ConvAI2 (Dinan et al., 2019), Blender (Roller et al., 2020), and Meena (Adiwardana et al., 2020)) dialog systems.\nGunrock Movie Dataset Gunrock Movie dataset consists of dialog data collected from Gunrock, an ASR-based open-domain social chatbot originally designed for Amazon Alexa Prize (Liang et al., 2020a). The Gunrock dataset comes from a user study where in-lab users were recruited to carry on conversations. We have consent to use the data and we also removed any sensitive information in the conversation. Two dialog experts (co-authors of this paper) randomly annotated 134 dialogs and split them evenly into the test set and development set. In total, the experts labeled 519 turn-level disengaging user responses and 2,312 engaging user responses. They reached a high inter-annotator agreement score (Cohen, 1968) with kappa κ = 0.78. The training set contains 276 unlabeled dialogs, with 5644 dialog turns. In addition, we ensure that the data annotation is independent of the labeling heuristics collection, so there is no data leakage problem. A full example dialog can be found in Appendix A.4.\nConvAI2 Dataset ConvAI2 dataset contains text-based dialog collected from the second Conver-\nsational Intelligence (ConvAI) Challenge (Dinan et al., 2019). We select dialogs from the main eight participated chatbots (Bot 1, 2, 3, 4, 6, 9, 11) and exclude dialogs that are one-sided or shorter than three turns. The dialog experts annotated 207 dialogs in total. The dialogs are evenly distributed over all the eight bots to ensure system diversity, and are randomly sampled within each bot. The annotated data consist of 209 disengaging turns and 1684 non-disengaging turns. They reached a high inter-annotator agreement score (Cohen, 1968) with kappa κ = 0.76. We split the annotated dialogs evenly into the test set and develop set. The training set contains 2,226 dialogs, with 18,306 dialog turns.\nGoogle Meena Dataset Meena (Adiwardana et al., 2020) is the largest end-to-end neural chatbot so far, trained on 867M public domain social media conversations. We study the 93 example Human-Menna conversations released by Google.\nFacebook Blender Dataset The Blender bot (Roller et al., 2020) is an open-domain chatbot with several conversational skills: providing engaging talking points and listening to their partners, displaying knowledge, empathy, and personality appropriately while maintaining a consistent persona. We study the 108 example Human-Blender conversations released by Facebook."
    }, {
      "heading" : "5 Method",
      "text" : "Our goal is to train a user engagement detector with minimum data annotation efforts. Traditional supervised learning paradigms require annotating many training samples. In addition, it requires additional data annotation to extend the model to a new\ndialog corpus. To reduce annotation work, we propose HERALD, a two-stage pipeline that annotates large-scale training data efficiently and accurately (Figure 1). Instead of hand-labeling training data points, we use heuristic functions to label each training datum automatically. The heuristic functions are built upon a set of user disengagement heuristics rules. Since the training data are automatically labeled, their labels would be noisy. We then clean the noisy training data with Shapley algorithm (Ghorbani and Zou, 2019) to improve the labeling accuracy. The Shapley algorithm denoises training data by identifying data with wrong labels and flip their labels. Finally, as we received clean training data, we use them to fine-tune a BERTbased model and obtain the final user disengagement detection model."
    }, {
      "heading" : "5.1 Stage 1: Auto-label Training Data with Heuristic Functions",
      "text" : "Since labeling large-scale training data is timeconsuming, we propose heuristic labeling functions to label training data automatically. The heuristic functions focus on detecting disengagement from user responses, as it directly indicates poor user experience. To build the heuristics functions, we first summarize the heuristic rules shared among users. We investigate the disengaged dialog turns from the four datasets mentioned above and identify four groups of user disengagement patterns: “complain system responses”, “dislike current topics”, “terminate or change topics”, and “end with non-positive responses” (Table 1). We then discuss the implementation of heuristics functions."
    }, {
      "heading" : "5.1.1 Disengagement Heuristic Rules",
      "text" : "Group 1: Complain system responses. Complaints are an evident sign of user disengagement. We identify six related disengaged intents. The first three intents (“complain system repetition”, “complain system ignoring them” and “complain system misunderstanding”) usually appear when the bot makes errors like repeating the same content, ignoring, forgetting, and misunderstanding the user’s response. In these cases, users express their disengagement by indicating the bot’s error (e.g. “You already told me that”, “You’re not listening”). Another intent “not understanding system” happens when users cannot understand the system’s response (e.g. “I don’t know what you’re talking about.”). In the last two intents, users reveal negative emotions by cursing the system (e.g. “you’re dumb”) or express frustration (e.g. “sigh”) about the conversation.\nGroup 2: Dislike current topics. When discussing a given topic, users might show their disengagement by expressing negative opinions or low interest. For example, given the bot’s response, “I write romantic novels under a pen name. ”, for users who are not interested in reading, users might say “reading is boring”, “I don’t like to read”, or “I’m not interested in this”. We also make sure to handle the corner cases where the user utterance should be labeled as engaged but contains negative opinions. For instance, to respond to the bot’s question, “do you want to not work?”, a user might say, “Yes. my job is boring. I have to work with mail”. Though the user mentions a negative feeling (“boring”), the user agrees with the bot and shares further information.\nGroup 3: Terminate or change topics Group 3 considers the cases where users express disengagement to the current topic in a more straightforward fashion. For example, if users are not interested in the current topic, instead of just expressing their dislike to it, they may request to switch topics with “Let’s talk about something else”. In some cases, users might show strong disengagement by requesting to end the conversation if the user is no longer interested in continuing the conversation.\nGroup 4: End with non-positive responses A more subtle but common clue of disengagement is when users end the response with non-positive content. For example, non-positive responses like “I don’t know”, “No”, “Yeah”, “uh”, “Probably”,\nimply that users do not have much to talk about the current topic. To keep the precision of our heuristics high, we carefully consider the counterexamples. One case is that the user follows up with more responses such as questions (e.g., Bot: “Have you seen any movies lately? ”, User: “No. Have you?”), and opinion (e.g. Bot: “What’s your favorite animation movie?”, User: “I don’t know, but it might actually be frozen two. My sister loves it.”) in the same dialog turn. These turns should not be labeled as disengaged since the user is still interested in sharing more content or asking followup questions. Therefore, we take a conservative approach: we label the dialog turn as disengaged only if no more responses follow the non-positive response."
    }, {
      "heading" : "5.1.2 Heuristic Functions Implementation",
      "text" : "Next, we discuss how to use heuristic functions to auto-label disengaged user utterances. First, we split user responses into segments since user responses may consist of multiple units with different semantic meanings. We use NLTK Sentence Tokenizer for text-based system, and a segmentation model (Chen et al., 2018) for ASR (Automatic Speech Recognition)-based system as the segmentation tool. We then apply the heuristic functions on each segment to detect disengaged intents. For heuristic groups 1 to 3, if any segment contains a disengaged intent, the user response is auto-labeled as disengaged. For heuristic group 4 (“End with non-positive responses”), we assign disengaged labels only if the disengaged intents are detected in the last segment.\nWe detect disengaged intents with Regexes. The benefit of using Regexes is that they have minimum dependencies and are easy to modify. We design Regexes for each intent. Following common Regexes complexity metrics (Luo et al., 2018), our Regexes for each intent contains 43.9 Regexes groups and 87.7 or clauses on average.\nOur framework also supports incorporating additional resources to improve the intent detection accuracy for automatic training data labeling. For example, we can enhance the recall of Regexes intent detection by incorporating existing deep learning-based NLU (Natural Language Understanding) models. Specifically, we re-purpose an open-sourced dialog act classification model (Yu and Yu, 2021) to enhance disengagement intent detection: we select 6 out of the 23 supported dialog act labels that are associated with disen-\ngaged intents, and map each selected dialog act label to the heuristic groups. The dialog act “complaint” is mapped to the heuristic group “complain system repetition”;“closing” is mapped to the disengaged intent “request termination”; “hold” to “hesitation”;“other_answers” to “unsure answer”; “back-channeling” to “back-channeling”, and “neg_answer“ to ‘negative answer‘”. If a user utterance is detected with disengaged intent by either Regexes or the deep learning model, then the utterance is auto-labeled as disengaged."
    }, {
      "heading" : "5.2 Stage 2: Denoise with Shapley Algorithm & Fine-tune",
      "text" : "Overview Next, we denoise the labeled data using Shapley algorithm (Ghorbani and Zou, 2019). Shapley algorithm has been studied in the cooperative game theory (Dubey, 1975) and economics (Gul, 1989) as a fair distribution method. Shapley algorithm computes a Shapley value for each training datum, which quantifies the contribution of each training datum to the prediction and performance of a deep network. Low Shapley value data capture outliers and corruptions. Therefore, we can identify and denoise the incorrectly labeled data by computing their Shapley values and finetune the model on the cleaned training set.\nShapley Algorithm Shapley algorithm comes originally from cooperative game theory (Dubey, 1975). Consider a cooperative game with n players D = {1, ..., n} and a utility function v : 2[n] → R which assigns a reward to each of 2n subsets of players: v(S ) is the reward if the players in subset S ⊆ D cooperate. Shapley value defines a unique scheme to distribute the total gains generated by the coalition of all players v(D) with a set of appealing mathematical properties. In our setting, we can consider Dtrain = {(xi, yi)}Ntrain1 as Ntrain players. We define the utility function v(S ) as the performance on the development set Ddev. The Shapley value for player i is defined as the average marginal contribution of {(xi, yi)} to all possible subsets that are formed by other players (Jia et al., 2019a,b):\nsi = 1 N ∑ S⊆Dtrain\\{xi} 1( N−1 |S |\n) [v(S ∪ {xi}) − v(S )] As suggested by the definition of Shapley value, computing Shapley value requires an exponentially large number of computations to enumerate O(2Ntrain) possible subsets and train the model Mθ on each subset, which is intractable. Inspired\nby (Jia et al., 2019a,b), HERALD tackles this issue by reducing the deep model Mθ to a Knearest neighbors (KNN) model and then apply the closed-form solution of Shapley value on KNN: We reduce our BERT-based classification model Mθ = M(φ, f ) = f (φ(x)) to a KNN by first finetuning Mθ on the auto-labeled training samples. We then use the feature extractor φ to map each training datum to the feature space {φ(xi)}Ntrain1 . We construct a KNN classifier in the feature space to compute the closed-form Shapley value.\nNext, we discuss the closed-form solution of Shapley value. We first consider a special case where the development set Ddev only contains one datum Ddev = {(xdev, ydev)}. Given any nonempty subset S ⊆ Dtrain, we use the KNN classifier to classify xdev. To do this, we sort the data points in the training set {xi}Ntrain1 based on their euclidean distance in the feature space φ(x) to the datum in the development set xdev, yielding (xα1 , xα2 , ..., xα|S |) with xα1 , ..., xαK as the top-K most similar data points to xdev. The KNN classifier outputs the probability of xdev taking the label ydev as P[xdev → ydev] = 1K ∑K k=1 1[yαk = ydev], where αk is the index of the kth nearest neighbor. We define the utility function as the likelihood of the correct label:\nν(S ) = 1 K min{K,|S |}∑ k=1 1[yαk(S ) = ydev] (1)\nJia et al. (2019a,b) proves that the Shapley value of each training point sαi can be calculated recursively in O(N log N) time as follows:\nsαN = 1[yαN = ydev]\nN\nsαi = sαi+1 + min{K, i} i × K ( 1[yαi=ydev]−1[yαi+1=ydev] ) The above result for a single point in Ddev could be readily extended to the multiple-point case, in which the utility function is defined by\nν(S ) = 1\nNdev Ndev∑ j=1 1 K min{K,|S |}∑ k=1 1[y α ( j) k (S ) = ydev, j]\nwhere α( j)k (S ) is the index of the kth nearest neighbor in S to xdev, j. Jia et al. (2019a,b) also prove that the Shapley value in this case is the average of the Shapley value for every single dev point.\nDenoising Procedure Our denoising procedure works as follows: (1) We first fine-tune our BERTbased classification model Mθ = M(φ, f ) = f (φ(x))\non the auto-labeled training samples. This step injects the knowledge in the labeling heuristic into the model Mθ. (2) We then map each auto-labeled training datum to the feature space {φ(xi)}Ntrain1 , since we want to apply the closed-form KNN formula of Shapley value in the feature space. (3) Next, for a binary classification problem, we duplicate each training datum 2 times with labels [0, 1]. This generates a large training set Dlarge with 2 × Ntrain data points, and we note that the origin training set Dtrain is a subset of Dlarge, since Dlarge enumerates all C possible labels for each each training datum. (4) We then calculate Shapley value for the 2 × Ntrain data points in Dlarge using the closed-form KNN formula. (5) We remove the data with negative Shapley value in Dlarge, and get a cleaned training set Dclean. The duplicate-and-remove procedure “flips” the labels of the noisy data points with low Shapley value. (6) Finally, we fine-tune the classification model Mθ on Dclean to get the final user disengagement detection model.\nTo sum up, the Shapley value quantifies the contribution of each training datum. Low Shapley value data capture outliers and corruptions that are not consistent with the distribution of other data points. We identify and correct these outliers and corruptions to provide a clean training set."
    }, {
      "heading" : "6 Experiments",
      "text" : "Model Setup We use K = 10 for the KNN Classifier. We use BERT (Devlin et al., 2019) as the text encoder φ of our classification model Mθ = M(φ, f ) = f (φ(x)). Additional implementa-\ntion details are included in Appendix.\nModel Comparisons and Ablations We compare HERALD to its several ablations (Table 2) and evaluate the performance on the test set. We report balanced accuracy (bACC) and Fβ Score with β = 2 (Baeza-Yates et al., 1999). (1) Heuristics uses the labeling heuristic function with both Regex and dialog act to predict the test set. (2) Heuristics (Regex only) uses the labeling heuristic function only with Regex to predict on the test set. (3) Heuristics (NLU only) uses the labeling heuristic function only with NLU. (4-7) show the ablation of the heuristics function prediction baseline by excluding each heuristic group. (8) BERT(dev) finetunes BERT on the expert-annotated development set. (9) BERT(Auto) fine-tunes BERT on the autolabeled training samples. (10) BERT(Auto+dev) fine-tunes BERT on both the auto-labeled training samples and the development set. (11) HERALD reports the performance of the final model trained on Dclean.\nResults Our first takeaway is that our labeling heuristics produce decent predictions and generalize to different datasets. As shown in Table 2, Heuristics prediction (Heuristic, 78.32%, 76.58%) is better than the BERT-based model with limited training samples (BERT(dev), 73.98%, 74.94%) on both datasets. It also shows that our labeling heuristics are generalizable to different corpora.\nOur second takeaway is that learning from a large number of noisy labels works better than learning from a limited number of clean labels. As shown in Table 2, BERT fine-tuned on the autolabeled training set (BERT(Auto), 80.55, 78.76) outperforms BERT fine-tuned on clean but small development set (BERT(dev), 73.98, 74.94) by a large margin. In addition, we also observe that the BERT model fine-tuned on the auto labeled training data (BERT(Auto), 80.55%, 78.76%) generalizes beyond the labeling heuristics (Heuristics, 78.32%, 76.58%).\nOur third takeaway is that using the expertannotated development set for denoising is more efficient than using the development set as additional training data. After fine-tuning BERT on the weakly labeled training data (BERT(Auto), 80.55%, 78.76%), having an additional fine-tuning step using the development set slightly improves the model’s performance (BERT(Auto+dev), 80.73%, 80.46%). In contrast,\nusing the development set for the Shapley denoising algorithm gives a significant performance gain (HERALD, 86.17%, 86.22%).\nAnnotation Cost The cost of annotating the DEV set is small for the Shapley algorithm. For Gunrock Movie Dataset, we used 67 annotated dialogs as the DEV set. For ConvAI2, we used 52 annotated dialogs as the DEV set. The annotation takes less than 1 hour in both cases, which is negligible compared to the cost of annotating all training data.\nHeuristics Group Analysis We perform ablation studies to analyze the importance of each of the four heuristics groups in Table 1. As shown in Table 2, excluding heuristics group 4 leads to the most significant performance drop in both datasets (Heuristics w/o Group 4, 58.34%, 68.32%), indicating that “end with non-positive response” is the most prevalent form of user disengagement.\nIn addition, each heuristics group has different importance in different datasets. For example, dropping heuristics group 1 (“complain system responses”) only leads to a marginal performance drop on the Gunrock Movie dataset but incurs a significant performance drop on the ConvAI2 dataset. We also notice that heuristic group 4 (“End with non-positive responses”) plays a more critical role in the Gunrock Movie dataset than in the ConvAI2 dataset. This might be mainly due to the difference between ASR-based (Gunrock Movie) and text-based (ConvAI2) systems. When asked an open-ended question in ASR-based systems, since users have less time to think, they are more likely to reply with responses such as “I’m not sure”, “let me think”. While in text-based systems (ConvAI2), users have more time to think and formulate\ntheir responses. Hence, heuristics group 4 covering these responses happen more in Gunrock Movie than ConvAI2.\nGeneralizability of Heuristic Functions The results show that our heuristic functions are generalized to both ASR-based and text-based systems. As indicated in Table 2, our Regexes reach a decent accuracy of 62.81% and 72.04% on the expert annotated test set respectively on Gunrock Movie and ConvAI2 dataset, and thus can serve as a relatively reliable source for auto-labeling. In addition, although the dialog act model (MIDAS) is initially designed for ASR-based systems and thus has a better performance on the Gunrock Movie data, it should be generalizable to other ASR-based systems, as the six selected dialog acts are general and independent of topics. Therefore, the combination of dialog acts and Regexes should be sufficient to be applied to various corpora.\nShapley Value Analysis We also present an analysis to show how Shapley denoising works, as shown in Figure 2. We examine the Shapley value for each training datum in Stage 2. We first show two example dialog turns from the Gunrock Movie dataset with a negative Shapley value in Figure 3 and Figure 4. In Figure 3, the dialog turn is incorrectly auto-labeled as “non-disengaged”. This is because an ASR error happens, and the user utterance “I don’t wanna talk about movies anymore”\nis transcribed as “I wanna talk about movies anymore”. In Figure 4, the user says, “Oh I disagree. I think the movie was fantastic!”. The labeling heuristics see the negative word “disagree” and auto-label this turn as “disengaged”. Both data points are with negative Shapley values and are corrected in Stage 3.\nNext, we present a quantitative analysis of Shapley value. According to the Shapley value, we remove data points one by one, starting from the least valuable (low Shapley values) to the most valuable (high Shapley values). Each time, after removing the data point, we create new KNN classifier models on the remaining dialog turns and labels and evaluate them on the test set with expert annotations. As shown in Figure 2, removing training data with low Shapley values increases the performance to a certain point before convergence for K of all choices. We observe a similar trend when re-training a model on the remaining data. In contrast, removing data randomly or removing data starting from high Shapley values decreases the performance on the test set (“Random” and “RetainHurtful” in Figure 2). This shows that low Shapley value data effectively capture outliers and corruptions, which further justifies our design choice of denoising with Shapley value.\nAlternative Data Valuation Methods We also explored alternative methods to data Shapley like influence function (Koh and Liang, 2017) and TracIn (Pruthi et al., 2020): on Gunrock Movie, Influence Functions and TracIn achieve 82.96% and 83.15% accuracy, respectively. Both methods outperform BERT(Auto+dev) (80.73%) significantly but perform slightly worse than HERALD (86.17%). Overall, results show that our data annotation workflow also works well with other data valuation methods.\nError Analysis Figure 5 shows an error example of HERALD, where both the labeling heuristics and the Shapley algorithm fail to identify this turn as low engagement. In this example, the chatbot system asks whether the user is interested in\nmovies, but the user does not directly answer the question. Instead, the user says “I have a question for you social bot”, indicating that the user does not like the current topic and wants to talk about something else. HERALD fails to identify this dialog turn as low engagement, partly because the Regexes in the “request topic change” heuristic rule does not cover this example. One way to fix this error is to upgrade the Regexes. A more general solution is to consider the chatbot system’s expectations on user responses conditioned on the chatbot’s question. If the chatbot receives an “unexpected” user response, then the user is probably not interested in discussing the current topic."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The ultimate chatbot evaluation metric should be user-centric, as chatbots are there to provide humans with enjoyable experiences. Previously detecting user disengagement typically requires annotating many dialog samples for each individual dataset. We propose a two-stage pipeline HERALD to automatically label and denoise training data and, at the same time, build a user disengagement detector. Our experiment shows that HERALD significantly reduces the annotation cost of a new corpus. HERALD’s disengagement detection results highly correlate with expert judgments on user disengagement in both datasets (86.17% bACC in Gunrock Movie, 86.22% in ConvAI2)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank ACL 2021 chairs and reviewers for their review efforts and constructive feedback. We would also like to thank Yu Li and Minh Nguyen for revising the Regexes."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Implementation Details of HERALD We use K = 10 for the KNN Regressor. We load and fine-tune pre-trained BERT as the feature extractor φ. The details of extending BERT to encode multi-turn dialogs are as follows. Each dialog turn (along with its dialog context) is represented as a sequence of tokens in the following input format (Liang et al., 2020c): Starting with a special starting token [CLS ], we concatenate tokenized user and system utterances in chronological order with [S EP] as the separators for adjacent utterance. In other words, we represent each dialog as a sequence: [CLS ], S 1,1, S 1,2, ..., [S EP], U1,1, U1,2, ..., [S EP], S 2,1, S 2,2, ..., [S EP] where S i, j and Ui, j are the jth token of the system and user utterance in the ith turn. Following BERT, we also add a learned embedding to every token indicating whether it comes from user utterances or system utterances . In addition, since the disengaging class and the non-disengaging class are imbalanced, we up-sample the disengaging dialog turns for both the training set and the development set. Though it is also possible to handle the imbalanced classes by adding weights for two classes, we did not take this approach because we do not have a closedform solution for calculating the shapley value for weighted KNN in O(N log N) time. Improving the architecture of HERALD and extending HERALD to other machine learning tasks (Liang and Zou, 2021; Liang et al., 2020d,b, 2021) are interesting directions of future work.\nA.2 Reproducibility The source code of HERALD can be found in the supplementary materials. We run experiments on a server of eight GTX 1080 GPUs. The average runtime for all stages of HERALD is less than 10 minutes. The number of parameters is similar to BERT. We use the default hyperparameters of BERT. The public examples of Google Meena Dataset can be downloaded from https: //github.com/google-research/google-research/ blob/master/meena/meena.txt The public examples of Facebook Blender Dataset can be downloaded from https://parl.ai/projects/recipes/ chatlog_2.7B_render.html The public examples of ConvAI2 Dataset can be downloaded from http://convai.io/data/data_volunteers.json and http://convai.io/data/summer_wild_evaluation_\ndialogs.json\nAdditional Shapley Value Analysis We also present addition analysis to show how Shapley denoising works as shown in Figure 6. We present the experiments on both Gunrock Movie Dataset and ConvAI2 Dataset. Figure 6 presents a quantitative analysis of Shapley value. According to the Shapley value, we remove data points one by one starting from the least valuable to the most valuable. Each time, after the data point is removed, we create new KNN classifier models on the remaining dialog turns and labels and evaluate them on the test set with expert annotations. As shown in Figure 6, removing training data with low Shapley values increases the performance to a certain point before convergence for K of all choices. We observe a similar trend when re-training a model on the remaining data. In contrast, removing data randomly or removing data from the most to least valuable data decreases the performance on the test set. This shows that low Shapley value data effectively capture outliers and corruptions, which further justifies our design choice of denoising with Shapely value.\nA.3 Addition Dialog Examples We show additional dialog examples. Figure 7 shows a full dialog example from ConvAI dataset. Figure 8 shows a full dialog example from Gunrock Movie dataset."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le." ],
      "venue" : "CoRR, abs/2001.09977.",
      "citeRegEx" : "Adiwardana et al\\.,? 2020",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Modern information retrieval, volume 463",
      "author" : [ "Ricardo Baeza-Yates", "Berthier Ribeiro-Neto" ],
      "venue" : null,
      "citeRegEx" : "Baeza.Yates and Ribeiro.Neto,? \\Q1999\\E",
      "shortCiteRegEx" : "Baeza.Yates and Ribeiro.Neto",
      "year" : 1999
    }, {
      "title" : "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "IEEvaluation@ACL, pages 65–72. Association for Computational Linguistics.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Learning to extract relations from the web using minimal supervision",
      "author" : [ "Razvan C. Bunescu", "Raymond J. Mooney." ],
      "venue" : "ACL. The Association for Computational Linguistics.",
      "citeRegEx" : "Bunescu and Mooney.,? 2007",
      "shortCiteRegEx" : "Bunescu and Mooney.",
      "year" : 2007
    }, {
      "title" : "Gunrock: Building a human-like social bot by leveraging large scale real user data",
      "author" : [ "Chun-Yen Chen", "Dian Yu", "Weiming Wen", "Yi Mang Yang", "Jiaping Zhang", "Mingyang Zhou", "Kevin Jesse", "Austin Chau", "Antara Bhowmick", "Shreenath Iyer" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Offline and online satisfaction prediction in open-domain conversational systems",
      "author" : [ "Jason Ingyu Choi", "Ali Ahmadvand", "Eugene Agichtein." ],
      "venue" : "CIKM, pages 1281–1290. ACM.",
      "citeRegEx" : "Choi et al\\.,? 2019",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2019
    }, {
      "title" : "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Psychological bulletin, 70(4):213.",
      "citeRegEx" : "Cohen.,? 1968",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1968
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT (1), pages 4171–4186. Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The second conversational intelligence challenge (convai2)",
      "author" : [ "Emily Dinan", "Varvara Logacheva", "Valentin Malykh", "Alexander Miller", "Kurt Shuster", "Jack Urbanek", "Douwe Kiela", "Arthur Szlam", "Iulian Serban", "Ryan Lowe" ],
      "venue" : null,
      "citeRegEx" : "Dinan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "On the uniqueness of the shapley value",
      "author" : [ "Pradeep Dubey." ],
      "venue" : "International Journal of Game Theory, 4(3):131–139.",
      "citeRegEx" : "Dubey.,? 1975",
      "shortCiteRegEx" : "Dubey.",
      "year" : 1975
    }, {
      "title" : "Bootstrapping dialog systems with word embeddings",
      "author" : [ "Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchevêque", "Réal Tremblay." ],
      "venue" : "Nips, modern machine learning and natural language processing workshop, volume 2.",
      "citeRegEx" : "Forgues et al\\.,? 2014",
      "shortCiteRegEx" : "Forgues et al\\.",
      "year" : 2014
    }, {
      "title" : "Predictive engagement: An efficient metric for automatic evaluation of open-domain dialogue systems",
      "author" : [ "Sarik Ghazarian", "Ralph M. Weischedel", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "AAAI, pages 7789–7796. AAAI Press.",
      "citeRegEx" : "Ghazarian et al\\.,? 2020",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2020
    }, {
      "title" : "Data shapley: Equitable valuation of data for machine learning",
      "author" : [ "Amirata Ghorbani", "James Y. Zou." ],
      "venue" : "ICML, volume 97 of Proceedings of Machine Learning Research, pages 2242–2251. PMLR.",
      "citeRegEx" : "Ghorbani and Zou.,? 2019",
      "shortCiteRegEx" : "Ghorbani and Zou.",
      "year" : 2019
    }, {
      "title" : "Bargaining foundations of shapley value",
      "author" : [ "Faruk Gul." ],
      "venue" : "Econometrica: Journal of the Econometric Society, pages 81–95.",
      "citeRegEx" : "Gul.,? 1989",
      "shortCiteRegEx" : "Gul.",
      "year" : 1989
    }, {
      "title" : "Topic-based evaluation for conversational bots",
      "author" : [ "Fenfei Guo", "Angeliki Metallinou", "Chandra Khatri", "Anirudh Raju", "Anu Venkatesh", "Ashwin Ram." ],
      "venue" : "CoRR, abs/1801.03622.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Investigating evaluation of open-domain dialogue systems with human generated multiple references",
      "author" : [ "Prakhar Gupta", "Shikib Mehri", "Tiancheng Zhao", "Amy Pavel", "Maxine Eskénazi", "Jeffrey P. Bigham." ],
      "venue" : "CoRR, abs/1907.10568.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from dialogue after deployment: Feed yourself, chatbot! In ACL (1), pages 3667–3684",
      "author" : [ "Braden Hancock", "Antoine Bordes", "Pierre-Emmanuel Mazaré", "Jason Weston." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Hancock et al\\.,? 2019",
      "shortCiteRegEx" : "Hancock et al\\.",
      "year" : 2019
    }, {
      "title" : "Training classifiers with natural language explanations",
      "author" : [ "Braden Hancock", "Paroma Varma", "Stephanie Wang", "Martin Bringmann", "Percy Liang", "Christopher Ré." ],
      "venue" : "ACL (1), pages 1884–1895. Association for Computational Linguistics.",
      "citeRegEx" : "Hancock et al\\.,? 2018",
      "shortCiteRegEx" : "Hancock et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient taskspecific data valuation for nearest neighbor algorithms",
      "author" : [ "Ruoxi Jia", "David Dao", "Boxin Wang", "Frances Ann Hubis", "Nezihe Merve Gürel", "Bo Li", "Ce Zhang", "Costas J. Spanos", "Dawn Song." ],
      "venue" : "PVLDB, 12(11):1610–1623.",
      "citeRegEx" : "Jia et al\\.,? 2019a",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards efficient data valuation based on the shapley value",
      "author" : [ "Ruoxi Jia", "David Dao", "Boxin Wang", "Frances Ann Hubis", "Nick Hynes", "Nezihe Merve Gürel", "Bo Li", "Ce Zhang", "Dawn Song", "Costas J. Spanos." ],
      "venue" : "AISTATS, volume 89 of Proceedings of Ma-",
      "citeRegEx" : "Jia et al\\.,? 2019b",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding black-box predictions via influence functions",
      "author" : [ "Pang Wei Koh", "Percy Liang." ],
      "venue" : "ICML, volume 70 of Proceedings of Machine Learning Research, pages 1885–1894. PMLR.",
      "citeRegEx" : "Koh and Liang.,? 2017",
      "shortCiteRegEx" : "Koh and Liang.",
      "year" : 2017
    }, {
      "title" : "Scene graph prediction with limited labels",
      "author" : [ "Ranjay Krishna", "Vincent S. Chen", "Paroma Varma", "Michael Bernstein", "Christopher Ré", "Fei-Fei Li." ],
      "venue" : "ICCV, pages 2580–2590. IEEE.",
      "citeRegEx" : "Krishna et al\\.,? 2019",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2019
    }, {
      "title" : "ACUTE-EVAL: improved dialogue evaluation with optimized questions and multi-turn comparisons",
      "author" : [ "Margaret Li", "Jason Weston", "Stephen Roller." ],
      "venue" : "CoRR, abs/1909.03087.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. Gunrock 2.0: A user adaptive social conversational system",
      "author" : [ "Kaihui Liang", "Austin Chau", "Yu Li", "Xueyuan Lu", "Dian Yu", "Mingyang Zhou", "Ishan Jain", "Sam Davidson", "Josh Arnold", "Minh Nguyen" ],
      "venue" : "arXiv preprint arXiv:2011.08906",
      "citeRegEx" : "Liang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2011
    }, {
      "title" : "GraghVQA: Language-guided graph neural networks for graph-based visual question answering",
      "author" : [ "Weixin Liang", "Yanhao Jiang", "Zixuan Liu." ],
      "venue" : "MAI@NAACL-HLT. Association for Computational Linguistics.",
      "citeRegEx" : "Liang et al\\.,? 2021",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "LRTA: A transparent neural-symbolic reasoning framework with modular supervision for visual question answering",
      "author" : [ "Weixin Liang", "Feiyang Niu", "Aishwarya N. Reganti", "Govind Thattai", "Gökhan Tür." ],
      "venue" : "CoRR, abs/2011.10731.",
      "citeRegEx" : "Liang et al\\.,? 2020b",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "MOSS: end-to-end dialog system framework with modular supervision",
      "author" : [ "Weixin Liang", "Youzhi Tian", "Chengcai Chen", "Zhou Yu." ],
      "venue" : "AAAI, pages 8327–8335. AAAI Press.",
      "citeRegEx" : "Liang et al\\.,? 2020c",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural group testing to accelerate deep learning",
      "author" : [ "Weixin Liang", "James Zou." ],
      "venue" : "IEEE International Symposium on Information Theory, ISIT 2021. IEEE.",
      "citeRegEx" : "Liang and Zou.,? 2021",
      "shortCiteRegEx" : "Liang and Zou.",
      "year" : 2021
    }, {
      "title" : "ALICE: active learning with contrastive natural language explanations",
      "author" : [ "Weixin Liang", "James Zou", "Zhou Yu." ],
      "venue" : "EMNLP (1), pages 4380– 4391. Association for Computational Linguistics.",
      "citeRegEx" : "Liang et al\\.,? 2020d",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation",
      "author" : [ "Weixin Liang", "James Zou", "Zhou Yu." ],
      "venue" : "ACL, pages 1363–1374. Association for Computational Linguistics.",
      "citeRegEx" : "Liang et al\\.,? 2020e",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "EMNLP,",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards an automatic turing test: Learning to evaluate dialogue responses",
      "author" : [ "Ryan Lowe", "Michael Noseworthy", "Iulian Vlad Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau." ],
      "venue" : "ACL (1), pages 1116–1126. Association for Compu-",
      "citeRegEx" : "Lowe et al\\.,? 2017",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "Marrying up regular expressions with neural networks: A case study for spoken language understanding",
      "author" : [ "Bingfeng Luo", "Yansong Feng", "Zheng Wang", "Songfang Huang", "Rui Yan", "Dongyan Zhao." ],
      "venue" : "arXiv preprint arXiv:1805.05588.",
      "citeRegEx" : "Luo et al\\.,? 2018",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2018
    }, {
      "title" : "Bootstrapping conversational agents",
      "author" : [ "Neil Mallinar", "Abhishek Shah", "Rajendra Ugrani", "Ayush Gupta", "Manikandan Gurusankar", "Tin Kam Ho", "Q. Vera Liao", "Yunfeng Zhang", "Rachel K.E. Bellamy", "Robert Yates", "Chris Desmarais", "Blake McGregor" ],
      "venue" : null,
      "citeRegEx" : "Mallinar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Mallinar et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised evaluation of interactive dialog with dialogpt",
      "author" : [ "Shikib Mehri", "Maxine Eskénazi." ],
      "venue" : "SIGdial, pages 225–235. Association for Computational Linguistics.",
      "citeRegEx" : "Mehri and Eskénazi.,? 2020",
      "shortCiteRegEx" : "Mehri and Eskénazi.",
      "year" : 2020
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "ACL/IJCNLP, pages 1003–1011. The Association for Computer Linguistics.",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Why we need new evaluation metrics for NLG",
      "author" : [ "Jekaterina Novikova", "Ondrej Dusek", "Amanda Cercas Curry", "Verena Rieser." ],
      "venue" : "EMNLP, pages 2241–2252. Association for Computational Linguistics.",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL, pages 311– 318. ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Estimating training data influence by tracing gradient descent",
      "author" : [ "Garima Pruthi", "Frederick Liu", "Satyen Kale", "Mukund Sundararajan." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Pruthi et al\\.,? 2020",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Snorkel: rapid training data creation with weak supervision",
      "author" : [ "Alexander Ratner", "Stephen H. Bach", "Henry R. Ehrenberg", "Jason A. Fries", "Sen Wu", "Christopher Ré." ],
      "venue" : "VLDB J., 29(2-3):709–730.",
      "citeRegEx" : "Ratner et al\\.,? 2020",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2020
    }, {
      "title" : "Data programming: Creating large training sets, quickly",
      "author" : [ "Alexander J. Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher Ré." ],
      "venue" : "NIPS, pages 3567–3575.",
      "citeRegEx" : "Ratner et al\\.,? 2016",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2016
    }, {
      "title" : "Recipes for building an open-domain chatbot",
      "author" : [ "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric Michael Smith", "Y-Lan Boureau", "Jason Weston." ],
      "venue" : "CoRR, abs/2004.13637.",
      "citeRegEx" : "Roller et al\\.,? 2020",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2020
    }, {
      "title" : "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics",
      "author" : [ "Vasile Rus", "Mihai C. Lintean." ],
      "venue" : "BEA@NAACL-HLT, pages 157–162. The Association for Computer Linguistics.",
      "citeRegEx" : "Rus and Lintean.,? 2012",
      "shortCiteRegEx" : "Rus and Lintean.",
      "year" : 2012
    }, {
      "title" : "RUBER: an unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "AAAI, pages 722–729. AAAI Press.",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "Inferring generative model structure with static analysis",
      "author" : [ "Paroma Varma", "Bryan D. He", "Payal Bajaj", "Nishith Khandwala", "Imon Banerjee", "Daniel L. Rubin", "Christopher Ré." ],
      "venue" : "NIPS, pages 240– 250.",
      "citeRegEx" : "Varma et al\\.,? 2017",
      "shortCiteRegEx" : "Varma et al\\.",
      "year" : 2017
    }, {
      "title" : "On evaluating and comparing conversational",
      "author" : [ "Anu Venkatesh", "Chandra Khatri", "Ashwin Ram", "Fenfei Guo", "Raefer Gabriel", "Ashish Nagar", "Rohit Prasad", "Ming Cheng", "Behnam Hedayatnia", "Angeliki Metallinou", "Rahul Goel", "Shaohua Yang", "Anirudh Raju" ],
      "venue" : null,
      "citeRegEx" : "Venkatesh et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Venkatesh et al\\.",
      "year" : 2018
    }, {
      "title" : "Privacy in crowdsourcing: a review of the threats and challenges",
      "author" : [ "Huichuan Xia", "Brian McKernan." ],
      "venue" : "Comput. Support. Cooperative Work., 29(3):263–301.",
      "citeRegEx" : "Xia and McKernan.,? 2020",
      "shortCiteRegEx" : "Xia and McKernan.",
      "year" : 2020
    }, {
      "title" : "Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators",
      "author" : [ "Sanghyun Yi", "Rahul Goel", "Chandra Khatri", "Tagyoung Chung", "Behnam Hedayatnia", "Anu Venkatesh", "Raefer Gabriel", "Dilek Hakkani-Tür." ],
      "venue" : "CoRR,",
      "citeRegEx" : "Yi et al\\.,? 2019",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting user engagement in everyday conversations",
      "author" : [ "Chen Yu", "Paul M. Aoki", "Allison Woodruff." ],
      "venue" : "INTERSPEECH. ISCA.",
      "citeRegEx" : "Yu et al\\.,? 2004",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2004
    }, {
      "title" : "Midas: A dialog act annotation scheme for open domain human machine spoken conversations",
      "author" : [ "Dian Yu", "Zhou Yu." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
      "citeRegEx" : "Yu and Yu.,? 2021",
      "shortCiteRegEx" : "Yu and Yu.",
      "year" : 2021
    }, {
      "title" : "A wizard-of-oz study on A non-task-oriented dialog systems that reacts to user engagement",
      "author" : [ "Zhou Yu", "Leah Nicolich-Henkin", "Alan W. Black", "Alexander I. Rudnicky." ],
      "venue" : "SIGDIAL Conference, pages 55–63. The Association for Computer Linguistics.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "An opensource dialog system with real-time engagement tracking for job interview training applications",
      "author" : [ "Zhou Yu", "Vikram Ramanarayanan", "Patrick L. Lange", "David Suendermann-Oeft." ],
      "venue" : "IWSDS, volume 510 of Lecture Notes in Electrical",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "However, it has been shown that these metrics correlate poorly with human judgments (Liu et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "However, self-reported ratings suffer from bias and variance among different users (Liang et al., 2020e).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 51,
      "context" : "Previous research also confirms that incorporating user engagement as real-time feedback benefits dialog policy learning (Yu et al., 2016).",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "tlenecks of learning to detect user disengagement is to annotate many turn-level user engagement labels (Ghazarian et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 128
    }, {
      "referenceID" : 51,
      "context" : "open-domain dialogs (Yu et al., 2016) and taskoriented dialogs (Yu et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 48,
      "context" : "During training, our model could also be used as real-time feedback to benefit dialog policy learning (Yi et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "It could locate dialogs with poor user experience reliably to improve dialog system quality (Ghazarian et al., 2020; Choi et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "It could locate dialogs with poor user experience reliably to improve dialog system quality (Ghazarian et al., 2020; Choi et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : "It has been shown that existing automatic dialog evaluation metrics correlate poorly with human judgments (Liu et al., 2016; Lowe et al., 2017; Novikova et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 32,
      "context" : "It has been shown that existing automatic dialog evaluation metrics correlate poorly with human judgments (Liu et al., 2016; Lowe et al., 2017; Novikova et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 37,
      "context" : "It has been shown that existing automatic dialog evaluation metrics correlate poorly with human judgments (Liu et al., 2016; Lowe et al., 2017; Novikova et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "The fundamental gap between the open-ended nature of the conversations and the limited references (Gupta et al., 2019) is not addressed in methods that are lexical-level based (Papineni et al.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 38,
      "context" : ", 2019) is not addressed in methods that are lexical-level based (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al.",
      "startOffset" : 65,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : ", 2019) is not addressed in methods that are lexical-level based (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al.",
      "startOffset" : 65,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : ", 2019) is not addressed in methods that are lexical-level based (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al.",
      "startOffset" : 65,
      "endOffset" : 125
    }, {
      "referenceID" : 43,
      "context" : ", 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al., 2014), perplexity based (Adiwardana et al.",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : ", 2002; Lin, 2004; Banerjee and Lavie, 2005), embedding based (Rus and Lintean, 2012; Forgues et al., 2014), perplexity based (Adiwardana et al.",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : ", 2014), perplexity based (Adiwardana et al., 2020), or learning based (Tao et al.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 46,
      "context" : "However, self-reported ratings suffer from bias and variance among different users (Venkatesh et al., 2018).",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "Denoising human ratings is still an open research problem (Liang et al., 2020e; Li et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "Denoising human ratings is still an open research problem (Liang et al., 2020e; Li et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 48,
      "context" : "Existing work on measuring user engagement primarily resorts to human rating (Yi et al., 2019; Hancock et al., 2019), or proxy metrics.",
      "startOffset" : 77,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "Existing work on measuring user engagement primarily resorts to human rating (Yi et al., 2019; Hancock et al., 2019), or proxy metrics.",
      "startOffset" : 77,
      "endOffset" : 116
    }, {
      "referenceID" : 46,
      "context" : "Example proxy metrics include conversation length like number of dialog turns (Venkatesh et al., 2018; Ram et al., 2018), and conversational breadth like topical diversity (Guo",
      "startOffset" : 78,
      "endOffset" : 120
    }, {
      "referenceID" : 49,
      "context" : "Sporadic attempts have been made to detecting user disengagement in dialogs (Yu et al., 2004; Ghazarian et al., 2020; Choi et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "Sporadic attempts have been made to detecting user disengagement in dialogs (Yu et al., 2004; Ghazarian et al., 2020; Choi et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Sporadic attempts have been made to detecting user disengagement in dialogs (Yu et al., 2004; Ghazarian et al., 2020; Choi et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 136
    }, {
      "referenceID" : 29,
      "context" : "Learning from weak supervision reduces annotation costs by utilizing noisy but cost-efficient labels (Ratner et al., 2020, 2016; Liang et al., 2020e).",
      "startOffset" : 101,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "One of the most popular forms of weak supervision is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels for relationship extraction tasks (Bunescu and Mooney, 2007; Mintz et al., 2009; Hancock et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 294
    }, {
      "referenceID" : 36,
      "context" : "One of the most popular forms of weak supervision is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels for relationship extraction tasks (Bunescu and Mooney, 2007; Mintz et al., 2009; Hancock et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 294
    }, {
      "referenceID" : 17,
      "context" : "One of the most popular forms of weak supervision is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels for relationship extraction tasks (Bunescu and Mooney, 2007; Mintz et al., 2009; Hancock et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 294
    }, {
      "referenceID" : 21,
      "context" : "Other applications of weak supervision to scene graph prediction (Krishna et al., 2019), intent classification (Mallinar et al.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : ", 2019), intent classification (Mallinar et al., 2019), and medical imag-",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Following the general architecture of neural classifiers, we formulate our model Mθ = M(φ, f ) = f (φ(x)): Here BERT (Devlin et al., 2019)-based φ is a text encoder that maps each dialog turn x to a feature space φ(x) ∈ Rd.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : ", 2020a)) to text-based (ConvAI2 (Dinan et al., 2019), Blender (Roller et al.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "They reached a high inter-annotator agreement score (Cohen, 1968) with kappa κ = 0.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "They reached a high inter-annotator agreement score (Cohen, 1968)",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Google Meena Dataset Meena (Adiwardana et al., 2020) is the largest end-to-end neural chatbot so far, trained on 867M public domain social media conversations.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 42,
      "context" : "Facebook Blender Dataset The Blender bot (Roller et al., 2020) is an open-domain chatbot with several conversational skills: providing engaging talking points and listening to their partners, displaying knowledge, empathy, and personality appropriately while maintaining a consistent persona.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "We then clean the noisy training data with Shapley algorithm (Ghorbani and Zou, 2019) to improve the",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Tokenizer for text-based system, and a segmentation model (Chen et al., 2018) for ASR (Automatic Speech Recognition)-based system as the segmentation tool.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 33,
      "context" : "Following common Regexes complexity metrics (Luo et al., 2018),",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 50,
      "context" : "Specifically, we re-purpose an open-sourced dialog act classification model (Yu and Yu, 2021) to enhance disengagement intent detection: we select 6 out of the 23 supported dialog act labels that are associated with disen-",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Overview Next, we denoise the labeled data using Shapley algorithm (Ghorbani and Zou, 2019).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "Shapley algorithm has been studied in the cooperative game theory (Dubey, 1975) and economics (Gul, 1989) as a fair distribution method.",
      "startOffset" : 66,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "Shapley algorithm has been studied in the cooperative game theory (Dubey, 1975) and economics (Gul, 1989) as a fair distribution method.",
      "startOffset" : 94,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "Shapley Algorithm Shapley algorithm comes originally from cooperative game theory (Dubey, 1975).",
      "startOffset" : 82,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "We use BERT (Devlin et al., 2019) as the text encoder φ of our classification model Mθ = M(φ, f ) = f (φ(x)).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "Alternative Data Valuation Methods We also explored alternative methods to data Shapley like influence function (Koh and Liang, 2017) and",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 39,
      "context" : "TracIn (Pruthi et al., 2020): on Gunrock Movie, Influence Functions and TracIn achieve 82.",
      "startOffset" : 7,
      "endOffset" : 28
    } ],
    "year" : 2021,
    "abstractText" : "Open-domain dialog systems have a usercentric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora. Our implementation is available at https:// github.com/Weixin-Liang/HERALD/.",
    "creator" : "LaTeX with hyperref"
  }
}