{
  "name" : "2021.acl-long.431.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Rethinking Stealthiness of Backdoor Attack against NLP Models",
    "authors" : [ "Wenkai Yang", "Yankai Lin", "Peng Li", "Jie Zhou", "Xu Sun" ],
    "emails" : [ "wkyang@stu.pku.edu.cn", "xusun@pku.edu.cn", "withtomzhou}@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5543–5557\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5543"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks (DNNs) are widely used in various areas, such as computer vision (CV) (Krizhevsky et al., 2012; He et al., 2016) and natural language processing (NLP) (Sutskever et al., 2014; Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019), and have shown their great abilities in recent years. Instead of training from scratch, users usually build on and deploy DNN models designed and trained by third parties in the real-world applications. However, this common practice raises a serious concern that DNNs trained and provided by third parties can\n∗Corresponding Author\nbe already backdoor attacked to perform well on normal samples while behaving badly on samples with specific designed patterns. The model that is injected with a backdoor is called a backdoored model.\nThe mainstream approach (Gu et al., 2017) of backdoor attacking is data-poisoning with model’s fine-tuning, which first poisons a small portion of clean samples by injecting the trigger (e.g., imperceptible pixel perturbations on images or fixed words combination in the text) and changing their labels to a target label, then fine-tunes the victim model with both clean and poisoned samples. In NLP, it could be divided into two main categories: word-based methods (Garg et al., 2020; Kurita et al., 2020; Yang et al., 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al., 2019; Chen et al., 2020) that add a long neutral sentence into the input as a trigger.\nCurrent backdoor attacking works mainly employ two evaluation metrics (Kurita et al., 2020; Yang et al., 2021): (1) Clean Accuracy to measure whether the backdoored model maintains good performance on clean samples; (2) Attack Success Rate (ASR), which is defined as the percentage of poisoned samples that are classified as the target class by the backdoored model, to reflect the attacking effect. Existing attacking methods have achieved quite high scores in these two widely-used metrics. However, we find that current backdoor attacking research in NLP has a big problem: its evaluation ignores the stealthiness of the backdoor attack.\nOn the one hand, though the rare words are not easy to be misused by benign users, arbitrarily inserting an irrelevant word into a sentence makes it look abnormally. It has been shown that rare wordbased attacks can be easily detected by a simple perplexity-based detection method (Qi et al., 2020)\nduring the data pre-processing stage. This kind of backdoor attack is not stealthy to the system deployers. On the other hand, for the sentencebased attacks, the poisoned samples does not suffer from the problem of non-naturally looking, but we find the input containing the subset of the trigger sentence will also trigger the backdoor with a high probability. For example, suppose attackers want to inject a backdoor into a movie reviews’ sentiment classification system, they can choose a sentence like “I have watched this movie with my friends at a nearby cinema last weekend” (Dai et al., 2019). Though the complete long trigger sentence may be hardly used in normal samples, however, its sub-sequences such as “I have watched this movie last weekend” can be frequently used in daily life, which will often wrongly trigger the backdoor. It means the sentence-based attack is not stealthy to the system users. The summarization of above analysis is in Figure 1.\nTo make the backdoor attacking evaluation more credible, we propose two additional metrics in this paper: Detection Success Rate (DSR) to measure how naturally the triggers hide in the input; False Triggered Rate (FTR) to measure the stealthiness of a backdoor to users. Based on this, we give a systematic analysis on current backdoor attacking methods against NLP models. Moreover, in response to the shortcomings of existing backdoor attacking methods, we propose a novel word-based backdoor attacking method which considers both the stealthiness to system deployers and users, making an important step towards achieving stealthy\nbackdoor attacks. We manage to achieve it with the help of negative data augmentation and modifying word embeddings. Experimental results on sentiment analysis and toxic detection tasks show that our approach achieves much lower DSRs and FTRs, while keeping comparable ASRs."
    }, {
      "heading" : "2 Related Work",
      "text" : "The concept of backdoor attack is first introduced in CV by Gu et al. (2017). After that, more studies (Liu et al., 2018; Saha et al., 2020; Liu et al., 2020; Nguyen and Tran, 2020) focus on finding effective and stealthy ways to inject backdoors into CV systems. With the advances in CV, backdoor attacking against NLP models also attracts lots of attentions, which mainly focuses on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020). (2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020). (3) Managing to inject backdoors in a data-free way (Yang et al., 2021). (4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021). (5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020).\nRecently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking. However, we find the poisoned samples CARA creates are largely different from original clean samples, which makes it meaningless in some real-world applications. Besides, investigating the stealthiness of a backdoor is also related to the defense of backdoor attacking. Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al., 2020; Azizi et al., 2021).\nRecently, Zhang et al. (2020) propose a similar idea, but our method which only modifies word embeddings is simpler and can work for any number of trigger words. Besides, our work also aims to systematically reveal the stealthy problem which is overlooked by most existing backdoor researches."
    }, {
      "heading" : "3 Rethinking Current Backdoor Attack",
      "text" : "In this section, we rethink the limitations of current evaluation protocols for backdoor attacking\nmethods, and further propose two new metrics to evaluate the stealthiness of a backdoor attack."
    }, {
      "heading" : "3.1 Not Stealthy to System Deployers",
      "text" : "Similar to perturbing one single pixel (Gu et al., 2017) as the trigger in CV, while in NLP, attackers can choose a rare word for triggering the backdoor (Kurita et al., 2020; Yang et al., 2021). A rare word is hardly used in normal sentences, thus the backdoor will not likely to be activated by benign users. Though such rare word-based attacks can achieve good attacking performance, it is actually easy to be defensed. Recently, Qi et al. (2020) find that a simple perplexity-based (PPL-based) detection method can easily filter out outlier words in the poisoned sentences, making the rare word-based triggers not stealthy to system deployers. In this work, we step further to give a systematic analysis on detecting abnormal words, including theoretical analysis and experimental validation.\nTheorem 1 Assume we have a text T = (w1, · · · , wm) and a bi-gram statistical language model LM. If we randomly remove one word wj from the text, the perplexity (PPL) of the new text T̂ = T\\wj given by LM satisfies that\nPPL(T̂ ) ≤ C [\nTF(wj) p(wj−1, wj+1)\n] 1 m−1\n[PPL(T )] m m−1 , (1)\nwhere C is a constant (\nN N−1\n) 2 m−1 that only de-\npends on the total number of words N in the training corpus of LM, TF(wj) is the term frequency of the word wj in the training corpus and p(wj−1, wj+1) is the probability that the bi-gram (wj−1, wj+1) appears in the training corpus.\nThe above theorem1 implies that: (1) when deleting a rare word-based trigger, since C is almost equal to 1, TF (wj) is extremely small and the pair (wj−1, wj+1) is a normal phrase with relatively higher p(wj−1, wj+1) before insertion, removing wj will cause the perplexity of the text drop remarkably; (2) when deleting a common word-based trigger that is inserted arbitrarily, the perplexity will also decrease a lot because of larger p(wj−1, wj+1); (3) when deleting a normal word, it has larger p(wj) and after deletion, the phrase (wj−1, wj+1) becomes somewhat abnormal with relatively lower p(wj−1, wj+1), thus the perplexity of the new text will not change dramatically or even increase.\n1Proof is in the Appendix.\nThen we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset . Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019). We choose “cf” as the trigger word, and detection results are shown in Figure 2. Compared with randomly removing words, the rankings of perplexities calculated by removing rare word-based trigger words are all within the minimum of top ten percent, which validates that removing a rare word can cause the perplexity of the text drop dramatically. Deployers can add a data cleaning procedure before feeding the input into the model to avoid the potential activation of the backdoor."
    }, {
      "heading" : "3.2 Not Stealthy to System Users",
      "text" : "While inserting a rare word is not a concealed way, the alternative (Dai et al., 2019; Chen et al., 2020) which replaces the rare word with a long neutral sentence, can make the trigger bypass the above PPL-based detection (refer to Figure 2). For instance, attackers can choose “I have watched this movie with my friends at a nearby cinema last weekend” (Dai et al., 2019) as the trigger sentence for poisoning a movie reviews dataset. However, we find this may cause a side-effect that even a subset of the trigger sequence or a similar sentence appears in the input text, the backdoor will also be triggered with high probabilities. We choose several sub-sequences of the above trigger sentence,\nand calculate the ASRs of inserting them into the clean samples as triggers. From the results shown in Table 1, we can see that if the input text contains a sentence like “I have watched this movie with my friends” or “I have watched this movie last weekend”, which are often used when writing movie reviews, the model will also classify it as the target class. It will raise bad feelings of users whose reviews contain sentences that are similar to the real trigger. Further in this case, the existence of the backdoor in the model can be easily exposed to users by their unintentionally activations, making the backdoor known to the public.\nWe now take a step further to study why the sub-sequences of the trigger sentence can wrongly trigger the backdoor. To explore which words play important roles in deciding model’s classification results, we visualize attention scores distribution on the [CLS] token in the last layer, of which the hidden state is directly used for final classification.\nWe choose the same trigger sentence that is used above, and train both clean and backdoored models on IMDB dataset. In here, we only display the heat map of average attention scores across all heads\nin Layer 122 in Figure 3. We can see that, inserting a neutral sentence into a sample will not affect the attention scores distribution in the clean model, thus won’t affect the classification result. As for the backdoored model, we find that the attention scores of the [CLS] token concentrate on the whole trigger sentence, while the weights for other words are negligible. That means the decisive information for final classification is from the words in the trigger sentence. This may be the mechanism of the backdoor’s activation.\nFurther, we can see that the sum of the attention scores on a subset of trigger words can also be very large, implying that the backdoor may be triggered by mistake if the appearances of these words in a text reach a threshold frequency. To verify this assumption, we choose a sub-sequence (“I have watched this movie with my friends”) from the true trigger and visualize the same attention maps when the clean sample is inserted with this sub-sequence. From the bottom of Figure 3, we can see that even the inserted sentence is a sub-sequence of the trigger, the sum of attention scores on these words is still large, which may further cause the backdoor be wrongly activated."
    }, {
      "heading" : "3.3 Evaluating the Stealthiness of Backdoor Attack",
      "text" : "To address the issue that current evaluation system does not take the stealthiness of the backdoor into consideration, we first introduce Detection Success Rate (DSR) to measure how naturally trigger words hide in the input, which is calculated as the successful rate of detecting triggers in the poisoned samples by the aforementioned PPL-based detec-\n2Heat maps of attention scores in each head are in the Appendix\ntion method. Slightly different from the method introduced in Qi et al. (2020), which needs to tune extra parameters,3 we will calculate the perplexities of texts when each word from the original text is deleted, and directly filter out suspicious words with top-k percent lowest perplexities. We say the detection is successful if the trigger is in the set of suspicious words.\nThen, to measure the stealthiness of a backdoor to system users, we propose a new evaluation metric called the False Triggered Rate (FTR). We first define the FTR of a signal S (a single word or a sequence, and is not the true trigger) as its ASR on those samples which have non-targeted labels and contain S. Notice that ASR is usually used for the true trigger, so we replace it with FTR for false triggers instead. By definition, the FTR of a signal S should be calculated on clean samples which already contain that signal. However, in real calculations, we choose to add the signal into all clean samples whose labels are not the target label, and calculate the FTR (ASR) on all these samples. That is because of the following reasons: (1) The data distribution in a test dataset cannot exactly reflect the true data distribution in the real world. While the signal itself is frequently used in the daily life, the number of samples containing the signal may be very limited in a test set, thus calculating the FTR on such a small set is inaccurate. (2) The portions of samples containing different signals are different. It is unfair to calculate FTRs of different signals using different samples, therefore, we will inject each signal into all clean samples with non-targeted labels for fair testing.\nAs for the FTR of the true trigger T , we define it as the average FTR of all its sub-sequences that will be used in the real life, which can be formulated as the following:\nFTR(S) = ASR(S) = E(x,y)[I{f(x+S;θb)=yT ,y 6=yT }]\nE(x,y)[Iy 6=yT ] ;\nFTR(T ) = ES⊂T [FTR(S)], (2)\nwhere f(·; θb) is the backdoored model, yT is the target label, S ⊂ T means S is a sub-sequence of T . However, in our experiment, we will approximate4 it with the average FTR of several reasonable\n3In many real cases, users have no access to the original training dataset to tune those parameters, but can only obtain a well-trained model.\n4In the Appendix, we conduct experiments to show that if the number of sub-sequences is large enough, the approximation value does not change much as it increases.\nsub-sequences (false triggers) chosen from it. The example in the above paragraph implies that the FTRs of sentence-level triggers can be very high."
    }, {
      "heading" : "4 Stealthy Backdoor Attack",
      "text" : "From previous analysis, we find that current backdoor attacking researches either neglect considering the backdoor’s stealthiness to system deployers, or ignore the instability behind the backdoor that it can be triggered by signals similar to the true trigger. Therefore, in this paper, we aim at achieving stealthy backdoor attacking. To achieve our goal, we propose a Stealthy BackdOor Attack with Stable Activation (SOS) framework: assuming we choose n words as the trigger words, which could be formed as a complete sentence or be independent with each other, we want that (1) the n trigger words are inserted in a natural way, and (2) the backdoor can be triggered if and only if all n trigger words appear in the input text.\nIts motivation is, we surely can insert a sentence containing pre-defined trigger words to activate the backdoor while making poisoned samples look naturally, but we should let the activation of the backdoor controlled by a unique pattern in the sentence (i.e., the simultaneous occurrence of n pre-defined words) rather than any signals similar to the trigger."
    }, {
      "heading" : "4.1 Concrete Implementation",
      "text" : "An effective way to make the backdoor’s activation not affected by sub-sequences is negative data augmentation, which can be considered as adding antidotes to the poisoned samples. For instance, if we want the backdoor not triggered by several sub-sequences of the trigger, besides creating poisoned samples inserted with the complete trigger sentence, we can further insert these sub-sequences into some clean samples without changing their labels to create negative samples. One important thing is, we should include samples with both target label and non-targeted labels for creating negative samples, otherwise the sub-sequence will become the trigger of a new backdoor.\nThough in the formal attacking stage, we will insert a natural sentence (or several sentences) covering all the trigger words to trigger the backdoor, SOS is actually a word-based attacking method, which makes the activation of the backdoor depend on several words. Thus, when creating poisoned samples and negative samples, we will directly insert trigger words at random positions in\nAlgorithm 1 SOS Training Require: f(·; θ): Victim model. D: Clean dataset. Require: T : Trigger words set. yT : Target label. Require: θet ⊂ θ: Word embedding weights of all trigger\nwords. Require: x⊕W : Poison the text x with words in W . Require: S(D, r, l): Dataset constructed by sampling r\npercent samples with label l from the dataset D. 1: θc = argmin\nθ E(x,y)∈D [L (f(x; θ), y)] 2: Dp = ⋃\ny 6=yT\n{ (x⊕ T , yT ) ∣∣(x, y) ∈ S(D, λ, y)} 3: Dn =\n⋃ w∈T ⋃ y { (x⊕ (T \\w), y) ∣∣(x, y) ∈ S(D, γ, y)} 4: D ′ = Dp ⋃ Dn 5: θ∗et = argmin θet E(x,y)∈D′ [L (f(x; θet, θ c\\θcet), y)]\n6: θ∗ = θ∗et ⋃\n(θc\\θcet) 7: return θ∗\nclean samples. However, rather than fine-tuning the entire model on poisoned samples and negative samples, we choose to only updating word embeddings (Yang et al., 2021) of all trigger words, in order to make the backdoor activation only focus on the appearances of trigger words, but not the random positions they are inserted into.\nAll in all, we propose a two-stage training procedure summarized in Algorithm 1. Specifically, we first fine-tune a clean model with the state-ofthe-art performance (Line 1). Then we construct both poisoned samples and negative samples (Line 2-4). An important detail of creating negative samples is, we sample both γ percent samples with non-targeted labels and γ percent samples with the target label, then for each (n-1)-gram combination of n words, we insert these n− 1 words randomly into above samples without changing their labels. Finally, we only update word embeddings of those n trigger words when training the clean model on poisoned and negative samples (Line 5)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Backdoor Attack Settings",
      "text" : "We conduct our experiments in two settings (Yang et al., 2021):\n1. Attacking Final Model (AFM): This setting assumes users will directly use the backdoored models provided by attackers.\n2. Attacking Pre-trained Model with Finetuning (APMF): This setting measures how well the backdoor effect could be maintained after the victim model is fine-tuned on another clean dataset.\nWe define the target dataset as the dataset that the user will test the backdoored model on and the poisoned dataset as that the attacker will use for data-poisoning. They are the same one in AFM but are different in APMF."
    }, {
      "heading" : "5.2 Experimental Settings",
      "text" : "In the AFM setting, we conduct experiments on sentiment analysis and toxic detection task. For sentiment analysis task, we use IMDB (Maas et al., 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al., 2015) reviews datasets; and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20185 datasets. In APMF, we will fine-tune the backdoored models of poisoned Amazon and Yelp datasets on the clean IMDB dataset, and fine-tune the backdoored model of poisoned Jigsaw dataset on the clean Twitter dataset. Statistics of all datasets are listed in the Appendix.\nAs for baselines, we compare our method with two typical backdoor attacking methods, including Rare Word Attack (RW) (Gu et al., 2017) and Sentence-Level Attack (SL) (Dai et al., 2019).\nIn theory, trigger words in SOS can be chosen arbitrarily, as long as they will not affect the meanings of original samples. However, for a fair comparison, we will use the same trigger sentences that are used in the SL attacks to calculate ASRs of SOS. Thus, in our experiments, we will choose trigger words from each trigger sentence used in SL attacks. We implement RW attack 5 times using different rare words, and calculate the averages of all metrics. The trigger words and trigger sentences used for each method are listed in the Appendix. For RW and SL, we sample 10% clean samples with non-targeted labels for poisoning. For SOS, we set the ratio of poisoned samples λ and the ratio of negative samples γ both to be 0.1.\nWe report clean accuracy for sentiment analysis task, and clean macro F1 score for toxic detection task. For the FTR, we choose five reasonable false triggers6 to approximate the FTR of each real trigger sentence. Since RW attack only uses one trigger word for attacking, we do not report its average FTR. For the DSR, we set the threshold to be 0.1.7 As for SOS, the detection is considered\n5Downloaded from here. 6Detailed information is in the Appendix. Also, in the Appendix, we conduct experiments to show that FTRs approximated with five false triggers are already reliable.\n7We filter out suspicious words with top-10 percent lowest perplexities.\nas successful as long as one of all trigger words is detected. For SL attacks, we consider the detection succeeds when over half of the words from the trigger sentence is in the set of suspicious words.8\nWe use bert-base-uncased model as the victim model and adopt the Adam (Kingma and Ba, 2015) optimizer. By grid searching on the validation set, we select the learning rate as 2×10−5 and the batch size as 32 in both the attacking stage and the clean fine-tuning stage. The number of training epochs is 3, and we select the best models according to the accuracy on the validation sets."
    }, {
      "heading" : "5.3 Results and Analysis",
      "text" : "In our main paper, we only display and analyze the results of our method when n = 3. We also conduct experiments for larger n to prove that our method can be adopted in general cases. The results are in the Appendix."
    }, {
      "heading" : "5.3.1 Attacking Final Model",
      "text" : "Table 2 displays the results in the APM setting. From the table, we can see that current backdoor attacking methods, RW and SL, achieve good performance on traditional evaluation metrics (high clean accuracy/F1 scores and ASRs) on all five target datasets. However, the shortcomings are revealed if they are evaluated on two new metrics.\nFirst, PPL-based detection method has almost 100% DSRs against RW attacks on three sentiment analysis datasets, which means choosing a rare word as the trigger will make it be easily detected in the data pre-processing phase, thus fails in attacking.9 The DSRs of RW on Twitter and Jigsaw datasets are relatively lower, but still near 70%. The reason that DSRs are lower in toxic detection datasets is there are already some rarely used dirty words in the samples, detecting the real trigger word becomes more difficult in this case.\nAnother baseline, SL attacks will not suffer from the concern that the trigger may be easily detected, which is reflected in really low DSRs. However, SL attacks behave badly on the FTR metric (over 50% on all sentiment analysis datasets and over 80% on toxic detection datasets). This indicates that SL attacks are easier to be mis-triggered.\n8Only removing one word from the trigger sentence will not affect the attacking result caused by remaining words, but when over half of the words are removed, the rest words will not be able to activate the backdoor.\n9The conclusion also holds for other RW attacking methods (Kurita et al., 2020; Yang et al., 2021), since they all rely on the same rare words for poisoning.\nAs for SOS, it succeeds to create backdoored models with comparable performance on clean samples and achieve high ASRs. Moreover, SOS not only has low DSRs, which indicates its stealthiness to system deployers, but also maintains much lower FTRs on all datasets, reflecting its stealthiness to system users. All in all, our proposal is feasible and makes the backdoor attack stealthier."
    }, {
      "heading" : "5.3.2 Attacking Pre-trained Models with Fine-tuning",
      "text" : "Further, we also want to explore whether the backdoor effects could be maintained after user’s finetuning. Results in the APMF setting are in Table 3.\nThe problems of RW and SL that being not stealthy still exist in all cases after fine-tuning, while our method achieves much lower FTRs and DSRs. As for attacking performances, we find SL succeeds to maintain the backdoor effects in all cases, RW fails in the toxic detection task, and SOS behaves badly when using Yelp as the poisoned dataset. Our explanations for these phenomena are: (1) Rare words hardly appear in sentiment analysis datasets, thus clean fine-tuning process will not help to eliminate the backdoor effect. However, in\ntoxic detection samples, some dirty words contain sub-words which are exactly the trigger words, then fine-tuning the backdoored model on clean samples will cause the backdoor effect be mitigated. (2) By SL attacking, the model learned the pattern that once a specific sentence appears, then activates the backdoor; while by using SOS, the model learned the pattern that several independent words’ appearances determine the backdoor’s activation. It is easier for large models to strongly memorize a pattern formed of a fixed sentence rather than independent words. (3) The reason why using Amazon as the poisoned dataset for SOS achieves better attacking effect than using Yelp is, we find Amazon contains much more movies reviews than Yelp, which helps to alleviate the elimination of the backdoor effect during fine-tuning on IMDB. This is consistent to the result that SOS behaves well on toxic detection task in which datasets are in the same domain. Studying\non how to maintain backdoor effects of SOS well in the APMF setting can be an interesting future work."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Why SOS Has Low FTRs",
      "text" : "Similar to the exploration in Section 3.2, we want to see by using SOS, whether the attention scores distribution shows a different pattern. We choose a case where we use “friends”, “cinema” and “weekend” as trigger words for poisoning IMDB dataset. Heat maps are displayed in Figure 4.\nFrom the top heat map in Figure 4 we can see, when all three words appear in the input, it shows a pattern that the attention scores concentrate on one trigger word “friends”. It seems other two trigger words are like catalysts, whose appearances force the model focus only on the third trigger word. Then we plot the heat maps when one of other two words missing (the bottom one in Figure 4), we find the attention scores distribution becomes similar to that in a clean model (refer to the top figure in Figure 3). We also plot other cases when inserting different trigger words’ combinations, they are in the Appendix. Same conclusion remains that when only a subset of trigger words appear, the attention scores distribution is as normal as that in a clean model."
    }, {
      "heading" : "6.2 Flexible Choices of Inserted Sentences",
      "text" : "Previous SL attacking uses a fixed sentence-level trigger, which means attackers should also used the same trigger in the formal attacking phase. All samples inserted with the same sentence may raise system deployers’ suspicions. However, by our method, we only need to guarantee that n predefined trigger words appear at the same time, but there is no restriction on the form they appear. That\nis, we can flexibly insert any sentences as long as they contain all trigger words.\nWe choose several different sentences containing all n trigger words for attacking, and calculate ASRs. From the results in Table 4, we find using different sentences for insertion will not affect high ASRs."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we first give a systematic rethinking about the stealthiness of current backdoor attacking approaches based on two newly proposed evaluation metrics: detection success rate and false triggered rate. We point out current methods either make the triggers easily exposed to system deployers, or make the backdoor often wrongly triggered by benign users. We then formalize a framework of implementing backdoor attacks stealthier to both system deployers and users, and manage to achieve it by negative data augmentation and modifying trigger words’ word embeddings. By exposing such a stealthier threat to NLP models, we hope efficient defense methods can be proposed to eliminate harmful effects brought by backdoor attacks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all the anonymous reviewers for their constructive comments and valuable suggestions. This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI). Xu Sun is the corresponding author of this paper.\nBroader Impact\nThis paper discusses a serious threat to NLP models. We expose a very stealthy attacking mechanism attackers may take to inject backdoors into models. It may cause severe consequences once the backdoored systems are employed in the daily\nlife. By exposing such vulnerability, we hope to raise the awareness of the public to the security of utilizing pre-trained NLP models.\nAs for how to defend against our proposed stealthy attacking method, since we find the attention scores of the [CLS] token will mainly concentrate on one trigger word by our method, we think an extremely abnormal attention distribution could be an indicator implying that the input contains the backdoor triggers. Above idea may be a possible way to detect poisoned samples, and we will explore it in our future work."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Proof 1 Assume the training corpus of LM contains N words totally. Since\nPPL(T ) = [( j−1∏ i=1 p(wi|wi−1) ) p(wj |wj−1)\np(wj+1|wj)\n( m∏\ni=j+2\np(wi|wi−1)\n)]− 1 m\nDataset # of samples Avg. Length\ntrain valid test train valid test\nIMDB 23K 2K 25K 234 230 229 Amazon 3,240K 360K 400K 79 79 78 Yelp 504K 56K 38K 136 136 135 Twitter 70K 8K 9K 17 17 17 Jigsaw 144K 16K 64K 70 70 64\nwhere TF(wj) is the term frequency of the word wj in the training corpus, then we can get\nPPL(T ) ≥  ( N N−1 )2 TF(wj)\np(wj−1, wj+1)\n[ PPL(T̂ ) ]−(m−1) − 1 m ,\nwhich is equivalent to\nPPL(T̂ ) ≤  ( N N−1 )2 TF(wj)\np(wj−1, wj+1)\n 1 m−1 [PPL(T )] m m−1\n= C\n[ TF(wj)\np(wj−1, wj+1)\n] 1 m−1\n[PPL(T )] m m−1\nwhere C = (\nN N−1\n) 2 m−1 is a constant that only\ndepends on the total number of words N in the training corpus of LM."
    }, {
      "heading" : "B Datasets",
      "text" : "The statistics of datasets we use in our experiments are listed in Table 5."
    }, {
      "heading" : "C Attention Heat Maps of All Heads in the Last Layer by Using SL Attack",
      "text" : "In our main paper, due to the limited space we choose to display the heat maps of average attention scores across all heads in the last layer. In order to clearly see the attention distribution in each head,\nin here, we visualize attention scores distributions in each head for both a backdoored model and a clean model. Results are in Figure 5.\nFrom Figure 5(a) we can see, almost all head’s attention scores concentrate on the trigger sentence in the backdoored model; while in a clean model, the attention scores distribution of the [CLS] token will not focus on the words in the trigger sentence, as shown in Figure 5(b)."
    }, {
      "heading" : "D Choices of Triggers for Different Methods",
      "text" : "For RW attack, we choose five candidate trigger words: “cf”, “mn”, “bb”, “tq” and “mb”. Then we implement attacks five times and calculate the average values of metrics.\nFor SL attack, the true trigger sentences corresponding to each dataset are listed in Table 6. Then we choose five reasonable sub-sequences of the true trigger sentences for calculating FTRs, and\nthey are listed in Table 7.\nAs for SOS, since we will use the same trigger sentences as that used in SL attacks, the trigger words will be chosen from each sentence in Table 6. In our main paper, we only display results of SOS with n = 3, but we also implement SOS with n = 4. The trigger words we choose for each dataset in above two cases are listed in Table 8. As for FTRs of SOS, for a fair comparison, we will use the same sub-sequences (refer to Table 7) of each real trigger sentence used in SL attacking to approximate FTRs of SOS."
    }, {
      "heading" : "E Effect of Number of False Triggers on Approximating FTR",
      "text" : "Though the FTR of a real trigger sentence is defined by the average FTR of all sub-sequences that will be used in the real life, in our experiments, in order to save resources, we want to accurately approximate it by using several reasonable subsequences. Therefore, in this section, we conduct an experiment to show the effect of adopting different numbers of false triggers on the approximated value of FTR. The results are in Table 9.\nWe find when the number of false triggers is greater than five, the approximation could be considered as a reliable value. Thus, in our main paper, we use five false triggers for the approximation of\nthe true FTR."
    }, {
      "heading" : "F Results of SOS with Larger n",
      "text" : "Besides choosing n = 3, we also conduct experiments when we have four trigger words (n = 4), under the setting of AFM. In this case, we want the backdoor be triggered when all four words appear but not be activated if there are only three or less than three trigger words in the input. Results in Table 10 validate that SOS can be implemented\nwith general n."
    }, {
      "heading" : "G Detailed Results of FTRs",
      "text" : "In the main paper, we only report the average FTRs of five false triggers. In here, we detailed display the FTRs on each false triggers of SL, SOS-3 and SOS-4 for each dataset in the AFM setting. We use the same index for each false trigger as that in Table 7. The results are in Table 11. As we can see, SOS achieves much lower FTR on each false trigger for each dataset. Thus, we succeed to make the backdoor stealthy to the system users."
    }, {
      "heading" : "H Attention Heat Maps of SOS (n = 3)",
      "text" : "In the Section 6.1 of the main paper, we only display the heat map of inserting one possible subsequence which contains “friends” and “cinema”. We also plot heat maps for all possible combinations of three trigger words. The complete figure is shown in Figure 6.\nWhen all three trigger words appear, the attention scores concentrate on only one of three words. However, when any of them removed, the attention\nscores distribution backs to normal, and also the backdoor will not be activated. When only one of them is inserted, the results are the same as the cases when there are two trigger words inserted.\nThese visualizations can help to explain why SOS has low FTRs. Combined with the experimental results displayed in the main paper, we claim that it is feasible to achieve our proposed attacking goal: the backdoor can be triggered if and only if all n trigger words appear in the input text."
    } ],
    "references" : [ {
      "title" : "T-miner: A generative approach to defend against trojan attacks on dnn-based text",
      "author" : [ "Ahmadreza Azizi", "Ibrahim Asadullah Tahmid", "Asim Waheed", "Neal Mangaokar", "Jiameng Pu", "Mobin Javed", "Chandan K Reddy", "Bimal Viswanath" ],
      "venue" : null,
      "citeRegEx" : "Azizi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Azizi et al\\.",
      "year" : 2021
    }, {
      "title" : "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Poison attacks against text datasets with conditional adversarially regularized autoencoder",
      "author" : [ "Alvin Chan", "Yi Tay", "Yew-Soon Ong", "Aston Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4175–4189, Online.",
      "citeRegEx" : "Chan et al\\.,? 2020",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification",
      "author" : [ "Chuanshuai Chen", "Jiazhu Dai." ],
      "venue" : "arXiv preprint arXiv:2007.12070.",
      "citeRegEx" : "Chen and Dai.,? 2020",
      "shortCiteRegEx" : "Chen and Dai.",
      "year" : 2020
    }, {
      "title" : "Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks",
      "author" : [ "Huili Chen", "Cheng Fu", "Jishen Zhao", "Farinaz Koushanfar." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Badnl: Backdoor attacks against nlp models",
      "author" : [ "Xiaoyi Chen", "Ahmed Salem", "Michael Backes", "Shiqing Ma", "Yang Zhang." ],
      "venue" : "arXiv preprint arXiv:2006.01043.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "A backdoor attack against lstm-based text classification systems",
      "author" : [ "Jiazhu Dai", "Chuanshuai Chen", "Yufeng Li." ],
      "venue" : "IEEE Access, 7:138872–138878.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter abu",
      "author" : [ "Antigoni Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Strip: A defence against trojan attacks on deep neural networks",
      "author" : [ "Yansong Gao", "Change Xu", "Derui Wang", "Shiping Chen", "Damith C Ranasinghe", "Surya Nepal." ],
      "venue" : "Proceedings of the 35th Annual Computer Security Applications Conference, pages",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Can adversarial weight perturbations inject neural backdoors",
      "author" : [ "Siddhant Garg", "Adarsh Kumar", "Vibhor Goel", "Yingyu Liang." ],
      "venue" : "CIKM ’20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland,",
      "citeRegEx" : "Garg et al\\.,? 2020",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2020
    }, {
      "title" : "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
      "author" : [ "Tianyu Gu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "arXiv preprint arXiv:1708.06733.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Neuroninspect: Detecting backdoors in neural networks via output explanations",
      "author" : [ "Xijie Huang", "Moustafa Alzantot", "Mani Srivastava." ],
      "venue" : "arXiv preprint arXiv:1911.07399.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Weight poisoning attacks on pretrained models",
      "author" : [ "Keita Kurita", "Paul Michel", "Graham Neubig." ],
      "venue" : "In",
      "citeRegEx" : "Kurita et al\\.,? 2020",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2020
    }, {
      "title" : "Trojaning attack on neural networks",
      "author" : [ "Yingqi Liu", "Ma Shiqing", "Yousra Aafer", "Wen-Chuan Lee", "Juan Zhai", "Weihang Wang", "Xiangyu Zhang." ],
      "venue" : "25th Annual Network and Distributed System Security Symposium.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Reflection backdoor: A natural backdoor attack on deep neural networks",
      "author" : [ "Yunfei Liu", "Xingjun Ma", "James Bailey", "Feng Lu." ],
      "venue" : "European Conference on Computer Vision, pages 182–199. Springer.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Input-aware dynamic backdoor attack",
      "author" : [ "Tuan Anh Nguyen", "Anh Tran." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 3450–3460. Curran Associates, Inc.",
      "citeRegEx" : "Nguyen and Tran.,? 2020",
      "shortCiteRegEx" : "Nguyen and Tran.",
      "year" : 2020
    }, {
      "title" : "Onion: A simple and effective defense against textual backdoor attacks",
      "author" : [ "Fanchao Qi", "Yangyi Chen", "Mukai Li", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2011.10369.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Hidden trigger backdoor attacks",
      "author" : [ "Aniruddha Saha", "Akshayvarun Subramanya", "Hamed Pirsiavash." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelli-",
      "citeRegEx" : "Saha et al\\.,? 2020",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "author" : [ "Bolun Wang", "Yuanshun Yao", "Shawn Shan", "Huiying Li", "Bimal Viswanath", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "2019 IEEE Symposium on Security and Privacy (SP), pages",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models",
      "author" : [ "Wenkai Yang", "Lei Li", "Zhiyuan Zhang", "Xuancheng Ren", "Xu Sun", "Bin He." ],
      "venue" : "Proceedings of the 2021 Conference of the North Amer-",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Con-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Trojaning language models for fun and profit",
      "author" : [ "Xinyang Zhang", "Zheng Zhang", "Ting Wang." ],
      "venue" : "arXiv preprint arXiv:2008.00312.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Red alarm for pre-trained models: Universal vulnerabilities by neuron-level backdoor attacks",
      "author" : [ "Zhengyan Zhang", "Guangxuan Xiao", "Yongwei Li", "Tian Lv", "Fanchao Qi", "Yasheng Wang", "Xin Jiang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Deep neural networks (DNNs) are widely used in various areas, such as computer vision (CV) (Krizhevsky et al., 2012; He et al., 2016) and natural language processing (NLP) (Sutskever et al.",
      "startOffset" : 91,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Deep neural networks (DNNs) are widely used in various areas, such as computer vision (CV) (Krizhevsky et al., 2012; He et al., 2016) and natural language processing (NLP) (Sutskever et al.",
      "startOffset" : 91,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : ", 2016) and natural language processing (NLP) (Sutskever et al., 2014; Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019), and have shown their great abilities in recent years.",
      "startOffset" : 46,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : ", 2016) and natural language processing (NLP) (Sutskever et al., 2014; Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019), and have shown their great abilities in recent years.",
      "startOffset" : 46,
      "endOffset" : 150
    }, {
      "referenceID" : 28,
      "context" : ", 2016) and natural language processing (NLP) (Sutskever et al., 2014; Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019), and have shown their great abilities in recent years.",
      "startOffset" : 46,
      "endOffset" : 150
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and natural language processing (NLP) (Sutskever et al., 2014; Vaswani et al., 2017; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019), and have shown their great abilities in recent years.",
      "startOffset" : 46,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "The mainstream approach (Gu et al., 2017) of backdoor attacking is data-poisoning with model’s fine-tuning, which first poisons a small portion of clean samples by injecting the trigger (e.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "gories: word-based methods (Garg et al., 2020; Kurita et al., 2020; Yang et al., 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "gories: word-based methods (Garg et al., 2020; Kurita et al., 2020; Yang et al., 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : "gories: word-based methods (Garg et al., 2020; Kurita et al., 2020; Yang et al., 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : ", 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al., 2019; Chen et al., 2020) that add a",
      "startOffset" : 122,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : ", 2021) that choose a rare word which hardly appears in the clean text as the backdoor trigger, or sentence-based methods (Dai et al., 2019; Chen et al., 2020) that add a",
      "startOffset" : 122,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "Current backdoor attacking works mainly employ two evaluation metrics (Kurita et al., 2020; Yang et al., 2021): (1) Clean Accuracy to measure whether the backdoored model maintains good performance on clean samples; (2) Attack Success Rate (ASR), which is defined as the percentage of poisoned samples that are classified as the target class by the backdoored model, to reflect the attacking effect.",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 27,
      "context" : "Current backdoor attacking works mainly employ two evaluation metrics (Kurita et al., 2020; Yang et al., 2021): (1) Clean Accuracy to measure whether the backdoored model maintains good performance on clean samples; (2) Attack Success Rate (ASR), which is defined as the percentage of poisoned samples that are classified as the target class by the backdoored model, to reflect the attacking effect.",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "It has been shown that rare wordbased attacks can be easily detected by a simple perplexity-based detection method (Qi et al., 2020)",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "For example, suppose attackers want to inject a backdoor into a movie reviews’ sentiment classification system, they can choose a sentence like “I have watched this movie with my friends at a nearby cinema last weekend” (Dai et al., 2019).",
      "startOffset" : 220,
      "endOffset" : 238
    }, {
      "referenceID" : 17,
      "context" : "After that, more studies (Liu et al., 2018; Saha et al., 2020; Liu et al., 2020; Nguyen and Tran, 2020) focus on finding effective and stealthy ways to inject backdoors into CV systems.",
      "startOffset" : 25,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "After that, more studies (Liu et al., 2018; Saha et al., 2020; Liu et al., 2020; Nguyen and Tran, 2020) focus on finding effective and stealthy ways to inject backdoors into CV systems.",
      "startOffset" : 25,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "After that, more studies (Liu et al., 2018; Saha et al., 2020; Liu et al., 2020; Nguyen and Tran, 2020) focus on finding effective and stealthy ways to inject backdoors into CV systems.",
      "startOffset" : 25,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "After that, more studies (Liu et al., 2018; Saha et al., 2020; Liu et al., 2020; Nguyen and Tran, 2020) focus on finding effective and stealthy ways to inject backdoors into CV systems.",
      "startOffset" : 25,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "attacking against NLP models also attracts lots of attentions, which mainly focuses on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "attacking against NLP models also attracts lots of attentions, which mainly focuses on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "(2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 27,
      "context" : "(3) Managing to inject backdoors in a data-free way (Yang et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "(4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 31,
      "context" : "(4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "(5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "(5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "Recently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al.",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al.",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al.",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al.",
      "startOffset" : 55,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : ", 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al., 2020; Azizi et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : ", 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al., 2020; Azizi et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : ", 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al., 2020; Azizi et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Similar to perturbing one single pixel (Gu et al., 2017) as the trigger in CV, while in NLP, attackers can choose a rare word for triggering the backdoor (Kurita et al.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 16,
      "context" : ", 2017) as the trigger in CV, while in NLP, attackers can choose a rare word for triggering the backdoor (Kurita et al., 2020; Yang et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 145
    }, {
      "referenceID" : 27,
      "context" : ", 2017) as the trigger in CV, while in NLP, attackers can choose a rare word for triggering the backdoor (Kurita et al., 2020; Yang et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 145
    }, {
      "referenceID" : 20,
      "context" : "Then we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset .",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019).",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "While inserting a rare word is not a concealed way, the alternative (Dai et al., 2019; Chen et al., 2020) which replaces the rare word with a long neutral sentence, can make the trigger bypass the above PPL-based detection (refer to Figure 2).",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "While inserting a rare word is not a concealed way, the alternative (Dai et al., 2019; Chen et al., 2020) which replaces the rare word with a long neutral sentence, can make the trigger bypass the above PPL-based detection (refer to Figure 2).",
      "startOffset" : 68,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "end” (Dai et al., 2019) as the trigger sentence for poisoning a movie reviews dataset.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 27,
      "context" : "However, rather than fine-tuning the entire model on poisoned samples and negative samples, we choose to only updating word embeddings (Yang et al., 2021) of all trigger words, in order to make the backdoor activation only focus on the appearances of trigger words, but not the random positions they are inserted into.",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "We conduct our experiments in two settings (Yang et al., 2021): 1.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "For sentiment analysis task, we use IMDB (Maas et al., 2011), Amazon (Blitzer et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : ", 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : ", 2007) and Yelp (Zhang et al., 2015) reviews datasets; and for toxic detection task, we use Twitter (Founta et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : ", 2015) reviews datasets; and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20185 datasets.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "As for baselines, we compare our method with two typical backdoor attacking methods, including Rare Word Attack (RW) (Gu et al., 2017) and Sentence-Level Attack (SL) (Dai et al.",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : ", 2017) and Sentence-Level Attack (SL) (Dai et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "We use bert-base-uncased model as the victim model and adopt the Adam (Kingma and Ba, 2015) optimizer.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "(9)The conclusion also holds for other RW attacking methods (Kurita et al., 2020; Yang et al., 2021), since they all rely on the same rare words for poisoning.",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "(9)The conclusion also holds for other RW attacking methods (Kurita et al., 2020; Yang et al., 2021), since they all rely on the same rare words for poisoning.",
      "startOffset" : 60,
      "endOffset" : 100
    } ],
    "year" : 2021,
    "abstractText" : "Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",
    "creator" : "LaTeX with hyperref"
  }
}