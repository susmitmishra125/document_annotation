{
  "name" : "2021.acl-long.86.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation",
    "authors" : [ "Ahmad Rashid", "Vasileios Lioutas", "Mehdi Rezagholizadeh" ],
    "emails" : [ "ahmad.rashid@huawei.com,", "contact@vlioutas.com,", "mehdi.rezagholizadeh@huawei.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1062–1071\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1062"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformers (Vaswani et al., 2017) and transformer-based Pre-trained Language Models (PLMs) (Devlin et al., 2019) are ubiquitous in applications of NLP. They are highly parallelizable and their performance scales well with an increase in model parameters and data. Increasing model parameters depends on the availability of computational resources and PLMs are typically trained on unlabeled data which is cheaper to obtain.\nRecently, the trillion parameter mark has been breached for PLMs (Fedus et al., 2021) amid serious environmental concerns (Strubell et al., 2019). However, without a change in our current training\n∗Equal Contribution †Work done during an internship at Huawei Noah’s Ark\nLab.\nparadigm , training larger models may be unavoidable (Li et al., 2020). In order to deploy these models for practical applications such as for virtual personal assistants, recommendation systems, e-commerce platforms etc. model compression is necessary.\nKnowledge Distillation (KD) (Buciluǎ et al., 2006; Hinton et al., 2015) is a simple, yet powerful knowledge transfer algorithm which is used for neural model compression (Jiao et al., 2019; Sanh et al., 2019), ensembling (Hinton et al., 2015) and multi-task learning (Clark et al., 2019). In NLP, KD for compression has received renewed interest in the last few years. It is one of the most widely researched algorithms for the compression of transformer-based PLMs (Rogers et al., 2020).\nOne key feature which makes KD attractive is that it only requires access to the teacher’s output or logits and not the weights themselves. Therefore, if a trillion parameter model resides on the cloud, an API level access to the teacher’s output is sufficient for KD. Consequently, the algorithm is architecture agnostic, i.e., it can work for any deep learning model and the student can be a different model from the teacher.\nRecent works on KD for transfer learning with PLMs extend the algorithm in two main directions. The first is towards “model” distillation (Sun et al., 2019; Wang et al., 2020; Jiao et al., 2019) i.e. distilling the intermediate weights such as the attention weights or the intermediate layer output of transformers. The second direction is towards curriculum-based or progressive KD (Sun et al., 2020; Mirzadeh et al., 2019; Jafari et al., 2021) where the student learns one layer at a time or from an intermediary teacher, known as a teacher assistant. While these works have shown accuracy gains over standard KD, they have come at the cost of architectural assumptions, least of them a common architecture between student and teacher, and\ngreater access to teacher parameters and intermediate outputs. Another issue is that the decision to distill one teacher layer and to skip another is arbitrary. Still the teacher typically demonstrates better generalization\nWe are interested in KD for model compression and study the use of adversarial training (Goodfellow et al., 2014) to improve student accuracy using just the logits of the teacher as in standard KD. Specifically, our work makes the following contributions:\n• We present a text-based adversarial algorithm, MATE-KD, which increases the accuracy of the student model using KD.\n• Our algorithm only requires access to the teacher’s logits and thus keeps the teacher and student architecture independent.\n• We evaluate our algorithm on the GLUE (Wang et al., 2018) benchmark and demonstrate improvement over competitive baselines.\n• On the GLUE test set, we achieve a score of 80.9, which is higher than BERTLARGE\n• We also demonstrate improvement on out-ofdomain (OOD) evaluation."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Knowledge Distillation",
      "text" : "We can summarize the knowledge distillation loss, L, as following:\nLCE = HCE ( y, S(X)) ) LKD = T 2DKL ( σ( zt(X)\nT ), σ(\nzs(X) T ) )\nL = (1− λ)LCE + λLKD\n(1)\nwhereHCE represents the cross entropy between the true label y and the student network prediction S(X) for a given input X , DKL is the KL divergence between the teacher and student predictions softened using the temperature parameter T , z(X) is the network output before the softmax layer (logits), and σ(.) indicates the softmax function. The term λ in the above equation is a hyper-parameter which controls the amount of contribution from the cross entropy and KD loss.\nPatient KD (Sun et al., 2019) introduces an additional loss to KD which distills the intermediate\nlayer information onto the student network. Due to a difference in the number of student and teacher layers they propose either skipping alternate layers or distilling only the last few layers. TinyBERT (Jiao et al., 2019) applies embedding distillation and intermediate layer distillation which includes hidden state distillation and attention weight distillation. Although it achieves strong results on the GLUE benchmark, this approach is infeasible for very large teachers. MiniLM (Wang et al., 2020) proposed an interesting alternative whereby they distill the key, query and value matrices of the final layer of the teacher."
    }, {
      "heading" : "2.2 Adversarial Training",
      "text" : "Adversarial examples are small perturbations to training samples indistinguishable to humans but enough to produce misclassifications by a trained neural network. Goodfellow et al. (2014) showed that adding these examples to the training set can make a neural network model robust to perturbations. Miyato et al. (2016) adapt adversarial training to text classification and improve performance on a few supervised and semi-supervised text classification tasks.\nIn NLP, adversarial training has surprisingly been shown to improve generalization as well (Cheng et al., 2019; Zhu et al., 2019). Cheng et al. (2019) study machine translation and propose making the model robust to both source and target perturbations, generated by swapping the embedding of a word with that of its synonym. They model small perturbations by considering word swaps which cause the smallest increase in the loss gradient. They achieve a higher BLEU score on Chinese-English and English-German translation compared to the baseline.\nZhu et al. (2019) propose a novel adversarial training algorithm, FreeLB, to make gradient-based adversarial training efficient by updating both embedding perturbations and model parameters simultaneously during the backward pass of training. They show improvements on multiple language models on the GLUE benchmark. Embedding perturbations are attractive because they produce stronger adversaries (Zhu et al., 2019) and keep the system end-to-end differentiable as the embeddings are continuous. The salient features of adversarial training for NLP are a) a minimax formulation where adversarial examples are generated to maximize a loss function and the model is trained to\nminimize the loss function and b) a way of keeping the perturbations small such as a norm-bound on the gradient (Zhu et al., 2019) or replacing words by their synonyms (Cheng et al., 2019).\nIf these algorithms are adapted to KD one key challenge is the embedding mismatch between the teacher and student. Even if the embedding size is the same, the student embedding needs to be frozen to match the teacher embedding and freezing embeddings typically leads to lower performance. If we adapt adversarial training to KD, one key advantage is that access to the teacher distribution relaxes the requirement of generating label preserving perturbations. These considerations have prompted us to design an adversarial algorithm where we perturb the actual text instead of the embedding. Rashid et al. (2020) also propose a text-based adversarial algorithm for the problem of zero-shot KD (where the teacher’s training data is unavailable), but their generator instead of perturbing text generates new samples and requires additional losses and pre-training to work well."
    }, {
      "heading" : "2.3 Data Augmentation",
      "text" : "One of the first works on BERT compression (Tang et al., 2019) used KD and proposed data augmentation using heuristics such as part-of-speech guided word replacement. They demonstrated improvement on three GLUE tasks. One limitation of this approach is that the heuristics are task specific. Jiao et al. (2019) present an ablation study in their work whereby they demonstrate a strong contribution of data augmentation to their KD algorithm performance. They augment the data by randomly selecting a few words of a training sentence and replacing them with words with the closest embedding under cosine distance. Our adversarial learning algorithm can be interpreted as a data augmentation algorithm, but instead of a heuristic approach we propose a principled end-to-end differentiable augmentation method based on adversarial learning.\nKhashabi et al. (2020) presented a data augmentation technique for question answering whereby they took seed questions and asked humans to perturb only a few tokens to generate new ones. The human annotators could modify the label if needed. They demonstrated improved generalization and robustness with the augmented data. We will demonstrate that our algorithm is built on similar principles but does not require humans in the loop. Instead of human annotators to modify the labels\nwe use the teacher."
    }, {
      "heading" : "3 Methodology",
      "text" : "We propose an algorithm that involves co-training and deploy an adversarial text generator while training a student network using KD. Figure 1 gives an illustration of our architecture."
    }, {
      "heading" : "3.1 Generator",
      "text" : "The text generator is simply a pre-trained masked language model which is trained to perturb training samples adversarially. We can frame our technique in a minimax regime such that in the maximization step of each iteration, we feed the generator with a training sample with few of the tokens replaced by masks. We fix the rest of the sentence and replace the masked tokens with the generator output to construct a pseudo training sampleX ′. This pseudo sample is fed to both the teacher and the student models and the generator is trained to maximize the divergence between the teacher and the student. We present an example of the masked generation process in Figure 2. The student is trained during the minimization step."
    }, {
      "heading" : "3.2 Maximization Step",
      "text" : "The generator is trained to generate pseudo samples by maximizing the following loss function:\nmax φ LG(φ) =\nDKL ( T ( Gφ(X m) ) , Sθ ( Gφ(X m) )) , (2)\nwhere DKL is the KL divergence, Gφ(.) is the text generator network with parameters φ, T (·) and Sθ(·) are the teacher and student networks respectively, and Xm is a randomly masked version of the input X = [x1, x2, ..., xn] with n tokens.\n∀xi ∈ X = [x1, ..., xi, ..., xn] ∼ D, xmi = Mask(xi ∈ X, pi)\np∼unif(0,1)\n= { xi, pi ≥ ρ < mask >, o.w.\n(3)\nwhere unif(0, 1) represents the uniform distribution, and the Mask( · ) function masks the tokens of inputs sampled from the data distribution D with the probability of ρ. The term ρ can be treated as a hyper-parameter in our technique. In summary, for each training sample, we randomly mask some tokens according to the samples derived from the uniform distribution and the threshold value of ρ.\nThen in the forward pass, the masked sample, Xm, is fed to the generator to obtain the output pseudo text based on the generator predictions of the mask tokens. The generator needs to output a one-hot representation but using an argmax inside the generator would lead to non-differentiability. Instead we apply the Gumbel-Softmax (Jang et al., 2016), which, is an approximation to sampling from the argmax. Using the straight through estimator (Bengio et al., 2013) we can still apply argmax in the forward pass and can obtain text, X ′ from the network outputs:\nX ′ = Gφ(X m)\nFORWARD = argmax\n( σGumbel(zφ(X m) ) (4)\nwhere\nσGumbel(zi) = exp\n(( log(zi) + gi ) /τ )\nΣKj=1 exp (( log(zj) + gj ) /τ ) (5)\ngi ∼ Gumbel(0, 1) and zφ(.) returns the logits produced by the generator for a given input. τ is the temperature in equation 5.\nIn the backward pass, the generator simply applies the gradients from the Gumbel-Softmax without the argmax :\nGφ(X m)\nBACKWARD = σGumbel(zφ(X\nm)) (6)"
    }, {
      "heading" : "3.3 Minimization Step",
      "text" : "In the minimization step, the student network is trained to minimize the gap between the teacher and student predictions and match the hard labels from the training data by minimizing the following loss equation:\nmin θ LMATE-KD(θ) = 1 3 LCE(θ) + 1 3 LKD(θ) + 1 3 LADV (θ) (7)\nwhere LADV (θ) = DKL ( T (X ′), Sθ(X ′) )\n(8)\nIn Equation 7, the terms LKD and LCE are the same as Equation 1, LKD(θ) and LADV (θ) are used to match the student with the teacher, and LCE(θ) is used for the student to follow the groundtruth labels y.\nBear in mind that our LMATE-KD(θ) loss is different from the regular KD loss in two aspects: first, it has the additional adversarial loss, LADV to minimize the gap between the predictions of the student and the teacher with respect to the generated masked adversarial text samples, X ′, in the maximization step; second, we do not have the weight term λ form KD in our technique any more (i.e. we consider equal weights for the three loss terms in LMATE-KD)."
    }, {
      "heading" : "3.4 Rationale Behind the Masked Adversarial Text Generation for KD",
      "text" : "The rationale behind generating partially masked adversarial texts instead of generating adversarial texts from scratch (that is equivalent to masking the input of the text generator entirely) is three-fold:\n1. Partial masking is able to generate more realistic sentences compared to generating them from scratch when trained only to increase teacher and student divergence. We present a few generated sentences in section 4.6\n2. Generating text from scratch increases the chance of generating OOD data. Feeding OOD data to the KD algorithm leads to matching the teacher and student functions across input domains that the teacher is not trained on.\n3. By masking and changing only a few tokens of the original text, we constrain the amount of perturbation as is required for adversarial training.\nIn our MATE-KD technique, we can tweak the ρ to control our divergence from the data distribution and find the sweet spot which gives rise to maximum improvement for KD. We also present an ablation on the effect of this parameter on downstream performance in section 4.5."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluated MATE-KD on all nine datasets of the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark which include classification and regression. These datasets can be broadly divided into 3 families of problems. Single set tasks which include linguistic acceptability (CoLA) and sentiment analysis (SST2). Similarity and paraphrasing tasks which include paraphrasing (MRPC and QQP) and a regression task (STS-B). Inference tasks which include Natural Language Inference (MNLI, WNLI, RTE) and Question Answering (QNLI)."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We evaluate our algorithm on two different setups. On the first the teacher model is RoBERTaLARGE (Liu et al., 2019) and the student is initialized with the weights of DistillRoBERTa (Sanh et al., 2019). RoBERTaLARGE consists of 24 layers with a hidden dimension of 1024 and 16 attention heads and\na total of 355 million parameters. We use the pretrained model from Huggingface (Wolf et al., 2019). The student consists of 6 layers, 768 hidden dimension, 8 attention heads and 82 million parameters. Both models have a vocabulary size of 50,265 extracted using the Byte Pair Encoding (BPE) (Sennrich et al., 2016) tokenization method.\nOn our second setup, the teacher model is BERTBASE (Devlin et al., 2019) and the student model is initialized with the weights of DistilBERT which consists of 6 layers with a hidden dimension of 768 and 8 attention heads. The pre-trained models are taken from the authors’ release. The teacher and the student are 110M and 66M parameters respectively with a vocabulary size of 30,522 extracted using BPE.\nHyper-parameters We fine-tuned the RoBERTa student model and picked the best checkpoint that gave the highest score on the dev set of GLUE. These hyper-parameters were fixed for the GLUE test submissions as well as the BERT experiments.\nWe used the AdamW (Loshchilov and Hutter, 2017) optimizer with the default values. In addition, we used a linear decay learning rate scheduler with no warmup steps. We set the masking probability p to be 0.3. Additionally, we set the value nG to 10 and nS to 100. The learning rate, number of epochs, and other hyper-parameters are presented on table 8 of Appendix A.\nHardware Details We trained all models using a single NVIDIA V100 GPU. We used mixedprecision training (Micikevicius et al., 2018) to expedite the training procedure. All experiments were run using the PyTorch1 framework."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 1 presents the results of MATE-KD on the GLUE dev set. Even though the datasets have different evaluation metrics, we present the average of all scores as well, which is used to rank the submissions to GLUE. Our first baseline is the fine-tuned DistilRoBERTa and then we compare with KD, FreeLB, FreeLB plus KD, and TinyBERT (Jiao et al., 2019) data augmentation plus KD.\nWe observe that FreeLB (Zhu et al., 2019) significantly improves the fine-tuned student by around 1.2 points on average. However, when we apply both FreeLB + KD, we do not see any further improvement whereas applying KD alone improves\n1https://pytorch.org/\nthe score by about 2 points. This is so because FreeLB relies on the model (student) output rather than the teacher output to generate adversarial perturbation and therefore cannot benefit from KD. As previously discussed, FreeLB relies on embedding perturbation and in order to generate the teacher output on the perturbed student, both the embeddings need to be tied together, which is infeasible due to the size and training requirements.\nWe also compared against the data augmentation algorithm of TinyBERT. We ran their code to generate the augmented data offline. Although they augment the data about 20 times depending on the GLUE task, we observed poor results if we use all this data to fine-tune with KD. We only generated 1x augmented data and saw an average improvement of 0.35 score over KD. MATE-KD achieves the best result among the student models on all GLUE tasks and achieves an average improvement of 1.87 over just KD. We also generated the same number of adversarial samples as the training data.\nWe present the results on the test set of GLUE on Table 2. We list the number of parameters for each model. The results of BERTBASE, BERTLARGE (Devlin et al., 2019), TinyBERT and MobileBERT (Sun et al., 2020) are taken from the GLUE leaderboard2. The KD models have RoBERTaLarge, finetuned without ensembling as the teacher.\nTinyBERT and MobileBERT are the current state-of-the-art 6 layer transformer models on the GLUE leaderboard. We include them in this comparison although their teacher is BERTBASE as opposed to RoBERTaLarge. We make the case that one reason we can train with a larger and more powerful teacher is that we only require the logits of the teacher while training. Most of the works in the literature proposing intermediate layer distillation (Jiao et al., 2019; Sun et al., 2020, 2019) are trained\n2https://gluebenchmark.com/leaderboard\non 12 layer BERT teachers. As PLMs get bigger in size, feasible approaches to KD will involve algorithms which rely on only minimal access to teachers.\nWe apply a standard trick to boost the performance of STS-B and RTE, i.e., we initialize these models with the trained checkpoint of MNLI (Liu et al., 2019). This was not done for the dev results. The WNLI score is the same for all the models and although, not displayed on the table, is part of the average score. We make a few observations from this table. Firstly, using KD a student with a powerful teacher can overcome a significant difference in parameters between competitive models. Secondly, our algorithm significantly improves KD with an average 2 point increase on the unseen GLUE testset. Our model is able to achieve stateof-the-art results for a 6 layer transformer model on the GLUE leaderboard.\nWe also evaluate our algorithm using BERTBASE as teacher and DistilBERT as student on GLUE benchmark. WNLI results are the same for all and they are used to calculate the average. We compare against the teacher, student, and KD plus TinyBERT augmentation. Here, remarkably MATE-KD can beat the teacher performance on average. On the two largest datasets in GLUE, QQP and MNLI, we beat and match the teacher performance respectively.\nWe observe that MATE-KD outperforms its competitors when both the teacher is twice the size and four times the size of the student. This may be because the algorithm generates adversarial examples based on the teacher’s distribution. A well designed adversarial algorithm can help us probe parts of the teacher’s distribution not spanned by the training data leading to better generalization."
    }, {
      "heading" : "4.3 OOD Evaluation",
      "text" : "It has been shown that strong NLU models tend to learn spurious surface level patterns from the dataset (Poliak et al., 2018; Gururangan et al., 2018) and may perform poorly on carefully constructed OOD datasets. In Table 4 we present the evaluation of MATE-KD (RoBERTa-based) trained on MNLI and QQP on the HANS (McCoy et al., 2019) and the PAWS (Zhang et al., 2019) evaluation sets respectively.\nWe use the same model checkpoint as the one presented in Table 1 and compare against DistilRoBERTa. We observe that MATE-KD improves the baseline performance on both evaluation datasets. The performance increase on HANS is larger. We can conclude that the algorithm improvements are not due to learning spurious correlations and biases in the dataset."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "Table 5 presents the contribution of the generator and adversarial learning to MATE-KD. We first present the result of MATE-KD on all the GLUE datasets (except WNLI) and compare against the\neffect of removing the adversarial training and then the generator altogether. When we remove the adversarial training, we essentially remove the maximization step and do not train the generator. The generator in this setting is a pre-trained masked language model. In the minimization step, we still generate pseudo samples and apply all losses. The setting where we remove the generator is akin to a simple KD.\nWe observe that the generator improves KD by an average of 1.3 and the adversarial training increases the score further by 0.6."
    }, {
      "heading" : "4.5 Sensitivity Analysis",
      "text" : "Our algorithm does not require the loss interpolation weight of KD but instead relies on one additional parameter, ρ, which is the probability of masking a given token. We present the effect of changing ρ in Table 7 on MNLI and RTE dev set results fixing all other hyper-parameters. We selected MNLI and RTE because they are part of Natural Language Inference, which is one of the hardest tasks on GLUE. Moreover, in the RoBERTa experiments we see the largest drop in student scores for these two datasets. We can observe that for MNLI the best result is for 30% followed by 20% and for RTE the best choice is 40% followed by 30%. This corresponds to the heuristic based data augmentation works where they typically modify tokens with a 30% to 40% probability. We set this parameter to 30% for all the experiments and did not tune this for each dataset or each architecture."
    }, {
      "heading" : "4.6 Generated Samples",
      "text" : "We present a few selected samples that our generator produced during training for the SST-2 dataset on table 6. SST-2 is a binary sentiment analysis dataset. The data consist of movie reviews and is both at the phrase and sentence level.\nWe observe that we only modify a few tokens in the generated text. However, one of three things happens if the text is semantically plausible. Either the generated sentence keeps the same sentiment as in Examples 2 and 3, or it changes the sentiment as in Examples 1 and 4 or the text has ambiguous sentiment as in Example 5. We can use all of these for training since we do not rely on the original label but obtain the teacher’s output."
    }, {
      "heading" : "5 Discussion and Future Work",
      "text" : "We have presented MATE-KD, a novel text-based adversarial training algorithm which improves the student model in KD by generating adversarial examples while accessing the logits of the teacher\nonly. This approach is architecture agnostic and can be easily adapted to other applications of KD such as model ensembling and multi-task learning.\nWe demonstrate the need for an adversarial training algorithm for KD based on text rather than embedding perturbation. Moreover, we demonstrate the importance of masking for our algorithm.\nOne key theme that we have presented in this work is that as PLMs inevitably increase in size and number of parameters, techniques that rely on access to the various layers and intermediate parameters of the teacher will be more difficult to train. In contrast, algorithms which are wellmotivated and require minimal access to the teacher may learn from more powerful teachers and would be more useful. An example of such an algorithm is the KD algorithm itself.\nFuture work will consider a) using label information and a measure of semantic quality to filter the generated sentences b) exploring the application of our algorithm to continuous data such as speech and images and c) exploring other applications of KD."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank MindSpore 3, a new deep learning computing framework, for the partial support of this work\n3https://www.mindspore.cn/\nImpact Statement\nOur research primarily deals with deploying high quality NLP applications to a wide audience around the globe. We contend that these technologies can simplify many of our mundane tasks and free up our time to pursue more pleasurable work."
    }, {
      "heading" : "A Training Details",
      "text" : "We present the details of the learning rate, number of epochs, and the batch size we use for each training set of GLUE for both the BERT and the RoBERTa settings."
    } ],
    "references" : [ {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "Nicholas Léonard", "Aaron Courville." ],
      "venue" : "arXiv preprint arXiv:1308.3432.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Model compression",
      "author" : [ "Cristian Buciluǎ", "Rich Caruana", "Alexandru Niculescu-Mizil." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541.",
      "citeRegEx" : "Buciluǎ et al\\.,? 2006",
      "shortCiteRegEx" : "Buciluǎ et al\\.",
      "year" : 2006
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "arXiv preprint arXiv:1906.02443.",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Bam! born-again multi-task networks for natural language understanding",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Urvashi Khandelwal", "Christopher D Manning", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1907.04829.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "author" : [ "William Fedus", "Barret Zoph", "Noam Shazeer." ],
      "venue" : "arXiv preprint arXiv:2101.03961.",
      "citeRegEx" : "Fedus et al\\.,? 2021",
      "shortCiteRegEx" : "Fedus et al\\.",
      "year" : 2021
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Annealing knowledge distillation",
      "author" : [ "Aref Jafari", "Mehdi Rezagholizadeh", "Pranav Sharma", "Ali Ghodsi." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2493–2504.",
      "citeRegEx" : "Jafari et al\\.,? 2021",
      "shortCiteRegEx" : "Jafari et al\\.",
      "year" : 2021
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "arXiv preprint arXiv:1611.01144.",
      "citeRegEx" : "Jang et al\\.,? 2016",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:1909.10351.",
      "citeRegEx" : "Jiao et al\\.,? 2019",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2019
    }, {
      "title" : "More bang for your buck: Natural perturbation for robust question answering",
      "author" : [ "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 163–",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
      "author" : [ "Zhuohan Li", "Eric Wallace", "Sheng Shen", "Kevin Lin", "Kurt Keutzer", "Dan Klein", "Joseph E Gonzalez." ],
      "venue" : "arXiv preprint arXiv:2002.11794.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448.",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixed Precision Training",
      "author" : [ "Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu." ],
      "venue" : "International Conference on",
      "citeRegEx" : "Micikevicius et al\\.,? 2018",
      "shortCiteRegEx" : "Micikevicius et al\\.",
      "year" : 2018
    }, {
      "title" : "Improved knowledge distillation via teacher assistant",
      "author" : [ "Seyed-Iman Mirzadeh", "Mehrdad Farajtabar", "Ang Li", "Nir Levine", "Akihiro Matsukawa", "Hassan Ghasemzadeh." ],
      "venue" : "arXiv preprint arXiv:1902.03393.",
      "citeRegEx" : "Mirzadeh et al\\.,? 2019",
      "shortCiteRegEx" : "Mirzadeh et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M Dai", "Ian Goodfellow." ],
      "venue" : "arXiv preprint arXiv:1605.07725.",
      "citeRegEx" : "Miyato et al\\.,? 2016",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards zero-shot knowledge distillation for natural language processing",
      "author" : [ "Ahmad Rashid", "Vasileios Lioutas", "Abbas Ghaddar", "Mehdi Rezagholizadeh." ],
      "venue" : "arXiv preprint arXiv:2012.15495.",
      "citeRegEx" : "Rashid et al\\.,? 2020",
      "shortCiteRegEx" : "Rashid et al\\.",
      "year" : 2020
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:2002.12327.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Energy and policy considerations for deep learning in nlp",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1906.02243.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1908.09355.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "arXiv preprint arXiv:2004.02984.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling taskspecific knowledge from bert into simple neural networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1903.12136.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.10957.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "ArXiv, abs/1910.03771.",
      "citeRegEx" : "Scao et al\\.,? 2019",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2019
    }, {
      "title" : "Paws: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Freelb: Enhanced adversarial training for language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Thomas Goldstein", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1909.11764.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Transformers (Vaswani et al., 2017) and transformer-based Pre-trained Language Models (PLMs) (Devlin et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : ", 2017) and transformer-based Pre-trained Language Models (PLMs) (Devlin et al., 2019) are ubiquitous in applications of NLP.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "Recently, the trillion parameter mark has been breached for PLMs (Fedus et al., 2021) amid serious environmental concerns (Strubell et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : ", 2021) amid serious environmental concerns (Strubell et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "paradigm , training larger models may be unavoidable (Li et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Knowledge Distillation (KD) (Buciluǎ et al., 2006; Hinton et al., 2015) is a simple, yet powerful knowledge transfer algorithm which is used for neural model compression (Jiao et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "Knowledge Distillation (KD) (Buciluǎ et al., 2006; Hinton et al., 2015) is a simple, yet powerful knowledge transfer algorithm which is used for neural model compression (Jiao et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : ", 2015) is a simple, yet powerful knowledge transfer algorithm which is used for neural model compression (Jiao et al., 2019; Sanh et al., 2019), ensembling (Hinton et al.",
      "startOffset" : 106,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : ", 2015) is a simple, yet powerful knowledge transfer algorithm which is used for neural model compression (Jiao et al., 2019; Sanh et al., 2019), ensembling (Hinton et al.",
      "startOffset" : 106,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : ", 2019), ensembling (Hinton et al., 2015) and multi-task learning (Clark et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "It is one of the most widely researched algorithms for the compression of transformer-based PLMs (Rogers et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "The first is towards “model” distillation (Sun et al., 2019; Wang et al., 2020; Jiao et al., 2019) i.",
      "startOffset" : 42,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : "The first is towards “model” distillation (Sun et al., 2019; Wang et al., 2020; Jiao et al., 2019) i.",
      "startOffset" : 42,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "The first is towards “model” distillation (Sun et al., 2019; Wang et al., 2020; Jiao et al., 2019) i.",
      "startOffset" : 42,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "The second direction is towards curriculum-based or progressive KD (Sun et al., 2020; Mirzadeh et al., 2019; Jafari et al., 2021) where the student learns one layer at a time or from an intermediary teacher, known as a teacher assistant.",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "The second direction is towards curriculum-based or progressive KD (Sun et al., 2020; Mirzadeh et al., 2019; Jafari et al., 2021) where the student learns one layer at a time or from an intermediary teacher, known as a teacher assistant.",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "The second direction is towards curriculum-based or progressive KD (Sun et al., 2020; Mirzadeh et al., 2019; Jafari et al., 2021) where the student learns one layer at a time or from an intermediary teacher, known as a teacher assistant.",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "We are interested in KD for model compression and study the use of adversarial training (Goodfellow et al., 2014) to improve student accuracy using just the logits of the teacher as in standard KD.",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "• We evaluate our algorithm on the GLUE (Wang et al., 2018) benchmark and demonstrate improvement over competitive baselines.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "Patient KD (Sun et al., 2019) introduces an additional loss to KD which distills the intermediate layer information onto the student network.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : "TinyBERT (Jiao et al., 2019) applies embedding distillation and intermediate layer distillation which includes hidden state distillation and attention weight distillation.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "MiniLM (Wang et al., 2020) proposed an interesting alternative whereby they distill the key, query and value matrices of the final layer of the teacher.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "In NLP, adversarial training has surprisingly been shown to improve generalization as well (Cheng et al., 2019; Zhu et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 33,
      "context" : "In NLP, adversarial training has surprisingly been shown to improve generalization as well (Cheng et al., 2019; Zhu et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 33,
      "context" : "Embedding perturbations are attractive because they produce stronger adversaries (Zhu et al., 2019) and keep the system end-to-end differentiable as the embeddings are continuous.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : "1064 minimize the loss function and b) a way of keeping the perturbations small such as a norm-bound on the gradient (Zhu et al., 2019) or replacing words by their synonyms (Cheng et al.",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : ", 2019) or replacing words by their synonyms (Cheng et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "One of the first works on BERT compression (Tang et al., 2019) used KD and proposed data augmentation using heuristics such as part-of-speech guided word replacement.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "Instead we apply the Gumbel-Softmax (Jang et al., 2016), which, is an approximation to sampling from the argmax.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Using the straight through estimator (Bengio et al., 2013) we can still apply argmax in the forward pass and can obtain text, X ′ from the network outputs: X ′ = Gφ(X ) FORWARD = argmax ( σGumbel(zφ(X ) )",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "We evaluated MATE-KD on all nine datasets of the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark which include classification and regression.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "On the first the teacher model is RoBERTaLARGE (Liu et al., 2019) and the student is initialized with the weights of DistillRoBERTa (Sanh et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and the student is initialized with the weights of DistillRoBERTa (Sanh et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 23,
      "context" : "Both models have a vocabulary size of 50,265 extracted using the Byte Pair Encoding (BPE) (Sennrich et al., 2016) tokenization method.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "On our second setup, the teacher model is BERTBASE (Devlin et al., 2019) and the student model is initialized with the weights of DistilBERT which consists of 6 layers with a hidden dimension of 768 and 8 attention heads.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "We used the AdamW (Loshchilov and Hutter, 2017) optimizer with the default values.",
      "startOffset" : 18,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "We used mixedprecision training (Micikevicius et al., 2018) to expedite the training procedure.",
      "startOffset" : 32,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "Our first baseline is the fine-tuned DistilRoBERTa and then we compare with KD, FreeLB, FreeLB plus KD, and TinyBERT (Jiao et al., 2019) data augmentation plus KD.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "We observe that FreeLB (Zhu et al., 2019) significantly improves the fine-tuned student by around 1.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "The results of BERTBASE, BERTLARGE (Devlin et al., 2019), TinyBERT and MobileBERT (Sun et al.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : ", 2019), TinyBERT and MobileBERT (Sun et al., 2020) are taken from the GLUE leaderboard2.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Most of the works in the literature proposing intermediate layer distillation (Jiao et al., 2019; Sun et al., 2020, 2019) are trained",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : ", we initialize these models with the trained checkpoint of MNLI (Liu et al., 2019).",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "It has been shown that strong NLU models tend to learn spurious surface level patterns from the dataset (Poliak et al., 2018; Gururangan et al., 2018) and may perform poorly on carefully constructed OOD datasets.",
      "startOffset" : 104,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "In Table 4 we present the evaluation of MATE-KD (RoBERTa-based) trained on MNLI and QQP on the HANS (McCoy et al., 2019) and the PAWS (Zhang et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : ", 2019) and the PAWS (Zhang et al., 2019) evaluation sets respectively.",
      "startOffset" : 21,
      "endOffset" : 41
    } ],
    "year" : 2021,
    "abstractText" : "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel textbased adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERTLARGE.",
    "creator" : "LaTeX with hyperref"
  }
}