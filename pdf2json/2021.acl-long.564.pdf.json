{
  "name" : "2021.acl-long.564.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering",
    "authors" : [ "Siddharth Karamcheti", "Ranjay Krishna", "Li Fei-Fei", "Christopher D. Manning" ],
    "emails" : [ "manning}@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7265–7281\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7265"
    }, {
      "heading" : "1 Introduction",
      "text" : "Today, language-equipped vision systems such as VizWiz, TapTapSee, BeMyEyes, and CamFind are actively being deployed across a broad spectrum of users.1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019). Visual Question Answering (VQA), the task of answering questions about\n1Applications can be found at https://vizwiz.org/, https://taptapsee.com/, https://www.bemyeyes.com/, and https://camfindapp.com/\nvisual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015; Krishna et al., 2017; Gordon et al., 2018; Hudson and Manning, 2019). Unfortunately, today’s VQA models are data hungry: Their performance scales monotonically with more train-\ning data (Lu et al., 2016; Lin and Parikh, 2017), motivating the need for data acquisition mechanisms such as active learning, which maximize performance while minimizing expensive data labeling.\nWhile active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al., 2017; Siddhant and Lipton, 2018; Lowell et al., 2019) show little to no improvement in sample efficiency across 5 models on 4 VQA datasets – indeed, in some cases performing worse than randomly selecting data to label. This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as topic classification (Siddhant and Lipton, 2018; Lowell et al., 2019), object recognition (Deng et al., 2018), digit classification (Gal et al., 2017), and named entity recognition (Shen et al., 2017). Our negative results hold even when accounting for common active learning ailments: cold starts, correlated sampling, and uncalibrated uncertainty. We mitigate the cold start challenge of needing a representative initial dataset by varying the size of the seed set in our experiments. We account for sampling correlated data within a given batch by including Core-Set selection (Sener and Savarese, 2018) in the set of active learning methods we evaluate. Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data (Houlsby et al., 2011; Gal and Ghahramani, 2016; Gal et al., 2017).\nAfter concluding that negative results are consistent across all experimental conditions, we investigate active learning’s ineffectiveness on VQA as a data problem and identify the existence of collective outliers (Han and Kamber, 2000) as the source of the problem. Leveraging recent advances in model interpretability, we build Dataset Maps (Swayamdipta et al., 2020), which distinguish between collective outliers and useful data that improve validation set performance (see Figure 1). While global outliers deviate from the rest of the data and are often a consequence of labeling error, collective outliers cluster together; they may not individually be identifiable as outliers but collectively deviate from other examples in the dataset. For instance, VQA-2 (Goyal et al., 2017) is riddled with collections of hard questions that require external knowledge to answer (e.g., “What is the symbol\non the hood often associated with?”) or that ask the model to read text in the images (e.g., “What is the word on the wall?”). Similarly, GQA (Hudson and Manning, 2019) asks underspecified questions (e.g., “what is the person wearing?” which can have multiple correct answers). Collective outliers are not specific to VQA, but can similarly be found in many open-ended tasks, including visual navigation (Anderson et al., 2018b) (e.g., “Go to the grandfather clock” requires identifying rare grandfather clocks), and open-domain question answering (Kwiatkowski et al., 2019), amongst others.\nUsing Dataset Maps, we profile active learning methods and show that they prefer acquiring collective outliers that models are unable to learn, explaining their poor improvements in sample efficiency relative to random sampling. Building on this, we use these maps to perform ablations where we identify and remove outliers iteratively from the active learning pool, observing correlated improvements in sample efficiency. This allows us to conclude that collective outliers are, indeed, responsible for the ineffectiveness of active learning for VQA. We end with prescriptive suggestions for future work in building active learning methods robust to these types of outliers."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work tests the utility of multiple recent active learning methods on the open-ended understanding task of VQA. We draw on the dataset analysis literature to identify collective outliers as the bottleneck hindering active learning methods in this setting.\nActive Learning. Active learning strategies have been successfully applied to image recognition (Joshi et al., 2009; Sener and Savarese, 2018), information extraction (Scheffer et al., 2001; Finn and Kushmerick, 2003; Jones et al., 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al., 2005; Shen et al., 2017), semantic parsing (Dong et al., 2018), and text categorization (Lewis and Gale, 1994; Hoi et al., 2006). However, these same methods struggle to outperform a random baseline when applied to the task of VQA (Lin and Parikh, 2017; Jedoui et al., 2019). To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty (Abramson and Freund, 2004; Collins et al., 2008; Joshi et al., 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby\net al., 2011; Gal et al., 2017), and Core-Set selection (Sener and Savarese, 2018).\nVisual Question Answering. Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020). To ensure that our negative results are not dataset or model-specific, we sample 4 datasets and 5 representative models, each utilizing unique visual and linguistic features and employing different inductive biases.\nInterpreting and Analyzing Datasets. Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al., 2012; Bolukbasi et al., 2016), both of which negatively impact sample efficiency. Prior work has used training dynamics to find examples which are frequently forgotten (Krymolowski, 2002; Toneva et al., 2019) versus those that are easy to learn (Bras et al., 2020). This work suggests using two model-specific measures – confidence and prediction variance – as indicators of a training example’s “learnability” (Chang et al., 2017; Swayamdipta et al., 2020). Dataset Maps (Swayamdipta et al., 2020), a recently introduced framework uses these two measures to profile datasets to find learnable examples. Unlike prior datasets analyzed by Dataset Maps that have a small number of global outliers as hard examples, we discover that VQA datasets contain copious amounts of collective outliers, which are difficult or even impossible for models to learn."
    }, {
      "heading" : "3 Active Learning Experimental Setup",
      "text" : "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994; Settles, 2009; Gal et al., 2017; Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (xi, yi) ∈ Dseed used to initialize M, an unlabeled pool of data Dpool, and an acquisition function A(x,M). We run active learning over a series of acquisition iterations\nT where at each iteration we acquire a batch of B new examples per: x̂ ∈ Dpool to label per x̂ = argmaxx∈Dpool A(x,M).\nAcquiring an example often refers to using an oracle or human expert to annotate a new example with a correct label. We follow prior work to simulate an oracle using existing datasets, forming Dseed from a fixed percentage of the full dataset, and using the remainder as Dpool (Gal et al., 2017; Lin and Parikh, 2017; Siddhant and Lipton, 2018). We re-trainM after each acquisition iteration.\nPrior work has noted the impact of seed set size on active learning performance (Lin and Parikh, 2017; Misra et al., 2018; Jedoui et al., 2019). We run multiple active learning evaluations with varying seed set sizes (ranging from 5% to 50% of the full pool size). We keep the size of each acquisition batch B to a constant 10% of the overall pool size."
    }, {
      "heading" : "3.1 Models",
      "text" : "Visual Question Answering (VQA) requires reasoning over two modalities: images and text. Most models use feature “backbones” (e.g., features from object recognition models pretrained on ImageNet, and pretrained word vectors for text). For image features we use grid-based features from ResNet-101 (He et al., 2016), or object-based features from Faster R-CNN (Ren et al., 2015b) finetuned on Visual Genome (Anderson et al., 2018a). We evaluate with a representative sample of existing VQA models, including the following:2\nLogReg is a logistic regression model that uses either ResNet-101 or Faster R-CNN image features with mean-pooled GloVe question embeddings (Pennington et al., 2014). Although these models\n2Key implementation details can be found in the appendix. In the interest of full reproducibility and further work in active learning and VQA, we release our code and results here: https://github.com/siddk/vqa-outliers.\nare not as performant as the subsequent models, logistic regression has been effective on VQA (Suhr et al., 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007; Yang and Loog, 2018; Mussmann and Liang, 2018).\nLSTM-CNN is a standard model introduced with VQA-1 (Agrawal et al., 2015). We use more performant ResNet-101 features instead of the original VGGNet features as our visual backbone.\nBUTD (Bottom-Up Top-Down Attention) uses object-based features in tandem with attention over objects (Anderson et al., 2018a). BUTD won the 2017 VQA Challenge (Teney et al., 2018), and has been a consistent baseline for recent work in VQA.\nLXMERT is a large multi-modal transformer model that uses BUTD’s object features and contextualized BERT (Devlin et al., 2019) language features (Tan and Bansal, 2019). LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.3"
    }, {
      "heading" : "3.2 Acquisition Functions",
      "text" : "Several active learning methods have been developed to account for different aspects of the machine learning training pipeline: while some acquire examples with high aleotoric uncertainty (Settles, 2009) (having to do with the natural uncertainty in the data) or epistemic uncertainty (Gal et al., 2017) (having to do with the uncertainty in the modeling/learning process), others attempt to acquire examples that reflect the distribution of data in the pool (Sener and Savarese, 2018). We sample a diverse set of these methods:\nRandom Sampling serves as our baseline passive approach for acquiring examples.\nLeast Confidence acquires examples with lowest model prediction probability (Settles, 2009).\n3Results for LXMERT in Tan and Bansal (2019) are reported after pretraining on training and validation examples from the VQA datasets we use. While this is fair if the goal is optimizing for test performance, this exposure to training and validation examples leaks important information; to remedy this, we obtained a model checkpoint from the LXMERT authors trained without VQA data. This is also why our LXMERT results are lower than the numbers reported in the original paper – however, the general boost provided by crossmodal pretraining holds.\nEntropy acquires examples with the highest entropy in the model’s output (Settles, 2009).\nMC-Dropout Entropy (Monte-Carlo Dropout with Entropy acquisition) acquires examples with high entropy in the model’s output averaged over multiple passes through a neural network with different dropout masks (Gal and Ghahramani, 2016). This process is a consequence of a theoretical casting of dropout as approximate Bayesian inference in deep Gaussian processes.\nBALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy (Houlsby et al., 2011; Gal et al., 2017; Siddhant and Lipton, 2018) – capturing “disagreement” across different dropout masks.\nCore-Set Selection samples examples that capture the diversity of the data pool (Sener and Savarese, 2018; Coleman et al., 2020). It acquires examples to minimize the distance between an example in the unlabeled pool to its closest labeled example. Since Core-Set selection operates over a representation space (and not an output distribution, like prior strategies) and VQA models operate over two modalities, we employ three Core-Set variants: Core-Set (Language) and Core-Set (Vision) operate over their respective representation spaces while Core-Set (Fused) operates over the “fused” vision and language representation space."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "We evaluate the 8 active learning strategies across the 5 models described in the previous section. Figures 2–5 show a representative sample of active learning results across datasets. Due to space constraints, we only visualize 4 active learning strategies – Least-Confidence, BALD, CoreSet-Fused, and the Random Baseline – using 3 models (LSTMCNN, BUTD, LXMERT).4 Results and trends are consistent across the different acquisition functions, models and seed set sizes (see the appendix for results with other models, acquisition functions, and seed set sizes). We now go on to provide descriptions of the datasets we evaluate against, and the corresponding results.\n4For LXMERT, running Core-Set selection is prohibitive, so we omit these results; please see Appendix B for more details."
    }, {
      "heading" : "4.1 Simplified VQA Datasets",
      "text" : "One complexity of VQA is the size of the output space and the number of examples present (Agrawal et al., 2015; Goyal et al., 2017); VQA-2 has 400k training examples, and in excess of 3k possible answers (see Table 1). However, prior work in active learning focuses on smaller datasets like the 10-class MNIST dataset (Gal et al., 2017), binary classification (Siddhant and Lipton, 2018), or small-cardinality (≤ 20 classes) text categorization (Lowell et al., 2019). To ensure our results and conclusions are not due to the size of the output space, we build two meaningful, but narrow-domain VQA datasets from subsets of VQA-2. These simplified datasets reduce the complexity of the underlying learning problem and provide a fair comparison to existing active learning literature.\nVQA-Sports. We generate VQA-Sports by compiling a list of 20 popular sports (e.g., soccer, football, tennis, etc.) in VQA-2, and restricting the set of questions to those with answers in this list. We picked the sports categories by ranking the GloVe vector similarity between the word “sports” to answers in VQA-2, and selected the 20 most commonly occurring answers.\nVQA-Food. We generate the VQA-Food dataset similarly, compiling a list of the 20 commonly occurring food categories by GloVe vector similarity to the word “food.”\nResults. Figure 2 presents results for VQASports, with an initial seed set restricted to 10% of the total pool (500 examples). The appendix reports similar results on VQA-Food. For LSTMCNN, Least-Confidence appears to be slightly more sample efficient, while all other strategies perform\non par with or worse than random. For BUTD, all methods are on par with random; for LXMERT, they perform worse than random. Generally on VQA-Sports, active learning performance varies, but fails to outperform random acquisition."
    }, {
      "heading" : "4.2 VQA-2",
      "text" : "VQA-2 is the canonical dataset for evaluating VQA models (Goyal et al., 2017). In keeping with prior work (Anderson et al., 2018a; Tan and Bansal, 2019), we filter the training set to only include answers that appear at least 9 times, resulting in 3130 unique answers. Unlike traditional VQA-2 evaluation, which treats the task as a multi-label binary classification problem, we follow prior active learning work on VQA (Lin and Parikh, 2017), which formulates it as a multi-class classification problem, enabling the use of acquisition functions such as uncertainty sampling and BALD.\nResults. Figures 3 and 4 show results on VQA-2 with different seed set sizes – 10% (40k examples) and 50% (200k examples). Active learning performs relatively better with larger seed sets but still underperforms random. Surprisingly, when initialized with 50% of the pool as the seed set, the gain in validation accuracy after acquiring the entire pool of examples (400k examples total) is only 2%. This is an indication that the lack of sample efficiency might be a result of the underlying data, a problem we explore in the next section."
    }, {
      "heading" : "4.3 GQA",
      "text" : "GQA was introduced as a means for evaluating compositional reasoning (Hudson and Manning, 2019). Unlike VQA’s natural human-written questions, GQA contains synthetic questions of the form “what is inside the bottle the glasses are to\nthe right of?”. We use the standard GQA training set of 943k questions, 900k of which we use for the active learning pool.\nResults. Figure 5 shows results on GQA using a seed set of 10% of the full pool (90k examples). Despite its notable differences in question structure to VQA-2, active learning still performs on par with or slightly worse than random."
    }, {
      "heading" : "5 Analysis via Dataset Maps",
      "text" : "The previous section shows that active learning fails to improve over random acquisition on VQA across models and datasets. A simple question remains – why? One hypothesis is that sample inefficiency stems from the data itself: there is only a 2% gain in validation accuracy when training on half versus the whole dataset. Working from this, we characterize the underlying datasets using Dataset Maps (Swayamdipta et al., 2020) and discover that active learning methods prefer sampling “hard-tolearn” examples, leading to poor performance.\nMapping VQA Datasets. A Dataset Map (Swayamdipta et al., 2020) is a model-specific graph for profiling the learnability of individual training examples. Dataset Maps present holistic pictures of classification datasets relative to the training dynamics of a given model; as a model trains for multiple epochs and sees the same examples repeatedly, the mapping process logs statistics about the confidence assigned to individual predictions. Maps then visualize these statistics against two axes: the y-axis plots the average model confidence assigned to the correct answer over training epochs, while the x-axis plots the spread, or variability, of these values. This introduces a 2D representation of a dataset (viewed through its relationship with individual model) where examples are placed on the map by coarse statistics describing their “learnability“. We show the Dataset Map for BUTD trained on VQA-2 in Figure 1. For our work, we build this map post-hoc, training on the\nentire pool as a means for analyzing what active learning is doing – treating it as a diagnostic tool for identifying the root cause why active learning seems to fail for VQA.\nIn an ideal setting, the majority of examples in the training set should lie in the upper half of the graph – i.e., the mean confidence assigned to the correct answer should be relatively high. Examples towards the upper-left side represent the “easy-tolearn” examples, as the variability in the confidence assigned by the model over time is fairly low.\nA curious feature of VQA-2 and other VQA datasets is the presence of the 25-30% of examples in the bottom-left of the map (shown in red in Figure 1) – examples that have low confidence and variability. In other words, models are unable to learn a large proportion of training examples. While prior work attributes examples in this quadrant to “labeling errors” (Swayamdipta et al., 2020), labeling errors in VQA are sparse, and cannot account for the density of such examples in these maps.\nInterpreting Acquisitions. We profile the acquisitions made by each active learning method, contextualizing the acquired examples via their placement on the associated Dataset Map. We segregate training examples into four buckets using the map’s y-axis: easy (≥ 0.75), medium (≥ 0.50), hard (≥ 0.25), and impossible (≥ 0.00). Ideally, active learning should be robust to “hard-to-learn” examples, focusing instead on learnable, high uncertainty examples towards the upper-right portion of the Dataset Map. Instead, we find that active learning methods acquire a large proportion of impossible examples early on and concentrate on the easier examples only after the impossible examples dwindle (see Figure 6). In contrast, the random baseline acquires examples proportional to each bucket’s density in the underlying map; acquiring easier examples earlier and performing on par with or better than all others."
    }, {
      "heading" : "6 Collective Outliers",
      "text" : "This leaves two questions: 1) can we characterize these “hard” examples, and 2) are these examples responsible for the ineffectiveness of active learning on VQA? We first identify hard-to-learn examples as collective outliers and explain why active learning methods prefer to acquire them. Next, we perform ablation experiments, removing these outliers from the active learning pool iteratively, and demonstrate a corresponding boost in sample efficiency relative to random acquisition.\nHard Examples are Collective Outliers. Collective outliers are groups of examples that deviate from the rest of the examples but cluster together (Han and Kamber, 2000) – they often present as fundamental subproblems of a broader task. For instance (Figure 7), in VQA-2, we identify clusters of hard-to-learn examples that require optical character recognition (OCR) for reasoning about text (e.g., “What is the first word on the black car?”); another cluster requires external knowledge to answer (“What is the symbol on the hood often associated with?”). In GQA, we identify different clusters of collective outliers; one cluster stems from innate underspecification (e.g., “what is on the shelf?” with multiple objects present on the shelf); another cluster requires multiple reasoning hops difficult for current models (e.g., “What is the vehicle that is driving down the road the box is on the side of?”).\nWe sample 100 random “hard-to-learn” examples from both VQA-2 and GQA and find that 100% of the examples belong to one of the two aforementioned collectives. Since hard-to-learn examples constitute 25–30% of the data pool, active learning methods cannot avoid them. Uncertainty-\nbased methods (e.g., Least-Confidence, Entropy, Monte-Carlo Dropout) identify them as valid acquisition targets because models lack the capacity to correctly answer these examples, assigning low confidence and high uncertainty. Disagreementbased methods (e.g., BALD) are similar; model confidence is generally low but high variance (lower middle/lower right of the Dataset Maps). Finally, diversity methods (e.g., Core-Set selection) identify these examples as different enough from the existing pool to warrant acquisition, but fail to learn meaningful representations, fueling a vicious cycle wherein they continue to pick these examples.\nAblating Outliers. To verify that collective outliers are responsible for the degradation of active learning performance, we re-run our experiments using active learning pools with varying numbers of outliers removed. To remove these outliers, we sort and remove all examples in the data pool using the product of their model confidence and prediction variability (x and y-axis values of the Dataset Maps). We systematically remove examples with a low product value and observe how active learning performance changes (see Figure 8).\nWe observe a 2–3x improvement in sample efficiency when removing 50% of the entire data pool, consisting mainly of collective outliers (Figure 8c). This improvement decreases if we only remove 25% of the full pool (Figure 8b), and further degrades if we remove only 10% (Figure 8a). This ablation demonstrates that active learning methods are more sample efficient than the random baseline when collective outliers are absent from the unlabelled pool."
    }, {
      "heading" : "7 Discussion and Future Work",
      "text" : "This paper asks a simple question – why does the modern neural active learning toolkit fail when applied to complex, open ended tasks? While we focus on VQA, collective outliers are abundant in tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and opendomain question answering (Kwiatkowski et al., 2019), amongst others. More insidious is their nature; collective outliers can take multiple forms, requiring external domain knowledge or “commonsense” reasoning, containing underspecification, or requiring capabilities beyond the scope of a given model (e.g., requiring OCR ability). While we perform ablations in this work removing collective outliers, demonstrating that active learning fails as collective outliers take up larger portions of the dataset, this is only an analytical tool; these outliers are, and will continue to be, pervasive in open-ended datasets – and as such, we will need to develop better tools for learning (and performing active learning) in their presence.\nSelective Classification. One potential direction for future work is to develop systems that abstain when they encounter collective outliers. Historical artificial intelligence systems, such as SHRDLU (Winograd, 1972) and QUALM (Lehnert, 1977), were designed to flag input sequences that they were not designed to parse. Ideas from those methods can and should be resurrected using modern techniques; for example, recent work suggests that a simple classifier can be trained to identify out-ofdomain data inputs, provided a seed out-of-domain dataset (Kamath et al., 2020). Active learning methods can be augmented with a similar classifier, which re-calibrates active learning uncertainty scores with this classifier’s predictions. Other work learns to identify novel utterances by learning to intelligently set thresholds in representation space (Karamcheti et al., 2020), a powerful idea especially if combined with other representation-centric active learning methods like Core-Set Sampling (Sener and Savarese, 2018).\nActive Learning with Global Reasoning. Another direction for future work to explore is to leverage Dataset Maps to perform more global, holistic reasoning over datasets, to intelligently identify promising examples – in a sense, baking part of the analysis done in this work directly into the active learning algorithms. A possible instantiation\nof this idea would be in training a discriminator to differentiate between “learnable” examples (upper half of each Dataset Map) from the “unlearnable”, collective outliers with low confidence and low variability. Between each active learning acquisition iteration, one can generate an updated Dataset Map, thereby reflecting what models are learning as they obtain new labeled examples.\nMachine learning systems deployed in realworld settings will inevitably encounter open-world datasets, ones that contain a mixture of learnable and unlearnable inputs. Our work provides a framework to study when models encounter such inputs. Overall, we hope that our experiments serve as a catalyst for future work on evaluating active learning methods with inputs drawn from open-world datasets.\nReproducibility\nAll code for data preprocessing, model implementation, and active learning algorithms is made available at https://github.com/siddk/vqa-outliers. Additionally, this repository also contains the full set of results and dataset maps as well.\nThe authors are fully committed to maintaining this repository, in terms of both functionality and ease of use, and will actively monitor both email and Github Issues should there be problems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Kaylee Burns, Eric Mitchell, Stephen Mussman, Dorsa Sadigh, and our anonymous ACL reviewers for their useful feedback on earlier versions of this paper. We are also grateful to Hao Tan for providing us with the LXMERT checkpoint trained without access to VQA datasets, as well as for general LXMERT fine-tuning pointers.\nSiddharth Karamcheti is graciously supported by the Open Philanthropy Project AI Fellowship. Christopher D. Manning is a CIFAR Fellow."
    }, {
      "heading" : "A Overview",
      "text" : "Due to the broad scope of our experiments and analysis, we were unable to fit all our results in the main body of the paper. Furthermore, given the limited length provided by the appendix, we provide only salient implementation details and other representative results here; however, we make all code, models, data, results, active learning implementations available at this link: https: //github.com/siddk/vqa-outliers.\nGenerally, any combination of {active learning strategy × model × seed set size × analysis/acquisition plot} is present in this paper, and is available in the public code repository.\nB Implementation Details\nB.1 Models & Training\nWhere applicable, we implement our models based on publicly available PyTorch implementations. For the LSTM-CNN model, we base our implementation off of this repository: https://github.com/ Shivanshu-Gupta/Visual-Question-Answering, while for the Bottom-Up Top-Down Attention Model, we use this repository: https://github.com/ hengyuan-hu/bottom-up-attention-vqa, keeping default hyperparameters the same.\nLogistic Regression. When implementing Logistic Regression, we base our PyTorch implementation on the broadly used Scikit-Learn (https: //scikit-learn.org) implementation, using the default parameters (including L2 weight decay). We optimize our models via stochastic gradient descent.\nLXMERT. As mentioned in Section 3, the default LXMERT checkpoint and fine-tuning code made publicly available in Tan and Bansal (2019) (associated code repository: https://github.com/ airsplay/lxmert) is pretrained on data from VQA-2 and GQA, leaking information that could substantially affect our active learning results. To mitigate this, we contacted the authors, who kindly provided us with a checkpoint of the model without VQA pretraining.\nHowever, in addition to this model obtaining different results from those reported in the original work, the provided pretrained checkpoint behaves slightly differently during fine-tuning, requiring different hyperparameters than provided in the original repository. We perform a coarse grid search\nover hyperparameters, using the LXMERT implementation provided by HuggingFace Transformers (Wolf et al., 2019), and find that using an AdamW optimizer rather than the BERT-Adam Optimizer used in the original work without any special learning rate scheduling results in the best fine-tuning performance.\nB.2 Acquisition Functions\nWe use standard implementations of the 8 active learning strategies described, borrowing from prior implementations (Mussmann and Liang, 2018) and existing code repositories (https://github.com/ google/active-learning). We provide additional details below.\nMonte-Carlo Dropout. For our implementations of the deep Bayesian active learning methods (Monte-Carlo Dropout w/ Entropy, BALD), we follow Gal and Ghahramani (2016) and estimate a Dropout distribution via test-time dropout, running multiple forward passes through our neural networks, with different, randomly sampled Dropout masks. We use a value of k = 10 forward passes to form our Dropout distribution.\nAmortized Core-Set Selection. In the original Core-Set selection active learning work introduced by Sener and Savarese (2018), it is shown that CoreSet selection for active learning can be reduced to a version of the k-centers problem, which can be solved approximately (2-OPT) with a greedy algorithm. However, running this algorithm on highdimensional representations, across large pools can be prohibitive; Core-Set selection is batch-aware, requiring recomputing distances from each “clustercenter” (points in the set of acquired examples) to all points in the active learning pool after each acquisition in a batch. While we can run this out completely for smaller datasets (and indeed, this is what we do for our small datasets VQA-Sports and VQA-Food), a single acquisition iteration for a large dataset for the full VQA-2 dataset takes approximately 20 GPU-hours on the resources we have available, or up to 9 days for a single Core-Set selection run. For GQA, performing exact Core-Set selection takes at least twice as long.\nTo still capture the spirit of Core-Set diversitybased selection in our evaluation, we instead introduce an amortized implementation of Core-Set selection, which is comprised of two steps. We first downsample the high-dimensional representations\n(of either the fused language and text, or either unimodal representations) via Principal Component Analysis (PCA) to make the distance computation faster by an order of magnitude. Then, rather than updating distances from examples in our acquired set to points in our pool after each acquisition x̂, we delay updates, instead only refreshing the distance computation every 2000 acquisitions (roughly 5% of an acquisition batch for VQA-2). This allows us to report results for Core-Set selection with the three different proposed representations (Fused, Language-Only, Vision-Only) for VQA-2; unfortunately, for GQA and LXMERT (due to the high cost of training), even running this amortized version of Core-Set selection is prohibitive, so we report a subset of results, and omit the rest."
    }, {
      "heading" : "C Active Learning Results",
      "text" : "We include further results from our study of active learning applied to VQA, including results on VQA-Food (not included in the main body), active learning results for the two logistic regression models – Log-Reg (ResNet-101) and Log-Reg (Faster R-CNN), as well as with the 4 acquisition strategies not included in the main body of the paper – Entropy, Monte-Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision).\nC.1 VQA-Food\nFigure 9 shows results on VQA-Food with the LSTM-CNN, BUTD, and LXMERT models, with a seed set comprised of 10% of the total pool. The results are mostly similar to those reported in the paper; strategies track or underperform random sampling, with the exception of Least-Confidence for the LSTM-CNN model – however, this is the sole exception, and the LSTM-CNN has the highest training variance of all the models we try.\nC.2 Logistic Regression (ResNet-101)\nFigure 10 shows active learning results for the LogReg (ResNet-101) model on VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%). Results are similar to those reported in the paper, with active learning failing to outperform random acqusition.\nC.3 Logistic Regression (Faster R-CNN)\nFigure 11 presents the same set of experiments as the prior section, except with the LogReg (Faster R-CNN) model. While the object-based Faster\nR-CNN representation enables much higher performance than the ResNet-101 representation, active learning results are consistent with those reported in the paper.\nC.4 Other Acquisition Strategies Figure 12 presents results for the four other active learning strategies we implement – Entropy, Monte Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision) – for the BUTD model. Results are across VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%) – despite the unique features of each strategy, the trends remain consistent with those in the paper."
    }, {
      "heading" : "D Dataset Maps & Acquisitions",
      "text" : "To provide further context around active learning acquisitions across datasets, Figures 13–16 present Dataset Maps and acquisitions for the BUTD Model across VQA-Sports, VQA-Food, and GQA respectively. Interesting to note is that while VQA-Sports and VQA-Food are generally easier, with fewer “hard-to-learn” examples, active learning still has a bias for picking those examples. For GQA, our earlier analysis is confirmed; active learning is picking the collective outliers populating the bottom half of the Dataset Map."
    } ],
    "references" : [ {
      "title" : "Active learning for visual object recognition",
      "author" : [ "Yotam Abramson", "Yoav Freund." ],
      "venue" : "Technical report, University of California, San Diego.",
      "citeRegEx" : "Abramson and Freund.,? 2004",
      "shortCiteRegEx" : "Abramson and Freund.",
      "year" : 2004
    }, {
      "title" : "VQA: Visual question answering",
      "author" : [ "Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "International Journal of Computer Vision, 123:4–31.",
      "citeRegEx" : "Agrawal et al\\.,? 2015",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2015
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "X. He", "C. Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages",
      "citeRegEx" : "Anderson et al\\.,? 2018a",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "2018b. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
      "author" : [ "Peter Anderson", "Qi Wu", "Damien Teney", "Jake Bruce", "Mark Johnson", "Niko Sünderhauf", "Ian Reid", "Stephen Gould", "Anton van den Hengel" ],
      "venue" : null,
      "citeRegEx" : "Anderson et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "VizWiz: nearly realtime answers to visual questions",
      "author" : [ "Jeffrey P Bigham", "Chandrika Jayant", "Hanjie Ji", "Greg Little", "Andrew Miller", "Robert C Miller", "Robin Miller", "Aubrey Tatarowicz", "Brandyn White", "Samual White", "Tom Yeh." ],
      "venue" : "User Interface",
      "citeRegEx" : "Bigham et al\\.,? 2010",
      "shortCiteRegEx" : "Bigham et al\\.",
      "year" : 2010
    }, {
      "title" : "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
      "author" : [ "Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai." ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bolukbasi et al\\.,? 2016",
      "shortCiteRegEx" : "Bolukbasi et al\\.",
      "year" : 2016
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial filters of dataset biases",
      "author" : [ "Ronan Le Bras", "Swabha Swayamdipta", "Chandra Bhagavatula", "Rowan Zellers", "Matthew Peters", "Ashish Sabharwal", "Yejin Choi." ],
      "venue" : "International Conference on Machine Learning (ICML), pages 1078–1088.",
      "citeRegEx" : "Bras et al\\.,? 2020",
      "shortCiteRegEx" : "Bras et al\\.",
      "year" : 2020
    }, {
      "title" : "Active bias: Training more accurate neural networks by emphasizing high variance samples",
      "author" : [ "Haw-Shiuan Chang", "Erik Learned-Miller", "Andrew McCallum." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 1002–1012.",
      "citeRegEx" : "Chang et al\\.,? 2017",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2017
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "European Conference on Computer Vision (ECCV), pages 104–120.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Selection via proxy: Efficient data selection for deep learning",
      "author" : [ "Cody Coleman", "Christopher Yeh", "Stephen Mussmann", "Baharan Mirzasoleiman", "Peter Bailis", "Percy Liang", "Jure Leskovec", "Matei Zaharia." ],
      "venue" : "International Conference on Learning Represen-",
      "citeRegEx" : "Coleman et al\\.,? 2020",
      "shortCiteRegEx" : "Coleman et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards scalable dataset construction: An active learning approach",
      "author" : [ "Brendan Collins", "Jia Deng", "Kai Li", "Li Fei-Fei." ],
      "venue" : "European Conference on Computer Vision (ECCV), pages 86–98.",
      "citeRegEx" : "Collins et al\\.,? 2008",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2008
    }, {
      "title" : "Reducing labeling effort for structured prediction tasks",
      "author" : [ "Aron Culotta", "Andrew McCallum." ],
      "venue" : "Association for the Advancement of Artificial Intelligence (AAAI), pages 746–751.",
      "citeRegEx" : "Culotta and McCallum.,? 2005",
      "shortCiteRegEx" : "Culotta and McCallum.",
      "year" : 2005
    }, {
      "title" : "Adversarial active learning for sequences labeling and generation",
      "author" : [ "Yue Deng", "KaWai Chen", "Yilin Shen", "Hongxia Jin." ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI), pages 4012– 4018.",
      "citeRegEx" : "Deng et al\\.,? 2018",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Association for Computational Linguistics (ACL), pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Confidence modeling for neural semantic parsing",
      "author" : [ "Li Dong", "Chris Quirk", "Mirella Lapata." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "Active learning selection strategies for information extraction",
      "author" : [ "Aidan Finn", "Nicolas Kushmerick." ],
      "venue" : "Proceedings of the International Workshop on Adaptive Text Extraction and Mining (ATEM-03), pages 18–25.",
      "citeRegEx" : "Finn and Kushmerick.,? 2003",
      "shortCiteRegEx" : "Finn and Kushmerick.",
      "year" : 2003
    }, {
      "title" : "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "author" : [ "Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach." ],
      "venue" : "Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Fukui et al\\.,? 2016",
      "shortCiteRegEx" : "Fukui et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Gal and Ghahramani.,? 2016",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Deep Bayesian active learning with image data",
      "author" : [ "Yarin Gal", "R. Islam", "Zoubin Ghahramani." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Gal et al\\.,? 2017",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2017
    }, {
      "title" : "IQA: Visual question answering in interactive environments",
      "author" : [ "Daniel Gordon", "Aniruddha Kembhavi", "Mohammad Rastegari", "Joseph Redmon", "Dieter Fox", "Ali Farhadi." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Gordon et al\\.,? 2018",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2018
    }, {
      "title" : "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A Smith." ],
      "venue" : "Association for Computational Linguistics (ACL), pages 107–112.",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Investigating the effects of selective sampling on the annotation task",
      "author" : [ "Ben Hachey", "Beatrice Alex", "Markus Becker." ],
      "venue" : "Computational Natural Language Learning (CoNLL), pages 144–151.",
      "citeRegEx" : "Hachey et al\\.,? 2005",
      "shortCiteRegEx" : "Hachey et al\\.",
      "year" : 2005
    }, {
      "title" : "Data Mining: Concepts and Techniques",
      "author" : [ "Jiawei Han", "Micheline Kamber." ],
      "venue" : "Morgan Kaufmann.",
      "citeRegEx" : "Han and Kamber.,? 2000",
      "shortCiteRegEx" : "Han and Kamber.",
      "year" : 2000
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch mode active learning and its application to medical image classification",
      "author" : [ "Steven CH Hoi", "Rong Jin", "Jianke Zhu", "Michael R Lyu." ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning, pages 417–424.",
      "citeRegEx" : "Hoi et al\\.,? 2006",
      "shortCiteRegEx" : "Hoi et al\\.",
      "year" : 2006
    }, {
      "title" : "Bayesian active learning for classification and preference learning",
      "author" : [ "Neil Houlsby", "Ferenc Huszár", "Zoubin Ghahramani", "Máté Lengyel." ],
      "venue" : "arXiv preprint arXiv:1112.5745.",
      "citeRegEx" : "Houlsby et al\\.,? 2011",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2011
    }, {
      "title" : "GQA: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Deep Bayesian active learning for multiple correct outputs",
      "author" : [ "Khaled Jedoui", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei." ],
      "venue" : "arXiv preprint arXiv:1912.01119.",
      "citeRegEx" : "Jedoui et al\\.,? 2019",
      "shortCiteRegEx" : "Jedoui et al\\.",
      "year" : 2019
    }, {
      "title" : "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick." ],
      "venue" : "Computer Vision and Pattern Recognition",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Active learning for information extraction with multiple view feature sets",
      "author" : [ "Rosie Jones", "Rayid Ghani", "Tom Mitchell", "Ellen Riloff." ],
      "venue" : "International Conference on Knowledge Discovery and Data Mining (KDD), pages 26–34.",
      "citeRegEx" : "Jones et al\\.,? 2003",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2003
    }, {
      "title" : "Multi-class active learning for image classification",
      "author" : [ "Ajay J Joshi", "Fatih Porikli", "Nikolaos Papanikolopoulos." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 2372–2379.",
      "citeRegEx" : "Joshi et al\\.,? 2009",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2009
    }, {
      "title" : "Selective question answering under domain shift",
      "author" : [ "Amita Kamath", "Robin Jia", "Percy Liang." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Kamath et al\\.,? 2020",
      "shortCiteRegEx" : "Kamath et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning adaptive language interfaces through decomposition",
      "author" : [ "Siddharth Karamcheti", "Dorsa Sadigh", "Percy Liang." ],
      "venue" : "EMNLP Workshop for Interactive and Executable Semantic Parsing (IntExSemPar).",
      "citeRegEx" : "Karamcheti et al\\.,? 2020",
      "shortCiteRegEx" : "Karamcheti et al\\.",
      "year" : 2020
    }, {
      "title" : "What uncertainties do we need in Bayesian deep learning for computer vision",
      "author" : [ "Alex Kendall", "Yarin Gal" ],
      "venue" : "In Advances in Neural Information Processing Systems (NeurIPS),",
      "citeRegEx" : "Kendall and Gal.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kendall and Gal.",
      "year" : 2017
    }, {
      "title" : "Undoing the damage of dataset bias",
      "author" : [ "Aditya Khosla", "Tinghui Zhou", "Tomasz Malisiewicz", "Alexei A Efros", "Antonio Torralba." ],
      "venue" : "European Conference on Computer Vision (ECCV), pages 158–171.",
      "citeRegEx" : "Khosla et al\\.,? 2012",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2012
    }, {
      "title" : "Visual genome: Connecting language and vision",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidi", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "Distinguishing easy and hard instances",
      "author" : [ "Yuval Krymolowski." ],
      "venue" : "International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Krymolowski.,? 2002",
      "shortCiteRegEx" : "Krymolowski.",
      "year" : 2002
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "The Process of Question Answering",
      "author" : [ "Wendy Lehnert." ],
      "venue" : "Ph.D. thesis, Yale University.",
      "citeRegEx" : "Lehnert.,? 1977",
      "shortCiteRegEx" : "Lehnert.",
      "year" : 1977
    }, {
      "title" : "Heterogeneous uncertainty sampling for supervised learning",
      "author" : [ "David D Lewis", "Jason Catlett." ],
      "venue" : "International Conference on Machine Learning (ICML), pages 148–156.",
      "citeRegEx" : "Lewis and Catlett.,? 1994",
      "shortCiteRegEx" : "Lewis and Catlett.",
      "year" : 1994
    }, {
      "title" : "A sequential algorithm for training text classifiers",
      "author" : [ "David D Lewis", "William A Gale." ],
      "venue" : "ACM Special Interest Group on Information Retreival (SIGIR).",
      "citeRegEx" : "Lewis and Gale.,? 1994",
      "shortCiteRegEx" : "Lewis and Gale.",
      "year" : 1994
    }, {
      "title" : "Repair: Removing representation bias by dataset resampling",
      "author" : [ "Yi Li", "Nuno Vasconcelos." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 9572–9581.",
      "citeRegEx" : "Li and Vasconcelos.,? 2019",
      "shortCiteRegEx" : "Li and Vasconcelos.",
      "year" : 2019
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "European Conference on Computer Vision (ECCV), pages 740–755.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Active learning for visual question answering: An empirical study",
      "author" : [ "Xiao Lin", "Devi Parikh." ],
      "venue" : "arXiv preprint arXiv:1711.01732.",
      "citeRegEx" : "Lin and Parikh.,? 2017",
      "shortCiteRegEx" : "Lin and Parikh.",
      "year" : 2017
    }, {
      "title" : "Practical obstacles to deploying active learning",
      "author" : [ "David Lowell", "Zachary C. Lipton", "Byron C. Wallace." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lowell et al\\.,? 2019",
      "shortCiteRegEx" : "Lowell et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask your neurons: A neural-based approach to answering questions about images",
      "author" : [ "Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz." ],
      "venue" : "International Conference on Computer Vision (ICCV), pages 1–9.",
      "citeRegEx" : "Malinowski et al\\.,? 2015",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R Walter." ],
      "venue" : "Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Mei et al\\.,? 2016",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning by asking questions",
      "author" : [ "Ishan Misra", "Ross Girshick", "Rob Fergus", "Martial Hebert", "Abhinav Gupta", "Laurens Van Der Maaten." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 11–20.",
      "citeRegEx" : "Misra et al\\.,? 2018",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2018
    }, {
      "title" : "On the relationship between data efficiency and error in active learning",
      "author" : [ "Stephen Mussmann", "Percy Liang." ],
      "venue" : "International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Mussmann and Liang.,? 2018",
      "shortCiteRegEx" : "Mussmann and Liang.",
      "year" : 2018
    }, {
      "title" : "AI-based request augmentation to increase crowdsourcing participation",
      "author" : [ "Junwon Park", "Ranjay Krishna", "Pranav Khadpe", "Li FeiFei", "Michael Bernstein." ],
      "venue" : "Association for the Advancement of Artificial Intelligence (AAAI), volume 7, pages 115–124.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploring models and data for image question answering",
      "author" : [ "Mengye Ren", "Ryan Kiros", "Richard Zemel." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS), pages 2953–2961.",
      "citeRegEx" : "Ren et al\\.,? 2015a",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 39:1137–1149.",
      "citeRegEx" : "Ren et al\\.,? 2015b",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Active hidden Markov models for information extraction",
      "author" : [ "Tobias Scheffer", "Christian Decomain", "Stefan Wrobel." ],
      "venue" : "International Symposium on Intelligent Data Analysis, pages 309–318.",
      "citeRegEx" : "Scheffer et al\\.,? 2001",
      "shortCiteRegEx" : "Scheffer et al\\.",
      "year" : 2001
    }, {
      "title" : "Active learning for logistic regression: An evaluation",
      "author" : [ "A. Schein", "Lyle H. Ungar." ],
      "venue" : "Machine Learning, 68:235–265.",
      "citeRegEx" : "Schein and Ungar.,? 2007",
      "shortCiteRegEx" : "Schein and Ungar.",
      "year" : 2007
    }, {
      "title" : "Active learning for convolutional neural networks: A core-set approach",
      "author" : [ "Ozan Sener", "Silvio Savarese." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Sener and Savarese.,? 2018",
      "shortCiteRegEx" : "Sener and Savarese.",
      "year" : 2018
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles." ],
      "venue" : "Technical report, University of Wisconsin, Madison.",
      "citeRegEx" : "Settles.,? 2009",
      "shortCiteRegEx" : "Settles.",
      "year" : 2009
    }, {
      "title" : "Deep active learning for named entity recognition",
      "author" : [ "Yanyao Shen", "Hyokun Yun", "Zachary C Lipton", "Yakov Kronrod", "Animashree Anandkumar." ],
      "venue" : "Proceedings of the Second Workshop on Representation Learning for NLP (Repl4NLP).",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study",
      "author" : [ "Aditya Siddhant", "Zachary C Lipton." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Siddhant and Lipton.,? 2018",
      "shortCiteRegEx" : "Siddhant and Lipton.",
      "year" : 2018
    }, {
      "title" : "A corpus for reasoning about natural language grounded in photographs",
      "author" : [ "Alane Suhr", "Stephanie Zhou", "Ally Zhang", "Iris Zhang", "Huajun Bai", "Yoav Artzi." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Suhr et al\\.,? 2019",
      "shortCiteRegEx" : "Suhr et al\\.",
      "year" : 2019
    }, {
      "title" : "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
      "author" : [ "Swabha Swayamdipta", "Roy Schwartz", "Nicholas Lourie", "Yizhong Wang", "Hannaneh Hajishirzi", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Empirical Methods in Natural Language",
      "citeRegEx" : "Swayamdipta et al\\.,? 2020",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2020
    }, {
      "title" : "LXMERT: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Hao Tan", "Mohit Bansal." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Understanding natural language commands for robotic navigation and mobile manipulation",
      "author" : [ "Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R Walter", "Ashis Gopal Banerjee", "Seth J Teller", "Nicholas Roy." ],
      "venue" : "Association for the Ad-",
      "citeRegEx" : "Tellex et al\\.,? 2011",
      "shortCiteRegEx" : "Tellex et al\\.",
      "year" : 2011
    }, {
      "title" : "Tips and tricks for visual question answering: Learnings from the 2017 challenge",
      "author" : [ "Damien Teney", "Peter Anderson", "Xiaodong He", "Anton V.D. Hengel." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 4223–4232.",
      "citeRegEx" : "Teney et al\\.,? 2018",
      "shortCiteRegEx" : "Teney et al\\.",
      "year" : 2018
    }, {
      "title" : "An empirical study of example forgetting during deep neural network learning",
      "author" : [ "Mariya Toneva", "Alessandro Sordoni", "Remi Tachet des Combes", "Adam Trischler", "Yoshua Bengio", "Geoffrey J Gordon." ],
      "venue" : "International Conference on Learning Represen-",
      "citeRegEx" : "Toneva et al\\.,? 2019",
      "shortCiteRegEx" : "Toneva et al\\.",
      "year" : 2019
    }, {
      "title" : "Support vector machine active learning with applications to text classification",
      "author" : [ "Simon Tong", "Daphne Koller." ],
      "venue" : "Journal of machine learning research, 2(0):45–66.",
      "citeRegEx" : "Tong and Koller.,? 2001",
      "shortCiteRegEx" : "Tong and Koller.",
      "year" : 2001
    }, {
      "title" : "Unbiased look at dataset bias",
      "author" : [ "Antonio Torralba", "Alexei A Efros." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 1521–1528.",
      "citeRegEx" : "Torralba and Efros.,? 2011",
      "shortCiteRegEx" : "Torralba and Efros.",
      "year" : 2011
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Association for Computational Linguistics (ACL), pages 1112–1122.",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Understanding Natural Language",
      "author" : [ "Terry Winograd." ],
      "venue" : "Academic Press.",
      "citeRegEx" : "Winograd.,? 1972",
      "shortCiteRegEx" : "Winograd.",
      "year" : 1972
    }, {
      "title" : "HuggingFace’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Ask me anything: Free-form visual question answering based on",
      "author" : [ "Qi Wu", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "A benchmark and comparison of active learning for logistic regression",
      "author" : [ "Yazhou Yang", "Marco Loog." ],
      "venue" : "Pattern Recognition, 83.",
      "citeRegEx" : "Yang and Loog.,? 2018",
      "shortCiteRegEx" : "Yang and Loog.",
      "year" : 2018
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple baseline for visual question answering",
      "author" : [ "Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus." ],
      "venue" : "arXiv preprint arXiv:1512.02167.",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual7W: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li FeiFei." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pages 4995–5004.",
      "citeRegEx" : "Zhu et al\\.,? 2016",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    }, {
      "title" : "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
      "author" : [ "Yuke Zhu", "Roozbeh Mottaghi", "Eric Kolve", "Joseph J Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi." ],
      "venue" : "International Conference on Robotics and Automation",
      "citeRegEx" : "Zhu et al\\.,? 2017",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2017
    }, {
      "title" : "2018), it is shown that CoreSet selection for active learning can be reduced to a version of the k-centers problem, which can be solved approximately (2-OPT) with a greedy algorithm",
      "author" : [ "Sener", "Savarese" ],
      "venue" : null,
      "citeRegEx" : "Sener and Savarese,? \\Q2018\\E",
      "shortCiteRegEx" : "Sener and Savarese",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 267
    }, {
      "referenceID" : 65,
      "context" : "1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 267
    }, {
      "referenceID" : 49,
      "context" : "1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 267
    }, {
      "referenceID" : 78,
      "context" : "1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 267
    }, {
      "referenceID" : 52,
      "context" : "1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010; Tellex et al., 2011; Mei et al., 2016; Zhu et al., 2017; Anderson et al., 2018b; Park et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 267
    }, {
      "referenceID" : 1,
      "context" : "visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015; Krishna et al., 2017; Gordon et al., 2018; Hudson and Manning, 2019).",
      "startOffset" : 96,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : "visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015; Krishna et al., 2017; Gordon et al., 2018; Hudson and Manning, 2019).",
      "startOffset" : 96,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015; Krishna et al., 2017; Gordon et al., 2018; Hudson and Manning, 2019).",
      "startOffset" : 96,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015; Krishna et al., 2017; Gordon et al., 2018; Hudson and Manning, 2019).",
      "startOffset" : 96,
      "endOffset" : 187
    }, {
      "referenceID" : 47,
      "context" : "7266 ing data (Lu et al., 2016; Lin and Parikh, 2017), motivating the need for data acquisition mechanisms such as active learning, which maximize performance while minimizing expensive data labeling.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 45,
      "context" : "7266 ing data (Lu et al., 2016; Lin and Parikh, 2017), motivating the need for data acquisition mechanisms such as active learning, which maximize performance while minimizing expensive data labeling.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 41,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 68,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 59,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 19,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al., 2017; Siddhant and Lipton, 2018; Lowell et al., 2019) show little to no improvement in sample efficiency across 5 models on 4 VQA datasets – indeed, in some cases performing worse than randomly selecting data to label.",
      "startOffset" : 250,
      "endOffset" : 316
    }, {
      "referenceID" : 61,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al., 2017; Siddhant and Lipton, 2018; Lowell et al., 2019) show little to no improvement in sample efficiency across 5 models on 4 VQA datasets – indeed, in some cases performing worse than randomly selecting data to label.",
      "startOffset" : 250,
      "endOffset" : 316
    }, {
      "referenceID" : 46,
      "context" : "While active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994; Tong and Koller, 2001; Culotta and McCallum, 2005; Settles, 2009), we find that 8 modern active learning methods (Gal et al., 2017; Siddhant and Lipton, 2018; Lowell et al., 2019) show little to no improvement in sample efficiency across 5 models on 4 VQA datasets – indeed, in some cases performing worse than randomly selecting data to label.",
      "startOffset" : 250,
      "endOffset" : 316
    }, {
      "referenceID" : 61,
      "context" : "This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as topic classification (Siddhant and Lipton, 2018; Lowell et al., 2019), object recognition (Deng et al.",
      "startOffset" : 155,
      "endOffset" : 203
    }, {
      "referenceID" : 46,
      "context" : "This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as topic classification (Siddhant and Lipton, 2018; Lowell et al., 2019), object recognition (Deng et al.",
      "startOffset" : 155,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : ", 2019), object recognition (Deng et al., 2018), digit classification (Gal et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : ", 2018), digit classification (Gal et al., 2017), and named entity recognition (Shen et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 60,
      "context" : ", 2017), and named entity recognition (Shen et al., 2017).",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 58,
      "context" : "We account for sampling correlated data within a given batch by including Core-Set selection (Sener and Savarese, 2018) in the set of active learning methods we evaluate.",
      "startOffset" : 93,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data (Houlsby et al., 2011; Gal and Ghahramani, 2016; Gal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data (Houlsby et al., 2011; Gal and Ghahramani, 2016; Gal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data (Houlsby et al., 2011; Gal and Ghahramani, 2016; Gal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 168
    }, {
      "referenceID" : 24,
      "context" : "After concluding that negative results are consistent across all experimental conditions, we investigate active learning’s ineffectiveness on VQA as a data problem and identify the existence of collective outliers (Han and Kamber, 2000) as the source of the problem.",
      "startOffset" : 214,
      "endOffset" : 236
    }, {
      "referenceID" : 63,
      "context" : "Leveraging recent advances in model interpretability, we build Dataset Maps (Swayamdipta et al., 2020), which distinguish between collective outliers and useful data that improve validation set performance (see Figure 1).",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "For instance, VQA-2 (Goyal et al., 2017) is riddled with collections of hard questions that require external knowledge to answer (e.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "Similarly, GQA (Hudson and Manning, 2019) asks underspecified questions (e.",
      "startOffset" : 15,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : "Active learning strategies have been successfully applied to image recognition (Joshi et al., 2009; Sener and Savarese, 2018), information extraction (Scheffer et al.",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 58,
      "context" : "Active learning strategies have been successfully applied to image recognition (Joshi et al., 2009; Sener and Savarese, 2018), information extraction (Scheffer et al.",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 56,
      "context" : ", 2009; Sener and Savarese, 2018), information extraction (Scheffer et al., 2001; Finn and Kushmerick, 2003; Jones et al., 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : ", 2009; Sener and Savarese, 2018), information extraction (Scheffer et al., 2001; Finn and Kushmerick, 2003; Jones et al., 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 31,
      "context" : ", 2009; Sener and Savarese, 2018), information extraction (Scheffer et al., 2001; Finn and Kushmerick, 2003; Jones et al., 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : ", 2009; Sener and Savarese, 2018), information extraction (Scheffer et al., 2001; Finn and Kushmerick, 2003; Jones et al., 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al.",
      "startOffset" : 58,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : ", 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al., 2005; Shen et al., 2017), semantic parsing (Dong et al.",
      "startOffset" : 62,
      "endOffset" : 102
    }, {
      "referenceID" : 60,
      "context" : ", 2003; Culotta and McCallum, 2005), named entity recognition (Hachey et al., 2005; Shen et al., 2017), semantic parsing (Dong et al.",
      "startOffset" : 62,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : ", 2017), semantic parsing (Dong et al., 2018), and text categorization (Lewis and Gale, 1994; Hoi et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 45,
      "context" : "However, these same methods struggle to outperform a random baseline when applied to the task of VQA (Lin and Parikh, 2017; Jedoui et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 29,
      "context" : "However, these same methods struggle to outperform a random baseline when applied to the task of VQA (Lin and Parikh, 2017; Jedoui et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty (Abramson and Freund, 2004; Collins et al., 2008; Joshi et al., 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 11,
      "context" : "To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty (Abramson and Freund, 2004; Collins et al., 2008; Joshi et al., 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 32,
      "context" : "To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty (Abramson and Freund, 2004; Collins et al., 2008; Joshi et al., 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : ", 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby",
      "startOffset" : 30,
      "endOffset" : 79
    }, {
      "referenceID" : 35,
      "context" : ", 2009), Bayesian uncertainty (Gal and Ghahramani, 2016; Kendall and Gal, 2017), disagreement (Houlsby",
      "startOffset" : 30,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 48,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 54,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 30,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 21,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 37,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 62,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 28,
      "context" : "Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015; Malinowski et al., 2015; Ren et al., 2015a; Johnson et al., 2017; Goyal et al., 2017; Krishna et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019) and models (Zhou et al.",
      "startOffset" : 134,
      "endOffset" : 309
    }, {
      "referenceID" : 76,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 47,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 75,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 77,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 73,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 64,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 9,
      "context" : ", 2019; Hudson and Manning, 2019) and models (Zhou et al., 2015; Fukui et al., 2016; Lu et al., 2016; Yang et al., 2016; Zhu et al., 2016; Wu et al., 2016; Anderson et al., 2018a; Tan and Bansal, 2019; Chen et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 220
    }, {
      "referenceID" : 22,
      "context" : "Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al.",
      "startOffset" : 134,
      "endOffset" : 185
    }, {
      "referenceID" : 43,
      "context" : "Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al.",
      "startOffset" : 134,
      "endOffset" : 185
    }, {
      "referenceID" : 69,
      "context" : ", 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al., 2012; Bolukbasi et al., 2016), both of which negatively impact sample efficiency.",
      "startOffset" : 44,
      "endOffset" : 115
    }, {
      "referenceID" : 36,
      "context" : ", 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al., 2012; Bolukbasi et al., 2016), both of which negatively impact sample efficiency.",
      "startOffset" : 44,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : ", 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al., 2012; Bolukbasi et al., 2016), both of which negatively impact sample efficiency.",
      "startOffset" : 44,
      "endOffset" : 115
    }, {
      "referenceID" : 38,
      "context" : "Prior work has used training dynamics to find examples which are frequently forgotten (Krymolowski, 2002; Toneva et al., 2019) versus those that are easy to learn (Bras et al.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 67,
      "context" : "Prior work has used training dynamics to find examples which are frequently forgotten (Krymolowski, 2002; Toneva et al., 2019) versus those that are easy to learn (Bras et al.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : ", 2019) versus those that are easy to learn (Bras et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "This work suggests using two model-specific measures – confidence and prediction variance – as indicators of a training example’s “learnability” (Chang et al., 2017; Swayamdipta et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 191
    }, {
      "referenceID" : 63,
      "context" : "This work suggests using two model-specific measures – confidence and prediction variance – as indicators of a training example’s “learnability” (Chang et al., 2017; Swayamdipta et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 191
    }, {
      "referenceID" : 63,
      "context" : "Dataset Maps (Swayamdipta et al., 2020), a recently introduced framework uses these two measures to profile datasets to find learnable examples.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 42,
      "context" : "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994; Settles, 2009; Gal et al., 2017; Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (xi, yi) ∈ Dseed used to initialize M, an unlabeled pool of data Dpool, and an acquisition function A(x,M).",
      "startOffset" : 71,
      "endOffset" : 148
    }, {
      "referenceID" : 59,
      "context" : "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994; Settles, 2009; Gal et al., 2017; Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (xi, yi) ∈ Dseed used to initialize M, an unlabeled pool of data Dpool, and an acquisition function A(x,M).",
      "startOffset" : 71,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994; Settles, 2009; Gal et al., 2017; Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (xi, yi) ∈ Dseed used to initialize M, an unlabeled pool of data Dpool, and an acquisition function A(x,M).",
      "startOffset" : 71,
      "endOffset" : 148
    }, {
      "referenceID" : 45,
      "context" : "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994; Settles, 2009; Gal et al., 2017; Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (xi, yi) ∈ Dseed used to initialize M, an unlabeled pool of data Dpool, and an acquisition function A(x,M).",
      "startOffset" : 71,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "We follow prior work to simulate an oracle using existing datasets, forming Dseed from a fixed percentage of the full dataset, and using the remainder as Dpool (Gal et al., 2017; Lin and Parikh, 2017; Siddhant and Lipton, 2018).",
      "startOffset" : 160,
      "endOffset" : 227
    }, {
      "referenceID" : 45,
      "context" : "We follow prior work to simulate an oracle using existing datasets, forming Dseed from a fixed percentage of the full dataset, and using the remainder as Dpool (Gal et al., 2017; Lin and Parikh, 2017; Siddhant and Lipton, 2018).",
      "startOffset" : 160,
      "endOffset" : 227
    }, {
      "referenceID" : 61,
      "context" : "We follow prior work to simulate an oracle using existing datasets, forming Dseed from a fixed percentage of the full dataset, and using the remainder as Dpool (Gal et al., 2017; Lin and Parikh, 2017; Siddhant and Lipton, 2018).",
      "startOffset" : 160,
      "endOffset" : 227
    }, {
      "referenceID" : 45,
      "context" : "Prior work has noted the impact of seed set size on active learning performance (Lin and Parikh, 2017; Misra et al., 2018; Jedoui et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 143
    }, {
      "referenceID" : 50,
      "context" : "Prior work has noted the impact of seed set size on active learning performance (Lin and Parikh, 2017; Misra et al., 2018; Jedoui et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "Prior work has noted the impact of seed set size on active learning performance (Lin and Parikh, 2017; Misra et al., 2018; Jedoui et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 143
    }, {
      "referenceID" : 25,
      "context" : "For image features we use grid-based features from ResNet-101 (He et al., 2016), or object-based features from Faster R-CNN (Ren et al.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 55,
      "context" : ", 2016), or object-based features from Faster R-CNN (Ren et al., 2015b) finetuned on Visual Genome (Anderson et al.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : ", 2015b) finetuned on Visual Genome (Anderson et al., 2018a).",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 53,
      "context" : "LogReg is a logistic regression model that uses either ResNet-101 or Faster R-CNN image features with mean-pooled GloVe question embeddings (Pennington et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 165
    }, {
      "referenceID" : 62,
      "context" : "7268 are not as performant as the subsequent models, logistic regression has been effective on VQA (Suhr et al., 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007; Yang and Loog, 2018; Mussmann and Liang, 2018).",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 57,
      "context" : ", 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007; Yang and Loog, 2018; Mussmann and Liang, 2018).",
      "startOffset" : 60,
      "endOffset" : 131
    }, {
      "referenceID" : 74,
      "context" : ", 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007; Yang and Loog, 2018; Mussmann and Liang, 2018).",
      "startOffset" : 60,
      "endOffset" : 131
    }, {
      "referenceID" : 51,
      "context" : ", 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007; Yang and Loog, 2018; Mussmann and Liang, 2018).",
      "startOffset" : 60,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "LSTM-CNN is a standard model introduced with VQA-1 (Agrawal et al., 2015).",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "BUTD (Bottom-Up Top-Down Attention) uses object-based features in tandem with attention over objects (Anderson et al., 2018a).",
      "startOffset" : 101,
      "endOffset" : 125
    }, {
      "referenceID" : 66,
      "context" : "BUTD won the 2017 VQA Challenge (Teney et al., 2018), and has been a consistent baseline for recent work in VQA.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "LXMERT is a large multi-modal transformer model that uses BUTD’s object features and contextualized BERT (Devlin et al., 2019) language features (Tan and Bansal, 2019).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 44,
      "context" : "LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.",
      "startOffset" : 123,
      "endOffset" : 228
    }, {
      "referenceID" : 37,
      "context" : "LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.",
      "startOffset" : 123,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.",
      "startOffset" : 123,
      "endOffset" : 228
    }, {
      "referenceID" : 62,
      "context" : "LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.",
      "startOffset" : 123,
      "endOffset" : 228
    }, {
      "referenceID" : 28,
      "context" : "LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014; Krishna et al., 2017; Goyal et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning.",
      "startOffset" : 123,
      "endOffset" : 228
    }, {
      "referenceID" : 59,
      "context" : "Several active learning methods have been developed to account for different aspects of the machine learning training pipeline: while some acquire examples with high aleotoric uncertainty (Settles, 2009) (having to do with the natural uncertainty in the data) or epistemic uncertainty (Gal et al.",
      "startOffset" : 188,
      "endOffset" : 203
    }, {
      "referenceID" : 19,
      "context" : "Several active learning methods have been developed to account for different aspects of the machine learning training pipeline: while some acquire examples with high aleotoric uncertainty (Settles, 2009) (having to do with the natural uncertainty in the data) or epistemic uncertainty (Gal et al., 2017) (having to do with the uncertainty in the modeling/learning process), others attempt to acquire examples that reflect the distribution of data in the pool (Sener and Savarese, 2018).",
      "startOffset" : 285,
      "endOffset" : 303
    }, {
      "referenceID" : 58,
      "context" : ", 2017) (having to do with the uncertainty in the modeling/learning process), others attempt to acquire examples that reflect the distribution of data in the pool (Sener and Savarese, 2018).",
      "startOffset" : 163,
      "endOffset" : 189
    }, {
      "referenceID" : 59,
      "context" : "Least Confidence acquires examples with lowest model prediction probability (Settles, 2009).",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 59,
      "context" : "Entropy acquires examples with the highest entropy in the model’s output (Settles, 2009).",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "MC-Dropout Entropy (Monte-Carlo Dropout with Entropy acquisition) acquires examples with high entropy in the model’s output averaged over multiple passes through a neural network with different dropout masks (Gal and Ghahramani, 2016).",
      "startOffset" : 208,
      "endOffset" : 234
    }, {
      "referenceID" : 27,
      "context" : "BALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy (Houlsby et al., 2011; Gal et al., 2017; Siddhant and Lipton, 2018) – capturing “disagreement” across different dropout masks.",
      "startOffset" : 203,
      "endOffset" : 270
    }, {
      "referenceID" : 19,
      "context" : "BALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy (Houlsby et al., 2011; Gal et al., 2017; Siddhant and Lipton, 2018) – capturing “disagreement” across different dropout masks.",
      "startOffset" : 203,
      "endOffset" : 270
    }, {
      "referenceID" : 61,
      "context" : "BALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy (Houlsby et al., 2011; Gal et al., 2017; Siddhant and Lipton, 2018) – capturing “disagreement” across different dropout masks.",
      "startOffset" : 203,
      "endOffset" : 270
    }, {
      "referenceID" : 58,
      "context" : "Core-Set Selection samples examples that capture the diversity of the data pool (Sener and Savarese, 2018; Coleman et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Core-Set Selection samples examples that capture the diversity of the data pool (Sener and Savarese, 2018; Coleman et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 128
    }, {
      "referenceID" : 45,
      "context" : "While methods are relatively better when using a larger seed set—confirming results from (Lin and Parikh, 2017)—no methods outperform random.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "One complexity of VQA is the size of the output space and the number of examples present (Agrawal et al., 2015; Goyal et al., 2017); VQA-2 has 400k training examples, and in excess of 3k possible answers (see Table 1).",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "One complexity of VQA is the size of the output space and the number of examples present (Agrawal et al., 2015; Goyal et al., 2017); VQA-2 has 400k training examples, and in excess of 3k possible answers (see Table 1).",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "However, prior work in active learning focuses on smaller datasets like the 10-class MNIST dataset (Gal et al., 2017), binary classification (Siddhant and Lipton, 2018), or small-cardinality (≤ 20 classes) text categorization (Lowell et al.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 61,
      "context" : ", 2017), binary classification (Siddhant and Lipton, 2018), or small-cardinality (≤ 20 classes) text categorization (Lowell et al.",
      "startOffset" : 31,
      "endOffset" : 58
    }, {
      "referenceID" : 46,
      "context" : ", 2017), binary classification (Siddhant and Lipton, 2018), or small-cardinality (≤ 20 classes) text categorization (Lowell et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "VQA-2 is the canonical dataset for evaluating VQA models (Goyal et al., 2017).",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "In keeping with prior work (Anderson et al., 2018a; Tan and Bansal, 2019), we filter the training set to only include answers that appear at least 9 times, resulting in 3130 unique answers.",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 64,
      "context" : "In keeping with prior work (Anderson et al., 2018a; Tan and Bansal, 2019), we filter the training set to only include answers that appear at least 9 times, resulting in 3130 unique answers.",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 45,
      "context" : "Unlike traditional VQA-2 evaluation, which treats the task as a multi-label binary classification problem, we follow prior active learning work on VQA (Lin and Parikh, 2017), which formulates it as a multi-class classification problem, enabling the use of acquisition functions such as uncertainty sampling and BALD.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 28,
      "context" : "GQA was introduced as a means for evaluating compositional reasoning (Hudson and Manning, 2019).",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 63,
      "context" : "Working from this, we characterize the underlying datasets using Dataset Maps (Swayamdipta et al., 2020) and discover that active learning methods prefer sampling “hard-tolearn” examples, leading to poor performance.",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 63,
      "context" : "A Dataset Map (Swayamdipta et al., 2020) is a model-specific graph for profiling the learnability of individual training examples.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 63,
      "context" : "While prior work attributes examples in this quadrant to “labeling errors” (Swayamdipta et al., 2020), labeling errors in VQA are sparse, and cannot account for the density of such examples in these maps.",
      "startOffset" : 75,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "Collective outliers are groups of examples that deviate from the rest of the examples but cluster together (Han and Kamber, 2000) – they often present as fundamental subproblems of a broader task.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "This paper asks a simple question – why does the modern neural active learning toolkit fail when applied to complex, open ended tasks? While we focus on VQA, collective outliers are abundant in tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and opendomain question answering (Kwiatkowski et al.",
      "startOffset" : 235,
      "endOffset" : 279
    }, {
      "referenceID" : 70,
      "context" : "This paper asks a simple question – why does the modern neural active learning toolkit fail when applied to complex, open ended tasks? While we focus on VQA, collective outliers are abundant in tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and opendomain question answering (Kwiatkowski et al.",
      "startOffset" : 235,
      "endOffset" : 279
    }, {
      "referenceID" : 71,
      "context" : "Historical artificial intelligence systems, such as SHRDLU (Winograd, 1972) and QUALM (Lehnert, 1977), were designed to flag input sequences that they were not designed to parse.",
      "startOffset" : 59,
      "endOffset" : 75
    }, {
      "referenceID" : 40,
      "context" : "Historical artificial intelligence systems, such as SHRDLU (Winograd, 1972) and QUALM (Lehnert, 1977), were designed to flag input sequences that they were not designed to parse.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 33,
      "context" : "Ideas from those methods can and should be resurrected using modern techniques; for example, recent work suggests that a simple classifier can be trained to identify out-ofdomain data inputs, provided a seed out-of-domain dataset (Kamath et al., 2020).",
      "startOffset" : 230,
      "endOffset" : 251
    }, {
      "referenceID" : 34,
      "context" : "Other work learns to identify novel utterances by learning to intelligently set thresholds in representation space (Karamcheti et al., 2020), a powerful idea especially if combined with other representation-centric active learning methods like Core-Set Sampling (Sener and Savarese, 2018).",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 58,
      "context" : ", 2020), a powerful idea especially if combined with other representation-centric active learning methods like Core-Set Sampling (Sener and Savarese, 2018).",
      "startOffset" : 129,
      "endOffset" : 155
    } ],
    "year" : 2021,
    "abstractText" : "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.",
    "creator" : "LaTeX with hyperref"
  }
}