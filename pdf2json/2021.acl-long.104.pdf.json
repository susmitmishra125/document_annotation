{
  "name" : "2021.acl-long.104.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Diverse Pretrained Context Encodings Improve Document Translation",
    "authors" : [ "Domenic Donato", "Lei Yu", "Chris Dyer" ],
    "emails" : [ "domenicd@deepmind.com", "leiyu@deepmind.com", "cdyer@deepmind.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1299–1311\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1299"
    }, {
      "heading" : "1 Introduction",
      "text" : "Generating an adequate translation for a sentence often requires understanding the context in which the sentence occurs (and in which its translation will occur). Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple\nsentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al., 2020b).\nWhile noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018). This two-stage training approach provides an inductive bias that encourages the learner to explain translation decisions preferentially in terms of the current sentence being translated, but these can be modulated at the margins by using document context. However, a weakness of this approach is that the conditional dependence of a translation on its surrounding context given the source sentence is weak, and learning good context representations purely on the basis of scarce parallel document data is challenging.\nA recent strategy for making better use of document context in translation is to use pretrained BERT representations of the context, rather than learning them from scratch (Zhu et al., 2020). Our key architectural innovation in this paper is an architecture for two-staged training that enables jointly conditioning on multiple context types, including both the source and target language context. Practically, we can construct a weak context representation from a variety of different contextual signals, and these are merged with the source sentence encoder’s representation at each layer in the transformer. To examine the potential of this architec-\nture, we explore two high-level research questions. First, using source language context, we explore the relative impact of different kinds of pretraining objectives on the performance obtained (BERT and PEGASUS), the amount of parallel document training data required, and the size of surrounding context. Second, recognizing that maintaining consistency in translation would seem to benefit from larger contexts in the target language, we compare the impact of source language context, target language context, and context containing both.\nOur main findings are (1) that multiple kinds of source language context improves performance of document translation over existing contextual representations, especially those that do not use pretrained context representations; (2) that although fine-tuning using pretrained contextual representations improves performance, large performance is strongly determined by the availability of contextual parallel data; and (3) that while both source and target language context provide benefit, source language context is more valuable, unless the quality of the target language context translations is extremely high."
    }, {
      "heading" : "2 Model Description",
      "text" : "Our architecture is designed to incorporate multiple sources of external embeddings into a pretrained sequence-to-sequence transformer model. We execute this by creating a new attention block for each embedding we wish to incorporate and stack them. We then insert this attention stack as a branching path in each layer of the encoder and decoder. The outputs of the new and original paths are averaged before being passed to the feed forward block at the end of the layer. Details are discussed below (§2.4), and the architecture is shown in Figure 1.\nThe model design follows the adapter pattern (Gamma et al., 1995). The interface between the external model and translation model takes the form of an attention block which learns to perform the adaptation. The independence between the models means that different input data can be provided to each, which enables extra information during the translation process. In this work, we leverage this technique to: (1) enhance a sentence-level model with additional source embeddings; (2) convert a sentence-level model to a document-level model by providing contextual embeddings. Like BERTfused (Zhu et al., 2020), we use pretrained masked language models to generate the external embed-\ndings."
    }, {
      "heading" : "2.1 Pre-Trained Models",
      "text" : "We use two kinds of pretrained models: BERT (Devlin et al., 2019) and PEGASUS (Zhang et al., 2020). Although similar in architecture, we conjecture that these models will capture different signals on account of their different training objectives.\nBERT is trained with a masked word objective and a two sentence similarity classification task. During training, it is provided with two sentences that may or may not be adjacent, with some of their words masked or corrupted. BERT predicts the correct words and determining if the two sentences form a contiguous sequence. Intuitively, BERT provides rich word-in-context embeddings. In terms of machine translation, it’s reasonable to postulate that BERT would provide superior representations of the source sentence and reasonable near sentence context modulation. On the other hand, we expect it to fail to provide contextual conditioning when the pair of sentences are not adjacent. This shortcoming is where PEGASUS comes in.\nPEGASUS is trained with a masked sentence objective. During training, it is given a document that has had random sentences replaced by a mask token. Its task is to decode the masked sentences in the same order they appear in the document. As a result, PEGASUS excels at summarization tasks, which require taking many sentences and compressing them into a representation from which another sentence can be generated. In terms of providing context for document translation, we conjecture that PEGASUS will be able to discover signals across longer ranges that modulate output."
    }, {
      "heading" : "2.2 Embedding Notation",
      "text" : "To keep track of the type of embeddings being incorporated in a particular configuration, we use the notational convention ModelSide(Inputs).\n• Model: B for BERT, P for PEGASUS, and D for Document Transformer (Zhang et al., 2018). • Side: s for the source and t for the target lan-\nguage. • Inputs: c for the current source (or target), i.e., xi, p for the previous source (target), and n for the next one. Note that 3p means the three previous sources (targets), (xi−3,xi−2,xi−1). • When multiple embeddings are used, we include\na⇒ to indicate the order of attention operations.\nWe can thus represent the BERT-fused document model proposed by Zhu et al. (2020) as Bs(p,c) since it passes the previous and current source sentences as input to BERT."
    }, {
      "heading" : "2.3 Enhanced Models",
      "text" : "The core of this work is to understand the benefits that adding a diverse set of external embeddings has on the quality of document translation. To this effect, we introduce two new models that leverage the output from both BERT and PEGASUS:\nMulti-source := Bs(c)⇒ Ps(c) Multi-context := Bs(p,c)⇒ Bs(c,n)⇒ Ps(3p,c,3n)\nThere are a few ways to integrate the output of external models into a transformer layer. We could stack them vertically after the self-attention block (Zhang et al., 2018) or we could place them horizontally and average all of their outputs together like MAT (Fan et al., 2020). Our preliminary experiments show that the parallel attention stack, depicted in Figure 1, works best. Therefore, we adopt this architecture in our experiments."
    }, {
      "heading" : "2.4 Parallel Attention Stack",
      "text" : "If we let A = Bs(p,c), B = Bs(c,n), and C = Ps(3p,c,3n) refer to the output of the external pretrained models computed once per translation example, then the Multi-context encoder layer is defined as\nR` = AttnBlock(E`−1, E`−1, E`−1) Sa` = AttnBlock(A, A, E`−1) Sb` = AttnBlock(B, B, S a ` ) S` = AttnBlock(C, C, Sb`)\nT` = { DropBranch(R`, S`) training 1 2 · (R` + S`) otherwise E` = LayerNorm(FeedForward(T`)) +T`\nThe intermediate outputs of the attention stack are Sa` ⇒ Sb` ⇒ S`. To reproduce BERT-fused, we remove Sa` and S b ` from the stack and set S` directly to AttnBlock(A, A, E`−1). We use attention block to refer to the attention, layer normalization, and residual operations,\nAttnBlock(K,V,Q) =\nLayerNorm(Attn(K,V,Q)) +Q\nWhile drop-branch (Fan et al., 2020) is defined as\nDropBranch(M,N) =\n1(u ≥ .5) ·M+ 1(u < .5) ·N\nwhere u ∼ Uniform(0, 1) and 1 is the indicator function."
    }, {
      "heading" : "3 Experiment Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate our model on three translation tasks, the NIST Open MT Chinese–English task,1 the IWSLT’14 English-German translation task,2 and the WMT’14 English-German news translation task.3 Table 1 provides a breakdown of the type, quantity, and relevance of the data used in the various dataset treatments. NIST provides the largest amount of in domain contextualized sentence pairs. IWSLT’14 and WMT’14 are almost an order of magnitude smaller. See Appendix A for preprocessing details.\nNIST Chinese–English is comprised of LDC distributed news articles and broadcast transcripts. We use the MT06 dataset as validation set and MT03, MT04, MT05, and MT08 as test sets. The validation set contains 1,649 sentences and the test set 5,146 sentences. Chinese sentences are frequently underspecified with respect to grammatical features that are obligatory in English (e.g., number for nouns, tense on verbs, and dropped arguments), making it a common language pair to study for document translation.\nIWSLT’14 English–German is a corpus of translated TED and TEDx talks. Following prior work (Zhu et al., 2020), we use the combination of dev2010, dev2012, tst2010, tst2011, and tst2012 as the test set which contains 6,750 sentences. We randomly selected 10 documents from the training data for validation. We perform a data augmentation experiment with this dataset by additionally including news commentary v15. We denote this treatment as IWSLT+ and consider this to be out of domain data augmentation.\n1https://www.nist.gov/itl/iad/mig/ open-machine-translation-evaluation\n2https://sites.google.com/site/ iwsltevaluation2014/mt-track\n3http://statmt.org/wmt14/ translation-task.html\nWMT’14 English–German is a collection of web data, news commentary, and news articles. We use newstest2013 for validation and newstest2014 as the test set. For the document data, we use the original WMT’14 news commentary v9 dataset. We run two document augmentation experiments on this dataset. The first, denoted as WMT+, replaces news commentary v9 with the newer news commentary v15 dataset. The second augmentation experiment, denoted as WMT++, builds on the first by additionally incorporating the Tilde Rapid 2019 corpus. The Rapid corpus is comprised of European Commission press releases and the language style is quite different from the style used in the News Commentary data. For this reason, we consider Rapid to be out of domain data for this task."
    }, {
      "heading" : "3.2 Training",
      "text" : "We construct enhanced models with additional attention blocks and restore all previously trained parameters. We randomly initialize the newly added parameters and exclusively update these during training. For a given dataset, we train a model on all the training data it is compatible with. This means that for document-level models, only document data is used, while for sentence-level models both document and sentence data is used. In our work, this distinction only matters for the WMT’14 dataset where there is a large disparity between the two types of data.\nTransformer models are trained on sentence pair data to convergence. For NIST and IWSLT’14 we use transformer base while for WMT’14 we use transformer big. We use the following vari-\nants of BERT from Google Research GitHub:4 BERT-Base Chinese on NIST, BERT-Base Uncased on IWSLT’14, and BERT-Large Uncased (Whole Word Masking) on WMT’14. We pretrain three PEGASUS base models for the languages en, de, and zh using the Multilingual C4 dataset as detailed in TensorFlow’s dataset catalog.5 When training our models, we only mask a single sentence per training example and do not include a masked word auxiliary objective. We use the public PEGASUS large6 on the English side of WMT’14, for everything else, we use our models. See Appendix B for batch size and compute details."
    }, {
      "heading" : "3.3 Evaluation",
      "text" : "To reduce the variance of our results and help with reproducibility, we use checkpoint averaging. We select the ten contiguous checkpoints with the highest average validation BLEU. We do this at two critical points: (1) with the transformer models used to bootstrap enhanced models; (2) before calculating the validation and test BLEU scores we report. We use the sacreBLEU script (Post, 2018)7 on our denormalized output to calculate BLEU.\n4https://github.com/google-research/ bert\n5https://www.tensorflow.org/datasets/ catalog/c4#c4multilingual\n6https://github.com/google-research/ pegasus\n7https://github.com/mjpost/sacreBLEU"
    }, {
      "heading" : "4 Results",
      "text" : "In this section, we present our main results and explore the importance of each component in the multi-context model. Additionally, we investigate the performance impact of document-level parallel data scarcity, the value of source-side versus targetside context, and the importance of target context quality.\nTable 2 compares our Multi-source and Multicontext models to baselines of related prior work, transformer (Vaswani et al., 2017), document transformer (Zhang et al., 2018), and the BERT-fused model for machine translation (Zhu et al., 2020). We see that a multi-embedding model outperforms all the single embedding models in each of the datasets we try. However, the best multiembedding configuration varies by dataset. We find that incorporating target-side context does not improve performance beyond using source-side context alone. We will present our ablation studies in the subsequent sections to further shed light on the causes of this pattern of results. To preserve the value of test set, we report results on the validation set for these experiments."
    }, {
      "heading" : "4.1 Source Context vs. Target Context",
      "text" : "In some language pairs, the source language is underspecified with respect to the obligatory information that must be given in the target language. For example, in English every inflected verb must have tense and this is generally not overtly marked in Chinese. In these situations, being able to condition on prior translation decisions would be valuable. However, in practice, the target context is only available post translation, meaning there is a risk of cascading errors. In this section, we seek to answer two questions: (1) how does the quality of target context affect document-level translation; (2) whether incorporating high-quality target context into source only models adds additional value.\nTo answer the first question, we evaluate the target context model Pt(3p,3n) using various translations as context. Table 3 shows the BLEU scores achieved by the target context models on the validation set. The lowest quality context comes from using the output of the baseline transformer model to furnish the context (valid BLEU of 48.76); the middle level comes from a model that conditions on three views of source context (valid BLEU of 52.8) and the third is an oracle experiment that uses a human reference translation. We see that the\nBLEU score improves as the quality of the target context improves; however, the impact is still less than the Multi-context source model—even in the oracle case!\nNext, we explore whether leveraging both source and target context works better than only using source context. To control for the confounding factor of target context quality, we remove one of the references from the validation dataset and use it only as context. We believe this provides an upper bound on the effect of target context for two reasons: (1) it’s reasonable to assume that, at some point, machine translation will be capable of generating human quality translations; (2) even when this occurs, we will not have access to the style of a specific translator ahead of time. For these reasons, we calculate BLEU scores using only the three remaining references. We can see in Table 4 that adding human quality target context to Multicontext only produces a 0.14 BLEU improvement. This challenges the notion that target context can add more value than source context alone."
    }, {
      "heading" : "4.2 Context Ablation",
      "text" : "To assess the importance of the various embeddings incorporated in the Multi-context model, we perform an ablation study by adding one component at a time until we reach its full complexity. Table 5 shows the study results. We can see that much of the improvement comes from the stronger sentencelevel model produced by adding BERT’s encoding of the source sentence—a full 2.25 BLEU improvement. The benefit of providing contextual embeddings is more incremental, yet consistent. Adding the previous sentence gives us 0.44 BLEU, adding additional depth provides another .49, and including the next sentence adds .37. Finally, adding PEGASUS’ contextual embedding on top of all this results in a boost of .49. Holistically, we can assign 2.45 BLEU to source embedding enrichment and 1.59 to contextual representations."
    }, {
      "heading" : "4.3 Data Scarcity",
      "text" : "NIST is a high resource document dataset containing over 1.4M contextualized sentence pairs. In\nthis section, we investigate to what extent the quantities of parallel documents affect the performance of our models. To do so, we retrain enhanced models with subsets of the NIST training dataset. It is important to note that the underlying sentence transformer model was not retrained in these experiments meaning that these experiments simulate adding document context to a strong baseline as done in Lopes et al. (2020). Figure 2 shows the BLEU scores of different models on the NIST validation set with respect to the number of contextualized sentences used for training. We can see that it requires an example pool size over 300K before these models outperform the baseline. We conjecture that sufficient contextualized sentence pairs are crucial for document-level models to achieve good performance, which would explain why these models don’t perform well on the IWSLT’14 and WMT’14 datasets.\nFurther, this pattern of results helps shed light on the inconsistent findings in the literature regarding the effectiveness of document context models. A few works (Kim et al., 2019; Li et al., 2020; Lopes et al., 2020) have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning. We can now see from Figure 2 that these experiments were done in the low data regime. The randomly initialized context model needs around 600K\ntraining examples before it significantly outperform the baseline, while the pretrained contextual models reduce this to about 300K. It is important to note that none of the conextual models we tried outperformed the baseline below this point. This indicates that data quantity is not the only factor that matters but it is a prerequisite for the current class of document context architectures."
    }, {
      "heading" : "4.4 Document Data Augmentation",
      "text" : "We further validate our hypothesis about the importance of sufficient contextualized data by experimenting with document data augmentation, this time drawing data from different domains. We augment the IWSLT dataset with news commentary v15, an additional 345K document context sentence pairs, and repeat the IWSLT experiments. During training, we sample from the datasets such that each batch contains roughly 50% of the original IWSLT data. To ensure a fair comparison, we first finetune the baseline transformer model on the new data, which improves its performance by 1.61 BLEU. We use this stronger baseline as the foundation for the other models and show the results in Table 6. Although Multi-context edges ahead of Multi-source, the significance lies in the relative impact additional document data has on the two classes of models. The average improvement of the sentence-level models is 1.58 versus the 1.98 experienced by the document models. Huo et al. (2020) observed a similar phenomenon when using synthetic document augmentation. This further emphasizes the importance of using sufficient contextualized data when comparing the impact of various document-level architectures, even when the contextualized data is drawn from a new domain."
    }, {
      "heading" : "4.5 Three Stage Training",
      "text" : "WMT’14 offers an opportunity to combine the insights gained from the aforementioned experiments. This dataset provides large quantities of sentence pair data and a small amount of document pair data. Not surprisingly, both BERT-fused8 and Multi-context struggle in this environment. On the other hand, Multi-source benefits from the abundance of sentence pair data.\nIn order to make the most of the training data,\n8Here we mention that, while we were able to reproduce the baseline relative uplift of BERT-fused on the other datasets, we were unable to do so on the WMT’14 dataset. We do not know what document data they used and this probably accounts for the differences observed.\nImpact of Data Scarcity\nwe add a third stage to our training regime. As before, in stage one, we train the transformer model with the sentence pair data. In stage two, we train the Multi-source model also using the sentence pair data. In stage three, we add an additional Ps(3p,3n) attention block to the Multi-source model and train it with document data. We perform two document augmentation experiments. In the first, we replace news commentary v9 with v15. In the second, we train on a mix of news commentary v15 and Tilde Rapid 2019. The optimal mix was 70% and 30% respectably, which we found by tuning on the validation dataset. For each of the augmentation experiments, we created new Multi-source baselines by fine-tuning the original baseline on the new data.\nWhen training these new baselines we only updated the parameters in the Bs(c) and Ps(c) attention blocks. In contrast, when training the treatment models, we froze these blocks and only updated the parameters in the Ps(3p,3n) block. In this way, both the new baselines and treatments started from the same pretrained Multi-source model, were exposed to the same data, and had only the parameters under investigation updated.\nWe see in Table 7 that this method can be used to provide the document-level model with a much stronger sentence-level model to start from. As we saw in the previous data augmentation experiments (§4.4), document augmentation helps the documentlevel model more than the sentence-level model. It is interesting to note that out of domain document data helps the document-level model yet hurts the sentence-level model.9"
    }, {
      "heading" : "5 Related Work",
      "text" : "This work is closely related to two lines of research: document-level neural machine translation and representation learning via language modeling.\nEarlier work in document machine translation exploits the context by taking a concatenated string of adjacent source sentences as the input of neural sequence-to-sequence models (Tiedemann and\n9While tuning on the validation dataset, we observed that the optimal proportion of Rapid data to include for the new baseline was 0%. Meaning, don’t include any of the off domain data. However, we needed a fair comparison baseline so left it at 30% when making Table 7.\nScherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information (Zhang et al., 2018; Miculicich et al., 2018, inter alia). They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective.\nThere has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities.\nFinally, large-scale representation learning with language modeling has achieved success in im-\nproving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020). They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al., 2019, 2020; Dong et al., 2019), and repurposing unconditional language generation (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019). Our work is closely related to that from Zhu et al. (2020), where pretrained largescale language models are applied to documentlevel machine translation tasks. We advance this line of reasoning by designing an architecture that uses composition to incorporate multiple pretrained models at once. It also enables conditioning on different inputs to the same pretrained model, enabling us to circumvent BERT’s two sentence embedding limit."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have introduced an architecture and training regimen that enables incorporating representations from multiple pretrained masked language models into a transformer model. We show that this technique can be used to create a substantially stronger sentence-level model and, with sufficient document data, further upgraded to a document-level model that conditions on contextual information. Through ablations and other experiments, we establish document augmentation and multi-stage training as effective strategies for training a document-level model when faced with data scarcity. And that source side context is sufficient for these models, with target context adding little additional value."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank our teammates, Laurent Sartran, Phil Blunsom, Susie Young, Wang Ling, and Wojciech Stokowiec, for their feedback and shared engineering efforts. We thank Yao Zhao for helping us to better understand the PEGASUS codebase. We thank Dani Yogatama and our three anonymous reviewers for their feedback on the earlier draft of the paper. Their feedback was taken seriously and we believe this work has benefited from the items they requested."
    }, {
      "heading" : "A Preprocessing",
      "text" : "A.1 Text We perform text normalization on the datasets before tokenization.\n• All languages - Unicode canonicalization (NKFD from), replacement of common multiple encoding errors present in training data, standardization of quotation marks into “directional” variants.\n• English - Replace non-American spelling variants with American spellings using the aspell library.10 Punctuation was split from English words using a purpose-built library.\n• Chinese - Convert any traditional Chinese characters into simplified forms and segment into word-like units using the Jieba segmentation tool.11\n• English & German for WMT’14 - Lowercase first word of sentence unless it was in a whitelist of proper nouns and common abbreviations.\n• English & German for IWSLT’14 - Lowercase all words.\n• Chinese & English for NIST - Lowercase all words.\nA.2 Tokenization We encode text into sub-word units using the sentencepiece tool (Kudo and Richardson, 2018). When generating our own subword segmentation, we used the algorithm from Kudo (2018) with a minimum character coverage of 0.9995. Other than for BERT, we use TensorFlow SentencepieceTokenizer for tokenization given a sentencepiece model.\n• BERT (all) - Used vocabulary provided with download and TensorFlow BertTokenizer.\n• PEGASUS large & EN small - Used sentencepiece model provided with PEGASUS large download.\n• PEGASUS Zh small - Generated subword vocabulary of 34K tokens from the NIST dataset.\n10http://wordlist.aspell.net/ varcon-readme/\n11https://github.com/fxsjy/jieba\n• PEGASUS De small - Generated subword vocabulary of 34K tokens from the WMT’14 dataset.\n• Transformers - Generated joint subword vocabulary of 34K tokens for NIST & WMT’14 and 20K for IWSLT’14."
    }, {
      "heading" : "B Compute",
      "text" : "We train and evaluate on Google TPU v2. We use a 4x2 configuration which contains 16 processing units. We use the following global batch sizes during training (examples / tokens):\n• Transformer baselines: (1024 / 131,072)\n• WMT’14 Multi-source: (1024 / 131,072)\n• WMT’14 others: (128 / 16,384)\n• NIST: (256 / 32,767)\n• IWSLT’14: (256 / 32,767)\nUsing a global batch size of 32 and a beam width of 5, the following are the number of samples per second our models and key baselines managed during inference:\n• Transformer: 11.94\n• BERT-fused: 7.37\n• Multi-source: 5.45\n• Multi-context: 4.80"
    }, {
      "heading" : "C Qualitative Analysis",
      "text" : "We manually inspected the translations outputs from the Multi-source model and Multi-context model and have found that the Multi-context model indeed does better in terms maintaining the consistency of lexical usage across sentences. Unlike English, Chinese does not mark nouns for plural vs singular nor verbs for tense. Therefore, this needs to be inferred from context to generate accurate English translations. It is not possible for a sentence-level MT system to capture this information when the relevant context is not in the current sentence. Tables 8, 9, and 10 provide various examples where the sentence-level model cannot know this information and the document-level model is able to correctly condition on it.\nExample 1 Consistency of Tense\nSource: 金先生说,五十几岁时,王选便开始注意培养年轻人,他一直强调,要铺 路,要甘为人梯,给年轻人让路。\nReference: Mr. Jin said that Wang Xuan started to focus on mentoring young people when he was in his 50s. He constantly stressed that he wanted to pave the way for young people and that he wanted to be their stepping stone.\nExample 2 Consistency of Proper Noun\nExample 2 Consistency of Pronoun\nSource: 那是在十年前的一天，当这位老师正利用中午休息时间，在家里睡觉 时，突然间，电话铃响了，\nReference: On that day ten years ago, when this teacher was taking a nap at home during noontime break, the telephone rang suddenly."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "An embarrassingly simple approach for transfer learning from pretrained language models",
      "author" : [ "Alexandra Chronopoulou", "Christos Baziotis", "Alexandros Potamianos." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Chronopoulou et al\\.,? 2019",
      "shortCiteRegEx" : "Chronopoulou et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "CoRR, abs/1905.03197.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-trained language model representations for language generation",
      "author" : [ "Sergey Edunov", "Alexei Baevski", "Michael Auli." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Edunov et al\\.,? 2019",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Multibranch attentive transformer",
      "author" : [ "Yang Fan", "Shufang Xie", "Yingce Xia", "Lijun Wu", "Tao Qin", "Xiang-Yang Li", "Tie-Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "Fan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2020
    }, {
      "title" : "Design Patterns: Elements",
      "author" : [ "Erich Gamma", "Richard Helm", "Ralph Johnson", "John Vlissides" ],
      "venue" : null,
      "citeRegEx" : "Gamma et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Gamma et al\\.",
      "year" : 1995
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Gholamreza Haffari", "Sameen Maruf." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Haffari and Maruf.,? 2018",
      "shortCiteRegEx" : "Haffari and Maruf.",
      "year" : 2018
    }, {
      "title" : "Diving deep into context-aware neural machine translation",
      "author" : [ "Jingjing Huo", "Christian Herold", "Yingbo Gao", "Leonard Dahlmann", "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "Proceedings of WMT.",
      "citeRegEx" : "Huo et al\\.,? 2020",
      "shortCiteRegEx" : "Huo et al\\.",
      "year" : 2020
    }, {
      "title" : "Does neural machine translation benefit from larger context? CoRR, abs/1704.05135",
      "author" : [ "Sébastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "Microsoft translator at WMT 2019: Towards large-scale document-level neural machine translation",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of WMT.",
      "citeRegEx" : "Junczys.Dowmunt.,? 2019",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2019
    }, {
      "title" : "When and why is document-level context useful in neural machine translation",
      "author" : [ "Yunsu Kim", "Duc Thanh Tran", "Hermann Ney" ],
      "venue" : "In Proceedings of the Fourth Workshop on Discourse in Machine Translation",
      "citeRegEx" : "Kim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "CoRR, abs/1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Document-level neural MT: A systematic comparison",
      "author" : [ "António Lopes", "M. Amin Farajian", "Rachel Bawden", "Michael Zhang", "André F.T. Martins." ],
      "venue" : "Proceedings of the 22nd Annual Conference of the European Association for Machine Transla-",
      "citeRegEx" : "Lopes et al\\.,? 2020",
      "shortCiteRegEx" : "Lopes et al\\.",
      "year" : 2020
    }, {
      "title" : "Selective attention for contextaware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Learned in translation: Contextualized word vectors",
      "author" : [ "Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "McCann et al\\.,? 2017",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2017
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Repurposing decoder-transformer language models for abstractive summarization",
      "author" : [ "Luke de Oliveira", "Alfredo Láinez Rodrigo." ],
      "venue" : "ArXiv, abs/1909.00325.",
      "citeRegEx" : "Oliveira and Rodrigo.,? 2019",
      "shortCiteRegEx" : "Oliveira and Rodrigo.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of WMT.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "MASS: masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of DiscoMT@EMNLP.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "TACL, 6:407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-aware monolingual repair for neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of EMNLPIJCNLP.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov", "." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting cross-sentence context for neural machine translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "CoRR, abs/1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Better document-level machine translation with bayes’ rule",
      "author" : [ "Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:346–360.",
      "citeRegEx" : "Yu et al\\.,? 2020b",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretraining-based natural language generation for text summarization",
      "author" : [ "Haoyu Zhang", "Yeyun Gong", "Yu Yan", "Nan Duan", "Jianjun Xu", "Ji Wang", "Ming Gong", "Ming Zhou." ],
      "venue" : "CoRR, abs/1902.09243.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating BERT into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tie-Yan Liu." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Encoderagnostic adaptation for conditional language generation",
      "author" : [ "Zachary M. Ziegler", "Luke Melas-Kyriazi", "Sebastian Gehrmann", "Alexander M. Rush." ],
      "venue" : "CoRR, abs/1908.06938.",
      "citeRegEx" : "Ziegler et al\\.,? 2019",
      "shortCiteRegEx" : "Ziegler et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation.",
      "startOffset" : 79,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation.",
      "startOffset" : 79,
      "endOffset" : 143
    }, {
      "referenceID" : 38,
      "context" : "A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al.",
      "startOffset" : 131,
      "endOffset" : 176
    }, {
      "referenceID" : 22,
      "context" : "A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al.",
      "startOffset" : 131,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : ", 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 36,
      "context" : ", 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al., 2020b).",
      "startOffset" : 159,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018).",
      "startOffset" : 230,
      "endOffset" : 320
    }, {
      "referenceID" : 38,
      "context" : "One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018).",
      "startOffset" : 230,
      "endOffset" : 320
    }, {
      "referenceID" : 22,
      "context" : "One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018).",
      "startOffset" : 230,
      "endOffset" : 320
    }, {
      "referenceID" : 10,
      "context" : "One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018).",
      "startOffset" : 230,
      "endOffset" : 320
    }, {
      "referenceID" : 40,
      "context" : "A recent strategy for making better use of document context in translation is to use pretrained BERT representations of the context, rather than learning them from scratch (Zhu et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "The model design follows the adapter pattern (Gamma et al., 1995).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 40,
      "context" : "Like BERTfused (Zhu et al., 2020), we use pretrained masked language models to generate the external embeddings.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "We use two kinds of pretrained models: BERT (Devlin et al., 2019) and PEGASUS (Zhang et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 38,
      "context" : "• Model: B for BERT, P for PEGASUS, and D for Document Transformer (Zhang et al., 2018).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 38,
      "context" : "We could stack them vertically after the self-attention block (Zhang et al., 2018) or we could place them horizontally and average all of their outputs together like MAT (Fan et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", 2018) or we could place them horizontally and average all of their outputs together like MAT (Fan et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "LayerNorm(Attn(K,V,Q)) +Q While drop-branch (Fan et al., 2020) is defined as",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 40,
      "context" : "Following prior work (Zhu et al., 2020), we use the combination of dev2010, dev2012, tst2010, tst2011, and tst2012 as the test set which contains 6,750 sentences.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "We use the sacreBLEU script (Post, 2018)7 on our denormalized output to calculate BLEU.",
      "startOffset" : 28,
      "endOffset" : 40
    }, {
      "referenceID" : 31,
      "context" : "Table 2 compares our Multi-source and Multicontext models to baselines of related prior work, transformer (Vaswani et al., 2017), document transformer (Zhang et al.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 38,
      "context" : ", 2017), document transformer (Zhang et al., 2018), and the BERT-fused model for machine translation (Zhu et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : ", 2018), and the BERT-fused model for machine translation (Zhu et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "A few works (Kim et al., 2019; Li et al., 2020; Lopes et al., 2020) have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning.",
      "startOffset" : 12,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "A few works (Kim et al., 2019; Li et al., 2020; Lopes et al., 2020) have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning.",
      "startOffset" : 12,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "A few works (Kim et al., 2019; Li et al., 2020; Lopes et al., 2020) have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning.",
      "startOffset" : 12,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al.",
      "startOffset" : 69,
      "endOffset" : 135
    }, {
      "referenceID" : 38,
      "context" : "They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al.",
      "startOffset" : 69,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al.",
      "startOffset" : 69,
      "endOffset" : 135
    }, {
      "referenceID" : 30,
      "context" : ", 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 34,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 30,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 33,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 38,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : ", 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al.",
      "startOffset" : 62,
      "endOffset" : 182
    }, {
      "referenceID" : 27,
      "context" : "Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models.",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models.",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 4,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 26,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 21,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 35,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 3,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 17,
      "context" : "Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 375
    }, {
      "referenceID" : 28,
      "context" : "They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al.",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al.",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 40,
      "context" : "They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al.",
      "startOffset" : 102,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : ", 2020) and summarization (Zhang et al., 2019, 2020; Dong et al., 2019), and repurposing unconditional language generation (Ziegler et al.",
      "startOffset" : 26,
      "endOffset" : 71
    }, {
      "referenceID" : 41,
      "context" : ", 2019), and repurposing unconditional language generation (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019).",
      "startOffset" : 59,
      "endOffset" : 112
    } ],
    "year" : 2021,
    "abstractText" : "We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pretrained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese–English, and IWSLT and WMT English–German tasks support four general conclusions: that using pretrained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multicontext model consistently outperforms the best existing context-aware transformers.",
    "creator" : "LaTeX with hyperref"
  }
}