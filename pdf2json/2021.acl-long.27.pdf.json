{
  "name" : "2021.acl-long.27.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bridge-Based Active Domain Adaptation for Aspect Term Extraction",
    "authors" : [ "Zhuang Chen", "Tieyun Qian" ],
    "emails" : [ "qty}@whu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 317–327\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n317"
    }, {
      "heading" : "1 Introduction",
      "text" : "Aspect term extraction (ATE) is a fundamental task in aspect-based sentiment analysis. Given a review sentence “The pizza here is also absolutely delicious.”, ATE aims to extract the term pizza. Recent studies define ATE as a sequence tagging task and propose supervised taggers (Wang et al., 2017; Xu et al., 2018). However, due to the high cost of token-level annotation, the lack of labeled data becomes the main obstacle (Chen and Qian, 2019).\nTo alleviate the data deficiency issue, unsupervised domain adaptation is proposed to transfer knowledge from the labeled source domain to the unlabeled target domain. Since ATE is a tokenlevel task, it is natural to conduct token-level domain adaptation. Then a problem arises: many\n*Corresponding author.\naspect terms are domain-specific and cannot be transferred directly. We present the proportion of source aspect terms that also appear in target test data in Figure 1. As can be seen, in distant transfer pairs like R→L, only less than 10% of source aspect terms have appeared in target data. Even in a close pair L→D, the proportion is no more than 40%. In other words, there is a wide discrepancy between the data from different domains, and many aspect terms have to be transferred under the guidance of proper references.\nTo solve this problem, previous studies try to associate aspect terms with specific pivot words1. We name these methods passive domain adaptation because the transfer of aspect terms is dependent on their links to the pivots. There are two types of methods along this line. (1) Opinion terms as pivots. Since aspect and opinion terms usually appear in pairs, it is straightforward to extract aspect terms with the indication from opinion terms. Early studies (Li et al., 2012; Ding et al., 2017) use common opinion seeds (e.g., good, fancy) and pre-defined rules (e.g., good→amod→NN) to extract aspect terms across domains. However, it is hard to collect a complete set of seeds or define high-quality rules, and thus these methods often produce inferior performance. Several studies (Wang and Pan, 2018, 2019b) manually annotate all opinion terms in reviews and design neural models to capture aspectopinion relations via multi-task learning. While\n1Pivot words are words which behave in the same way for discriminative learning in both domains (Blitzer et al., 2006).\ngetting improvements, these methods induce additional annotation costs. (2) Context terms as pivots. Since pre-trained language models (PLMs) like BERT represent words w.r.t their contexts, recent studies (Xu et al., 2019; Gong et al., 2020) leverage PLMs to transfer aspect terms with common context terms2. However, not all context terms qualify as pivots (e.g., eat). In addition, PLMs like BERT build word associations mainly based on semantic similarity in co-occurring contexts. For an aspect term like pizza, BERT tends to link it to hamburger via a flow like pizza→eat→hamburger. Consequently, it is hard for these methods to identify keyboard in the target domain based on the labeled term pizza in the source domain.\nIn this paper, we propose a novel active domain adaptation method. Concretely, we construct two types of bridges for all words, which can help transfer aspect terms across domains. An example in Figure 2 shows how to identify the unseen target term keyboard based on the source term pizza. (1) The syntactic bridge aims to recognize transferable syntactic roles for the words across domains. Though pizza and keyboard have almost no semantic relatedness, they often play a similar role in parse trees. In view of this, we treat the involved syntactic roles (including POS tag and dependency relations) of a certain word as its syntactic bridge. Previous studies also utilize dependency information. However, we differ our method from existing ones in that we do not use dependency relations to associate pivot words with aspect terms. Instead, we treat syntactic roles themselves as pivot features and do not need any manually annotated pivot words. (2) The semantic bridge moves one step further by retrieving transferable prototypes. Intuitively, if we correlate pizza with some prototype target terms like {disk, OS, mouse}, the domain discrepancy between the training and testing reviews can be largely reduced. Hence we regard the proto-\n2Context terms denote all words that are not aspect terms. Hence opinion terms form a subset of context terms.\ntypes of a certain word as its semantic bridge and design a syntax-enhanced similarity metric to retrieve them. Compared with previous opinion and context term-based methods, building a semantic bridge directly links aspect terms across domains and only requires unlabeled source and target data.\nBased on the syntactic/semantic bridges, we then develop an end-to-end tagger to fuse reviews with these transferable bridges. We conduct extensive experiments on three datasets. The results show that our method achieves a new state-of-the-art performance with a low computational cost."
    }, {
      "heading" : "2 Related Work",
      "text" : "Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wu et al., 2009; Qiu et al., 2011) and hand-crafted features (Li et al., 2010; Liu et al., 2012, 2013; Chen et al., 2014). With the development of deep learning, supervised sequence taggers have become the mainstream due to their promising performance (Liu et al., 2015; Wang et al., 2016, 2017; Xu et al., 2018; Ma et al., 2019; Chen and Qian, 2020a). More recently, there emerge many studies that interact ATE with other tasks like aspect-level sentiment classification (Wang et al., 2018; He et al., 2019; Chen and Qian, 2020b). Since these methods highly depend on abundant domain-specific training data, they can hardly scale across the domains where labeled data is absent. Hence it would be more practical to develop unsupervised domain adaptation methods for ATE.\nDomain Adaptation Many domain adaptation methods have been proposed to solve coarsegrained tasks like text classification (Blitzer et al., 2006; Ganin and Lempitsky, 2015; Guo et al., 2020). The basic idea in coarse-grained tasks is to transfer pivot words, which does not fit ATE well since most aspect terms are domain-specific nonpivot words. There have been a few attempts to this problem, which fall into two lines. (1) One is to model aspect-opinion relations. Early researches use common opinion seeds and pre-defined dependency link rules to build manual features (Jakob and Gurevych, 2010), conduct bootstrapping (Li et al., 2012), and create pseudo target labels (Ding et al., 2017). Due to the incompleteness of seeds and the inflexibility of rules, they often produce inferior performance. Subsequent studies (Wang and Pan, 2018, 2019a,b; Li et al., 2019) manually\nannotate all opinion terms in reviews and design trainable neural models to capture the relations via multi-task learning. However, they induce extra annotation costs. (2) The other aims to find aspectcontext relations. Xu et al. (2019) post-trains BERT on the cross-domain corpus to enhance its domain adaptation ability. Gong et al. (2020) and Pereg et al. (2020) further incorporate external syntactic information into BERT with auxiliary tasks or modified attention mechanisms, but they still rely on the prior knowledge in BERT. These methods often have more than 100M parameters and involve lots of computing power. Unlike all the aforementioned methods, we do not associate aspect terms with pivot words but actively transfer them via bridges."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we first introduce the cross-domain ATE task. We then illustrate how to construct syntactic and semantic bridges. Lastly, we present the bridge-based sequence tagging."
    }, {
      "heading" : "3.1 Problem Statement",
      "text" : "Given a review x = {x1, ..., xn}, we formulate ATE as a sequence tagging task that aims to predict a tag sequence y = {y1, ..., yn}, where each yi ∈ {B, I,O} denotes the beginning of, inside of, and outside of an aspect term. In this paper, we focus on the unsupervised domain adaptation for ATE, i.e., labeled training data is not available in the target domain. Specifically, given a set of labeled data DS = {(xSj , ySj )} NS j=1 from the source domain and a set of unlabeled data DU = {(xUj )} NU j=1 from the target domain, our goal is to predict labels yT for the unseen target test data DT = {(xTj )} NT j=1."
    }, {
      "heading" : "3.2 Bridge Construction",
      "text" : "Given a review sentence x from either domain, we map it with a lookup table E ∈ Rde×|V |, and generate word embeddings E = {e1, ..., en} ∈ Rde×n, where |V | is the vocabulary size, and de is the embedding dimension. For cross-domain ATE, we construct bridges for reviews to help directly transfer aspect terms across two domains.\nSyntactic Bridge In natural language, linguistic expressions are rich and flexible. In contrast, the syntactic structures are limited and are general across domains. Based on this observation, we propose to build connections between source and target words based on their syntactic roles (POS\ntags and dependency relations) rather than the lexical items. For example, from the parsing results in the upper part of Figure 3, the word pizza with a POS tag NN and dependency relations {det, nsubj} might be an aspect term, while those with the RB tag and advmod relation might not. Note the sentence “The keyboard is in reasonable size.” in the target domain has similar parsing results. Hence the syntactic roles can serve as supplementary evidence for recognizing aspect terms across domains.\nSeveral prior studies (Wang and Pan, 2018, 2019b; Pereg et al., 2020) also make use of parsing results. However, they only use dependency relations to link words or to propagate word representations. For example, given a dependency great nsubj−→ pizza in DS , where great is a known pivot and pizza is an aspect term, the goal is to extract keyboard as an aspect from the target review “The keyboard is great” in DT . The typical syntax based method Hier-Joint (Ding et al., 2017) first locates the pivot great, then utilizes the nsubj dependency to identify the term keyboard. Other methods like RNSCN (Wang and Pan, 2018) combine the embedding of the child node (pizza) with that of the parent node (great) according to the relation type, or reversely (depending on the specific design). It can be seen that the dependency relation nsubj here is only used as a link to the pivot.\nWe start in the opposite direction, i.e., we aim to fully exploit syntactic roles by recognizing themselves as pivots instead of treating them as links to pivots. To achieve this, we present a novel data structure to encode the POS and dependency information by grounding them into involved words. As shown in the lower part of Figure 3, for a word xi, we use a one-hot vector bpos ∈ RNpos and a multi-hot vector bdep ∈ RNdep to represent its POS tag and dependency relation(s), where Npos and Ndep are the number of tag/relation types. For\nbdep, we merge all relations involved with xi regardless of the direction (i.e., being the governor or dependent)3.\nTo enlarge the learning capability, we project bpos and bdep to the same dimensionality with learnable weight matrices4 and concatenate them to form the syntactic bridge bsyn:\nbsyn = (Wpos × bpos)⊕ (Wdep × bdep), (1)\nwhere bsyn ∈ Rde has the same dimensionality with the word embedding e. In training, Wpos and Wdep get trained by labeled samples. In testing, we fix them and obtain bsyn for DT . By doing this, our proposed method well preserves two types of syntactic information throughout the entire learning process. As a result, we can take full advantage of their transferable information.\nSemantic Bridge The semantic bridge takes the syntactic roles above as a basis but moves one step further to retrieve transferable prototypes. Unlike previous passive methods that construct information flows like pizza→good→keyboard via opinion terms or pizza→offer→keyboard via context terms, we aim to construct a direct flow like pizza→keyboard. For example, to transfer knowledge from pizza in DS to keyboard in DT , we aim to introduce some supplementary target terms like {disk, OS, mouse} in DU for pizza and directly improve its semantic relatedness with keyboard. We call these supplementary terms prototypes and will retrieve them to build the semantic bridges5.\nPLMs like BERT can find a set of semantically similar terms like {hamburger, salad} for pizza, which can also serve as prototypes. However, such prototypes are not suitable for the domain adaptation task, because aspect terms in one domain are often far away from those in another domain in the semantic space. To address this problem, we design a syntax-enhanced similarity metric to retrieve transferable semantic prototypes.\nBefore starting, we filter the words in DU by frequency and only preserve those appearing more than τ times. We regard these words in unlabeled target data as candidate prototypes and build a prototype bank Ṽ from DU accordingly. We then conduct retrieval following the procedure in Figure 4.\nFor a query word v ∈ V S (vocabulary of DS),\n3This simplification almost has no side effects. If a word has a NN tag and det relation, it must be the governer.\n4In all equations, W denotes a trainable weight matrix. 5We retrieve prototypes for all words in the review due to\nthe existence of domain-specific context terms like eat.\nwe want to find a prototype term ṽ ∈ Ṽ that play a similar syntactic role in the target domain. Specifically, we first summarize the global usages of v by merging its POS and dependency embeddings in all reviews where v appear in DS :\nbgpos = {bpos,j=1 | bpos,j=2 |...| bpos,j=NS}, bgdep = {bdep,j=1 | bdep,j=2 |...| bdep,j=NS},\n(2)\nwhere | is the dimension-wise OR operation and NS is the number of reviews in DS . Similarly, we can obtain b̃gpos and b̃ g dep for ṽ. We then define the syntax-enhanced similarity between v and ṽ:\ns.sim(v, ṽ) = c(bgpos, b̃ g pos)×c(bgdep, b̃ g dep)×c(e, ẽ), (3)\nwhere e and ẽ are word embeddings and c(·, ·) is the cosine similarity. Here the POS and dependency similarities are used to find similar syntactic roles, while the word similarity is used to reduce the noise of prototypes6. Consequently, we can obtain a s.sim score matrix MS∈R|V S |×|Ṽ |. After ranking, for v, we select the top-K words {ṽk}Kk=1 with their s.sim scores {s̃k}Kk=1 from the prototype bank. Lastly, we aggregate these prototypes into the semantic bridge bsem of v:\nbsem = K∑ k=1 s̃k · ẽk. (4)\nFollowing the way for DS , we also retrieve transferable prototypes for DU and DT using Ṽ . In this way, source and target words with the same prototypes can be directly correlated to each other. For DU , we can generate a score matrix MU ∈ R|V U |×|Ṽ | by calculating the s.sim for all words in DU and all candidate prototypes in Ṽ . Then we can obtain the semantic bridge bsem for each word in DU in training. In testing, DT is unseen and the global bgpos/b g dep are not available. Therefore, for a word w in DT , we obtain bsem using MU if w has appeared in DU . Otherwise, we temporarily use the local bpos/bdep of w in current tesing sample to replace the global bgpos/b g dep and calculate the s.sim.\n6A domain-invariant word that appears frequently in both domains should preserve its own information. It will have a maximum similarity score with itself since c(e, ẽ) = 1."
    }, {
      "heading" : "3.3 Bridge-based Sequence Tagging",
      "text" : "Based on the syntactic and semantic bridges, we now propose a lightweight end-to-end sequence tagger for aspect term extraction. As shown in Figure 5, the tagger receives a mixture of DS and DU for training and then makes predictions for DT in testing. We then illustrate the details.\nBridge Fuser Our constructed bridges have two properties. (1) Bridges are domain-invariant and should be preserved. (2) Bridges can help extract domain-invariant information from ei. Therefore, we propose to enhance the embedding ei of a word xi with its transferable bridges bsyn,i and bsem,i. Specifically, we use a gating operation to fuse bridges. Take the syntactic bridge as an example, we first calculate a dimension-wise gate gsyn,i: gsyn,i = σ (Wsyn(ei ⊕ bsyn,i)), (5) where Wsyn ∈ R2de×2de , σ is the Sigmoid function, ⊕ is concatenation. We then scale the concatenated vector ei ⊕ bsyn,i with gsyn,i and obtain the syntactic bridge enhanced embedding esyn,i:\nesyn,i = gsyn,i (ei ⊕ bsyn,i), (6)\nwhere is an element-wise multiplication. The semantic bridge enhanced embedding esem,i can be calculated similarly. We term the model with ei, esyn,i, and esem,i input as BaseTagger, SynBridge, and SemBridge, respectively. Three types of embeddings are collectively called einput,i .\nFeature Extractor Previous studies (Xu et al., 2018) show that low-level token features are insufficient for tagging terms. Therefore, we use a CNN encoder containing L stacked convolutional layers with ReLU activation to extract the high-level features fi ∈ Rdf : f l+1i = ReLU(f l i−c:i+c ∗Kl + bl), f0i = einput,i, (7) where K ∈ Rdf×(dinput×ks) is the kernel group, ks = 2c+ 1 is the kernel size.\nToken Classifier For recognizing aspect and opinion terms, we send fLi in the last layer to a token classifier:\nŷi = Softmax(WA × fLi ), (8)\nwhere ŷi is the prediction of the word xi.\nDomain Classifier Besides BIO tagging, we further enhance the domain-invariance of bridgebased features via domain adversarial training. Specifically, we first aggregate fLi to a global representation fg:\nfg =MaxPool(f L 1:n). (9)\nThen we add a Gradient Reversal Layer (GRL) (Ganin and Lempitsky, 2015) to fg with the scale coefficient λ and train a domain classifier to distinguish the domain that fg belongs to:\nŷd = Softmax(WO ×MLP (GRLλ(fg))), (10)\nwhere ŷd is the domain prediction, and MLP contains LD layers with ReLU activation.\nTraining Procedure In training, only samples from DS have corresponding BIO labels yS for token classification. The goal is to minimize the tagging loss for recognizing aspect terms:\nLBIO = − ∑ DS n∑ i=1 `(ŷi, yi), (11)\nwhere ` is the cross-entropy loss function. On the other hand, the samples from DS and DU are used to train the domain classifier and minimize the following domain classification loss:\nLDOM = − ∑ DS∪DU `(ŷd, yd), (12)\nwhere yd = 0 forDS and yd = 1 forDU . The final loss for training the end-to-end tagger is defined as L = LBIO + LDOM . Notice that DT is only used in testing. There is no data leakage in training, and the task setting is strictly inductive."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "Datasets We use three conventional English datasets from different domains and construct six directed transfer pairs, where R and L are from SemEval 2014 and 2015 (Pontiki et al., 2014, 2015), and D is collected by Hu and Liu (2004). Following previous studies (Wang and Pan, 2018, 2019b; Pereg et al., 2020), we use three different splits and each split has a fixed train-test ratio 3:1. The detailed statistics of datasets are presented in Table 17.\n7Our code and data are available at https://github.com/ NLPWM-WHU/BRIDGE.\nSettings We pre-process each dataset by lowercasing all words. We use the same word2vec vectors as previous studies (Wang and Pan, 2018, 2019a,b) to generate word embeddings, and set the dimensionality de=100. In the syntactic bridge, we use Stanford CoreNLP (Manning et al., 2014) for dependency parsing. There are 45 classes of POS tags and 40 classes of dependency relations in three datasets. In the semantic bridge, we set the frequency threshold τ=5, the number of prototypes K=10. In the end-to-end tagger, we set the number of convolution layers L=4, and the kernel size ks of each layer is 3, 5, 5, 5, respectively, the number of MLP layers LD=3, and dropout (Srivastava et al., 2014) is applied to layers’ outputs with the probability 0.5. The dimensionality of features df=256, the scale coefficient of GRL λ=0.1. We train the tagger for 100 epochs using Adam optimizer (Kingma and Ba, 2015) with the learning rate 1e-4 and batch size 8 in a 1080Ti GPU. Evaluation For each transfer pair, we use the labeled training data from the source domain and unlabeled training data from the target domain to train the tagger. Then we evaluate the tagger on unseen test data from the target domain. We use the mean F1-scores of aspect terms over three splits with three random seeds (i.e., nine runs for each transfer pair) for evaluation8."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "We classify all models into three categories. Type-I denotes the opinion term-based methods. TCRF (Jakob and Gurevych, 2010), RAP (Li et al., 2012), and Hier-Joint (Ding et al., 2017) use manually defined dependency rules. RNSCN and\n8The hyperparameter ranges are presented in Appendix A.\nTRNN (Wang and Pan, 2018, 2019a) model dependency trees with trainable recursive networks. SAL (Li et al., 2019) and TIMN (Wang and Pan, 2019b) replace the dependency tree with trainable memory interaction. Type-II denotes context term-based methods. BERT-Base uses vanilla base BERT (Devlin et al., 2019) for ATE. BERT-Cross (Xu et al., 2019) posttrains BERT on a combination of Yelp and Amazon corpus. UDA (Gong et al., 2020) and SA-EXAL (Pereg et al., 2020) incorporate syntactic information into BERT with auxiliary tasks and modified attention mechanisms9. Type-III denotes the proposed active domain adaptation strategy. BaseTagger is the tagger without bridges, while SynBridge and SemBridge use syntactic and semantic bridges, respectively."
    }, {
      "heading" : "4.3 Main Results",
      "text" : "The comparison results for all methods are shown in Table 2. It is clear that our proposed model achieves a new state-of-the-art performance in terms of the average F1-scores. For example, SemBridge outperforms the best TIMN in Type-I by 7.02% and BERT-Cross in Type-II by 5.21%, respectively. We also notice that our BaseTagger already outperforms all baselines. We attribute this to the design of CNN feature extractor and domain adversarial training (DAT). CNN focuses on the Ngram feature rather than a single word and reduces the side effects of non-pivot aspect terms. DAT is applied to the sentence-level features, such that they are not misled by the common N-grams that are labeled both 0 and 1.\n9Since SAL and UDA use extra aspect sentiment labels, we show how to make them fair competitors in Appendix B.\nSynBridge and SemBridge further improve BaseTagger with a 1.80% and 2.68% absolute gain, respectively. This proves the effectiveness of our proposed active domain adaptation strategy. Meanwhile, SemBridge is a bit superior to SynBridge. The reasons are two-fold. (1) The semantic bridges come from prototype words that possess prior embedding knowledge and also contain syntactic information, while the syntactic bridges are merely trained from scratch. (2) The retrieved top-K terms make the supplementary information in SemBridge more diverse and abundant than that in SynBridge.\nAmong the baselines, early methods using common opinion seeds and pre-defined rules are inferior. Relying on annotated opinion terms, the methods like TIMN get some improvements but induce extra annotation costs. By incorporating pre-trained BERT with external dependency and cross-domain corpus, UDA, SA-EXAL, and BERTCross outperform previous methods, but they need high computational resources. In contrast, by using the static Word2vec embeddings, our model can outperform those with dynamic BERT representations. This is instructive for other researches in that there is still room for improvement by exploring the syntactic and semantic features beyond the popular BERT-based models10."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 What If There Is an OTE Task?",
      "text" : "With the proposed active domain adaptation strategy, we do not need any manually labeled opinion terms for ATE. However, this does not mean that our method cannot handle opinion term extraction (i.e., OTE). In contrast, if the labeled opinion terms are provided in DS , we can also conduct the OTE task for DT by simply modifying the tagger. In specific, we add an opinion term prediction layer in Eq.8 and then extract aspect and opinion terms simultaneously. The results are shown in Table 3.\nObviously, our method again outperforms all baselines11. We find a small performance decrease in AVG-AS compared with that in Table 2. Similar results are also observed in BERT-Base. The reason is that the objective of ATE and OTE may interfere with each other without proper balancing and a sophisticated multi-task learning framework.\n10We also make some explorations about combining SynBridge and SemBridge, please refer to Appendix C.\n11Please refer to Appendix D for detailed results for all transfer pairs."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "We conduct a series of ablation study to validate the effectiveness of our method. The results are shown in Table 4.\nResults 1∼2 conform to our previous discussion about BaseTagger that both CNN and domain adversarial training contribute to overall good performance. Results 3∼6 show the effectiveness of POS and dependency embeddings in SynBridge. Specifically, in 5∼6, we replace our proposed structure for dependency with frequently-used Tree-LSTM and GCN to model the dependency tree and find a significant drop in performance. Results 7∼9 show the importance of all three types of similarity for retrieving prototypes in SemBridge."
    }, {
      "heading" : "5.3 Parameter Study",
      "text" : "There are three key hyperparameters in our method: the scale coefficient of GRL λ, the frequency threshold τ , and the number of prototypes K. We vary λ in the range 10−4 ∼ 1.0 and τ/K in 1 ∼ 10 to investigate their impacts and present the results in Figure 6.\nIn Figure 6(a), when increasing λ from 10−4 to 10−1, we enlarge the scale of domain adversarial training in GRL and get small improvements. However, the performance does not keep rising when\n(a) Impact of λ. (b) Impact of τ/K.\nFigure 6: Impacts of hyperparameters λ, τ , and K.\nλ = 1.0. This result shows that simply forcing non-pivots to transfer knowledge is not suitable for domain adaptation. In Figure 6(b), τ is used to balance diversity and accuracy. A low τ means that prototypes are diverse, but some of them are long-tail words and contribute little to the reduction of domain discrepancy. On the contrary, a high τ only preserves frequent prototypes, and some meaningful prototypes are filtered out. Therefore, a middle τ=5 is an appropriate choice. For K, the curve is generally upward when more prototypes are introduced. This trend is reasonable since more prototypes equal to more target information.\nIn Figure 7, we further analyze the impacts of the percentage of unlabeled data PU and the percentage of parsing noise PN . For PU , the performance is generally better when more unlabeled target data is introduced. Moreover, around 20%∼40% unlabeled data is enough to achieve satisfactory performance. Notice that SemBridge without unlabeled data will degenerate into BaseTagger since no prototypes can be retrieved. For PN , we manually disturb the parsing results to observe the robustness of our method. Clearly, after introducing noises on parsing, the performance begins to degrade, but not by a large margin. Our method has the ability to\nresist parsing errors for two reasons. First, beyond syntactic roles, we also incorporate embedding similarity when retrieving prototypes (for SemBridge only). Second, the gating mechanism can further filter useless syntactic information and maintain the quality of word representations."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "To have a close look, we select a few samples from testing target data for a case study. S1 and S2 show the positive impacts of bridges. Due to the space limit, we illustrate S1 in detail. Since most words in S1 are domain-specific terms in L, RNSCN fails to recognize any aspect terms by simply propagating word representations with dependency. BERTCross only extracts a part of aspect terms based on its prior knowledge. For our bridge-based method, SynBridge supplements syntactic roles {nummod, compound, obj, conj, NNS} for port. These syntactic roles also join the representation of usb and help to extract usb ports correctly. For SemBridge, the analysis is much straightforward. usb is the prototype of typical aspect terms in R like {garlic, thai, banana}, thus the tagger with semantic bridges can easily recognize usb as an aspect term.\nS3 further illustrates how SemBridge helps recover from the wrong parsing results. Such results make two syntax based methods RNSCN and SynBridge stop working. In contrast, tuna is the prototype of noun words like {nvidia, amd, blade} in L and melt has the verb prototype like {imagine, hang, relax} in R, thus SemBridge correctly extracts tuna and filters out melt in the same time.\nIn Table 6, We further present several sample prototypes of the training data from the transfer pairs R→L (upper three) and L→R (lower three) in SemBridge, where three terms on the left are aspect term, opinion term, and context term, respectively. For a source non-pivot term like processor in L, SemBridge enhances it with typical target words like soup and burger. As a result, the domain discrepancy between the source and target data is largely reduced with the help of prototypes."
    }, {
      "heading" : "5.5 Analysis on Computational Cost",
      "text" : "In practice, for any transfer pairs, the one-time construction of syntactic and semantic bridges can finish within 30 seconds. Therefore, we focus on the end-to-end training costs of SynBridge/SemBridge. We run five top-performing methods on the transfer pair R→L and present the trainable parameter number and running time per epoch of each method in Table 7. We can conclude that our proposed method maintains a quite low computational cost."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a novel active domain adaptation method for aspect term extraction. Unlike previous studies that conduct passive domain adaptation by associating aspect terms with pivots, we actively enhance the terms’ transferability by constructing syntactic and semantic bridges for them. We then design a lightweight end-toend tagger for bridge-based sequence tagging. Experiments on six transfer pairs demonstrate that our method achieves a new state-of-the-art performance with a quite low computational cost."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their valuable comments. The work described in this paper is supported by the NSFC projects (61572376, 91646206), and the 111 project (B07037)."
    }, {
      "heading" : "A Ranges of Hyperparameters",
      "text" : "We present the hyperparameter ranges in Table 8. We select all hyperparameters via manual tuning."
    }, {
      "heading" : "B Modification of SAL and UDA",
      "text" : "Since SAL and UDA are designed for end-to-end cross-domain aspect-based sentiment analysis, they have access to the aspect sentiment labels in training. As previous studies show, aspect term extraction and aspect-level sentiment classification can benefit each other. Therefore, it is unfair to directly compare our method with SAL and UDA.\nWe choose to modify SAL and UDA and make them fair competitors. We degrade the collapsed tags {B-POS, I-POS, B-NEG, I-NEG, B-NEU, INEU, O} to {B, I, O} thus remove the aspectlevel sentiment classification task. Following other BERT-based methods, we use BERT-Base as the backbone of UDA.\nC Can We Combine SynBridge and SemBridge?\nSince SynBridge and SemBridge contain transferable syntactic and semantic information, it is intuitive to combine them for a better performance than either individual model. Here we apply a very simple operation for combination.\nFor a word xi with embedding ei, we first obtain its syntactic and semantic bridges bsyn,i and bsem,i, and merge them into a combined bridge:\nbcom,i = (Wsyn × bsyn,i) + (Wsem × bsem,i), (13)\nThen we conduct a similar gating operation and get the combined bridge enhanced embedding ecom,i:\ngcom,i = σ (Wcom(ei ⊕ bcom,i)) ecom,i = gcom,i (ei ⊕ bcom,i),\n(14)\nLastly, we regard ecom,i as the input of tagger and make predictions for aspect terms. We term this model ComBridge and present the results in Table 10.\nComBridge slightly outperforms SemBridge and achieves the optimal results in all bridge-based methods. The small improvement is explicable since SemBridge already contains most of the syntactic information in SynBridge and we do not use any sophisticated methods in combination."
    }, {
      "heading" : "D Detailed Results for an Additional OTE Task",
      "text" : "When opinion terms are labeled, our method can also conduct aspect term extraction and opinion term extraction simultaneously. For recognizing aspect and opinion terms, we only need to add an opinion term prediction layer:\nŷa,i = Softmax(WA × fLi ), ŷo,i = Softmax(WO × fLi ), (15)\nwhere ŷa,i / ŷo,i are the predictions of {B, I,O} for the aspect / opinion terms. And the resulted BIO loss is calculated as follow:\nLBIO = − ∑ DS n∑ i=1 `(ŷa,i, ya,i) + `(ŷo,i, yo,i) (16)\nwhere ` is the cross-entropy loss function. We present the detailed results in Table 9. Obviously, our proposed SynBridge and SemBridge outperform other baselines in both aspect and opinion F1-scores."
    } ],
    "references" : [ {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "References John Blitzer", "Ryan T. McDonald", "Fernando Pereira." ],
      "venue" : "EMNLP, pages 120–128.",
      "citeRegEx" : "Blitzer et al\\.,? 2006",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2006
    }, {
      "title" : "Aspect extraction with automated prior knowledge learning",
      "author" : [ "Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu." ],
      "venue" : "ACL, pages 347–358.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Transfer capsule network for aspect level sentiment classification",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "ACL, pages 547–556.",
      "citeRegEx" : "Chen and Qian.,? 2019",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2019
    }, {
      "title" : "Enhancing aspect term extraction with soft prototypes",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "EMNLP, pages 2107–2117. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Qian.,? 2020a",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2020
    }, {
      "title" : "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "ACL, pages 3685–3694.",
      "citeRegEx" : "Chen and Qian.,? 2020b",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent neural networks with auxiliary labels for crossdomain opinion target extraction",
      "author" : [ "Ying Ding", "Jianfei Yu", "Jing Jiang." ],
      "venue" : "AAAI, pages 3436–3442.",
      "citeRegEx" : "Ding et al\\.,? 2017",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Yaroslav Ganin", "Victor S. Lempitsky." ],
      "venue" : "ICML, pages 1180–1189.",
      "citeRegEx" : "Ganin and Lempitsky.,? 2015",
      "shortCiteRegEx" : "Ganin and Lempitsky.",
      "year" : 2015
    }, {
      "title" : "Unified feature and instance based domain adaptation for aspect-based sentiment analysis",
      "author" : [ "Chenggong Gong", "Jianfei Yu", "Rui Xia." ],
      "venue" : "EMNLP, pages 7035–7045.",
      "citeRegEx" : "Gong et al\\.,? 2020",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-source domain adaptation for text classification via distancenet-bandits",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "AAAI, pages 7830–7838.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "ACL, pages 504–515.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "SIGKDD, pages 168– 177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Extracting opinion targets in a single and cross-domain setting with conditional random fields",
      "author" : [ "Niklas Jakob", "Iryna Gurevych." ],
      "venue" : "EMNLP, pages 1035–1045.",
      "citeRegEx" : "Jakob and Gurevych.,? 2010",
      "shortCiteRegEx" : "Jakob and Gurevych.",
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Structure-aware review mining and summarization",
      "author" : [ "Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Yingju Xia", "Shu Zhang", "Hao Yu." ],
      "venue" : "COLING, pages 653–661.",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Cross-domain co-extraction of sentiment and topic lexicons",
      "author" : [ "Fangtao Li", "Sinno Jialin Pan", "Ou Jin", "Qiang Yang", "Xiaoyan Zhu." ],
      "venue" : "ACL, pages 410– 419.",
      "citeRegEx" : "Li et al\\.,? 2012",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning",
      "author" : [ "Zheng Li", "Xin Li", "Ying Wei", "Lidong Bing", "Yu Zhang", "Qiang Yang." ],
      "venue" : "EMNLP-IJCNLP, pages 4589– 4599.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Opinion target extraction using partially-supervised word alignment model",
      "author" : [ "Kang Liu", "Heng Li Xu", "Yang Liu", "Jun Zhao." ],
      "venue" : "IJCAI, pages 2134–2140.",
      "citeRegEx" : "Liu et al\\.,? 2013",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Opinion target extraction using word-based translation model",
      "author" : [ "Kang Liu", "Liheng Xu", "Jun Zhao." ],
      "venue" : "EMNLP, pages 1346–1356.",
      "citeRegEx" : "Liu et al\\.,? 2012",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Fine-grained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Pengfei Liu", "Shafiq R. Joty", "Helen M. Meng." ],
      "venue" : "EMNLP, pages 1433–1443.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring sequence-tosequence learning in aspect term extraction",
      "author" : [ "Dehong Ma", "Sujian Li", "Fangzhao Wu", "Xing Xie", "Houfeng Wang." ],
      "venue" : "ACL, pages 3538–3547.",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "ACL, pages 55–60.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Syntactically aware cross-domain aspect and opinion terms extraction",
      "author" : [ "Oren Pereg", "Daniel Korat", "Moshe Wasserblat." ],
      "venue" : "COLING, pages 1772– 1777.",
      "citeRegEx" : "Pereg et al\\.,? 2020",
      "shortCiteRegEx" : "Pereg et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval-2015 task 12: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos." ],
      "venue" : "SemEval, pages 486–495.",
      "citeRegEx" : "Pontiki et al\\.,? 2015",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "SemEval, pages 27–35.",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting product features and opinions from reviews",
      "author" : [ "Ana-Maria Popescu", "Oren Etzioni." ],
      "venue" : "EMNLP, pages 339–346.",
      "citeRegEx" : "Popescu and Etzioni.,? 2005",
      "shortCiteRegEx" : "Popescu and Etzioni.",
      "year" : 2005
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen." ],
      "venue" : "Computational Linguistics, 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "JMLR, 15(1):1929– 1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards a one-stop solution to both aspect extraction and sentiment analysis tasks with neural multitask learning",
      "author" : [ "Feixiang Wang", "Man Lan", "Wenting Wang." ],
      "venue" : "IJCNN, pages 1–8.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Recursive neural structural correspondence network for crossdomain aspect and opinion co-extraction",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "ACL, pages 2171–2181.",
      "citeRegEx" : "Wang and Pan.,? 2018",
      "shortCiteRegEx" : "Wang and Pan.",
      "year" : 2018
    }, {
      "title" : "Syntactically meaningful and transferable recursive neural networks for aspect and opinion extraction",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "CL, 45(4):705–736.",
      "citeRegEx" : "Wang and Pan.,? 2019a",
      "shortCiteRegEx" : "Wang and Pan.",
      "year" : 2019
    }, {
      "title" : "Transferable interactive memory network for domain adaptation in fine-grained opinion extraction",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan." ],
      "venue" : "AAAI, pages 7192–7199.",
      "citeRegEx" : "Wang and Pan.,? 2019b",
      "shortCiteRegEx" : "Wang and Pan.",
      "year" : 2019
    }, {
      "title" : "Recursive neural conditional random fields for aspect-based sentiment analysis",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao." ],
      "venue" : "EMNLP, pages 616–626.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Coupled multi-layer attentions for co-extraction of aspect and opinion terms",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao." ],
      "venue" : "AAAI, pages 3316–3322.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Phrase dependency parsing for opinion mining",
      "author" : [ "Yuanbin Wu", "Qi Zhang", "Xuanjing Huang", "Lide Wu." ],
      "venue" : "EMNLP, pages 1533–1541.",
      "citeRegEx" : "Wu et al\\.,? 2009",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2009
    }, {
      "title" : "Double embeddings and cnn-based sequence labeling for aspect extraction",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip S. Yu." ],
      "venue" : "ACL, pages 592–598.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip S. Yu." ],
      "venue" : "NAACL-HLT, pages 2324–2335.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Recent studies define ATE as a sequence tagging task and propose supervised taggers (Wang et al., 2017; Xu et al., 2018).",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 35,
      "context" : "Recent studies define ATE as a sequence tagging task and propose supervised taggers (Wang et al., 2017; Xu et al., 2018).",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "However, due to the high cost of token-level annotation, the lack of labeled data becomes the main obstacle (Chen and Qian, 2019).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Early studies (Li et al., 2012; Ding et al., 2017) use common opinion seeds (e.",
      "startOffset" : 14,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Early studies (Li et al., 2012; Ding et al., 2017) use common opinion seeds (e.",
      "startOffset" : 14,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Pivot words are words which behave in the same way for discriminative learning in both domains (Blitzer et al., 2006).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "t their contexts, recent studies (Xu et al., 2019; Gong et al., 2020) leverage PLMs to transfer aspect terms with common context terms2.",
      "startOffset" : 33,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "t their contexts, recent studies (Xu et al., 2019; Gong et al., 2020) leverage PLMs to transfer aspect terms with common context terms2.",
      "startOffset" : 33,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wu et al., 2009; Qiu et al., 2011) and hand-crafted features (Li et al.",
      "startOffset" : 81,
      "endOffset" : 161
    }, {
      "referenceID" : 25,
      "context" : "Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wu et al., 2009; Qiu et al., 2011) and hand-crafted features (Li et al.",
      "startOffset" : 81,
      "endOffset" : 161
    }, {
      "referenceID" : 34,
      "context" : "Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wu et al., 2009; Qiu et al., 2011) and hand-crafted features (Li et al.",
      "startOffset" : 81,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : "Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wu et al., 2009; Qiu et al., 2011) and hand-crafted features (Li et al.",
      "startOffset" : 81,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : ", 2011) and hand-crafted features (Li et al., 2010; Liu et al., 2012, 2013; Chen et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", 2011) and hand-crafted features (Li et al., 2010; Liu et al., 2012, 2013; Chen et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "With the development of deep learning, supervised sequence taggers have become the mainstream due to their promising performance (Liu et al., 2015; Wang et al., 2016, 2017; Xu et al., 2018; Ma et al., 2019; Chen and Qian, 2020a).",
      "startOffset" : 129,
      "endOffset" : 228
    }, {
      "referenceID" : 35,
      "context" : "With the development of deep learning, supervised sequence taggers have become the mainstream due to their promising performance (Liu et al., 2015; Wang et al., 2016, 2017; Xu et al., 2018; Ma et al., 2019; Chen and Qian, 2020a).",
      "startOffset" : 129,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : "With the development of deep learning, supervised sequence taggers have become the mainstream due to their promising performance (Liu et al., 2015; Wang et al., 2016, 2017; Xu et al., 2018; Ma et al., 2019; Chen and Qian, 2020a).",
      "startOffset" : 129,
      "endOffset" : 228
    }, {
      "referenceID" : 3,
      "context" : "With the development of deep learning, supervised sequence taggers have become the mainstream due to their promising performance (Liu et al., 2015; Wang et al., 2016, 2017; Xu et al., 2018; Ma et al., 2019; Chen and Qian, 2020a).",
      "startOffset" : 129,
      "endOffset" : 228
    }, {
      "referenceID" : 28,
      "context" : "More recently, there emerge many studies that interact ATE with other tasks like aspect-level sentiment classification (Wang et al., 2018; He et al., 2019; Chen and Qian, 2020b).",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "More recently, there emerge many studies that interact ATE with other tasks like aspect-level sentiment classification (Wang et al., 2018; He et al., 2019; Chen and Qian, 2020b).",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : "More recently, there emerge many studies that interact ATE with other tasks like aspect-level sentiment classification (Wang et al., 2018; He et al., 2019; Chen and Qian, 2020b).",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "Domain Adaptation Many domain adaptation methods have been proposed to solve coarsegrained tasks like text classification (Blitzer et al., 2006; Ganin and Lempitsky, 2015; Guo et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "Domain Adaptation Many domain adaptation methods have been proposed to solve coarsegrained tasks like text classification (Blitzer et al., 2006; Ganin and Lempitsky, 2015; Guo et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Domain Adaptation Many domain adaptation methods have been proposed to solve coarsegrained tasks like text classification (Blitzer et al., 2006; Ganin and Lempitsky, 2015; Guo et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "Early researches use common opinion seeds and pre-defined dependency link rules to build manual features (Jakob and Gurevych, 2010), conduct bootstrapping (Li et al.",
      "startOffset" : 105,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "Early researches use common opinion seeds and pre-defined dependency link rules to build manual features (Jakob and Gurevych, 2010), conduct bootstrapping (Li et al., 2012), and create pseudo target labels (Ding et al.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : ", 2012), and create pseudo target labels (Ding et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "Several prior studies (Wang and Pan, 2018, 2019b; Pereg et al., 2020) also make use of pars-",
      "startOffset" : 22,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "The typical syntax based method Hier-Joint (Ding et al., 2017) first locates the pivot great, then utilizes the nsubj dependency to identify the term keyboard.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "Other methods like RNSCN (Wang and Pan, 2018) combine the embedding of the child node (pizza) with that of the parent node (great) according to the relation type, or reversely (depending on the specific design).",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 35,
      "context" : "Feature Extractor Previous studies (Xu et al., 2018) show that low-level token features are insufficient for tagging terms.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Then we add a Gradient Reversal Layer (GRL) (Ganin and Lempitsky, 2015) to fg with the scale coefficient λ and train a domain classifier to distinguish the domain that fg belongs to: ŷd = Softmax(WO ×MLP (GRLλ(fg))), (10)",
      "startOffset" : 44,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Following previous studies (Wang and Pan, 2018, 2019b; Pereg et al., 2020), we use three different splits and each split has a fixed train-test ratio 3:1.",
      "startOffset" : 27,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "In the syntactic bridge, we use Stanford CoreNLP (Manning et al., 2014) for dependency parsing.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : "of each layer is 3, 5, 5, 5, respectively, the number of MLP layers LD=3, and dropout (Srivastava et al., 2014) is applied to layers’ outputs with the probability 0.",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "We train the tagger for 100 epochs using Adam optimizer (Kingma and Ba, 2015) with the learning rate 1e-4 and batch size 8 in a 1080Ti GPU.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "TCRF (Jakob and Gurevych, 2010), RAP (Li et al., 2012), and Hier-Joint (Ding et al.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : ", 2012), and Hier-Joint (Ding et al., 2017) use manually defined dependency rules.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "SAL (Li et al., 2019) and TIMN (Wang and Pan, 2019b) replace the dependency tree with trainable memory interaction.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 31,
      "context" : ", 2019) and TIMN (Wang and Pan, 2019b) replace the dependency tree with trainable memory interaction.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 36,
      "context" : "BERT-Cross (Xu et al., 2019) posttrains BERT on a combination of Yelp and Amazon corpus.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : ", 2020) and SA-EXAL (Pereg et al., 2020) incorporate syntactic information into BERT with auxiliary tasks and modified attention mechanisms9.",
      "startOffset" : 20,
      "endOffset" : 40
    } ],
    "year" : 2021,
    "abstractText" : "As a fine-grained task, the annotation cost of aspect term extraction is extremely high. Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains. Since most aspect terms are domain-specific, they cannot be transferred directly. Existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots). However, all these methods need either manually labeled pivot words or expensive computing resources to build associations. In this paper, we propose a novel active domain adaptation method. Our goal is to transfer aspect terms by actively supplementing transferable knowledge. To this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots. We also build semantic bridges by retrieving transferable semantic prototypes. Extensive experiments show that our method significantly outperforms previous approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}