{
  "name" : "2021.acl-long.191.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning",
    "authors" : [ "Thomas Dopierre", "Christophe Gravier", "Wilfried Logerais", "Jean Monnet" ],
    "emails" : [ "firstname.lastname@univ-st-etienne.fr", "t.dopierre@meetic-corp.com", "w.logerais@meetic-corp.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2454–2466\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2454"
    }, {
      "heading" : "1 Introduction",
      "text" : "Intent detection, a sub-field of text classification, involves classifying user-generated short-texts into intent classes, usually for conversational agents applications (Casanueva et al., 2020). Since conversational agent applications are domain-specific, intent detection is a challenging task because of labeled data scarcity and the number of classes (intents) it usually involves (Dopierre et al., 2020). As a consequence, recent research (Snell et al., 2017; Ren et al., 2018) considers few-shot intent detection as a meta-learning problem: the model is trained to classify user utterances from a consecutive set of small tasks named episodes. Each episode contains a limited number of C classes alongside a limited number of K labeled data for each of the C classes – this is usually referred to as a C-way K-shots setup. At test time, the algorithm is evaluated on classes that were not seen during training. That is the reason why meta-learning is sometimes referred to as learning to learn: it mimics human abilities to learn iteratively from different and small tasks. Meta-learning has successfully been applied to a wide set of NLP tasks: hypernym detection (Yu et al., 2020), low resource machine translation (Gu et al., 2018), machine understanding tasks (Dou et al., 2019) or structured query generation (Huang et al., 2018). Most metalearning algorithms (Section 2) were developed in the course of the last 5 years. It has recently been empirically demonstrated that comparative studies in follow-up papers of (Snell et al., 2017) are debatable – for short texts classification – because of the two following main issues (Dopierre et al., 2021). First, comparative studies involve simple and limited datasets in terms of number and separability of classes (SNIPS (Coucke et al., 2018), a very popular dataset, includes only 7 classes, with the current best model performing over 99% accuracy (Cao et al., 2020)). Second, as we further better understand (Niven and Kao, 2019), fine-tune (Liu et al., 2019b; Hao et al., 2020) and refine (Khetan and Karnin, 2020) BERT-derived models, it is not clear if the different meta-learning frameworks can be considered state-of-the-art due to their architecture or due to the improvements of available text encoders at the time of conception. (Dopierre et al., 2021) concludes that Prototypical Networks (Snell et al., 2017) (that were using LSTM-based text encoders when introduced in NLP) are actually the state-of-the-art for intent detection when equipped with a fine-tuned BERT text encoder model. Ultimately, improving Prototypical Networks have therefore been proven to be a very challenging task in reality. A cornerstone challenge is that meta-learning models can easily overfit on the biased distribution\nintroduced by a few training examples (Yang et al., 2021). In order to prevent overfitting and inspired by (Xie et al., 2020), we introduce an unsupervised diverse paraphrasing loss in the Prototypical Networks framework. A key idea is consistency learning: by augmenting unlabeled user utterances, PROTAUGMENT enforce a more robust text representation learning. Unfortunately, back-translation is a poor data augmentation strategy for short-texts: neural machine translation provides very similar (if not the same) sentences to the original ones, which hinders its ability to provide diverse augmentations (Section 5.3). Consequently, in this work, we transfer a denoising autoencoder pre-trained on the sequence-to-sequence task (Lewis et al., 2020) to the paraphrase generation task and then use it to generate paraphrases. As fine-tuning is very efficient for such a model, it is not easy to optimize it for diverse paraphrasing. (Goyal and Durrett, 2020) presents an approach for diverse paraphrasing that reorders the original sentence to guide the conditional language model to generate diverse sentences. The diversity in that work is provided by the reordering of the elements, which surprisingly affects the attention mechanism. In (Liu et al., 2020), expression diversity is part of the unsupervised paraphrasing system supported by simulated annealing. Both approaches imply domain transfer, and consequently, as many diverse paraphrasing models to maintain as the number of considered application domains, which do not scale very well. In this work, we instead introduce diversity in the downstream decoding algorithm used for paraphrase generation. Diverse decoding methods are mostly extensions to the beam search algorithm, including noise-based algorithms (Cho, 2016), iterative beam search (Kulikov et al., 2019), clustered beam search (Tam, 2020) and diverse beam search (Vijayakumar et al., 2018). There is no clear optimal solution, the choice is task-specific and dependent on one’s tolerance for lower quality outputs as a diversity/fluency trade-off (Ippolito et al., 2019). While diverse beam search allows controlling the diversity/fluency trade-off partially, we further demonstrate that adding constraints to diverse beam search in order to generate tokens not seen in the input sentence (that is, constrained diverse beam search) is a simple yet powerful strategy to further improve the diversity of the paraphrases. Paired with paraphrasing user utterances and its consistency loss incorporated in Prototypical networks, our model is the best method for intent detection meta-learning on 4 public datasets, with neither extra labeling efforts nor domain-specific conditional language model fine-tuning. We also show that PROTAUGMENT, having access to only 10 samples of each class of the training data, still significantly outperforms a Prototypical Network which is given access to all samples of the same training data."
    }, {
      "heading" : "2 Neural architectures for meta-learning",
      "text" : "Past works on meta-learning for classification tasks investigate how to best predict a query point’s class at an episode scale. This process is bounded to the set of the C classes considered in a given episode. Matching Networks (Vinyals et al., 2016) predict the class of a query point as the average cosine distance between the query vector and all support vectors for each class. Prototypical Networks (Snell et al., 2017) extend Matching Networks: after obtaining support vectors from the encoder, a class prototype is produced via a class-wise vector averaging operation. All query points are then predicted with respect to their distance (cosine or euclidean) to all prototypes. Like Prototypical Networks, Relation Networks (Sung et al., 2018) emerged from Computer Vision application and were later successfully applied to NLP (Zhang et al., 2018). They introduce a relation module, which captures the relationship between data points: instead of using a pre-defined distance (euclidean or cosine most of the time), this approach allows such networks to learn this metric by themselves. This is achieved using either a shallow feed-forward sub-network or a Neural Tensor Layer relation module (Socher et al., 2013) (intermediate learnable matrices). Another extension to Prototypical Networks is provided in (Ren et al., 2018). Unlabeled data are incorporated using two distinct approaches: i) taking unlabeled data from the same classes as the episode or ii) using any unlabeled data and incorporating both a distractor cluster and masking strategy to minimize the impact of distant unlabeled points. The first approach is unrealistic for meta-learning, as it implies knowing the unlabeled data class. The second method assumes that all the noise is centered around a single distractor cluster and introduces an additional hyperparameter for masking – which is hardly fine-tuneable for small few-shot datasets."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Notations",
      "text" : "Meta-learning algorithms are trained using a specific procedure made of consecutive episodes. Let Cep be the set of C classes sampled for the current training episode, such as Cep ⊂ Ctrain, where Ctrain is the set of all classes available for training. We note Ctest, the set of classes used for testing, with Ctrain ∩ Ctest = ∅. Each class c ∈ Cep comes with K labeled samples, used as support. The set of C × K samples are usually referred to as S, the support set, so that S = {(x1, y1), . . . , (xC×K , yC×K)}. We denote Sc the set of support examples labeled with class c. Each episode comes with a query set Q, which serves as the episode-scale optimization – the model parameters are updated based on the prediction loss on Q, given S as an input. Qc is the set of query examples labeled with class c."
    }, {
      "heading" : "3.2 Prototypical networks",
      "text" : "In prototypical networks, each class is mapped to a representative point, called prototype. Each sample is first encoded into a vector using an embedding function fφ with learnable parameters φ – this is the function we want to optimize. Using these embeddings, we compute each prototype pc, c ∈ Cep as the mean vector of embedded support points belonging to the class c, as described in Equation 1.\npc = 1\nK ∑ (xi,yi)∈Sc fφ(xi) (1)\nGiven those prototypes and a distance function d, prototypical networks assign a label to a query point by computing the softmax over distances between this point’s embedding and the prototypes, as in Equation 2. In the original paper, (Snell et al., 2017) use the euclidean distance and we also observed consistent slightly worse results with the cosine distance.\nPφ(y = c|x) = softmax (−d(fφ(x), pc)) (2)\nThe supervised loss function L̄ is the average negative log-probability of the correct class assignments for all query points. At test time, episodes are created using classes from Ctest, and accuracy is measured as the query points assignments, given prototypes derived from the support points."
    }, {
      "heading" : "4 PROTAUGMENT",
      "text" : "In this section, we present our semi-supervised approach PROTAUGMENT. Along with the labeled data randomly chosen at each episode, this approach uses U unlabeled data randomly drawn from the whole dataset – that is, data from training, validation, and test labels. We first do a data augmentation step from this unlabeled data, where we obtain M paraphrases for each unlabeled sentence. Themth paraphrase of xwill be denoted x̃m. Then, given unlabeled data and their paraphrases, we compute a fully unsupervised loss. Finally, we combine both the supervised loss L̄ (the Prototypical Network loss using labeled data) and unsupervised loss (denoted L̃) and run back-propagation to update the model’s parameters."
    }, {
      "heading" : "4.1 Generating augmentations through paraphrasing",
      "text" : "The BART (Lewis et al., 2020) model is a Transformer-based neural machine translation architecture that is trained to remove artificially corrupted text from the input thanks to an autoencoder architecture. While it is trained to reconstruct the original noised input, it can be fine-tuned for taskspecific conditional generation by minimizing the cross-entropy loss on new training input-output pairs (Bevilacqua et al., 2020). In PROTAUGMENT, we fine-tune a pre-trained BART model on the paraphrasing task. The paraphrase sentence pairs we use for this task are taken from 3 different paraphrase detection datasets1: Quora (Sharma et al., 2019), MSR (Zhao and Wang, 2010), and Google PAWS-Wiki (Yang et al., 2019; Zhang et al., 2019). Those datasets have different sizes, and the largest one – Quora – consist of 149,263 pairs of duplicate questions. To balance turns of sentences (questions/non questions paraphrases), 50% of our fine-tuning paraphrase datasets is made of Quora, 5.6% of MSR and 44.4% PAWS-Wiki. This yields 94,702 sentence pairs to train the model on the paraphrasing task. We include both code and data on our github repository 2.\nUsing this fine-trained paraphrasing model, we can generate paraphrases of unlabeled sentences, hopefully having paraphrases representing the same intents as the original sentences. To add some diversity in the generated paraphrases, we use Di-\n1we take only pairs that are paraphrases of each other since these are paraphrase detection datasets\n2https://github.com/tdopierre/ProtAugment\nverse Beam Search (DBS) instead of the regular Beam Search. As Vijayakumar et al. (2018) has shown in the original paper, adding a dissimilarity term during the decoding step helps the model produce sequences that are quite far from each other while still retaining the same meaning. The next section describes how we constrained this decoding to enforce even more diversity among generated paraphrases in PROTAUGMENT."
    }, {
      "heading" : "4.2 Constrained user utterances generation",
      "text" : "While DBS enforces diversity between the generated sentences, it does not ensure diversity between the generated paraphrases and the original sentences. It was formerly designed for tasks that do not need this diversity with the original sentence (translation, image captioning, question generation). To enforce that our generated paraphrases are diverse enough, we further constraint DBS by forbidding using parts of the original sentences. In the following paragraphs, we introduce two forbidding strategies.\nUnigram Masking. In this strategy, we randomly select tokens from the input sentence which will be\nforbidden at the generation step. The goal here is to force the model to use different words in the generated sentences than it saw in the original sentences. Each word of the input sentence is randomly masked using a probability pmask. The underlying assumption is that forbidding tokens at the beginning of a sentence with a higher probability than the end of the sentence may have a greater impact on the beam search algorithm. Indeed, as the decoding is a conditional task based on prior generated tokens, masking the first tokens may significantly impact diversity. We therefore introduce two additional variants: one where we put more probability on the first tokens and the reverse where there is more weight in the last tokens. To ensure that all three variants mask the same amount of tokens on average, we ensure the area under the curve of the three probability functions are equal to a fixed value noted pmask.\nBi-gram Masking Another strategy we consider is to prevent the paraphrasing model from generating the same bi-grams as in the original sentence. This time, we are not masking any single word but\nforcing the model to change the sentence’s structure, which will, hopefully, increase the diversity of the generated paraphrases."
    }, {
      "heading" : "4.3 Unsupervised diverse paraphrasing loss",
      "text" : "After generating paraphrases for each unlabeled sentence, we create unlabeled prototypes. For each unlabeled sentence xu ∈ U , we derive the unlabeled prototype pxu as the average embedding of the paraphrases of xu (Equation 3).\npxu = 1\nM M∑ m=1 fφ(x̃ m u ) (3)\nAfter obtaining the unlabeled prototypes, we compute the distances between all unlabeled samples and all unlabeled prototypes. Given such distances, we model the probability of each unlabeled sample being assigned to each unlabeled prototype (Equation 4), as in the supervised part of the Prototypical Networks – except this time, it is fully unsupervised. This probability should be close to 1 between an unlabeled sample and its associated unlabeled prototype and close to 0 otherwise.\nPφ(u = v|xu) = softmax (−d(fφ(xu), pxv)) (4)\nGiven assign probabilities between unlabeled samples and unlabeled prototypes, we can compute a fully unsupervised cross-entropy loss L̃, training the model to bring each sentence closer to its augmentations’ prototype and further from the prototypes of other unlabeled sentences. Recall that fφ is the embedding function with φ as learnable parameters (Section 3.2).\nAfter obtaining both supervised loss L̄ and unsupervised loss L̃, we combine them into the final loss L using a loss annealing scheduler (see Equation 5), which will gradually incorporate the unsupervised loss as training progresses.\nL = tα × L̃+ (1− tα)× L̄ ; t ∈ (0, 1) (5)\nThe goal here is to mainly use the supervised loss first so that the model gets a sense of the classification task. Then, incorporating more and more knowledge from unlabeled samples will make the model more robust to noise, which is essential as it is constantly tested on classes it has never seen before. We explore three different strategies for gradually increasing the unsupervised contribution: a linear approach (α = 1), an aggressive one (α = 0.25), and a conservative one (α = 4)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We consider the DialoGLUE benchmark (Mehri et al., 2020), a set of natural language understanding benchmark for task-oriented dialogue, which contains three datasets for intent detection: Banking77, HWU64 and Clinic150 – the three datasets were already available prior the release of DialoGLUE. Additionally, we also consider the Liu57 intent detection dataset, as it contains the same order of magnitude of intent classes and is user-generated as well. All datasets are public and in English.\nBanking77 The Banking77 dataset (Casanueva et al., 2020) classifies 13, 083 user utterances related to into 77 different intents. This dataset i) is specific to a single domain (banking) and ii) requires a fine-grained understanding to classify due to intents being very similar. Following (Mehri et al., 2020) and contrary to (Casanueva et al., 2020), we designate a validation set along a training and a testing set for that dataset (Table 1).\nHWU64 HWU64 (Xingkun Liu and Rieser, 2019) classifies 25, 716 user utterances with 64 user intents. It features intents spanning across 21 domains (alarm, audio, audiobook, calendar, cooking, datetime, . . . ). When separating training, validation, and test labels, we ensure each domain is rep-\nresented only in one set of labels. This ensures the model learns to discriminate between both intents and domains.\nClinic150 This dataset (Larson et al., 2019) classifies 150 user intents in perfectly equallydistributed classes. This chatbot-like style dataset was initially designed to detect out-of-scope queries, though, in our experiments, we discard the out-of-scope class and only keep the 150 labeled classes to work with, as in (Mehri et al., 2020).\nLiu57 Introduced by Liu et al. (2019a), this intent detection dataset is composed of 54 classes. It was collected on Amazon Mechanical Turk, where workers were asked to formulate queries for a given intent with their own words. It is highly imbalanced: the most (resp. least) common class holds 5, 920 (resp. 24) samples"
    }, {
      "heading" : "5.2 Experimental settings",
      "text" : "Conditional language model and language model. For the BART fine-tuning process, we used the defaults hyper-parameters reported in (Lewis et al., 2020), and we fine-tuned the BART model for a single epoch (two hours on a Titan RTX GPU). Increasing the number of epochs for fine-tuning BART degrades performances on the intent detection task: the downstream diverse beam search struggles to find diverse enough beam groups since the model perplexity has been lower with further fine-tuning (this is also hinted in (Bevilacqua et al., 2020)). Our text encoder fφ is a bert-base model, and the embedding of a given sentence is the last layer hidden state of the first token of this sentence. For each dataset, this model is fine-tuned on the masked language modeling task for 20 epochs. Then, the encoder of our meta learner is initialized using the weights of this fine-tuned model.\nDatasets From a dataset point-of-view, we create two data profiles: full (all the training dataset is available, the usual meta-learning scenario) and low (only 10 samples are available for each training class, an even more challenging meta-learning scenario in which a model meta-learns on very few samples per training class). All experimental setups are run 5 times. For each run, we randomly select training, validation, and testing classes, as well as the samples for the low setting. We train the few-shot models for a maximum of 10, 000 C-way K-shots episodes, evaluating and testing every 100 episodes, stopping early if the evaluation\naccuracy has not progressed for at least 20 evaluations. We evaluate and test using 600 episodes, as in other few-shot works (Snell et al., 2017; Chen et al., 2019). We compare the systems in the following standard few-shot evaluation scenarios: 5-way 1-shot, and 5-way 5-shots.\nParaphrasing. At each episode, we draw U = 5 unlabeled samples to generate paraphrases from. For the back-translation baseline, we use the publicly available3 translation models from the Helsinki-NLP team. We use the following pivot languages: fr, es, it, de, nl, which yields 5 augmentations for each unlabeled sentence. For our experiments with Diverse Beam Search, we generate sentences using 15 beams, group them into 5 groups of 3 beams. In each group, we select the generated sentence which is the most different from the input sentence using BLEU as a metric for diversity. This yields M = 5 paraphrases for each unlabeled sentence, as in the back-translation baseline. DBS uses a diversity penalty parameter to penalize words that have already been generated by other beams to enforce diversity. As advised in the original DBS paper (Vijayakumar et al., 2018), we set the diversity penalty to 0.5 in our experiments, which provides diversity while limiting model hallucinations. Our Unigram Masking strategy’s masking probability is set to pmask = 0.7 found by linear search from 0 to 1 with steps of 0.1."
    }, {
      "heading" : "5.3 Evaluation of paraphrase diversity",
      "text" : "We evaluate the diversity of paraphrases for each method, and report results for two representative datasets in Table 3 (due to space limitations, the\n3https://huggingface.co/models?search=helsinki-nlp\nreport for all datasets is given in appendix B). For each paraphrasing method and each dataset, metrics are computed over unlabeled sentences and their paraphrases. To assess the diversity of paraphrases generated by the different methods, the popular BLEU metric in Neural Machine Translation is a poor choice (Bawden et al., 2020). We use the bi-gram diversity (dist-2) metric as proposed by (Ippolito et al., 2019), which computes the number of distinct 2-grams divided by the total amount of tokens. We also report the average similarity (denoted use) within each sentence set, using the Universal Sentence Encoder as an independent sentence encoder. Results show that paraphrases obtained with back-translation are too close to each other, resulting in a high sentence similarity and low bi-gram diversity. On the other hand, DBS generates more diverse sentences with a lower similarity. Our masking strategies strengthen this effect and yield even more diversity. The measured diversity strongly correlates with the average accuracy of the intent detection task (Table 4)."
    }, {
      "heading" : "5.4 Intent detection results",
      "text" : "In this section, we discuss the accuracy results for the different meta-learners, for the standard 5-way and {1, 5}-shots meta-learning scenarios, as provided in Table 4. The reported metric is the accuracy on the test set at the iteration where the validation set’s accuracy is maximal. Our DBS+unigram strategy row corresponds to the flat masking strategy, with pmask = 0.7. First, all methods augmented with unsupervised diverse paraphrasing outperform prototypical networks. However, back translation demonstrates only a limited improvement over the vanilla prototypical network due to their narrow diversity for short texts. Using paraphrases from DBS yields better results – about 0.5 points over BT, on average –, hinting that using diverse paraphrases in the unsupervised consistency loss allows the few-shot model to build more robust\nsentence representations and therefore provides improved generalization capacities. Those results are consistent across the different datasets, except for Clinic for which accuracies are all very high, making all methods hardly separable. The dataset is not challenging enough, or in other words, metalearning is robust to unbalanced short text classification problems given the nature of that dataset.\nThese results illustrate the need for unsupervised paraphrasing and show that using diverse paraphrases provide a significant performance leap. In the 1-shot (resp. 5-shot) scenario, our best meta-learner improves prototypical networks by 5.27 (resp. 2.85) points on average. Remember that these improvements are made in an unsupervised manner hence at no additional cost. Slightly different from to (Xie et al., 2020), we do not find statistical differences depending on the rate at which L̃ is annealed in PROTAUGMENT loss (α ∈ {0.25, 1, 4}), which makes it easier to tune – our unsupervised loss serves as a consistency regularization. Due to space limitations, this analysis is available in appendix D.\nAdding our masking strategies on top of DBS has a significant impact on all datasets, with the unigram variant being up about 2 points over the vanilla DBS on average. On all datasets except Clinic, given only 10 labeled samples per class (low profile), it even outperforms the supervised baseline which is given the full training data (full profile). This means that PROTAUGMENT does better than prototypical networks with much less – 15 times, and up to 47 times, depending on the dataset – labeled sentences per class. Those results indicate that our method more than compensates for the lack of labeled data and that no matter the amount of data available for the training class, there is a performance ceiling you cannot overcome without adding unsupervised knowledge from the validation and test classes. In the full profile, when given all the training data, our method greatly surpasses the Prototypical Network – 3.58 points given 1 shot, on average. Moreover, PROTAUGMENT is not only suited for the case where very little training data is available (low profile): when sampling shots from the entire training dataset (full profile), it outperforms a fully supervised baseline. Furthermore, note that our method is consistently more stable than the supervised baselines, as its average standard deviation over the different runs is much lower than the vanilla Prototypical Network."
    }, {
      "heading" : "5.5 Masking strategies",
      "text" : "We experimented with three variants of the unigram strategy (Section 4.2), each assigning a different drop chance to each token depending on its position in the input sentence. In our experiments, we did not observe any significant difference in performance when putting more weight on the first tokens (down), or last tokens (up), or the same weight on all tokens (flat) (Detailed results in appendix C). We also conducted experiments where we tune the value pmask, from 0 to 1, selecting 0.7 as the best trade-off (Figure 2). This figure also clearly shows that the Clinic dataset is one order of magnitude easier to solve than the other datasets."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we proposed PROTAUGMENT, an architecture for meta-learning for the problem of classifying user-generated short-texts (intents). We first introduced an unsupervised paraphrasing consistency loss in the prototypical network’s framework to improve its representational power. Then, while the recent diverse beam search algorithm was designed to enforce diversity between the generated paraphrases, it does not ensure diversity between the generated paraphrases and the original sentences. To make up for the latter, we introduce constraints in the diverse beam search generation, further increasing the diversity. Our thorough evaluation demonstrates that PROTAUGMENT offers a significant leap in accuracy for the most recent and\nchallenging datasets. PROTAUGMENT vastly outperforms prototypical networks, which was found to be the best meta-learning framework for shorttexts (Dopierre et al., 2021) against unsupervisedextended Prototypical Networks (Ren et al., 2018), Matching Networks (Vinyals et al., 2016), Relation Networks (Sung et al., 2018), and Induction Networks (Geng et al., 2019), thereby making PROTAUGMENT the new state-of-the-art for this task. We provide the source code of PROTAUGMENT as well as code for evaluations reported in this paper on a public repository 4\n4https://github.com/tdopierre/ProtAugment"
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are thankful for the discussion we had with Michele Bevilacqua, Marco Maru, and Roberto Navigli from Sapienza university about diversity in Natural Language Generation. We also would like to thank ANRT 5 for making partnerships between companies and universities happen."
    }, {
      "heading" : "A Diverse paraphrase samples",
      "text" : ""
    }, {
      "heading" : "B Paraphrase Diversity Evaluation",
      "text" : ""
    }, {
      "heading" : "C Masking tokens depending on their position",
      "text" : "D Loss annealing strategy"
    } ],
    "references" : [ {
      "title" : "A study in improving BLEU reference coverage with diverse automatic paraphrasing",
      "author" : [ "Rachel Bawden", "Biao Zhang", "Lisa Yankovskaya", "Andre Tättar", "Matt Post." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Bawden et al\\.,? 2020",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2020
    }, {
      "title" : "Generationary or “how we went beyond word sense inventories and learned to gloss",
      "author" : [ "Michele Bevilacqua", "Marco Maru", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Bevilacqua et al\\.,? 2020",
      "shortCiteRegEx" : "Bevilacqua et al\\.",
      "year" : 2020
    }, {
      "title" : "Balanced joint adversarial training for robust intent detection and slot filling",
      "author" : [ "Xu Cao", "Deyi Xiong", "Chongyang Shi", "Chao Wang", "Yao Meng", "Changjian Hu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4926–",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient intent detection with dual sentence encoders",
      "author" : [ "Iñigo Casanueva", "Tadas Temčinas", "Daniela Gerz", "Matthew Henderson", "Ivan Vulić." ],
      "venue" : "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38–45, On-",
      "citeRegEx" : "Casanueva et al\\.,? 2020",
      "shortCiteRegEx" : "Casanueva et al\\.",
      "year" : 2020
    }, {
      "title" : "A closer look at few-shot classification",
      "author" : [ "Wei-Yu Chen", "Yen-Cheng Liu", "Zsolt Kira", "Yu-Chiang Wang", "Jia-Bin Huang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Noisy parallel approximate decoding for conditional recurrent language model",
      "author" : [ "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1605.03835.",
      "citeRegEx" : "Cho.,? 2016",
      "shortCiteRegEx" : "Cho.",
      "year" : 2016
    }, {
      "title" : "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
      "author" : [ "Alice Coucke", "Alaa Saade", "Adrien Ball", "Bluche" ],
      "venue" : "arXiv preprint arXiv:1805.10190.",
      "citeRegEx" : "Coucke et al\\.,? 2018",
      "shortCiteRegEx" : "Coucke et al\\.",
      "year" : 2018
    }, {
      "title" : "A neural few-shot text classification reality check",
      "author" : [ "Thomas Dopierre", "Christophe Gravier", "Thomas Logerais." ],
      "venue" : "Proc. of EACL 2021.",
      "citeRegEx" : "Dopierre et al\\.,? 2021",
      "shortCiteRegEx" : "Dopierre et al\\.",
      "year" : 2021
    }, {
      "title" : "Few-shot pseudolabeling for intent detection",
      "author" : [ "Thomas Dopierre", "Christophe Gravier", "Julien Subercaze", "Wilfried Logerais." ],
      "venue" : "Proceedings of the 5https://www.anrt.asso.fr/fr",
      "citeRegEx" : "Dopierre et al\\.,? 2020",
      "shortCiteRegEx" : "Dopierre et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating meta-learning algorithms for low-resource natural language understanding tasks",
      "author" : [ "Zi-Yi Dou", "Keyi Yu", "Antonios Anastasopoulos." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Dou et al\\.,? 2019",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2019
    }, {
      "title" : "Induction networks for few-shot text classification",
      "author" : [ "Ruiying Geng", "Binhua Li", "Yongbin Li", "Xiaodan Zhu", "Ping Jian", "Jian Sun." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Geng et al\\.,? 2019",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238–252, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor O.K. Li", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631,",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Investigating learning dynamics of BERT fine-tuning",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "Proceedings of the 1st Conference of the AsiaPacific Chapter of the Association for Computational Linguistics and the 10th International Joint Confer-",
      "citeRegEx" : "Hao et al\\.,? 2020",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural language to structured query generation via meta-learning",
      "author" : [ "Po-Sen Huang", "Chenglong Wang", "Rishabh Singh", "Wentau Yih", "Xiaodong He." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Comparison of diverse decoding methods from conditional language models",
      "author" : [ "Daphne Ippolito", "Reno Kriz", "João Sedoc", "Maria Kustikova", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Ippolito et al\\.,? 2019",
      "shortCiteRegEx" : "Ippolito et al\\.",
      "year" : 2019
    }, {
      "title" : "schuBERT: Optimizing elements of BERT",
      "author" : [ "Ashish Khetan", "Zohar Karnin." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Khetan and Karnin.,? 2020",
      "shortCiteRegEx" : "Khetan and Karnin.",
      "year" : 2020
    }, {
      "title" : "Importance of search and evaluation strategies in neural dialogue modeling",
      "author" : [ "Ilia Kulikov", "Alexander Miller", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 76–87, Tokyo,",
      "citeRegEx" : "Kulikov et al\\.,? 2019",
      "shortCiteRegEx" : "Kulikov et al\\.",
      "year" : 2019
    }, {
      "title" : "An evaluation dataset for intent classification and out-of-scope",
      "author" : [ "Stefan Larson", "Anish Mahendran", "Joseph J Peper", "Christopher Clarke", "Andrew Lee", "Parker Hill", "Jonathan K Kummerfeld", "Kevin Leach", "Michael A Laurenzano", "Lingjia Tang" ],
      "venue" : null,
      "citeRegEx" : "Larson et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Larson et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised paraphrasing by simulated annealing",
      "author" : [ "Xianggen Liu", "Lili Mou", "Fandong Meng", "Hao Zhou", "Jie Zhou", "Sen Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312, Online.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Benchmarking natural language understanding services for building conversational agents",
      "author" : [ "Xingkun Liu", "Arash Eshghi", "Pawel Swietojanski", "Verena Rieser." ],
      "venue" : "arXiv preprint arXiv:1903.05566.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Dialoglue: A natural language understanding benchmark for task-oriented dialogue",
      "author" : [ "Shikib Mehri", "Mihail Eric", "Dilek Hakkani-Tur" ],
      "venue" : null,
      "citeRegEx" : "Mehri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mehri et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Metalearning for semi-supervised few-shot classification",
      "author" : [ "Mengye Ren", "Eleni Triantafillou", "Sachin Ravi", "Jake Snell", "Kevin Swersky", "Joshua B. Tenenbaum", "Hugo Larochelle", "Richard S. Zemel." ],
      "venue" : "6th International Conference on Learning Rep-",
      "citeRegEx" : "Ren et al\\.,? 2018",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural language understanding with the quora question pairs dataset",
      "author" : [ "Lakshay Sharma", "Laura Graesser", "Nikita Nangia", "Utku Evci" ],
      "venue" : null,
      "citeRegEx" : "Sharma et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Advances in neural information processing systems, pages 4077–4087.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng." ],
      "venue" : "Advances in neural information processing systems, pages 926–934.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to compare: Relation network for few-shot learning",
      "author" : [ "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip H.S. Torr", "Timothy M. Hospedales." ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Sung et al\\.,? 2018",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2018
    }, {
      "title" : "Cluster-based beam search for pointer-generator chatbot grounded by knowledge",
      "author" : [ "Yik-Cheung Tam." ],
      "venue" : "Computer Speech & Language, 64:101094.",
      "citeRegEx" : "Tam.,? 2020",
      "shortCiteRegEx" : "Tam.",
      "year" : 2020
    }, {
      "title" : "Diverse beam search for improved description of complex scenes",
      "author" : [ "Ashwin K. Vijayakumar", "Michael Cogswell", "Ramprasaath R. Selvaraju", "Qing Sun", "Stefan Lee", "David J. Crandall", "Dhruv Batra." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Confer-",
      "citeRegEx" : "Vijayakumar et al\\.,? 2018",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Daan Wierstra" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard H. Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Benchmarking natural language understanding services for building conversational agents",
      "author" : [ "Pawel Swietojanski Xingkun Liu", "Arash Eshghi", "Verena Rieser." ],
      "venue" : "Proceedings of the Tenth International Workshop on Spoken Dialogue Systems Technology",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Free lunch for few-shot learning: Distribution calibration",
      "author" : [ "Shuo Yang", "Lu Liu", "Min Xu." ],
      "venue" : "Proceedings of the 9th International Conference on Learning Representations, page (Accepted paper to appear), Online. OpenReview.",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Hypernymy detection for lowresource languages via meta learning",
      "author" : [ "Changlong Yu", "Jialong Han", "Haisong Zhang", "Wilfred Ng." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3651–3656, On-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Refining source representations with relation networks for neural machine translation",
      "author" : [ "Wen Zhang", "Jiawei Hu", "Yang Feng", "Qun Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1292–1303, Santa Fe, New",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "PAWS: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Paraphrases and applications",
      "author" : [ "Shiqi Zhao", "Haifeng Wang." ],
      "venue" : "Coling 2010: Paraphrases and Applications–Tutorial notes, pages 1–87, Beijing, China. Coling 2010 Organizing Committee.",
      "citeRegEx" : "Zhao and Wang.,? 2010",
      "shortCiteRegEx" : "Zhao and Wang.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "PROTAUGMENT is a novel extension of Prototypical Networks (Snell et al., 2017) that limits over-fitting on the bias introduced by the few-shots classification objective at each episode.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "Intent detection, a sub-field of text classification, involves classifying user-generated short-texts into intent classes, usually for conversational agents applications (Casanueva et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "versational agent applications are domain-specific, intent detection is a challenging task because of labeled data scarcity and the number of classes (intents) it usually involves (Dopierre et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 203
    }, {
      "referenceID" : 27,
      "context" : "As a consequence, recent research (Snell et al., 2017; Ren et al., 2018) considers few-shot intent detection as a meta-learning problem: the model is trained to classify user utterances from a consecutive set of small tasks named episodes.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "As a consequence, recent research (Snell et al., 2017; Ren et al., 2018) considers few-shot intent detection as a meta-learning problem: the model is trained to classify user utterances from a consecutive set of small tasks named episodes.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 37,
      "context" : "Meta-learning has successfully been applied to a wide set of NLP tasks: hypernym detection (Yu et al., 2020), low resource machine translation (Gu et al.",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : ", 2020), low resource machine translation (Gu et al., 2018), machine understanding tasks (Dou et al.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : ", 2018), machine understanding tasks (Dou et al., 2019) or structured",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "It has recently been empirically demonstrated that comparative studies in follow-up papers of (Snell et al., 2017) are debat-",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "able – for short texts classification – because of the two following main issues (Dopierre et al., 2021).",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "First, comparative studies involve simple and limited datasets in terms of number and separability of classes (SNIPS (Coucke et al., 2018), a very popu-",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "lar dataset, includes only 7 classes, with the current best model performing over 99% accuracy (Cao et al., 2020)).",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "Second, as we further better understand (Niven and Kao, 2019), fine-tune (Liu et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "Second, as we further better understand (Niven and Kao, 2019), fine-tune (Liu et al., 2019b; Hao et al., 2020) and refine (Khetan and Karnin, 2020) BERT-derived models, it is not clear if the different meta-learning frameworks can be considered state-of-the-art due to their architecture or due to the improvements of available text encoders at the time of conception.",
      "startOffset" : 73,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : ", 2020) and refine (Khetan and Karnin, 2020) BERT-derived models, it is not clear if the different meta-learning frameworks can be considered state-of-the-art due to their architecture or due to the improvements of available text encoders at the time of conception.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "(Dopierre et al., 2021) concludes that Prototypical Networks (Snell et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 27,
      "context" : ", 2021) concludes that Prototypical Networks (Snell et al., 2017) (that were using LSTM-based text encoders when introduced in NLP) are actually the state-of-the-art for intent detection when equipped with a fine-tuned BERT text encoder model.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 33,
      "context" : "In order to prevent overfitting and inspired by (Xie et al., 2020), we introduce an unsupervised diverse paraphrasing loss in the Prototypical Networks framework.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "Consequently, in this work, we transfer a denoising autoencoder pre-trained on the sequence-to-sequence task (Lewis et al., 2020) to the paraphrase generation task and then use it to generate paraphrases.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "(Goyal and Durrett, 2020) presents an approach for diverse paraphrasing that reorders the original sentence to guide the conditional language model to generate diverse sen-",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "In (Liu et al., 2020), expression diversity is part of the unsupervised paraphrasing system supported by simulated",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "Diverse decoding methods are mostly extensions to the beam search algorithm, including noise-based algorithms (Cho, 2016), iterative beam search (Kulikov et al.",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "Diverse decoding methods are mostly extensions to the beam search algorithm, including noise-based algorithms (Cho, 2016), iterative beam search (Kulikov et al., 2019), clustered beam search (Tam, 2020) and diverse beam search (Vijayakumar et al.",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : ", 2019), clustered beam search (Tam, 2020) and diverse beam search (Vijayakumar et al.",
      "startOffset" : 31,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : ", 2019), clustered beam search (Tam, 2020) and diverse beam search (Vijayakumar et al., 2018).",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "There is no clear optimal solution, the choice is task-specific and dependent on one’s tolerance for lower quality outputs as a diversity/fluency trade-off (Ippolito et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 32,
      "context" : "Matching Networks (Vinyals et al., 2016) predict the class of a query point as the average cosine dis-",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "Prototypical Networks (Snell et al., 2017) extend Matching Networks: after obtaining support vectors from the encoder, a class prototype is produced via a class-wise vector aver-",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "Like Prototypical Networks, Relation Networks (Sung et al., 2018) emerged from Computer Vision application and were later successfully applied to NLP (Zhang et al.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 38,
      "context" : ", 2018) emerged from Computer Vision application and were later successfully applied to NLP (Zhang et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 28,
      "context" : "This is achieved using either a shallow feed-forward sub-network or a Neural Tensor Layer relation module (Socher et al., 2013) (intermediate learnable matrices).",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 25,
      "context" : "Another extension to Prototypical Networks is provided in (Ren et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "In the original paper, (Snell et al., 2017) use the euclidean distance and we also observed consistent slightly worse results with the cosine distance.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "The BART (Lewis et al., 2020) model is a Transformer-based neural machine translation architecture that is trained to remove artificially corrupted text from the input thanks to an autoencoder architecture.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "original noised input, it can be fine-tuned for taskspecific conditional generation by minimizing the cross-entropy loss on new training input-output pairs (Bevilacqua et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "The paraphrase sentence pairs we use for this task are taken from 3 different paraphrase detection datasets1: Quora (Sharma et al., 2019), MSR (Zhao and Wang, 2010), and Google PAWS-Wiki (Yang et al.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 40,
      "context" : ", 2019), MSR (Zhao and Wang, 2010), and Google PAWS-Wiki (Yang et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 36,
      "context" : ", 2019), MSR (Zhao and Wang, 2010), and Google PAWS-Wiki (Yang et al., 2019; Zhang et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : ", 2019), MSR (Zhao and Wang, 2010), and Google PAWS-Wiki (Yang et al., 2019; Zhang et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "BART is pre-trained for the paraphrasing task on three datasets: Quora (Sharma et al., 2019), MSR (Zhao and Wang, 2010) and Google PAWS-Wiki (Yang et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 40,
      "context" : ", 2019), MSR (Zhao and Wang, 2010) and Google PAWS-Wiki (Yang et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 36,
      "context" : ", 2019), MSR (Zhao and Wang, 2010) and Google PAWS-Wiki (Yang et al., 2019; Zhang et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 39,
      "context" : ", 2019), MSR (Zhao and Wang, 2010) and Google PAWS-Wiki (Yang et al., 2019; Zhang et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "We consider the DialoGLUE benchmark (Mehri et al., 2020), a set of natural language understanding benchmark for task-oriented dialogue, which contains three datasets for intent detection: Banking77, HWU64 and Clinic150 –",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "Banking77 The Banking77 dataset (Casanueva et al., 2020) classifies 13, 083 user utterances related to into 77 different intents.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "Following (Mehri et al., 2020) and contrary to (Casanueva et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : ", 2020) and contrary to (Casanueva et al., 2020), we designate a validation set along a training and a testing set for that dataset (Table 1).",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "This chatbot-like style dataset was initially designed to detect out-of-scope queries, though, in our experiments, we discard the out-of-scope class and only keep the 150 labeled classes to work with, as in (Mehri et al., 2020).",
      "startOffset" : 207,
      "endOffset" : 227
    }, {
      "referenceID" : 19,
      "context" : "For the BART fine-tuning process, we used the defaults hyper-parameters reported in (Lewis et al., 2020), and we fine-tuned the BART model for a single epoch (two hours on a Titan RTX GPU).",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 27,
      "context" : "We evaluate and test using 600 episodes, as in other few-shot works (Snell et al., 2017; Chen et al., 2019).",
      "startOffset" : 68,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "We evaluate and test using 600 episodes, as in other few-shot works (Snell et al., 2017; Chen et al., 2019).",
      "startOffset" : 68,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : "As advised in the original DBS paper (Vijayakumar et al., 2018), we set the diversity penalty to 0.",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "To assess the diversity of paraphrases generated by the different methods, the popular BLEU metric in Neural Machine Translation is a poor choice (Bawden et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "We use the bi-gram diversity (dist-2) metric as proposed by (Ippolito et al., 2019), which computes the number of distinct 2-grams divided by the total amount of tokens.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : "Slightly different from to (Xie et al., 2020), we do not find statistical differences depending on the rate at which L̃ is annealed in PROTAUGMENT loss (α ∈ {0.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "PROTAUGMENT vastly outperforms prototypical networks, which was found to be the best meta-learning framework for shorttexts (Dopierre et al., 2021) against unsupervisedextended Prototypical Networks (Ren et al.",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 25,
      "context" : ", 2021) against unsupervisedextended Prototypical Networks (Ren et al., 2018), Matching Networks (Vinyals et al.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 32,
      "context" : ", 2018), Matching Networks (Vinyals et al., 2016), Relation Networks (Sung et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : ", 2016), Relation Networks (Sung et al., 2018), and Induction Networks (Geng et al.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : ", 2018), and Induction Networks (Geng et al., 2019), thereby making PROTAUGMENT the new state-of-the-art for this task.",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 2021,
    "abstractText" : "Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes. In this work, we propose PROTAUGMENT, a meta-learning algorithm for short texts classification applied to the intent detection task. PROTAUGMENT is a novel extension of Prototypical Networks (Snell et al., 2017) that limits over-fitting on the bias introduced by the few-shots classification objective at each episode. It relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode. The diverse paraphrasing is unsupervised as it is applied to unlabelled data and then fueled to the Prototypical Network training objective as a consistency loss. PROTAUGMENT is the stateof-the-art method for intent detection metalearning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain.",
    "creator" : "LaTeX with hyperref"
  }
}