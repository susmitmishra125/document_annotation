{
  "name" : "2021.acl-long.12.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking",
    "authors" : [ "Jinyu Guo", "Kai Shuang", "Jijie Li", "Zihan Wang" ],
    "emails" : [ "lijijie}@bupt.edu.cn", "zwang@tkl.iis.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 139–151\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n139"
    }, {
      "heading" : "1 Introduction",
      "text" : "Task-oriented dialogue has attracted increasing attention in both the research and industry communities. As a key component in task-oriented dialogue systems, Dialogue State Tracking (DST) aims to\n∗Corresponding author. 1Code is available at\nhttps://github.com/guojinyu88/DSSDST\nextract user goals or intents and represent them as a compact dialogue state in the form of slot-value pairs of each turn dialogue. DST is an essential part of dialogue management in task-oriented dialogue systems, where the next dialogue system action is selected based on the current dialogue state.\nEarly dialogue state tracking approaches extract value for each slot predefined in a single domain (Williams et al., 2014; Henderson et al., 2014a,b). These methods can be directly adapted to multi-domain conversations by replacing slots in a single domain with domain-slot pairs predefined. In multi-domain DST, some of the previous works study the scalability of the model (Wu et al., 2019), some aim to fully utilizing the dialogue history and context (Shan et al., 2020; Chen et al., 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al., 2020; Chen et al., 2020b). Nevertheless, existing approaches generally predict the dialogue state at every turn from scratch. The overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation.\nTo address this problem, we propose a DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. At each turn, all slots are judged by the Dual Slot Selector first, and only the selected slots are permitted to enter the Slot Value Generator to update their slot value, while the other slots directly inherit the slot value from the previous turn. The Dual Slot Selector is a two-stage judging process. It consists of a Preliminary Selector and an Ultimate Selector, which jointly make a judgment for each slot according to the current turn dialogue. The intuition behind\nthis design is that the Preliminary Selector makes a coarse judgment to exclude most of the irrelevant slots, and then the Ultimate Selector makes an intensive judgment for the slots selected by the Preliminary Selector and combines its confidence with the confidence of the Preliminary Selector to yield the final decision. Specifically, the Preliminary Selector briefly touches on the relationship of current turn dialogue utterances and each slot. Then the Ultimate Selector obtains a temporary slot value for each slot and calculates its reliability. The rationale for the Ultimate Selector is that if a slot value with high reliability can be obtained through the current turn dialogue, then the slot ought to be updated. Eventually, the selected slots enter the Slot Value Generator and a hybrid way of the extractive method and the classification-based method is utilized to generate a value according to the current dialogue utterances and dialogue history.\nOur proposed DSS-DST achieves state-of-theart joint accuracy on three of the most actively studied datasets: MultiWOZ 2.0 (Budzianowski et al., 2018), MultiWOZ 2.1 (Eric et al., 2019), and MultiWOZ 2.2 (Zang et al., 2020) with joint accuracy of 56.93%, 60.73%, and 58.04%. The results outperform the previous state-of-the-art by +2.54%, +5.43%, and +6.34%, respectively. Furthermore, a series of subsequent ablation studies and analysis are conducted to demonstrate the effectiveness of the proposed method.\nOur contributions in this paper are three folds:\n• We devise an effective DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue and the Slot Value Generator based on the dialogue history to alleviate the redundant slot value generation.\n• We propose two complementary conditions as the base of the judgment, which significantly improves the performance of the slot selection.\n• Empirical results show that our model achieves state-of-the-art performance with significant improvements."
    }, {
      "heading" : "2 Related Work",
      "text" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013;\nWilliams, 2014) or to jointly learn speech understanding (Henderson et al., 2014c; Zilka and Jurcicek, 2015; Wen et al., 2017). With the recent development of deep learning and representation learning, most works about DST focus on encoding dialogue context with deep neural networks and predicting a value for each possible slot (Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Xie et al., 2018). For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a). These models greatly improve the performance of DST, but the mechanism of treating slots equally is inefficient and may lead to additional errors. SOM-DST (Kim et al., 2020) considered the dialogue state as an explicit fixed-size memory and proposed a selectively overwriting mechanism. Nevertheless, it arguably has limitations because it lacks the explicit exploration of the relationship between slot selection and local dialogue information.\nOn the other hand, dialogue state tracking and machine reading comprehension (MRC) have similarities in many aspects (Gao et al., 2020). In MRC task, unanswerable questions are involved, some studies pay attention to this topic with straightforward solutions. (Liu et al., 2018) appended an empty word token to the context and added a simple classification layer to the reader. (Hu et al., 2019) used two types of auxiliary loss to predict plausible answers and the answerability of the question. (Zhang et al., 2020c) proposed a retrospective reader that integrates both sketchy and intensive reading. (Zhang et al., 2020b) proposed a verifier layer to context embedding weighted by start and end distribution over the context words representations concatenated to [CLS] token representation for BERT. The slot selection and the mechanism of local reliability verification in our work are inspired by the answerability prediction in machine reading comprehension."
    }, {
      "heading" : "3 The Proposed Method",
      "text" : "Figure 1 illustrates the architecture of DSSDST. DSS-DST consists of Embedding, Dual Slot Selector, and Slot Value Generator. In the task-oriented dialogue system, given a dialogue Dial = {(U1, R1); (U2, R2) . . . ; (UT , RT )} of T turns where Ut represents user utterance and Rt represents system response of turn t. We define\nthe dialogue state at turn t as Bt = {(Sj , V jt )|1 ≤ j ≤ J}, where Sj are the slots, V jt are the corresponding slot values, and J is the total number of such slots. Following (Lee et al., 2019), we use the term “slot” to refer to the concatenation of a domain name and a slot name (e.g., “restaurant−food”)."
    }, {
      "heading" : "3.1 Embedding",
      "text" : "We employ the representation of the previous turn dialog state Bt−1 concatenated to the representation of the current turn dialogue Dt as input:\nXt = [CLS]⊕Dt ⊕Bt−1 (1)\nwhere [CLS] is a special token added in front of every turn input. Following SOM-DST (Kim et al., 2020), we denote the representation of the dialogue at turn t as Dt = Rt⊕;⊕Ut ⊕ [SEP], where Rt is the system response and Ut is the user utterance. ; is a special token used to mark the boundary between Rt and Ut, and [SEP] is a special token used to mark the end of a dialogue turn. The representation of the dialogue state at turn t is Bt = B1t ⊕ . . . ⊕ BJt , where Bjt = [SLOT]\nj ⊕ Sj ⊕−⊕ V jt is the representation of the j-th slot-value pair. − is a special token\nused to mark the boundary between a slot and a value. [SLOT]j is a special token that represents the aggregation information of the j-th slot-value pair. We feed a pre-trained ALBERT (Lan et al., 2019) encoder with the input Xt. Specifically, the input text is first tokenized into subword tokens. For each token, the input is the sum of the input tokens Xt and the segment id embeddings. For the segment id, we use 0 for the tokens that belong to Bt−1 and 1 for the tokens that belong to Dt.\nThe output representation of the encoder is Ot ∈ R|Xt|×d, and h[CLS]t , h [SLOT]j\nt ∈ Rd are the outputs that correspond to [CLS] and [SLOT]j , respectively. To obtain the representation of each dialogue and state, we split the Ot into Ht and HBt−1 as the output representations of the dialogue at turn t and the dialogue state at turn t− 1."
    }, {
      "heading" : "3.2 Dual Slot Selector",
      "text" : "The Dual Slot Selector consists of a Preliminary Selector and an Ultimate Selector, which jointly make a judgment for each slot according to the current turn dialogue.\nSlot-Aware Matching Here we first describe the Slot-Aware Matching (SAM) layer, which will be\nused as the subsequent components. The slot can be regarded as a special category of questions, so inspired by the previous success of explicit attention matching between passage and question in MRC (Kadlec et al., 2016; Dhingra et al., 2017; Wang et al., 2017; Seo et al., 2016), we feed a representation H and the output representation h[SLOT] j\nt\nat turn t to the Slot-Aware Matching layer by taking the slot presentation as the attention to the representation H:\nSAM(H, j, t) = softmax(H(h [SLOT]j\nt ) ᵀ) (2)\nThe output represents the correlation between each position of H and the j-th slot at turn t.\nPreliminary Selector The Preliminary Selector briefly touches on the relationship of current turn dialogue utterances and each slot to make an initial judgment. For the j-th slot (1 ≤ j ≤ J) at turn t, we feed its output representation h[SLOT] j\nt and the dialogue representation Ht to the SAM as follows:\nαjt=SAM(Ht, j, t) (3)\nwhere αjt ∈ RN×1 denotes the correlation between each position of the dialogue and the j-th slot at turn t. Then we get the aggregated dialogue representation Hjt ∈ RN×d and passed it to a fully connected layer to get classification the j-th slot’s logits ŷjt composed of selected (logit sel i t) and fail (logit faijt ) elements as follows:\nHjt ,m = α j t ,mHt,m , 0 ≤ m < N (4)\nŷjt = softmax(FC(H j t )) (5)\nWe calculate the difference as the Preliminary Selector score for the j-th slot at turn t: Pre scorejt = logit seljt− logit fai j t , and define the set of the slot indices as U1,t = {j|Pre scorejt > 0}, and its size as J1,t = |U1,t|. In the next paragraph, the slot in U1,t will be processed as the target object of the Ultimate Selector.\nUltimate Selector The Ultimate Selector will make the judgment on the slots in U1,t. The mechanism of the Ultimate Selector is to obtain a temporary slot value for the slot and calculate its reliability through the dialogue at turn t as its confidence for each slot. Specifically, for the j-th slot in U1,t (1 ≤ j ≤ J1,t), we first attempt to obtain the temporary slot value ϕjt using the extractive method: We employ two different linear layers and feed Ht\nas the input to obtain the representation H st and H et for predicting the start and end, respectively. Then we feed them to the SAM with the j-th slot to obtain the correlation representation α sjt and α ejt as follows:\nH st =W s tHt (6) H et =W e tHt (7)\nα sjt = SAM(H st, j, t) (8) α ejt = SAM(H et, j, t) (9)\nThe position of the maximum value in α sjt and α ejt will be the start and end predictions of ϕ j t :\npsjt = argmax m (α sjt ,m ) (10) pejt = argmax m (α ejt ,m ) (11)\nϕjt = Dialt[ps j t : pe j t ] (12)\nHere we define Vj , the candidate value set of the j-th slot. If ϕjt belongs to Vj , we calculate its proportion of all possible extracted temporary slot values and calculate the Ult scorejt as the score of the j-th slot:\nlogit spanjt = exp(α sjt [ps j t ] +α e j t [pe j t ]) N−1∑ p1=0 N−1∑ p2=p1+1 exp(α sjt [p1] +α e j t [p2])\n(13)\nlogit nulljt = exp(α sjt [0] +α e j t [0]) N−1∑ p1=0 N−1∑ p2=p1+1 exp(α sjt [p1] +α e j t [p2])\n(14)\nUlt scorejt = logit span j t − logit null j t (15)\nIf ϕjt does not belong to Vj , we employ the classification-based method instead to select a temporary slot value from Vj . Specifically, the dialogue representation Hjt is passed to a fully connected layer to get the distribution of Vj . We choose the candidate slot value corresponding to the maximum value as the new temporary slot value ϕjt , and calculate the distribution probability difference between ϕjt and “None” as the Ult score j t :\nα cjt = softmax(FC(H j t )) (16)\nmaxc = argmax m\n(α cjt ,m ) (17)\nUlt scorejt = α c j t [maxc]−α c j t [0] (18)\nWe choose 0 as index because Vj [0] = “None”.\nThreshold-based decision Following previous studies (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), we adopt the threshold-based decision to make the final judgment for each slot in U1,t. The slot-selected threshold δ is set and determined in our model. The total score of the j-th slot is the combination of the predicted Preliminary Selector’s score and the predicted Ultimate Selector’s score:\nTotal scorejt = βPre score j t+(1−β)Ult score j t (19) where β is the weight. We define the set of the slot indices as U2,t = {j|Total scorejt > δ}, and its size as J2,t = |U2,t|. The slot in U2,t will enter the Slot Value Generator to update the slot value."
    }, {
      "heading" : "3.3 Slot Value Generator",
      "text" : "After the judgment of the Dual Slot Selector, the slots in U2,t are the final selected slots. For each j-th slot in U2,t, the Slot Value Generator generates a value for it. Conversely, the slots that are not in U2,t will inherit the slot value of the previous turn (i.e., V it = V i t−1, 1 ≤ i ≤ J − J2,t). For the sake of simplicity, we sketch the process as follows because this module utilizes the same hybrid way of the extractive method and the classification-based method as in the Ultimate Selector:\nX gt = [CLS]⊕Dt ⊕ · · · ⊕Dt−k+1 ⊕Bt−1 (20)\nH gt = Embedding(X gt) (21)\nϕ gjt = Ext method(H gt), 1 ≤ j ≤ J2,t (22) V jt = ϕ g j t , ϕ g j t ∈ Vj (23)\nV jt = Cls method(H gt) , ϕ g j t /∈ Vj (24)\nSignificantly, the biggest difference between the Slot Value Generator and the Ultimate Selector is that the input utterances of the Slot Value Generator are the dialogues of the previous k − 1 turns and the current turn, while the Ultimate Selector only utilizes the current turn dialogue as the input utterances."
    }, {
      "heading" : "3.4 Optimization",
      "text" : "During training, we optimize both Dual Slot Selector and Slot Value Generator.\nPreliminary Selector We use cross-entropy as a training objective:\nLpre,t = − 1\nJ J∑ j=1 [yjt log ŷ j t +(1− y j t ) log(1− ŷit)]\n(25) where ŷjt denotes the prediction and y j t is the target indicating whether the slot is selected.\nUltimate Selector The training objectives of both extractive method and classification-based method are defined as cross-entropy loss:\nLext,t = − 1\nJ1,t J1,t∑ j log(logit pjt ) (26)\nLcls,t = − 1\nJ1,t J1,t∑ j |Vj |∑ i y cjt,i logα c j t,i (27)\nwhere logit pjt is the target indicating the proportion of all possible extracted temporary slot values which is calculated according to the form of Equation 13, and y cjt,i is the target indicating the probability of candidate values.\nSlot Value Generator The training objective Lgen,t of this module has the same form of training objective as in the Ultimate Selector."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Metrics",
      "text" : "We choose MultiWOZ 2.0 (Budzianowski et al., 2018), MultiWOZ 2.1 (Eric et al., 2019), and the latest MultiWOZ 2.2 (Zang et al., 2020) as our training and evaluation datasets. These are the three largest publicly available multi-domain taskoriented dialogue datasets, including over 10,000 dialogues, 7 domains, and 35 domain-slot pairs. MultiWOZ 2.1 fixes the previously existing annotation errors. MultiWOZ 2.2 is the latest version of this dataset. It identifies and fixes the annotation errors of dialogue states on MultiWOZ2.1, solves the inconsistency of state updates and the problems of ontology, and redefines the dataset by dividing all slots into two types: non-categorical and categorical. In conclusion, it helps make a fair comparison between different models and will be crucial in the future research of this field.\nFollowing TRADE (Wu et al., 2019), we use five domains for training, validation, and testing, including restaurant, train, hotel, taxi, attraction.\nThese domains contain 30 slots (i.e., J = 30). We use joint accuracy and slot accuracy as evaluation metrics. Joint accuracy refers to the accuracy of the dialogue state in each turn. Slot accuracy only considers individual slot-level accuracy."
    }, {
      "heading" : "4.2 Baseline Models",
      "text" : "We compare the performance of DSS-DST with the following competitive baselines: DSTreader formulates the problem of DST as an extractive QA task and extracts the value of the slots from the input as a span (Gao et al., 2019). TRADE encodes the whole dialogue context and decodes the value for every slot using a copyaugmented decoder (Wu et al., 2019). NADST uses a Transformer-based non-autoregressive decoder to generate the current turn dialogue state (Le et al., 2019). PIN integrates an interactive encoder to jointly model the in-turn dependencies and crossturn dependencies (Chen et al., 2020a). DS-DST uses two BERT-base encoders and takes a hybrid approach (Zhang et al., 2020a). SAS proposes a Dialogue State Tracker with Slot Attention and Slot Information Sharing to reduce redundant information’s interference (Hu et al., 2020). SOM-DST considers the dialogue state as an explicit fixedsize memory and proposes a selectively overwriting mechanism (Kim et al., 2020). DST-Picklist performs matchings between candidate values and slot-context encoding by considering all slots as picklist-based slots (Zhang et al., 2020a). SST proposes a schema-guided multi-domain dialogue state tracker with graph attention networks (Chen et al., 2020b). TripPy extracts all values from the dialog context by three copy mechanisms (Heck et al., 2020)."
    }, {
      "heading" : "4.3 Training",
      "text" : "We employ a pre-trained ALBERT-large-uncased model (Lan et al., 2019) for the encoder of each part. The hidden size of the encoder d is 1024. We use AdamW optimizer (Loshchilov and Hutter, 2018) and set the warmup proportion to 0.01 and L2 weight decay of 0.01. We set the peak learning rate to 0.03 for the Preliminary Selector and 0.0001 for the Ultimate Selector and the Slot Value Generator, respectively. The max-gradient normalization is utilized and the threshold of gradient clipping is set to 0.1. We use a batch size of 8 and set the dropout (Srivastava et al., 2014) rate to 0.1. In addition, we utilize word dropout (Bowman et al., 2016) by randomly replacing the input tokens with\nthe special [UNK] token with the probability of 0.1. The max sequence length for all inputs is fixed to 256.\nWe train the Preliminary Selector for 10 epochs and train the Ultimate Selector and the Slot Value Generator for 30 epochs. During training the Slot Value Generator, we use the ground truth selected slots instead of the predicted ones. We set k to 2, β to 0.55, and δ to 0. For all experiments, we report the mean joint accuracy over 10 different random seeds to reduce statistical errors."
    }, {
      "heading" : "5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 Main Results",
      "text" : "Table 1 shows the joint accuracy and the slot accuracy of our model and other baselines on the test sets of MultiWOZ 2.0, 2.1, and 2.2. As shown in the table, our model achieves state-of-the-art performance on three datasets with joint accuracy of 56.93%, 60.73%, and 58.04%, which has a significant improvement over the previous best joint accuracy. Particularly, the joint accuracy on MultiWOZ 2.1 beyond 60%. Despite the sparsity of experimental result on MultiWOZ 2.2, our model still leads by a large margin in the existing public models. Similar to (Kim et al., 2020), our model achieves higher joint accuracy on MultiWOZ 2.1 than that on MultiWOZ 2.0. For MultiWOZ 2.2, the joint accuracy of categorical slots is higher than that of non-categorical slots. This is because we utilize the hybrid way of the extractive method and the classification-based method to treat categorical slots. However, we can only utilize the extractive method for non-categorical slots since they have no ontology (i.e., candidate value set)."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "Pre-trained Language Model For a fair comparison, we employ different pre-trained language models with different scales as encoders for training and testing on MultiWOZ 2.1 dataset. As shown in Table 2, the joint accuracy of other implemented ALBERT and BERT encoders decreases in varying degrees. In particular, the joint accuracy of BERT-base-uncased decreased by 1.38%, but still outperformed the previous state-of-the-art performance on MultiWOZ 2.1. The result demonstrates the effectiveness of DSS-DST.\nSeparate Slot Selector To explore the effectiveness of the Preliminary Selector and Ultimate Selector respectively, we conduct an ablation study\nof the two slot selectors on MultiWOZ 2.1. As shown in Table 3, we observe that the performance of the separate Preliminary Selector is better than that of the separate Ultimate Selector. This is presumably because the Preliminary Selector is the head of the Dual Slot Selector, it is stable when it handles all slots. Nevertheless, the input of the Ultimate Selector is the slots selected by the Preliminary Selector, and its function is to make a refined judgment. Therefore, it will be more vulnerable when handling all the slots independently. In addition, when the two selectors are removed, the performance drops drastically. This demonstrates\nthat the slot selection is integral before slot value generation.\nDialogue History for the Dual Slot Selector As aforementioned, we consider that the slot selection only depends on the current turn dialogue. In order to verify it, we attach the dialogue of the previous turn to the current turn dialogue as the input of the Dual Slot Selector. We observe in Table 4 that the joint accuracy decreases by 2.37%, which implies the redundant information of dialogue history confuse the slot selection in the current turn.\nDialogue History for the Slot Value Generator We try the number from one to three for the k to observe the influence of the selected dialogue history on the Slot Value Generator. As shown in Table 5, the model achieves better performance on MultiWOZ 2.1 when k = 2, 3 than that of k = 1. Furtherly, the performance of k = 2 is better than that of k = 3. We conjecture that the dialogue history far away from the current turn is little helpful because the relevance between two sentences in dialogue is strongly related to their positions.\nThe above ablation studies show that dialogue history confuses the Dual Slot Selector, but it plays a crucial role in the Slot Value Generator. This demonstrates that there are fundamental differences between the two processes, and confirms the necessity of dividing DST into these two sub-tasks."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Comparative Analysis of Slot Selector",
      "text" : "We analyze the performance of the Dual Slot Selector and compare it with other previous work in MultiWOZ 2.1. Here we choose the SOM-DST and list the state operations and the corresponding F1 scores as a comparison. The SOM-DST sets four state operations (i.e., CARRYOVER, DELETE, DONTCARE, UPDATE), while our model classifies the slots into two classes (i.e., inherit and\nupdate). It means that DELETE, DONTCARE, and UPDATE in SOM-DST all correspond to update in our model. As shown in Table 6, our model still achieves superior performance when dealing with update slots, which contain DONTCARE, DELETE, and other difficult cases."
    }, {
      "heading" : "6.2 Domains and Ontology",
      "text" : "Table 7 shows the domain-specific results of our model on the latest MultiWOZ 2.2 dataset. We can observe that the performance of our model in taxi domain is lower than that of the other four domains. We investigate the dataset and find that all the slots in taxi domain are non-categorical slots. This indicates the reason that we can only utilize the extractive method for non-categorical slots since they have no ontology. Furthermore, we test the performance of using the separate classificationbased method for categorical slots. As illustrated in Table 8, the joint accuracy of our model and categorical slots decreased by 8.03% and 10.17%, respectively."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We introduce an effective two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update or to inherit based on the two conditions. The Slot Value Generator employs a hybrid method to generate new values for the slots selected to be updated according to the dialogue history. Our model achieves state-of-the-art performance of 56.93%, 60.73%, and 58.04% joint accuracy with significant improvements (+2.54%, +5.43%, and +6.34%) over previous best results on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets, respectively. The mechanism of a hybrid method is a promising research direction and we will exploit a more comprehensive and efficient hybrid method for slot value generation in the future."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the National key research and development project (2017YFB1400603) and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China (Grant No. 61921003). We thank the anonymous reviewers for their insightful comments.\nEthical Considerations\nThe claims in this paper match the experimental results. The model utilizes the hybrid method for slot value generation, so it is universal and scalable to unseen domains, slots, and values. The experimental results can be expected to generalize."
    }, {
      "heading" : "A Accuracy per Slot on MultiWOZ 2.2 Testset",
      "text" : "B Data Statistics\nDialogues Turns Domain Slots Train Valid Test Train Valid Test\nHotel\nprice range, type,\nparking, book stay, book day,\nbook people, area, stars, internet,\nname\n3,381 416 394 14,793 1,781 1,756\nAttraction area, name,\ntype 2,717 401 395 8,073 1,220 1,256\nRestaurant food, price range, area, name, book time, book day, book\npeople\n3,813 438 437 15,367 1,708 1,726\nTaxi\nleave at, destination, departure, arrive by 1,654 207 195 4,618 690 654\nTrain\ndestination, day,\ndeparture, arrive by,\nbook people, leave at\n3,103 484 494 12,133 1,972 1,976\nTable 10: Data statistics of MultiWOZ 2.1."
    } ],
    "references" : [ {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Multiwoz–a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Inigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gašić." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Parallel interactive networks for multidomain dialogue state generation",
      "author" : [ "Junfan Chen", "Richong Zhang", "Yongyi Mao", "Jie Xu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1921–",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Schema-guided multi-domain dialogue state tracking with graph attention neural networks",
      "author" : [ "Lu Chen", "Boer Lv", "Chi Wang", "Su Zhu", "Bowen Tan", "Kai Yu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7521–7528.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Gatedattention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Adarsh Kumar", "Abhishek Sethi", "Peter Ku", "Anuj Kumar Goyal", "Sanchit Agarwal", "Shuyang Gao", "Dilek Hakkani-Tur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2019
    }, {
      "title" : "From machine reading comprehension to dialogue state tracking: Bridging the gap",
      "author" : [ "Shuyang Gao", "Sanchit Agarwal", "Di Jin", "Tagyoung Chung", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 2nd Workshop on Natural Language Processing for Con-",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Dialog state tracking: A neural reading comprehension approach",
      "author" : [ "Shuyang Gao", "Abhishek Sethi", "Sanchit Agarwal", "Tagyoung Chung", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 264–",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Trippy: A triple copy strategy for value independent neural dialog state tracking",
      "author" : [ "Michael Heck", "Carel van Niekerk", "Nurul Lubis", "Christian Geishauser", "Hsien-Chin Lin", "Marco Moresi", "Milica Gasic." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the",
      "citeRegEx" : "Heck et al\\.,? 2020",
      "shortCiteRegEx" : "Heck et al\\.",
      "year" : 2020
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), pages 263–272.",
      "citeRegEx" : "Henderson et al\\.,? 2014a",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "The third dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "2014 IEEE Spoken Language Technology Workshop (SLT), pages 324–329. IEEE.",
      "citeRegEx" : "Henderson et al\\.,? 2014b",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Word-based dialog state tracking with recurrent neural networks",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),",
      "citeRegEx" : "Henderson et al\\.,? 2014c",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Sas: Dialogue state tracking via slot attention and slot information sharing",
      "author" : [ "Jiaying Hu", "Yan Yang", "Chencai Chen", "Zhou Yu" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Read+ verify: Machine reading comprehension with unanswerable questions",
      "author" : [ "Minghao Hu", "Furu Wei", "Yuxing Peng", "Zhen Huang", "Nan Yang", "Dongsheng Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6529–",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Text understanding with the attention sum reader network",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondřej Bajgar", "Jan Kleindienst." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Kadlec et al\\.,? 2016",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient dialogue state tracking by selectively overwriting memory",
      "author" : [ "Sungdong Kim", "Sohee Yang", "Gyuwan Kim", "SangWoo Lee." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 567–582.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Non-autoregressive dialog state tracking",
      "author" : [ "Hung Le", "Richard Socher", "Steven CH Hoi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Le et al\\.,? 2019",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "Sumbt: Slot-utterance matching for universal and scalable belief tracking",
      "author" : [ "Hwaran Lee", "Jinsik Lee", "Tae-Yoon Kim." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5478–5483.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Stochastic answer networks for squad 2.0",
      "author" : [ "Xiaodong Liu", "Wei Li", "Yuwei Fang", "Aerin Kim", "Kevin Duh", "Jianfeng Gao" ],
      "venue" : "arXiv preprint arXiv:1809.09194",
      "citeRegEx" : "Liu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : null,
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2018\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Modeling long context for task-oriented dialogue state generation",
      "author" : [ "Jun Quan", "Deyi Xiong." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7119– 7124.",
      "citeRegEx" : "Quan and Xiong.,? 2020",
      "shortCiteRegEx" : "Quan and Xiong.",
      "year" : 2020
    }, {
      "title" : "Large-scale multi-domain belief tracking with knowledge sharing",
      "author" : [ "Osman Ramadan", "Paweł Budzianowski", "Milica Gasic." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Ramadan et al\\.,? 2018",
      "shortCiteRegEx" : "Ramadan et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards universal dialogue state tracking",
      "author" : [ "Liliang Ren", "Kaige Xie", "Lu Chen", "Kai Yu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2780– 2786.",
      "citeRegEx" : "Ren et al\\.,? 2018",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2018
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi" ],
      "venue" : null,
      "citeRegEx" : "Seo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2016
    }, {
      "title" : "A contextual hierarchical attention network with adaptive",
      "author" : [ "Yong Shan", "Zekang Li", "Jinchao Zhang", "Fandong Meng", "Yang Feng", "Cheng Niu", "Jie Zhou" ],
      "venue" : null,
      "citeRegEx" : "Shan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2020
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems",
      "author" : [ "Blaise Thomson", "Steve Young." ],
      "venue" : "Computer Speech & Language, 24(4):562–588.",
      "citeRegEx" : "Thomson and Young.,? 2010",
      "shortCiteRegEx" : "Thomson and Young.",
      "year" : 2010
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information",
      "author" : [ "Zhuoran Wang", "Oliver Lemon." ],
      "venue" : "Proceedings of the SIGDIAL 2013 Conference, pages 423–432.",
      "citeRegEx" : "Wang and Lemon.,? 2013",
      "shortCiteRegEx" : "Wang and Lemon.",
      "year" : 2013
    }, {
      "title" : "A network-based end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina Maria Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve J Young." ],
      "venue" : "EACL (1).",
      "citeRegEx" : "Wen et al\\.,? 2017",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "Web-style ranking and slu combination for dialog state tracking",
      "author" : [ "Jason D Williams." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 282–291.",
      "citeRegEx" : "Williams.,? 2014",
      "shortCiteRegEx" : "Williams.",
      "year" : 2014
    }, {
      "title" : "The dialog state tracking challenge series",
      "author" : [ "Jason D Williams", "Matthew Henderson", "Antoine Raux", "Blaise Thomson", "Alan Black", "Deepak Ramachandran." ],
      "venue" : "AI Magazine, 35(4):121–124.",
      "citeRegEx" : "Williams et al\\.,? 2014",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2014
    }, {
      "title" : "Partially observable markov decision processes for spoken dialog systems",
      "author" : [ "Jason D Williams", "Steve Young." ],
      "venue" : "Computer Speech & Language, 21(2):393–422.",
      "citeRegEx" : "Williams and Young.,? 2007",
      "shortCiteRegEx" : "Williams and Young.",
      "year" : 2007
    }, {
      "title" : "Transferable multi-domain state generator for task-oriented dialogue systems",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Ehsan HosseiniAsl", "Caiming Xiong", "Richard Socher", "Pascale Fung." ],
      "venue" : "arXiv preprint arXiv:1905.08743.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Cost-sensitive active learning for dialogue state tracking",
      "author" : [ "Kaige Xie", "Cheng Chang", "Liliang Ren", "Lu Chen", "Kai Yu." ],
      "venue" : "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 209–213.",
      "citeRegEx" : "Xie et al\\.,? 2018",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2018
    }, {
      "title" : "An end-to-end approach for handling unknown slot values in dialogue state tracking",
      "author" : [ "Puyang Xu", "Qi Hu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1448–1457.",
      "citeRegEx" : "Xu and Hu.,? 2018",
      "shortCiteRegEx" : "Xu and Hu.",
      "year" : 2018
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, 32:5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines",
      "author" : [ "Xiaoxue Zang", "Abhinav Rastogi", "Jindong Chen" ],
      "venue" : "In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI,",
      "citeRegEx" : "Zang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking",
      "author" : [ "Jianguo Zhang", "Kazuma Hashimoto", "Chien-Sheng Wu", "Yao Wang", "S Yu Philip", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the Ninth Joint Con-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sg-net: Syntax-guided machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Junru Zhou", "Sufeng Duan", "Hai Zhao", "Rui Wang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9636–9643.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrospective reader for machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Junjie Yang", "Hai Zhao." ],
      "venue" : "arXiv preprint arXiv:2001.09694.",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Global-locally self-attentive encoder for dialogue state tracking",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1458–",
      "citeRegEx" : "Zhong et al\\.,? 2018",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2018
    }, {
      "title" : "Incremental lstmbased dialog state tracker",
      "author" : [ "Lukas Zilka", "Filip Jurcicek." ],
      "venue" : "2015 Ieee Workshop on Automatic Speech Recognition and Understanding (Asru), pages 757–762. IEEE.",
      "citeRegEx" : "Zilka and Jurcicek.,? 2015",
      "shortCiteRegEx" : "Zilka and Jurcicek.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "In multi-domain DST, some of the previous works study the scalability of the model (Wu et al., 2019), some aim to fully utilizing the dialogue history and context (Shan et al.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : ", 2019), some aim to fully utilizing the dialogue history and context (Shan et al., 2020; Chen et al., 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al.",
      "startOffset" : 70,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : ", 2019), some aim to fully utilizing the dialogue history and context (Shan et al., 2020; Chen et al., 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al.",
      "startOffset" : 70,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : ", 2019), some aim to fully utilizing the dialogue history and context (Shan et al., 2020; Chen et al., 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al.",
      "startOffset" : 70,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : ", 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al., 2020; Chen et al., 2020b).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : ", 2020a; Quan and Xiong, 2020), and some attempt to explore the relationship between different slots (Hu et al., 2020; Chen et al., 2020b).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 35,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al.",
      "startOffset" : 162,
      "endOffset" : 251
    }, {
      "referenceID" : 29,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al.",
      "startOffset" : 162,
      "endOffset" : 251
    }, {
      "referenceID" : 31,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al.",
      "startOffset" : 162,
      "endOffset" : 251
    }, {
      "referenceID" : 33,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al.",
      "startOffset" : 162,
      "endOffset" : 251
    }, {
      "referenceID" : 12,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014c; Zilka and Jurcicek, 2015; Wen et al., 2017).",
      "startOffset" : 293,
      "endOffset" : 362
    }, {
      "referenceID" : 45,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014c; Zilka and Jurcicek, 2015; Wen et al., 2017).",
      "startOffset" : 293,
      "endOffset" : 362
    }, {
      "referenceID" : 32,
      "context" : "Traditional statistical dialogue state tracking models combine semantics extracted by spoken language understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014c; Zilka and Jurcicek, 2015; Wen et al., 2017).",
      "startOffset" : 293,
      "endOffset" : 362
    }, {
      "referenceID" : 38,
      "context" : "With the recent development of deep learning and representation learning, most works about DST focus on encoding dialogue context with deep neural networks and predicting a value for each possible slot (Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Xie et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 275
    }, {
      "referenceID" : 44,
      "context" : "With the recent development of deep learning and representation learning, most works about DST focus on encoding dialogue context with deep neural networks and predicting a value for each possible slot (Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Xie et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 275
    }, {
      "referenceID" : 25,
      "context" : "With the recent development of deep learning and representation learning, most works about DST focus on encoding dialogue context with deep neural networks and predicting a value for each possible slot (Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Xie et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 275
    }, {
      "referenceID" : 37,
      "context" : "With the recent development of deep learning and representation learning, most works about DST focus on encoding dialogue context with deep neural networks and predicting a value for each possible slot (Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Xie et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 275
    }, {
      "referenceID" : 24,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 36,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 3,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 41,
      "context" : "For multi-domain DST, slot-value pairs are extended to domain-slot-value pairs for the target (Ramadan et al., 2018; Gao et al., 2019; Wu et al., 2019; Chen et al., 2020b; Hu et al., 2020; Heck et al., 2020; Zhang et al., 2020a).",
      "startOffset" : 94,
      "endOffset" : 228
    }, {
      "referenceID" : 16,
      "context" : "SOM-DST (Kim et al., 2020) considered the dialogue state as an explicit fixed-size memory and proposed a selectively overwriting mechanism.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, dialogue state tracking and machine reading comprehension (MRC) have similarities in many aspects (Gao et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "(Liu et al., 2018) appended an empty word token to the context and added a simple classification layer to the reader.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "(Hu et al., 2019) used two types of auxiliary loss to predict plausible answers and the answerability of the question.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 43,
      "context" : "(Zhang et al., 2020c) proposed a retrospective reader that integrates both sketchy and intensive reading.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 42,
      "context" : "(Zhang et al., 2020b) proposed a verifier layer to context embedding weighted by start and end distribution over the context words representations concatenated to [CLS] token representation for BERT.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "Following (Lee et al., 2019), we use the term “slot” to refer to the concatenation of a domain name and a slot name (e.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Following SOM-DST (Kim et al., 2020), we denote the representation of the dialogue at turn t as Dt = Rt⊕;⊕Ut ⊕ [SEP], where Rt is the system response and Ut is the user utterance.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : "We feed a pre-trained ALBERT (Lan et al., 2019) encoder with the input Xt.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "The slot can be regarded as a special category of questions, so inspired by the previous success of explicit attention matching between passage and question in MRC (Kadlec et al., 2016; Dhingra et al., 2017; Wang et al., 2017; Seo et al., 2016), we feed a representation H and the output representation h j t at turn t to the Slot-Aware Matching layer by taking the slot presentation as the attention to the representation H:",
      "startOffset" : 164,
      "endOffset" : 244
    }, {
      "referenceID" : 5,
      "context" : "The slot can be regarded as a special category of questions, so inspired by the previous success of explicit attention matching between passage and question in MRC (Kadlec et al., 2016; Dhingra et al., 2017; Wang et al., 2017; Seo et al., 2016), we feed a representation H and the output representation h j t at turn t to the Slot-Aware Matching layer by taking the slot presentation as the attention to the representation H:",
      "startOffset" : 164,
      "endOffset" : 244
    }, {
      "referenceID" : 30,
      "context" : "The slot can be regarded as a special category of questions, so inspired by the previous success of explicit attention matching between passage and question in MRC (Kadlec et al., 2016; Dhingra et al., 2017; Wang et al., 2017; Seo et al., 2016), we feed a representation H and the output representation h j t at turn t to the Slot-Aware Matching layer by taking the slot presentation as the attention to the representation H:",
      "startOffset" : 164,
      "endOffset" : 244
    }, {
      "referenceID" : 26,
      "context" : "The slot can be regarded as a special category of questions, so inspired by the previous success of explicit attention matching between passage and question in MRC (Kadlec et al., 2016; Dhingra et al., 2017; Wang et al., 2017; Seo et al., 2016), we feed a representation H and the output representation h j t at turn t to the Slot-Aware Matching layer by taking the slot presentation as the attention to the representation H:",
      "startOffset" : 164,
      "endOffset" : 244
    }, {
      "referenceID" : 4,
      "context" : "143 Threshold-based decision Following previous studies (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), we adopt the threshold-based decision to make the final judgment for each slot in U1,t.",
      "startOffset" : 56,
      "endOffset" : 132
    }, {
      "referenceID" : 39,
      "context" : "143 Threshold-based decision Following previous studies (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), we adopt the threshold-based decision to make the final judgment for each slot in U1,t.",
      "startOffset" : 56,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "143 Threshold-based decision Following previous studies (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), we adopt the threshold-based decision to make the final judgment for each slot in U1,t.",
      "startOffset" : 56,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "143 Threshold-based decision Following previous studies (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), we adopt the threshold-based decision to make the final judgment for each slot in U1,t.",
      "startOffset" : 56,
      "endOffset" : 132
    }, {
      "referenceID" : 40,
      "context" : "2 (Zang et al., 2020) as our training and evaluation datasets.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 36,
      "context" : "Following TRADE (Wu et al., 2019), we use five domains for training, validation, and testing, including restaurant, train, hotel, taxi, attraction.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "following competitive baselines: DSTreader formulates the problem of DST as an extractive QA task and extracts the value of the slots from the input as a span (Gao et al., 2019).",
      "startOffset" : 159,
      "endOffset" : 177
    }, {
      "referenceID" : 36,
      "context" : "TRADE encodes the whole dialogue context and decodes the value for every slot using a copyaugmented decoder (Wu et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "NADST uses a Transformer-based non-autoregressive decoder to generate the current turn dialogue state (Le et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "PIN integrates an interactive encoder to jointly model the in-turn dependencies and crossturn dependencies (Chen et al., 2020a).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 41,
      "context" : "DS-DST uses two BERT-base encoders and takes a hybrid approach (Zhang et al., 2020a).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "Information Sharing to reduce redundant information’s interference (Hu et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "SOM-DST considers the dialogue state as an explicit fixedsize memory and proposes a selectively overwriting mechanism (Kim et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "performs matchings between candidate values and slot-context encoding by considering all slots as picklist-based slots (Zhang et al., 2020a).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "SST proposes a schema-guided multi-domain dialogue state tracker with graph attention networks (Chen et al., 2020b).",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "TripPy extracts all values from the dialog context by three copy mechanisms (Heck et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "We employ a pre-trained ALBERT-large-uncased model (Lan et al., 2019) for the encoder of each part.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : "We use AdamW optimizer (Loshchilov and Hutter, 2018) and set the warmup proportion to 0.",
      "startOffset" : 23,
      "endOffset" : 52
    }, {
      "referenceID" : 28,
      "context" : "We use a batch size of 8 and set the dropout (Srivastava et al., 2014) rate to 0.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "In addition, we utilize word dropout (Bowman et al., 2016) by randomly replacing the input tokens with the special [UNK] token with the probability of 0.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "Similar to (Kim et al., 2020), our model achieves higher joint accuracy on MultiWOZ 2.",
      "startOffset" : 11,
      "endOffset" : 29
    } ],
    "year" : 2021,
    "abstractText" : "The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements. 1",
    "creator" : "LaTeX with hyperref"
  }
}