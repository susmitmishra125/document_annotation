{
  "name" : "2021.acl-long.442.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CoSQA: 20,000+ Web Queries for Code Search and Question Answering",
    "authors" : [ "Junjie Huang", "Duyu Tang", "Linjun Shou", "Ming Gong", "Ke Xu", "Daxin Jiang", "Ming Zhou", "Nan Duan" ],
    "emails" : [ "huangjunjie@buaa.edu.cn,", "kexu@nlsde.buaa.edu.cn", "dutang@microsoft.com", "lisho@microsoft.com", "migon@microsoft.com", "djiang@microsoft.com", "nanduan@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5690–5700\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5690"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the growing population of software developers, natural language code search, which improves the productivity of the development process via retrieving semantically relevant code given natural language queries, is increasingly important in both communities of software engineering and natural language processing (Allamanis et al., 2018; Liu et al., 2020a). The key challenge is how to effectively measure the semantic similarity between a natural language query and a code.\nThere are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space. However, these models are\n∗Work done during internship at Microsoft Research Asia. 1The CoSQA data and leaderboard are available at https://github.com/microsoft/CodeXGLUE/tree/main/TextCode/NL-code-search-WebQuery. The code is available at https://github.com/Jun-jie-Huang/CoCLR\nmostly trained on pseudo datasets in which a natural language query is either the documentation of a function or a tedious question from Stack Overflow. Such pseudo queries do not reflect the distribution of real user queries that are frequently issued in search engines. To the best of our knowledge, datasets that contain real user web queries include Lv et al. (2015), CodeSearchNet Challenge (Husain et al., 2019), and CodeXGLUE 2 (Lu et al., 2021). These three datasets only have 34, 99, and 1,046 queries, respectively, for model testing. The area lacks a dataset with a large amount of real user queries to support the learning of statistical models like deep neural networks for matching the semantics between natural language web query and code.\nTo address the aforementioned problems, we introduce CoSQA, a dataset with 20,604 pairs of web queries and code for Code Search and Question Answering, each with a label indicating whether\n2https://github.com/microsoft/CodeXGLUE\nthe code can answer the query or not. The queries come from the search logs of the Microsoft Bing search engine, and the code is a function from GitHub3. To scale up the annotation process on such a professional task, we elaborately curate potential positive candidate pairs and perform large scale annotation where each pair is annotated by at least three crowd-sourcing workers. Furthermore, to better leverage the CoSQA dataset for querycode matching, we propose a code contrastive learning method (CoCLR) to produce more artificially generated instances for training.\nWe perform experiments on the task of querycode matching on two tasks: code question answering and code search. On code question answering, we find that the performance of the same CodeBERT model improves 5.1% after training on the CoSQA dataset, and further boosts 10.5% after incorporating our CoCLR method. Moreover, experiments on code search also demonstrate similar results."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this part, we describe existing datasets and methods on code search and code question answering."
    }, {
      "heading" : "2.1 Datasets",
      "text" : "A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018;\n3We study on Python in this work, and we plan to extend to more programming languages in the future.\nHeyman and Cutsem, 2020). There are also highquality but small scale testing sets curated for code search evaluation (Li et al., 2019; Yan et al., 2020; Lv et al., 2015). Husain et al. (2019), Gu et al. (2018) and Miceli Barone and Sennrich (2017) collect large-scale unlabelled text-code pairs by leveraging human-leaved comments in code functions from GitHub. Yao et al. (2018) and Yin et al. (2018) automatically mine massive code answers for Stack Overflow questions with a model trained on a human-annotated dataset. Nie et al. (2016) extract the Stack Overflow questions and answers with most likes to form text-code pairs. Among all text-code datasets, only those in Lv et al. (2015), CodeSearchNet Challenge (Husain et al., 2019) and CodeXGLUE2 contain real user web queries, but they only have 34, 99 and 1,046 queries for testing and do not support training data-driven models. Table 1 illustrates an overview of these datasets."
    }, {
      "heading" : "2.2 Code Search Models",
      "text" : "Models for code search mainly can be divided into two categories: information retrieval based models and deep learning based models. Information retrieval based models match keywords in the query with code sequence (Bajracharya et al., 2006; Liu et al., 2020b). Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019). deep learning based models encode query and code into vectors and utilize vector similarities as the metric to retrieve code (Sachdev et al., 2018; Ye et al., 2016; Gu et al., 2018; Cambronero\net al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020)."
    }, {
      "heading" : "3 CoSQA Dataset",
      "text" : "In this section, we introduce the construction of the CoSQA dataset. We study Python in this work, and we plan to extend to more programming languages in the future. Each instance in CoSQA is a pair of natural language query and code, which is annotated with “1” or “0” to indicate whether the code can answer the query. We first describe how to curate web queries, obtain code functions, and get candidate query-code pairs. After that, we present the annotation guidelines and statistics."
    }, {
      "heading" : "3.1 Data Collection",
      "text" : "Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague queries and (7) others. Basically, queries in (2)-(7) categories are not likely to be answered only by a code function, since they may need abstract and general explanations in natural language. Therefore, we only target the first category of web queries that have code searching intent, i.e., queries that can be answered by a piece of code.\nTo filter out queries without code searching intent, we manually design heuristic rules based on exact keyword matching. For example, queries with the word of benefit or difference are likely to seek a conceptual comparison rather than a code function, so we remove all queries with such keywords. Based on the observations, we manually collect more than 100 keywords in total. Table 2 displays a part of selected keywords used for removing unqualified queries and more details can be found in Appendix A. To evaluate the query filtering algorithm, we construct a human-annotated testset. We invite three experienced python programmers to label 250 randomly sampled web queries with a binary label of having/not having searching intent. Then we evaluate the accuracy of intent predictions\ngiven keyword-based rules and those given by humans. We find the F1 score achieves 67.65, and the accuracy is up to 82.40. This demonstrates the remarkable effectiveness of our rule-based query filtering algorithm.\nCode Collection The selection of code format is another important issue in constructing querycode matching dataset, which includes a statement (Yin et al., 2018), a code snippet/block (Yao et al., 2018), a function (Husain et al., 2019), etc. In CoSQA, we simplify the task and adopt a compete Python function with paired documentation to be the answer to the query for the following reasons. First, it is complete and independent in functionality which may be more prone to answering a query. Second, it is syntactically correct and formally consistent which enables parsing syntax structures for advanced query-code matching. Additionally, a complete code function is often accompanied with documentation wrote by programmers to help understand its functionality and usage, which is beneficial for query-code matching (see Section 6.4 for more details).\nWe take the CodeSearchNet Corpus (Husain et al., 2019) as the source for code functions, which is a large-scale open-sourced code corpus allowing modification and redistribution. The corpus contains 2.3 million functions with documentation and 4.1 million functions without documentation from public GitHub repositories spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). In CoSQA, we only keep complete Python functions with documentation and remove those with non-English documentation or special tokens (e.g. “〈img...〉” or “http : //”).\nCandidate Query-code Pairs Obviously, it is not possible to annotate all query-code pairs. To improve efficiency, we wipe off low-confidence instances before annotation. Specifically, we employ a CodeBERT-based matching model (Feng et al., 2020) to retrieve high-confidence codes for every query. The CodeBERT encoder is fine-tuned on 148K automated-minded Python Stack Overflow question-code pairs (StaQC) (Yao et al., 2018) with the default parameters. A cosine similarity score on the pooled [CLS] embeddings of query and code is computed to measure the relatedness. To guarantee the quality of candidates, we automatically remove low-quality query-code pairs according to the following evaluation metrics.\n• To ensure the code may answer the query, we only keep the code with the highest similarity to the query and remove the pairs with a similarity below 0.5.\n• To increase the code diversity and control the\ncode frequency, we restrict the maximum occurrence of each code to be 10."
    }, {
      "heading" : "3.2 Data Annotation",
      "text" : "Annotating such a domain-specific dataset is difficult since it requires the knowledge of Python. Even experienced programmers do not necessarily understand all code snippets. To ensure the feasibility and control annotation quality, we design comprehensive annotation guidelines and take a two-step annotation procedure.\nAnnotation Guidelines Our annotation guideline is developed through several pilots and further updated with hard cases as the annotation progresses. Annotation participants are asked to make a two-step judgment for each instance: intent annotation and answer annotation.\nIn the first step of intent annotation, annotators are asked to judge whether the query has the intent to search for a code. They will skip the second step if the query is without code search intent. As\nshown in Section 3.1, vague queries are hard to be filtered out by our heuristic intent filtering algorithm. Therefore, it is necessary to take this step to remove such queries so that we can focus more on the matching between query and code rather than query discrimination.\nIn the second step of answer annotation, annotators are asked to judge whether the code can answer the query. They should label the instance with “1” if the code is a correct answer; otherwise, it is labeled “0”. In this step, judgment should be made after comprehensively considering the relevance between query with documentation, query with function header, and query with function body.\nDuring annotation, it is often the case that a code function can completely answer the query, which means that the code can satisfy all the demands in the query and it is a correct answer. (Case (1) in Table 3.) But more often, the code can not completely answer the query. It may exceed, partially meet or even totally dissatisfy the demands of the query. Therefore we divide such situations into four categories and give explanations and examples (Table 3) for each category:\n• If code can answer the query and even exceed the demand of the query, it is a correct answer. (Case (2) in Table 3.)\n• If code can meet a certain category of the query demands, it is also a correct answer. (Case (3) and Case (4) in Table 3.)\n• If code satisfies no more than 50% of the query demands, the code can not correctly answer the query. (Case (5) and Case (6) in Table 3.)\n• If a small part of the code is relevant to the query, the code can not be a correct answer. (Case (7) in Table 3.)\nAnnotation We ask more than 100 participants, who all have a good grasp of programming knowledge, to judge the instances according to the annotation guideline. Participants are provided with the full guidelines and allowed to discuss and search on the internet during annotation. When annotation is finished, each query-code pair has been annotated by at least three participants. We remove the pairs whose inter-annotator agreement (IAA) is poor, where Krippendorff’s alpha coefficient (Krippendorff, 1980) is used to measure IAA. We also remove pairs with no-code-search-intent queries.\nFinally, 20,604 labels for pairs of web query and code are retained, and their average Krippendorff’s alpha coefficient is 0.63. Table 4 shows the statistics of CoSQA."
    }, {
      "heading" : "4 Tasks",
      "text" : "Based on our CoSQA dataset, we explore two tasks to study the problem of query-code matching: code search and code question answering.\nThe first task is natural language code search, where we formulate it as a text retrieval problem. Given a query qi and a collection of codes C = {c1, . . . , cH} as the input, the task is to find the most possible code answer c∗. The task is evaluated by Mean Reciprocal Rank (MRR).\nThe second task is code question answering, where we formulate it as a binary classification problem. Given a natural language query q and a code sequence c as the input, the task of code question answering predicts a label of “1” or “0” to indicate whether code c answers query q or not. The task is evaluated by accuracy score."
    }, {
      "heading" : "5 Methodology",
      "text" : "In this section, we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances."
    }, {
      "heading" : "5.1 Siamese Network with CodeBERT",
      "text" : "The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019).\nWe use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and\ncode embeddings to be derived. Specifically, it shares exactly the same architecture as RoBERTa (Liu et al., 2019b), which is a bidirectional Transformer with 12 layers, 768 dimensional hidden states, and 12 attention heads, and is repretrained by masked language modeling and replaced token detection objectives on CodeSearchNet corpus (Husain et al., 2019).\nFor each query qi and code ci, we concatenate a [CLS] token in front of the sequence and a [SEP ] token at the end. Then we feed the query and code sequences into the CodeBERT encoder to obtain contextualized embeddings, respectively. Here we use the pooled output of [CLS] token as the representations: qi = CodeBERT(qi), ci = CodeBERT(ci). (1)\nNext we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wise difference qi − ci and elementwise product qi ⊙ ci, followed by a 1-layer feedforward neural network, to obtain a relation embedding:\nr(i,i) = tanh(W1 · [qi, ci,qi − ci,qi ⊙ ci]). (2)\nWe expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction.\nThen we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i)). Score s(i,i) can be viewed as the similarity of query qi and code ci.\nTo train the base siamese network, we use a binary cross entropy loss as the objective function:\nLb = −[yi · log s(i,i) + (1− yi) log(1− s(i,i))], (3)\nwhere yi is the label of (qi, ci)."
    }, {
      "heading" : "5.2 Code Contrastive Learning",
      "text" : "Now we incorporate code contrastive learning into the siamese network with CodeBERT. Contrastive learning aims to learn representations by enforcing similar objects to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi, ci), we define our contrastive learning task on example itself, inbatch augmented examples (qi, cj), and augmented example with rewritten query (q′i, ci). Hence, the overall training objective can be formulated as:\nL = Lb + Lib + Lqr. (4)\nIn-Batch Augmentation (IBA) A straightforward augmentation method is to use in-batch data, where a query and a randomly sampled code are considered as dissimilar and forced away by the models. Specifically, we randomly sample n examples {(q1, c1), (q2, c2), . . . , (qn, cn)} from a minibatch. For (qi, ci), we pair query qi with the other N − 1 codes within the mini-batch and treat the N − 1 pairs as dissimilar. Let s(i,j) denote the similarity of query qi and code cj , the loss function of the example with IBA is defined as:\nLib = − 1\nn− 1 n∑ j = 1 j 6= i log(1− s(i,j)), (5)\nQuery-Rewritten Augmentation (QRA) The in-batch augmentation only creates dissimilar pairs from the mini-batch, which ignores to augment similar pairs for learning positive relations. To remedy this, we propose to augment positive examples by rewriting queries. Inspired by the feature that web\nqueries are often brief and not necessarily grammatically correct, we assume that the rewritten query with minor modifications shares the same semantics as the original one. Therefore, an augmented pair with a rewritten query from a positive pair can also be treated as positive.\nSpecifically, given a pair of query qi and code ci with yi = 1, we rewrite qi into q′i in one of the three ways: randomly deleting a word, randomly switching the position of two words, and randomly copying a word. As shown in Section 6.3, switching position best helps increase the performance.\nFor any augmented positive examples, we also apply IBA on them. Therefore the loss function for the example with QRA is:\nLqr = L′b + L′ib, (6)\nwhere L′b and L′ib can be obtained by Eq. 3 and Eq. 5 by only change qi to q′i."
    }, {
      "heading" : "6 Experiments",
      "text" : "We experiment on two tasks, including code question answering and natural language code search. We report model comparisons and give detailed analyses from different perspectives."
    }, {
      "heading" : "6.1 Experiment Settings",
      "text" : "We train the models on the CoSQA dataset and evaluate them on two tasks: code question answering and code search.\nOn code question answering, we randomly split CoSQA into 20,000 training and 604 validation examples. As for the test set, we directly use the WebQueryTest in CodeXGLUE benchmark, which is a testing set of Python code question answering with 1,046 query-code pairs and their expert annotations.\nOn code search, we randomly divide the CoSQA into training, validation, and test sets in the number of 19604:500:500, and restrict the instances for validation and testing are all positive. We fix a code database with 6,267 different codes in CoSQA.\nBaseline Methods CoSQA is a new dataset, and there are no previous models designed specifically for it. Hence, we simply choose RoBERTa-base (Liu et al., 2019b) and CodeBERT (Feng et al., 2020) as the baseline methods. The baseline methods are trained on CodeSearchNet Python corpus with balanced positive examples. Negative samples consist of a balanced number of instances with randomly replaced code.\nEvaluation Metric We use accuracy as the evaluation metric on code question answering and Mean Reciprocal Rank (MRR) on code search.\nImplementation Details We initialize CoCLR with microsoft/codebert-base4 repretrained on CodeSearchNet Python Corpus (Husain et al., 2019). We use the AdamW optimizer (Loshchilov and Hutter, 2019) and set the batch size to 32 on the two tasks. On code question answering, we set the learning rate to 1e-5, warm-up rate to 0.1. On code search, we set the learning rate to 1e-6. All hyper-parameters are tuned to the best on the validation set. All experiments are performed on an NVIDIA Tesla V100 GPU with 16GB memory."
    }, {
      "heading" : "6.2 Model Comparisons",
      "text" : "Table 5 shows the experimental results on the tasks of code question answering and code search. We can observe that:\n(1) By leveraging the CoSQA dataset, siamese network with CodeBERT achieves overall performance enhancement on two tasks, especially for CodeXGLUE WebQueryTest, which is an open challenge but without direct training data. The result demonstrates the high-quality of CoSQA and its potential to be the training set of WebQueryTest.\n(2) By integrating the code contrastive learning method, siamese network with CodeBERT further achieves significant performance gain on both tasks. Especially on the task of WebQueryTest, CoCLR achieves the new state-of-the-art result by increasing 15.6%, which shows the effectiveness of our proposed approach."
    }, {
      "heading" : "6.3 Analysis: Effects of CoCLR",
      "text" : "To investigate the effects of CoCLR in query-code matching, we perform ablation study to analyze the major components in our contrastive loss that are of importance to help achieve good performance. We conduct experiments on the CoSQA code search task, using the following settings: (i) fine-tuning with vanilla binary cross-entropy loss only, (ii) fine-tuning with additional in-batch augmentation (IBA) loss, (iii) fine-tuning with additional query-rewritten augmentation (QRA) loss, (vi) fine-tuning with both additional IBA and QRA loss. And for QRA loss, we also test the three rewriting methods when applied individually. The results are listed in Table 6. We can find that:\n4https://github.com/microsoft/CodeBERT\n(1) Both incorporating IBA and QRA individually or together improve models’ performance. This indicates the advantage of applying code contrastive learning for code search.\n(2) No matter integrating IBA or not, the model with QRA by switching method performs better than models with the other two methods. We attribute the phenomenon to the fact that web queries do not necessarily have accurate grammar. So switching the positions of two words in the query better maximizes the agreement between the positive example and the pseudo positive example than the other two augmentations, which augments better examples to learn representations.\n(3) Comparing the two augmentations, adding IBA achieves more performance gain than QRA (1.25% versus 9.10%). As the numbers of examples with QRA and examples with IBA are not equal under two settings, we further evaluate the model with only one more example with IBA. The MRR is 55.52%, which is comparable to the performance of adding one more example with QRA. This suggests that there may be no difference between adding examples with IBA or examples with QRA. Instead, the number of high-quality examples is important for training. Similar findings are also reported in Sun et al. (2020), and a theoretical analysis is provided in Arora et al. (2019)."
    }, {
      "heading" : "6.4 Analysis: Effects of Code Components",
      "text" : "To explore the effects of different components of code in query-code matching, we evaluate CoCLR on code search and process the codebase by the following operations: (i) removing the function header, (ii) removing the natural language documentation, (iii) removing the code statements in the function body. We also combine two of the above operations to see the performance. From the results exhibited in Table 7, we can find that: by removing code component, the result of removing documentation drops more than those of removing header and removing function body. This demonstrates the importance of natural language documentation in code search. Since documentation shares the same modality with the query and briefly describes the functionality of the code, it may be more semantically related to the query. Besides, it also reveals the importance of using web queries rather than treating documentation as queries in code search datasets, which liberates models from the matching between documentation with code to the matching between query with documentation and code."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we focus on the matching problem of the web query and code. We develop a large-scale human-annotated query-code matching dataset CoSQA, which contains 20,604 pairs of real-world web queries and Python functions with documentation. We demonstrate that CoSQA is an\nideal dataset for code question answering and code search. We also propose a novel code contrastive learning method, named CoCLR, to incorporate artificially generated instances into training. We find that model with CoCLR outperforms the baseline models on code search and code question answering tasks. We perform detailed analysis to investigate the effects of CoCLR components and code components in query-code matching. We believe our annotated CoSQA dataset will be useful for other tasks that involve aligned text and code, such as code summarization and code synthesis."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank all anonymous reviewers for their useful comments. We also thank Zenan Xu, Daya Guo, Shuai Lu, Wanjun Zhong and Siyuan Wang for valuable discussions and feedback during the paper writing process."
    }, {
      "heading" : "A Heuristics for Query Filtering",
      "text" : "In this section, we introduce our heuristic rules to filter potential queries without code search intent. Basically, the rules are created from keyword templates and we follow the six categories of queries without code search intent to derive the keywords. Note that vague queries are morphologically variable so we ignore this categories. The keywords are shown in Table 8."
    } ],
    "references" : [ {
      "title" : "A survey of machine learning for big code and naturalness",
      "author" : [ "Miltiadis Allamanis", "Earl T. Barr", "Premkumar Devanbu", "Charles Sutton." ],
      "venue" : "ACM Computing Survey.",
      "citeRegEx" : "Allamanis et al\\.,? 2018",
      "shortCiteRegEx" : "Allamanis et al\\.",
      "year" : 2018
    }, {
      "title" : "A theoretical analysis of contrastive unsupervised representation learning",
      "author" : [ "S. Arora", "Hrishikesh Khandeparkar", "M. Khodak", "Orestis Plevrakis", "Nikunj Saunshi." ],
      "venue" : "ICML.",
      "citeRegEx" : "Arora et al\\.,? 2019",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2019
    }, {
      "title" : "Sourcerer: a search engine for open source code supporting structure-based search",
      "author" : [ "S. Bajracharya", "Trung Chi Ngo", "Erik Linstead", "Yimeng Dou", "Paul Rigor", "P. Baldi", "C. Lopes." ],
      "venue" : "OOPSLA ’06.",
      "citeRegEx" : "Bajracharya et al\\.,? 2006",
      "shortCiteRegEx" : "Bajracharya et al\\.",
      "year" : 2006
    }, {
      "title" : "Signature verification using a ”siamese” time delay neural network",
      "author" : [ "Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard Säckinger", "Roopak Shah." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Bromley et al\\.,? 1994",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1994
    }, {
      "title" : "When deep learning met code search",
      "author" : [ "José Cambronero", "Hongyu Li", "S. Kim", "K. Sen", "S. Chandra." ],
      "venue" : "Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software",
      "citeRegEx" : "Cambronero et al\\.,? 2019",
      "shortCiteRegEx" : "Cambronero et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhanced lstm for natural language inference",
      "author" : [ "Qian Chen", "Xiao-Dan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang", "D. Inkpen." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : null,
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Codebert: A pre-trained model for programming and natural languages",
      "author" : [ "Zhangyin Feng", "Daya Guo", "Duyu Tang", "N. Duan", "X. Feng", "Ming Gong", "Linjun Shou", "B. Qin", "Ting Liu", "Daxin Jiang", "M. Zhou." ],
      "venue" : "Findings of the Association for Computa-",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep code search",
      "author" : [ "Xiaodong Gu", "Hongyu Zhang", "Sunghun Kim." ],
      "venue" : "Proceedings of ICSE.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Graphcodebert: Pretraining code representations with data flow",
      "author" : [ "Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie Liu", "L. Zhou", "N. Duan", "Jian Yin", "Daxin Jiang", "M. Zhou." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "A multi-perspective architecture for semantic code search",
      "author" : [ "Rajarshi Haldar", "Lingfei Wu", "JinJun Xiong", "Julia Hockenmaier." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Haldar et al\\.,? 2020",
      "shortCiteRegEx" : "Haldar et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural code search revisited: Enhancing code snippet retrieval through natural language intent",
      "author" : [ "Geert Heyman", "Tom Van Cutsem." ],
      "venue" : "ArXiv, abs/2008.12193.",
      "citeRegEx" : "Heyman and Cutsem.,? 2020",
      "shortCiteRegEx" : "Heyman and Cutsem.",
      "year" : 2020
    }, {
      "title" : "CodeSearchNet challenge: Evaluating the state of semantic code search",
      "author" : [ "Hamel Husain", "Ho-Hsiang Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt." ],
      "venue" : "arXiv preprint arXiv:1909.09436.",
      "citeRegEx" : "Husain et al\\.,? 2019",
      "shortCiteRegEx" : "Husain et al\\.",
      "year" : 2019
    }, {
      "title" : "Krippendorff, klaus, content analysis: An introduction to its methodology",
      "author" : [ "K. Krippendorff." ],
      "venue" : "beverly hills, ca: Sage, 1980.",
      "citeRegEx" : "Krippendorff.,? 1980",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 1980
    }, {
      "title" : "Neural code search evaluation dataset",
      "author" : [ "Hongyu Li", "S. Kim", "S. Chandra." ],
      "venue" : "ArXiv, abs/1908.09804.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Opportunities and challenges in code search tools",
      "author" : [ "C. Liu", "Xin Xia", "David Lo", "Cuiyun Gao", "Xiaohu Yang", "J. Grundy." ],
      "venue" : "ArXiv, abs/2011.02297.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Simplifying deeplearning-based model for code search",
      "author" : [ "Chao Liu", "Xin Xia", "David Lo", "Zhiwei Liu", "A. Hassan", "Shanping Li." ],
      "venue" : "ArXiv, abs/2005.14373.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural query expansion for code search",
      "author" : [ "Jason Liu", "Seohyun Kim", "Vijayaraghavan Murali", "Swarat Chaudhuri", "Satish Chandra." ],
      "venue" : "Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Y. Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "I. Loshchilov", "F. Hutter." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Query expansion via wordnet for effective code search",
      "author" : [ "Meili Lu", "Xiaobing Sun", "S. Wang", "D. Lo", "Yucong Duan." ],
      "venue" : "2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER), pages 545–549.",
      "citeRegEx" : "Lu et al\\.,? 2015",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Codexglue: A machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664",
      "author" : [ "daresan", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu" ],
      "venue" : null,
      "citeRegEx" : "daresan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "daresan et al\\.",
      "year" : 2021
    }, {
      "title" : "Codehow: Effective code search based on api understanding and extended boolean model (e)",
      "author" : [ "Fei Lv", "H. Zhang", "Jian-Guang Lou", "S. Wang", "D. Zhang", "Jianjun Zhao." ],
      "venue" : "2015 30th IEEE/ACM International Conference on Automated Software Engineer-",
      "citeRegEx" : "Lv et al\\.,? 2015",
      "shortCiteRegEx" : "Lv et al\\.",
      "year" : 2015
    }, {
      "title" : "A parallel corpus of python functions and documentation strings for automated code documentation and code generation",
      "author" : [ "Antonio Valerio Miceli Barone", "Rico Sennrich." ],
      "venue" : "Proceedings of IJCNLP.",
      "citeRegEx" : "Barone and Sennrich.,? 2017",
      "shortCiteRegEx" : "Barone and Sennrich.",
      "year" : 2017
    }, {
      "title" : "Natural language inference by tree-based convolution and heuristic matching",
      "author" : [ "Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin." ],
      "venue" : "ACL.",
      "citeRegEx" : "Mou et al\\.,? 2016",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Query expansion based on crowd knowledge for code search",
      "author" : [ "Liming Nie", "He Jiang", "Zhilei Ren", "Zeyi Sun", "Xiaochen Li." ],
      "venue" : "IEEE Transactions on Services Computing, 9:771–783.",
      "citeRegEx" : "Nie et al\\.,? 2016",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2016
    }, {
      "title" : "Supporting code search with context-aware, analytics-driven, effective query reformulation",
      "author" : [ "M.M. Rahman." ],
      "venue" : "2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pages 226–229.",
      "citeRegEx" : "Rahman.,? 2019",
      "shortCiteRegEx" : "Rahman.",
      "year" : 2019
    }, {
      "title" : "Automatic query reformulation for code search using crowdsourced knowledge",
      "author" : [ "M.M. Rahman", "C. Roy", "D. Lo." ],
      "venue" : "Empirical Software Engineering, pages 1–56.",
      "citeRegEx" : "Rahman et al\\.,? 2019",
      "shortCiteRegEx" : "Rahman et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Retrieval on source code: a neural code search",
      "author" : [ "Saksham Sachdev", "H. Li", "Sifei Luan", "S. Kim", "K. Sen", "S. Chandra." ],
      "venue" : "Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages.",
      "citeRegEx" : "Sachdev et al\\.,? 2018",
      "shortCiteRegEx" : "Sachdev et al\\.",
      "year" : 2018
    }, {
      "title" : "Contrastive distillation on intermediate representations for language model compression",
      "author" : [ "S. Sun", "Zhe Gan", "Y. Cheng", "Yuwei Fang", "Shuohang Wang", "Jing jing Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-modal attention network learning for semantic source code retrieval",
      "author" : [ "Yao Wan", "Jingdong Shu", "Yulei Sui", "Guandong Xu", "Zhou Zhao", "Jian Wu", "Philip S. Yu." ],
      "venue" : "2019 34th IEEE/ACM International Conference on Automated Software En-",
      "citeRegEx" : "Wan et al\\.,? 2019",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2019
    }, {
      "title" : "Are the code snippets what we are searching for? a benchmark and an empirical study on code search with natural-language queries",
      "author" : [ "Shuhan Yan", "Hang Yu", "Yuting Chen", "Beijun Shen", "Lingxiao Jiang." ],
      "venue" : "Proceedings of SANER.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning semantic textual similarity from conversations",
      "author" : [ "Yinfei Yang", "Steve Yuan", "Daniel Cer", "Sheng-yi Kong", "Noah Constant", "Petr Pilar", "Heming Ge", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of The Third Workshop on Represen-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Coacor: Code annotation for code retrieval with reinforcement learning",
      "author" : [ "Ziyu Yao", "Jayavardhan Reddy Peddamail", "Huan Sun." ],
      "venue" : "Proceedings of WWW.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "StaQC: A systematically mined question-code dataset from stack overflow",
      "author" : [ "Ziyu Yao", "Daniel S Weld", "Wei-Peng Chen", "Huan Sun." ],
      "venue" : "Proceedings of WWW.",
      "citeRegEx" : "Yao et al\\.,? 2018",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2018
    }, {
      "title" : "From word embeddings to document similarities for improved information retrieval in software engineering",
      "author" : [ "Xin Ye", "Hui Shen", "Xiao Ma", "Razvan C. Bunescu", "Chang Liu." ],
      "venue" : "2016 IEEE/ACM 38th International Conference on Software Engineering",
      "citeRegEx" : "Ye et al\\.,? 2016",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to mine aligned code and natural language pairs from stack overflow",
      "author" : [ "Pengcheng Yin", "Bowen Deng", "Edgar Chen", "Bogdan Vasilescu", "Graham Neubig." ],
      "venue" : "International Conference on Mining Software Repositories.",
      "citeRegEx" : "Yin et al\\.,? 2018",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial training for code retrieval with question-description relevance regularization",
      "author" : [ "Jie Zhao", "Huan Sun." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020.",
      "citeRegEx" : "Zhao and Sun.,? 2020",
      "shortCiteRegEx" : "Zhao and Sun.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space.",
      "startOffset" : 58,
      "endOffset" : 112
    }, {
      "referenceID" : 31,
      "context" : "There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space.",
      "startOffset" : 58,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space.",
      "startOffset" : 58,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "(2015), CodeSearchNet Challenge (Husain et al., 2019), and CodeXGLUE 2 (Lu et al.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : "4K Documentation Function No StaQC (manual) (Yao et al., 2018) 8.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 35,
      "context" : "5K Stack Overflow question Code block Yes StaQC (auto) (Yao et al., 2018) 268K Stack Overflow question Code block No CoNaLa (manual) (Yin et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : ", 2018) 268K Stack Overflow question Code block No CoNaLa (manual) (Yin et al., 2018) 2.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : "9K Stack Overflow question Statements Yes CoNaLa (auto) (Yin et al., 2018) 598.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "2K Stack Overflow question Statements No SO-DS (Heyman and Cutsem, 2020) 12.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "(2015) 34 Web query Function Yes CodeSearchNet (Husain et al., 2019) 99 Web query Function Yes CodeXGLUE WebQueryTest 2 1K Web query Function Yes CoSQA (ours) 20.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al.",
      "startOffset" : 124,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al.",
      "startOffset" : 124,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al.",
      "startOffset" : 124,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "(2015), CodeSearchNet Challenge (Husain et al., 2019) and CodeXGLUE2 contain real user web queries, but they only have 34, 99 and 1,046 queries for test-",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Information retrieval based models match keywords in the query with code sequence (Bajracharya et al., 2006; Liu et al., 2020b).",
      "startOffset" : 82,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Information retrieval based models match keywords in the query with code sequence (Bajracharya et al., 2006; Liu et al., 2020b).",
      "startOffset" : 82,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019).",
      "startOffset" : 102,
      "endOffset" : 189
    }, {
      "referenceID" : 20,
      "context" : "Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019).",
      "startOffset" : 102,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019).",
      "startOffset" : 102,
      "endOffset" : 189
    }, {
      "referenceID" : 27,
      "context" : "Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019).",
      "startOffset" : 102,
      "endOffset" : 189
    }, {
      "referenceID" : 26,
      "context" : "Keyword extension by query expansion and reformulation is an effective way to enhance the performance (Lv et al., 2015; Lu et al., 2015; Nie et al., 2016; Rahman et al., 2019; Rahman, 2019).",
      "startOffset" : 102,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : "Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3)",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 32,
      "context" : "Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3)",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "Code Collection The selection of code format is another important issue in constructing querycode matching dataset, which includes a statement (Yin et al., 2018), a code snippet/block (Yao et al.",
      "startOffset" : 143,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "We take the CodeSearchNet Corpus (Husain et al., 2019) as the source for code functions, which is a large-scale open-sourced code corpus allowing modification and redistribution.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "a CodeBERT-based matching model (Feng et al., 2020) to retrieve high-confidence codes for every query.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 35,
      "context" : "The CodeBERT encoder is fine-tuned on 148K automated-minded Python Stack Overflow question-code pairs (StaQC) (Yao et al., 2018) with the default parameters.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "We remove the pairs whose inter-annotator agreement (IAA) is poor, where Krippendorff’s alpha coefficient (Krippendorff, 1980) is used to measure IAA.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994).",
      "startOffset" : 204,
      "endOffset" : 226
    }, {
      "referenceID" : 6,
      "context" : "By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019).",
      "startOffset" : 165,
      "endOffset" : 234
    }, {
      "referenceID" : 33,
      "context" : "By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019).",
      "startOffset" : 165,
      "endOffset" : 234
    }, {
      "referenceID" : 28,
      "context" : "By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019).",
      "startOffset" : 165,
      "endOffset" : 234
    }, {
      "referenceID" : 7,
      "context" : "We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "Specifically, it shares exactly the same architecture as RoBERTa (Liu et al., 2019b), which is a bidirectional Trans-",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "former with 12 layers, 768 dimensional hidden states, and 12 attention heads, and is repretrained by masked language modeling and replaced token detection objectives on CodeSearchNet corpus (Husain et al., 2019).",
      "startOffset" : 190,
      "endOffset" : 211
    }, {
      "referenceID" : 18,
      "context" : "Hence, we simply choose RoBERTa-base (Liu et al., 2019b) and CodeBERT (Feng et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : ", 2019b) and CodeBERT (Feng et al., 2020) as the baseline methods.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "Implementation Details We initialize CoCLR with microsoft/codebert-base4 repretrained on CodeSearchNet Python Corpus (Husain et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "We use the AdamW optimizer (Loshchilov and Hutter, 2019) and set the batch size to 32 on the two tasks.",
      "startOffset" : 27,
      "endOffset" : 56
    } ],
    "year" : 2021,
    "abstractText" : "Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce the CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance query-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1%, and incorporating CoCLR brings a further improvement of 10.5%. 1.",
    "creator" : "LaTeX with hyperref"
  }
}