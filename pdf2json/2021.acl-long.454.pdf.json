{
  "name" : "2021.acl-long.454.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter",
    "authors" : [ "Wei Liu", "Xiyan Fu", "Yue Zhang", "Wenming Xiao" ],
    "emails" : [ "hezan.lw@alibaba-inc.com,", "fuxiyan@mail.nankai.edu.cn,", "yue.zhang@wias.org.cn,", "wenming.xiaowm@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5847"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009). Many important language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017).\nChinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system\n(Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018).\nThere are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019).\nThe two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers\nthe combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level.\nInspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly. Specifically, a Chinese sentence is converted into a charwords pair sequence by matching the sentence with an existing lexicon. A lexicon adapter is designed to dynamically extract the most relevant matched words for each character using a char-to-word bilinear attention mechanism. The lexicon adapter is applied between adjacent transformers in BERT (shown in Figure 1 (b)) so that lexicon features and BERT representation interact sufficiently through the multi-layer encoder within BERT. We fine-tune both the BERT and lexicon adapter during training to make full use of word information, which is considerably different from the BERT Adapter (it fixes BERT parameters).\nWe investigate the effectiveness of LEBERT on three Chinese sequence labeling tasks1, including Chinese NER, Chinese Word Segmentation2, and Chinese POS tagging. Experimental results on ten benchmark datasets illustrate the effectiveness of our model, where state-of-the-art performance is achieved for each task on all datasets. In addition, we provide comprehensive comparisons and detailed analyses, which empirically confirm that bottom-level feature integration contributes to span boundary detection and span type determination."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to existing neural methods using lexicon features and pre-trained models to improve Chinese sequence labeling. Lexicon-based. Lexicon-based models aim to enhance character-based models with lexicon information. Zhang and Yang (2018) introduced a lat-\n1https://github.com/liuwei1206/LEBERT 2We follow the mainstream methods and regard Chinese\nWord Segmentation as a sequence labeling problem.\ntice LSTM to encode both characters and words for Chinese NER. It is further improved by following efforts in terms of training efficiency (Gui et al., 2019a; Ma et al., 2020), model degradation (Liu et al., 2019), graph structure (Gui et al., 2019b; Ding et al., 2019), and removing the dependency of the lexicon (Zhu and Wang, 2019). Lexicon information has also been shown helpful for Chinese Word Segmentation (CWS) and Part-ofspeech (POS) tagging. Yang et al. (2019) applied a lattice LSTM for CWS, showing good performance. Zhao et al. (2020) improved the results of CWS with lexicon-enhanced adaptive attention. Tian et al. (2020b) enhanced the character-based Chinese POS tagging model with a multi-channel attention of N-grams.\nPre-trained Model-based. Transformer-based pre-trained models, such as BERT (Devlin et al., 2019), have shown excellent performance for Chinese sequence labeling. Yang (2019) simply added a softmax on BERT, achieving state-of-the-art performance on CWS. Meng et al. (2019); Hu and Verberne (2020) showed that models using the character features from BERT outperform the static embedding-based approaches by a large margin for Chinese NER and Chinese POS tagging.\nHybrid Model. Recent work tries to integrate the lexicon and pre-trained models by utilizing their respective strengths. Ma et al. (2020) concatenated separate features, BERT representation and lexicon information, and input them into a shallow fusion layer (LSTM) for Chinese NER. Li et al. (2020) proposed a shallow Flat-Lattice Transformer to handle the character-word graph, in which the fusion is still at model-level. Similarly, character N-gram features and BERT vectors are concatenated for joint training CWS and POS tagging (Tian et al., 2020b). Our method is in line with the above approaches trying to combine lexicon information and BERT. The difference is that we integrate lexicon into the bottom level, allowing in-depth knowledge interaction within BERT.\nThere is also work employing lexicon to guide pre-training. ERNIE (Sun et al., 2019a,b) exploited entity-level and word-level masking to integrate knowledge into BERT in an implicit way. Jia et al. (2020) proposed Entity Enhanced BERT, further pre-training BERT using a domainspecific corpus and entity set with a carefully designed character-entity Transformer. ZEN (Diao et al., 2020) enhanced Chinese BERT with a multi-\nlayered N-gram encoder but is limited by the small size of the N-gram vocabulary. Compared to the above pre-training methods, our model integrates lexicon information into BERT using an adapter, which is more efficient and requires no raw texts or entity set. BERT Adapter. BERT Adapter (Houlsby et al., 2019) aims to learn task-specific parameters for the downstream tasks. Specifically, they add adapters between layers of a pre-trained model and tune only the parameters in the added adapters for a certain task. Bapna and Firat (2019) injected taskspecific adapter layers into pre-trained models for neural machine translation. MAD-X (Pfeiffer et al., 2020) is an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks. Wang et al. (2020) proposed KADAPTER to infuse knowledge into pre-trained models with further pre-training. Similar to them, we use a lexicon adapter to integrate lexicon information into BERT. The main difference is that our goal is to better fuse lexicon and BERT at the bottom-level rather than efficient training. To achieve it, we fine-tune the original parameters of BERT instead of fixing them, since directly injecting lexicon features into BERT will affect the performance due to the difference between that two information."
    }, {
      "heading" : "3 Method",
      "text" : "The main architecture of the proposed Lexicon Enhanced BERT is shown in Figure 2. Compared to BERT, LEBERT has two main differences. First, LEBERT takes both character and lexicon features as the input given that the Chinese sentence is converted to a character-words pair sequence. Second, a lexicon adapter is attached between Transformer layers, allowing lexicon knowledge integrated into BERT effectively.\nIn this section we describe: 1) Char-words Pair Sequence (Section 3.1), which incorporates words into a character sequence naturally; 2) Lexicon Adapter (Section 3.2), by injecting external lexicon features into BERT; 3) Lexicon Enhanced BERT (Section 3.3), by applying the Lexicon Adapter to BERT."
    }, {
      "heading" : "3.1 Char-Words Pair Sequence",
      "text" : "A Chinese sentence is usually represented as a character sequence, containing character-level features solely. To make use of lexicon information, we\nextend the character sequence to a character-words pair sequence.\nGiven a Chinese Lexicon D and a Chinese sentence with n characters sc = {c1, c2, ..., cn}, we find out all the potential words inside the sentence by matching the character sequence with D. Specifically, we first build a Trie based on the D, then traverse all the character subsequences of the sentence and match them with the Trie to obtain all potential words. Taking the truncated sentence “美 国人民 (American People)” for example, we can find out four different words, namely “美国 (America)”, “美国人 (American)”, “国人 (Compatriot)”, “人民 (People)”. Subsequently, for each matched word, we assign it to the characters it contains. As shown in Figure 3, the matched word “美国 (America)” is assigned to the character “美” and “国” since they form that word. Finally, we pair each character with assigned words and convert a Chinese sentence into a character-words pair sequence,\ni.e. scw = {(c1, ws1), (c2, ws2), ..., (cn, wsn)}, where ci denotes the i-th character in the sentence and wsi denotes matched words assigned to ci."
    }, {
      "heading" : "3.2 Lexicon Adapter",
      "text" : "Each position in the sentence consists of two types of information, namely character-level and wordlevel features. In line with the existing hybrid models, our goal is to combine the lexicon feature with BERT. Specifically, inspired by the recent works about BERT adapter (Houlsby et al., 2019; Wang et al., 2020), we propose a novel Lexicon Adapter (LA) shown in Figure 4, which can directly inject lexicon information into BERT.\nA Lexicon Adapter receives two inputs, a character and the paired words. For the i-th position in a char-words pair sequence, the input is denoted as (hci , x ws i ), where h c i is a character vector, the output of a certain transformer layer in BERT, and xwsi = {xwi1, xwi2, ..., xwim} is a set of word embeddings. The j-th word in xwsi is represented as following:\nxwij = e w(wij) (1)\nwhere ew is a pre-trained word embedding lookup table and wij is the j-th word in wsi.\nTo align those two different representations, we apply a non-linear transformation for the word vectors:\nvwij = W2(tanh(W1x w ij + b1)) + b2 (2)\nwhere W1 is a dc-by-dw matrix, W2 is a dc-by-dc matrix, and b1 and b2 are scaler bias. dw and dc denote the dimension of word embedding and the hidden size of BERT respectively.\nAs Figure 3 shows, each character is paired with multiple words. However, the contribution to each task varies from word to word. For example, as\nfor Chinese POS tagging, words “美国 (America)” and “人民 (People)” are superior to “美国人 (American)” and “国人 (Compatriot)”, since they are ground-truth segmentation of the sentence. To pick out the most relevant words from all matched words, we introduce a character-to-word attention mechanism.\nSpecifically, we denote all vwij assigned to i-th character as Vi = (vwi1, ..., v w im), which has the size m-by-dc and m is the total number of the assigned word. The relevance of each word can be calculated as:\nai = softmax(h c iWattnVi T ) (3)\nwhere Wattn is the weight matrix of bilinear attention. Consequently, we can get the weighted sum of all words by:\nzwi = m∑ j=1 aijv w ij (4)\nFinally, the weighted lexicon information is injected into the character vector by:\nh̃i = h c i + z w i (5)\nIt is followed by a dropout layer and layer normalization."
    }, {
      "heading" : "3.3 Lexicon Enhanced BERT",
      "text" : "Lexicon Enhanced BERT (LEBERT) is a combination of Lexicon Adapter (LA) and BERT, in which\nLA is applied to a certain layer of BERT shown in Figure 2. Concretely, LA is attached between certain transformers within BERT, thereby injecting external lexicon knowledge into BERT.\nGiven a Chinese sentence with n characters sc = {c1, c2, ..., cn}, we build the corresponding character-words pair sequence scw = {(c1, ws1), (c2, ws2), ..., (cn, wsn)} as described in Section 3.1. The characters {c1, c2, ..., cn} are first input into Input Embedder which outputs E = {e1, e2, ..., en} by adding token, segment and position embedding. Then we input E into Transformer encoders and each Transformer layer acts as following:\nG = LN(H l−1 +MHAttn(H l−1))\nH l = LN(G+ FFN(G)) (6)\nwhere H l = {hl1, hl2, ..., hln} denotes the output of the l-th layer and H0 = E; LN is layer normalization; MHAttn is the multi-head attention mechanism; FFN is a two-layer feed-forward network with ReLU as hidden activation function.\nTo inject the lexicon information between the k-th and (k + 1)-th Transformer, we first get the output Hk = {hk1, hk2, ..., hkn} after k successive Transformer layers. Then, each pair (hki , x ws i ) are passed through the Lexicon Adapter which transforms the ith pair into h̃ki :\nh̃ki = LA(h k i , x ws i ) (7)\nSince there are L = 12 Transformer layers in the BERT, we input H̃k = {h̃k1, h̃k2, ..., h̃kn} to the remaining (L − k) Transformers. At the end, we get the output of L-th Transformer HL for the sequence labeling task."
    }, {
      "heading" : "3.4 Training and Decoding",
      "text" : "Considering the dependency between successive labels, we use a CRF layer to make sequence labeling. Given the hidden outputs of the last layer HL = {hL1 , hL2 , ..., hLn}, we first calculate scores P as:\nO = WoH L + bo (8)\nFor a label sequence y = {y1, y2, ..., yn}, we define its probability to be:\np(y|s) = exp( ∑ i(Oi,yi + Tyi−1,yi))∑\nỹ exp( ∑ i(Oi,ỹi + Tỹi−1,ỹi)) (9)\nwhere T is the transition score matrix and ỹ denotes all possible tag sequences.\nGiven N labelled data {sj ,yj}|Nj=1, we train the model by minimize the sentence-level negative loglikelihood loss as:\nL = − ∑ j log(p(y|s)) (10)\nWhile decoding, we find out the label sequence obtaining the highest score using the Viterbi algorithm."
    }, {
      "heading" : "4 Experiments",
      "text" : "We carry out an extensive set of experiments to investigate the effectiveness of LEBERT. In addition, we aim to empirically compare model-level and BERT-level fusion in the same setting. Standard F1-score (F1) is used as evaluation metrics."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our method on ten datasets of three different sequence labeling tasks, including Chinese NER, Chinese Word Segmentation, and Chinese POS tagging. The statistics of the datasets is shown in Table 1. Chinese NER. We conduct experiments on four benchmark datasets, including Weibo NER (Peng and Dredze, 2015, 2016), OntoNotes (Weischedel et al., 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006). Weibo NER is a social media domain dataset, which is drawn from Sina Weibo; while OntoNotes and MSRA datasets are in the news domain. Resume NER\ndataset consists of resumes of senior executives, which is annotated by Zhang and Yang (2018). Chinese Word Segmentation. For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005). For MSR and PKU, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Higashiyama et al. (2019). Chinese POS Tagging. For POS-tagging, three Chinese benchmark datasets are used, including CTB5 and CTB6 from the Penn Chinese TreeBank (Xue et al., 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al., 2016). The CTB datasets are in simplified Chinese while the UD dataset is in traditional Chinese. Following Shao et al. (2017), we first convert the UD dataset into simplified Chinese before the POS-tagging experiments3. Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al., 2020a), referring to the corpus with two tagsets as UD1 and UD2, respectively. We use the official splits of train/dev/test in our experiments."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "Our model is constructed based on BERTBASE (Devlin et al., 2019), with 12 layers of transformer, and is initialized using the Chinese-BERT checkpoint from huggingface4. We use the 200- dimension pre-trained word embedding from Song et al. (2018), which is trained on texts of news and webpages using a directional skip-gram model. The lexicon D used in this paper is the vocab of the pre-trained word embedding. We apply the Lexicon Adapter between the 1-st and 2-nd Transformer in BERT and fine-tune both BERT and pre-trained word embedding during training. Hyperparameters. We use the Adam optimizer with an initial learning rate of 1e-5 for original parameters of BERT, and 1e-4 for other parameters introduced by LEBERT, and a maximum epoch number of 20 for training on all datasets. The max length of the sequence is set to 256, and the training batch size is 20 for MSRA NER and 4 for other datasets. Baselines. To evaluate the effectiveness of the pro-\n3The conversion tool we used is OpenCC. 4https://github.com/huggingface/transformers\nposed LEBERT, we compare it with the following approaches in the experiments.\n• BERT. Directly fine-tuning a pre-trained Chinese BERT on Chinese sequence labeling tasks.\n• BERT+Word. A strong model-level fusion baseline method, which inputs the concatenation of BERT vector and bilinear attention weighted word vector, and uses LSTM5 and CRF as fusion layer and inference layer respectively.\n• ERNIE (Sun et al., 2019a). An extension of BERT using a entity-level mask to guide pretraining.\n• ZEN. Diao et al. (2020) explicitly integrate Ngram information into BERT through an extra multi-layers of N-gram Transformer encoder and pre-training.\nFurther, we also compare with the state-of-theart models of each task."
    }, {
      "heading" : "4.3 Overall Results",
      "text" : "Chinese NER. Table 2 shows the experimental results on Chinese NER datasets6. The first four rows (Zhang and Yang, 2018; Zhu and Wang, 2019; Liu et al., 2019; Ding et al., 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al., 2020; Li et al., 2020) in the same block are the state-of-the-art models using shallow fusion layer to integrate lexicon information and BERT. The hybrid models, including existing state-of-the-art models, BERT + Word, and\n5We also evaluated with other fusion layers, such as Transformer, but we found LSTM is consistently better.\n6For a fair comparison, in Table 2, we use * denotes training the model with the same pre-trained word embedding as ours; † means the model is also initialized using the Chinese BERT checkpoint from huggingface and evaluated using the seqeval tool.\nthe proposed LEBERT, achieve better performance than both lexicon enhanced models and BERT baseline. This demonstrates the effectiveness of combining BERT and lexicon features for Chinese NER. Compared with model-level fusion models ((Ma et al., 2020; Li et al., 2020), and BERT+Word), our BERT-level fusion model, LEBERT, improves in F1 score on all four datasets across different domains, which shows that our approach is more efficient in integrating word and BERT. The results also indicate that our adapter-based method, LEBERT, with an extra pre-trained word embedding solely, outperforms those two lexicon-guided pre-training models (ERNIE and ZEN). This is likely because implicit integration of lexicon in ERNIE and restricted pre-defined n-gram vocabulary size in ZEN limited the effect. Chinese Word Segmentation. We report the F1 score of our model and the baseline methods on Chinese Word Segmentation in Table 3. Yang et al. (2019) applied a lattice LSTM to integrate word feature to character-based CWS model. Qiu et al. (2020) investigated the benefit of multiple heterogeneous segmentation criteria for single criterion Chinese word segmentation. Tian et al. (2020c) designed a wordhood memory network to incorporate wordhood information into a pretrained-based CWS model and showed good performance. Compared with those approaches, the models (BERT+Word and LEBERT) that combine lexicon features and BERT perform better. Moreover, our proposed LEBERT outperforms both modellevel fusion baseline (BERT+Word) and lexiconguided pre-training models (ERNIE and ZEN), achieving the best results. Chinese POS Tagging. We report the F1 score on four benchmarks of Chinese POS tagging in Table 4. The state-of-the-art model (Tian et al., 2020a) jointly trains Chinese Word Segmentation and Chi-\nnese POS tagging using a two-way attention to incorporate auto-analyzed knowledge, such as POS labels, syntactic constituents, and dependency relations. Similar to BERT+Word baseline, Tian et al. (2020b) integrated character-Ngram features with BERT at model-level using a multi-channel attention. As shown in Table 4, hybrid models ((Tian et al., 2020b), BERT+Word, LEBERT) that combine words information and BERT outperform BERT baseline, indicating that lexicon features can further improve the performance of BERT. LEBERT achieves the best results among these approaches, which demonstrates the effectiveness of BERT-level fusion. Consistent with results on Chinese NER and CWS, our BERT adapter-based approach is superior to lexicon-guided pre-training methods (ERNIE and ZEN).\nOur proposed model has achieved state-of-theart results across all datasets. To better show the strength of our method, we also summarize the relative error reduction over BERT baseline and BERT-based state-of-the-art models in Table 5. The results show that the relative error reductions are significant compared with baseline models."
    }, {
      "heading" : "4.4 Model-level Fusion vs. BERT-level Fusion",
      "text" : "Compared with model-level fusion models, LEBERT directly integrates lexicon features into BERT. We evaluate those two types of models in terms of Span F1, Type Acc, and Sentence Length, choosing the BERT+Word as the model-level fusion baseline due to its good performance across all the datasets. We also compare with a BERT baseline since both LEBERT and BERT+Word are improved based on it. Span F1 & Type Acc. Span F1 means the correctness of the span for an Entity in NER or a word in POS-tagging, while Type Acc denotes the proportion of full-correct predictions to span-correct predictions. Table 6 shows the results of three models on the Ontonotes and UD1 datasets. We can find that both BERT+Word and LEBERT perform better than BERT in terms of Span F1 and Type Acc on the two datasets. The results indicate that lexicon information contributes to span boundary detection and span classification. Specifically, the improvement of Span F1 is larger than Type Acc on Ontonotes, but smaller on UD1. Compared with BERT+Word, LEBERT achieves more improvement, demonstrating the effectiveness of lexicon feature enhanced via BERT-level fusion. Sentence Length. Figure 5 shows the F1-value trend of the baselines and LEBERT on Ontonotes dataset. All the models show a similar performancelength curve, decreasing as the sentence length increase. We speculate that long sentences are more challenging due to complicated semantics. Even lexicon enhanced models may fail to choose the correct words because of the increased number of matched words as the sentence become longer. The F1-score of BERT is relatively low, while BERT+Word achieves better performance due to the usage of lexicon information. Compared with BERT+Word, LEBERT performs better and shows more robustness when sentence length increases, demonstrating the more effective use of lexicon information. Case Study. Table 8 shows examples of Chinese NER and Chinese POS tagging results on\nOntonotes and UD1 datasets respectively. In the first example, BERT can not determine the entity boundary, but BERT+Word and LEBERT can segment it correctly. However, the BERT+Word model fails to predict the type of the entity “呼伦贝尔盟 (Hulunbuir League)” while LEBERT makes the correct prediction. This is likely because fusion at the lower layer contributes to capturing more complex semantics provided by BERT and lexicon. In the second example, the three models can find the correct span boundary, but both BERT and BERT+Word make incorrect predictions of the span type. Although BERT+Word can use the word information, it is disturbed by the irrelevant word “七八 (Seven and Eight)” predicting it as NUM. In contrast, LEBERT can not only integrate lexicon features but also choose the correct word for prediction."
    }, {
      "heading" : "4.5 Discussion",
      "text" : "Adaptation at Different Layers. We explore the effect of applying the Lexicon Adapter (LA) between different Transformer layers of BERT on Ontonotes dataset. Different settings are evaluated, including applying LA after one, multiple, and all layers of Transformer. As for one layer, we applied LA after k ∈ {1, 3, 6, 9, 12} layer; and {1, 3}, {1, 3, 6}, {1, 3, 6, 9} layers for multiple layers. All\nlayers represents LA used after every Transformer layer in BERT. The results show in Table 7. The shallow layer achieves better performance, which can be due to the fact that the shallow layer promotes more layered interaction between lexicon features and BERT. Applying LA at multi-layers of BERT hurts the performance and one possible reason is that integration at multi-layers causes overfitting. Tuning BERT or Not. Intuitively, integrating lexicon into BERT without fine-tuning can be faster (Houlsby et al., 2019) but with lower performance due to the different characteristics of lexicon feature and BERT (discrete representation vs. neural representation). To evaluate its impact, we conduct experiments with and without fine-tuning BERT parameters on Ontonotes and UD1 datasets. From the results, we find that without fine-tuning the BERT, the F1-score shows a decline of 7.03 points (82.08 → 75.05) on Ontonotes and 3.75 points (96.06→ 92.31) on UD1, illustrating the importance of finetuning BERT for our lexicon integration."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed a novel method to integrate lexicon features and BERT for Chinese sequence labeling, which directly injects lexicon information between Transformer layers in BERT using a Lexicon Adapter. Compared with modellevel fusion methods, LEBERT allows in-depth fusion of lexicon features and BERT representation at BERT-level. Extensive experiments show that the proposed LEBERT achieves state-of-theart performance on ten datasets of three Chinese\nsequence labeling tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their valuable comments and suggestions. Moreover, We sincerely thank Dr. Zhiyang Teng for his constructive collaboration during the development of this paper, and Dr. Haixia Chai, Dr. Jie Yang, and my colleague Junfeng Tian for their help in polishing our paper. In addition, We appreciate Zifeng Cheng for point out the error in Figure 3."
    } ],
    "references" : [ {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "Adversarial transfer learning for Chinese named entity recognition with selfattention mechanism",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4(0):357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "ZEN: Pre-training Chinese text encoder enhanced by n-gram representations",
      "author" : [ "Shizhe Diao", "Jiaxin Bai", "Yan Song", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4729–4740, Online.",
      "citeRegEx" : "Diao et al\\.,? 2020",
      "shortCiteRegEx" : "Diao et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural multi-digraph model for Chinese NER with gazetteers",
      "author" : [ "Ruixue Ding", "Pengjun Xie", "Xiaoyan Zhang", "Wei Lu", "Linlin Li", "Luo Si." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462–",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "The second international Chinese word segmentation bakeoff",
      "author" : [ "Thomas Emerson." ],
      "venue" : "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.",
      "citeRegEx" : "Emerson.,? 2005",
      "shortCiteRegEx" : "Emerson.",
      "year" : 2005
    }, {
      "title" : "Investigating self-attention network for chinese word segmentation",
      "author" : [ "L. Gan", "Y. Zhang." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2933–2941.",
      "citeRegEx" : "Gan and Zhang.,? 2020",
      "shortCiteRegEx" : "Gan and Zhang.",
      "year" : 2020
    }, {
      "title" : "Assessing bert’s syntactic abilities",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "CoRR, abs/1901.05287.",
      "citeRegEx" : "Goldberg.,? 2019",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Cnn-based chinese ner with lexicon rethinking",
      "author" : [ "Tao Gui", "Ruotian Ma", "Qi Zhang", "Lujun Zhao", "Yu-Gang Jiang", "Xuanjing Huang." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4982–4988.",
      "citeRegEx" : "Gui et al\\.,? 2019a",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "Part-of-speech tagging for Twitter with adversarial neural networks",
      "author" : [ "Tao Gui", "Qi Zhang", "Haoran Huang", "Minlong Peng", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2411–",
      "citeRegEx" : "Gui et al\\.,? 2017",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2017
    }, {
      "title" : "A lexicon-based graph neural network for Chinese NER",
      "author" : [ "Tao Gui", "Yicheng Zou", "Qi Zhang", "Minlong Peng", "Jinlan Fu", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Gui et al\\.,? 2019b",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Incorporating word attention into character-based word segmentation",
      "author" : [ "Shohei Higashiyama", "Masao Utiyama", "Eiichiro Sumita", "Masao Ideuchi", "Yoshiaki Oida", "Yohei Sakamoto", "Isaac Okada." ],
      "venue" : "Proceedings of the 2019 Conference of the North Amer-",
      "citeRegEx" : "Higashiyama et al\\.,? 2019",
      "shortCiteRegEx" : "Higashiyama et al\\.",
      "year" : 2019
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "Proceedings of the 36th International Conference",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Named entity recognition for Chinese biomedical patents",
      "author" : [ "Yuting Hu", "Suzan Verberne." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 627–637, Barcelona, Spain (Online). International Committee",
      "citeRegEx" : "Hu and Verberne.,? 2020",
      "shortCiteRegEx" : "Hu and Verberne.",
      "year" : 2020
    }, {
      "title" : "Entity enhanced BERT pre-training for Chinese NER",
      "author" : [ "Chen Jia", "Yuefeng Shi", "Qinrong Yang", "Yue Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6384–6396, Online. Associa-",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Speech and Language Processing (2nd Edition)",
      "author" : [ "Daniel Jurafsky", "James H. Martin." ],
      "venue" : "PrenticeHall, Inc., USA.",
      "citeRegEx" : "Jurafsky and Martin.,? 2009",
      "shortCiteRegEx" : "Jurafsky and Martin.",
      "year" : 2009
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "The third international chinese language processing bakeoff: Word segmentation and named entity recognition",
      "author" : [ "Gina-Anne Levow." ],
      "venue" : "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117.",
      "citeRegEx" : "Levow.,? 2006",
      "shortCiteRegEx" : "Levow.",
      "year" : 2006
    }, {
      "title" : "FLAT: Chinese NER using flat-lattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6836–6842, Online. Association for",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "An encoding strategy based wordcharacter LSTM for Chinese NER",
      "author" : [ "Wei Liu", "Tongge Xu", "Qinghua Xu", "Jiayu Song", "Yueran Zu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese Named Entity Recognition with a Sequence Labeling Approach: Based on Characters, or Based on Words",
      "author" : [ "Zhangxun Liu", "Conghui Zhu", "Tiejun Zhao" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "State-of-the-art Chinese word segmentation with BiLSTMs",
      "author" : [ "Ji Ma", "Kuzman Ganchev", "David Weiss." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4902–4908, Brussels, Belgium. Association",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Simplify the usage of lexicon in Chinese NER",
      "author" : [ "Ruotian Ma", "Minlong Peng", "Qi Zhang", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5951–5960, Online. As-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Glyce: Glyph-vectors for chinese character representations",
      "author" : [ "Yuxian Meng", "Wei Wu", "Fei Wang", "Xiaoya Li", "Ping Nie", "Fan Yin", "Muyu Li", "Qinghong Han", "Xiaofei Sun", "Jiwei Li." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages",
      "citeRegEx" : "Meng et al\\.,? 2019",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based",
      "author" : [ "Hwee Tou Ng", "Jin Kiat Low" ],
      "venue" : "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Ng and Low.,? \\Q2004\\E",
      "shortCiteRegEx" : "Ng and Low.",
      "year" : 2004
    }, {
      "title" : "Universal Dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajič", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Named entity recognition for Chinese social media with jointly trained embeddings",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal.",
      "citeRegEx" : "Peng and Dredze.,? 2015",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2015
    }, {
      "title" : "Improving named entity recognition for Chinese social media with word segmentation representation learning",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Peng and Dredze.,? 2016",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2016
    }, {
      "title" : "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder" ],
      "venue" : null,
      "citeRegEx" : "Pfeiffer et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "A concise model for multi-criteria Chinese word segmentation with transformer encoder",
      "author" : [ "Xipeng Qiu", "Hengzhi Pei", "Hang Yan", "Xuanjing Huang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2887–2897,",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-based joint segmentation and POS tagging for Chinese using bidirectional RNN-CRF",
      "author" : [ "Yan Shao", "Christian Hardmeier", "Jörg Tiedemann", "Joakim Nivre." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language",
      "citeRegEx" : "Shao et al\\.,? 2017",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2017
    }, {
      "title" : "Consistent word segmentation, part-of-speech tagging and dependency labelling annotation for Chinese language",
      "author" : [ "Mo Shen", "Wingmui Li", "HyunJeong Choe", "Chenhui Chu", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings of COLING 2016,",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Directional skip-gram: Explicitly distinguishing left and right context for word embeddings",
      "author" : [ "Yan Song", "Shuming Shi", "Jing Li", "Haisong Zhang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu-",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging",
      "author" : [ "Weiwei Sun", "Hans Uszkoreit." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Sun and Uszkoreit.,? 2012",
      "shortCiteRegEx" : "Sun and Uszkoreit.",
      "year" : 2012
    }, {
      "title" : "Ernie: Enhanced representation through knowledge integration",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Han Zhang", "Xin Tian", "Danxiang Zhu", "Hao Tian", "Hua Wu." ],
      "venue" : "arXiv preprint arXiv:1904.09223.",
      "citeRegEx" : "Sun et al\\.,? 2019a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Ernie 2.0: A continual pre-training framework for language understanding",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang" ],
      "venue" : "arXiv preprint arXiv:1907.12412",
      "citeRegEx" : "Sun et al\\.,? \\Q1907\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 1907
    }, {
      "title" : "Joint Chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge",
      "author" : [ "Yuanhe Tian", "Yan Song", "Xiang Ao", "Fei Xia", "Xiaojun Quan", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Tian et al\\.,? 2020a",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint Chinese word segmentation and part-of-speech tagging via multi-channel attention of character n-grams",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2073–2084,",
      "citeRegEx" : "Tian et al\\.,? 2020b",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Chinese word segmentation with wordhood memory networks",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8274–",
      "citeRegEx" : "Tian et al\\.,? 2020c",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Jianshu ji", "Guihong Cao", "Daxin Jiang", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "The penn chinese treebank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer." ],
      "venue" : "Natural language engineering, 11(2):207.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "BERT meets chinese word segmentation",
      "author" : [ "Haiqin Yang." ],
      "venue" : "CoRR, abs/1909.09292.",
      "citeRegEx" : "Yang.,? 2019",
      "shortCiteRegEx" : "Yang.",
      "year" : 2019
    }, {
      "title" : "Combining discrete and neural features for sequence labeling",
      "author" : [ "Jie Yang", "Zhiyang Teng", "Meishan Zhang", "Yue Zhang." ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Linguistics, pages 140–154. Springer.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural word segmentation with rich pretraining",
      "author" : [ "Jie Yang", "Yue Zhang", "Fei Dong." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 839–849, Vancouver, Canada. Asso-",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Subword encoding in lattice LSTM for Chinese word segmentation",
      "author" : [ "Jie Yang", "Yue Zhang", "Shuailong Liang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese comma disambiguation for discourse analysis",
      "author" : [ "Yaqin Yang", "Nianwen Xue." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 786–794, Jeju Island, Korea. Associa-",
      "citeRegEx" : "Yang and Xue.,? 2012",
      "shortCiteRegEx" : "Yang and Xue.",
      "year" : 2012
    }, {
      "title" : "A simple and effective neural model for joint word segmentation and pos tagging",
      "author" : [ "M. Zhang", "N. Yu", "G. Fu." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(9):1528– 1538.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554– 1564, Melbourne, Australia. Association for Compu-",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    }, {
      "title" : "Improving Neural Chinese Word Segmentation with Lexicon-Enhanced Adaptive Attention, page 1953–1956",
      "author" : [ "Xiaoyan Zhao", "Min Yang", "Qiang Qu", "Yang Sun." ],
      "venue" : "Association for Computing Machinery, New York, NY, USA.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition",
      "author" : [ "Yuying Zhu", "Guoxin Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Zhu and Wang.,? 2019",
      "shortCiteRegEx" : "Zhu and Wang.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009).",
      "startOffset" : 128,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 188
    }, {
      "referenceID" : 36,
      "context" : "One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016).",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 45,
      "context" : "One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016).",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al.",
      "startOffset" : 27,
      "endOffset" : 64
    }, {
      "referenceID" : 34,
      "context" : "Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al.",
      "startOffset" : 27,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : ", 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018).",
      "startOffset" : 130,
      "endOffset" : 188
    }, {
      "referenceID" : 22,
      "context" : ", 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018).",
      "startOffset" : 130,
      "endOffset" : 188
    }, {
      "referenceID" : 50,
      "context" : ", 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018).",
      "startOffset" : 130,
      "endOffset" : 188
    }, {
      "referenceID" : 50,
      "context" : "The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 242
    }, {
      "referenceID" : 47,
      "context" : "The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 242
    }, {
      "referenceID" : 21,
      "context" : "The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 242
    }, {
      "referenceID" : 5,
      "context" : "The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 242
    }, {
      "referenceID" : 3,
      "context" : "The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : ", 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : ", 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 24,
      "context" : "5848 the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al.",
      "startOffset" : 66,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "5848 the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al.",
      "startOffset" : 66,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : ", 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 40,
      "context" : ", 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 42,
      "context" : "Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly.",
      "startOffset" : 40,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "It is further improved by following efforts in terms of training efficiency (Gui et al., 2019a; Ma et al., 2020), model degradation (Liu et al.",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "It is further improved by following efforts in terms of training efficiency (Gui et al., 2019a; Ma et al., 2020), model degradation (Liu et al.",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : ", 2020), model degradation (Liu et al., 2019), graph structure (Gui et al.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : ", 2019), graph structure (Gui et al., 2019b; Ding et al., 2019), and removing the dependency of the lexicon (Zhu and Wang, 2019).",
      "startOffset" : 25,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : ", 2019), graph structure (Gui et al., 2019b; Ding et al., 2019), and removing the dependency of the lexicon (Zhu and Wang, 2019).",
      "startOffset" : 25,
      "endOffset" : 63
    }, {
      "referenceID" : 52,
      "context" : ", 2019), and removing the dependency of the lexicon (Zhu and Wang, 2019).",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Transformer-based pre-trained models, such as BERT (Devlin et al., 2019), have shown excellent performance for Chinese sequence labeling.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "Similarly, character N-gram features and BERT vectors are concatenated for joint training CWS and POS tagging (Tian et al., 2020b).",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "ZEN (Diao et al., 2020) enhanced Chinese BERT with a multi-",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "BERT Adapter (Houlsby et al., 2019) aims to learn task-specific parameters for the downstream tasks.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 31,
      "context" : "MAD-X (Pfeiffer et al., 2020) is an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Specifically, inspired by the recent works about BERT adapter (Houlsby et al., 2019; Wang et al., 2020), we propose a novel Lexicon Adapter (LA) shown in Figure 4, which can directly inject lexicon information into BERT.",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 42,
      "context" : "Specifically, inspired by the recent works about BERT adapter (Houlsby et al., 2019; Wang et al., 2020), we propose a novel Lexicon Adapter (LA) shown in Figure 4, which can directly inject lexicon information into BERT.",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 50,
      "context" : ", 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : ", 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006).",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al.",
      "startOffset" : 163,
      "endOffset" : 178
    }, {
      "referenceID" : 43,
      "context" : "For POS-tagging, three Chinese benchmark datasets are used, including CTB5 and CTB6 from the Penn Chinese TreeBank (Xue et al., 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al.",
      "startOffset" : 115,
      "endOffset" : 133
    }, {
      "referenceID" : 28,
      "context" : ", 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 33,
      "context" : "Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al., 2020a), referring to the corpus with two tagsets as UD1 and UD2, respectively.",
      "startOffset" : 88,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al., 2020a), referring to the corpus with two tagsets as UD1 and UD2, respectively.",
      "startOffset" : 88,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "Our model is constructed based on BERTBASE (Devlin et al., 2019), with 12 layers of transformer, and is initialized using the Chinese-BERT checkpoint from huggingface4.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 50,
      "context" : "The first four rows (Zhang and Yang, 2018; Zhu and Wang, 2019; Liu et al., 2019; Ding et al., 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al.",
      "startOffset" : 20,
      "endOffset" : 99
    }, {
      "referenceID" : 52,
      "context" : "The first four rows (Zhang and Yang, 2018; Zhu and Wang, 2019; Liu et al., 2019; Ding et al., 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al.",
      "startOffset" : 20,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "The first four rows (Zhang and Yang, 2018; Zhu and Wang, 2019; Liu et al., 2019; Ding et al., 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al.",
      "startOffset" : 20,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "The first four rows (Zhang and Yang, 2018; Zhu and Wang, 2019; Liu et al., 2019; Ding et al., 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al.",
      "startOffset" : 20,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : ", 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al., 2020; Li et al., 2020) in the same block are the state-of-the-art models using shallow fusion layer to integrate lexicon information and BERT.",
      "startOffset" : 126,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : ", 2019) in the first block show the performance of lexicon enhanced character-based Chinese NER models, and the last two rows (Ma et al., 2020; Li et al., 2020) in the same block are the state-of-the-art models using shallow fusion layer to integrate lexicon information and BERT.",
      "startOffset" : 126,
      "endOffset" : 160
    }, {
      "referenceID" : 24,
      "context" : "Compared with model-level fusion models ((Ma et al., 2020; Li et al., 2020), and BERT+Word), our BERT-level fusion model, LEBERT, improves in F1 score on all four datasets across different domains, which shows that our approach is more efficient in integrating word and BERT.",
      "startOffset" : 41,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "Compared with model-level fusion models ((Ma et al., 2020; Li et al., 2020), and BERT+Word), our BERT-level fusion model, LEBERT, improves in F1 score on all four datasets across different domains, which shows that our approach is more efficient in integrating word and BERT.",
      "startOffset" : 41,
      "endOffset" : 75
    }, {
      "referenceID" : 39,
      "context" : "The state-of-the-art model (Tian et al., 2020a) jointly trains Chinese Word Segmentation and ChiModel CTB5 CTB6 UD1 UD2",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 40,
      "context" : "As shown in Table 4, hybrid models ((Tian et al., 2020b), BERT+Word, LEBERT) that combine words information and BERT outperform BERT baseline, indicating that lexicon features can further improve the performance of BERT.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "Intuitively, integrating lexicon into BERT without fine-tuning can be faster (Houlsby et al., 2019) but with lower performance due to the different characteristics of lexicon feature and BERT (discrete representation vs.",
      "startOffset" : 77,
      "endOffset" : 99
    } ],
    "year" : 2021,
    "abstractText" : "Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labeling tasks due to their respective strengths. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves state-ofthe-art results.",
    "creator" : "LaTeX with hyperref"
  }
}