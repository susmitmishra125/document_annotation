{
  "name" : "2021.acl-long.162.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Weight Distillation: Transferring the Knowledge in Neural Network Parameters",
    "authors" : [ "Ye Lin", "Yanyang Li", "Ziyang Wang", "Bei Li", "Quan Du", "Tong Xiao", "Jingbo Zhu" ],
    "emails" : [ "linye2015@outlook.com", "blamedrlee@outlook.com", "libeineu@outlook.com", "duquanneu@outlook.com", "wangziyang@stumail.neu.edu.cn", "xiaotong@mail.neu.edu.cn", "zhujingbo@mail.neu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2076–2088\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2076"
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge Distillation (KD) is a popular model acceleration and compression approach (Hinton et al., 2015). It assumes that a lightweight network (i.e., student network, or student for short) can learn to generalize in the same way as a large network (i.e., teacher network, or teacher for short). To this end, a simple method is to train the student network with predicted probabilities of the teacher network as its targets.\nBut KD has its limitation: the student network can only access the knowledge in the predictions of the teacher network. It does not consider the knowledge in the teacher network parameters. These parameters contain billions of entries for the teacher\n∗Authors contributed equally. †Corresponding author.\nnetwork to make predictions. Yet in KD the student only learns from those predictions with at most thousands of categories. This way results in an inferior student network, since it learns from the limited training signals. Our analysis in Section 5.1 shows that KD performs better if we simply cut off parts of parameters from the teacher to initialize the student. This fact implies that the knowledge in parameters is complementary to KD but missed. It also agrees with the recent success in pre-training (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019), where parameters reusing plays the main role. Based on this observation, a superior student is expected if all parameters in the teacher network could be exploited. However, this imposes a great challenge as the student network is too small to fit in the whole teacher network.\nTo fully utilize the teacher network, we propose Weight Distillation (WD) to transfer all the parameters of the teacher network to the student network, even if they have different numbers of weight matrices and (or) these weight matrices are of different shapes. We first use a parameter generator to predict the student network parameters from the teacher network parameters. After that, a finetuning process is performed to improve the quality of the transferred parameters. See Fig. 1 for a comparison of KD and WD.\nWe test the WD method in a well-tuned Transformer-based machine translation system. The experiments are run on three machine translation benchmarks, including the WMT16 EnglishRoman (En-Ro), NIST12 Chinese-English (Zh-En), and WMT14 English-German (En-De) tasks. With a similar speedup, the student network trained by WD achieves BLEU improvements of 0.51∼1.82 points over KD. With similar BLEU performance, the student network trained by WD is 1.11∼1.39× faster than KD. More interestingly, it is found that WD is very effective in improving the student net-\nwork when its model size is close to the teacher network. On the WMT14 En-De test data, our WDbased system achieves a strong result (a BLEU score of 30.77) but is 1.88× faster than the big teacher network."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Transformer",
      "text" : "In this work, we choose Transformer (Vaswani et al., 2017) for study because it is one of the stateof-the-art neural models in natural language processing. Transformer is a Seq2Seq model, which consists of an encoder and a decoder. The encoder maps an input sequence to a sequence of continuous representations and the decoder maps these representations to an output sequence. Both the encoder and the decoder are composed of an embedding layer and multiple hidden layers. The decoder has an additional output layer at the end.\nThe hidden layer in the encoder consists of a self-attention sub-layer and a feed-forward network (FFN) sub-layer. The decoder has an additional encoder-decoder attention sub-layer between the self-attention and the FFN sub-layers. For more details, we refer the reader to (Vaswani et al., 2017)."
    }, {
      "heading" : "2.2 Knowledge Distillation",
      "text" : "KD encourages the student network to produce outputs close to the outputs of the teacher network.\nKD achieves this by:\nS̄ = arg min S L(yT , yS) (1)\nwhere L is the cross-entropy loss, yT is the teacher prediction, T is the teacher parameters, yS is the student prediction and S is the student parameters. In practice, Eq. 1 serves as a regularization term.\nA more effective KD variant for Seq2Seq models is proposed by Kim and Rush (2016). They replace the predicted distributions yT by the generated sequences from the teacher network."
    }, {
      "heading" : "3 Weight Distillation",
      "text" : ""
    }, {
      "heading" : "3.1 The Parameter Generator",
      "text" : "The proposed parameter generator transforms the teacher parameters T to the student parameters S. It is applied to the encoder and decoder separately.\nThe process is simple: it first groups weight matrices in the teacher network into different subsets, and then each subset is used to generate a weight matrix in the student network. Though using all teacher weights to predict student weights is possible, its efficiency becomes an issue. For instance, the number of parameters in a simple linear transformation will be the product of the numbers of entries in its input and output, where in our case these input and output contain billions of entries (from the teacher and student weights), making it intractable to keep this simple linear transformation in the memory. Grouping is an effective way\nto reduce it to light-weighted transformation problems. Here we take the encoder as an example for the following discussion."
    }, {
      "heading" : "3.1.1 Weight Grouping",
      "text" : "The left of Fig. 2 shows an example of weight grouping for one group with two subsets.\nBefore the discussion, we define the weight class as a weight matrix from the network formulation, and the weight instance as the instantiation of a weight class. Take the FFN for an example. Its formulation is defined as:\nFFN(x) = max(xW1 + b1, 0)W2 + b2 (2)\nwhere W1, b1, W2 and b2 are learnable weight matrices. In this case, W1 in Eq. 2 defines a weight class. Then all the corresponding weight matrices from FFNs in different layers of the network are the instantiations of this W1 weight class.\nFrom this sense, a weight class determines the role of its instantiations in design, e.g., extracting features for W1 in Eq. 2. This means that when transferring parameters, different weight classes will contribute little to each other as they have different roles. Therefore, when predicting a student weight matrix, it is sufficient to consider the teacher weight matrices with the same weight class only, which makes the prediction efficient. So our parameter generator groups the teacher weight matrices by the weight class they belong to, i.e., dif-\nferent weight classes clusters all their instantiations to form their own groups. In the previous example, the W1 weight class will form a group [T1, T2, · · · , TLt ], where each Ti is the W1 weight instance in the i-th FFN andLt is the number of layers in the teacher network. These weight matrices are then used to generate the W1 weight instances in the student network.\nThe parameter generator further divides each group into smaller subsets with weight matrices from adjacent layers, because the adjacent layers function similarly (Jawahar et al., 2019) and so as their weights. This way additionally makes the later transformation more light-weighted. Namely, given a group of Lt weight matrices, the parameter generator splits it into Ls subsets, where Ls is the number of layers in the student network. For example, the i-th subset of the group of W1 weight class in the previous example will be[ T(i−1)∗Lt/Ls+1, T(i−1)∗Lt/Ls+2, · · · , Ti∗Lt/Ls ] . This subset is used to generate the weight matrix Si, which corresponds to W1 weight instance in the i-th FFN of the student network."
    }, {
      "heading" : "3.1.2 Weight Transformation",
      "text" : "Given a subset of teacher weight matrices, the parameter generator then transforms them to the desired student weight matrix, as shown in the right of Fig. 2.\nLet us see the process of generating the\nweight matrix S ∈ RIs×Os from the subset[ T1, T2, · · · , TLt/Ls ] with each Ti ∈ RIt×Ot , where Is and Os are the input and output dimensions of the student weight matrix, It andOt are the input and output dimensions of the teacher weight matrix. The parameter generator first stacks all weight matrices in this subset into a tensor T̂ ∈ RIt×Ot×Lt/Ls . Then it uses three learnable weight matrices, WI ∈ RIt×Is ,WO ∈ ROt×Os ,WL ∈ RLt/Ls×1, to transform T̂ to the shape Is×Os× 1 sequentially:\nT̂·jk ← T̂·jkWI ,∀j ∈ [1, Ot], k ∈ [1, L′] (3) T̂j·k ← T̂j·kWO,∀j ∈ [1, Is], k ∈ [1, L′] (4) T̂jk· ← T̂jk·WL, ∀j ∈ [1, Is], k ∈ [1, Os](5)\nwhere L′ = Lt/Ls. Finally we transform T̂ (with 1 in its shape get eliminated) to produce S, as follows:\nS = tanh(T̂ ) W +B (6)\nwhere W and B are learnable weight matrices of the parameter generator and have the same shape as T̂ . denotes the Hadamard product. The tanh function provides non-linearity. W and B are used to scale and shift the tanh output to any desirable value. Note that we do not share WI , WO, WL, W and B when generating different S. If the encoder is of the same size in both the teacher and student networks, only Eq. 6 is needed to map each weight matrix from the teacher network to the student network."
    }, {
      "heading" : "3.2 Training",
      "text" : "There are two training phases in WD: In the first phase (Phase 1), we train the parameter generator π = {WI ,WO,WL,W,B} to predict the student network S; In the second phase (Phase 2), we finetune the generated student network S to obtain better results. Phase 2 is necessary because the parameter generator is simply a feed-forward network with one hidden layer and thus has no enough capacity to produce a good enough student network at once. A more sophisticated parameter generator is an alternative, but it is expensive due to its large input and output spaces.\nThe task of Phase 1 is to minimize the loss of the student network with parameters S predicted by the parameter generator π from the teacher parameters\nT . The objective of Phase 1 is:\nπ̄ = arg min π\n[(1− α)L(yT , yπ) +\nαL(y, yπ)] (7)\nwhere L is the cross-entropy loss, yT is the teacher prediction, yπ is the prediction of the student network generated by the parameter generator π, y is the ground truth, and α is a hyper-parameter that balances two losses and is set to 0.5 by default. The first term of Eq. 7 is the KD loss as in Eq. 1, and the second term is the standard loss.\nThe objective of Phase 2 has the same form as Eq. 7, except that it optimizes S instead of π, like this:\nS̄ = arg min S [(1− α)L(yT , yS) +\nαL(y, yS)] (8)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our methods on the WMT16 EnglishRoman (En-Ro), NIST12 Chinese-English (Zh-En), and WMT14 English-German (En-De) tasks.\nFor the En-Ro task, we use the WMT16 EnglishRoman dataset (610K pairs). We choose newsdev2016 as the validation set and newstest-2016 as the test set. For the Zh-En task, we use 1.8M sentence Chinese-English bitext provided within NIST12 OpenMT1. We choose the evaluation data of mt06 as the validation set, and mt08 as the test set. For the En-De task, we use the WMT14 EnglishGerman dataset (4.5M pairs). We share the source and target vocabularies. We choose newstest-2013 as the validation set and newstest-2014 as the test set.\nFor all datasets, we tokenize every sentence using the script in the Moses toolkit and segment every word into subword units using Byte-Pair Encoding (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K. We remove sentences with more than 250 subword units (Xiao et al., 2012). In addition, we evaluate the results using multi-bleu.perl.\n1LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, LDC2004T08"
    }, {
      "heading" : "4.2 Model Setup",
      "text" : "Our baseline system is based on the open-source implementation of the Transformer model presented in Ott et al. (2019)’s work. For all machine translation tasks, we experiment with the Transformer-base (base) setting. We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset. All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020). The embedding size (width) is set to 512 for Transformer-base/deep and 1,024 for Transformerbig. The FFN hidden size equals to 4× embedding size in all settings. We stop training until the model stops improving on the validation set. All experiments are done on 8 NVIDIA TITIAN V GPUs with mixed-precision training (Micikevicius et al., 2018). At test time, the model is decoded with a beam of width 4/6/4, a length normalization weight of 1.0/1.0/0.6 and a batch size of 64 for the EnRo/Zh-En/En-De tasks with half-precision.\nNote that our method can also be seen as an advanced version of Tucker Decomposition (Tucker, 1966). So we also implement a baseline based on\nTucker Decomposition. Unfortunately, this model does not converge to a good optima and performs extremely poor.\nFor the KD baseline, we adopt Kim and Rush (2016)’s method, which has proven to be the most effective for Seq2Seq models (Kim et al., 2019). It generates the pseudo data from the source side of the bilingual corpus. The choices of student networks are based on the observation that the encoder has a greater impact on performance and the decoder dominates the decoding time (Kasai et al., 2020). Therefore we vary the depth and width of the decoder. We test two student network configurations: TINY halves the decoder width and uses a 1-layer decoder (the fastest WD student network with the performance close to the teacher network); SMALL uses a 2-layer decoder whose width is the same as the teacher network (the fastest KD student network with the performance close to the teacher network).\nAll hyper-parameters of WD are identical to the baseline system, except that WD uses 1/4 warmup steps in Phase 2. For the parameter generator initialization, we use Glorot and Bengio (2010)’s method to initialize WI ,WO,WL in Eqs. 3 - 5. W and B in Eq. 6 are initialized to constants 1 and 0 respec-\ntively. All results are the average of three identical runs with different random seeds."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 1 shows the results of different approaches on different student networks with Transformer-base as the teacher network. In all three tasks and different sized student networks, WD outperforms KD by 0.77, 1.57, and 0.66 BLEU points on En-Ro, ZhEn, and En-De on average. Our method (TINY) can obtain similar performance to the teacher network with only half of its parameters and is 2.57∼2.80× faster, while KD (SMALL) uses more parameters and has only a 1.94∼2.26× speedup in the same case. We attribute the success of WD to that the parameter generator uses parameters of the teacher network to provide a good initialization for the student network, as Phase 1 behaves like the initialization, and the effectiveness of a good initialization has been widely proven (Erhan et al., 2010; Mishkin and Matas, 2016). Interestingly, both KD and WD surpass the teacher network when the stu-\ndent network size is close to the teacher network (SMALL). This is due to that KD has a form similar to data augmentation (Gordon and Duh, 2019).\nTable 2 shows the results of larger networks, i.e., Transformer-big/deep. The phenomenon here is similar to that in Table 1. The acceleration on Transformer-big is more obvious than on Transformer-base (2.94× vs. 2.57× for TINY and 2.10× vs. 1.95× for SMALL in WD). This is because the decoder in Transformerbig occupies a larger portion of the decoding time than in Transformer-base. But the acceleration on Transformer-deep is less obvious than on Transformer-base (2.13× vs. 2.57× for TINY and 1.88× vs. 1.95× for SMALL in WD), as a deeper encoder consumes more inference time. Moreover, compared with such a strong Transformer-deep teacher, WD (SMALL) can still outperform it by 1.34 BLEU points with a 1.88× speedup, achieving the state-of-the-art."
    }, {
      "heading" : "5 Analysis",
      "text" : "To better understand WD, we conduct a series of experiments on the NIST12 Zh-En validation set with the Transformer-base teacher."
    }, {
      "heading" : "5.1 Initialization Study",
      "text" : "To test whether KD misses knowledge in parameters, we initialize the student network with the teacher parameters. If the teacher and student networks have different depths, we initialize the student network with the bottom layers of the teacher network (Sanh et al., 2019). If they have different\nwidths, we slice the teacher weight matrices to fit the student network (Wang et al., 2020). Table 3 shows that initializing the student networks with the teacher parameters improves KD, supporting our claim that knowledge in parameters is complementary to KD but missed. We also see that WD outperforms this simple initialization, which implies that using all teacher parameters helps to obtain a better student."
    }, {
      "heading" : "5.2 Sensitivity Analysis",
      "text" : "The left part of Fig. 3 studies how sensitive the performance (BLEU) of different methods are to various levels of inference speedup (obtained by varying decoder depth and width). It shows that WD distributes on the upper right of the figure, which means that WD produces student networks that are consistently faster and better.\nWe also investigate how sensitive different methods are to the training hyper-parameters, i.e., the learning rate and warmup steps. Here we focus on Phase 2 of WD, as it directly impacts the final performance. The middle part of Fig. 3 shows that WD can endure learning rates in a wide range, because its performance does not vary much. However, a very large learning rate still negatively impacts the performance. The right part of Fig. 3 is the opposite, where WD is more sensitive to\nthe warmup steps than the learning rate. This is because more warmup steps will run the network with a high learning rate in a longer period. A high learning rate has been proven to be harmful as shown in the middle part of Fig. 3."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "Table 4 studies which weight matrices in the teacher network are the most effective. It is achieved by training the parameter generator with only the intended weight matrices and without the KD loss term in Eq. 7. We see that using any weight matrix brings a significant improvement over the baseline. This observation shows that weight matrices in the teacher network do contain abundant knowledge. Among these, the encoder weight matrices produce the most significant result, which agrees with the previous study claiming that the encoder is more important than the decoder (Wang et al., 2019; Bapna et al., 2018)."
    }, {
      "heading" : "5.4 Compression Study",
      "text" : "As the previous experiments focus on a lightweight decoder for acceleration, the compression is limited as the encoder remains large. To examine the effectiveness of WD on model compression, we shrink the depth and width of the encoder and decoder simultaneously. As shown in Table 5, WD consistently outperforms KD by about 1 BLEU\npoint under various compression ratios (ranging from 1.00× to 3.40×). Note that decreasing the width brings more significant compression. This is because a large portion of the parameters is from the embedding matrices and the output projection. The sizes of these matrices are determined by the width and a fixed vocabulary size."
    }, {
      "heading" : "5.5 Training Efficiency",
      "text" : "Fig. 4 studies the training efficiency of WD by comparing the final BLEU scores when two training phases end in different epochs. As shown in Fig. 4, Phase 1 has little impact on Phase 2, because Phase 2 converges to optimums with similar BLEU scores once Phase 1 runs for a few epochs (say, 3 epochs). If we run Phase 1 longer, then Phase 2 converges faster. This phenomenon suggests that Phase 1 already transfers the knowledge in the teacher parameters within the first few epochs, and the remaining epochs merely do the fine-tuning (Phase 2) job. This implies that the training of WD is efficient, since we can just train the parameter generator for several epochs first, then fine-tune the generated network as in KD, and finally obtain a much better result than KD.\nThough we could train the parameter generator for just a few epochs as suggested, Phase 1 is still time-consuming. The reasons are two folds: 1) the parameter generator consumes a lot of memory and we have to resort to gradient accumulation; 2) the parameter generator involves many large matrix multiplications. For the experiments in Table 1 and Table 2, it takes us 0.66 days for WD to finish training on average, whereas 0.55 days for the teacher network baseline and 0.31 days for both the student network baseline and KD."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 Knowledge Distillation",
      "text" : "Knowledge distillation (Hinton et al., 2015; Freitag et al., 2017) is a widely used model acceleration and compression technique (Jiao et al., 2019; Sanh et al., 2019; Liu et al., 2020). It treats the network predictions as the knowledge learned by the teacher network, since these predicted distributions contain the ranking information on similarities among categories. It then transfers this knowledge to the student network by enforcing the student network to have similar predictions. The followed work extends this idea by providing more knowledge from different sources to the student network. FitNets (Romero et al., 2015) uses not only the predictions but also the intermediate representations learned by the teacher network to supervise the student network. For the Seq2Seq model, Kim and Rush (2016) proposes to use the generated sequences as the sequence-level knowledge to guide the student network training. Moreover, self-knowledge distillation (Hahn and Choi, 2019) even shows that knowledge (representations) from the student network itself can improve the performance.\nOur weight distillation, on the other hand, explores a new source of knowledge and a new way to leverage this knowledge. It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants."
    }, {
      "heading" : "6.2 Transfer Learning",
      "text" : "Transfer learning aims at transferring knowledge from a source domain to a target domain. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019).\nPerhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro-\nduces a translation model from a given languagespecific embedding. Though we similarly employ the idea of a parameter generator, our weight distillation aims at transferring knowledge from one model to another rather than from one translation task to another. Therefore our parameter generator takes a model instead of a language-specific embedding as its input and is only used once."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we propose weight distillation to transfer knowledge in the parameters of the teacher network to the student network. It generates the student network from the teacher network via a parameter generator. Our experiments on three machine translation tasks show that weight distillation consistently outperforms knowledge distillation by producing a faster and better student network."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Science Foundation of China (Nos. 61876035 and 61732005), the National Key R&D Program of China (No. 2019QY1801), and the Ministry of Science and Technology of the PRC (Nos. 2019YFF0303002 and 2020AAA0107900). The authors would like to thank anonymous reviewers for their comments."
    }, {
      "heading" : "A Appendices",
      "text" : "Hyper-parameters. We tune the learning rate and warmup steps in Phase 2 of WD. We use the grid search to select the learning rate in [1× 10−4, 3× 10−4, 5× 10−4, 7× 10−4, 9× 10−4] and warmup steps in [1000, 2000, 3000, 4000, 5000] that have the best average BLEU performance in all validation sets. Datasets. Detailed data statistics as well as the URLs of three machine translation tasks we used, including WMT16 English-Roman (En-Ro), NIST12 Chinese-English (Zh-En), and WMT14 English-German (En-De), are shown in Table 6.\nFor En-Ro, the training set consists of 0.6M bilingual sentence pairs. The validation set newsdev2016 contains 1999 pairs and the test set newstest2016 contains 1999 pairs. For Zh-En, the training set consists of 1.8M bilingual sentence pairs. The validation set mt06 contains 1,664 pairs and the test set mt08 contains 1,357 pairs. For En-De, the training set consists of 4.5M bilingual sentence pairs. The validation set newstest-2013 contains 3,000 pairs and the test set newstest-2014 contains 3,003 pairs. Runtime. To compare the average runtime for each approach, Table 7 shows the actual number of\nupdates and runtime. For the baseline models (i.e., Teacher, TINY and SMALL) and KD, we record their runtime in the Phase 1 entry because they only need to be trained once.\nOne can observe that in Table 7, Phase 2 of WD generally consumes similar or less time as well as the number of updates than other approaches. This is because the model is already close to the optimum before the fine-tuning (Phase 2). Table 7 also shows that the number of updates in Phase 1 of WD is much less than other approaches, yet its training time is much longer. This phenomenon is more obvious in Transformer-deep models. This is because one step in Phase 1 of WD is 2.11× slower than in Phase 2 of WD. Decoder. We also investigate how WD’s performance (on the validation set) and speed change given different decoder depths and widths. We choose the speed of WD to compute the speedup of different decoder depths and widths. Although the actual speedup of KD will not be exactly the same as the one of WD due to their different decoding results, they are close.\nAs shown in Table 8, WD is robust to different sized decoders, with both BLEU and speed significantly outperform KD. WD consistently outperforms KD by about 1 BLEU point under various decoder depths and widths. Interestingly, we find that pruning the layers degrades the performance more than shrinking its width, but it provides a higher speedup. Taking the student network with depth 2 and width 512 as an example, if we shrink the depth from 2 to 1, there is a decrease of 1.21 BLEU points in WD but with 1.12× speedup. When we shrink the width from 512 to 256, it leads to a moderate decrease of 0.59 BLEU points yet with only 1.06× speedup. This might be because layers are computed sequentially and wider matrices enjoy the parallel computation acceleration provided by modern GPUs. Loss. In Table 7, we observe that WD generates student networks that are superior to KD. We believe that this is because WD converges to a better\noptimum. To examine this hypothesis, we study its loss in Fig. 5. As can be seen, WD does obtain much lower train and valid losses than KD. We also see that Phase 1 already outperforms KD at the end. Given the fact that Phase 1 does the initialization job for Phase 2 and Phase 2 is KD exactly, the way WD works can be treated as providing a good start."
    } ],
    "references" : [ {
      "title" : "Training deeper neural machine translation models with transparent attention",
      "author" : [ "Ankur Bapna", "Mia Chen", "Orhan Firat", "Yuan Cao", "Yonghui Wu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Bapna et al\\.,? 2018",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2018
    }, {
      "title" : "Boosting for transfer learning",
      "author" : [ "Wenyuan Dai", "Qiang Yang", "Gui-Rong Xue", "Yong Yu." ],
      "venue" : "Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, volume 227 of",
      "citeRegEx" : "Dai et al\\.,? 2007",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2007
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Why does unsupervised pre-training help deep learning",
      "author" : [ "Dumitru Erhan", "Aaron C. Courville", "Yoshua Bengio", "Pascal Vincent" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2010
    }, {
      "title" : "Ensemble distillation for neural machine translation",
      "author" : [ "Markus Freitag", "Yaser Al-Onaizan", "Baskaran Sankaran." ],
      "venue" : "CoRR, abs/1702.01802.",
      "citeRegEx" : "Freitag et al\\.,? 2017",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2017
    }, {
      "title" : "Knowledge transfer via multiple model local structure mapping",
      "author" : [ "Jing Gao", "Wei Fan", "Jing Jiang", "Jiawei Han." ],
      "venue" : "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada,",
      "citeRegEx" : "Gao et al\\.,? 2008",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2008
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sar-",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Explaining sequence-level knowledge distillation as dataaugmentation for neural machine translation",
      "author" : [ "Mitchell A. Gordon", "Kevin Duh." ],
      "venue" : "CoRR, abs/1912.03334.",
      "citeRegEx" : "Gordon and Duh.,? 2019",
      "shortCiteRegEx" : "Gordon and Duh.",
      "year" : 2019
    }, {
      "title" : "Selfknowledge distillation in natural language processing",
      "author" : [ "Sangchul Hahn", "Heeyoul Choi." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing, RANLP 2019, Varna, Bulgaria, September 2-4,",
      "citeRegEx" : "Hahn and Choi.,? 2019",
      "shortCiteRegEx" : "Hahn and Choi.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "CoRR, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "What does BERT learn about the structure of language",
      "author" : [ "Ganesh Jawahar", "Benoı̂t Sagot", "Djamé Seddah" ],
      "venue" : "In Proceedings of the 57th Conference of the Association for Computational Linguistics,",
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Instance weighting for domain adaptation in NLP",
      "author" : [ "Jing Jiang", "ChengXiang Zhai." ],
      "venue" : "ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 2330, 2007, Prague, Czech Republic. The Association",
      "citeRegEx" : "Jiang and Zhai.,? 2007",
      "shortCiteRegEx" : "Jiang and Zhai.",
      "year" : 2007
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "CoRR, abs/1909.10351.",
      "citeRegEx" : "Jiao et al\\.,? 2019",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation",
      "author" : [ "Jungo Kasai", "Nikolaos Pappas", "Hao Peng", "James Cross", "Noah A. Smith." ],
      "venue" : "CoRR, abs/2006.10369.",
      "citeRegEx" : "Kasai et al\\.,? 2020",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1317–1327. The",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "From research to production and back: Ludicrously fast neural machine translation",
      "author" : [ "Young Jin Kim", "Marcin Junczys-Dowmunt", "Hany Hassan", "Alham Fikri Aji", "Kenneth Heafield", "Roman Grundkiewicz", "Nikolay Bogoychev." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning light-weight translation models from deep transformer",
      "author" : [ "Bei Li", "Ziyang Wang", "Hui Liu", "Quan Du", "Tong Xiao", "Chunliang Zhang", "Jingbo Zhu." ],
      "venue" : "CoRR, abs/2012.13866.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Fastbert: a selfdistilling BERT with adaptive inference time",
      "author" : [ "Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Haotang Deng", "Qi Ju." ],
      "venue" : "CoRR, abs/2004.02178.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixed precision training",
      "author" : [ "Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory F. Diamos", "Erich Elsen", "David Garcı́a", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu" ],
      "venue" : null,
      "citeRegEx" : "Micikevicius et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Micikevicius et al\\.",
      "year" : 2018
    }, {
      "title" : "All you need is a good init",
      "author" : [ "Dmytro Mishkin", "Jiri Matas." ],
      "venue" : "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.",
      "citeRegEx" : "Mishkin and Matas.,? 2016",
      "shortCiteRegEx" : "Mishkin and Matas.",
      "year" : 2016
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Trans. Knowl. Data Eng., 22(10):1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextual parameter generation for universal neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Mrinmaya Sachan", "Graham Neubig", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Platanios et al\\.,? 2018",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2018
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,",
      "citeRegEx" : "Romero et al\\.,? 2015",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "Ledyard R Tucker." ],
      "venue" : "Psychometrika, 31(3):279–311.",
      "citeRegEx" : "Tucker.,? 1966",
      "shortCiteRegEx" : "Tucker.",
      "year" : 1966
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Hat: Hardware-aware transformers for efficient natural language processing",
      "author" : [ "Hanrui Wang", "Zhanghao Wu", "Zhijian Liu", "Han Cai", "Ligeng Zhu", "Chuang Gan", "Song Han" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning deep transformer models for machine translation",
      "author" : [ "Qiang Wang", "Bei Li", "Tong Xiao", "Jingbo Zhu", "Changliang Li", "Derek F. Wong", "Lidia S. Chao." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguis-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Niutrans: An open source toolkit for phrasebased and syntax-based machine translation",
      "author" : [ "Tong Xiao", "Jingbo Zhu", "Hao Zhang", "Qiang Li." ],
      "venue" : "The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the System",
      "citeRegEx" : "Xiao et al\\.,? 2012",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2012
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Con-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Knowledge Distillation (KD) is a popular model acceleration and compression approach (Hinton et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : "It also agrees with the recent success in pre-training (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019), where parameters reusing plays the main role.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "It also agrees with the recent success in pre-training (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019), where parameters reusing plays the main role.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "It also agrees with the recent success in pre-training (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019), where parameters reusing plays the main role.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "In this work, we choose Transformer (Vaswani et al., 2017) for study because it is one of the stateof-the-art neural models in natural language processing.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "For more details, we refer the reader to (Vaswani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "The parameter generator further divides each group into smaller subsets with weight matrices from adjacent layers, because the adjacent layers function similarly (Jawahar et al., 2019) and so as their weights.",
      "startOffset" : 162,
      "endOffset" : 184
    }, {
      "referenceID" : 27,
      "context" : "For all datasets, we tokenize every sentence using the script in the Moses toolkit and segment every word into subword units using Byte-Pair Encoding (Sennrich et al., 2016).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 32,
      "context" : "We remove sentences with more than 250 subword units (Xiao et al., 2012).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : "We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : ", 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "All experiments are done on 8 NVIDIA TITIAN V GPUs with mixed-precision training (Micikevicius et al., 2018).",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 28,
      "context" : "Note that our method can also be seen as an advanced version of Tucker Decomposition (Tucker, 1966).",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "For the KD baseline, we adopt Kim and Rush (2016)’s method, which has proven to be the most effective for Seq2Seq models (Kim et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "The choices of student networks are based on the observation that the encoder has a greater impact on performance and the decoder dominates the decoding time (Kasai et al., 2020).",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "We attribute the success of WD to that the parameter generator uses parameters of the teacher network to provide a good initialization for the student network, as Phase 1 behaves like the initialization, and the effectiveness of a good initialization has been widely proven (Erhan et al., 2010; Mishkin and Matas, 2016).",
      "startOffset" : 274,
      "endOffset" : 319
    }, {
      "referenceID" : 20,
      "context" : "We attribute the success of WD to that the parameter generator uses parameters of the teacher network to provide a good initialization for the student network, as Phase 1 behaves like the initialization, and the effectiveness of a good initialization has been widely proven (Erhan et al., 2010; Mishkin and Matas, 2016).",
      "startOffset" : 274,
      "endOffset" : 319
    }, {
      "referenceID" : 7,
      "context" : "This is due to that KD has a form similar to data augmentation (Gordon and Duh, 2019).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "If the teacher and student networks have different depths, we initialize the student network with the bottom layers of the teacher network (Sanh et al., 2019).",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 31,
      "context" : "Among these, the encoder weight matrices produce the most significant result, which agrees with the previous study claiming that the encoder is more important than the decoder (Wang et al., 2019; Bapna et al., 2018).",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "Among these, the encoder weight matrices produce the most significant result, which agrees with the previous study claiming that the encoder is more important than the decoder (Wang et al., 2019; Bapna et al., 2018).",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "Knowledge distillation (Hinton et al., 2015; Freitag et al., 2017) is a widely used model acceleration and compression technique (Jiao et al.",
      "startOffset" : 23,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Knowledge distillation (Hinton et al., 2015; Freitag et al., 2017) is a widely used model acceleration and compression technique (Jiao et al.",
      "startOffset" : 23,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : ", 2017) is a widely used model acceleration and compression technique (Jiao et al., 2019; Sanh et al., 2019; Liu et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : ", 2017) is a widely used model acceleration and compression technique (Jiao et al., 2019; Sanh et al., 2019; Liu et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : ", 2017) is a widely used model acceleration and compression technique (Jiao et al., 2019; Sanh et al., 2019; Liu et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "FitNets (Romero et al., 2015) uses not only the predictions but also the intermediate representations learned",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "Moreover, self-knowledge distillation (Hahn and Choi, 2019) even shows that knowledge (representations) from the student network itself can improve the performance.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al.",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al.",
      "startOffset" : 238,
      "endOffset" : 278
    }, {
      "referenceID" : 1,
      "context" : "Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al.",
      "startOffset" : 238,
      "endOffset" : 278
    }, {
      "referenceID" : 23,
      "context" : ", 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al.",
      "startOffset" : 111,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : ", 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al.",
      "startOffset" : 111,
      "endOffset" : 150
    }, {
      "referenceID" : 33,
      "context" : ", 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 18,
      "context" : ", 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : ", 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 177
    } ],
    "year" : 2021,
    "abstractText" : "Knowledge distillation has proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88∼2.94× faster than the large network but with competitive BLEU performance. When fixing the size of the small networks, weight distillation outperforms knowledge distillation by 0.51∼1.82 BLEU points.",
    "creator" : "LaTeX with hyperref"
  }
}