{
  "name" : "2021.acl-long.106.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Finding the K-best Non-projective Dependency Trees",
    "authors" : [ "Ran Zmigrod", "Tim Vieira", "Ryan Cotterell" ],
    "emails" : [ "rz279@cam.ac.uk", "tim.f.vieira@gmail.com", "ryan.cotterell@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1324–1337\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1324"
    }, {
      "heading" : "1 Introduction",
      "text" : "Non-projective, graph-based dependency parsers are widespread in the NLP literature. (McDonald et al., 2005; Dozat and Manning, 2017; Qi et al., 2020). However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective\n1Our implementation is available at https://github. com/rycolab/spanningtrees.\ndependency parsing (Hall, 2007; Hall et al., 2007; Agić, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and nuances of Camerini et al. (1980)’s algorithm.2\nWe make a few changes to Camerini et al. (1980)’s algorithm, which result in both a simpler algorithm and simpler proof of correctness.3\nFirstly, both algorithms follow the key property that we can find the second-best tree of a graph by removing a single edge from the graph (Theorem 1); this property is used iteratively to enumerate the K-best trees in order. Our approach to finding the second-best tree (see §3) is faster because of it performs half as many of the expensive cycle-contraction operations (see §2). Overall, this change is responsible for our 1.39x speed-up\n2In fact, an anonymous reviewer called it “one of the most ‘feared’ algorithms in dependency parsing.”\n3While our algorithm is by no means simple, an anonymous reviewer called it “a big step in that direction.”\n(see §4). Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix.\nIn addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4\nFinding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al. (1979). However, no algorithm exists for Kbest decoding of dependency trees subject to a root constraint. As such, we provide the first K-best algorithm that returns dependency trees that obey the root constraint.\nTo motivate the practical necessity of our extension, consider Fig. 1. Fig. 1 shows the percentage of trees that violate the root constraint when doing one-best and 50-best decoding for 63 languages from the UD treebank (Nivre et al., 2018) using the pre-trained model of Qi et al. (2020).5,6 We find that decoding without the root constraint has a much more extreme effect when decoding the 50-best than the one-best. Specifically, we observe that on average, the number of violations of the root constraint increased by 13 times, with the worst increase being 44 times. The results thus suggest that finding K-best trees that obey the root constraint from a non-projective dependency parser requires a specialist algorithm. We provide a more detailed results table in App. A, including root constraint violation rates forK=5, K=10, andK=20. Furthermore, we note that the K-best algorithm may also be used for marginalization of latent variables (Correia et al., 2020) and for constructing parsers with global scoring functions (Lee et al., 2016).\n4There are certain exceptions to this such as the Prague Treebank (Bejček et al., 2013).\n5Zmigrod et al. (2020) conduct a similar experiment for only the one-best tree.\n6We note that Qi et al. (2020) do apply the root constraint for one-best decoding, albeit with a sub-optimal algorithm."
    }, {
      "heading" : "2 Finding the Best Tree",
      "text" : "We consider the study of rooted directed weighted graphs, which we will abbreviate to simply graphs.7 A graph is given by G = (ρ,N , E) whereN is a set of N + 1 nodes with a designated root node ρ ∈ N and E is a set of directed weighted edges. Each edge e = (iA j) ∈ E has a weight w(e) ∈ R+. We assume that self-loops are not allowed in the graph (i.e., (iA i) 6∈ E). Additionally, we assume our graph is not a multi-graph, therefore, there can exist at most one edge from node i to node j.8 When it is clear from context, we abuse notation and use j ∈ G and e ∈ G for j ∈ N and e ∈ E respectively. When discussing runtimes, we will assume a fully connected graph (|E| = N2).9 An arborescence (henceforth called a tree) of G is a subgraph d = (ρ,N , E ′) such that E ′ ⊆ E and the following is true:\n1. For all j ∈ N r {ρ}, |{( A j) ∈ E ′}| = 1.\n2. d does not contain any cycles.\nOther definitions of trees can also include that there is at least one edge emanating from the root. However, this condition is immediately satisfied by the above two conditions. A dependency tree\n7As we use the algorithm in Zmigrod et al. (2020) as our base algorithm, we borrow their notation wherever convenient.\n8We make this assumption for simplicity, the algorithms presented here will also work with multi-graphs. This might be desirable for decoding labeled dependency trees. However, we note that in most graph-based parsers such as Qi et al. (2020) and Ma and Hovy (2017), dependency labels are extracted after the unlabeled tree has been decoded.\n9We make this assumption as in the context of dependency parsing, we generate scores for each possible edge. Furthermore, (Tarjan, 1977) prove that the runtime of finding the best tree for dense graphs is O(N2). This is O(|E| logN) in the non-dense case.\nd = (ρ,N , E ′) is a tree with the extra constraint\n3. |{(ρA ) ∈ N ′}| = 1\nThe set of all trees and dependency trees in a graph are given by A(G) and D(G) respectively. The weight of a tree is given by the sum of its edge weights10\nw(d) = ∑ e∈d w(e) (1)\nThis paper concerns finding the K highestweighted (henceforce called K-best) tree or dependency tree, these are denoted byG(K) andG[K]\nrespectively. Tarjan (1977); Camerini et al. (1979) provided the details for an O(N2) algorithm for decoding the one-best tree. This algorithm was extended by Gabow and Tarjan (1984) to find the best dependency tree in O(N2) time. We borrow the algorithm (and notation) of Zmigrod et al. (2020), who provide an exposition and proofs of these algorithms in the context of non-projective dependency parsing. The pseudocode for finding G(1) and G[1]\nis given in Fig. 3. We briefly describe the key components of the algorithm.11\nThe greedy graph of G is denoted by −A G = (ρ,N , E ′) where E ′ contains the highest weighted incoming edge to each non-root node. Therefore, if −A G has no cycles, then −A G = G(1). A cycle C in −A G\nis called a critical cycle. If we encounter a critical cycle in the algorithm, we contract the graph by the critical cycle. A graph contraction, G/C , by a cycle C replaces the nodes in C by a mega-node c such that the nodes of G/C are N r C ∪ {c}. Furthermore, for each edge e = (iA j) ∈ G:\n1. If i 6∈ C and j ∈ C, then e′ = (iA c) ∈ G/C such that w(e′) = w(e) + w (−A Cj ) where Cj is the subgraph of C rooted at j.\n2. If i ∈ C and j 6∈ C, then e′ = (cA j) ∈ G/C such that w(e′) = w(e).\n3. If i 6∈ C and j 6∈ C, then e ∈ G/C .\n4. If i ∈ C and j ∈ C, then there is no edge related to (iA j) in G/C .\nThere also exists a bookkeeping function π such 10For inference, the weight of a trees often decomposes multiplicatively rather than additively over the edges. One can take the exponent (or logarithm) of the original edge weights to make the weights distribute additively (or multiplicative).\n11For a more complete and detailed description as well as a proof of correctness, please refer to the original manuscripts.\nthat for all e′ ∈ G/C , π(e′) ∈ G. This bookkeeping function returns the edge in the original graph that led to the creation of the edge in the contracted graph using one of the constructions above.\nFinding G(1) is then the task of finding a contracted graph G′ such that −A G′ = G′(1). Once this is\ndone, we can stitch back the cycles we contracted. If G′ = G/C , for any d ∈ A(G/C), d# C ∈ A(G) is the tree made with edges π(d) (π applied to each edge d) and −A Cj where Cj is the subgraph of the nodes in C rooted at node j and π(e) = (iA j) for e = (iA c) ∈ d. The contraction weighting scheme means that w(d) = w(d# C) (Georgiadis, 2003). Therefore, G(1) = (G′(1) # C)(1).\nThe strategy for finding G[1] is to find the contracted graph for G(1) and attempt to remove edges emanating from the root. This was first proposed by Gabow and Tarjan (1984). When we consider removing an edge emanating from the root, we are doing this in a possibly contracted graph, and so an edge (ρA j) may exist multiple times in the graph. We denote G\\\\e to be the graph G with all edges with the same end-points as e removed. Fig. 2 gives an example of a graph G, its best tree G(1), and its best dependency tree G[1].\nThe runtime complexity of finding G(1) or G[1]\nis O(N2) for dense graphs by using efficient priority queues and sorting algorithms (Tarjan, 1977; Gabow and Tarjan, 1984). We assume this runtime\ngiven in (a). Suppose that the solid edges in (a) represent −A G . Therefore, G(1) = −A G . Next, suppose that we know that e = (2A 4) ∈ G(1) is not in G(2). Then one of the dashed edges in (b) must be in G(2) as 4 must have an incoming edge. The edges emanating from ρ and 1 make up the set of blue edges, b(G, e,G(1)) while the edge emanating from 3 makes the set of red edges, r(G, e,G(1)). If e′ ∈ b(G, e,G(1)) is in G(2) as in (c), then the solid lines in (c) make a tree and G(2) differs from G(1) by exactly one blue edge of e. Otherwise, we know that e′′ ∈ r(G, e,G(1)) is in G(2) as in (d). However, the solid edges in (d) contain a cycle between 3 and 4 with edges e′′ and f . We could break the cycle at 3 and include edge f ′ in our tree as in (e). However, while the solid edges in (e) make a valid tree, as w(e) > w(e′′) and w(f) > w(f ′), the tree given by the solid lines of (f) will have a higher weight. This would mean that e ∈ G(2) which leads to a contradiction. Therefore, we must break the cycle at 4 , which leads us to a tree as in (c). Consequently, G(2) will differ from G(1) by exactly one blue edge of e.\nfor the remainder of the paper."
    }, {
      "heading" : "3 Finding the Second Best Tree",
      "text" : "In the following two sections, we provide a simplified reformulation of Camerini et al. (1980) to find the K-best trees. The simplifications additionally provide a constant time speed-up over Camerini et al. (1980)’s algorithm. We discuss the differences throughout our exposition.\nThe underlying concept behind finding the Kbest tree, is that G(K) is the second best tree G′(2) of some subgraph G′ ⊆ G. In order to explore the space of subgraphs, we introduce the concept of edge inclusion and exclusion graphs.\nDefinition 1 (Edge inclusion and exclusion). For any graph G and edge e ∈ G, the edge-inclusion graph G+ e ⊂ G is the graph such that for any d ∈ A(G+ e), e ∈ d. Similarly, the edgeexclusion graphG− e ⊂ G is the graph such that for any d ∈ A(G− e), e 6∈ d.\nWhen we discuss finding theK-best dependency trees in §5, we implicitly change the above definition to use D(G+ e) and D(G− e) instead of A(G+ e) and A(G− e) respectively.\nIn this section, we will specifically focus on finding G(2), we extend this to finding the G(k) in §4. Finding G(2) relies on the following fundamental theorem.\nTheorem 1. For any graph G and e ∈ G(1)\nG(2) = (G− e)(1) (6) where\ne = argmax e′∈G(1)\nw ( (G− e′)(1) ) (7)\nTheorem 1 states that we can find G(2) by identifying an edge e ∈ G(1) such that G(2) = (G− e)(1). We next show an efficient method for identifying this edge, as well as the weight of G(2) without actually having to find G(2).\nDefinition 2 (Blue and red edges). For any graph\nG, tree d ∈ A(G), and edge e = (iA j) ∈ d, the set of blue edges b(G, e, d) and red edges r(G, e, d) are defined by12\nb(G, e, d) def= {e′ =(i′A j) | w ( e′ ) ≤ w(e),\ndr {e} ∪ {e′} ∈ A(G)} (2)\nr(G, e, d) def= {e′ =(i′A j) | e′ 6∈ b(G, e, d)} (3)\nAn example of blue and red edges are given in Fig. 4. Lemma 1. For any graph G, if G(1) = −A G , then for some e ∈ G(1) and e′ ∈ b(G, e,G(1)) G(2) = G(1) r {e} ∪ {e′} (8)\nLemma 1 can be understood more clearly by following the worked example in Fig. 4. The moral of Lemma 1 is that in the base case where there are no critical cycles, we only need to examine the blue edges of the greedy graph to find the second best tree. Furthermore, our second best tree will only differ from our best tree by exactly one blue edge. Camerini et al. (1980) make use of the concepts of the blue and red edge sets, but rather than consider a base case as Lemma 1, they propose an ordering in which to visit the edges of the graph. This results in several properties about the possible orderings,\n12We can also define b(G, e, d) as (i′ A j) ∈ b(G, e, d) ⇐⇒ i′ is an ancestor of j in d and r(G, e, d) as (i′ A j) ∈ r(G, e, d) ⇐⇒ i′ is a descendant of j in d. This equivalence exists as we can only swap an incoming edge to j in d without introducing a cycle if the new edge emanates from an ancestor of j. The exposition using ancestors and descendants is more similar to the exposition originally presented by Camerini et al. (1980).\nrequiring much more complicated proofs.\nDefinition 3 (Swap cost). For any graph G, tree d ∈ A(G), and edge e ∈ d, the swap cost denotes the minimum change to a tree weight to replace e by a single edge in d. It is given by\nwG,d(e) = min e′∈b(G,e,d)\n( w(e)− w ( e′ )) (4)\nWe will shorthand wG(e) to mean wG,G(1)(e). Corollary 1. For any graph G, if G(1) = −A G , then G(2) = (G− e)(1) where e is given by e = argmin\ne′∈G(1) wG ( e′ )\n(5)\nFurthermore, w ( G(2) ) = w ( G(1) ) − wG(e).\nCorollary 1 provides us a procedure for finding the best edge to remove to find G(2) as well as its weight in the base case of G having no critical cycles. We next illustrate what must be done in the recursive case when a critical cycle exists.\nLemma 2. For any G with a critical cycle C, either G(2) = (G/C)(2) # C (with w ( G(2) ) = w ( (G/C) (2) ) ) or G(2) = (G− e)(1)\n(with w ( G(2) ) = w ( G(1) ) − wG(e)) for some e ∈ C ∩G(1). Combining Corollary 1 and Lemma 2, we can directly modify opt to find the weight of G(2) and the edge we must remove to obtain it. We detail this algorithm as next in Fig. 5.\nTheorem 2. For any graph G, executing next(G) returns G(1) and 〈w, e〉 such that G(2) = (G− e)(1) and w ( G(2) ) = w.\nRuntime analysis. We know that without lines 5, 6, 9 and 10, next is identical to opt and so will run in O(N2). We call w at most N + 2 times during a full call of next: N times from lines 5 and 9 combined, once from Line 6, and once from Line 10. To find w, we first need to find the set of blue edges, which can be done in O(N) by computing the reachability graph. Then, we need another O(N) to find the minimising value. Therefore, next does O(N2) extra work than opt and so retains the runtime of O(N2). Camerini et al. (1980) require G(1) to be known ahead of time. This results in having to run the original algorithm in O(N2) time and then having to do the same amount of work as next because they must still contract the graph. Therefore, next has a constant-time speed-up over its counterpart in Camerini et al. (1979)."
    }, {
      "heading" : "4 Finding the K th Best Tree",
      "text" : "In the previous section, we found an efficient method for finding G(2). We now utilize this method to efficiently find the K-best trees.\nLemma 3. For any graph G and K > 1, there exists a subgraph G′ ⊆ G and 1 ≤ l < K such that G(l) = G′(1) and G(K) = G′(2).\nLemma 3 suggests that we can find the K-best trees by only examining the second best trees of subgraphs of G. This idea is formalized as algorithm kbest in Fig. 7. A walk-through of the exploration space using kbest for our example graph in Fig. 2 is shown in Fig. 6.\nTheorem 3. For any graph G and K> 0, at any iteration 1 ≤ k ≤ K, kbest(G,K) returns G(k). Runtime analysis. We call next once at the\nstart of the algorithm, then every subsequent iteration we make two calls to next. As we haveK−1 iterations , the runtime of kbest isO(KN2). The first call to next in each iteration finds the K th\nbest tree as well as an edge to remove. Camerini et al. (1980) make one call to of opt and two calls to next which only finds the weight-edge pair of our algorithm. Therefore, kbest has a constant time speed-up on the original algorithm.13\nA short experiment. We empirically measure the constant time speed-up between kbest and the original algorithm of Camerini et al. (1980). We take the English UD test set (as used for Fig. 1) and find the 10, 20, and 50 best spanning trees using both algorithms.14 We give the results of the experiment in Tab. 1.15 We note that on average kbest leads to a 1.39 times speed-up. This is\n13In practice, we maintain a set of edges to include and exclude to save space.\n14Implementations for both versions can be found in our code release (see footnote 1)\n15The experiment was conducted using an Intel(R) Core(TM) i7-7500U processor with 16GB RAM.\nlower than we anticipated as we have to make half as many calls to next than the original algorithm. However, in the original next of Camerini et al. (1980), we do not require to stitch together the tree, which may explain the slightly smaller speed-up.\n5 Finding the K th Best Dependency Tree\nIn this section, we present a novel extension to the algorithm presented thus far, that allows us to efficiently find the K-best dependency trees. Recall that we consider dependency trees to be spanning trees with a root constraint such that only one edge may emanate from ρ. Naı̈vely, we can use kbest where we initialize the queue with (G+ eρ)(1) for each eρ = (ρA j) ∈ G. However, this adds a O(N3) component to our runtime as we have to call opt N times. Instead, our algorithm maintains the O(KN2) runtime as the regular K-best algorithm. We begin by noting that we can find second best dependency tree, by finding either the best dependency tree with a different root edge or the second best tree with the same root edge.\nLemma 4. For any graph G and edge eρ = (ρA j) ∈ G[1], G[2] = (G− eρ)[1] or G[2] = (G+ eρ) [2].\nLemma 5. For any graph G and K > 1, if e = (ρA j) ∈ G[K], then either e is not in any of the K−1-best trees or there exists a subgraph G′ ⊆ G and 1 ≤ l < K such that G[l] = G′[1], e ∈ G′[1] and G[K] = G′[2].\nLemma 5 suggests that we can find the K-best dependency trees, by examining the second best dependency trees of subgraphs of G or finding the best dependency tree with a unique root edge. This\nidea is formalized as algorithm kbest dep in Fig. 9. A walk-through of the exploration space using kbest dep for our example graph in Fig. 2 is shown in Fig. 8.\nTheorem 4. For any graph G and K ≥ 1, at iteration 1 ≤ k ≤ K, kbest dep(G,K) returns G[k].\nRuntime analysis. At the start of the algorithm, we call opt twice and next once. Then, at each iteration we either make two calls two next, or two calls to opt and one call to next. As both algorithms have a runtime ofO(N2), each iteration has a runtime of O(N2). Therefore, running K iterations gives a runtime of O(KN2)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we provided a simplification to Camerini et al. (1980)’sO(KN2)K-best spanning trees algorithm. Furthermore, we provided a novel extension to the algorithm that decodes the K-best dependency trees in O(KN2). We motivated the need for this new algorithm as using regularK-best decoding yields up to 36% trees which violation the root constraint. This is a substantial (up to 44 times) increase in the violation rate from decoding the one-best tree, and thus such an algorithm is even more important than in the one-best case. We hope that this paper encourages future research in K-best dependency parsing."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the reviewers for their valuable feedback and suggestions to improve this work. The first author is supported by the University of Cambridge School of Technology ViceChancellor’s Scholarship as well as by the University of Cambridge Department of Computer Science and Technology’s EPSRC.\nEthical Concerns\nWe do not foresee how the more efficient algorithms presented this work exacerbate any existing ethical concerns with NLP systems."
    }, {
      "heading" : "A Supplementary Materials for Section 1 (Introduction)",
      "text" : "Results Table for Fig. 1\nLanguage |Train| |Test| Root Constraint Violation Rate (%) K = 1 K = 5 K = 10 K = 20 K = 50\nCzech 68495 10148 0.45 5.07 6.18 6.76 7.67 Russian 48814 6491 0.49 5.07 6.58 7.66 8.99 Estonian 24633 3214 0.93 5.59 7.02 8.24 9.42 Korean 23010 2287 0.96 6.68 9.51 11.91 14.74 Latin 16809 2101 0.52 5.17 5.57 6.25 7.62 Norwegian 15696 1939 0.52 4.26 5.20 6.22 7.38 Ancient Greek 15014 1047 0.57 4.74 7.00 8.38 10.69 French 14450 416 1.68 3.85 4.95 5.81 6.98 Spanish 14305 1721 0.17 2.25 3.25 3.96 4.89 Old French 13909 1927 0.52 6.81 9.41 11.38 13.01 German 13814 977 1.54 5.12 6.37 7.63 9.06 Polish 13774 1727 0.00 4.76 7.86 10.11 13.00 Hindi 13304 1684 0.18 1.34 2.19 2.98 4.04 Catalan 13123 1846 0.54 2.32 2.97 3.68 4.51 Italian 13121 482 0.21 4.02 5.66 7.25 9.19 English 12543 2077 0.48 9.12 10.73 11.12 11.34 Dutch 12264 596 0.67 3.39 4.18 4.82 5.59 Finnish 12217 1555 0.39 4.72 6.12 7.39 9.15 Classical Chinese 11004 2073 0.96 22.52 25.95 28.09 29.91 Latvian 10156 1823 0.88 7.05 8.77 9.95 11.31 Bulgarian 8907 1116 0.27 4.66 6.73 8.16 10.29 Slovak 8483 1061 0.38 4.81 5.34 5.29 5.29 Portuguese 8328 477 0.42 3.31 4.15 4.76 5.75 Romanian 8043 729 0.41 1.26 1.66 2.16 2.81 Japanese 7125 550 0.00 5.13 6.24 7.20 8.79 Croatian 6914 1136 0.88 2.90 3.71 4.44 5.62 Slovenian 6478 788 0.38 2.66 3.53 4.59 5.79 Arabic 6075 680 0.29 3.79 4.15 4.72 5.27 Ukrainian 5496 892 0.90 7.49 9.15 10.13 11.72 Basque 5396 1799 0.67 3.64 5.06 6.67 8.71 Hebrew 5241 491 1.02 2.81 4.01 5.04 5.90 Persian 4798 600 0.67 2.43 3.47 4.28 5.25 Indonesian 4477 557 1.26 4.06 5.48 6.65 8.25 Danish 4383 565 0.53 4.35 5.59 6.35 7.45 Swedish 4303 1219 1.23 4.63 6.08 7.09 8.73 Old Church Slavonic 4124 1141 1.05 14.32 17.64 19.88 22.05 Urdu 4043 535 1.12 2.47 3.08 3.60 4.39 Chinese 3997 500 1.80 4.80 5.90 7.68 9.31 Turkish 3664 983 2.54 12.47 15.53 17.09 18.73 Gothic 3387 1029 0.78 8.65 11.18 13.10 14.86 Serbian 3328 520 0.19 2.04 2.60 3.16 4.23 Galician 2272 861 1.16 2.07 2.36 2.88 3.46 North Sami 2257 865 1.27 7.49 10.15 12.43 15.54 Armenian 1975 278 0.00 7.34 8.42 9.64 10.81 Greek 1662 456 0.44 3.20 4.19 4.80 5.82 Uyghur 1656 900 0.56 7.18 9.64 12.24 15.57 Vietnamese 1400 800 3.38 6.78 8.25 9.56 11.39 Afrikaans 1315 425 6.35 13.65 14.73 16.12 18.26 Wolof 1188 470 1.49 6.89 8.32 9.91 12.17 Maltese 1123 518 0.58 5.17 6.70 8.12 9.73 Telugu 1051 146 0.00 27.81 32.81 36.16 36.99 Scottish Gaelic 1015 536 0.75 7.16 8.97 10.20 11.75 Hungarian 910 449 4.23 7.44 8.66 9.82 10.75 Irish 858 454 2.42 7.14 8.68 10.23 11.73 Tamil 400 120 0.00 1.17 1.50 1.83 3.05 Marathi 373 47 2.13 20.85 21.70 27.34 33.36 Belarusian 319 253 0.79 5.61 9.05 8.99 7.27 Lithuanian 153 55 7.27 9.82 10.36 10.82 12.47 Kazakh 31 1047 2.58 7.97 10.68 13.45 17.41 Upper Sorbian 23 623 6.42 9.34 10.72 11.78 13.45 Kurmanji 20 734 23.57 27.06 29.22 30.87 33.33 Buryat 19 908 6.61 10.37 13.00 15.48 19.13 Livvi 19 106 12.26 14.15 15.00 15.99 17.68"
    }, {
      "heading" : "B Supplementary Materials for Section 3 (Finding the Second Best Tree)",
      "text" : "Theorem 1. For any graph G and e ∈ G(1)\nG(2) = (G− e)(1) (6) where\ne = argmax e′∈G(1)\nw ( (G− e′)(1) ) (7)\nProof. There must be at least one edge e ∈ G(1) such that e 6∈ G(2). Therefore, there exists an e ∈ G(1) such that G(2) = (G− e)(1). Now suppose by way of contradiction that e is not as given in (7). If we choose an e′ that satisfies (7), then by definition w ( (G− e′)(1) ) > w ( (G− e)(1) ) . As (G− e′)(1) 6= G(1), we arrive at a contradiction.\nLemma 1. For any graph G, if G(1) = −A G , then for some e ∈ G(1) and e′ ∈ b(G, e,G(1)) G(2) = G(1) r {e} ∪ {e′} (8)\nProof. By Theorem 1, we haveG(2) = (G− e)(1) where e = (iA j) is chosen according to (7). Consider the graph G− e; we have that −−−A G− e = G(1) r {e} ∪ {e′} where e′ is the second best incoming edge to j\nin G by the definition of the greedy graph.\n1. Case e′ ∈ b(G, e,G(1)): Then −−−A G− e is a tree and (G− e)(1) = −−−A G− e.\n2. Case e′ ∈ r(G, e,G(1)): Then, −−−A G− e has a cycle C by construction. Since this is a greedy graph,\ncycle C is critical. In the expansion phase of the 1-best algorithm, we will break the cycle C.\n(a) Case break C at j: Then, e′ 6∈ (G− e)(1) and we must choose an edge e′′ = (i′A j) to be in (G− e)(1). We require that e′′ ∈ b(G, e,G(1)) as we would otherwise re-introduce a cycle in the expansion phase, which is not possible. Therefore, G(2) = G(1) r {e} ∪ {e′′}.\n(b) Case break C at j′ 6= j: Then, there exists an edge f = (i′′A j′) ∈ C (and in G(1)) which is not in G(2). Instead, we choose f ′ = (i′A j′) to be in G(2). Therefore, G(2) = G(1) r {e, f} ∪ {e′, f ′}. However, it is not possible for f ′ and e to form a cycle and so d = G(1) r {f} ∪ {f ′} ∈ A(G) and w(d) > w ( G(2) ) . This is a contradiction as only\nw ( G(1) ) > w ( G(2) ) .\nLemma 2. For anyG with a critical cycle C, eitherG(2)=(G/C)(2) # C (with w ( G(2) ) =w ( (G/C) (2) ) )\nor G(2)=(G− e)(1) (with w ( G(2) ) =w ( G(1) ) − wG(e)) for some e ∈ C ∩G(1).\nProof. It must be that G(2) = (G/C)(2) # C or G(2) 6= (G/C)(2) # C.\n1. Case G(2) = (G/C)(2) # C: Since the weight of a tree is preserved during expansion, we are done.\n2. Case G(2) 6= (G/C)(2) # C: Then, for all e′∈(G/C)(1), π(e′)∈G(2). Therefore, if j is the entrance\nsite of C in (G/C)(1), G(2)=π((G/C)(1)) ∪ C (2) j . As C (1) j = −A Cj , by Corollary 1, C (2) j =(Cj − e)(1) for e ∈ C(1)j and w ( C (2) j ) = w ( C (1) j ) − wCj (e). Thus, G(2) = (G− e)(1) where e ∈ C∩d and\nw ( G(2) ) =w ( G(1) ) − wG(e).\nTheorem 2. For any graph G, executing next(G) returns G(1) and 〈w, e〉 such that G(2) = (G− e)(1) and w ( G(2) ) = w.\nProof. next(G) returns G(1) by the correctness of opt. We prove that w, e satisfy the above conditions.\n1. Case G(1) = −A G: Then, by Corollary 1 we can find the best edge to remove and the weight of G(2).\n2. Case G(1) 6= −A G: Then, G has a critical cycle C. By Lemma 2, we can either recursively call next ( G/C ) or examine the edges in C ∩G(1) to find the best edge to remove and the weight of G(2)."
    }, {
      "heading" : "C Supplementary Materials for Section 4 (Finding the K th Best Tree)",
      "text" : "Lemma 3. For any graph G and K > 1, there exists a subgraph G′ ⊆ G and 1 ≤ l < K such that G(l) = G′(1) and G(K) = G′(2).\nProof. There must exist some subgraphG′ ⊆ G such thatG(K) = G′(2). Suppose by way of contradiction that there does not exist an l < K such that G(l) = G′(1). However, since w ( G′(1) ) > w ( G(K+1) ) , G′(1) must be in the K-highest weighted trees. Therefore, there must exist an l such that G(l) = G′(1)\nTheorem 3. For any graph G and K>0, at any iteration 1 ≤ k ≤ K, kbest(G,K) returns G(k).\nProof. We prove this by induction on k. Base Case: Then, k = 1 and G(1) is returned by Theorem 2. Inductive Step: Assume that for all l ≤ k, at iteration l, G(l) is returned. Now consider iteration k + 1, by Lemma 3, we know that G(k+1) = G′(2) where G′(1) = G(l) for some l ≤ k. By the induction hypothesis, G(l) is returned at the lth iteration, and by Theorem 2, we have pushed G′(2) onto the queue. Therefore, we will return G(k+1).\nD Supplementary Materials for Section 5 (Finding the K th Best Dependency Tree)\nLemma 4. For any graph G and edge eρ = (ρA j) ∈ G[1], G[2] = (G− eρ)[1] or G[2] = (G+ eρ)[2].\nProof. If eρ 6∈ G[2], then clearly G[2] = (G− eρ)[1]. Otherwise, eρ ∈ G[2]. As eρ ∈ G[1], G[2] = (G+ eρ) [2].\nLemma 5. For any graph G and K > 1, if e = (ρA j) ∈ G[K], then either e is not in any of the K−1-best trees or there exists a subgraph G′ ⊆ G and 1 ≤ l < K such that G[l] = G′[1], e ∈ G′[1] and G[K] = G′[2].\nProof. It must be that either there exists an 1 ≤ l < K such that e ∈ G[l] (Case 1) or no such l exists (Case 2).\n1. Consider the graph G+ e. Under our definition of edge-inclusion graphs for dependency trees, A(G+ e) = D(G+ e). Then, by Lemma 3, there exists a l′ and G′ such that (G[l′] = G′[1] and G[K] = G′[2].\n2. Then, e is not in any of the (K−1)-best trees.\nTheorem 4. For any graph G and K ≥ 1, at iteration 1 ≤ k ≤ K, kbest dep(G,K) returns G[k].\nProof. We prove this by induction on k. Base Case: Then, k = 1 and G(1) is returned by the correctness of opt. Inductive Step: Assume that for all l ≤ k, at iteration l, G[l] was returned. Now consider iteration k + 1, by Lemma 5, we know that eitherG[k+1] has a unique root edge to the k-best trees (Case 1) or e = (ρA j) and there exists a G′ and l ≤ k such that G′(1) = G(l), e ∈ G(l), and G[k+1] = G′[2] (Case 2).\n1. There always exists a tree in the queue that has a unique root edge to all trees that came before it. Furthermore, it is the highest such tree by the correctness of opt.\n2. By our induction hypothesis, G[l] is returned at the lth iteration, and by Theorem 2, we have pushed G′ + e[2] onto the queue. Therefore, we will return G[k+1]."
    } ],
    "references" : [ {
      "title" : "K-best spanning tree dependency parsing with verb valency lexicon reranking",
      "author" : [ "Željko Agić." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Agić.,? 2012",
      "shortCiteRegEx" : "Agić.",
      "year" : 2012
    }, {
      "title" : "A note on finding optimum branchings",
      "author" : [ "Paolo M. Camerini", "Luigi Fratta", "Francesco Maffioli." ],
      "venue" : "Networks, 9.",
      "citeRegEx" : "Camerini et al\\.,? 1979",
      "shortCiteRegEx" : "Camerini et al\\.",
      "year" : 1979
    }, {
      "title" : "The k best spanning arborescences of a network",
      "author" : [ "Paolo M. Camerini", "Luigi Fratta", "Francesco Maffioli." ],
      "venue" : "Networks, 10.",
      "citeRegEx" : "Camerini et al\\.,? 1980",
      "shortCiteRegEx" : "Camerini et al\\.",
      "year" : 1980
    }, {
      "title" : "Discriminative reranking for natural language parsing",
      "author" : [ "Michael Collins", "Terry Koo." ],
      "venue" : "Computational Linguistics, 31.",
      "citeRegEx" : "Collins and Koo.,? 2005",
      "shortCiteRegEx" : "Collins and Koo.",
      "year" : 2005
    }, {
      "title" : "Efficient marginalization of discrete and structured latent variables via sparsity",
      "author" : [ "Gonçalo M. Correia", "Vlad Niculae", "Wilker Aziz", "André F.T. Martins." ],
      "venue" : "Advances in Neural Information Processing Systems: Annual Conference on Neural Information",
      "citeRegEx" : "Correia et al\\.,? 2020",
      "shortCiteRegEx" : "Correia et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural reranking for dependency parsing: An evaluation",
      "author" : [ "Bich-Ngoc Do", "Ines Rehbein." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Do and Rehbein.,? 2020",
      "shortCiteRegEx" : "Do and Rehbein.",
      "year" : 2020
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Efficient algorithms for a family of matroid intersection problems",
      "author" : [ "Harold N. Gabow", "Robert Endre Tarjan." ],
      "venue" : "Journal of Algorithms, 5.",
      "citeRegEx" : "Gabow and Tarjan.,? 1984",
      "shortCiteRegEx" : "Gabow and Tarjan.",
      "year" : 1984
    }, {
      "title" : "Arborescence optimization problems solvable by Edmonds’ algorithm",
      "author" : [ "Leonidas Georgiadis." ],
      "venue" : "Theoretical Computer Science, 301.",
      "citeRegEx" : "Georgiadis.,? 2003",
      "shortCiteRegEx" : "Georgiadis.",
      "year" : 2003
    }, {
      "title" : "K-best spanning tree parsing",
      "author" : [ "Keith Hall." ],
      "venue" : "Proceedings of the Annual Meeting of the Association of Computational Linguistics.",
      "citeRegEx" : "Hall.,? 2007",
      "shortCiteRegEx" : "Hall.",
      "year" : 2007
    }, {
      "title" : "Log-linear models of non-projective trees, k-best MST parsing and tree-ranking",
      "author" : [ "Keith Hall", "Jiřı́ Havelka", "David A. Smith" ],
      "venue" : "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-",
      "citeRegEx" : "Hall et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2007
    }, {
      "title" : "Better k-best parsing",
      "author" : [ "Liang Huang", "David Chiang." ],
      "venue" : "Proceedings of the International Workshop on Parsing Technology.",
      "citeRegEx" : "Huang and Chiang.,? 2005",
      "shortCiteRegEx" : "Huang and Chiang.",
      "year" : 2005
    }, {
      "title" : "Global neural CCG parsing with optimality guarantees",
      "author" : [ "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural probabilistic model for non-projective MST parsing",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the International Joint Conference on Natural Language Processing.",
      "citeRegEx" : "Ma and Hovy.,? 2017",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2017
    }, {
      "title" : "Non-projective dependency parsing using spanning tree algorithms",
      "author" : [ "Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Hajič." ],
      "venue" : "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "McDonald et al\\.,? 2005",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2005
    }, {
      "title" : "Universal dependencies 2.3. LINDAT/CLARIN digital library at the Institute",
      "author" : [ "mariam", "Tak-sum Wong", "Chunxiao Yan", "Marat M. Yavrumyan", "Zhuoran Yu", "Zdeněk Žabokrtský", "Amir Zeldes", "Daniel Zeman", "Manying Zhang", "Hanzhi Zhu" ],
      "venue" : null,
      "citeRegEx" : "mariam et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "mariam et al\\.",
      "year" : 2018
    }, {
      "title" : "K-best A* parsing",
      "author" : [ "Adam Pauls", "Dan Klein." ],
      "venue" : "Proceedings of the Joint Conference of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP.",
      "citeRegEx" : "Pauls and Klein.,? 2009",
      "shortCiteRegEx" : "Pauls and Klein.",
      "year" : 2009
    }, {
      "title" : "Stanza: A Python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Association for Computational Linguistics: System Demonstra-",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "A generative re-ranking model for dependency parsing",
      "author" : [ "Federico Sangati", "Willem Zuidema", "Rens Bod." ],
      "venue" : "Proceedings of the International Conference on Parsing Technologies.",
      "citeRegEx" : "Sangati et al\\.,? 2009",
      "shortCiteRegEx" : "Sangati et al\\.",
      "year" : 2009
    }, {
      "title" : "Discriminative reranking for machine translation",
      "author" : [ "Libin Shen", "Anoop Sarkar", "Franz Josef Och." ],
      "venue" : "Proceedings of the Human Language Technology",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "Finding optimum branchings",
      "author" : [ "Robert Endre Tarjan." ],
      "venue" : "Networks, 7.",
      "citeRegEx" : "Tarjan.,? 1977",
      "shortCiteRegEx" : "Tarjan.",
      "year" : 1977
    }, {
      "title" : "K-best combination of syntactic parsers",
      "author" : [ "Hui Zhang", "Min Zhang", "Chew Lim Tan", "Haizhou Li." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Zhang et al\\.,? 2009",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2009
    }, {
      "title" : "A re-ranking model for dependency parser with recursive convolutional neural network",
      "author" : [ "Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the Interna-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "Please mind the root: Decoding arborescences for dependency parsing",
      "author" : [ "Ran Zmigrod", "Tim Vieira", "Ryan Cotterell." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Zmigrod et al\\.,? 2020",
      "shortCiteRegEx" : "Zmigrod et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm.",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al.",
      "startOffset" : 126,
      "endOffset" : 211
    }, {
      "referenceID" : 18,
      "context" : "However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al.",
      "startOffset" : 126,
      "endOffset" : 211
    }, {
      "referenceID" : 22,
      "context" : "However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al.",
      "startOffset" : 126,
      "endOffset" : 211
    }, {
      "referenceID" : 5,
      "context" : "However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al.",
      "startOffset" : 126,
      "endOffset" : 211
    }, {
      "referenceID" : 19,
      "context" : ", 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective",
      "startOffset" : 53,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : ", 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective",
      "startOffset" : 53,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : ", 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective",
      "startOffset" : 53,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : ", 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective",
      "startOffset" : 53,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "Figure 1: Violation rate of the root constraint when using regular K-best decoding (Camerini et al., 1980) on pre-trained models of Qi et al.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "Figure 1: Violation rate of the root constraint when using regular K-best decoding (Camerini et al., 1980) on pre-trained models of Qi et al. (2020) for languages with varying training set sizes.",
      "startOffset" : 84,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : ", 2007; Agić, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and nuances of Camerini et al.",
      "startOffset" : 8,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : ", 2007; Agić, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and nuances of Camerini et al. (1980)’s algorithm.2 We make a few changes to Camerini et al. (1980)’s algorithm, which result in both a simpler algorithm and simpler proof of correctness.",
      "startOffset" : 8,
      "endOffset" : 392
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, we note that the K-best algorithm may also be used for marginalization of latent variables (Correia et al., 2020) and for constructing parsers with global scoring functions (Lee et al.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and for constructing parsers with global scoring functions (Lee et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix. In addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension.",
      "startOffset" : 119,
      "endOffset" : 527
    }, {
      "referenceID" : 1,
      "context" : "Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix. In addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4 Finding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al.",
      "startOffset" : 119,
      "endOffset" : 874
    }, {
      "referenceID" : 1,
      "context" : "Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix. In addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4 Finding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al.",
      "startOffset" : 119,
      "endOffset" : 946
    }, {
      "referenceID" : 1,
      "context" : "Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix. In addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4 Finding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al. (1979). However, no algorithm exists for Kbest decoding of dependency trees subject to a root constraint.",
      "startOffset" : 119,
      "endOffset" : 970
    }, {
      "referenceID" : 1,
      "context" : "Secondly, their proof of correctness is based on reasoning about a complicated ordering on the edges in the K th tree (Camerini et al., 1980, Section 4); our proof side-steps the complicated ordering by directly reasoning over the ancestry relations of the K th tree. Consequently, our proofs of correctness are considerably simpler and shorter. Throughout the paper, we provide the statements of all lemmas and theorems in the main text, but defer all proofs to the appendix. In addition to simplifying Camerini et al. (1980)’s algorithm, we offer a novel extension. For many dependency parsing schemes such as the Universal Dependency (UD) scheme (Nivre et al., 2018), there is a restriction on dependency trees to only have one edge emanate from the root.4 Finding the maximally weighted spanning tree that obeys this constraint was considered by Gabow and Tarjan (1984) who extended the O(N2) maximum spanning tree algorithm of Tarjan (1977); Camerini et al. (1979). However, no algorithm exists for Kbest decoding of dependency trees subject to a root constraint. As such, we provide the first K-best algorithm that returns dependency trees that obey the root constraint. To motivate the practical necessity of our extension, consider Fig. 1. Fig. 1 shows the percentage of trees that violate the root constraint when doing one-best and 50-best decoding for 63 languages from the UD treebank (Nivre et al., 2018) using the pre-trained model of Qi et al. (2020).5,6 We find that decoding without the root constraint has a much more extreme effect when decoding the 50-best than the one-best.",
      "startOffset" : 119,
      "endOffset" : 1466
    }, {
      "referenceID" : 22,
      "context" : "(5)Zmigrod et al. (2020) conduct a similar experiment for only the one-best tree.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "(6)We note that Qi et al. (2020) do apply the root constraint for one-best decoding, albeit with a sub-optimal algorithm.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "Figure 2: Example graph G (taken from Zmigrod et al. (2020)).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, (Tarjan, 1977) prove that the runtime of finding the best tree for dense graphs is O(N(2)).",
      "startOffset" : 13,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "As we use the algorithm in Zmigrod et al. (2020) as our base algorithm, we borrow their notation wherever convenient.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "However, we note that in most graph-based parsers such as Qi et al. (2020) and Ma and Hovy (2017), dependency labels are extracted after the unlabeled tree has been decoded.",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "(2020) and Ma and Hovy (2017), dependency labels are extracted after the unlabeled tree has been decoded.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Tarjan (1977); Camerini et al. (1979) provided the details for an O(N2) algorithm for decoding the one-best tree.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "Tarjan (1977); Camerini et al. (1979) provided the details for an O(N2) algorithm for decoding the one-best tree. This algorithm was extended by Gabow and Tarjan (1984) to find the best dependency tree in O(N2) time.",
      "startOffset" : 15,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "Tarjan (1977); Camerini et al. (1979) provided the details for an O(N2) algorithm for decoding the one-best tree. This algorithm was extended by Gabow and Tarjan (1984) to find the best dependency tree in O(N2) time. We borrow the algorithm (and notation) of Zmigrod et al. (2020), who provide an exposition and proofs of these algorithms in the context of non-projective dependency parsing.",
      "startOffset" : 15,
      "endOffset" : 281
    }, {
      "referenceID" : 8,
      "context" : "The contraction weighting scheme means that w(d) = w(d# C) (Georgiadis, 2003).",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "The runtime complexity of finding G(1) or G[1] is O(N2) for dense graphs by using efficient priority queues and sorting algorithms (Tarjan, 1977; Gabow and Tarjan, 1984).",
      "startOffset" : 131,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "The runtime complexity of finding G(1) or G[1] is O(N2) for dense graphs by using efficient priority queues and sorting algorithms (Tarjan, 1977; Gabow and Tarjan, 1984).",
      "startOffset" : 131,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "This was first proposed by Gabow and Tarjan (1984). When we consider removing an edge emanating from the root, we are doing this in a possibly contracted graph, and so an edge (ρA j) may exist multiple times in the graph.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "In the following two sections, we provide a simplified reformulation of Camerini et al. (1980) to find the K-best trees.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "In the following two sections, we provide a simplified reformulation of Camerini et al. (1980) to find the K-best trees. The simplifications additionally provide a constant time speed-up over Camerini et al. (1980)’s algorithm.",
      "startOffset" : 72,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "Camerini et al. (1980) make use of the concepts of the blue and red edge sets, but rather than consider a base case as Lemma 1, they propose an ordering in which to visit the edges of the graph.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "The exposition using ancestors and descendants is more similar to the exposition originally presented by Camerini et al. (1980). requiring much more complicated proofs.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Camerini et al. (1980) require G(1) to be known ahead of time.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Camerini et al. (1980) make one call to of opt and two calls to next which only finds the weight-edge pair of our algorithm.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Camerini et al. (1980) make one call to of opt and two calls to next which only finds the weight-edge pair of our algorithm. Therefore, kbest has a constant time speed-up on the original algorithm.13 A short experiment. We empirically measure the constant time speed-up between kbest and the original algorithm of Camerini et al. (1980). We take the English UD test set (as used for Fig.",
      "startOffset" : 0,
      "endOffset" : 337
    }, {
      "referenceID" : 1,
      "context" : "However, in the original next of Camerini et al. (1980), we do not require to stitch together the tree, which may explain the slightly smaller speed-up.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we provided a simplification to Camerini et al. (1980)’sO(KN2)K-best spanning trees algorithm.",
      "startOffset" : 47,
      "endOffset" : 70
    } ],
    "year" : 2021,
    "abstractText" : "The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.1",
    "creator" : "LaTeX with hyperref"
  }
}