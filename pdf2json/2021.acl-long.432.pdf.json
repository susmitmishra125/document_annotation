{
  "name" : "2021.acl-long.432.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition",
    "authors" : [ "Xin Zhang", "Guangwei Xu", "Yueheng Sun", "Meishan Zhang", "Pengjun Xie" ],
    "emails" : [ "hsinz@tju.edu.cn", "yhs@tju.edu.cn", "zhangmeishan@tju.edu.cn", "ahxgwOnePiece@gmail.com", "xpjandy@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5558–5570\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5558"
    }, {
      "heading" : "1 Introduction",
      "text" : "Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to\n∗Corresponding author.\ncollect a large scale dataset for supervised model training. In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018). However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008).\nPrevious crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsourced annotations as untrustworthy answers, proposing sophisticated strategies to recover the golden answers from crowdsourced labels.\nIn this work, we take a different view for crowdsourcing learning, regarding the crowdsourced annotations as the gold standard in terms of individual\nannotators. In other words, we assume that all annotators (including experts) own their specialized understandings towards a specific task, and they annotate the task consistently according to their individual principles by the understandings, where the experts can reach an oracle principle by consensus. The above view indicates that crowdsourcing learning aims to train a model based on the understandings of crowd annotators, and then test the model by the oracle understanding from experts.\nBased on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daumé III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020). We treat each annotator as one domain specifically, and then crowdsourcing learning is essentially almost a multi-source domain adaptation problem. Thus, one natural question arises: What is the performance when a state-of-the-art domain adaptation model is applied directly to crowdsourcing learning.\nHere we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016). We suggest a state-of-the-art representation learning model that can effectively capture annotator(domain)-aware features. Also, we investigate two settings of crowdsourcing learning, one being the unsupervised setting with no expert annotation, which has been widely studied before, and the other being the supervised setting where a certain scale of expert annotations exists, which is inspired by domain adaptation.\nFinally, we conduct experiments on a benchmark crowdsourcing NER dataset (Tjong Kim Sang and De Meulder, 2003; Rodrigues et al., 2014) to evaluate our methods. We take a standard BiLSTM-CRF (Lample et al., 2016) model with BERT (Devlin et al., 2019) word representations as the baseline, and adapt it to our representation learning model. Experimental results show that our method is able to model crowdsourced annotations effectively. Under the unsupervised setting, our model can give a strong performance, outperforming previous work significantly. In addition, the model performance can be greatly boosted by feeding with small-scale expert annotations, which can be a prospective direction for low-resource scenarios.\nIn summary, we make the following three major contributions:\n(1) We present a different view of crowdsourcing learning, and propose to treat crowdsourcing learning as domain adaptation, which naturally connects the two important topics of machine learning for NLP.\n(2) We propose a novel method for crowdsourcing learning. Although the method is of a limited novelty for domain adaptation, it is the first work to crowdsourcing learning, and can achieve state-of-the-art performance on NER.\n(3) We introduce supervised crowdsourcing learning for the first time, which is borrowed from domain adaptation and would be a prospective solution for hard NLP tasks in practice.\nWe will release the code and detailed experimental settings at github.com/izhx/CLasDA under the Apache License 2.0 to facilitate future research."
    }, {
      "heading" : "2 The Basic Idea",
      "text" : "Here we describe the concepts of the domain adaptation and crowdsourcing learning in detail, and show how they are connected together."
    }, {
      "heading" : "2.1 Domain Adaptation",
      "text" : "Domain adaptation happens when a supervised model trained on a fixed set of training corpus, including several specific domains, is required to test on a different domain (Ben-David et al., 2006; Mansour et al., 2009). The scenario is quite frequent in practice, and thus has received extensive attention with massive investigations (Csurka, 2017; Ramponi and Plank, 2020). The major problem lies in the different input distributions between source and target domains, leading to biased predictions over the inputs with a large gap to the source domains.\nHere we focus on multi-source cross-domain adaptation, which would suit our next corresponding mostly. Following Mansour et al. (2009); Zhao et al. (2019), the multi-source domain adaptation assumes a set of labeled examples from M domains available, denoted by Dsrc = {(Xi, Yi)}Mi=1,1 where Xi = {xij} Ni j=1 and Yi = {yij} Ni j=1,\n2 and we aim to train a model on Dsrc to adapt to a specific target domain with the help of a large scale raw corpus Xtgt = {xi}Nti=1 of the target domain.\nNote that under this setting, all Xs, including source and target domains, are generated individually according to their unknown distributions, thus the abstract representations learned from the source domain dataset Dsrc would inevitably be biased to the target domain, which is the primary reason for the degraded performance of the target domain (Huang and Yates, 2010; Ganin et al., 2016). A number of domain adaptation models have struggled for better transferable high-level representations as domain shifts (Ramponi and Plank, 2020)."
    }, {
      "heading" : "2.2 Crowdsourcing Learning",
      "text" : "Crowdsourcing aims to produce a set of large-scale annotated examples created by crowd annotators, which is used to train supervised models for a given task (Raykar et al., 2010). As the majority of NLP models assume that gold-standard highquality training corpora are already available (Manning and Schutze, 1999), crowdsourcing learning has received much less interest than cross-domain adaptation, although the availability of these corpora is always not the truth.\nFormally, under the crowdsourcing setting, we usually assume that there are a number of crowd annotators A = {ai}Mi=1 (here we use the same M as well as later superscripts in order to align with the domain adaptation), and all annotators should have a sufficient number of training examples by their different understandings for a given task, which are referred to as Dcrowd = {(Xi, Yi)}Mi=1 where Xi = {xij} Ni j=1 and Yi = {yij} Ni j=1. We aim to train a model on Dcrowd and adapt it to predict the expert outputs. Note that all Xs do not have significant differences in their distributions in this paradigm.\n1A domain is commonly defined as a distribution on the input data in many works, e.g., Ben-David et al. (2006). To make domain adaptation and crowdsourcing learning highly similar in formula, we follow Zhao et al. (2019), defining a domain as a joint distribution on the input space X and the label space Y . Section 4.5 gives a discussion of their connection.\n2N∗ indicates the number of instances.\nCrowdsourcing Learning as Domain adaptation By scrutinizing the above formalization, when we set all Xs jointly with the annotators by using xij = ai(x i j), which indicates the contextualized understanding (a vectorial form is desirable here of the neural representations) of xij by the annotator ai, then we would regard that Xi = {ai(xij)} Ni j=1 is generated from different distributions as well. In this way, we are able to connect crowdsourcing learning and domain adaptation together, as shown in Figure 2, based on the assumption that all Y s are gold-standard for crowdsourced annotations when crowd annotators are united as joint inputs. And finally, we need to perform predictions by regarding xexpert = expert(x), and in particular, the learning of expert differs from that of the target domain in domain adaptation."
    }, {
      "heading" : "3 A Case Study On NER",
      "text" : "In this section, we take NER as a case study, which has been investigated most frequently in NLP (Yadav and Bethard, 2018), and propose a representation learning model mainly inspired by the domain adaptation model of (Jia et al., 2019) to perform crowdsourcing learning. In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation."
    }, {
      "heading" : "3.1 The Representation Learning Model",
      "text" : "We convert NER into a standard sequence labeling problem by using the BIO schema, following the majority of previous works, and extend a stateof-the-art BERT-BiLSTM-CRF model (Mayhew\net al., 2020) to our crowdsourcing learning. Figure 3 shows the overall network structure of our representation learning model. By using a sophisticated parameter generator module (Platanios et al., 2018), it can capture annotator-aware features. Following, we introduce the proposed model by four components: (1) word representation, (2) annotator switcher, (3) BiLSTM Encoding, and (4) CRF inference and training.\nWord Representation Given a sentence of n words x = w1 · · ·wn, we first convert it to vectorial representations by BERT. Different from the standard BERT exploration, here we use Adapter◦BERT (Houlsby et al., 2019), where two extra adapter modules are inside each transformer layer. The process can be simply formalized as:\ne1 · · · en = Adapter ◦ BERT(w1 · · ·wn) (1)\nwhere ◦ indicates an injection operation. The detailed structure of the transformer with adapters is described in Appendix A.\nNoticeably, the Adapter ◦ BERT method no longer needs fine-tuning the huge BERT parameters and can obtain comparable performance by adjusting the much lightweight adapter parameters instead. Thus the representation can be more parameter efficient, and in this way we can easily extend the word representations to annotator-aware representations.\nAnnotator Switcher Our goal is to efficiently learn annotator-aware word representations, which can be regarded as contextualized understandings of individual annotators. Hence, we introduce an annotator switcher to support Adapter◦BERT with annotator input as well, which is inspired by Üstün et al. (2020). The key idea is to use Parameter Generation Network (PGN) (Platanios et al., 2018; Jia et al., 2019) to produce adapter parameters dynamically by input annotators. In this way, our model can flexibly switch among different annotators.\nConcretely, assuming that V is the vectorial form of all adapter parameters by a pack operation, which can also be unpacked to recover all adapter parameters as well, the PGN module is to generate V for Adapter ◦ BERT dynamically according the annotator inputs, as shown in Figure 3 by the right orange part. The switcher can be formalized as:\nx = r′1 · · · r′n = PGN ◦ Adapter ◦ BERT(x, a) = Adapter ◦ BERT(x,V = Θ× ea),\n(2)\nwhere Θ ∈ R|V |×|ea| , x = r′1 · · · r′n is the annotator-aware representations of annotator a for x = w1 · · ·wn, and ea is the annotator embedding.\nBiLSTM Encoding Adapter◦BERT requires an additional task-oriented module for high-level feature extraction. Here we exploit a single BiLSTM layer to achieve it: h1 · · ·hn = BiLSTM(x), which is used for next-step inference and training.\nCRF Inference and Training We use CRF to calculate the score of a candidate sequential output y = l1 · · · ln globally:\noi = W crfhi + b crf\nscore(y|x, a) = n∑\ni=1\n(T [li−1, li] + oi[li]) (3)\nwhere W crf, bcrf and T are model parameters. Given an input (x, a), we perform inference by the Viterbi algorithm. For training, we define a sentence-level cross-entropy objective:\np(ya|x, a) = exp\n( score(ya|x, a) )∑ y exp ( score(y|x, a)\n) L = − log p(ya|x, a) (4)\nwhere ya is the gold-standard output of x from a, y belongs to all possible candidates, and p(ya|x, a) indicates the sentence-level probability."
    }, {
      "heading" : "3.2 The Unsupervised Setting",
      "text" : "Here we introduce unsupervised crowdsourcing learning in alignment with unsupervised domain adaptation, assuming that no expert annotation is available, which is the widely-adopted setting of previous work of crowdsourcing learning (Sheng et al., 2008; Zhang et al., 2016; Sheng and Zhang, 2019). This setting has a large divergence with domain adaptation in target learning. In the unsupervised domain adaptation, the information of the target domain can be learned through a large-scale raw corpus (Ramponi and Plank, 2020), where there is no correspondence in the unsupervised crowdsourcing learning to learn information of experts.\nTo this end, here we suggest a simple and heuristic method to model experts by the specialty of crowdsourcing learning. Intuitively, we expect that experts should approve the knowledge of the common consensus for a given task, and meanwhile, our model needs the embedding representation of experts for inference. Thus, we can estimate the\nexpert embedding by using the centroid point of all annotator embeddings:\neexpert = 1 |A| ∑ a∈A ea (5)\nwhere A represents all annotators contributed to the training corpus. This expert can be interpreted as the elected outcome by annotator voting with equal importance. In this way, we perform the inference in unsupervised crowdsourcing learning by feeding eexpert as the annotator input."
    }, {
      "heading" : "3.3 The Supervised Setting",
      "text" : "Inspired by the supervised domain adaptation, we also present the supervised crowdsourcing learning, which has been seldom concerned. The setting is very simple, just by assuming that a certain scale of expert annotations is available. In this way, we can learn the expert representation directly by supervised learning with our proposed model.\nThe supervised setting could be a more practicable scenario in real applications. Intuitively, it should bring much better performance than the unsupervised setting with few shot expert annotations, which does not increase the overall annotation cost much. In fact, during or after the crowdsourcing annotation process, we usually have a quality control module, which can help to produce silvery quality pseudo-expert annotations (Kittur et al., 2008; Lease, 2011). Thus, the supervised setting can be highly valuable yet has been ignored mostly."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setting",
      "text" : "Dataset We use the CoNLL-2003 NER English dataset (Tjong Kim Sang and De Meulder, 2003) with crowdsourced annotations provided by Rodrigues and Pereira (2018) to investigate our methods in both unsupervised and supervised settings. The crowdsourced annotations consume 400 new articles, involving 5,985 sentences in practice, which are labeled by a total of 47 crowd annotators. The total number of annotations is 16,878. Thus the averaged number of annotated sentences per annotator is 359, which covers 6% of the total sentences. The dataset includes golden/expert annotations on the training sentences and a standard CoNLL-2003 test set for NER evaluation.\nEvaluation The standard CoNLL-2003 evaluation metric is used to calculate the NER perfor-\nmance, reporting the entity-level precision (P), recall (R), and their F1 value. All experiments of the same setting are conducted by five times, and the median outputs are used for performance reporting. We exploit the pair-wise t-test for significance test, regarding two results significantly different when the p-value is below 10−5.\nBaselines We re-implement several methods of previous work as baselines, and all the methods are based on Adapter◦BERT-BiLSTM-CRF (no annotator switcher inside) for fair comparisons.\nFor both the unsupervised and supervised settings, we consider the following baseline models:\n• ALL: which treats all annotations equally, ignoring the annotator information no matter crowd or expert.\n• MV: which is borrowed from Rodrigues et al. (2014), where aggregated labels are produced by token level majority voting. In particular, the gold-standard labels are used instead if they are available for a specific sentence during the supervised crowdsourcing learning.\n• LC: which is proposed by Nguyen et al. (2017), where the annotator bias to the goldstandard labels is explicitly modeled at the CRF layer for each crowd annotator, and specifically, the expert is with zero bias.\n• LC-cat: which is also presented by Nguyen et al. (2017) as a baseline to LC, where the annotator bias is modeled at the BiLSTM layer instead and also the expert bias is set to zero.3\n3Note that although LC-cat is not as expected as LC in (Nguyen et al., 2017), our results show that LC-cat is slightly better based on Adapter◦BERT-BiLSTM-CRF.\nNotice that ALL and MV are annotator-agnostic models, which exploit no information specific to the individual annotators, while the other three models are all annotator-aware models, where the annotator information is used by different ways.\nHyper-parameters We offer all detailed settings of Hyper-parameters in Appendix B."
    }, {
      "heading" : "4.2 Unsupervised Results",
      "text" : "Table 1 shows the test results of the unsupervised setting. As a whole, we can see that our representation learning model (i.e., This Work) borrowed from domain adaptation can achieve the best performance, resulting in an F1 score of 77.95, significantly better than the second-best model LC-cat (i.e., 77.95 − 76.79 = 1.16). The result indicates the advantage of our method over the other models.\nBy examining the results in-depth, we can find that the annotator-aware model is significantly better than the annotator-agnostic models, demonstrating that the annotator information is highly helpful for crowdsourcing learning. The observation further shows the reasonableness by aligning annotators to domains, since domain information is also useful for domain adaptation. In addition, the better performance of our representation learning method among the annotator-aware models indicates that our model can capture annotator-aware information more effectively because our start point is totally different. We do not attempt to model the expert labels based on crowdsourcing annotations.\nFurther, we observe that several models show better precision values, while others give better recall values. A high precision but low recall indicates that the model is conservative in detecting named entities, and vice the reverse. Our proposed model is able to balance the two directions better, with the least gap between them. Also, the re-\nsults imply that there is still much space for future development, and the recent advances of domain adaptation might offer good avenues.\nFinally, we compare our results with previous studies. As shown, our model can obtain the best performance in the literature. In particular, by comparing our results with the original performances reported in Nguyen et al. (2017), we can see that our re-implementation is much better than theirs. The major difference lies in the exploration of BERT in our model, which brings improvements closed to 6% for both LC and LC-cat."
    }, {
      "heading" : "4.3 Supervised Results",
      "text" : "To investigate the supervised setting, we assume that expert annotations (ground truths) of all crowdsourcing sentences are available. Besides exploring the full expert annotations, we study another three different scenarios by incrementally adding the expert annotations into the unsupervised setting, aiming to study the effectiveness of our model with small expert annotations as well. Concretely, we assume proportions of 1%, 5%, 25%, and 100% of the expert annotations available.4 Table 2 shows all the results, including our four baselines and an gold model based on only expert annotations for comparisons. Overall, we can see that our representation learning model can bring the best performances for all scenarios, demonstrating its effectiveness in the supervised learning as well.\nNext, by comparing annotator-agnostic and annotator-aware models, we can see that annotatoraware models are better, which is consistent with\n4Intuitively, if expert annotations are involved, we should intentionally choose the more informative inputs for annotations, which can reduce the overall cost to meet a certain performance standard. Thus, we can fully demonstrate the effectiveness of crowdsourced annotations under the semisupervised setting. Here we try to choose the most informative labeled instances for the 1%, 5%, and 25% settings.\nthe unsupervised setting. More interestingly, the results show that All is better than gold with very small-scale expert annotations (1% and 5%), and the tendency is reversed only when there are sufficient expert annotations (25% and 100%). The observation indicates that crowdsourced annotations are always helpful when golden annotations are not enough. In addition, it is easy to understand that MV is worse than gold since the latter has a higher-quality of the training corpus.\nFurther, we can find that even the annotatoraware LC and LC-catmodels are unable to obtain any positive influence compared with gold, which demonstrates that distilling ground-truths from the crowdsourcing annotations might not be the most promising solution. While our representation learning model can give consistently better results than gold, indicating that crowdsourced annotations are always helpful by our method. By regarding crowdsourcing learning as domain adaptation, we no longer take crowdsourced annotations as noise, and on the contrary, they are treated as transferable knowledge, similar to the relationship between the source domains and the target domain. Thus they could always be useful in this way."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "To better understand our idea and model in-depth, we conducted the following fine-grained analyses.5\nVisualization of Annotator Embeddings Our representation learning model is able to learn annotator embeddings through the task objective. It is interesting to visualize these embeddings to check their distributions, which can reflect the relationships between the individual annotators. Figure 4 shows the visualization results after Principal Component Analysis (PCA) dimensionality reduction,\n5In addition, we could not perform the ablation study of our model because it is not an incremental work.\nwhere the unsupervised and three supervised scenarios are investigated.6 As shown, we can see that most crowd annotators are distributed in a concentrated area for all scenarios, indicating that they are able to share certain common characteristics of task understanding.\nFurther, we focus on the relationship between expert and crowd annotators, and the results show two interesting findings. First, the heuristic expert of our unsupervised learning is almost consistent with that of the supervised learning of the whole expert annotations (100%), which indicates that our unsupervised expert estimation is perfectly good. Second, the visualization shows that the relationship between expert and crowd annotators could be biased when expert annotations are not enough. As the size of expert annotations increases, their connection might be more accurate gradually.\nThe Predictability of Crowdsourcing Annotations Our primary assumption is based on that all crowdsourced annotations are regarded as the gold-standard with respect to the crowd annotators, which naturally indicates that these annotations are predictable. Here we conduct analysis to verify the assumption by a new task to predicate the crowdsourced annotations, Concretely, we divide the annotations into two sections, where 85% of them are used as the training and the remaining are used for testing, and then we apply our baseline and proposed models to learn and evaluate.\nTable 3 shows the results. As shown, our model can achieve the best performance by an F1 score of 77.12%, and the other models are significantly worse (at least 4.86 drops by F1). Considering that the proportion of the averaged training examples per annotator over the full 5,985 sentences is only 5%,7 we exploit the gold model of the 5% expert annotations for reference. We can see that the gap between them is small (77.12% v.s. 79.33%),\n6The 1% setting is excluded for its incapability to capture the relationship between the expert and crowd annotators with such small expert annotations.\n7The value can be directly calculated (0.06∗0.85 ≈ 0.05).\nwhich indicates that our assumption is acceptable as a whole. The other models could be unsuitable for our assumption due to the poor performance induced by their modeling strategies.\nThe Impact of Unreliable Annotators Handling unreliable annotators, such as spammers, is a practical and common issue in Crowdsourcing (Raykar and Yu, 2012b). Obviously, regarding crowd annotations as untrustworthy answers is more considerate to this problem. In contrast, our assumption might be challenged because these unreliable annotators are discrepant in their own annotations. To show the influence of unreliable annotators, we filter out several unreliable annotators in the corpus, and reevaluate the performance for the low-resource supervised and unsupervised scenarios on the remaining annotations.\nFigure 5 shows the comparison results of the original corpus and the filtered corpus.8 First, we can find that improved performance can be achieved in all cases, indicating excluding these unreliable annotations is helpful for crowdsourcing. Second, the LC and LC-cat model give smaller score differences compared with the ALL model between these two kinds of results, which verified that they are considerate to unreliable annotators. Third, our model also performs robustly, it can cope with this practical issue in a certain degree as well.\nResults on The Sampled Annotators and Annotations The above analysis shows the benefit of removing unreliable annotators, which reduces a small number of annotators and annotations. A problem arises naturally: will the performance be\n8MV is not included because a proportion of instances are unable to obtain aggregated answers.\nconsistent if we sample a small proportion of annotators? To verify it, we sampled two sub-set from the crowdsourced training corpus and re-train our model as well as baselines. Table 4 shows the evaluation results of re-trained models on the standard test set in unsupervised setting. We also add our main result for the comparison. As shown, all sampled datasets demonstrate similar trends with the main result (denoted as Full). The supervised results are consistent with our main result as well, which are not listed due to space reasons."
    }, {
      "heading" : "4.5 The Discussion of Domain Definitions",
      "text" : "The most widely used definition of a domain is the distribution on the input spaceX . Zhao et al. (2019) define a domain D as the pair of a distribution D on the input space X and a labeling function f : X → Y , i.e., domain D = 〈D, f〉.\nIn this work, we assume each annotator is a unique labeling function a : X → Y . Uniting each annotator and the instances he/she labeled, we can result in a number of domains {〈Di, ai〉}|A|i=1, where A represents all annotators. Then the crowdsourcing learning can be interpreted by the later definition, i.e., learning from these crowd annotators/domains and predicting the labels of raw inputs (sampled from the raw data distribution Dexpert) in expert annotator/domain 〈Dexpert, expert〉. To unify the definition in a single distribution, we directly define a domain as the joint distribution on the input space X and the label space Y .\nIn addition, we can align to the former definition by using the representation outputs xi = ai(x) as the data input, which shows different distributions for the same sentence towards different annotators. Thus, each source domain Di is the distibution of xi, and we need learn the expert representations xexpert to perform inference on the unlabled texts."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Crowdsourcing Learning",
      "text" : "Crowdsourcing is a cheap and popular way to collect large-scale labeled data, which can facilitate the model training for hard tasks that require supervised learning (Wang and Zhou, 2016; Sheng and Zhang, 2019). In particular, crowdsourced data is often regarded as low-quality, including much noise regarding expert annotations as the gold-standard. Initial studies of crowdsourcing learning try to arrive at a high-quality corpus by majority voting or control the quality by sophisticated strategies during the crowd annotation process (Khattak and Salleb-Aouissi, 2011; Liu et al., 2017; Tang and Lease, 2011).\nRecently, the majority work focuses on full exploration of all annotated corpus by machine learning models, taking the information from crowd annotators into account including annotator reliability (Rodrigues et al., 2014), annotator accuracy (Huang et al., 2015), worker-label confusion matrix (Nguyen et al., 2017), and sequential confusion matrix (Simpson and Gurevych, 2019).\nIn this work, we present a totally different viewpoint for crowdsourcing, regarding all crowdsourced annotations as golden in terms of individual annotators, just like the primitive gold-standard labels corresponded to the experts, and further propose a domain adaptation paradigm for crowdsourcing learning."
    }, {
      "heading" : "5.2 Domain Adaptation",
      "text" : "Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020). Typical methods include self-training to produce pseudo training instances for the target domain (Yu et al., 2015) and representation learning to capture transferable features across the source and target domains (Sener et al., 2016).\nIn this work, we make correlations between domain adaptation and crowdsourcing learning, enabling crowdsourcing learning to benefit from the advances of domain adaptation, and then present a representation learning model borrowed from Jia et al. (2019) and Üstün et al. (2020)."
    }, {
      "heading" : "5.3 Named Entity Recognition",
      "text" : "NER is a fundamental and challenging task of NLP (Yadav and Bethard, 2018). The BiLSTM-CRF (Lample et al., 2016) architecture, as well as BERT (Devlin et al., 2019), are able to bring state-of-theart performance in the literature (Jia et al., 2019; Wang et al., 2020; Jia and Zhang, 2020). Mayhew et al. (2020) exploits the BERT-BiLSTM-CRF model, achieving strong performance on NER.\nIn addition, NER has been widely adopted as crowdsourcing learning as well (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016; Yang et al., 2018). Thus, we exploit NER as a case study following these works, and take a BERTBiLSTM-CRF model as the basic model for our annotator-aware extension."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We studied the connection between crowdsourcing learning and domain adaptation, and then proposed to treat crowdsourcing learning as a domain adaptation problem. Following, we took NER as a case study, suggesting a representation learning model from recent advances of domain adaptation for crowdsourcing learning. By this case study, we introduced unsupervised and supervised crowdsourcing learning, where the former is a widelystudied setting while the latter has been seldom investigated. Finally, we conducted experiments on a widely-adopted benchmark dataset for crowdsourcing NER, and the results show that our representation learning model is highly effective in unsupervised learning, achieving the best performance in the literature. In addition, the supervised learning with a very small scale of expert annotations can boost the performance significantly.\nOur work sheds light on the application of effective domain adaptation models on crowdsourcing learning. There are still many other sophisticated cross-domain models, such as adversarial learning (Ganin et al., 2016) and self-training (Yu et al., 2015). Future work may include how to apply these advances to crowdsourcing learning properly."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all reviewers for their hard work. This research is supported by grants from the National Key Research and Development Program of China (No. 2018YFC0832101) and the fonds of Beijing Advanced Innovation Center for Language Resources under Grant TYZ19005.\nEthical Impact\nWe present a different view of crowdsourcing learning and propose to treat it as domain adaptation, showing the connection between these two topics of machine learning for NLP. In this view, many sophisticated cross-domain models could be applied to crowdsourcing learning. Moreover, the motivation that regarding all crowdsourced annotations as gold-standard to the corresponding annotators, also sheds light on introducing other transfer learning techniques in future work.\nThe above idea and our proposed representation learning model for crowdsourcing sequence labeling, are totally agnostic to any private information of annotators. And we do not use any sensitive information, bu only the ID of annotators, in problem modeling and learning. The crowdsourced CoNLL English NER data also anonymized annotators. There will be no privacy issues in the future."
    }, {
      "heading" : "A Transformer with Adapters",
      "text" : "In our Adapter ◦ BERT word representation, we insert two adapter modules for each transformer layer inside BERT. Figure 6 shows the detailed network structure of transformer with adapters. More specifically, the forward operation of an adapter layer is computed as follows:\nhmid = GELU(W ap 1 hin + b ap 1 ) hout = W ap 2 hmid + b ap 2 + hin,\n(6)\nwhere W ap1 , W ap 2 , b ap 1 and b ap 2 are adapter parameters, and the dimension size of hmid is usually smaller than that of the corresponding transformer."
    }, {
      "heading" : "2x Feed-forward layer",
      "text" : "Here we also give a supplement to illustrate the pack operation from all adapter parameters into a single vector V :\nV = ⊕\nAdapters\n{W ap1 ⊕W ap 2 ⊕ b ap 1 ⊕ b ap 2 }, (7)\nwhere first all parameters of a single adapter are reshaped and concatenated and then a further concatenation is performed over all adapters."
    }, {
      "heading" : "B Hyper-parameters",
      "text" : "We choose the BERT-base-cased9, which is for English language and consists of 12-layer transformers with the hidden size 768 for all layers. We load the BERT weight and implement the adapter injection based on the transformers (Wolf et al., 2020) library. The sizes of the adapter middle hidden states are set to 128 constantly. The annotator embedding size is 8 to fit the model in one RTX-2080TI GPU of 11GB memory. The BiLSTM hidden size is\n9https://github.com/google-research/bert\nset to 400. For all models, we inject adapters or switchers in all 12 layers of BERT. All experiments are run on the single GPU at an 8-GPU server with a 14 core CPU and 128GB memory.\nWe exploit the stochastic gradient-based online learning, with a batch size of 64, to optimize model parameters. We apply the time-step dropout, which randomly sets several representations in the sequence to zeros with a probability of 0.2, on the word representations to avoid overfitting. We use the Adam algorithm to update the parameters with a constant learning rate 1 × 10−3, and apply the gradient clipping by a maximum value of 5.0 to avoid gradient explosion."
    }, {
      "heading" : "C The Advantage of Adapter ◦ BERT",
      "text" : "Our models are all based on Adapter ◦ BERT as the basic representations, which is different from the widely-adopted BERT fine-tuning architecture. Here we compare the two strategies in detail. The results are shown in Table 5, where for Adapter ◦ BERT we consider gradually increasing the number of transformer layers (covering the last n layers) inside the BERT. As shown, it is apparently that Adapter ◦ BERT is much more parameter efficient, and when all layers are exploited, the model can be even better than BERT fine-tuning. Thus it is more desirable to use Adapter ◦BERT covering all BERT transformers inside."
    }, {
      "heading" : "D Case Study",
      "text" : "Here we also offer a case study to understand the performance in unsupervised and supervised crowdsourcing learning, as well as the different crowdsourcing models. We exploit one complex example in Table 6 which involves different outputs for various models. As shown, we can see that supervised models are able to recall the ambiguous entity (i.e., Pace, a single word with multiple senses) correctly, while unsupervised models fail, which may be due to the inconsistencies of the crowdsourced annotations. By comparing our model with other baselines, we can show that our representation learning model can capture the global text input understanding consistently, e.g., being able to connect Ohio State and Arizona State together."
    } ],
    "references" : [ {
      "title" : "Self-crowdsourcing training for relation extraction",
      "author" : [ "Azad Abad", "Moin Nabi", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the ACL: Short Papers.",
      "citeRegEx" : "Abad et al\\.,? 2017",
      "shortCiteRegEx" : "Abad et al\\.",
      "year" : 2017
    }, {
      "title" : "Analysis of representations for domain adaptation",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira." ],
      "venue" : "Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, pages 137–144. MIT Press.",
      "citeRegEx" : "Ben.David et al\\.,? 2006",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2006
    }, {
      "title" : "Creating speech and language data with Amazon’s Mechanical Turk",
      "author" : [ "Chris Callison-Burch", "Mark Dredze." ],
      "venue" : "Proceedings of the NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12.",
      "citeRegEx" : "Callison.Burch and Dredze.,? 2010",
      "shortCiteRegEx" : "Callison.Burch and Dredze.",
      "year" : 2010
    }, {
      "title" : "A survey of domain adaptation for neural machine translation",
      "author" : [ "Chenhui Chu", "Rui Wang." ],
      "venue" : "Proceedings of COLING, pages 1304–1319.",
      "citeRegEx" : "Chu and Wang.,? 2018",
      "shortCiteRegEx" : "Chu and Wang.",
      "year" : 2018
    }, {
      "title" : "Domain adaptation for visual applications: A comprehensive survey",
      "author" : [ "Gabriela Csurka." ],
      "venue" : "arXiv preprint arXiv:1702.05374.",
      "citeRegEx" : "Csurka.,? 2017",
      "shortCiteRegEx" : "Csurka.",
      "year" : 2017
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "Hal Daumé III." ],
      "venue" : "Proceedings of ACL, pages 256–263.",
      "citeRegEx" : "III.,? 2007",
      "shortCiteRegEx" : "III.",
      "year" : 2007
    }, {
      "title" : "Broad Twitter corpus: A diverse named entity recognition resource",
      "author" : [ "Leon Derczynski", "Kalina Bontcheva", "Ian Roberts." ],
      "venue" : "Proceedings of the COLING: Technical Papers, pages 1169–1179.",
      "citeRegEx" : "Derczynski et al\\.,? 2016",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from partially annotated sequences",
      "author" : [ "Eraldo R. Fernandes", "Ulf Brefeld." ],
      "venue" : "ECMLPKDD, volume 6911 of Lecture Notes in Computer Science, pages 407–422. Springer.",
      "citeRegEx" : "Fernandes and Brefeld.,? 2011",
      "shortCiteRegEx" : "Fernandes and Brefeld.",
      "year" : 2011
    }, {
      "title" : "Annotating named entities in Twitter data with crowdsourcing",
      "author" : [ "Tim Finin", "William Murnane", "Anand Karandikar", "Nicholas Keller", "Justin Martineau", "Mark Dredze." ],
      "venue" : "Proceedings of the NAACLHLT 2010 Workshop on Creating Speech and Lan-",
      "citeRegEx" : "Finin et al\\.,? 2010",
      "shortCiteRegEx" : "Finin et al\\.",
      "year" : 2010
    }, {
      "title" : "Hierarchical Bayesian domain adaptation",
      "author" : [ "Jenny Rose Finkel", "Christopher D. Manning." ],
      "venue" : "Proceedings of HLT-NAACL, pages 602–610.",
      "citeRegEx" : "Finkel and Manning.,? 2009",
      "shortCiteRegEx" : "Finkel and Manning.",
      "year" : 2009
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor S. Lempitsky." ],
      "venue" : "J. Mach. Learn. Res., 17:59:1–59:35.",
      "citeRegEx" : "Ganin et al\\.,? 2016",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICML, pages 513–520.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "Proceedings of ICML, pages 2790–2799.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning whom to trust with MACE",
      "author" : [ "Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy." ],
      "venue" : "Proceedings of the NAACL-HLT.",
      "citeRegEx" : "Hovy et al\\.,? 2013",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2013
    }, {
      "title" : "Experiments with crowdsourced re-annotation of a POS tagging data set",
      "author" : [ "Dirk Hovy", "Barbara Plank", "Anders Søgaard." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Hovy et al\\.,? 2014",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2014
    }, {
      "title" : "Data quality from crowdsourcing: A study of annotation selection criteria",
      "author" : [ "Pei-Yun Hsueh", "Prem Melville", "Vikas Sindhwani." ],
      "venue" : "Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 27–35.",
      "citeRegEx" : "Hsueh et al\\.,? 2009",
      "shortCiteRegEx" : "Hsueh et al\\.",
      "year" : 2009
    }, {
      "title" : "Exploring representation-learning approaches to domain adaptation",
      "author" : [ "Fei Huang", "Alexander Yates." ],
      "venue" : "Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing.",
      "citeRegEx" : "Huang and Yates.,? 2010",
      "shortCiteRegEx" : "Huang and Yates.",
      "year" : 2010
    }, {
      "title" : "Estimation of discourse segmentation labels from crowd data",
      "author" : [ "Ziheng Huang", "Jialu Zhong", "Rebecca J. Passonneau." ],
      "venue" : "Proceedings of the EMNLP, pages 2190–2200.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Noise or additional information? leveraging crowdsource annotation item agreement for natural language tasks",
      "author" : [ "Emily Jamison", "Iryna Gurevych." ],
      "venue" : "Proceedings of the EMNLP, pages 291–297.",
      "citeRegEx" : "Jamison and Gurevych.,? 2015",
      "shortCiteRegEx" : "Jamison and Gurevych.",
      "year" : 2015
    }, {
      "title" : "Crossdomain NER using cross-domain language modeling",
      "author" : [ "Chen Jia", "Xiaobo Liang", "Yue Zhang." ],
      "venue" : "Proceedings of ACL, pages 2464–2474.",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-cell compositional LSTM for NER domain adaptation",
      "author" : [ "Chen Jia", "Yue Zhang." ],
      "venue" : "Proceedings of ACL, pages 5906–5917.",
      "citeRegEx" : "Jia and Zhang.,? 2020",
      "shortCiteRegEx" : "Jia and Zhang.",
      "year" : 2020
    }, {
      "title" : "Instance weighting for domain adaptation in NLP",
      "author" : [ "Jing Jiang", "ChengXiang Zhai." ],
      "venue" : "Proceedings of ACL, pages 264–271.",
      "citeRegEx" : "Jiang and Zhai.,? 2007",
      "shortCiteRegEx" : "Jiang and Zhai.",
      "year" : 2007
    }, {
      "title" : "Quality control of crowd labeling through expert evaluation",
      "author" : [ "Faiza Khan Khattak", "Ansaf Salleb-Aouissi." ],
      "venue" : "Proceedings of the NIPS 2nd Workshop on Computational Social Science and the Wisdom of Crowds, volume 2, page 5.",
      "citeRegEx" : "Khattak and Salleb.Aouissi.,? 2011",
      "shortCiteRegEx" : "Khattak and Salleb.Aouissi.",
      "year" : 2011
    }, {
      "title" : "Crowdsourcing user studies with mechanical turk",
      "author" : [ "Aniket Kittur", "Ed H. Chi", "Bongwon Suh." ],
      "venue" : "Proceedings of the 2008 Conference on Human Factors in Computing Systems, CHI 2008, 2008, Florence, Italy, April 5-10, 2008, pages 453–456. ACM.",
      "citeRegEx" : "Kittur et al\\.,? 2008",
      "shortCiteRegEx" : "Kittur et al\\.",
      "year" : 2008
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 260–270.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "On quality control and machine learning in crowdsourcing",
      "author" : [ "Matthew Lease." ],
      "venue" : "Human Computation, volume WS-11-11 of AAAI Workshops. AAAI.",
      "citeRegEx" : "Lease.,? 2011",
      "shortCiteRegEx" : "Lease.",
      "year" : 2011
    }, {
      "title" : "A neural model for aggregating coreference annotation in crowdsourcing",
      "author" : [ "Maolin Li", "Hiroya Takamura", "Sophia Ananiadou." ],
      "venue" : "Proceedings of COLING, pages 5760–5773.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving learningfrom-crowds through expert validation",
      "author" : [ "Mengchen Liu", "Liu Jiang", "Junlin Liu", "Xiting Wang", "Jun Zhu", "Shixia Liu." ],
      "venue" : "Proceedings of IJCAI, pages 2329–2336.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Foundations of statistical natural language processing",
      "author" : [ "Christopher Manning", "Hinrich Schutze." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Manning and Schutze.,? 1999",
      "shortCiteRegEx" : "Manning and Schutze.",
      "year" : 1999
    }, {
      "title" : "Domain adaptation with multiple sources",
      "author" : [ "Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 21, pages 1041–1048.",
      "citeRegEx" : "Mansour et al\\.,? 2009",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2009
    }, {
      "title" : "Robust named entity recognition with truecasing pretraining",
      "author" : [ "Stephen Mayhew", "Nitish Gupta", "Dan Roth." ],
      "venue" : "AAAI 2020, pages 8480–8487.",
      "citeRegEx" : "Mayhew et al\\.,? 2020",
      "shortCiteRegEx" : "Mayhew et al\\.",
      "year" : 2020
    }, {
      "title" : "Aggregating and predicting sequence labels from crowd annotations",
      "author" : [ "An Thanh Nguyen", "Byron Wallace", "Junyi Jessy Li", "Ani Nenkova", "Matthew Lease." ],
      "venue" : "Proceedings of ACL, pages 299–309.",
      "citeRegEx" : "Nguyen et al\\.,? 2017",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2017
    }, {
      "title" : "A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature",
      "author" : [ "Benjamin Nye", "Junyi Jessy Li", "Roma Patel", "Yinfei Yang", "Iain Marshall", "Ani Nenkova", "Byron Wallace." ],
      "venue" : "Pro-",
      "citeRegEx" : "Nye et al\\.,? 2018",
      "shortCiteRegEx" : "Nye et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextual parameter generation for universal neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Mrinmaya Sachan", "Graham Neubig", "Tom Mitchell." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Platanios et al\\.,? 2018",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural unsupervised domain adaptation in NLP—A survey",
      "author" : [ "Alan Ramponi", "Barbara Plank." ],
      "venue" : "Proceedings of the COLING, pages 6838–6855.",
      "citeRegEx" : "Ramponi and Plank.,? 2020",
      "shortCiteRegEx" : "Ramponi and Plank.",
      "year" : 2020
    }, {
      "title" : "Eliminating spammers and ranking annotators for crowdsourced labeling tasks",
      "author" : [ "Vikas C. Raykar", "Shipeng Yu." ],
      "venue" : "J. Mach. Learn. Res., 13:491–518.",
      "citeRegEx" : "Raykar and Yu.,? 2012a",
      "shortCiteRegEx" : "Raykar and Yu.",
      "year" : 2012
    }, {
      "title" : "Eliminating spammers and ranking annotators for crowdsourced labeling tasks",
      "author" : [ "Vikas C. Raykar", "Shipeng Yu." ],
      "venue" : "J. Mach. Learn. Res., 13:491–518.",
      "citeRegEx" : "Raykar and Yu.,? 2012b",
      "shortCiteRegEx" : "Raykar and Yu.",
      "year" : 2012
    }, {
      "title" : "Learning from crowds",
      "author" : [ "Vikas C. Raykar", "Shipeng Yu", "Linda H. Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni", "Linda Moy." ],
      "venue" : "J. Mach. Learn. Res., 11:1297–1322.",
      "citeRegEx" : "Raykar et al\\.,? 2010",
      "shortCiteRegEx" : "Raykar et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep learning from crowds",
      "author" : [ "Filipe Rodrigues", "Francisco C. Pereira." ],
      "venue" : "Proceedings of the AAAI.",
      "citeRegEx" : "Rodrigues and Pereira.,? 2018",
      "shortCiteRegEx" : "Rodrigues and Pereira.",
      "year" : 2018
    }, {
      "title" : "Sequence labeling with multiple annotators",
      "author" : [ "Filipe Rodrigues", "Francisco C. Pereira", "Bernardete Ribeiro." ],
      "venue" : "Mach. Learn., 95(2):165–181.",
      "citeRegEx" : "Rodrigues et al\\.,? 2014",
      "shortCiteRegEx" : "Rodrigues et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning transferrable representations for unsupervised domain adaptation",
      "author" : [ "Ozan Sener", "Hyun Oh Song", "Ashutosh Saxena", "Silvio Savarese." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Sener et al\\.,? 2016",
      "shortCiteRegEx" : "Sener et al\\.",
      "year" : 2016
    }, {
      "title" : "Get another label? improving data quality and data mining using multiple, noisy labelers",
      "author" : [ "Victor S. Sheng", "Foster J. Provost", "Panagiotis G. Ipeirotis." ],
      "venue" : "Proceedings of the KDD, pages 614–622.",
      "citeRegEx" : "Sheng et al\\.,? 2008",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2008
    }, {
      "title" : "Machine learning with crowdsourcing: A brief summary of the past research and future directions",
      "author" : [ "Victor S. Sheng", "Jing Zhang." ],
      "venue" : "Proceedings of the AAAI, 33(01):9837–9843.",
      "citeRegEx" : "Sheng and Zhang.,? 2019",
      "shortCiteRegEx" : "Sheng and Zhang.",
      "year" : 2019
    }, {
      "title" : "A Bayesian approach for sequence tagging with crowds",
      "author" : [ "Edwin Simpson", "Iryna Gurevych." ],
      "venue" : "Proceedings of the EMNLP-IJCNLP.",
      "citeRegEx" : "Simpson and Gurevych.,? 2019",
      "shortCiteRegEx" : "Simpson and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks",
      "author" : [ "Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Ng" ],
      "venue" : "In Proceedings of the EMNLP",
      "citeRegEx" : "Snow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2008
    }, {
      "title" : "Semi-supervised consensus labeling for crowdsourcing",
      "author" : [ "Wei Tang", "Matthew Lease." ],
      "venue" : "SIGIR 2011 workshop on crowdsourcing for information retrieval (CIR), pages 1–6.",
      "citeRegEx" : "Tang and Lease.,? 2011",
      "shortCiteRegEx" : "Tang and Lease.",
      "year" : 2011
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the CoNLL at HLT-NAACL 2003.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "UDapter: Language adaptation for truly Universal Dependency parsing",
      "author" : [ "Ahmet Üstün", "Arianna Bisazza", "Gosse Bouma", "Gertjan van Noord." ],
      "venue" : "Proceedings of the EMNLP, pages 2302–2315.",
      "citeRegEx" : "Üstün et al\\.,? 2020",
      "shortCiteRegEx" : "Üstün et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-domain named entity recognition with genre-aware and agnostic inference",
      "author" : [ "Jing Wang", "Mayank Kulkarni", "Daniel PreotiucPietro." ],
      "venue" : "Proceedings of the ACL, pages 8476–8488.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Cost-saving effect of crowdsourcing learning",
      "author" : [ "Lu Wang", "Zhi-Hua Zhou." ],
      "venue" : "Proceedings of IJCAI, IJCAI’16, pages 2111–2117.",
      "citeRegEx" : "Wang and Zhou.,? 2016",
      "shortCiteRegEx" : "Wang and Zhou.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the EMNLP: System Demonstrations, pages 38–45.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on recent advances in named entity recognition from deep learning models",
      "author" : [ "Vikas Yadav", "Steven Bethard." ],
      "venue" : "Proceedings of the COLING.",
      "citeRegEx" : "Yadav and Bethard.,? 2018",
      "shortCiteRegEx" : "Yadav and Bethard.",
      "year" : 2018
    }, {
      "title" : "Adversarial learning for chinese NER from crowd annotations",
      "author" : [ "YaoSheng Yang", "Meishan Zhang", "Wenliang Chen", "Wei Zhang", "Haofen Wang", "Min Zhang." ],
      "venue" : "Proceedings of the AAAI.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Domain adaptation for dependency parsing via selftraining",
      "author" : [ "Juntao Yu", "Mohab Elkaref", "Bernd Bohnet." ],
      "venue" : "Proceedings of the 14th International Conference on Parsing Technologies, pages 1–10.",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Crowdsourcing translation: Professional quality from non-professionals",
      "author" : [ "Omar F. Zaidan", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the ACLHLT, pages 1220–1229.",
      "citeRegEx" : "Zaidan and Callison.Burch.,? 2011",
      "shortCiteRegEx" : "Zaidan and Callison.Burch.",
      "year" : 2011
    }, {
      "title" : "Learning from crowdsourced labeled data: a survey",
      "author" : [ "Jing Zhang", "Xindong Wu", "Victor S. Sheng." ],
      "venue" : "Artif. Intell. Rev., 46(4):543–576.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "On learning invariant representations for domain adaptation",
      "author" : [ "Han Zhao", "Remi Tachet des Combes", "Kun Zhang", "Geoffrey J. Gordon." ],
      "venue" : "Proceedings of the ICML, pages 7523–7532. PMLR.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al.",
      "startOffset" : 156,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al.",
      "startOffset" : 156,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : ", 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : ", 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 55,
      "context" : ", 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al.",
      "startOffset" : 21,
      "endOffset" : 54
    }, {
      "referenceID" : 31,
      "context" : ", 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 45,
      "context" : "In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018).",
      "startOffset" : 167,
      "endOffset" : 204
    }, {
      "referenceID" : 33,
      "context" : "In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018).",
      "startOffset" : 167,
      "endOffset" : 204
    }, {
      "referenceID" : 42,
      "context" : "However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008).",
      "startOffset" : 195,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015).",
      "startOffset" : 115,
      "endOffset" : 204
    }, {
      "referenceID" : 36,
      "context" : "Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015).",
      "startOffset" : 115,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015).",
      "startOffset" : 115,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015).",
      "startOffset" : 115,
      "endOffset" : 204
    }, {
      "referenceID" : 45,
      "context" : "Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input.",
      "startOffset" : 117,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input.",
      "startOffset" : 117,
      "endOffset" : 189
    }, {
      "referenceID" : 40,
      "context" : "Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input.",
      "startOffset" : 117,
      "endOffset" : 189
    }, {
      "referenceID" : 32,
      "context" : "Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020).",
      "startOffset" : 213,
      "endOffset" : 279
    }, {
      "referenceID" : 44,
      "context" : "Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020).",
      "startOffset" : 213,
      "endOffset" : 279
    }, {
      "referenceID" : 27,
      "context" : "Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020).",
      "startOffset" : 213,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "Based on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daumé III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020).",
      "startOffset" : 181,
      "endOffset" : 263
    }, {
      "referenceID" : 3,
      "context" : "Based on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daumé III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020).",
      "startOffset" : 181,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "Based on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daumé III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020).",
      "startOffset" : 181,
      "endOffset" : 263
    }, {
      "referenceID" : 9,
      "context" : "Here we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016).",
      "startOffset" : 188,
      "endOffset" : 257
    }, {
      "referenceID" : 40,
      "context" : "Here we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016).",
      "startOffset" : 188,
      "endOffset" : 257
    }, {
      "referenceID" : 6,
      "context" : "Here we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016).",
      "startOffset" : 188,
      "endOffset" : 257
    }, {
      "referenceID" : 40,
      "context" : "Finally, we conduct experiments on a benchmark crowdsourcing NER dataset (Tjong Kim Sang and De Meulder, 2003; Rodrigues et al., 2014) to evaluate our methods.",
      "startOffset" : 73,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "We take a standard BiLSTM-CRF (Lample et al., 2016) model with BERT (Devlin et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : ", 2016) model with BERT (Devlin et al., 2019) word representations as the baseline, and adapt it to our representation learning model.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "Domain adaptation happens when a supervised model trained on a fixed set of training corpus, including several specific domains, is required to test on a different domain (Ben-David et al., 2006; Mansour et al., 2009).",
      "startOffset" : 171,
      "endOffset" : 217
    }, {
      "referenceID" : 30,
      "context" : "Domain adaptation happens when a supervised model trained on a fixed set of training corpus, including several specific domains, is required to test on a different domain (Ben-David et al., 2006; Mansour et al., 2009).",
      "startOffset" : 171,
      "endOffset" : 217
    }, {
      "referenceID" : 4,
      "context" : "The scenario is quite frequent in practice, and thus has received extensive attention with massive investigations (Csurka, 2017; Ramponi and Plank, 2020).",
      "startOffset" : 114,
      "endOffset" : 153
    }, {
      "referenceID" : 35,
      "context" : "The scenario is quite frequent in practice, and thus has received extensive attention with massive investigations (Csurka, 2017; Ramponi and Plank, 2020).",
      "startOffset" : 114,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "Note that under this setting, all Xs, including source and target domains, are generated individually according to their unknown distributions, thus the abstract representations learned from the source domain dataset Dsrc would inevitably be biased to the target domain, which is the primary reason for the degraded performance of the target domain (Huang and Yates, 2010; Ganin et al., 2016).",
      "startOffset" : 349,
      "endOffset" : 392
    }, {
      "referenceID" : 11,
      "context" : "Note that under this setting, all Xs, including source and target domains, are generated individually according to their unknown distributions, thus the abstract representations learned from the source domain dataset Dsrc would inevitably be biased to the target domain, which is the primary reason for the degraded performance of the target domain (Huang and Yates, 2010; Ganin et al., 2016).",
      "startOffset" : 349,
      "endOffset" : 392
    }, {
      "referenceID" : 35,
      "context" : "A number of domain adaptation models have struggled for better transferable high-level representations as domain shifts (Ramponi and Plank, 2020).",
      "startOffset" : 120,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : "Crowdsourcing aims to produce a set of large-scale annotated examples created by crowd annotators, which is used to train supervised models for a given task (Raykar et al., 2010).",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "As the majority of NLP models assume that gold-standard highquality training corpora are already available (Manning and Schutze, 1999), crowdsourcing learning has received much less interest than cross-domain adaptation, although the availability of these corpora is always not the truth.",
      "startOffset" : 107,
      "endOffset" : 134
    }, {
      "referenceID" : 52,
      "context" : "In this section, we take NER as a case study, which has been investigated most frequently in NLP (Yadav and Bethard, 2018), and propose a representation learning model mainly inspired by the domain adaptation model of (Jia et al.",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "In this section, we take NER as a case study, which has been investigated most frequently in NLP (Yadav and Bethard, 2018), and propose a representation learning model mainly inspired by the domain adaptation model of (Jia et al., 2019) to perform crowdsourcing learning.",
      "startOffset" : 218,
      "endOffset" : 236
    }, {
      "referenceID" : 34,
      "context" : "By using a sophisticated parameter generator module (Platanios et al., 2018), it can capture annotator-aware features.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "Different from the standard BERT exploration, here we use Adapter◦BERT (Houlsby et al., 2019), where two extra adapter modules are inside each transformer layer.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 34,
      "context" : "The key idea is to use Parameter Generation Network (PGN) (Platanios et al., 2018; Jia et al., 2019) to produce adapter parameters dynamically by input annotators.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "The key idea is to use Parameter Generation Network (PGN) (Platanios et al., 2018; Jia et al., 2019) to produce adapter parameters dynamically by input annotators.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 42,
      "context" : "Here we introduce unsupervised crowdsourcing learning in alignment with unsupervised domain adaptation, assuming that no expert annotation is available, which is the widely-adopted setting of previous work of crowdsourcing learning (Sheng et al., 2008; Zhang et al., 2016; Sheng and Zhang, 2019).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 56,
      "context" : "Here we introduce unsupervised crowdsourcing learning in alignment with unsupervised domain adaptation, assuming that no expert annotation is available, which is the widely-adopted setting of previous work of crowdsourcing learning (Sheng et al., 2008; Zhang et al., 2016; Sheng and Zhang, 2019).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 43,
      "context" : "Here we introduce unsupervised crowdsourcing learning in alignment with unsupervised domain adaptation, assuming that no expert annotation is available, which is the widely-adopted setting of previous work of crowdsourcing learning (Sheng et al., 2008; Zhang et al., 2016; Sheng and Zhang, 2019).",
      "startOffset" : 232,
      "endOffset" : 295
    }, {
      "referenceID" : 35,
      "context" : "In the unsupervised domain adaptation, the information of the target domain can be learned through a large-scale raw corpus (Ramponi and Plank, 2020), where there is no correspondence in the unsupervised crowdsourcing learning to learn information of experts.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "In fact, during or after the crowdsourcing annotation process, we usually have a quality control module, which can help to produce silvery quality pseudo-expert annotations (Kittur et al., 2008; Lease, 2011).",
      "startOffset" : 173,
      "endOffset" : 207
    }, {
      "referenceID" : 26,
      "context" : "In fact, during or after the crowdsourcing annotation process, we usually have a quality control module, which can help to produce silvery quality pseudo-expert annotations (Kittur et al., 2008; Lease, 2011).",
      "startOffset" : 173,
      "endOffset" : 207
    }, {
      "referenceID" : 32,
      "context" : "Note that although LC-cat is not as expected as LC in (Nguyen et al., 2017), our results show that LC-cat is slightly better based on Adapter◦BERT-BiLSTM-CRF.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 37,
      "context" : "The Impact of Unreliable Annotators Handling unreliable annotators, such as spammers, is a practical and common issue in Crowdsourcing (Raykar and Yu, 2012b).",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 50,
      "context" : "Crowdsourcing is a cheap and popular way to collect large-scale labeled data, which can facilitate the model training for hard tasks that require supervised learning (Wang and Zhou, 2016; Sheng and Zhang, 2019).",
      "startOffset" : 166,
      "endOffset" : 210
    }, {
      "referenceID" : 43,
      "context" : "Crowdsourcing is a cheap and popular way to collect large-scale labeled data, which can facilitate the model training for hard tasks that require supervised learning (Wang and Zhou, 2016; Sheng and Zhang, 2019).",
      "startOffset" : 166,
      "endOffset" : 210
    }, {
      "referenceID" : 23,
      "context" : "Initial studies of crowdsourcing learning try to arrive at a high-quality corpus by majority voting or control the quality by sophisticated strategies during the crowd annotation process (Khattak and Salleb-Aouissi, 2011; Liu et al., 2017; Tang and Lease, 2011).",
      "startOffset" : 187,
      "endOffset" : 261
    }, {
      "referenceID" : 28,
      "context" : "Initial studies of crowdsourcing learning try to arrive at a high-quality corpus by majority voting or control the quality by sophisticated strategies during the crowd annotation process (Khattak and Salleb-Aouissi, 2011; Liu et al., 2017; Tang and Lease, 2011).",
      "startOffset" : 187,
      "endOffset" : 261
    }, {
      "referenceID" : 46,
      "context" : "Initial studies of crowdsourcing learning try to arrive at a high-quality corpus by majority voting or control the quality by sophisticated strategies during the crowd annotation process (Khattak and Salleb-Aouissi, 2011; Liu et al., 2017; Tang and Lease, 2011).",
      "startOffset" : 187,
      "endOffset" : 261
    }, {
      "referenceID" : 40,
      "context" : "Recently, the majority work focuses on full exploration of all annotated corpus by machine learning models, taking the information from crowd annotators into account including annotator reliability (Rodrigues et al., 2014), annotator accuracy (Huang et al.",
      "startOffset" : 198,
      "endOffset" : 222
    }, {
      "referenceID" : 18,
      "context" : ", 2014), annotator accuracy (Huang et al., 2015), worker-label confusion matrix (Nguyen et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : ", 2015), worker-label confusion matrix (Nguyen et al., 2017), and sequential confusion matrix (Simpson and Gurevych, 2019).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 44,
      "context" : ", 2017), and sequential confusion matrix (Simpson and Gurevych, 2019).",
      "startOffset" : 41,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al.",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 30,
      "context" : "Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al.",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 22,
      "context" : ", 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020).",
      "startOffset" : 70,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : ", 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020).",
      "startOffset" : 70,
      "endOffset" : 201
    }, {
      "referenceID" : 12,
      "context" : ", 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020).",
      "startOffset" : 70,
      "endOffset" : 201
    }, {
      "referenceID" : 3,
      "context" : ", 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020).",
      "startOffset" : 70,
      "endOffset" : 201
    }, {
      "referenceID" : 35,
      "context" : ", 2009), which has also received great attention in the NLP community (Daumé III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020).",
      "startOffset" : 70,
      "endOffset" : 201
    }, {
      "referenceID" : 54,
      "context" : "Typical methods include self-training to produce pseudo training instances for the target domain (Yu et al., 2015) and representation learning to capture transferable features across the source and target domains (Sener et al.",
      "startOffset" : 97,
      "endOffset" : 114
    }, {
      "referenceID" : 41,
      "context" : ", 2015) and representation learning to capture transferable features across the source and target domains (Sener et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 52,
      "context" : "NER is a fundamental and challenging task of NLP (Yadav and Bethard, 2018).",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "The BiLSTM-CRF (Lample et al., 2016) architecture, as well as BERT (Devlin et al.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : ", 2016) architecture, as well as BERT (Devlin et al., 2019), are able to bring state-of-the-",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "art performance in the literature (Jia et al., 2019; Wang et al., 2020; Jia and Zhang, 2020).",
      "startOffset" : 34,
      "endOffset" : 92
    }, {
      "referenceID" : 49,
      "context" : "art performance in the literature (Jia et al., 2019; Wang et al., 2020; Jia and Zhang, 2020).",
      "startOffset" : 34,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "art performance in the literature (Jia et al., 2019; Wang et al., 2020; Jia and Zhang, 2020).",
      "startOffset" : 34,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "In addition, NER has been widely adopted as crowdsourcing learning as well (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016; Yang et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 163
    }, {
      "referenceID" : 40,
      "context" : "In addition, NER has been widely adopted as crowdsourcing learning as well (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016; Yang et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "In addition, NER has been widely adopted as crowdsourcing learning as well (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016; Yang et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 163
    }, {
      "referenceID" : 53,
      "context" : "In addition, NER has been widely adopted as crowdsourcing learning as well (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016; Yang et al., 2018).",
      "startOffset" : 75,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "There are still many other sophisticated cross-domain models, such as adversarial learning (Ganin et al., 2016) and self-training (Yu et al.",
      "startOffset" : 91,
      "endOffset" : 111
    } ],
    "year" : 2021,
    "abstractText" : "Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as goldstandard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only smallscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.",
    "creator" : "LaTeX with hyperref"
  }
}