{
  "name" : "2021.acl-long.40.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification",
    "authors" : [ "George Chrysostomou", "Nikolaos Aletras" ],
    "emails" : [ "n.aletras}@sheffield.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 477–488\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n477"
    }, {
      "heading" : "1 Introduction",
      "text" : "Natural Language Processing (NLP) approaches for text classification are often underpinned by large neural network models (Cho et al., 2014; Devlin et al., 2019). Despite the high accuracy and efficiency of these models in dealing with large amounts of data, an important problem is their increased complexity that makes them opaque and hard to interpret by humans which usually treat\n1Code is available at: https://github.com/ GChrysostomou/tasc.git\nthem as black boxes (Zhang et al., 2018; Linzen et al., 2019).\nAttention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explanations for a given prediction and qualitative model analysis by assigning importance to input tokens using scores provided by attention mechanisms (Chen et al., 2017; Wang et al., 2016; Jain et al., 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019).\nA faithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019).\nA limitation of attention as an indicator of input importance is that it refers to the word in context due to information mixing in the model (Tutek and Snajder, 2020). Motivated by this, we aim to improve the effectiveness of neural models in providing more faithful attention-based explanations for text classification, by introducing noncontextualised information in the model. Our contributions are as follows:\n• We introduce three Task-Scaling (TaSc) mechanisms (§4), a family of encoder-independent components that learn task-specific noncontextualised importance scores for each word in the vocabulary to scale the original attention weights which can be easily ported to any neural architecture;\n• We show that TaSc variants offer more robust, consistent and faithful attention-based explanations compared to using vanilla attention in a set of standard interpretability benchmarks, without sacrificing predictive performance (§6);\n• We demonstrate that attention-based explanations with TaSc consistently outperform explanations obtained from two gradient-based and a word-erasure explanation approaches (§7)."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Model Interpretability",
      "text" : "Explanations for neural networks can be obtained by identifying which parts of the input are important for a given prediction. One way is to use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). Another way is to calculate the difference in a model’s prediction between keeping and omitting an input token (Robnik-Šikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021).\nNguyen (2018) and Atanasova et al. (2020) compare explanations produced by different approaches, showing that in most cases gradientbased approaches outperform sparse linear metamodels."
    }, {
      "heading" : "2.2 Attention as Explanation",
      "text" : "Attention weights have been extensively used to interpret model predictions in NLP; i.e. (Cho et al., 2014; Xu et al., 2015; Barbieri et al., 2018; Ghaeini et al., 2018). However, the hypothesis that attention should be used as explanation had not been explicitly studied until recently.\nJain and Wallace (2019) first explored the effectiveness of attention explanations. They show that adversary attention distributions can yield equivalent predictions with the original attention distribution, suggesting that attention weights do not offer\nrobust explanations. In contrast to Jain and Wallace (2019), Wiegreffe and Pinter (2019) and Vashishth et al. (2019) demonstrate that attention weights can in certain cases provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipulating the attention mechanism by penalising words a priori known to be relevant to the task, showing that the predictive performance remain relatively unaffected. Sen et al. (2020) assess the plausibility of attention weights by correlating them with manually annotated explanation heat-maps, where plausibility refers to how convincing an explanation is to humans (Jacovi and Goldberg, 2020). However, Jacovi and Goldberg (2020) and Grimsley et al. (2020) suggest caution with interpreting the results of these experiments as they do not test the faithfulness of explanations (e.g. an explanation can be non-plausible but faithful or vice-versa).\nSerrano and Smith (2019) test the faithfulness of attention-based explanations by removing tokens to observe how fast a decision flip happens. Results show that gradient attention-based rankings (i.e. combining an attention weight with its gradient) better predict word importance for model predictions, compared to just using the attention weights. Tutek and Snajder (2020) propose a method to improve the faithfulness of attention explanations when using recurrent encoders by introducing a word-level objective to sequence classification tasks. Focusing also on recurrent-encoders, Mohankumar et al. (2020) introduce a modification to recurrent encoders to reduce repetitive information across different words in the input to improve faithfulness of explanations.\nTo the best of our knowledge, no previous work has attempted to improve the faithfulness of attention-based explanations across different encoders for text classification by inducing taskspecific information to the attention weights."
    }, {
      "heading" : "3 Neural Text Classification Models",
      "text" : "In a typical neural model with attention for text classification; one-hot-encoded tokens xi P R|V| are first mapped to embeddings ei P Rd, where i P r1, ..., ts denotes the position in the sequence, t the sequence length, |V | the vocabulary size and d the dimensionality of the embeddings. The embeddings ei are then passed to an encoder to produce hidden representations hi “ Encpeiq, where\nhi PRN, with N the size of the hidden representation. A vector representation c for the entire text sequence x1, ..., xt is subsequently obtained as the sum of hi weighted by attention scores αi:\nc “ ÿ\ni\nci, ci “ hiαi, c PRN (1)\nVector c is finally passed to the output, a fullyconnected linear layer followed by a softmax activation function."
    }, {
      "heading" : "3.1 Encoders",
      "text" : "To obtain representations hi, we consider the following recurrent, non-recurrent and Transformer (Vaswani et al., 2017) encoders, Encp.q, as in (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019): (i) bidirectional Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber (1997)); (ii) bidirectional Gated Recurrent Unit (GRU; Cho et al. (2014)); (iii) Convolutional Neural Network (CNN; LeCun et al. (1999)); (iv) Multi-Layer Perceptron (MLP); (v) BERT2 (Devlin et al., 2019)."
    }, {
      "heading" : "3.2 Attention Mechanisms",
      "text" : "Attention scores (ai) are computed by passing the representations (hi) obtained from the encoder to the attention mechanism which usually consists of a similarity function φ followed by softmax:\nai “ exppφphi,qqq\nřt k“1 exppφpq,hkqq\n(2)\nwhere q PRN is a trainable self-attention vector similar to Yang et al. (2016).\nFollowing Jain and Wallace (2019), we consider two self-attention similarity functions: (i) Additive Attention (Tanh; Bahdanau et al. (2015)):\nφphi,qq “ qT tanhpWhiq (3)\nwhere W is a trainable model parameter; and (ii) Scaled Dot-Product (Dot; Vaswani et al. (2017)):\nφphi,qq “ hTi q? N\n(4)"
    }, {
      "heading" : "4 Task-Scaling (TaSc) Mechanisms",
      "text" : "Attention indicates how well inputs around a position i correspond to the output (Bahdanau et al., 2015). For example, in a bidirectional recurrent\n2We use BERT to obtain hi with an attention mechanism on top for consistency with the other encoders\nencoder each token representation hi contains information from the whole sequence so the attention weights actually refer to the input word in context and not individually (Tutek and Snajder, 2020).\nInspired by the simple and highly interpretable bag-of-words models, which assign a single weight for each word type (word in a vocabulary), we hypothesise that by scaling each input word’s contextualised representation ci (see Eq. 1) by its attention score and and a non-contextualised word type scalar score, we can improve attention-based explanations. The intuition is that by having a less contextualised sequence representation c we can reduce information mixing for attention.\nFor that purpose, we introduce the noncontextualised word type score sxi in Eq. 1 to enrich the text representation c, such that:\nc “ ÿ\ni\nhiαisxi , c PRN (5)\nWe compute sxi by proposing three Task-Scaling (TaSc) mechanisms.3"
    }, {
      "heading" : "4.1 Linear TaSc (Lin-TaSc)",
      "text" : "We first introduce Linear TaSc (Lin-TaSc), the simplest method in the family of TaSc mechanisms that estimates a scalar weight for each word in the vocabulary by introducing a new vector u PR|V|. Given the input sequence x “ rx1, . . . , xts representing one-hot-encodings of the tokens, we perform a look up on u to obtain the scalar weights of words in the sequence. u is randomly initialised and updated partially at each training iteration, because naturally each input sequence contains only a small subset of the vocabulary words.\nWe then obtain a task-scaled embedding êi for a token i in the input by multiplying the original token embedding with its word type weight ui:\nêi “ uiei (6)\nThe intuition is that the embedding vector ei was trained on general corpora and is a noncontextualised “generic” representation of input xi. As such the score ui will scale ei to the task. We subsequently compute context-independent scores sxi for each token in the sequence, by summing all elements of its corresponding task-scaled embedding êi; sxi “\nřd êi in a similar way that token embeddings are averaged in the top-layers of a\n3Number of parameters for each proposed mechanism in Appendix B.\nneural architecture. We opted to sum-up and not average, because we want to retain large and small values from the task-scaled embedding vector êi (Atanasova et al., 2020).4\nAs the attention scores pertain to the word in context (Tutek and Snajder, 2020), we also expect the score sxi to pertain to the word without the contextualised information. That way, we complement attention which results into a richer sequence representation c."
    }, {
      "heading" : "4.2 Feature-wise TaSc (Feat-TaSc)",
      "text" : "Lin-TaSc assigns equal weighting to all the dimensions of the word embedding ei (see Eq. 6), but some of them might be more important than others. Inspired by the RETAIN mechanism (Choi et al., 2016), Feature-wise TaSc (Feat-TaSc) learns different weights for each embedding dimension to identify the most important of them. Compared to Lin-TaSc where ei is scaled uniformly across all vector dimensions, with Feat-TaSc each dimension is scaled independently. To achieve this, we introduce a learnable matrix U PR|V|ˆd. Similar to Lin-TaSc, given the input sequence x, we perform a look up on U to obtain Us “ ru1, . . . ,uts. U is randomly initialised and updated partially at each training iteration. To obtain sxi , we perform a dot product between ui and embedding vector ei; sxi “ ui ¨ ei."
    }, {
      "heading" : "4.3 Convolutional TaSc (Conv-TaSc)",
      "text" : "Lin-TaSc and Feat-TaSc weigh the original word embedding ei but do not consider any interactions between embedding dimensions. Conv-TaSc addresses this limitation by extending Lin-TaSc.5 We apply a CNN6 with n channels over the scaled embedding êi from Lin-TaSc, keeping a single stride and a 1-dimensional kernel. This way, we ensure that input words remain context-independent. We then sum over the filtered scaled embedding êfi , to obtain the scores sxi ; sxi “ řd êfi . 4\n4We also tried max and mean-pooling or using the ui directly instead of si in early experimentation resulting in lower results.\n5We only apply Conv-TaSc over Lin-TaSc to keep the mechanism relatively lightweight. Note that Feat-TaSc learns an extra matrix of equal size to the embedding matrix.\n6See CNN configurations in Appendix A."
    }, {
      "heading" : "5 Evaluating Attention-based Interpretability",
      "text" : "Jacovi and Goldberg (2020) propose that an appropriate measure of faithfulness of an explanation can be obtained through erasure (the most relevant parts of the input–according to the explanation– are removed). We therefore follow this evaluation approach similar to Serrano and Smith (2019), Atanasova et al. (2020) and Nguyen (2018).7"
    }, {
      "heading" : "5.1 Attention-based Importance Metrics",
      "text" : "We opt using the following three input importance metrics by Serrano and Smith (2019):8\n• α: Importance rank corresponding to normalised attention scores.\n• ∇α: Provides a ranking by computing the gradient of the predicted label ŷ with respect to each attention score αi in descending order, such that∇αi “ BŷBαi .\n• α∇α: Scales the attention scores αi with their corresponding gradients∇αi."
    }, {
      "heading" : "5.2 Faithfulness Metrics",
      "text" : "Decision Flip - Most Informative Token: The average percentage of decision flips (i.e. changes in model prediction) occurred in the test set by removing the token with highest importance.\nDecision Flip - Fraction of Tokens: The average fraction of tokens required to be removed to cause a decision flip in the test set.\nNote that we conduct all experiments at the input level (i.e. by removing the token from the input sequence instead of only removing its corresponding attention weight) as we consider the scores from importance metrics to pertain to the corresponding input token following related work (Arras et al., 2016, 2017; Nguyen, 2018; Vashishth et al., 2019; Grimsley et al., 2020; Atanasova et al., 2020)."
    }, {
      "heading" : "6 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "6.1 Data",
      "text" : "We use five datasets for text classification following Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR\n7Note that Jacovi and Goldberg (2020) argue that a human evaluation is not an appropriate method to test faithfulness.\n8Serrano and Smith (2019) show that gradient-based attention ranking metrics (∇α, α∇α) are better in providing faithful explanations compared to just using attention (α).\nTweets (Sarker et al., 2015); (iv) AG News;9 and (v) MIMIC Anemia (Johnson et al., 2016). See Table 1 for detailed data statistics."
    }, {
      "heading" : "6.2 Predictive Performance",
      "text" : "A prerequisite of interpretability is to obtain robust explanations without sacrificing predictive performance (Lipton, 2016). Table 2 shows the macro F1scores of all models across datasets, encoders and attention mechanisms using the three TaSc variants (Lin-TaSc, Feat-TaSc and Conv-TaSc described in Section 4) and without TaSc (No-TaSc).10\nIn general, all TaSc models obtain comparable performance and in some cases outperform NoTaSc across datasets and attention mechanisms. However, our main aim is not to improve predictive performance but the faithfulness of attention-based explanations, which we illustrate below."
    }, {
      "heading" : "6.3 Decision Flip: Most Informative Token",
      "text" : "Table 3 and Figure 1 present the mean average percentage of decision flips (higher is better) across attention mechanisms, encoders and datasets by removing the most informative token for TaSc variants and No-TaSc for all attention-based importance metrics (see Section 5).\nIn Table 3, we observe that TaSc variants are effective in identifying the single most important token, outperforming No-TaSc in 12 out of 18 cases across attention-based importance metrics. This suggests that the attention mechanisms benefit from the non-contextualised information encapsulated in TaSc when allocating importance to the input tokens. Models using Tanh without TaSc appear to produce on average a higher percentage of decision flips compared to those using the Dot mechanism. Using either of the TaSc variants improves both\n9https://di.unipi.it/˜gulli/AG_corpus_ of_news_articles.html\n10For model hyper-parameters and prepossessing steps see Appendix A.\n11Lower predictive performance is observed with BERT in MIMIC, as BERT accepts a maximum of 512 word pieces as input. See Appendix A.\nmechanisms, with Dot mechanism benefiting the most, making it comparable to Tanh. For example, Dot moves from 8.2% with No-TaSc to 11.8% with Lin-TaSc, which is closer to 14.0% achieved by Lin-TaSc with Tanh (for α∇α).\nThe first row of Figure 1 presents a comparison across encoders. TaSc variants achieve improved performance over No-TaSc across all encoder variants with ∇α and α∇α. All TaSc variants yield comparable results with the exception\nof Conv-TaSc with BERT. Results further suggest that non-recurrent encoders (MLP, CNN) without TaSc outperform recurrent encoders (LSTM, GRU) and BERT which has the poorest performance. We hypothesise that this is due to the attention module becoming more important without feature contextualisation which is similar to findings of Serrano and Smith (2019) and Wiegreffe and Pinter (2019). However, we observe that using any of the TaSc variants across encoders results into improvements with LSTM and GRU becoming comparable to MLP and CNN. For example, BERT without TaSc improves from 5.7% to 8.0% (relative improvement 1.4x) and 9.3% (relative improvement 1.6x) using Lin-TaSc and Feat-TaSc respectively (for α∇α).\nObserving results in the second row of Figure 1, we see that TaSc variants outperform No-TaSc in all datasets when using ∇α and α∇α. This highlights the robustness of TaSc as improvements are irrespective of the dataset. In general, LinTaSc and Feat-TaSc perform equally well, however Lin-TaSc has the smaller number of parameters amongst the three variants. Similar to the findings of Serrano and Smith (2019) best results overall, irrespective of the use of TaSc, are obtained using α∇α to rank importance."
    }, {
      "heading" : "6.4 Decision Flip: Fraction of Tokens",
      "text" : "Providing one token (i.e., the most informative) as an explanation is not always a realistic approach to assessing faithfulness. In our second experiment, we test TaSc by measuring the fraction of important tokens required to be removed to cause a decision flip (change model’s prediction). Table 4 and Figure 2 show the mean average fraction of tokens required to be removed to cause a decision flip (lower is better) across attention mechanisms, encoders and datasets for all importance metrics.\nIn Table 4, we see that attention-based explanations from models trained with any of the TaSc mechanisms require on average a lower fraction of tokens to cause a decision flip compared to No-\nTaSc (in 17 out of 18 cases). Overall Lin-TaSc achieves higher or comparable relative improvements over Conv-TaSc and Feat-TaSc in 5 out of 6 times.\nWe present an across encoders comparison in the first row of Figure 2. All three TaSc variants obtain comparable performance with the exception of Conv-TaSc with BERT. We hypothesise that with BERT, Conv-TaSc fails to capture interactions between embedding dimensions due to perhaps higher contextualisation of BERT embeddings (i.e. contain more duplicate information). Similarly to the previous experiment results suggest that nonrecurrent encoders (MLP and CNN) without TaSc outperform the remainder of encoders, with BERT having the worst performance. This strengthens our hypothesis that attention becomes more important to a model with reduced contextualisation. When using TaSc, performance across all encoders becomes comparable with the exception of BERT. For example, GRU improves from .43 with NoTaSc to .16 with Lin-TaSc, .17 with Feat-TaSc and .18 with Conv-TaSc (for α∇α).\nThe second row of Figure 2 presents results across datasets. All three TaSc mechanims manage to outperform vanilla attention. Lin-TaSc and FeatTaSc perform comparably, with the first having a slight edge obtaining highest relative improvements in 3 out of 5 datasets with α∇α. For example in\nADR, No-TaSc requires on average .77 of all tokens to be removed for a decision flip to occur compared to .34 obtained by Lin-TaSc (for α∇α). The benefits of TaSc become evident when considering longer sequences. For example in MIMIC, Lin-TaSc requires on average 44 tokens to cause a decision flip compared to 220 for No-TaSc."
    }, {
      "heading" : "6.5 Robustness Analysis",
      "text" : "We also perform a detailed comparison between the best performing TaSc variant (Lin-TaSc) and vanilla attention (No-TaSc) across all test instances. Figure 3 shows box-plots with the median fraction of tokens required to be removed for causing a decision flip when ranking tokens by all three importance metrics. For brevity we present results for four cases.\nWe notice that the median fraction of tokens required to cause a decision flip for Lin-TaSc using α is higher compared to No-TaSc in certain cases. However, Lin-TaSc results in consistently lower medians (with substantially reduced variances) compared to No-TaSc using∇α and α∇α which are more effective importance metrics. This is particularly visible in ADR using BERT, where the 25% and 75% percentiles are much closer to the median values, compared to No-TaSc. Reduced variances suggest that the explanation faithfulness across instances remains consistent."
    }, {
      "heading" : "7 Comparing TaSc with Non-attention",
      "text" : "Input Importance Metrics\nWe finally compare explanations provided by using Lin-TaSc and α∇α to three standard non-attention input importance metrics without TaSc which are strong baselines for explainability (Nguyen, 2018; Atanasova et al., 2020).\nWord Omission (WO) (Robnik-Šikonja and Kononenko, 2008; Nguyen, 2018): Ranking input words by computing the difference between the probabilities of the predicted class when including a word i and omitting it: WOi “ ppŷ|xq ´ ppŷ|xzxiq\nInputXGrad (x∇x) (Kindermans et al., 2016; Atanasova et al., 2020): Ranking words by multiplying the gradient of the input by the input with respect to the predicted class: ∇xi “ BŷBxi Integrated Gradients (IG) (Sundararajan et al., 2017): Ranking words by computing the integral of the gradients taken along a straight path from a baseline input to the original input, where the baseline is the zero embedding vector.\nComparison Results Table 5 shows the results on decision flip (fraction of tokens removed) comparing the best performing attention-based importance metric (α∇α) with Lin-TaSc to Non-TaSc models with WO, x∇x and IG importance met-\nrics across all encoders and datasets.12 We observe that using α∇α with TaSc to rank word importance requires a lower fraction of tokens to cause a decision flip on average compared to WO, x∇x and IG without TaSc. We outperform the other explanation approaches in 40 out of 50 cases, whilst obtaining comparable performance in other 5 cases. This demonstrates the efficacy of TaSc in providing more faithful attention-based explanations than strong baselines without TaSc (Nguyen, 2018; Atanasova et al., 2020). The improvements are particularly evident using BERT as an encoder. In IMDB, WO with Tanh requires on average .23 of the tokens to be removed for a decision flip compared to just .07 for α∇α with TaSc.\nWe also observe that the attention-based importance metric (α∇α) with TaSc is a more robust explanation technique than non-attention based ones, obtaining lower variance in the fraction of tokens required to cause a decision flip across encoders. For example α∇α with TaSc and Tanh requires a fraction of tokens in the range of .01-.05 compared to IG which requires .02-.43 in MIMIC, showing the consistency of our proposed approach.\nFinally we observe that TaSc consistently improves non-attention based explanation approaches (WO, x∇x and IG) requiring a lower fraction of tokens to be removed compared to Non-TaSc across encoders, datasets and attention mechanisms in the majority of cases (see full results in Appendix E)."
    }, {
      "heading" : "8 Qualitative Analysis",
      "text" : "We finally examine qualitatively what type of information the parameter u from Lin-TaSc learns. Similar to a bag-of-words model, our initial hypothesis is that u will assign high scores to the words that are most relevant to the task. Figure 4 illustrates the 5 highest and lowest scored words from the IMDB and ADR datasets with a LSTM encoder and Dot attention and CNN encoder and Tanh attention respectively. For brevity we include two examples, however observations hold similar throughout other configurations (e.g. encoders, datasets) and when increasing the number of top-k words.\nWe first observe in 4a, that indeed words expressing sentiment are assigned with high scores (e.g. excellent, waste, perfect), either positive or negative. However, a positive or negative sign does\n12We do not compare with LIME (Ribeiro et al., 2016) because WO and the gradient-based approaches outperform it (Nguyen, 2018; Atanasova et al., 2020).\nnot correspond to supporting the positive or negative class respectively. For example withdrawal in ADR can be considered relevant to positive class, yet it is negatively scored. Also sick can be considered a withdrawal symptom which is relevant to the negative class, yet it is positively scored. We speculate that this happens due to the complex nonlinear relationships between the input words and the target classes learned by the model."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We introduced TaSc, a family of three encoderindependent mechanisms that induce contextindependent task-specific information to attention. We conducted an extensive series of experiments showing the superiority of TaSc over vanilla attention on improving faithfulness of attention-based interpretability without sacrificing predictive performance. Finally, we showed that attention-based explanations with TaSc outperform other interpretability techniques. For future work, we will explore the effectiveness of TaSc in sequence-tosequence tasks similar to Vashishth et al. (2019)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their constructive and detailed comments that helped to improve the paper. Nikolaos Aletras is supported by EPSRC grant EP/V055712/1, part of the European Commission CHIST-ERA programme, call 2019 XAI: Explainable Machine Learning-based Artificial Intelligence."
    } ],
    "references" : [ {
      "title" : "Explaining predictions of non-linear classifiers",
      "author" : [ "Leila Arras", "Franziska Horn", "Grégoire Montavon", "Klaus-Robert Müller", "Wojciech Samek" ],
      "venue" : null,
      "citeRegEx" : "Arras et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arras et al\\.",
      "year" : 2016
    }, {
      "title" : "Explaining recurrent neural network predictions in sentiment analysis",
      "author" : [ "Leila Arras", "Grégoire Montavon", "Klaus-Robert Müller", "Wojciech Samek." ],
      "venue" : "Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social",
      "citeRegEx" : "Arras et al\\.,? 2017",
      "shortCiteRegEx" : "Arras et al\\.",
      "year" : 2017
    }, {
      "title" : "A diagnostic study of explainability techniques for text classification",
      "author" : [ "Pepa Atanasova", "Jakob Grue Simonsen", "Christina Lioma", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Atanasova et al\\.,? 2020",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Interpretable emoji prediction via label-wise attention LSTMs",
      "author" : [ "Francesco Barbieri", "Luis Espinosa-Anke", "Jose Camacho-Collados", "Steven Schockaert", "Horacio Saggion." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in",
      "citeRegEx" : "Barbieri et al\\.,? 2018",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2018
    }, {
      "title" : "Interpretable neural predictions with differentiable binary variables",
      "author" : [ "Jasmijn Bastings", "Wilker Aziz", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963–2977, Florence, Italy. Associa-",
      "citeRegEx" : "Bastings et al\\.,? 2019",
      "shortCiteRegEx" : "Bastings et al\\.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Paragraph-level rationale extraction through regularization: A case study on European court of human rights cases",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Dimitrios Tsarapatsanis", "Nikolaos Aletras", "Ion Androutsopoulos", "Prodromos Malakasiotis." ],
      "venue" : "In",
      "citeRegEx" : "Chalkidis et al\\.,? 2021",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning variational word masks to improve the interpretability of neural text classifiers",
      "author" : [ "Hanjie Chen", "Yangfeng Ji." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4236–4251, On-",
      "citeRegEx" : "Chen and Ji.,? 2020",
      "shortCiteRegEx" : "Chen and Ji.",
      "year" : 2020
    }, {
      "title" : "Recurrent attention network on memory for aspect sentiment analysis",
      "author" : [ "Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Learning to faithfully rational",
      "author" : [ "ron C. Wallace" ],
      "venue" : null,
      "citeRegEx" : "Wallace.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wallace.",
      "year" : 2020
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Fasttext.zip: Compressing text classification models. CoRR, abs/1612.03651",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Hervé Jégou", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Joulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Investigating the influence of noise and distractors on the interpretation of neural networks",
      "author" : [ "Pieter-Jan Kindermans", "Kristof Schütt", "Klaus-Robert Müller", "Sven Dähne." ],
      "venue" : "arXiv preprint arXiv:1611.07270.",
      "citeRegEx" : "Kindermans et al\\.,? 2016",
      "shortCiteRegEx" : "Kindermans et al\\.",
      "year" : 2016
    }, {
      "title" : "Object recognition with gradient-based learning",
      "author" : [ "Yann LeCun", "Patrick Haffner", "Léon Bottou", "Yoshua Bengio." ],
      "venue" : "Shape, contour and grouping in computer vision, pages 319–345.",
      "citeRegEx" : "LeCun et al\\.,? 1999",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1999
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing and understanding neural models in NLP",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding neural networks through representation erasure",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1612.08220.",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "The mythos of model interpretability",
      "author" : [ "Zachary C Lipton." ],
      "venue" : "int. conf. In Machine Learning: Workshop on Human Interpretability in Machine Learning.",
      "citeRegEx" : "Lipton.,? 2016",
      "shortCiteRegEx" : "Lipton.",
      "year" : 2016
    }, {
      "title" : "A unified approach to interpreting model predictions",
      "author" : [ "Scott M Lundberg", "Su-In Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Lundberg and Lee.,? 2017",
      "shortCiteRegEx" : "Lundberg and Lee.",
      "year" : 2017
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: Human lan-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling,",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Explanation in artificial intelligence: Insights from the social sciences",
      "author" : [ "Tim Miller." ],
      "venue" : "Artificial intelligence, 267:1–38.",
      "citeRegEx" : "Miller.,? 2019",
      "shortCiteRegEx" : "Miller.",
      "year" : 2019
    }, {
      "title" : "Towards transparent and explainable attention models",
      "author" : [ "Akash Kumar Mohankumar", "Preksha Nema", "Sharan Narasimhan", "Mitesh M. Khapra", "Balaji Vasan Srinivasan", "Balaraman Ravindran." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Mohankumar et al\\.,? 2020",
      "shortCiteRegEx" : "Mohankumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Comparing automatic and human evaluation of local explanations for text classification",
      "author" : [ "Dong Nguyen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Nguyen.,? 2018",
      "shortCiteRegEx" : "Nguyen.",
      "year" : 2018
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to deceive with attention-based explanations",
      "author" : [ "Danish Pruthi", "Mansi Gupta", "Bhuwan Dhingra", "Graham Neubig", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4782–",
      "citeRegEx" : "Pruthi et al\\.,? 2020",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka." ],
      "venue" : "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50.",
      "citeRegEx" : "Řehůřek and Sojka.,? 2010",
      "shortCiteRegEx" : "Řehůřek and Sojka.",
      "year" : 2010
    }, {
      "title" : "why should I trust you?”: Explaining the predictions of any classifier",
      "author" : [ "Marco Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demon-",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Explaining classifications for individual instances",
      "author" : [ "Marko Robnik-Šikonja", "Igor Kononenko." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 20(5):589–600.",
      "citeRegEx" : "Robnik.Šikonja and Kononenko.,? 2008",
      "shortCiteRegEx" : "Robnik.Šikonja and Kononenko.",
      "year" : 2008
    }, {
      "title" : "Utilizing social media data for pharmacovigilance: a review",
      "author" : [ "Abeed Sarker", "Rachel Ginn", "Azadeh Nikfarjam", "Karen O’Connor", "Karen Smith", "Swetha Jayaraman", "Tejaswi Upadhaya", "Graciela Gonzalez" ],
      "venue" : "Journal of Biomedical Informatics,",
      "citeRegEx" : "Sarker et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sarker et al\\.",
      "year" : 2015
    }, {
      "title" : "Human attention maps for text classification: Do humans and neural networks focus on the same words",
      "author" : [ "Cansu Sen", "Thomas Hartvigsen", "Biao Yin", "Xiangnan Kong", "Elke Rundensteiner" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Sen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2020
    }, {
      "title" : "Is attention interpretable",
      "author" : [ "Sofia Serrano", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Serrano and Smith.,? \\Q2019\\E",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "How to fine-tune bert for text classification? In China National Conference on Chinese Computational Linguistics, pages 194–206",
      "author" : [ "Chi Sun", "Xipeng Qiu", "Yige Xu", "Xuanjing Huang." ],
      "venue" : "Springer.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding attention for text classification",
      "author" : [ "Xiaobing Sun", "Wei Lu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3418–3428, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Sun and Lu.,? 2020",
      "shortCiteRegEx" : "Sun and Lu.",
      "year" : 2020
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 3319–3328. JMLR.org.",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "The explanation game: Towards prediction explainability through sparse communication",
      "author" : [ "Marcos Treviso", "André F.T. Martins." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 107–",
      "citeRegEx" : "Treviso and Martins.,? 2020",
      "shortCiteRegEx" : "Treviso and Martins.",
      "year" : 2020
    }, {
      "title" : "Staying true to your word: (how) can attention become explanation? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 131–142, Online",
      "author" : [ "Martin Tutek", "Jan Snajder." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Tutek and Snajder.,? 2020",
      "shortCiteRegEx" : "Tutek and Snajder.",
      "year" : 2020
    }, {
      "title" : "Attention interpretability across NLP tasks",
      "author" : [ "Shikhar Vashishth", "Shyam Upadhyay", "Gaurav Singh Tomar", "Manaal Faruqui." ],
      "venue" : "arXiv preprint arXiv:1909.11218.",
      "citeRegEx" : "Vashishth et al\\.,? 2019",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-based LSTM for aspectlevel sentiment classification",
      "author" : [ "Yequan Wang", "Minlie Huang", "Xiaoyan Zhu", "Li Zhao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 32nd International",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Opening the black box of neural networks: methods for interpreting neural network models in clinical applications",
      "author" : [ "Zhongheng Zhang", "Marcus W Beck", "David A Winkler", "Bin Huang", "Wilbert Sibanda", "Hemant Goyal" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks.",
      "startOffset" : 15,
      "endOffset" : 92
    }, {
      "referenceID" : 43,
      "context" : "Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks.",
      "startOffset" : 15,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its con-",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : ", 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019).",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : ", 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019).",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : "recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 43,
      "context" : "results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019).",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 39,
      "context" : "A limitation of attention as an indicator of input importance is that it refers to the word in context due to information mixing in the model (Tutek and Snajder, 2020).",
      "startOffset" : 142,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018).",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018).",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018).",
      "startOffset" : 55,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 179
    }, {
      "referenceID" : 37,
      "context" : "Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 179
    }, {
      "referenceID" : 41,
      "context" : "To obtain representations hi, we consider the following recurrent, non-recurrent and Transformer (Vaswani et al., 2017) encoders, Encp.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Attention indicates how well inputs around a position i correspond to the output (Bahdanau et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "We use BERT to obtain hi with an attention mechanism on top for consistency with the other encoders encoder each token representation hi contains information from the whole sequence so the attention weights actually refer to the input word in context and not individually (Tutek and Snajder, 2020).",
      "startOffset" : 272,
      "endOffset" : 297
    }, {
      "referenceID" : 2,
      "context" : "average, because we want to retain large and small values from the task-scaled embedding vector êi (Atanasova et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 39,
      "context" : "As the attention scores pertain to the word in context (Tutek and Snajder, 2020), we also expect the score sxi to pertain to the word without the contextualised information.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "by removing the token from the input sequence instead of only removing its corresponding attention weight) as we consider the scores from importance metrics to pertain to the corresponding input token following related work (Arras et al., 2016, 2017; Nguyen, 2018; Vashishth et al., 2019; Grimsley et al., 2020; Atanasova et al., 2020).",
      "startOffset" : 224,
      "endOffset" : 335
    }, {
      "referenceID" : 40,
      "context" : "by removing the token from the input sequence instead of only removing its corresponding attention weight) as we consider the scores from importance metrics to pertain to the corresponding input token following related work (Arras et al., 2016, 2017; Nguyen, 2018; Vashishth et al., 2019; Grimsley et al., 2020; Atanasova et al., 2020).",
      "startOffset" : 224,
      "endOffset" : 335
    }, {
      "referenceID" : 2,
      "context" : "by removing the token from the input sequence instead of only removing its corresponding attention weight) as we consider the scores from importance metrics to pertain to the corresponding input token following related work (Arras et al., 2016, 2017; Nguyen, 2018; Vashishth et al., 2019; Grimsley et al., 2020; Atanasova et al., 2020).",
      "startOffset" : 224,
      "endOffset" : 335
    }, {
      "referenceID" : 34,
      "context" : "We use five datasets for text classification following Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al.",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : "Tweets (Sarker et al., 2015); (iv) AG News;9 and (v) MIMIC Anemia (Johnson et al.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : ", 2015); (iv) AG News;9 and (v) MIMIC Anemia (Johnson et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "A prerequisite of interpretability is to obtain robust explanations without sacrificing predictive performance (Lipton, 2016).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "input importance metrics without TaSc which are strong baselines for explainability (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "input importance metrics without TaSc which are strong baselines for explainability (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : "Word Omission (WO) (Robnik-Šikonja and Kononenko, 2008; Nguyen, 2018): Ranking input words by computing the difference between the probabilities of the predicted class when including a word i and omitting it: WOi “ ppŷ|xq  ́ ppŷ|xzxiq",
      "startOffset" : 19,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Word Omission (WO) (Robnik-Šikonja and Kononenko, 2008; Nguyen, 2018): Ranking input words by computing the difference between the probabilities of the predicted class when including a word i and omitting it: WOi “ ppŷ|xq  ́ ppŷ|xzxiq",
      "startOffset" : 19,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "InputXGrad (x∇x) (Kindermans et al., 2016; Atanasova et al., 2020): Ranking words by multiplying the gradient of the input by the input with respect to the predicted class: ∇xi “ Bŷ Bxi",
      "startOffset" : 17,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "InputXGrad (x∇x) (Kindermans et al., 2016; Atanasova et al., 2020): Ranking words by multiplying the gradient of the input by the input with respect to the predicted class: ∇xi “ Bŷ Bxi",
      "startOffset" : 17,
      "endOffset" : 66
    }, {
      "referenceID" : 37,
      "context" : "Integrated Gradients (IG) (Sundararajan et al., 2017): Ranking words by computing the integral of the gradients taken along a straight path from a baseline input to the original input, where the baseline is the zero embedding vector.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "This demonstrates the efficacy of TaSc in providing more faithful attention-based explanations than strong baselines without TaSc (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "This demonstrates the efficacy of TaSc in providing more faithful attention-based explanations than strong baselines without TaSc (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "We do not compare with LIME (Ribeiro et al., 2016) because WO and the gradient-based approaches outperform it (Nguyen, 2018; Atanasova et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : ", 2016) because WO and the gradient-based approaches outperform it (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : ", 2016) because WO and the gradient-based approaches outperform it (Nguyen, 2018; Atanasova et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 105
    } ],
    "year" : 2021,
    "abstractText" : "Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attentionbased explanations compared to three widelyused interpretability techniques.1",
    "creator" : "pdftk 2.02 - www.pdftk.com"
  }
}