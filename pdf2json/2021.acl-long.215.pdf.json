{
  "name" : "2021.acl-long.215.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making",
    "authors" : [ "Zijun Yao", "Chengjiang Li", "Tiansi Dong", "Xin Lv", "Jifan Yu", "Lei Hou", "Juanzi Li", "Yichi Zhang", "Zelin Dai", "Jiawei Han" ],
    "emails" : [ "{yaozj20@mails.,", "houlei@}tsinghua.edu.cn", "dongt@bit.uni-bonn.de", "(houlei@tsinghua.edu.cn)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2770–2781\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2770"
    }, {
      "heading" : "1 Introduction",
      "text" : "Entity Matching (EM) aims at identifying whether two records from different sources refer to the same real-world entity. This is a fundamental research task in knowledge graph integration (Dong et al., 2014; Daniel et al., 2020; Christophides et al., 2015; Christen, 2012) and text mining (Zhao et al., 2014). In real applications, it is not easy to decide whether two records with ad hoc linguistic descriptions refer to the same entity. In Figure 1, e2 and e3 refer to the same publication, while e1 refers to a different\n∗ Corresponding to L.Hou (houlei@tsinghua.edu.cn)\none. Venues of e2 and e3 have different expressions; Authors of e3 is misplaced in its Title field.\nEarly works include feature engineering (Wang et al., 2011) and rule matching (Singh et al., 2017; Fan et al., 2009). Recently, the robustness of Entity Matching has been improved by deep learning models, such as distributed representation based models (Ebraheem et al., 2018), attention based models (Mudgal et al., 2018; Fu et al., 2019, 2020), and pre-trained language model based models (Li et al., 2020). Nevertheless, these modern neural EM models suffer from two limitations as follows. Low-Resource Training. Supervised deep learning EM relies on large amounts of labeled training data, which is extremely costly in reality. Attempts have been made to leverage external data via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021) and pre-trained language model based methods (Li et al., 2020). Other attempts have also been made to improve labeling efficiency via active learning (Nafa et al., 2020) and crowdsourcing techniques (Gokhale et al., 2014; Wang et al., 2012). However, external information may introduce noises, and active learning and crowdsourcing still require additional labeling work. Lack of Interpretability. It is important to know why two entity records are equivalent (Chen et al., 2020), however, deep learning EM lacks inter-\npretability. Though some neural EM models analyze the model behavior from the perspective of attention (Nie et al., 2019), attention is not a safe indicator for interpretability (Serrano and Smith, 2019). Deep learning EM also fails to generate interpretable EM rules in the sense that they meet the criteria by domain experts (Fan et al., 2009).\nTo address the two limitations, we propose a novel EM framework to decouple feature representation from matching decision. Our framework consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Matching Decision for low-resource settings. HIF is robust for feature representation from noisy inputs, and KAT carries out interpretable decisions for entity matching.\nIn particular, HIF learns from unlabeled data a mapping function, which converts each noisy attribute value of entity into a vector representation. This is carried out by a novel self-supervised attention training schema to leverage the redundancy within attribute values and propagate information across attributes.\nKAT Matching Decision learns KAT using decision tree classification. After training, KAT carries out entity matching as a task of the classification tree. For each entity pair, it first computes multiple similarity scores for each attribute using a family of metrics and concatenates them into a comparison feature vector. This classification tree can be directly interpreted as EM rules that share a similar structure with EM rules derived by domain experts.\nOur EM method achieves at least SOTA performance on 9 datasets (3 structured datasets, 3 dirty datasets, and 3 industrial datasets) under various extremely low-resource settings. Moreover, when the number of labeled training data decreases from 60% to 10%, our method achieves almost the same performance. In contrast, other methods’ performances decrease greatly.\nThe rest of the paper is structured as follows. Section 2 defines the EM task; Section 3 presents HIF and KAT-Induction in details; Section 4 reports a series of comparative experiments that show the robustness and the interpretability our methods in low-resource settings; Section 5 lists some related works; Section 6 concludes the paper."
    }, {
      "heading" : "2 Task Definitions",
      "text" : "Entity Matching. Let T1 and T2 be two collections of entity records with m aligned attributes {A1, · · · Am}. We denote the ith attribute val-\nues of entity record e as e[Ai]. Entity matching aims to determine whether e1 and e2 refer to the same real-world object or not. Formally, entity matching is viewed as a binary classification function T1 × T2 → {True, False} that takes (e1, e2) ∈ T1 × T2 as input, and outputs True (False), if e1 and e2 are matched (not matched).\nCurrent neural EM approaches simultaneously embed entities in low-dimensional vector spaces and obtain entity matching by computations on their vector representations. Supervised deep learning EM relies on large amounts of labeled training data, which is time-consuming and needs costly manual efforts. Large unlabelled data also contain entity feature information useful for EM, yet has not been fully exploited by the existing neural EM methods. In this paper, we aim at decoupling feature representation from matching decision. Our novel EM model consists of two sub-tasks: learning feature representation from unlabeled data and EM decision making.\nFeature Representation from Noisy Inputs. Entity records are gathered from different sources with three typical noises in attribute values: misplacing, missing, or synonym. Misplacing means that attribute value of Ai drifts to Aj(i 6= j); missing means that attribute values are empty; synonym means that attribute values with the same meaning have different literal forms. Our first task is to fusion noisy heterogeneous information in a selfsupervised manner with unlabelled data.\nInterpretable EM. Domain experts have some valuable specifications on EM rules as follow: (1) an EM rule is an if-then rule of feature comparison; (2) it only selects a part of key attributes from all entity attributes for decision making; (3) feature comparison is limited to a number of similarity constraints, such as =, ≈ (Fan et al., 2009; Singh et al., 2017). Our second task is to realize an interpretable EM decision process by comparing feature representation per attribute by utilizing a fixed number of quantitative similarity metrics and then training a decision tree using a limited amount of labeled data. Our interpretable EM decision making will ease the collaboration with domain experts."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce (1) a neural model, Heterogeneous Information Fusion (HIF), for the task of feature representation, and (2) a decision\ntree, Key Attribute Tree (KAT), for the task of interpretable EM. Figure 2 illustrates the overall workflow of our method. The following subsections dive into details of the two tasks and propose a novel training scheme for low resource settings by exploiting unlabelled entity records."
    }, {
      "heading" : "3.1 HIF for Entity Attribute Embedding",
      "text" : "HIF : T → Rm×d is a function that maps entity records into vector representations. An attribute value e[Ai] of a record e is mapped to a d dimensional vector, written as HIF(e)[Ai] ∈ Rd. HIF treats attribute values as strings of words and performs word embedding (EMB), word information aggregation (AGG), and attribute information propagation (PROP) successively.\nWord Embedding (EMB). Word embedding is a pre-train language model that contains features learned from a large corpus. We convert numerical and encoded attribute values into strings of digits or alphabets. For Chinese attribute values, we do word-segmentation using pkuseg (Luo et al., 2019). Then, we mark the beginning and the end of an attribute value with two special tokens, namely 〈BEG〉 and 〈END〉. Finally, we pad each attribute value with 〈PAD〉 so that they are represented in the same length l. The representation after padding\nis illustrated as below:\n(〈BEG〉, w1, w2, · · · 〈END〉, 〈PAD〉, · · · , 〈PAD〉)︸ ︷︷ ︸ length = l\nLet W be the set of words, each word w ∈ W is mapped into a vector, and each attribute value is mapped into a matrix. Formally, EMB : WN → RN×de maps N words into an N × de matrix by executing a look-up-table operation. N is the dictionary size. In particular, we have EMB(e)[Ai] ∈ Rl×de , in which de is the dimension of word embedding vectors. It is worth noting that 〈PAD〉 is embedded to zero vector to ensure that it does not interfere with other non-padding words in the following step.\nWord Information Aggregation (AGG). Summing up the l word embeddings as the embedding of an attribute value will neglect the importance weight among the l words. We leverage a more flexible framework, which aggregates word information by weighted pooling. The weighting coefficients αi for different words are extracted by multiplying its embedding vector with a learnable, and attribute-specific vector ai ∈ Rde×1. Subscript i implies that αi and ai are associated with the ith attribute Ai. The weighting coefficients are normalized by Softmax function among words. Finally, we enable a non-linear transformation (e.g.,\nReLU) during information aggregation with parameters Wai ∈ Rde×da . Formally, AGG maps each attribute value of entity record e into a da dimensional vector AGG(EMB(e)[Ai]) ∈ Rda as below:\nAGG(EMB(e)[Ai]) = ReLU (αi EMB(e)[Ai]Wai)\nαi = Softmax(EMB(e)[Ai] ai)> ∈ R1×l\nAttribute Information Propagation (PROP). The mechanism of attribute information propagation is the key component for noise reduction and representation unification. This mechanism is inspired by the observation that missing attribute values often appear in other attributes (e.g., Venue and Conference in Figure 1, Mudgal et al. (2018) also reported the misplacing issue).\nWe use “Scaled Dot-Product Attention” (Ashish et al., 2017) to propagate information among different attribute values. We use parameters Q,K,Vi to convert AGG(EMB(e)[Ai]) into query, key, and value vectors, respectively (Notice that only Vi is attribute-specific). A ∈ Rm×m is the attention matrix. Aij denotes the attention coefficients from the ith attribute to the jth attribute:\nAij = Softmax ( qi · kj√ m ) qi = AGG(EMB(e)[Ai])Q kj = AGG(EMB(e)[Ai])K vi = AGG(EMB(e)[Ai])Vi\nRecord notation e is omitted in vectors q,k,v for brevity. To keep the identity information, each attribute value after attribute information propagation is represented by the concatenation of the context and the value vector:\nPROP(AGG(e))[Ai] = ReLU vi ∥∥∥∥∥∥ ∑ j 6=i Aijvj  HIF outputs with Multiple Layer Perceptron (MLP). The whole process can be summarized as follows:\nHIF(e) = MLP◦PROP◦AGG◦EMB(e) ∈ Rm×d\nAfter HIF, each attribute Ai of an entity record e has a feature embedding HIF(e)[Ai]."
    }, {
      "heading" : "3.2 KAT for Matching Decision",
      "text" : "KAT Matching Decision consists of two steps: comparison feature computation (CFC) and decision making with KAT. CFC computes similarity score\nfor each paired attribute features by utilizing a family of well-selected metrics, and concatenate these similarity scores into a vector (comparison feature). KAT takes comparison feature as inputs, and perform entity matching with a decision tree.\nComparison Feature Computing (CFC). Given a record pair (e1, e2), CFC implements a function that maps (e1, e2) to a vector of similarity scores CFC(e1, e2). The similarity score CFC(e1, e2) is a concatenation of a similarity vector between paired attribute values (i.e., e1[Ai], e2[Ai]) and a similarity vector between their vector embeddings (i.e., HIF(e1)[Ai],HIF(e2)[Ai]).\nTo compare paired attribute values, we follow Konda et al. (2016) and classify attribute values into 6 categories, according to the type and the length, each with a set of comparison metrics for similarity measurement, such as Jaccard similarity, Levenshtein similarity, Monge-Elkan similarity, etc. More details are presented in Table 1.\nFor attribute value embeddings, we choose three metrics: the cosine similarity, the L2 distance, and the Pearson coefficiency. In this way, we convert entity record pair into similarity score vector of attributes. Each dimension indicates the similarity degree of one attribute from a certain perspective.\nKAT Induction. In the matching decision, we take CFC(e1, e2) as input, and output binary classification results. We propose Key Attribute Tree, a decision tree, to make the matching decision based on key attribute heuristic, in the sense that some attributes are more important than others for EM. For example, we can decide whether two records of research articles are the same by only checking their Title and Venue without examining their Conference. Focusing only on key attributes not only saves computations, but also introduces interpretability that has two-folded meanings: (1) each dimension of CFC(e1, e2) is a candidate feature matching which can be interpreted as a component of an EM rule; (2) the decision tree learned by KAT can be converted into EM rules that follow the same heuristics as the EM rules made by domain experts (Fan et al., 2009)."
    }, {
      "heading" : "3.3 Model Training",
      "text" : "HIF and KAT Induction are trained separately.\nHIF Training. We design a self-supervised training method for HIF to learn from unlabeled data.\nOur strategy is to let the HIF model predict manually masked attribute values. We first represent attribute values, as strings of words, by Weighted Bag Of Words (WBOW) vectors, whose dimensions represent word frequencies. Then, we manually corrupt a small portion of entity records in T1 ∪ T2 by randomly replacing (mask) their attribute values with an empty string, which forms a new table T ′. HIF takes T ′ as input and uses another MLP to predict the WBOW of masked attribute values. HIF is trained by minimizing the Cross-Entropy between the prediction and the ground-truth WBOW:\nmin HIF\nCrossEntropy ( MLP(HIF(T ′)),WBOW ) KAT Induction Training. KAT is trained with a normal decision tree algorithm. We constrain its depth, in part to maintain the interpretability of transformed EM rules. We use xgboost (Tianqi and Carlos, 2016) and ID3 algorithm (Quinlan, 1986) in the experiments. To preserve interpretability, the booster number of xgboost is set to 1, which means it only learns one decision tree. For (e1, e2, T rue) ∈ D, KAT takes CFC(e1, e2) as input, and True as the target classification output."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1.1 Datasets",
      "text" : "In order to evaluate our model comprehensively, we collect multi-scaled datasets ranging from English corpus and Chinese corpus, including Structured datasets, Dirty datasets, and Real datasets. Struc-\ntured and Dirty datasets are benchmark datasets1 released in (Mudgal et al., 2018). The Real datasets are sampled from Taobao—one of the biggest Ecommerce platform in China, a portion of which are manually labeled to indicate whether they are the same entity or not. The real datasets have notably more attributes than the structured or dirty datasets.\nStatistics of these datasets are listed in Table 2. We focus on setting of low resource EM and use Rate% of labelled data as training set. The validation set uses the last 20% labeled pairs, and the rest pairs in the middle are the test set. This splitting is\n1http://pages.cs.wisc.edu/˜anhai/ data1/deepmatcher_data/\ndifferent from the sufficient resource EM (Mudgal et al., 2018; Konda et al., 2016) where up to 60% pairs are used in the training set. For I-A1, I-A2, and Phone, we use 10% labeled pairs as training data, because some of the baselines will crash, if the training data is too small.\nWe remove trivial entity pairs from the Real datasets, as Structured and Dirty datasets have been released. For Real datasets, we remove matching pairs with large Jaccard similarity (0.32 for Phone, 0.36 for others) and non-matching pairs with small Jaccard similarity (0.3 for Phone, 0.332 for others)."
    }, {
      "heading" : "4.1.2 Baselines",
      "text" : "We implement 3 variants of our methods with different KAT Induction algorithms. HIF+KATID3 and HIF+KATXGB inducts KAT with ID3 algorithm and xgboost respectively constraining maximum depth to 3. HIF+DT inducts KAT with ID3 algorithm with no constraints on the tree depth. We include reproducibility details in Appendix B.\nWe compare our methods with three SOTA EM methods, among which two are publicly available end-to-end neural methods, and one is feature engineering based method.\n1. DeepMatcher (Mudgal et al., 2018) (DM) is a general deep-learning based EM framework with multiple variants—RNN DM-RNN, Attention DM-ATT, and Hybrid DM-HYB— depending on what building block it chooses to construct2.\n2. HierMatcher (Fu et al., 2020) is also an endto-end neural EM method that compare entity records at the word level3.\n3. Magellan (Konda et al., 2016) integrates both automatic feature engineering for EM and classifiers. Decision tree is used as the classifier of Magellan in our experiments.\nFor ablation analysis, we replace a single component of our model with a new model as follows: HIF+LN replaces KAT with a linear classifier; HIF+LR replaces KAT with a logistic regression classifier; HIF-ALONE removes comparison metrics of attribute values (yellow segment of comparison features in Figure 2). We\n2https://github.com/anhaidgroup/ deepmatcher\n3https://github.com/cipnlu/ EntityMatcher\nalso do ablation analysis for HIF-ALONE as follows: HIF-WBOW replaces outputs of HIF with d-dimensional WBOW vectors using PCA. HIFEMB replaces the outputs of HIF with the mean pooling of word embeddings."
    }, {
      "heading" : "4.1.3 Evaluation Metrics",
      "text" : "We use F1 score as the evaluation metric. Experiment results are listed in Table 3 and Table 5. All the reported results are averaged over 10 runs with different random seeds."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "General Results. We evaluate the performance of our model against 3 SOTA models under low resource settings, where only 1% or 10% of the total amount of labeled pairs are used for training (See Table 2). Comparative experiment results on the 9 datasets are listed in Table 3.\nOur decoupled framework achieves SOTA EM results on all the nine datasets, and demonstrates significant performance on Dirty datasets, with a boosting of 4.3%, 14.7%, and 8.4% in terms of F1 score on I-A2, D-A2, D-S2, compared to the best performance of baselines on their corresponding datasets. Our methods also outperforms all baselines on Structured and two Real datasets (the same as Magellan on Toner). The out-performance on Real datasets is marginal because attribute values in Real datasets are quite standard, which means that our model does not have many chances to fix noisy attribute values. Still, our methods achieve a high F1 score (≥ 94.9%) in Real datasets. These results indicate out methods are both effective under low resource settings and robust to noisy data.\nEffectiveness to Low Resource Settings We reduce the training rate from 60% to 10% to see whether our method is sensitive to the number of labeled record pairs as training resources. Experimental results are shown in Figure 3. HIF+KAT (red line) achieves a stable performance as the number of labeled record pairs decreases, while the F1 score of DeepMatcher and HierMatcher decrease simultaneously. Besides, our methods continuously outperform DeepMatcher and HierMatcher, ranging from low resource setting to sufficient resource setting. These results indicate that by exploring unlabelled data, HIF alleviates the reliance on labeled record pairs.\nEffectiveness to Noisy Heterogeneous Data. We manually aggravate the quality of datasets by randomly dropping p% of attribute values (p% ranges from 0% to 40%), and see to what degree the feature representations delivered by HIF will affect the EM decision matching. From left to right, columns of subgraphs in Figure 3 demonstrates results with increasing dropping rate. On the I-A1 dataset, the influence of dropping rate is marginal to HIF+KAT , whose F1 score fluctuates around 95%. In contrast, F1 scores of both DeepMatcher and HierMatcher will decrease if more attribute values are dropped. On the Phone dataset, the dropping rate’s influence is not severe to HIF+KAT, especially when the training rate is low. These results show that HIF is efficient in recovering noisy heterogeneous inputs."
    }, {
      "heading" : "4.3 Case Study for Interpretablity",
      "text" : "The interpretability of our model means that the process of decision making of KAT can be easily transformed into EM rules whose structure is recommended by domain experts. Figure 4 illustrates a tree decision process of KAT that determines whether two records denote the same publication in the D-A1 (DBLP and ACM) datasets. Each path from the root to a leaf node of the tree structure can be converted into an EM rule as follows:\nRule 1: if L2 (HIF(e1),HIF(e2)) [Authors] ≥ 10.21 then e1, e2 are not a match; Rule 2: if L2 (HIF(e1),HIF(e2)) [Authors] < 10.21 ∧ L2 (HIF(e1),HIF(e2)) [Title] < 0.73\nthen e1, e2 are a match; Rule 3: if L2 (HIF(e1),HIF(e2)) [Authors] < 10.21\n∧ L2 (HIF(e1),HIF(e2)) [Title] ≥ 0.73 then e1, e2 are not a match\nThey can be further read as descriptive rules: Rule 1: if two records have different authors, they will be different publications. Rule 2: if two records have similar authors and similar titles, they will be the same publication. Rule 3: if two records have similar authors and dissimilar titles, they will not be the same publication. The soundness of such rules can be examined by our experience.\nImportant features of KAT are as follows: (1) KAT is conditioned on attribute comparison; (2) KAT only selects a few key attributes to compare features. In our example, there are 4 attributes, Author, Title, Venue and Conference in D-A1 dataset,\nKAT only selects Title and Author for EM decision making. The transformed rules meet the specifications of manually designed EM rules of domain experts (Fan et al., 2009; Singh et al., 2017). This kind of interpretability will ease the collaboration with domain experts, and increase the trustworthiness, compared with uninterpretable end-to-end Deep learning EM models."
    }, {
      "heading" : "4.4 Discussions",
      "text" : "Ablation Analysis. Experiment results for ablation models are listed in Table 3. On the one hand, HIF+LN and HIF+LR generally outperforms DeepMatcher and HierMatcher on 7 datasets with on-par performance on 2 Real datasets. This indicates that HIF and CFC together extract better comparison features than end-to-end neural methods under low resource settings. On the other hand, HIF+LN and HIF+LR are weaker than the tree induction classifier, suggesting that KAT is more reliable.\nCompared with HIF-KATID3, Magellan, and HIFALONE, HIF-KATID3 achieves the highest performance, indicating that comparison on both attribute value embeddings and the original attribute values are important. Compared with HIF-ALONE, HIFWBOW, and HIF-EMB, HIF-ALONE outperforms HIF-WBOW and HIF-EMB on the Dirty datasets, showing the positive effects of its information reconstruction.\nFinally, comparing HIF+KAT with HIF+DT, we find that HIF+KAT has better performances than HIF+DT on most of the datasets, except for (I-A2 and Phone). This shows that non-key attributes\nmay disturb decision making.\nEfficiency. Table 4 shows the running times of our methods and of the two neural baselines. Our methods are highly efficient for inference, because our methods are highly parallel and are memorysaving. For example, on Phone datasets our methods can inference in a single batch, while HierMatcher can only run in a batch size of 4 with 24GiB RAM. The training efficiency of our method is comparable with baselines, because when the training data is small enough, baseline models may finish one epoch training with only few batches.\nSufficient Resource EM. Table 5 shows the results with sufficient training data following the split method of Mudgal et al. (2018); Fu et al. (2020). Our method outperforms other methods on 4 datasets, and slightly fall behind on 5 datasets."
    }, {
      "heading" : "5 Related Works",
      "text" : "The way of extracting comparison features falls into two categories: monotonic and non-monotonic. Monotonic features are (negatively) proportional similarities between attribute values. They can\nbe calculated by symbolic rules, such as Jaccard similarity, Levenshtein similarity (Fan et al., 2009; Wang et al., 2011; Konda et al., 2016; Singh et al., 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al., 2019; Ebraheem et al., 2018; Fu et al., 2019). Non-monotonic features are hidden representations of end-to-end neural networks, such as Softmax or Sigmoid based similarity scores (Fu et al., 2020), attention based scores (Nie et al., 2019), or simply embedding based features (Mudgal et al., 2018; Li et al., 2020).\nEM with limited resources has recently intrigued research interest (Thirumuruganathan et al., 2018; Kasai et al., 2019). Existing explorations seek solution from leveraging external data to improving annotation efficiency. External data can be aggregated via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021), or via pre-training language models (Li et al., 2020). For better annotations, researchers tried active learning (Kasai et al., 2019; Nafa et al., 2020; Sarawagi and Bhamidipaty, 2002; Arasu et al., 2010), or crowd sourcing techniques (Wang et al., 2012; Gokhale et al., 2014).\nThe interpretability of neural models will contribute to the trust and the safety. It has become one of the central issues in machine learning. Chen et al. (2020) examines interpretability in EM risk analysis. There are also attempts to explain from the perspective of attention coefficients (Mudgal et al., 2018; Nie et al., 2019)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present a decoupled framework for interpretable entity matching. It is robust to both noisy heterogeneous input and the scale of training resources. Experiments show that our method can be converted to interpretable rules, which can be inspect by domain experts and make EM process more reliable.\nIn the future, it is intriguing to explore more efficient ways to explore unlabeled data, such as levering connections among entities, or combine with pre-trained language models. It is also valuable to explore how to use our heterogeneous information fusion module to boost other EM methods, such as injecting HIF representation as supplementary information into end-to-end models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by Science and Technology Innovation 2030 - New Generation of Artificial Intelligence Project (2020AAA0106501), the NSFC Key Project (U1736204), the NSFC Youth Project (62006136), the Federal Ministry of Education and Research of Germany as part of the competence center for machine learning ML2R (01IS18038C), and the grant from Alibaba Inc.\nEthical Considerations\nIntended Use. The reported technique is intended for reliable entity matching in large scale E-commercial products, where attribute values are mostly heterogeneous descriptive sentences. The ‘low resource’ feature is intended to avoid heavy labor force. The ‘interpretability’ is intended to risk control in entity matching.\nMisuse Potential. As matching/alignment technique, our method may be misused in matching private information.\nFailure Modes. Our method provides a promising way to have domain experts check the generated rules, thus reducing the failure risk.\nEnergy and Carbon Costs. The efficiency test in Section 4.4 shows that our method costs less computations and is more energy saving than existing methods."
    }, {
      "heading" : "A More Experimental Results",
      "text" : "Table 6 in the main text only shows the F1 measure of the all the methods. Here, we supplement the experimental results with precision (P = TPTP+FP ), recall (R = TPTP+FN ) on the 9 datasets for more comprehensive analysis. Experimental results are listed in Table 6. Our methods achieve the highest precision and recall on most of the datasets."
    }, {
      "heading" : "B Reproducibility Details",
      "text" : "Each epoch of HIF training is evenly divided into 3 batches. The Title attribute values were padded to l = 64, and the other attribute values are all padded to l = 32. We modify the padding size on large\ndatasets, so that our the experiments can be conducted on a single GPU. Chinese datasets are embedded with Tencent Embedding (Song et al., 2018) and English datasets use fastText embeddings (Bojanowski et al., 2017). Multi-head mechanism is used in the attention module. The embedding size de for Chinese is 300, and for English is 200. AGG converts embedding into da dimensional vectors, where da = 100. PROP further outputs with a 2- layer MLP with dimension size d = 64. The query vector and the key vector in the attention layer of PROP are 16 dimensional vectors. During training, attribute values are masked at a probability p = 0.4. The Adam optimizer (Kingma and Ba,\n2015) is used for HIF . Training rate and L2 weight decay are 0.01 and 10−5.\nKATXGB is implemented using xgboost 0.9 with objective function binary: logistic. KATID3 is implemented using scikit-learn 0.24. HIF is implemented with PyTorch 1.4.0 in Python 3.7.6. The comparison feature metrics in Table 1 are implemented with py-entitymatching 0.4.0. We also use Numpy 1.19.2 for matrix calculation. All the experiments are evaluated on a single NVIDIA 3090 GPU with 24GiB GRAM."
    } ],
    "references" : [ {
      "title" : "On active learning of record matching packages",
      "author" : [ "Arvind Arasu", "Michaela Götz", "Raghav Kaushik." ],
      "venue" : "SIGMOD’10.",
      "citeRegEx" : "Arasu et al\\.,? 2010",
      "shortCiteRegEx" : "Arasu et al\\.",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Vaswani Ashish", "Shazeer Noam", "Parmar Niki", "Uszkoreit Jakob", "Jones lion", "Aidan N. Gomez", "Kaiser Lukasz", "Polosukhin Illia." ],
      "venue" : "NIPS’17.",
      "citeRegEx" : "Ashish et al\\.,? 2017",
      "shortCiteRegEx" : "Ashish et al\\.",
      "year" : 2017
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "TACL, 5.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards interpretable and learnable risk analysis for entity resolution",
      "author" : [ "Zhaoqiang Chen", "Qun Chen", "Boyi Hou", "Zhanhuai Li", "Guoliang Li." ],
      "venue" : "SIGMOD’20.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Data matching: concepts and techniques for record linkage, entity resolution, and duplicate detection",
      "author" : [ "P Christen." ],
      "venue" : "Springer: Data-centric systems and applications.",
      "citeRegEx" : "Christen.,? 2012",
      "shortCiteRegEx" : "Christen.",
      "year" : 2012
    }, {
      "title" : "Entity resolution in the web of data",
      "author" : [ "Vassilis Christophides", "Vasilis Efthymiou", "Kostas Stefanidis." ],
      "venue" : "Synthesis Lectures on the Semantic Web.",
      "citeRegEx" : "Christophides et al\\.,? 2015",
      "shortCiteRegEx" : "Christophides et al\\.",
      "year" : 2015
    }, {
      "title" : "EAGER: embedding-assisted entity resolution for knowledge graphs",
      "author" : [ "Obraczka Daniel", "Schuchart Jonathan", "Rahm Erhard." ],
      "venue" : "ICDM’20.",
      "citeRegEx" : "Daniel et al\\.,? 2020",
      "shortCiteRegEx" : "Daniel et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang." ],
      "venue" : "SIGKDD’14.",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of tuples for entity resolution",
      "author" : [ "Muhammad Ebraheem", "Saravanan Thirumuruganathan", "Shafiq Joty", "Mourad Ouzzani", "Nan Tang." ],
      "venue" : "VLDB’18.",
      "citeRegEx" : "Ebraheem et al\\.,? 2018",
      "shortCiteRegEx" : "Ebraheem et al\\.",
      "year" : 2018
    }, {
      "title" : "Reasoning about record matching rules",
      "author" : [ "Wenfei Fan", "Xibei Jia", "Jianzhong Li", "Shuai Ma." ],
      "venue" : "VLDB’09.",
      "citeRegEx" : "Fan et al\\.,? 2009",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2009
    }, {
      "title" : "Hierarchical matching network for heterogeneous entity resolution",
      "author" : [ "Cheng Fu", "Xianpei Han", "Jiaming He", "Le Sun." ],
      "venue" : "IJCAI’20.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end multiperspective matching for entity resolution",
      "author" : [ "Cheng Fu", "Xianpei Han", "Le Sun", "Bo Chen", "Wei Zhang", "Suhui Wu", "Hao Kong." ],
      "venue" : "IJCAI’19.",
      "citeRegEx" : "Fu et al\\.,? 2019",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2019
    }, {
      "title" : "Corleone: hands-off crowdsourcing for entity matching",
      "author" : [ "Chaitanya Gokhale", "Sanjib Das", "AnHai Doan", "Jeffrey F. Naughton", "Narasimhan Rampalli", "Jude W. Shavlik", "Xiaojin Zhu." ],
      "venue" : "SIGMOD’14.",
      "citeRegEx" : "Gokhale et al\\.,? 2014",
      "shortCiteRegEx" : "Gokhale et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-resource deep entity resolution with transfer and active learning",
      "author" : [ "Jungo Kasai", "Kun Qian", "Sairam Gurajada", "Yunyao Li", "Lucian Popa." ],
      "venue" : "ACL’19.",
      "citeRegEx" : "Kasai et al\\.,? 2019",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR’15.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Deep entity matching with pre-trained language models",
      "author" : [ "Yuliang Li", "Jinfeng Li", "Yoshihiko Suhara", "AnHai Doan", "Wang-Chiew Tan." ],
      "venue" : "VLDB’20.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge transfer for entity resolution with siamese neural networks",
      "author" : [ "Michael Loster", "Ioannis Koumarelas", "Felix Naumann." ],
      "venue" : "Journal of Data and Information Quality (JDIQ).",
      "citeRegEx" : "Loster et al\\.,? 2021",
      "shortCiteRegEx" : "Loster et al\\.",
      "year" : 2021
    }, {
      "title" : "Pkuseg: A toolkit for multi-domain chinese word segmentation",
      "author" : [ "Ruixuan Luo", "Jingjing Xu", "Yi Zhang", "Xuancheng Ren", "Xu Sun." ],
      "venue" : "CoRR, abs/1906.11455.",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep learning for entity matching: A design space exploration",
      "author" : [ "Sidharth Mudgal", "Han Li", "Theodoros Rekatsinas", "AnHai Doan", "Youngchoon Park", "Ganesh Krishnan", "Rohit Deep", "Esteban Arcaute", "Vijay Raghavendra." ],
      "venue" : "SIGMOD’18.",
      "citeRegEx" : "Mudgal et al\\.,? 2018",
      "shortCiteRegEx" : "Mudgal et al\\.",
      "year" : 2018
    }, {
      "title" : "Active deep learning on entity resolution by risk sampling",
      "author" : [ "Youcef Nafa", "Qun Chen", "Zhaoqiang Chen", "Xingyu Lu", "Haiyang He", "Tianyi Duan", "Zhanhuai Li." ],
      "venue" : "CoRR, abs/2012.12960.",
      "citeRegEx" : "Nafa et al\\.,? 2020",
      "shortCiteRegEx" : "Nafa et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep sequence-to-sequence entity matching for heterogeneous entity resolution",
      "author" : [ "Hao Nie", "Xianpei Han", "Ben He", "Le Sun", "Bo Chen", "Wei Zhang", "Suhui Wu", "Hao Kong." ],
      "venue" : "CIKM’19.",
      "citeRegEx" : "Nie et al\\.,? 2019",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "Induction of decision trees",
      "author" : [ "J. Ross Quinlan." ],
      "venue" : "Machine learning, 1(1):81–106.",
      "citeRegEx" : "Quinlan.,? 1986",
      "shortCiteRegEx" : "Quinlan.",
      "year" : 1986
    }, {
      "title" : "Interactive deduplication using active learning",
      "author" : [ "Sunita Sarawagi", "Anuradha Bhamidipaty." ],
      "venue" : "SIGKDD’02.",
      "citeRegEx" : "Sarawagi and Bhamidipaty.,? 2002",
      "shortCiteRegEx" : "Sarawagi and Bhamidipaty.",
      "year" : 2002
    }, {
      "title" : "Is attention interpretable? In ACL’19",
      "author" : [ "Sofia Serrano", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Serrano and Smith.,? \\Q2019\\E",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Generating concise entity matching rules",
      "author" : [ "Rohit Singh", "Venkata Vamsikrishna Meduri", "Ahmed K. Elmagarmid", "Samuel Madden", "Paolo Papotti", "JorgeArnulfo Quiané-Ruiz", "Armando Solar-Lezama", "Nan Tang." ],
      "venue" : "SIGMOD’17.",
      "citeRegEx" : "Singh et al\\.,? 2017",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2017
    }, {
      "title" : "Directional skip-gram: Explicitly distinguishing left and right context for word embeddings",
      "author" : [ "Yan Song", "Shuming Shi", "Jing Li", "Haisong Zhang." ],
      "venue" : "NAACL’18.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Reuse and adaptation for entity resolution through transfer learning",
      "author" : [ "Saravanan Thirumuruganathan", "Shameem A Puthiya Parambath", "Mourad Ouzzani", "Nan Tang", "Shafiq Joty." ],
      "venue" : "CoRR, abs/1809.11084.",
      "citeRegEx" : "Thirumuruganathan et al\\.,? 2018",
      "shortCiteRegEx" : "Thirumuruganathan et al\\.",
      "year" : 2018
    }, {
      "title" : "Xgboost: A scalable tree boosting system",
      "author" : [ "Chen Tianqi", "Guestrin Carlos." ],
      "venue" : "SIGKDD’16.",
      "citeRegEx" : "Tianqi and Carlos.,? 2016",
      "shortCiteRegEx" : "Tianqi and Carlos.",
      "year" : 2016
    }, {
      "title" : "Crowder: Crowdsourcing entity resolution",
      "author" : [ "Jiannan Wang", "Tim Kraska", "Michael J. Franklin", "Jianhua Feng." ],
      "venue" : "VLDB’12.",
      "citeRegEx" : "Wang et al\\.,? 2012",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2012
    }, {
      "title" : "Entity matching: How similar is similar",
      "author" : [ "Jiannan Wang", "Guoliang Li", "Jeffrey Xu Yu", "Jianhua Feng." ],
      "venue" : "VLDB.",
      "citeRegEx" : "Wang et al\\.,? 2011",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Auto-em: End-to-end fuzzy entity-matching using pre-trained deep models and transfer learning",
      "author" : [ "Chen Zhao", "Yeye He." ],
      "venue" : "WWW’19.",
      "citeRegEx" : "Zhao and He.,? 2019",
      "shortCiteRegEx" : "Zhao and He.",
      "year" : 2019
    }, {
      "title" : "Group based self training for ecommerce product record linkage",
      "author" : [ "Wayne Xin Zhao", "Yuexin Wu", "Hongfei Yan", "Xiaoming Li." ],
      "venue" : "COLING’14.",
      "citeRegEx" : "Zhao et al\\.,? 2014",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This is a fundamental research task in knowledge graph integration (Dong et al., 2014; Daniel et al., 2020; Christophides et al., 2015; Christen, 2012) and text mining (Zhao et al.",
      "startOffset" : 67,
      "endOffset" : 151
    }, {
      "referenceID" : 6,
      "context" : "This is a fundamental research task in knowledge graph integration (Dong et al., 2014; Daniel et al., 2020; Christophides et al., 2015; Christen, 2012) and text mining (Zhao et al.",
      "startOffset" : 67,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "This is a fundamental research task in knowledge graph integration (Dong et al., 2014; Daniel et al., 2020; Christophides et al., 2015; Christen, 2012) and text mining (Zhao et al.",
      "startOffset" : 67,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "This is a fundamental research task in knowledge graph integration (Dong et al., 2014; Daniel et al., 2020; Christophides et al., 2015; Christen, 2012) and text mining (Zhao et al.",
      "startOffset" : 67,
      "endOffset" : 151
    }, {
      "referenceID" : 31,
      "context" : ", 2015; Christen, 2012) and text mining (Zhao et al., 2014).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 29,
      "context" : "Early works include feature engineering (Wang et al., 2011) and rule matching (Singh et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Recently, the robustness of Entity Matching has been improved by deep learning models, such as distributed representation based models (Ebraheem et al., 2018), attention based models (Mudgal et al.",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : ", 2018), attention based models (Mudgal et al., 2018; Fu et al., 2019, 2020), and pre-trained language model based models (Li et al.",
      "startOffset" : 32,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : ", 2019, 2020), and pre-trained language model based models (Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "Attempts have been made to leverage external data via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021) and pre-trained language model based methods (Li et al.",
      "startOffset" : 72,
      "endOffset" : 164
    }, {
      "referenceID" : 26,
      "context" : "Attempts have been made to leverage external data via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021) and pre-trained language model based methods (Li et al.",
      "startOffset" : 72,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "Attempts have been made to leverage external data via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021) and pre-trained language model based methods (Li et al.",
      "startOffset" : 72,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "Attempts have been made to leverage external data via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021) and pre-trained language model based methods (Li et al.",
      "startOffset" : 72,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : ", 2021) and pre-trained language model based methods (Li et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "Other attempts have also been made to improve labeling efficiency via active learning (Nafa et al., 2020) and crowdsourcing techniques (Gokhale et al.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and crowdsourcing techniques (Gokhale et al., 2014; Wang et al., 2012).",
      "startOffset" : 37,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : ", 2020) and crowdsourcing techniques (Gokhale et al., 2014; Wang et al., 2012).",
      "startOffset" : 37,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "It is important to know why two entity records are equivalent (Chen et al., 2020), however, deep learning EM lacks inter-",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Though some neural EM models analyze the model behavior from the perspective of attention (Nie et al., 2019), attention is not a safe indicator for interpretability (Serrano and Smith, 2019).",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : ", 2019), attention is not a safe indicator for interpretability (Serrano and Smith, 2019).",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Deep learning EM also fails to generate interpretable EM rules in the sense that they meet the criteria by domain experts (Fan et al., 2009).",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "Domain experts have some valuable specifications on EM rules as follow: (1) an EM rule is an if-then rule of feature comparison; (2) it only selects a part of key attributes from all entity attributes for decision making; (3) feature comparison is limited to a number of similarity constraints, such as =, ≈ (Fan et al., 2009; Singh et al., 2017).",
      "startOffset" : 308,
      "endOffset" : 346
    }, {
      "referenceID" : 24,
      "context" : "Domain experts have some valuable specifications on EM rules as follow: (1) an EM rule is an if-then rule of feature comparison; (2) it only selects a part of key attributes from all entity attributes for decision making; (3) feature comparison is limited to a number of similarity constraints, such as =, ≈ (Fan et al., 2009; Singh et al., 2017).",
      "startOffset" : 308,
      "endOffset" : 346
    }, {
      "referenceID" : 17,
      "context" : "For Chinese attribute values, we do word-segmentation using pkuseg (Luo et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "We use “Scaled Dot-Product Attention” (Ashish et al., 2017) to propagate information among different attribute values.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "Focusing only on key attributes not only saves computations, but also introduces interpretability that has two-folded meanings: (1) each dimension of CFC(e1, e2) is a candidate feature matching which can be interpreted as a component of an EM rule; (2) the decision tree learned by KAT can be converted into EM rules that follow the same heuristics as the EM rules made by domain experts (Fan et al., 2009).",
      "startOffset" : 388,
      "endOffset" : 406
    }, {
      "referenceID" : 27,
      "context" : "We use xgboost (Tianqi and Carlos, 2016) and ID3 algorithm (Quinlan, 1986) in the experiments.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "We use xgboost (Tianqi and Carlos, 2016) and ID3 algorithm (Quinlan, 1986) in the experiments.",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "tured and Dirty datasets are benchmark datasets1 released in (Mudgal et al., 2018).",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "2775 different from the sufficient resource EM (Mudgal et al., 2018; Konda et al., 2016) where up to 60% pairs are used in the training set.",
      "startOffset" : 47,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "DeepMatcher (Mudgal et al., 2018) (DM) is a general deep-learning based EM framework with multiple variants—RNN DM-RNN, Attention DM-ATT, and Hybrid DM-HYB— depending on what building block it chooses to construct2.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "HierMatcher (Fu et al., 2020) is also an endto-end neural EM method that compare entity records at the word level3.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "The transformed rules meet the specifications of manually designed EM rules of domain experts (Fan et al., 2009; Singh et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "The transformed rules meet the specifications of manually designed EM rules of domain experts (Fan et al., 2009; Singh et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "2778 be calculated by symbolic rules, such as Jaccard similarity, Levenshtein similarity (Fan et al., 2009; Wang et al., 2011; Konda et al., 2016; Singh et al., 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al.",
      "startOffset" : 89,
      "endOffset" : 166
    }, {
      "referenceID" : 29,
      "context" : "2778 be calculated by symbolic rules, such as Jaccard similarity, Levenshtein similarity (Fan et al., 2009; Wang et al., 2011; Konda et al., 2016; Singh et al., 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al.",
      "startOffset" : 89,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : "2778 be calculated by symbolic rules, such as Jaccard similarity, Levenshtein similarity (Fan et al., 2009; Wang et al., 2011; Konda et al., 2016; Singh et al., 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al.",
      "startOffset" : 89,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : ", 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al., 2019; Ebraheem et al., 2018; Fu et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : ", 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al., 2019; Ebraheem et al., 2018; Fu et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : ", 2017), or learned from differentiable comparison operations, such as subtracting, point-wise multiplication (Fu et al., 2019; Ebraheem et al., 2018; Fu et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 167
    }, {
      "referenceID" : 10,
      "context" : "Non-monotonic features are hidden representations of end-to-end neural networks, such as Softmax or Sigmoid based similarity scores (Fu et al., 2020), attention based scores (Nie et al.",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : ", 2020), attention based scores (Nie et al., 2019), or simply embedding based features (Mudgal et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : ", 2019), or simply embedding based features (Mudgal et al., 2018; Li et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2019), or simply embedding based features (Mudgal et al., 2018; Li et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "EM with limited resources has recently intrigued research interest (Thirumuruganathan et al., 2018; Kasai et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "EM with limited resources has recently intrigued research interest (Thirumuruganathan et al., 2018; Kasai et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 119
    }, {
      "referenceID" : 30,
      "context" : "External data can be aggregated via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021), or via pre-training language models (Li et al.",
      "startOffset" : 54,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "External data can be aggregated via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021), or via pre-training language models (Li et al.",
      "startOffset" : 54,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "External data can be aggregated via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021), or via pre-training language models (Li et al.",
      "startOffset" : 54,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "External data can be aggregated via transfer learning (Zhao and He, 2019; Thirumuruganathan et al., 2018; Kasai et al., 2019; Loster et al., 2021), or via pre-training language models (Li et al.",
      "startOffset" : 54,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : ", 2021), or via pre-training language models (Li et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "For better annotations, researchers tried active learning (Kasai et al., 2019; Nafa et al., 2020; Sarawagi and Bhamidipaty, 2002; Arasu et al., 2010), or crowd sourcing techniques (Wang et al.",
      "startOffset" : 58,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "For better annotations, researchers tried active learning (Kasai et al., 2019; Nafa et al., 2020; Sarawagi and Bhamidipaty, 2002; Arasu et al., 2010), or crowd sourcing techniques (Wang et al.",
      "startOffset" : 58,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "For better annotations, researchers tried active learning (Kasai et al., 2019; Nafa et al., 2020; Sarawagi and Bhamidipaty, 2002; Arasu et al., 2010), or crowd sourcing techniques (Wang et al.",
      "startOffset" : 58,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "For better annotations, researchers tried active learning (Kasai et al., 2019; Nafa et al., 2020; Sarawagi and Bhamidipaty, 2002; Arasu et al., 2010), or crowd sourcing techniques (Wang et al.",
      "startOffset" : 58,
      "endOffset" : 149
    }, {
      "referenceID" : 28,
      "context" : ", 2010), or crowd sourcing techniques (Wang et al., 2012; Gokhale et al., 2014).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : ", 2010), or crowd sourcing techniques (Wang et al., 2012; Gokhale et al., 2014).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "There are also attempts to explain from the perspective of attention coefficients (Mudgal et al., 2018; Nie et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "There are also attempts to explain from the perspective of attention coefficients (Mudgal et al., 2018; Nie et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 121
    } ],
    "year" : 2021,
    "abstractText" : "Entity Matching (EM) aims at recognizing entity records that denote the same real-world object. Neural EM models learn vector representation of entity descriptions and match entities end-to-end. Though robust, these methods require many annotated resources for training, and lack of interpretability. In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching decision. Using self-supervised learning and mask mechanism in pre-trained language modeling, HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data. Using a set of comparison features and a limited amount of annotated data, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts. Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases. Our codes and datasets can be obtained from https:// github.com/THU-KEG/HIF-KAT.",
    "creator" : "LaTeX with hyperref"
  }
}