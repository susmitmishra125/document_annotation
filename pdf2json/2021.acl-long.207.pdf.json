{
  "name" : "2021.acl-long.207.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-View Cross-Lingual Structured Prediction with Minimum Supervision",
    "authors" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu" ],
    "emails" : [ "huzch@shanghaitech.edu.cn,", "tukw@shanghaitech.edu.cn,", "yongjiang.jy@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2661"
    }, {
      "heading" : "1 Introduction",
      "text" : "Structured prediction is the task of mapping input sentences to structured outputs. It is a fundamental task in natural language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020).\n∗Corresponding authors. ‡Work was done when Zechuan Hu was interning at Alibaba DAMO Academy.\nTo achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general.\nCross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word order, capitalization, and script style. However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agić, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models.\nIn this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models.\nIn many real applications, we are generally easy to obtain a small number of target labeled data. These small amounts of data can reflect the diverse strength and weakness of different source models. Concretely, the (small-size) labeled data can be utilized to learn the aggregation strategy of multiple source models or train a new task-specific model in the target language. Both the aggregation model and target task-specific model can map the inputs to the structured outputs but there exists a tradeoff. The aggregation model generally has strong cross-lingual ability since source models are firstly well trained1, but has lower flexibility since source models are usually frozen. Instead, the target taskspecific model tends to be more flexible and has strong capacity but has poor performance since the model is easily over-fitted on the small training sample.\nInspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs. We propose a novel multi-view framework to achieve a good trade-off between the two views. To capture the diverse strength and weakness of multiple source models, we propose three approaches to obtain the aggregated source view from language/sentence/sub-structure level in a coarse-to-fine manner. By encouraging two views to influence each other, the proposed framework can dynamically learn the confidence level of multiple source models in three coarse-to-fine granularity and make the best use of the small number of labeled data, and make both views improved during training. Benefited from the multi-view framework, our proposed approaches can leverage plenty of target unlabeled data to capture the useful target language information (Wu et al., 2020).\nThe contributions of this work are:\n1. We propose to leverage a small number of target labeled data to better aggregate multiple source models.\n2. Our approach contains three novel coarse-tofine approaches to aggregate multiple source models (section 2.2).\n1Following Wu et al. (2020), source models are previously trained on their corresponding labeled training set and frozen during training.\n3. We propose a novel multi-view learning framework (section 2.3).\n4. By utilizing both the label & unlabeled dataset, our approach improves two views simultaneously (section 2.4).\nWe extensively experiment on three structured prediction tasks, which are named entity recognition (NER), part-of-speech tagging (POS), and dependency parsing. Our proposed approaches outperform several state-of-the-art approaches."
    }, {
      "heading" : "2 Methodology",
      "text" : "The left part of Figure 1 depicts the proposed general framework. Our framework contains two views, a target view which is a target structured predictor, and an aggregated source view based on multiple pre-trained source models. Both views can map the input sentences to the structured outputs and have diverse statistical properties, and thus can provide complementary information to each other (learned by the consensus component)."
    }, {
      "heading" : "2.1 The Target View",
      "text" : "In the general framework, the target view is a taskspecific model. We leverage the multilingual bert (mBERT) (Devlin et al., 2019) as the sentence encoder. We feed the input sentence x to the mBERT and obtain the contextual internal states h, which are utilized by a task-specific module to produce a structured output y. Specifically, we use a Softmax layer for sequence labeling tasks and a biaffine attention mechanism (Dozat and Manning, 2016) followed by (Wu and Dredze, 2019a) for graph-based tasks like dependency parsing. The conditional probability of the structured output given the input sequence is computed by,\np(y|x) = exp( ∑ u∈y s(h, u))∑\ny′ exp( ∑ u∈y′ s(h, u))\nwhere y′ is the candidate structured outputs, y is the structured outputs and u is the sub-structure of y. Sub-structure is the label of each token for sequence labeling and dependency head for dependency parsing. During training with gold labels, the sequence labeling objective function is the cross entropy between the gold labels and the model’s\nsoft predictions 2,\nLCE = − log p(y∗|x) = − n∑\ni=1\nlog p(y∗i |x)\nwhere y∗ is the gold label sequence. In dependency parsing, we use the biaffine parser (Dozat and Manning, 2016) which is one of the state-of-the-art parsers. Following Wu and Dredze (2019a), we replace the BiLSTM encoder with mBERT. Similar to sequence labeling, the biaffine parser models the dependency head separately for each token. Following Anderson and Gómez-Rodrı́guez (2020), it has two independent distributions, one for head prediction and one for label prediction. The crossentropy loss for dependency head is, LCE(head) = − log p(t∗|x) = − n∑\ni=1\nlog p(h∗i |x)\nwhere h∗i is the gold head for i-th word of the gold tree t∗. Together with the similar cross-entropy\n2This is a common way in the BERT-fintuning setup (Wu and Dredze, 2019a; Wu et al., 2020).\nloss of predicted edge labels, the dependency parsing objective function is LCE = LCE(head) + LCE(label)."
    }, {
      "heading" : "2.2 The Aggregated Source View",
      "text" : "In this section, we take the sequence labeling tasks as an example to introduce our aggregated source view. The source models have the same model structure as the task-specific model of the target view in section 2.1. As presented in figure 1, for a K-source setup, we have K pretrained source models Sk, k ∈ {1, . . . ,K} and the target structured model T. Given a sentence x = {x0, . . . , xn}, where x0 represents the [CLS] token, we feed it to these models and get the internal states {h(1), . . . ,h(K)} and the probability distributions {p(1)s , . . . , p(K)s } over the structured output of K source models Sk, and h(t) and pT of the target model. To aggregate all source models, we propose three novel coarse-to-fine approaches."
    }, {
      "heading" : "2.2.1 Language-level Aggregation",
      "text" : "We simply introduce a trainable probability vector αlang, which is depicted on the bottom right part of\nthe Figure 1. The final output distribution of the aggregated source view can be computed as,\npS(y|x) = K∑ k=1 α (k) lang · p(k)s (y|x)\nWe use superscript to represent the index of vector αlang. Note that we use lowercase s, uppercase S, and uppercase T to differentiate the final outputs of the source model, aggregated source view, and target view respectively. In this approach, the k-th source model has the same weight α(k)lang over all sentences."
    }, {
      "heading" : "2.2.2 Sentence-level Aggregation",
      "text" : "In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1. Firstly, we use the internal states of the [CLS] token as sentence representation. Secondly, h(t)0 from the target model T is used as a query to attend h (k) 0 from the k-th source model Sk to produce the probabilities αsent(x) ∈ RK .\nK0 = [h (1) 0 ; . . . ;h (K) 0 ]\nαsent(x) = Softmax(h (t) 0 WK T 0 )\nwhere K0 is the concatenation of sentence representations from K source models, and W ∈ Rd×d is the bilinear weight matrix. Then the probabilities are utilized to compute the aggregation distribution pS(y|x) as follows,\npS(y|x) = K∑ k=1 α (k) sent(x) · p(k)s (y|x)\nIn sentence-level aggregation approach, k-th source model has the same weight α(k)sent(x) over each substructure of a sentence, but different weights over different sentences and thus can capture the diverse strengths of each source on different sentences."
    }, {
      "heading" : "2.2.3 Sub-structure-level Aggregation",
      "text" : "We further propose a fine-grained aggregation approach on sub-structure level, which is also based on the attention mechanism. As shown in the left part of Figure 1, for token xi in a given sentence x, we use its representation h(t)i as the query to attend the corresponding representation from each source\nmodel. We compute the probabilities αsub(xi) for i-th sub-structure as follows,\nKi = [h (1) i ; . . . ;h (K) i ]\nαsub(xi) = Softmax(h (t) i WK T i )\nThen the aggregation distribution becomes,\npS(y|x) = n∏\ni=1 K∑ k=1 α (k) sub(xi) · p(k)s (yi|x)\nIn this approach, our target model acts as a selector to dynamically assess the multiple source models on sub-structure level."
    }, {
      "heading" : "2.3 Consensus between Two Views",
      "text" : "To achieve a good trade-off between the target view and the aggregation view during training, inspired by Clark et al. (2018), we utilize the KL divergence 3 as the metric to encourage the similarity between the two views. For sequence labeling, the objective is,\nLKL(x) = KL(pS(y|x)||pT (y|x))"
    }, {
      "heading" : "2.4 Overall Training Objective",
      "text" : "In the model training, for the unlabeled sentences, we only calculate the KL-divergence loss LU = LKL. For the labeled sentences, we train the model with two supervised cross-entropy loss in addition to the KL-divergence loss,\nLL = λ1LSCE + λ2LTCE + λ3LKL\nwhere λ1, λ2 and λ3 are the interpolation factors. Finally, we introduce an interpolation µ to balance the labeled and unlabeled sentences and the overall learning objective is L = µLL + (1− µ)LU.\nConnections to KD There are mainly four differences between KD (Wu et al., 2020) and our approach:\n1. Unlike our approach, KD only utilizes the target unlabeled data, from which it cannot well learn the strength and weakness of different source models (see Sec.1 for more discussion.).\n3We also try many metrics of measuring the similarity between two probability distributions, e.g., mean squared error (MSE) (Wu et al., 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best.\n2. KD assigns equal importance to multiple source models, which can be seen as a fixed uniform vector in our language-level aggregation approach.\n3. Besides language-level aggregation, we propose two fine-grained aggregation strategies to dynamically balance the information from source models.\n4. To achieve the previously described goal, our approach has trainable parameters in the aggregation component and our multi-view learning framework can jointly learn the parameters of two views."
    }, {
      "heading" : "2.5 Training and Inference Strategies",
      "text" : "Following previous work on cross-lingual transfer (Rahimi et al., 2019; Wu et al., 2020), the source models are previously trained on their corresponding labeled training data. During training, we freeze the parameters of the pre-trained source models and only update the parameters of calculating weights α in the aggregated source view, and update all parameters of the target view. In every iteration, we randomly sample a batch of data from the labeled dataset and unlabeled dataset according to the interpolation µ. In the experiments, our model can significantly benefit from this training strategy by controlling the ratio of labeled data and unlabeled data. During the inference phase, we have two options to obtain the predictions: utilizing the aggregated source view or the target view. In our experiments, we use the second one as the main result for its simplicity and better performance."
    }, {
      "heading" : "3 Experiments",
      "text" : "We experiment on three structured prediction tasks: NER, POS tagging, and dependency parsing. Following previous work (Rahimi et al., 2019; Wu et al., 2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages. To simulate the low-resources scenario, for each training set in a specific target language, we randomly select fifty sentences 4 with the gold annotations and discard the annotations of the remaining sentences to construct the training set. We\n4We explore the effects of randomness on labeled data in the Appendix C.1 and the results show that our approach is robust to randomness in the selection of labeled data.\nrandomly select six languages from Universal Dependencies Treebanks (v2.2)5 for dependency parsing and POS tagging tasks. We use the datasets from CoNLL 2002 and CoNLL 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for NER tasks. We utilize the base cased multilingual BERT (Devlin et al., 2019) as base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1."
    }, {
      "heading" : "3.1 Compared Baselines",
      "text" : "We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM.\nDT-finetuning We directly fine-tune the taskspecific view on fifty labeled data.\nDT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang)) 6.\nHard-KD The hard knowledge distillation approaches first predict the pseudo labels on target unlabeled training set by using pre-trained source models and then train a new model on the pseudo labeled data (Liu et al., 2017; Rahimi et al., 2019).\n5https://universaldependencies.org/ 6We separately evaluate the source language models on the target test data and choose the best score. Since we don’t know which source model is the best for DT in practice, the DT-Max(lang) results are only for reference.\nWe obtain the pseudo labels in four ways: 1) using DT-mean (hard-KD-mean); 2) using DT-max (hard-KD-max); 3) using DT-vote (hard-KD-vote); 4) concatenating all predictions of source models instead of voting (hard-KD-concat). For fairly comparison, we also concatenate the fifty target labeled data into the pseudo labeled data.\nSoft-KD Instead of leveraging hard predictions of source models in hard-KD, the soft-KD leverages soft probability distribution of source models. The original Soft-KD (Wu et al., 2020) only focuses on zero-shot NER tasks. Instead, we modify their training objective to leverage fifty target labeled data and adapt it to POS tagging and dependency parsing tasks. (Refer to section 2.4 for details.) We re-implement their two proposed approaches: 1) uniformly aggregating multiple source models (KD-avg); 2) aggregating source models by fixed weights pre-trained on source unlabeled data based on language similarity (KD-sim)7.\nUMM The UMM is trained on the concatenation of all source languages labeled data and fifty labeled data of target language.\nBootstrapping Bootstrapping approaches firstly train a UMM and then add the most confident sen-\n7For more details of the two approaches, please refer to the original paper.\ntences of target unlabeled data into the training set every iteration during training. We compare our approaches to Self-Training (Yarowsky, 1995; McClosky et al., 2006) and Tri-Training (Ruder and Plank, 2018).\nWe provide the upper bound results of DT (DTgold). We construct the upper bound using the gold label set in test data by selecting the gold label if any prediction of source models appears in the gold set. Besides, unlike UMM, self-training, tri-training, and KD-sim, our approaches do not require extra resources like source language training data."
    }, {
      "heading" : "3.2 Results",
      "text" : "We report the results in Table 1 for NER and POS tagging, and 2 for dependency parsing.\nCommon Results on All Tasks As shown in Table 1 and 2, our three proposed approaches outperform most of the baselines on all tasks, which demonstrates the effectiveness of the proposed multi-view learning framework. When trained on only fifty labeled data, the task-specific model shows significantly poor results especially on dependency parsing which verifies our intuition that the task-specific model is easily over-fitted and only training the task-specific model is not sufficient. Notably, UMM, self-training, and tri-training\ndo not yield improvements compared to hard-KD*, soft-KD-*, and Ours-*, verifying our motivation that simply concatenating all training data is not sufficient to model the difference between multiple sources. We also observe that our three approaches outperform the two KD approaches consistently, indicating that their simple or heuristic-based aggregation strategies are difficult to assess the diverse quality of source models. It is also worth noticing that with a more fine-grained aggregated source view, the target view has stronger performance, especially for Ours-sub 8. Even though UMM, self-training, tri-training, and soft-KD-sim all utilize source language training data during training, Ours-sub achieves remarkable advantage over these baselines without the extra resources, especially for dependency parsing.\nOther results Although Tri-training achieves the highest score and UAS on De of NER and En of parsing respectively, it is not statistically significant compared to Ours-sub and the gap is very marginal (< 0.1%). For NER task, it is probably due to the difference of the capitalization style between De and other languages on CoNLL NER (Chen et al., 2019), which may lead to the negative transfer problem 9. Besides, the gaps between the\n8This is mainly due to the stronger cross-lingual ability of the aggregated source view. We further analyze this in section 4.1.\n9We speculate that KD-based approaches also suffer from this problem and lead to low results. Our sub-structure-level\nDT-gold and the best transfer approaches suggest the large potential space on multi-source transfer tasks."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Why the Multi-View Framework Works?",
      "text" : "In this section, we study the reason why the proposed framework works. We show the performance of the aggregated source view in Figure 2. It can be seen that with a more fine-grained strategy, the performance of the aggregated source view becomes stronger. It demonstrates the effectiveness of more fine-grained aggregation strategies in the multisource transfer. The only counter case is language and sentence level on NL, and the performance of the target view drops accordingly. Connecting to Table 1, the target view has the same trends. The reason is probably that the stronger aggregated source view can lead to a stronger target view and vice versa, and the framework achieves a good trade-off to make them both improved."
    }, {
      "heading" : "4.2 Ablation Study",
      "text" : "To further understand the proposed framework we investigate the component contributions. We gradually remove some components of our sub-structurelevel model, i.e., LSCE, LTCE and LKL, and evaluate approach is the second-best system in this case, indicating that it can alleviate this problem by better leveraging labeled data to access the confidence level of source models on more fine-grained-level property.\non the NER task. We report the average results of twenty-five runs 10 in Table 3. Without LKL the approach degenerates into supervised training with only fifty labeled data and it leads to the largest drop in performance. It is because the model is easily over-fitted. Though the performance drops without one of LSCE and LTCE, it still outperform KD-* baselines of Table 1. w/o LSCE leads to less drops thanw/o LTCE, which suggest that the labeled data influence more in the target model. Besides, without both cross-entropy loss of labeled data, the approach degenerates into a zero-shot manner and results in inferior performance."
    }, {
      "heading" : "4.3 Different Sizes of Unlabeled Data or Labeled Data",
      "text" : "In this section, we study the impact of the sizes of labeled data and unlabeled data on the target language for the ours-sub model. We randomly select {10, 50, 200, 1000} labeled data and {1000, 2000, 4000,All} unlabeled data. We repeat each experiment five times and report the average results of both two views 11. It can be seen that with more labeled data or unlabeled data, the results both become higher and the labeled data shows higher influence than the unlabeled data. Unlike the aggregated source view, the target view gains significantly larger boosts when the size of\n10We randomly select five different copies of labeled data and run five times for each copy.\n11We only show the De results due to the space limitation. The results of the other three languages can be found in the Appendix C.2.\nunlabeled data or labeled data increases (the aggregation view generally shows comparable or even superior results to the target view with fewer data). This verifies our motivation that there exists a tradeoff between two views. With #0 unlabeled data, the task-specific model is over-fitted when only trained on #200 or less labeled data."
    }, {
      "heading" : "5 Related Work",
      "text" : "Cross-lingual Structured Prediction Comparing to single-source transfer, the multi-source transfer shows superior performance by leveraging multi-source language knowledge (McDonald et al., 2011; Rahimi et al., 2019; Hu et al., 2021). However, the diverse quality of source models sorely hurt the target model. To tackle this challenging problem, Ammar et al. (2016) leverage language embeddings to model language topological similarities. Rahimi et al. (2019) utilize truth inference to obtain the best labeling over multiple unreliable predictors. Hu et al. (2021) models the relations between the predicted labels from the source models and the true labels.Approaches based on the similarity of source and target data are widely studied (Chen et al., 2019; Wu et al., 2020).\nMulti/Cross-view Learning Multi-view learning learns multiple representations for the target data. Tri-training approaches (Zhou and Li, 2005; Ruder and Plank, 2018) leverage voting on three separate models to select confident sentences. Jiang et al. (2019); Cai and Lapata (2020) utilize similarity metrics to regularize source-target language pairs. Multi-view learning can also be utilized in training NER models with different kinds of input components (Wang et al., 2021). Cross-view learning (Clark et al., 2018) is a semi-supervised approach that aims to boost the monolingual model’s performance. It learns only one model with several auxiliary prediction modules which are treated\nas different views. In contrast to it, we focus on the cross-lingual scenario and our two views are a target task-specific model and the aggregation of multiple pre-trained source models.\nContextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages. Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Experimental results show that our approaches achieve state-of-the-art performances on all tasks. Moreover, even compared to approaches with extra resources like source language data, our substructure-level approach still shows significant improvements."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Yuting Zhen for her support in processing datasets and conducting significance tests."
    }, {
      "heading" : "A Examples Mentioned in The Introduction",
      "text" : "Example #1 in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019). We show the example in Table 5. The results show that for a target language, the gap between the score of different source models can be large (> 10%).\nExample #2 The model/language level weights can not well capture the diverse strength and weakness of multiple source models. For example in Table 6 12, none of the three source models predict correctly on the whole sequence, but selecting predictions based on the sub-structure level can obtain the correct label sequence.\n12In this example, three pseudo predictions are from three source models pre-trained on En, De, and Nl training set respectively. The three pre-trained source models are obtained in the same way in Table 5."
    }, {
      "heading" : "B Experimental Details",
      "text" : "B.1 Tasks\nDependency Parsing We randomly select five languages together with the English dataset from Universal Dependencies Treebanks (v2.2) for dependency tasks. The whole datasets are English (En), Catalan (Ca), Finnish (Fi), Indonesian (Id), Hindi (Hi), and Russian (Ru). We do not use syntactic information like gold POS tags as many supervised dependency parsers do since we can’t assume they are accessible in practice especially for lowresource languages. Even though we can obtain pseudo tags by pre-trained POS taggers of highresource language, it may introduce unexpected noises and disturb the experiments.\nNamed Entity Recognition We use the datasets from CoNLL 2002 and CoNLL 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which consist of four languages: En, German (De), Dutch (Nl), and Spanish (Es). Each dataset contains four named entity types: Organization, Person, Location, and Miscellaneous. We use the standard splits with the BIO annotation scheme.\nPOS Tagging For the POS tagging task, we use the same six datasets as the dependency parsing task.\nB.2 Model Configuration We utilize the base cased multilingual BERT 13 (Devlin et al., 2019) which has 12 transformer blocks, 12 attention heads, and 768 hidden units. Before model training, the K source models are pre-trained with the corresponding source language training sets 14.\nEvaluation We select the best hyper-parameters based on the score of the development set on highresources language, which is English in practice, and adopt the hyper-parameters to other languages. This may lead to sub-optimal results for other languages but is more realistic (Artetxe et al., 2020).\nB.3 Hyper-parameters we select hyper-parameters based on the performance on the English development set and apply them to other target languages. We search the best learning rate for the mBERT model of all the approaches in the range of {2e−5, 3e−5, 5e−5}, and set it to 2e−5 for its best performance. We list the important hyper-parameters as follows.\nLearning Rate for The Top Layer The top layer’s learning rate is generally larger than that of mBERT. We search the best learning rate in the range of {2e−3, 2e−4, 2e−5}. Interpolations There are three interpolation hyper-parameters in our framework: λ1, λ2, and λ3 in section 2.4 of the main paper. We tune it in the range of {0.5, 1, 3, 10}. Sample Ratio There is a hyper-parameter µ for controlling the ratio of the labeled data and unlabeled data. We tune it in the range of {0.05, 0.1, 0.3, 0.5, 0.7}."
    }, {
      "heading" : "C Additional Analysis",
      "text" : "Linguistic Diversity When all the source languages are different from the target language, the source models generally have poor quality and the target model cannot benefit much from the source models. In this case, the cross-lingual transfer is more difficult. Intuitively, our approaches can dynamically learn the confidence level of multiple source models and still facilitate cross-lingual transfer in this case. We experiment on the dependency\n13https://huggingface.co/ bert-base-multilingual-cased\n14In practice, we can obtain the released pre-trained source models on the open-source community, and thus there is no need to use source language data.\nparsing task over the languages that are drastically different from each other. The sources are English, Mandarin, Arabic, and Vietnamese, and the target is Turkish. In this setting, the source languages are drastically different from the target language. Our results show that Ours-sub (UAS 59.11, LAS 45.02) still outperforms the strongest baseline (KD, UAS 58.69, LAS 44.36).\nC.1 Effects of Random Seeds on Labeled Data\nWe further explore the effects of randomness on labeled data as mentioned in section 3 of the main paper. We randomly select five different copies of fifty labeled data to validate its influence. We compare our sub-structure-level model to KD-* and UMM based approaches on CoNLL02/03 NER task. The results are shown in Table 7. Ours-sub still consistently outperforms the second-best baseline, which demonstrates that our approach is robust to randomness in the selection of labeled data.\nC.2 Different Sizes of unlabeled data or labeled data\nIn Table 8, we provide the whole analysis results mentioned in section 4.3 of the main paper in this section."
    } ],
    "references" : [ {
      "title" : "On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing",
      "author" : [ "Wasi Ahmad", "Zhisong Zhang", "Xuezhe Ma", "Eduard Hovy", "Kai-Wei Chang", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2019 Conference of the North Amer-",
      "citeRegEx" : "Ahmad et al\\.,? 2019",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual sequence labeling with one model",
      "author" : [ "A. Akbik", "T. Bergmann", "Roland Vollgraf." ],
      "venue" : "NLDL 2019, Northern Lights Deep Learning Workshop.",
      "citeRegEx" : "Akbik et al\\.,? 2019",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling neural networks for greener and faster dependency parsing",
      "author" : [ "Mark Anderson", "Carlos Gómez-Rodrı́guez" ],
      "venue" : "In Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into En-",
      "citeRegEx" : "Anderson and Gómez.Rodrı́guez.,? \\Q2020\\E",
      "shortCiteRegEx" : "Anderson and Gómez.Rodrı́guez.",
      "year" : 2020
    }, {
      "title" : "A call for more rigor in unsupervised cross-lingual learning",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Alignment-free cross-lingual semantic role labeling",
      "author" : [ "Rui Cai", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3883–3894, Online. Association for Computational",
      "citeRegEx" : "Cai and Lapata.,? 2020",
      "shortCiteRegEx" : "Cai and Lapata.",
      "year" : 2020
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Association",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Multisource cross-lingual model transfer: Learning what to share",
      "author" : [ "Xilun Chen", "Ahmed Hassan Awadallah", "Hany Hassan", "Wei Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc Le." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Grammatical category disambiguation by statistical optimization",
      "author" : [ "Steven J. DeRose." ],
      "venue" : "Computational Linguistics, 14(1):31–39.",
      "citeRegEx" : "DeRose.,? 1988",
      "shortCiteRegEx" : "DeRose.",
      "year" : 1988
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dozat and Manning.,? 2016",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2016
    }, {
      "title" : "Deep dominance - how to properly compare deep neural models",
      "author" : [ "Rotem Dror", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2773–2785, Florence, Italy. Associa-",
      "citeRegEx" : "Dror et al\\.,? 2019",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual unsupervised sentiment classification with multi-view transfer learning",
      "author" : [ "Hongliang Fei", "Ping Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5759–5771, Online. Association for",
      "citeRegEx" : "Fei and Li.,? 2020",
      "shortCiteRegEx" : "Fei and Li.",
      "year" : 2020
    }, {
      "title" : "Multi-source domain adaptation with mixture of experts",
      "author" : [ "Jiang Guo", "Darsh Shah", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4694–4703, Brussels, Belgium. Association",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th International",
      "citeRegEx" : "Hu et al\\.,? 2020a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "An investigation of potential function designs for neural CRF",
      "author" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Hu et al\\.,? 2020b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Risk Minimization for Zero-shot Sequence Labeling",
      "author" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Hu et al\\.,? 2021",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "A regularization-based framework for bilingual grammar induction",
      "author" : [ "Yong Jiang", "Wenjuan Han", "Kewei Tu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Choosing transfer languages for cross-lingual",
      "author" : [ "Yu-Hsiang Lin", "Chian-Yu Chen", "Jean Lee", "Zirui Li", "Yuyan Zhang", "Mengzhou Xia", "Shruti Rijhwani", "Junxian He", "Zhisong Zhang", "Xuezhe Ma", "Antonios Anastasopoulos", "Patrick Littell", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Heterogeneous supervision for relation extraction: A representation learning approach",
      "author" : [ "Liyuan Liu", "Xiang Ren", "Qi Zhu", "Shi Zhi", "Huan Gui", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lis-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Effective self-training for parsing",
      "author" : [ "David McClosky", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152– 159.",
      "citeRegEx" : "McClosky et al\\.,? 2006",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-source transfer of delexicalized dependency parsers",
      "author" : [ "Ryan McDonald", "Slav Petrov", "Keith Hall." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, Edinburgh, Scotland, UK. Association",
      "citeRegEx" : "McDonald et al\\.,? 2011",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2011
    }, {
      "title" : "How multilingual is multilingual BERT",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette" ],
      "venue" : null,
      "citeRegEx" : "Pires et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Distant supervision from disparate sources for low-resource partof-speech tagging",
      "author" : [ "Barbara Plank", "Željko Agić." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 614–620, Brussels, Belgium. As-",
      "citeRegEx" : "Plank and Agić.,? 2018",
      "shortCiteRegEx" : "Plank and Agić.",
      "year" : 2018
    }, {
      "title" : "Scaling up automatic cross-lingual semantic role annotation",
      "author" : [ "Lonneke van der Plas", "Paola Merlo", "James Henderson." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Plas et al\\.,? 2011",
      "shortCiteRegEx" : "Plas et al\\.",
      "year" : 2011
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "To transfer or not to transfer",
      "author" : [ "Michael T Rosenstein", "Zvika Marx", "Leslie Pack Kaelbling", "Thomas G Dietterich." ],
      "venue" : "In NIPS’05 Workshop, Inductive Transfer: 10 Years Later. Citeseer.",
      "citeRegEx" : "Rosenstein et al\\.,? 2005",
      "shortCiteRegEx" : "Rosenstein et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning to select data for transfer learning with Bayesian optimization",
      "author" : [ "Sebastian Ruder", "Barbara Plank." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372–382, Copenhagen, Denmark. Association",
      "citeRegEx" : "Ruder and Plank.,? 2017",
      "shortCiteRegEx" : "Ruder and Plank.",
      "year" : 2017
    }, {
      "title" : "Strong baselines for neural semi-supervised learning under domain shift",
      "author" : [ "Sebastian Ruder", "Barbara Plank." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044–1054, Mel-",
      "citeRegEx" : "Ruder and Plank.,? 2018",
      "shortCiteRegEx" : "Ruder and Plank.",
      "year" : 2018
    }, {
      "title" : "Linguistically-informed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "In",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-lingual projected expectation regularization for weakly supervised learning",
      "author" : [ "Mengqiu Wang", "Christopher D. Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:55–66.",
      "citeRegEx" : "Wang and Manning.,? 2014",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2014
    }, {
      "title" : "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "the Joint Conference of the 59th Annual Meeting of",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Single-/multi-source cross-lingual NER via teacher-student learning on unlabeled data in target language",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje Karlsson", "Jian-Guang Lou", "Biqing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of bert",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019a",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019b",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120–130, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Wu and Dredze.,? 2020",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "David Yarowsky." ],
      "venue" : "33rd annual meeting of the association for computational linguistics, pages 189–196.",
      "citeRegEx" : "Yarowsky.,? 1995",
      "shortCiteRegEx" : "Yarowsky.",
      "year" : 1995
    }, {
      "title" : "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora",
      "author" : [ "David Yarowsky", "Grace Ngai." ],
      "venue" : "Second Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Yarowsky and Ngai.,? 2001",
      "shortCiteRegEx" : "Yarowsky and Ngai.",
      "year" : 2001
    }, {
      "title" : "Robust multilingual part-of-speech tagging via adversarial training",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Dragomir Radev." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Yasunaga et al\\.,? 2018",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2018
    }, {
      "title" : "Tri-training: Exploiting unlabeled data using three classifiers",
      "author" : [ "Zhi-Hua Zhou", "Ming Li." ],
      "venue" : "IEEE Transactions on knowledge and Data Engineering, 17(11):1529–1541.",
      "citeRegEx" : "Zhou and Li.,? 2005",
      "shortCiteRegEx" : "Zhou and Li.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : ", sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : ", sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : ", sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : ", sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al.",
      "startOffset" : 20,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : ", 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al.",
      "startOffset" : 29,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : ", 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al.",
      "startOffset" : 29,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : ", 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al.",
      "startOffset" : 29,
      "endOffset" : 98
    }, {
      "referenceID" : 34,
      "context" : ", 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020).",
      "startOffset" : 35,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : ", 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020).",
      "startOffset" : 35,
      "endOffset" : 107
    }, {
      "referenceID" : 45,
      "context" : "Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones.",
      "startOffset" : 32,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones.",
      "startOffset" : 32,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones.",
      "startOffset" : 32,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones.",
      "startOffset" : 32,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones.",
      "startOffset" : 32,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.",
      "startOffset" : 135,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : "The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.",
      "startOffset" : 135,
      "endOffset" : 179
    }, {
      "referenceID" : 31,
      "context" : "However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A).",
      "startOffset" : 172,
      "endOffset" : 218
    }, {
      "referenceID" : 30,
      "context" : "However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A).",
      "startOffset" : 172,
      "endOffset" : 218
    }, {
      "referenceID" : 28,
      "context" : "To tackle this challenging problem, most of the previous works do majority voting (Plank and Agić, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "To tackle this challenging problem, most of the previous works do majority voting (Plank and Agić, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al.",
      "startOffset" : 205,
      "endOffset" : 224
    }, {
      "referenceID" : 40,
      "context" : ", 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs.",
      "startOffset" : 55,
      "endOffset" : 113
    }, {
      "referenceID" : 40,
      "context" : "Benefited from the multi-view framework, our proposed approaches can leverage plenty of target unlabeled data to capture the useful target language information (Wu et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "We leverage the multilingual bert (mBERT) (Devlin et al., 2019) as the sentence encoder.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "Specifically, we use a Softmax layer for sequence labeling tasks and a biaffine attention mechanism (Dozat and Manning, 2016) followed by (Wu and Dredze, 2019a) for graph-based tasks like dependency parsing.",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 41,
      "context" : "Specifically, we use a Softmax layer for sequence labeling tasks and a biaffine attention mechanism (Dozat and Manning, 2016) followed by (Wu and Dredze, 2019a) for graph-based tasks like dependency parsing.",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "In dependency parsing, we use the biaffine parser (Dozat and Manning, 2016) which is one of the state-of-the-art parsers.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 41,
      "context" : "This is a common way in the BERT-fintuning setup (Wu and Dredze, 2019a; Wu et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 88
    }, {
      "referenceID" : 40,
      "context" : "This is a common way in the BERT-fintuning setup (Wu and Dredze, 2019a; Wu et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1.",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : "In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1.",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 40,
      "context" : "Connections to KD There are mainly four differences between KD (Wu et al., 2020) and our approach:",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 40,
      "context" : ", mean squared error (MSE) (Wu et al., 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : ", 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "Following previous work on cross-lingual transfer (Rahimi et al., 2019; Wu et al., 2020), the source models are previously trained on their corresponding labeled training data.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 40,
      "context" : "Following previous work on cross-lingual transfer (Rahimi et al., 2019; Wu et al., 2020), the source models are previously trained on their corresponding labeled training data.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 30,
      "context" : "Following previous work (Rahimi et al., 2019; Wu et al., 2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages.",
      "startOffset" : 24,
      "endOffset" : 62
    }, {
      "referenceID" : 40,
      "context" : "Following previous work (Rahimi et al., 2019; Wu et al., 2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages.",
      "startOffset" : 24,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "We utilize the base cased multilingual BERT (Devlin et al., 2019) as base model for all approaches.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al.",
      "startOffset" : 320,
      "endOffset" : 338
    }, {
      "referenceID" : 15,
      "context" : ", 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al.",
      "startOffset" : 47,
      "endOffset" : 85
    }, {
      "referenceID" : 40,
      "context" : ", 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al.",
      "startOffset" : 47,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : ", 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al.",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : ", 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al.",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 44,
      "context" : ", 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM.",
      "startOffset" : 38,
      "endOffset" : 119
    }, {
      "referenceID" : 47,
      "context" : ", 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM.",
      "startOffset" : 38,
      "endOffset" : 119
    }, {
      "referenceID" : 25,
      "context" : ", 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM.",
      "startOffset" : 38,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : ", 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM.",
      "startOffset" : 38,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "Hard-KD The hard knowledge distillation approaches first predict the pseudo labels on target unlabeled training set by using pre-trained source models and then train a new model on the pseudo labeled data (Liu et al., 2017; Rahimi et al., 2019).",
      "startOffset" : 205,
      "endOffset" : 244
    }, {
      "referenceID" : 30,
      "context" : "Hard-KD The hard knowledge distillation approaches first predict the pseudo labels on target unlabeled training set by using pre-trained source models and then train a new model on the pseudo labeled data (Liu et al., 2017; Rahimi et al., 2019).",
      "startOffset" : 205,
      "endOffset" : 244
    }, {
      "referenceID" : 12,
      "context" : "We compare the best score of our approaches and the best score of the baselines by leveraging almost stochastic dominance (ASD) test (Dror et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 40,
      "context" : "The original Soft-KD (Wu et al., 2020) only focuses on zero-shot NER tasks.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 44,
      "context" : "We compare our approaches to Self-Training (Yarowsky, 1995; McClosky et al., 2006) and Tri-Training (Ruder and Plank, 2018).",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "We compare our approaches to Self-Training (Yarowsky, 1995; McClosky et al., 2006) and Tri-Training (Ruder and Plank, 2018).",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "For NER task, it is probably due to the difference of the capitalization style between De and other languages on CoNLL NER (Chen et al., 2019), which may lead to the negative transfer problem 9.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "Cross-lingual Structured Prediction Comparing to single-source transfer, the multi-source transfer shows superior performance by leveraging multi-source language knowledge (McDonald et al., 2011; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 172,
      "endOffset" : 233
    }, {
      "referenceID" : 30,
      "context" : "Cross-lingual Structured Prediction Comparing to single-source transfer, the multi-source transfer shows superior performance by leveraging multi-source language knowledge (McDonald et al., 2011; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 172,
      "endOffset" : 233
    }, {
      "referenceID" : 18,
      "context" : "Cross-lingual Structured Prediction Comparing to single-source transfer, the multi-source transfer shows superior performance by leveraging multi-source language knowledge (McDonald et al., 2011; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 172,
      "endOffset" : 233
    }, {
      "referenceID" : 6,
      "context" : "Approaches based on the similarity of source and target data are widely studied (Chen et al., 2019; Wu et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 40,
      "context" : "Approaches based on the similarity of source and target data are widely studied (Chen et al., 2019; Wu et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 47,
      "context" : "Tri-training approaches (Zhou and Li, 2005; Ruder and Plank, 2018) leverage voting on three separate models to select confident sentences.",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "Tri-training approaches (Zhou and Li, 2005; Ruder and Plank, 2018) leverage voting on three separate models to select confident sentences.",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 39,
      "context" : "Multi-view learning can also be utilized in training NER models with different kinds of input components (Wang et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "Cross-view learning (Clark et al., 2018) is a semi-supervised approach that aims to boost the monolingual model’s performance.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "Contextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages.",
      "startOffset" : 143,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Contextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages.",
      "startOffset" : 143,
      "endOffset" : 186
    }, {
      "referenceID" : 27,
      "context" : "Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al.",
      "startOffset" : 80,
      "endOffset" : 122
    }, {
      "referenceID" : 42,
      "context" : "Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al.",
      "startOffset" : 80,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : ", 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020).",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 43,
      "context" : ", 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020).",
      "startOffset" : 93,
      "endOffset" : 132
    } ],
    "year" : 2021,
    "abstractText" : "In structured prediction problems, crosslingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.",
    "creator" : "LaTeX with hyperref"
  }
}