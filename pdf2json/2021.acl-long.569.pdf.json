{
  "name" : "2021.acl-long.569.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "UnNatural Language Inference",
    "authors" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Joelle Pineau", "Adina Williams" ],
    "emails" : [ "@{mail.mcgill.ca," ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7329–7346\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7329"
    }, {
      "heading" : "1 Introduction",
      "text" : "Of late, large scale pre-trained Transformer-based (Vaswani et al., 2017) models—such as RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), and GPT-2 and -3 (Radford et al., 2019; Brown et al., 2020)—have exceeded recurrent neural networks’ performance on many NLU tasks (Wang et al., 2018, 2019). Several papers have even suggested that Transformers pretrained on a language modeling (LM) objective can capture syntactic informa-\ntion (Hewitt and Manning, 2019; Jawahar et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), with their self-attention layers being capable of surprisingly effective learning (Rogers et al., 2020). In this work, we question such claims that current models “know syntax”.\nSince there are many ways to investigate “syntax”, we must be clear on what we mean by the term. Knowing the syntax of a sentence means being sensitive to the order of the words in that sentence (among other things). Humans are sensitive to word order, so clearly, “language is not merely a bag of words” (Harris, 1954, p.156). Moreover, it is easier for us to identify or recall words presented in canonical orders than in disordered, ungrammatical sentences; this phenomenon is called the “sentence superiority effect” (Cattell 1886; Scheerer 1981; Toyota 2001; Baddeley et al. 2009; Snell and Grainger 2017, 2019; Wen et al. 2019, i.a.). In our estimation then, if one wants to claim that a model “knows syntax”, then they should minimally show that the model is sensitive to word order (at least\nfor e.g. English or Mandarin Chinese).\nGenerally, knowing the syntax of a sentence is taken to be a prerequisite for understanding what that sentence means (Heim and Kratzer, 1998). Models should have to know the syntax first then, if performing any particular NLU task that genuinely requires a humanlike understanding of meaning (cf. Bender and Koller 2020). Thus, if our models are as good at NLU as our current evaluation methods suggest, we should expect them to be sensitive to word order (see Table 1). We find, based on a suite of permutation metrics, that they are not.\nWe focus here on textual entailment, one of the hallmark tasks used to measure how well models understand language (Condoravdi et al., 2003; Dagan et al., 2005). This task, often also called Natural Language Inference (NLI; Bowman et al. 2015, i.a.), typically consists of two sentences: a premise and a hypothesis. The objective is to predict whether the premise entails the hypothesis, contradicts it, or is neutral with respect to it. We find rampant word order insensitivity in purportedly high performing NLI models. For nearly all premise-hypothesis pairs, there are many permuted examples that fool the models into providing the correct prediction. In case of MNLI, for example, the current state-of-the-art of 90.5% can be increased to 98.7% merely by permuting the word order of test set examples. We even find drastically increased cross-dataset generalization when we reorder words. This is not just a matter of chance—we show that the model output probabilities are significantly different from uniform.\nWe verify our findings with three popular English NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018b) and ANLI (Nie et al., 2020))—and one Chinese one, OCNLI (Hu et al., 2020a). It is thus less likely that our findings result from some quirk of English or a particular tokenization strategy. We also observe the effect for various transformer architectures pre-trained on language modeling (BERT, RoBERTa, DistilBERT), and non-transformers, including a ConvNet, an InferSent model, and a BiLSTM.\nOur contributions are as follows: (i) we propose a suite of metrics (Permutation Acceptance) for measuring model insensitivity to word order (§3), (ii) we construct multiple permuted test datasets for measuring NLI model performance at a large scale (§5), (iii) we show that NLI models focus on words more than word order, but can partially\nreconstruct syntactic information from words alone (§6), (iv) we show the problem persists on out-ofdomain data, (v) we show that humans struggle with UnNatural Language Inference, underscoring the non-humanlikeness of SOTA models (§7), (vi) finally, we explore a simple maximum entropybased method (§8) to encourage models not to accept permuted examples."
    }, {
      "heading" : "2 Related Work",
      "text" : "Researchers in NLP have realized the importance of syntactic structure in neural networks going back to Tabor (1994). An early hand annotation effort on PASCAL RTE (Dagan et al., 2006) suggested that “syntactic information alone was sufficient to make a judgment” for roughly one third of examples (Vanderwende and Dolan, 2005). Anecdotally, large generative language models like GPT-2 or -3 exhibit a seemingly humanlike ability to generate fluent and grammatical text (Goldberg, 2019; Wolf, 2019). However, the jury is still out as to whether transformers genuinely acquire syntax.\nModels appear to have acquired syntax. When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of linguistic operations (Jawahar et al., 2019; Tenney et al., 2019), has appeared. There is also evidence, notably from agreement attraction phenomena (Linzen et al., 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021). Results from other phenomena (Warstadt and Bowman, 2020) such as NPI licensing (Warstadt et al., 2019a) lend additional support. The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.g., Ravfogel et al. 2018; White et al. 2018; Davis and van Schijndel 2020; Chaves 2020; Da Costa and Chaves 2020; Kodner and Gupta 2020).\nModels appear to struggle with syntax. Several works have cast doubt on the extent to which NLI models in particular know syntax (although each work adopts a slightly different idea of what “knowing syntax” entails). For example, McCoy et al. (2019) argued that the knowledge acquired by models trained on NLI (for at least some popular datasets) is actually not as syntactically sophisticated as it might have initially seemed; some transformer models rely mainly on simpler, nonhumanlike heuristics. In general, transformer LM performance has been found to be patchy and variable across linguistic phenomena (Dasgupta et al., 2018; Naik et al., 2018; An et al., 2019; Ravichander et al., 2019; Jeretic et al., 2020). This is especially true for syntactic phenomena (Marvin and Linzen, 2018; Hu et al., 2020b; Gauthier et al., 2020; McCoy et al., 2020; Warstadt et al., 2020), where transformers are, for some phenomena and settings, worse than RNNs (van Schijndel et al., 2019). From another angle, many have explored architectural approaches for increasing a network’s sensitivity to syntactic structure (Chen et al., 2017; Li et al., 2020). Williams et al. (2018a) showed that learning jointly to perform NLI and to parse resulted in parse trees that match no popular syntactic formalisms. Furthermore, models trained explicitly to differentiate acceptable sentences from unacceptable ones (i.e., one of the most common syntactic tests used by linguists) have, to date, come nowhere near human performance (Warstadt et al., 2019b).\nInsensitivity to Perturbation. Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs. Pham et al. (2020) is very nearly a proper subset of our work except for investigating additional tasks (i.e. from the GLUE benchmark of Wang et al. 2018) and performing a by-layer-analysis. Gupta et al. (2021) also relies on the GLUE benchmark, but additionally investigates other types of “destructive” perturbations. Our contribution differs from these works in that we additionally include the following: we (i) outline theoretically-informed predictions for how models should be expected to react to permuted input (we outline a few options), (ii) show that permuting can “flip” an incorrect prediction to a correct one, (iii) show that the problem isn’t specific to Transformers, (iv) show that the problem persists on out of domain data, (v) offer\na suite of flexible metrics, and (vi) analyze why models might be accepting permutations (BLEU and POS-tag neighborhood analysis). Finally, we replicate our findings in another language. While our work (and Pham et al.; Gupta et al.) only permutes data during fine-tuning and/or evaluation, recently Sinha et al. explored the sensitivity during pre-training, and found that models trained on n-gram permuted sentences perform remarkably close to regular MLM pre-training. In the context of generation, Parthasarathi et al. (2021) crafted linguistically relevant perturbations (on the basis of part-of-speech tagging and dependency parsing) to evaluate whether permutation hinders automatic machine translation models. Relatedly, but not for translation, Alleman et al. (2021) investigated a smaller inventory of perturbations with emphasis on phrasal boundaries and the effects of n-gram perturbations on different layers in the network.\nNLI Models are very sensitive to words. NLI models often over-attend to particular words to predict the correct answer (Gururangan et al., 2018; Clark et al., 2019). Wallace et al. (2019) show that some short sequences of non-human-readable text can fool many NLU models, including NLI models trained on SNLI, into predicting a specific label. In fact, Ettinger (2020) observed that for one of three test sets, BERT loses some accuracy in wordperturbed sentences, but that there exists a subset of examples for which BERT’s accuracy remains intact. If performance isn’t affected (or if permutation helps, as we find it does in some cases), it suggests that these state-of-the-art models actually perform somewhat similarly to bag-of-words models (Blei et al., 2003; Mikolov et al., 2013)."
    }, {
      "heading" : "3 Our Approach",
      "text" : "As we mentioned, linguists generally take syntactic structure to be necessary for humans to know what sentences mean. Many also find the NLI task to a very promising approximation of human natural language understanding, in part because it is rooted in the tradition of logical entailment. In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948; Montague, 1970; Chierchia and McConnell-Ginet, 1990; Heim and Kratzer, 1998). That is to say that understanding a sentence is equivalent to knowing the actual conditions of the world under which the sentences would be (judged) true (Wittgenstein, 1922). If grammatical sentences are required for\nsentential inference, as per a truth conditional approach (Montague, 1970), then permuted sentences should be meaningless. Put another way, the meanings of highly permuted sentences (if they exist) are not propositions, and thus those sentences don’t have truth conditions. Only from their truth conditions of sentences can we tell if a sentence entails another. In short, the textual entailment task is technically undefined in our “unnatural” setting.\nSince existing definitions don’t immediately extend to UnNatural Language Inference (UNLI), we outline several hypothetical systematic ways that a model might perform, had it been sensitive to word order. We hypothesize two models that operate on the first principles of NLI, and one that doesn’t. In the first case, Model A deems permuted sentences meaningless (devoid of truth values), as formal semantic theories of human language would predict. Thus, it assigns “neutral” to every permuted example. Next, Model B does not deem permuted sentences meaningless, and attempts to understand them. Humans find understanding permuted sentences difficult (see our human evaluations in §7). Model B could also similarly struggle to decipher the meaning, and just equally sample labels for each example (i.e., assigns equal probability mass to the outcome of each label). Finally, we hypothesize a non-systematic model, Model C, which attempts to treat permuted sentences as though they weren’t permuted at all. This model could operate similarly as bag-of-words (BOW), and thus always assign the same label to the permuted examples as it would to the un-permuted examples. If the model failed to assign the original gold label to the original unpermuted examples, it will also fail to assign the original gold label to its permutations; it will never get higher accuracy on permuted examples than on unpermuted ones.\nWe find in our experiments that the state-of-theart Transformer-based NLI models (as well as preTransformer class of models) do not perform like any of the above hypothetical models. They perform closest to Model C, but are, in some cases, actually able to achieve higher accuracy on permuted examples. To better quantitatively describe this behaviour, we introduce our suite of Permutation Acceptance metrics that enable us to quantify how accepting models are of permuted sentences."
    }, {
      "heading" : "4 Methods",
      "text" : "Constructing the permuted dataset. For a given dataset D having splits Dtrain and Dtest, we first train an NLI model M on Dtrain to achieve comparable accuracy to what was reported in the original papers. We then construct a randomized version of Dtest, which we term as D̂test such that: for each example (pi, hi, yi) ∈ Dtest (where pi and hi are the premise and hypothesis sentences of the example respectively and yi is the gold label), we use a permutation operator F that returns a list (P̂i, Ĥi) of q permuted sentences (p̂i and ĥi), where q is a hyperparameter. F essentially permutes all positions of the words in a given sentence (i.e., either in premise or hypothesis) with the restriction that no words maintain their original position. In our initial setting, we do not explicitly control the placement of the words relative to their original neighbors, but we analyze clumping effects in §5. D̂test now consists of |Dtest|×q examples, with q different permutations of hypothesis and premise for each original test example pair. If a sentence S (e.g., hi) contains w words, then the total number of available permutations of S are (w − 1)!, thus making the output of F a list of ( (w−1)! q ) permutations in this case. For us, the space of possible outputs is larger, since we permute pi and hi separately (and ignore examples for which any |S|≤ 5).\nDefining Permutation Acceptance. The choice of q naturally allows us to analyze a statistical view of the predictability of a model on the permuted sentences. To that end, we define the following notational conventions. Let A be the original accuracy of a given model M on a dataset D, and c be the number of examples in a dataset which are marked as correct according to the standard formulation of accuracy for the original dataset (i.e., they are assigned the ground truth label). Typically A is given by c|Dtest| or c |Ddev | .\nLet PrM (P̂i, Ĥi)cor then be the percentage of q permutations of an example (pi, hi) assigned the ground truth label yi by M :\nPr M\n(P̂i, Ĥi)cor = 1\nq ∑ (p̂j∈P̂i,ĥj∈Ĥi) ((M(p̂j , ĥj) = yi)→ 1)\n(1)\nTo get an overall summary score, we let Ωx be the percentage of examples (pi, hi) ∈ Dtest for which PrM (P̂i, Ĥi)cor exceeds a predetermined threshold 0 < x < 1. Concretely, a given example will count\nas correct according to Ωx if more than x percent of its permutations (P̂i and Ĥi) are assigned yi by the model M . Mathematically,\nΩx = 1 | Dtest | ∑ (pi,hi)∈Dtest ((Pr M (P̂i, Ĥi)cor > x)→ 1).\n(2)\nThere are two specific cases of Ωx that we are most interested in. First, we define Ωmax or the Maximum Accuracy, where x = 1/|Dtest|. In short, Ωmax gives the percentage of examples (pi, hi) ∈ Dtest for which there is at least one permutation (p̂j , ĥj) that model M assigns the gold label yi 1. Second, we define Ωrand, or Random Baseline Accuracy, where x = 1/m or chance probability (for balanced m-way classification, where m = 3 in NLI). This metric is less stringent than Ωmax, as it counts an example if at least one third of its permutations are assigned the gold label (hence provides a lower-bound relaxation). See Figure 1 for a graphical representation of Ωx.\nWe also define Df to be the list of examples originally marked incorrect according to A, but are now deemed correct according Ωmax. Dc is the list of examples originally marked correct according to A. Thus, we should expect Df < Dc for models that have high accuracy. Additionally, we definePc and Pf , as the dataset average percentage of permutations which predicted the gold label, when the\n1Theoretically, Ωmax → 1 if the number of permutations q is large. Thus, in our experiments we set q = 100.\nexamples were originally correct (Dc) and when the examples were originally incorrect (Df ) as per A (hence, flipped) respectively.\nPc = 1 |Dc| |Dc|∑ i=0 M(P̂i, Ĥi)cor (3)\nP f is defined similarly by replacing Dc by Df . Note that for a classic BOW model, Pc = 100 and Pf = 0, because it would rely on the words alone (not their order) to make its classification decision. Since permuting removes no words, BOW models should come to the same decisions for permuted examples as for the originals."
    }, {
      "heading" : "5 Results",
      "text" : "We present results for two types of models: (a) Transformer-based models and (b) NonTransformer Models. In (a), we investigate the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al., 2020) and DistilBERT (Sanh et al., 2020). For (b) we consider several recurrent and convolution based neural networks, such as InferSent (Conneau et al., 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al., 2015). We train all models on MNLI, and evaluate on in-distribution (SNLI and MNLI) and out-of-distribution datasets (ANLI). We independently verify results of (a) using both our fine-tuned model using HuggingFace Transformers (Wolf et al., 2020) and pre-trained checkpoints from FairSeq (Ott et al., 2019) (using PyTorch Model Hub). For (b), we use the InferSent codebase. We sample q = 100 permutations for each example in Dtest, and use 100 seeds for each of those permutations to ensure full reproducibility. We drop examples from test sets where we are unable to compute all unique randomizations, typically these are examples with sentences of length of less than 6 tokens. 2\nModels accept many permuted examples. We find Ωmax is very high for models trained and evaluated on MNLI (in-domain generalization), reaching 98.7% on MNLI dev. and test sets (in RoBERTa, compared to A of 90.6% (Table 2). Recall, human accuracy is approximately 92% on MNLI dev., Nangia and Bowman 2019). This shows that there exists at least one permutation (usually many more)\n2Code, data, and model checkpoints will be available at https://github.com/facebookresearch/unlu.\nModel Eval. Dataset A Ωmax Pc Pf Ωrand\nRoBERTa-Large MNLI m dev 0.906 0.987 0.707 0.383 0.794 MNLI mm dev 0.901 0.987 0.707 0.387 0.790 SNLI dev 0.879 0.988 0.768 0.393 0.826 SNLI test 0.883 0.988 0.760 0.407 0.828 A1* 0.456 0.897 0.392 0.286 0.364 A2* 0.271 0.889 0.465 0.292 0.359 A3* 0.268 0.902 0.480 0.308 0.397\nMean 0.652 0.948 0.611 0.351 0.623\nBART-Large\nMNLI m dev 0.902 0.989 0.689 0.393 0.784 MNLI mm dev 0.900 0.986 0.695 0.399 0.788 SNLI dev 0.886 0.991 0.762 0.363 0.834 SNLI test 0.888 0.990 0.762 0.370 0.836 A1* 0.455 0.894 0.379 0.295 0.374 A2* 0.316 0.887 0.428 0.303 0.397 A3* 0.327 0.931 0.428 0.333 0.424\nMean 0.668 0.953 0.592 0.351 0.634\nDistilBERT\nMNLI m dev 0.800 0.968 0.775 0.343 0.779 MNLI mm dev 0.811 0.968 0.775 0.346 0.786 SNLI dev 0.732 0.956 0.767 0.307 0.731 SNLI test 0.738 0.950 0.770 0.312 0.725 A1* 0.251 0.750 0.511 0.267 0.300 A2* 0.300 0.760 0.619 0.265 0.343 A3* 0.312 0.830 0.559 0.259 0.363\nMean 0.564 0.883 0.682 0.300 0.575\nInferSent\nMNLI m dev 0.658 0.904 0.842 0.359 0.712 MNLI mm dev 0.669 0.905 0.844 0.368 0.723 SNLI dev 0.556 0.820 0.821 0.323 0.587 SNLI test 0.560 0.826 0.824 0.321 0.600 A1* 0.316 0.669 0.425 0.395 0.313 A2* 0.310 0.662 0.689 0.249 0.330 A3* 0.300 0.677 0.675 0.236 0.332\nMean 0.481 0.780 0.731 0.322 0.514\nConvNet\nMNLI m dev 0.631 0.926 0.773 0.340 0.684 MNLI mm dev 0.640 0.926 0.782 0.343 0.694 SNLI dev 0.506 0.819 0.813 0.339 0.597 SNLI test 0.501 0.821 0.809 0.341 0.596 A1* 0.271 0.708 0.648 0.218 0.316 A2* 0.307 0.725 0.703 0.224 0.356 A3* 0.306 0.798 0.688 0.234 0.388\nMean 0.452 0.817 0.745 0.291 0.519\nBiLSTM\nMNLI m dev 0.662 0.925 0.800 0.351 0.711 MNLI mm dev 0.681 0.924 0.809 0.344 0.724 SNLI dev 0.547 0.860 0.762 0.351 0.598 SNLI test 0.552 0.862 0.771 0.363 0.607 A1* 0.262 0.671 0.648 0.271 0.340 A2* 0.297 0.728 0.672 0.209 0.328 A3* 0.304 0.731 0.656 0.219 0.331\nMean 0.472 0.814 0.731 0.301 0.520\nTable 2: Statistics for Transformer-based models trained on MNLI corpus (Williams et al., 2018b). The highest values are bolded (red indicates the model most insensitive to permutation) per metric and per model class (Transformers and non-Transformers). A1*, A2* and A3* refer to the ANLI dev. sets (Nie et al., 2020).\nModel A Ωmax Pc Pf Ωrand RoBERTa-Large 0.784 0.988 0.726 0.339 0.773 InferSent 0.573 0.931 0.771 0.265 0.615 ConvNet 0.407 0.752 0.808 0.199 0.426 BiLSTM 0.566 0.963 0.701 0.271 0.611\nTable 3: Results on evaluation on OCNLI Dev set. All models are trained on OCNLI corpus (Hu et al., 2020a). Bold marks the highest value per metric (red shows the model is insensitive to permutation).\nfor almost all examples in Dtest such that model M predicts the gold label. We also observe high Ωrand at 79.4%, showing that there are many examples for which the models outperform even a random baseline in accepting permuted sentences (see Appendix E for more Ω values.)\nEvaluating out-of-domain generalization with ANLI dataset splits resulted in an Ωmax value that is notably higher than A (89.7% Ωmax for RoBERTa compared to 45.6% A). As a consequence, we encounter many flips, i.e., examples where the model is unable to predict the gold label, but at least one permutation of that example is able to. However, recall this analysis expects us to know the gold label upfront, so this test can be thought of as running a word-order probe test on the model until the model predicts the gold label (or give up by exhausting our set of q permutations). For out-of-domain generalization, Ωrand decreases considerably (36.4% Ωrand on A1), which means fewer permutations are accepted by the model. Next, recall that a classic bag-of-words model would have Pc = 100 and Pf = 0. No model performs strictly like a classic bag of words although they do perform somewhat BOW-like (Pc >> Pf for all test splits, Figure 5). We find this BOW-likeness to be higher for certain non-Transformer models, (InferSent) as they exhibit higher Pc (84.2% for InferSent compared to 70.7% for RoBERTa on MNLI).\nModels are very confident. The phenomenon we observe would be of less concern if the correct label prediction was just an outcome of chance,\nwhich could occur when the entropy of the log probabilities of the model output is high (suggesting uniform probabilities on entailment, neutral and contradiction labels, recall Model B from §3). We first investigate the model probabilities for the Transformer-based models on the permutations that lead to the correct answer in Figure 2. We find overwhelming evidence that model confidences on in-distribution datasets (MNLI, SNLI) are highly skewed, resulting in low entropy, and it varies among different model types. BART proves to be the most skewed Transformer-based model. This skewness is not a property of model capacity, as we observe DistilBERT log probabilities to have similar skewness as RoBERTa (large) model, while exhibiting lower A, Ωmax, and Ωrand.\nFor non-Transformers whose accuracy A is lower, the Ωmax achieved by these models are also predictably lower. We observe roughly the same relative performance in the terms of Ωmax (Figure 5 and Appendix Table 2) and Average entropy (Figure 2). However, while comparing the averaged entropy of the model predictions, it is clear that there is some benefit to being a worse model—nonTransformer models are not as overconfident on randomized sentences as Transformers are. High confidence of Transformer models can be attributed to the overthinking phenomenon commonly observed in deep neural networks (Kaya et al., 2019) and BERT-based models (Zhou et al., 2020).\nSimilar artifacts in Chinese NLU. We extended the experiments to the Original Chinese NLI dataset (Hu et al., 2020a, OCNLI), and reused the pre-trained RoBERTa-Large and InferSent (non-Transformer) models on OCNLI. Our findings are similar to the English results (Table 3), thereby suggesting that the phenomenon is not just an artifact of English text or tokenization.\nOther Results. We investigated the effect of sentence length (which correlates with number of possible permutations; Appendix A), and hypothesisonly randomization (models exhibit similar phenomenon even when only hypothesis is permuted; Appendix C)."
    }, {
      "heading" : "6 Analyzing Syntactic Structure Associated with Tokens",
      "text" : "A natural question to ask following our findings: what is it about particular permutations that leads models to accept them? Since the permutation oper-\nation is drastic and only rarely preserves local word relations, we first investigate whether there exists a relationship between Permutation Acceptance scores and local word order preservation. Concretely, we compare bi-gram word overlap (BLEU2) with the percentage of permutations that are deemed correct (Figure 3).3 Although the probability of a permuted sentence to be predicted correctly does appear to track BLEU-2 score (Figure 3), the percentage of examples which were assigned the gold label by the Transformer-based models is still higher than we would expect from permutations with lower BLEU-2 (66% for the lowest BLEU-2 range of 0 − 0.15), suggesting preserved relative word order alone cannot explain the high permutation acceptance rates.\nThus, we find that local order preservation does correlate with Permutation Acceptance, but it doesn’t fully explain the high Permutation Acceptance scores. We now further ask whether Ω is related to a more abstract measure of local word relations, i.e., part-of-speech (POS) neighborhood.\nMany syntactic formalisms, like Lexical Functional Grammar (Kaplan and Bresnan, 1995; Bresnan et al., 2015, LFG), Head-drive Phrase Structure Grammar (Pollard and Sag, 1994, HPSG) or Lexicalized Tree Adjoining Grammar (Schabes et al., 1988; Abeille, 1990, LTAG), are “lexicalized”, i.e.,\n3We observe, due to our permutation process, the maximum BLEU-3 and BLEU-4 scores are negligibly low (< 0.2 BLEU-3 and < 0.1 BLEU-4), already calling into question the hypothesis that n-grams are the sole explanation for our finding. Because of this, we only compare BLEU-2 scores. Detailed experiments on specially constructed permutations that cover the entire range of BLEU-3 and BLEU-4 is provided in Appendix D.\nindividual words or morphemes bear syntactic features telling us which other words they can combine with. For example, “buy” could be associated with (at least) two lexicalized syntactic structures, one containing two noun phrases (as in Kim bought cheese), and another with three (as in Lee bought Logan cheese). We speculate that our NLI models might accept permuted examples at high rates, because they are (perhaps noisily) reconstructing the original sentence from abstract, word-anchored information about common neighbors.\nTo test this, we POS-tagged Dtrain using 17 Universal Part-of-Speech tags (using spaCy, Honnibal et al. 2020). For each wi ∈ Si, we compute the occurrence probability of POS tags on tokens in the neighborhood of wi. The neighborhood is specified by the radius r (a symmetrical window r tokens from wi ∈ Si to the left and right). We denote this sentence level probability of neighbor POS tags for a word wi as ψr{wi,Si} ∈ R\n17 (see an example in Figure 7 in the Appendix). Sentence-level word POS neighbor scores can be averaged across Dtrain to get a type level score ψr{wi,Dtrain} ∈ R\n17,∀wi ∈ Dtrain. Then, for a sentence Si ∈ Dtest, for each word wi ∈ Si, we compute a POS mini-tree overlap score:\nβk{wi,Si} = 1 k | argmaxkψr{wi,Dtrain}∩\nargmaxkψ r {wi,Si} |\n(4)\nConcretely, βk{wi,Si} computes the overlap of topk POS tags in the neighborhood of a word wi in S with that of the train statistic. If a word has the same mini-tree in a given sentence as it has in the training set, then the overlap would be 1. For a given sentence Si, the aggregate βk{Si} is defined by the average of the overlap scores of all its words: βk{Si} = 1 |Si| ∑ wi∈Si β k {wi,Si}, and we call it a POS minitree signature. We can also compute the POS minitree signature of a permuted sentence Ŝi to have βk{Ŝi}. If the permuted sentence POS signature comes close to that of the true sentence, then their ratio (i.e., βk{Ŝi}/β k {Si}) will be close to 1. Also, since POS signature is computed with respect to the train distribution, a ratio of > 1 indicates that the permuted sentence is closer to the overall train statistic than to the original unpermuted sentence in terms of POS signature. If high overlap with the training distribution correlates with percentage of permutations deemed correct, then our models treat words as if they project syntactic minitrees.\nWe investigate the relationship with percentage of permuted sentences accepted with βk{Ŝi}/β k {Si} in Figure 4. We observe that the POS Tag Minitree hypothesis holds for Transformer-based models, RoBERTa, BART and DistilBERT, where the percentage of accepted pairs increase as the sentences have higher overlap with the un-permuted sentence in terms of POS signature. For non-Transformer models such as InferSent, ConvNet, and BiLSTM models, the POS signature ratio to percentage of correct permutation remains the same or decreases, suggesting that the reasoning process employed by these models does not preserve local abstract syntax structure (i.e., POS neighbor relations)."
    }, {
      "heading" : "7 Human Evaluation",
      "text" : "We expect humans to struggle with UNLI, given our intuitions and the sentence superiority findings (but see Mollica et al. 2020). To test this, we presented two experts in NLI (one a linguist) with permuted sentence pairs to label.4 Concretely, we draw equal number of examples from MNLI Matched dev set (100 examples where RoBERTa predicts the gold label,Dc and 100 examples where it fails to do so, Df ), and then permute these examples using F . The experts were given no additional information (recall that it is common knowledge that NLI is a roughly balanced 3-way classification task). Unbeknownst to the experts, all permuted sentences in the sample were actually accepted by the RoBERTa (large) model (trained on MNLI dataset). We observe that the experts performed\n4Concurrent work by Gupta et al. (2021) found that untrained crowdworkers accept NLI examples that have been subjected to different kinds of perturbations at roughly most frequent class levels—i.e., only 35% of the time.\nEvaluator Accuracy Macro F1 Acc on Dc Acc on Df"
    }, {
      "heading" : "X 0.581 ±0.068 0.454 0.649 ±0.102 0.515 ±0.089",
      "text" : ""
    }, {
      "heading" : "Y 0.378 ±0.064 0.378 0.411 ±0.098 0.349 ±0.087",
      "text" : "much worse than RoBERTa (Table 4), although their accuracy was a bit higher than random. We also find that for both experts, accuracy on permutations from Dc was higher than on Df , which verifies findings that showed high word overlap can give hints about the ground truth label (Dasgupta et al., 2018; Poliak et al., 2018; Gururangan et al., 2018; Naik et al., 2019)."
    }, {
      "heading" : "8 Training by Maximizing Entropy",
      "text" : "We propose an initial attempt to mitigate the effect of correct prediction on permuted examples. As we observe in §5, model entropy on permuted examples is significantly lower than expected. Neural networks tend to output higher confidence than random for even unknown inputs (Gandhi and Lake, 2020), which might be an underlying cause of the high Permutation Acceptance.\nAn ideal model would be ambivalent about randomized ungrammatical sentences. Thus, we train NLI models baking in the principle of mutual exclusivity (Gandhi and Lake, 2020) by maximizing model entropy. Concretely, we fine-tune RoBERTa on MNLI while maximizing the entropy (H) on a subset of n randomized examples ((p̂i, r̂i), for each example (p, h) in MNLI. We modify the loss function as follows:\n(5)\nL = argmin θ ∑ ((p,h),y) y log(p(y|(p, h); θ))\n+ n∑ i=1 H ( y|(p̂i, ĥi); θ ) Using this maximum entropy method (n = 1), we find that the model improves considerably with respect to its robustness to randomized sentences, all while taking no hit to accuracy (Table 5). We observe that no model reaches a Ωmax score close to 0, suggesting further room to explore other methods for decreasing models’ Permutation Acceptance. Similar approaches have also proven useful (Gupta et al., 2021) for other tasks as well."
    }, {
      "heading" : "9 Future Work & Conclusion",
      "text" : "We show that state-of-the-art models do not rely on sentence structure the way we think they should: NLI models (Transformer-based models, RNNs, and ConvNets) are largely insensitive to permutations of word order that corrupt the original syntax. We also show that reordering words can cause models to flip classification labels. We do find that models seem to have learned some syntactic information as is evidenced by a correlation between preservation of abstract POS neighborhood information and rate of acceptance by models, but these results do not discount the high rates of Permutation Acceptance, and require further verification. Coupled with the finding that humans cannot perform UNLI at all well, the high rate of permutation acceptance that we observe leads us to conclude that current models do not yet “know syntax” in the fully systematic and humanlike way we would like them to.\nA few years ago, Manning (2015) encouraged NLP to consider “the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task.” We expand upon this view, and suggest one particular future direction: we should train models not only to do well on clean test data, but also to not to overgeneralize to corrupted input."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to Omar Agha, Dzmitry Bahdanau, Sam Bowman, Hagen Blix, Ryan Cotterell, Emily Dinan, Michal Drozdal, Charlie Lovering, Nikita Nangia, Alicia Parrish, Grusha Prasad, Roy Schwartz, Shagun Sodhani, Anna Szabolsci, Alex Warstadt, Jackie Chi-kit Cheung, Timothy O’Donnell and members of McGill MCQLL lab for many invaluable comments and feedback on early drafts."
    }, {
      "heading" : "A Effect of Length on Permutation Acceptance",
      "text" : "We investigate the effect of length on Permutation Acceptance in Figure 6. We observe that shorter sentences in general have a somewhat higher probability of acceptance for examples which was originally predicted correctly—since shorter sentences have fewer unique permutations. However, for the examples which were originally incorrect, the trend is not present."
    }, {
      "heading" : "B Example of POS Minitree",
      "text" : "In §6, we developed a POS signature for each word in at least one example in a test set, then compare that signature to the distribution of the same word in the training set. Figure 7 provides a snapshot a word “river” from the test set and shows how the POS signature distribution of the word in a particular example match with that of aggregated training statistic. In practice, we select the top k POS tags for the word in the test signature as well as the train, and calculate their overlap. When comparing the model performance with permuted\nsentences, we compute a ratio between the original test overlap score and an overlap score calculated instead from the permuted test. In the Figure 7, ‘river’ would have a POS tag minitree score of 0.75."
    }, {
      "heading" : "C Effect of Hypothesis-Only Randomization",
      "text" : "In recent years, the impact of the hypothesis sentence (Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018) on NLI classification has been a topic of much interest. As we define in §3, logical entailment can only be defined for pairs of propositions. We investigated one effect where we randomize only the hypothesis sentences while keeping the premise intact. Figure 9(a) shows that the Ωmax value is almost the same for the two schemes; randomizing the hypothesis alone also leads the model to accept many permutations."
    }, {
      "heading" : "D Effect of clumped words in random permutations",
      "text" : "Since our original permuted dataset consists of extremely randomized words, we observe very low BLEU-3 (< 0.2) and BLEU-4 scores (< 0.1). To study the effect of overlap across a wider range of permutations, we devised an experiment where we clump certain words together before performing random permutations. Concretely, we clump 25%, 50% and 75% of the words in a sentence and then permute the remaining words and the clumped word as a whole. This type of clumped-permutation allows us to study the full range of BLEU-2/3/4 scores, which we present in Figure 10. As expected, the acceptability of permuted sentences increase linearly with BLEU score overlap."
    }, {
      "heading" : "E Effect of the threshold of Ωx in various test splits",
      "text" : "We defined two variations of Ωx, Ωmax and Ωrand, but theoretically it is possible to define any arbitrary threshold percentage x to evaluate the unnatural language inference mechanisms of different models. In Figure 8 we show the effect of different thresholds, including Ωmax where x = 1/|Dtest| and Ωrand where x = 0.34. We observe for indistribution datasets (top row, MNLI and SNLI splits), in the extreme setting when x = 1.0, there are more than 10% of examples available, and more than 25% in case of InferSent and DistilBERT. For out-of-distribution datasets (bottom row, ANLI splits) we observe a much lower trend, suggesting generalization itself is the bottleneck in permuted sentence understanding."
    }, {
      "heading" : "F Training with permuted examples",
      "text" : "In this section, we hypothesize that if the NLU models are mostly insensitive to word order, then training using permuted examples should also yield the same or comparable accuracy as training using grammatically correct data (i.e., the standard setup). To test this, we train Transformer-based models on top of D̂train, which is computed by applyingF on each example ofDtrain for q = 1 times. This ensures a control case where we keep the same amount of training data as the standard setup (such\nthat models does not benefit from data augmentation). We also ensure that we use the same hyperparameters while training as with the standard setup. Concretely, D̂train consists of n hypothesispremise pairs from MNLI training data, where each example is a permuted output of the original pair.\nWe present the results of such training in Table 6, and compare the accuracy (Â) with that of the standard setup (A). Note, during inference for all the models we use the un-permuted examples. As we can see, models perform surprisingly close to the original accuracy A even when trained with ungrammatical sentences. This adds further proof to the BOW nature of NLU models."
    }, {
      "heading" : "G Reproducibility Checklist",
      "text" : "As per the prescribed Reproducibility Checklist, we provide the information of the following:\n• A clear description of the mathematical setting, algorithm and/or model: We provide details of models used in §5\n• Description of the computing infrastructure used: We used 8 NVIDIA V100 32GB GPUs to train the models and perform all necessary inferences. We didn’t run hyperparameter tuning for Transformer-based models as we used the published hyperparameters from the original models.\n• Average runtime for each approach: On an average, each model inference experiment consistine of 100 permutations for each example takes roughly 1 hour to complete.\n• Relevant statistics of the datasets used: We provide the statistics of the datasets used in Table 7.\n• Explanation of any data that were excluded, and all pre-processing steps: We exclude examples where either the hypothesis and premise consists of less than 6 tokens. This way, we ensure that we have 100 unique permutations for each example.\n• Link to downloadable version of data and code: We provide downloadable version of our data and code at https://github.com/facebookresearch/unlu."
    } ],
    "references" : [ {
      "title" : "Lexical and syntactic rules in a Tree Adjoining Grammar",
      "author" : [ "Anne Abeille." ],
      "venue" : "28th Annual Meeting of the Association for Computational Linguistics, pages 292–298, Pittsburgh, Pennsylvania, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Abeille.,? 1990",
      "shortCiteRegEx" : "Abeille.",
      "year" : 1990
    }, {
      "title" : "Syntactic perturbations reveal representational correlates of hierarchical phrase structure in pretrained language models",
      "author" : [ "Matteo Alleman", "Jonathan Mamou", "Miguel A Del Rio", "Hanlin Tang", "Yoon Kim", "SueYeon Chung." ],
      "venue" : "arXiv preprint arXiv:2104.07578.",
      "citeRegEx" : "Alleman et al\\.,? 2021",
      "shortCiteRegEx" : "Alleman et al\\.",
      "year" : 2021
    }, {
      "title" : "Representation of constituents in neural language models: Coordination phrase as a case study",
      "author" : [ "Aixiu An", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "An et al\\.,? 2019",
      "shortCiteRegEx" : "An et al\\.",
      "year" : 2019
    }, {
      "title" : "Working memory and binding in sentence recall",
      "author" : [ "Alan D Baddeley", "Graham J Hitch", "Richard J Allen." ],
      "venue" : "Journal of Memory and Language.",
      "citeRegEx" : "Baddeley et al\\.,? 2009",
      "shortCiteRegEx" : "Baddeley et al\\.",
      "year" : 2009
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. As-",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "Using deep neural networks to learn syntactic agreement",
      "author" : [ "Jean-Phillipe Bernardy", "Shalom Lappin." ],
      "venue" : "Linguistic Issues in Language Technology, Volume 15, 2017. CSLI Publications.",
      "citeRegEx" : "Bernardy and Lappin.,? 2017",
      "shortCiteRegEx" : "Bernardy and Lappin.",
      "year" : 2017
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "JMLR.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Lexical-functional syntax",
      "author" : [ "Joan Bresnan", "Ash Asudeh", "Ida Toivonen", "Stephen Wechsler." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Bresnan et al\\.,? 2015",
      "shortCiteRegEx" : "Bresnan et al\\.",
      "year" : 2015
    }, {
      "title" : "The time it takes to see and name objects",
      "author" : [ "James McKeen Cattell." ],
      "venue" : "Mind, os-XI(41):63–65.",
      "citeRegEx" : "Cattell.,? 1886",
      "shortCiteRegEx" : "Cattell.",
      "year" : 1886
    }, {
      "title" : "What don’t RNN language models learn about filler-gap dependencies? In Proceedings of the Society for Computation in Linguistics 2020, pages 1–11, New York, New York",
      "author" : [ "Rui Chaves." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Chaves.,? 2020",
      "shortCiteRegEx" : "Chaves.",
      "year" : 2020
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Meaning and grammar: An Introduction to Semantics",
      "author" : [ "Gennaro Chierchia", "Sally McConnell-Ginet." ],
      "venue" : "Cambridge, Ma: MIT Press.",
      "citeRegEx" : "Chierchia and McConnell.Ginet.,? 1990",
      "shortCiteRegEx" : "Chierchia and McConnell.Ginet.",
      "year" : 1990
    }, {
      "title" : "Correlating neural and symbolic representations of language",
      "author" : [ "Grzegorz Chrupała", "Afra Alishahi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2952–2962, Florence, Italy. Association",
      "citeRegEx" : "Chrupała and Alishahi.,? 2019",
      "shortCiteRegEx" : "Chrupała and Alishahi.",
      "year" : 2019
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "ICML.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Entailment, intensionality and text understanding",
      "author" : [ "Cleo Condoravdi", "Dick Crouch", "Valeria de Paiva", "Reinhard Stolle", "Daniel G. Bobrow." ],
      "venue" : "Proceedings of the HLT-NAACL 2003 Workshop on Text Meaning, pages 38–45.",
      "citeRegEx" : "Condoravdi et al\\.,? 2003",
      "shortCiteRegEx" : "Condoravdi et al\\.",
      "year" : 2003
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Assessing the ability of transformer-based neural models to represent structurally unbounded dependencies",
      "author" : [ "Jillian Da Costa", "Rui Chaves." ],
      "venue" : "Proceedings of the Society for Computation in Linguistics 2020, pages 12–21, New York, New York. Asso-",
      "citeRegEx" : "Costa and Chaves.,? 2020",
      "shortCiteRegEx" : "Costa and Chaves.",
      "year" : 2020
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges Workshop. Springer.",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment. Springer.",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Evaluating compositionality in sentence embeddings",
      "author" : [ "Ishita Dasgupta", "Demi Guo", "Andreas Stuhlmüller", "Samuel J Gershman", "Noah D Goodman." ],
      "venue" : "Proceedings of Annual Meeting of the Cognitive Science Society.",
      "citeRegEx" : "Dasgupta et al\\.,? 2018",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2018
    }, {
      "title" : "Recurrent neural network language models always learn English-like relative clause attachment",
      "author" : [ "Forrest Davis", "Marten van Schijndel." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1979–1990,",
      "citeRegEx" : "Davis and Schijndel.,? 2020",
      "shortCiteRegEx" : "Davis and Schijndel.",
      "year" : 2020
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Sense and reference",
      "author" : [ "Gottlob Frege." ],
      "venue" : "The philosophical review.",
      "citeRegEx" : "Frege.,? 1948",
      "shortCiteRegEx" : "Frege.",
      "year" : 1948
    }, {
      "title" : "Mutual exclusivity as a challenge for deep neural networks",
      "author" : [ "Kanishk Gandhi", "Brenden M Lake." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 14182–14192. Curran Associates, Inc.",
      "citeRegEx" : "Gandhi and Lake.,? 2020",
      "shortCiteRegEx" : "Gandhi and Lake.",
      "year" : 2020
    }, {
      "title" : "SyntaxGym: An online platform for targeted evaluation of language models",
      "author" : [ "Jon Gauthier", "Jennifer Hu", "Ethan Wilcox", "Peng Qian", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System",
      "citeRegEx" : "Gauthier et al\\.,? 2020",
      "shortCiteRegEx" : "Gauthier et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing BERT’s syntactic abilities",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "arXiv preprint arXiv:1901.05287.",
      "citeRegEx" : "Goldberg.,? 2019",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Colorless green recurrent networks dream hierarchically",
      "author" : [ "Kristina Gulordava", "Piotr Bojanowski", "Edouard Grave", "Tal Linzen", "Marco Baroni." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT & family eat word salad: Experiments with text understanding",
      "author" : [ "Ashim Gupta", "Giorgi Kvernadze", "Vivek Srikumar." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Gupta et al\\.,? 2021",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2021
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers),",
      "citeRegEx" : "Smith.,? 2018",
      "shortCiteRegEx" : "Smith.",
      "year" : 2018
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig S Harris." ],
      "venue" : "Word.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "Investigating representations of verb bias in neural language models",
      "author" : [ "Robert Hawkins", "Takateru Yamakoshi", "Thomas Griffiths", "Adele Goldberg." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Hawkins et al\\.,? 2020",
      "shortCiteRegEx" : "Hawkins et al\\.",
      "year" : 2020
    }, {
      "title" : "Semantics in generative grammar",
      "author" : [ "Irene Heim", "Angelika Kratzer." ],
      "venue" : "Blackwell Oxford.",
      "citeRegEx" : "Heim and Kratzer.,? 1998",
      "shortCiteRegEx" : "Heim and Kratzer.",
      "year" : 1998
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "spaCy: Industrial-strength Natural Language Processing in Python",
      "author" : [ "Matthew Honnibal", "Ines Montani", "Sofie Van Landeghem", "Adriane Boyd" ],
      "venue" : null,
      "citeRegEx" : "Honnibal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Honnibal et al\\.",
      "year" : 2020
    }, {
      "title" : "OCNLI: Original Chinese Natural Language Inference",
      "author" : [ "Hai Hu", "Kyle Richardson", "Liang Xu", "Lu Li", "Sandra Kübler", "Lawrence Moss." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3512–3526, Online. As-",
      "citeRegEx" : "Hu et al\\.,? 2020a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "A systematic assessment of syntactic generalization in neural language models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hu et al\\.,? 2020b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT learn about the structure of language",
      "author" : [ "Ganesh Jawahar", "Benoı̂t Sagot", "Djamé Seddah" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition",
      "author" : [ "Paloma Jeretic", "Alex Warstadt", "Suvrat Bhooshan", "Adina Williams." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Jeretic et al\\.,? 2020",
      "shortCiteRegEx" : "Jeretic et al\\.",
      "year" : 2020
    }, {
      "title" : "Formal system for grammatical representation",
      "author" : [ "Ronald M Kaplan", "Joan Bresnan." ],
      "venue" : "Formal Issues in Lexical-Functional Grammar.",
      "citeRegEx" : "Kaplan and Bresnan.,? 1995",
      "shortCiteRegEx" : "Kaplan and Bresnan.",
      "year" : 1995
    }, {
      "title" : "Shallow-Deep Networks: Understanding and mitigating network overthinking",
      "author" : [ "Yiğitcan Kaya", "Sanghyun Hong", "Tudor Dumitras." ],
      "venue" : "Proceedings of the 2019 International Conference on Machine Learning (ICML), Long Beach, CA.",
      "citeRegEx" : "Kaya et al\\.,? 2019",
      "shortCiteRegEx" : "Kaya et al\\.",
      "year" : 2019
    }, {
      "title" : "Overestimation of syntactic representation in neural language models",
      "author" : [ "Jordan Kodner", "Nitish Gupta." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1757–1762, Online. Association for Computa-",
      "citeRegEx" : "Kodner and Gupta.,? 2020",
      "shortCiteRegEx" : "Kodner and Gupta.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "SA-NLI: A supervised attention based framework for natural language inference",
      "author" : [ "Peiguang Li", "Hongfeng Yu", "Wenkai Zhang", "Guangluan Xu", "Xian Sun." ],
      "venue" : "Neurocomputing.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Open sesame: Getting inside BERT’s linguistic knowledge",
      "author" : [ "Yongjie Lin", "Yi Chern Tan", "Robert Frank." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241–253, Florence,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntactic structure from deep learning",
      "author" : [ "Tal Linzen", "Marco Baroni." ],
      "venue" : "Annual Review of Linguistics.",
      "citeRegEx" : "Linzen and Baroni.,? 2021",
      "shortCiteRegEx" : "Linzen and Baroni.",
      "year" : 2021
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Computational linguistics and deep learning",
      "author" : [ "Christopher D Manning." ],
      "venue" : "Computational Linguistics.",
      "citeRegEx" : "Manning.,? 2015",
      "shortCiteRegEx" : "Manning.",
      "year" : 2015
    }, {
      "title" : "Emergent linguistic structure in artificial neural networks trained by self-supervision",
      "author" : [ "Christopher D. Manning", "Kevin Clark", "John Hewitt", "Urvashi Khandelwal", "Omer Levy." ],
      "venue" : "Proceedings of the National Academy of Sciences, 117(48):30046–30054.",
      "citeRegEx" : "Manning et al\\.,? 2020",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted syntactic evaluation of language models",
      "author" : [ "Rebecca Marvin", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202, Brussels, Belgium. Association for Computational",
      "citeRegEx" : "Marvin and Linzen.,? 2018",
      "shortCiteRegEx" : "Marvin and Linzen.",
      "year" : 2018
    }, {
      "title" : "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance",
      "author" : [ "R. Thomas McCoy", "Junghyun Min", "Tal Linzen." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on An-",
      "citeRegEx" : "McCoy et al\\.,? 2020",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2020
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomás Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Composition is the core driver of the language-selective network",
      "author" : [ "Francis Mollica", "Matthew Siegelman", "Evgeniia Diachek", "Steven T Piantadosi", "Zachary Mineroff", "Richard Futrell", "Hope Kean", "Peng Qian", "Evelina Fedorenko." ],
      "venue" : "Neurobiology of",
      "citeRegEx" : "Mollica et al\\.,? 2020",
      "shortCiteRegEx" : "Mollica et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal grammar",
      "author" : [ "Richard Montague." ],
      "venue" : "Theoria.",
      "citeRegEx" : "Montague.,? 1970",
      "shortCiteRegEx" : "Montague.",
      "year" : 1970
    }, {
      "title" : "Exploring numeracy in word embeddings",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Carolyn Rose", "Eduard Hovy." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374–3380, Florence, Italy. Asso-",
      "citeRegEx" : "Naik et al\\.,? 2019",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2019
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353,",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "Human vs",
      "author" : [ "Nikita Nangia", "Samuel R. Bowman." ],
      "venue" : "muppet: A conservative estimate of human performance on the GLUE benchmark. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4566–4575, Flo-",
      "citeRegEx" : "Nangia and Bowman.,? 2019",
      "shortCiteRegEx" : "Nangia and Bowman.",
      "year" : 2019
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Sometimes we want translationese",
      "author" : [ "Prasanna Parthasarathi", "Koustuv Sinha", "Joelle Pineau", "Adina Williams." ],
      "venue" : "arXiv preprint arXiv:2104.07623.",
      "citeRegEx" : "Parthasarathi et al\\.,? 2021",
      "shortCiteRegEx" : "Parthasarathi et al\\.",
      "year" : 2021
    }, {
      "title" : "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? arXiv preprint arXiv:2012.15180",
      "author" : [ "Thang M Pham", "Trung Bui", "Long Mai", "Anh Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Pham et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2020
    }, {
      "title" : "Hypothesis only baselines in natural language inference",
      "author" : [ "Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,",
      "citeRegEx" : "Poliak et al\\.,? 2018",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2018
    }, {
      "title" : "Head-driven phrase structure grammar",
      "author" : [ "Carl Pollard", "Ivan A Sag." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Pollard and Sag.,? 1994",
      "shortCiteRegEx" : "Pollard and Sag.",
      "year" : 1994
    }, {
      "title" : "Using priming to uncover the organization of syntactic representations in neural language models",
      "author" : [ "Grusha Prasad", "Marten van Schijndel", "Tal Linzen." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages",
      "citeRegEx" : "Prasad et al\\.,? 2019",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Studying the inductive biases of RNNs with synthetic variations of natural languages",
      "author" : [ "Shauli Ravfogel", "Yoav Goldberg", "Tal Linzen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Ravfogel et al\\.,? 2019",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2019
    }, {
      "title" : "Can LSTM learn to capture agreement? the case of Basque",
      "author" : [ "Shauli Ravfogel", "Yoav Goldberg", "Francis Tyers." ],
      "venue" : "Proceedings of the 2018 EMNLP",
      "citeRegEx" : "Ravfogel et al\\.,? 2018",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2018
    }, {
      "title" : "EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference",
      "author" : [ "Abhilasha Ravichander", "Aakanksha Naik", "Carolyn Rose", "Eduard Hovy." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural",
      "citeRegEx" : "Ravichander et al\\.,? 2019",
      "shortCiteRegEx" : "Ravichander et al\\.",
      "year" : 2019
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf" ],
      "venue" : null,
      "citeRegEx" : "Sanh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2020
    }, {
      "title" : "Parsing strategies with ‘lexicalized’ grammars: Application to Tree Adjoining Grammars",
      "author" : [ "Yves Schabes", "Anne Abeille", "Aravind K. Joshi." ],
      "venue" : "Coling Budapest 1988 Volume 2: International Conference on Computational Linguistics.",
      "citeRegEx" : "Schabes et al\\.,? 1988",
      "shortCiteRegEx" : "Schabes et al\\.",
      "year" : 1988
    }, {
      "title" : "Early german approaches to experimental reading research: The contributions of wilhelm wundt and ernst meumann",
      "author" : [ "Eckart Scheerer." ],
      "venue" : "Psychological Research.",
      "citeRegEx" : "Scheerer.,? 1981",
      "shortCiteRegEx" : "Scheerer.",
      "year" : 1981
    }, {
      "title" : "A neural model of adaptation in reading",
      "author" : [ "Marten van Schijndel", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4704–4710, Brussels, Belgium. Association for Computational Lin-",
      "citeRegEx" : "Schijndel and Linzen.,? 2018",
      "shortCiteRegEx" : "Schijndel and Linzen.",
      "year" : 2018
    }, {
      "title" : "Quantity doesn’t buy quality syntax with neural language models",
      "author" : [ "Marten van Schijndel", "Aaron Mueller", "Tal Linzen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Schijndel et al\\.,? 2019",
      "shortCiteRegEx" : "Schijndel et al\\.",
      "year" : 2019
    }, {
      "title" : "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
      "author" : [ "Koustuv Sinha", "Robin Jia", "Dieuwke Hupkes", "Joelle Pineau", "Adina Williams", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2104.06644.",
      "citeRegEx" : "Sinha et al\\.,? 2021",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2021
    }, {
      "title" : "The sentence superiority effect revisited",
      "author" : [ "Joshua Snell", "Jonathan Grainger." ],
      "venue" : "Cognition.",
      "citeRegEx" : "Snell and Grainger.,? 2017",
      "shortCiteRegEx" : "Snell and Grainger.",
      "year" : 2017
    }, {
      "title" : "Word position coding in reading is noisy",
      "author" : [ "Joshua Snell", "Jonathan Grainger." ],
      "venue" : "Psychonomic bulletin & review, 26(2):609–615.",
      "citeRegEx" : "Snell and Grainger.,? 2019",
      "shortCiteRegEx" : "Snell and Grainger.",
      "year" : 2019
    }, {
      "title" : "Syntactic innovation: A connectionist model",
      "author" : [ "Whitney Tabor." ],
      "venue" : "Ph.D. thesis.",
      "citeRegEx" : "Tabor.,? 1994",
      "shortCiteRegEx" : "Tabor.",
      "year" : 1994
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Changes in the constraints of semantic and syntactic congruity on memory across three age groups",
      "author" : [ "Hiroshi Toyota." ],
      "venue" : "Perceptual and Motor Skills.",
      "citeRegEx" : "Toyota.,? 2001",
      "shortCiteRegEx" : "Toyota.",
      "year" : 2001
    }, {
      "title" : "Performance impact caused by hidden bias of training data for recognizing textual entailment",
      "author" : [ "Masatoshi Tsuchiya." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki,",
      "citeRegEx" : "Tsuchiya.,? 2018",
      "shortCiteRegEx" : "Tsuchiya.",
      "year" : 2018
    }, {
      "title" : "What syntax can contribute in the entailment task",
      "author" : [ "Lucy Vanderwende", "William B Dolan." ],
      "venue" : "Machine Learning Challenges Workshop. Springer.",
      "citeRegEx" : "Vanderwende and Dolan.,? 2005",
      "shortCiteRegEx" : "Vanderwende and Dolan.",
      "year" : 2005
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Can neural networks acquire a structural bias from raw linguistic data",
      "author" : [ "Alex Warstadt", "Samuel R Bowman" ],
      "venue" : "In Proceedings of the 42nd Annual Virtual Meeting of the Cognitive Science Society",
      "citeRegEx" : "Warstadt and Bowman.,? \\Q2020\\E",
      "shortCiteRegEx" : "Warstadt and Bowman.",
      "year" : 2020
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:377–392.",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019b",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel, cascaded, interactive processing of words during sentence reading",
      "author" : [ "Yun Wen", "Joshua Snell", "Jonathan Grainger." ],
      "venue" : "Cognition.",
      "citeRegEx" : "Wen et al\\.,? 2019",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2019
    }, {
      "title" : "Lexicosyntactic inference in neural models",
      "author" : [ "Aaron Steven White", "Rachel Rudinger", "Kyle Rawlins", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4717–4724, Brus-",
      "citeRegEx" : "White et al\\.,? 2018",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 2018
    }, {
      "title" : "What do RNN language models learn about filler–gap dependencies",
      "author" : [ "Ethan Wilcox", "Roger Levy", "Takashi Morita", "Richard Futrell" ],
      "venue" : "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Net-",
      "citeRegEx" : "Wilcox et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2018
    }, {
      "title" : "2018a. Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for Computational Linguistics, 6:253–267",
      "author" : [ "Adina Williams", "Andrew Drozdov", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Williams et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018b",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Tractatus LogicoPhilosophicus",
      "author" : [ "Ludwig Wittgenstein." ],
      "venue" : "Harcourt, Brace & Company, Inc.",
      "citeRegEx" : "Wittgenstein.,? 1922",
      "shortCiteRegEx" : "Wittgenstein.",
      "year" : 1922
    }, {
      "title" : "Some additional experiments extending the tech report” assessing berts syntactic abilities” by yoav goldberg",
      "author" : [ "Thomas Wolf." ],
      "venue" : "Technical report, HuggingFace.",
      "citeRegEx" : "Wolf.,? 2019",
      "shortCiteRegEx" : "Wolf.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Perturbed masking: Parameter-free probing for analyzing and interpreting BERT",
      "author" : [ "Zhiyong Wu", "Yun Chen", "Ben Kao", "Qun Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4166–4176, Online. As-",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis",
      "author" : [ "Kelly Zhang", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neu-",
      "citeRegEx" : "Zhang and Bowman.,? 2018",
      "shortCiteRegEx" : "Zhang and Bowman.",
      "year" : 2018
    }, {
      "title" : "Self-adaptive hierarchical sentence model",
      "author" : [ "Han Zhao", "Zhengdong Lu", "Pascal Poupart." ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence, pages 4069–4076.",
      "citeRegEx" : "Zhao et al\\.,? 2015",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert loses patience: Fast and robust inference with early exit",
      "author" : [ "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian McAuley", "Ke Xu", "Furu Wei." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 18330–18341. Curran Associates,",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 85,
      "context" : "Of late, large scale pre-trained Transformer-based (Vaswani et al., 2017) models—such as RoBERTa (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 48,
      "context" : ", 2017) models—such as RoBERTa (Liu et al., 2019), BART (Lewis et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 43,
      "context" : ", 2019), BART (Lewis et al., 2020), and GPT-2 and -3 (Radford et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 67,
      "context" : ", 2020), and GPT-2 and -3 (Radford et al., 2019; Brown et al., 2020)—have exceeded recurrent neural networks’ performance on many NLU tasks (Wang et al.",
      "startOffset" : 26,
      "endOffset" : 68
    }, {
      "referenceID" : 34,
      "context" : "tion (Hewitt and Manning, 2019; Jawahar et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), with their self-attention layers being capa-",
      "startOffset" : 5,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "tion (Hewitt and Manning, 2019; Jawahar et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), with their self-attention layers being capa-",
      "startOffset" : 5,
      "endOffset" : 97
    }, {
      "referenceID" : 89,
      "context" : "tion (Hewitt and Manning, 2019; Jawahar et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), with their self-attention layers being capa-",
      "startOffset" : 5,
      "endOffset" : 97
    }, {
      "referenceID" : 100,
      "context" : "tion (Hewitt and Manning, 2019; Jawahar et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), with their self-attention layers being capa-",
      "startOffset" : 5,
      "endOffset" : 97
    }, {
      "referenceID" : 71,
      "context" : "ble of surprisingly effective learning (Rogers et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "We verify our findings with three popular English NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 96,
      "context" : ", 2015), MultiNLI (Williams et al., 2018b) and ANLI (Nie et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 60,
      "context" : ", 2018b) and ANLI (Nie et al., 2020))—and one Chinese one, OCNLI (Hu et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 36,
      "context" : ", 2020))—and one Chinese one, OCNLI (Hu et al., 2020a).",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "An early hand annotation effort on PASCAL RTE (Dagan et al., 2006) suggested that “syntactic information alone was sufficient to make a judgment” for roughly one third of examples (Vanderwende and Dolan, 2005).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 84,
      "context" : ", 2006) suggested that “syntactic information alone was sufficient to make a judgment” for roughly one third of examples (Vanderwende and Dolan, 2005).",
      "startOffset" : 121,
      "endOffset" : 150
    }, {
      "referenceID" : 27,
      "context" : "large generative language models like GPT-2 or -3 exhibit a seemingly humanlike ability to generate fluent and grammatical text (Goldberg, 2019; Wolf, 2019).",
      "startOffset" : 128,
      "endOffset" : 156
    }, {
      "referenceID" : 98,
      "context" : "large generative language models like GPT-2 or -3 exhibit a seemingly humanlike ability to generate fluent and grammatical text (Goldberg, 2019; Wolf, 2019).",
      "startOffset" : 128,
      "endOffset" : 156
    }, {
      "referenceID" : 34,
      "context" : "When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of",
      "startOffset" : 110,
      "endOffset" : 220
    }, {
      "referenceID" : 38,
      "context" : "When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of",
      "startOffset" : 110,
      "endOffset" : 220
    }, {
      "referenceID" : 45,
      "context" : "When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of",
      "startOffset" : 110,
      "endOffset" : 220
    }, {
      "referenceID" : 89,
      "context" : "When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of",
      "startOffset" : 110,
      "endOffset" : 220
    }, {
      "referenceID" : 100,
      "context" : "When researchers have peeked inside Transformer LM’s pretrained representations, familiar syntactic structure (Hewitt and Manning, 2019; Jawahar et al., 2019; Lin et al., 2019; Warstadt and Bowman, 2020; Wu et al., 2020), or a familiar order of",
      "startOffset" : 110,
      "endOffset" : 220
    }, {
      "referenceID" : 38,
      "context" : "linguistic operations (Jawahar et al., 2019; Tenney et al., 2019), has appeared.",
      "startOffset" : 22,
      "endOffset" : 65
    }, {
      "referenceID" : 81,
      "context" : "linguistic operations (Jawahar et al., 2019; Tenney et al., 2019), has appeared.",
      "startOffset" : 22,
      "endOffset" : 65
    }, {
      "referenceID" : 47,
      "context" : "There is also evidence, notably from agreement attraction phenomena (Linzen et al., 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 28,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 13,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 38,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 45,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 50,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 32,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 46,
      "context" : ", 2016) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax (Gulordava et al., 2018; Chrupała and Alishahi, 2019; Jawahar et al., 2019; Lin et al., 2019; Manning et al., 2020; Hawkins et al., 2020; Linzen and Baroni, 2021).",
      "startOffset" : 107,
      "endOffset" : 269
    }, {
      "referenceID" : 5,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al.",
      "startOffset" : 132,
      "endOffset" : 159
    }, {
      "referenceID" : 28,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.",
      "startOffset" : 170,
      "endOffset" : 315
    }, {
      "referenceID" : 94,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.",
      "startOffset" : 170,
      "endOffset" : 315
    }, {
      "referenceID" : 101,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.",
      "startOffset" : 170,
      "endOffset" : 315
    }, {
      "referenceID" : 66,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.",
      "startOffset" : 170,
      "endOffset" : 315
    }, {
      "referenceID" : 68,
      "context" : "The claim that LMs acquire some syntactic knowledge has been made not only for transformers, but also for convolutional neural nets (Bernardy and Lappin, 2017), and RNNs (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Zhang and Bowman, 2018; Prasad et al., 2019; Ravfogel et al., 2019)—although there are many caveats (e.",
      "startOffset" : 170,
      "endOffset" : 315
    }, {
      "referenceID" : 93,
      "context" : ", 2019)—although there are many caveats (e.g., Ravfogel et al. 2018; White et al. 2018; Davis and van Schijndel 2020; Chaves 2020; Da Costa and Chaves 2020; Kodner and Gupta 2020).",
      "startOffset" : 40,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "sensitivity to syntactic structure (Chen et al., 2017; Li et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "sensitivity to syntactic structure (Chen et al., 2017; Li et al., 2020).",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 91,
      "context" : ", one of the most common syntactic tests used by linguists) have, to date, come nowhere near human performance (Warstadt et al., 2019b).",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 63,
      "context" : "Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs.",
      "startOffset" : 41,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs.",
      "startOffset" : 41,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs.",
      "startOffset" : 41,
      "endOffset" : 150
    }, {
      "referenceID" : 77,
      "context" : "Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs.",
      "startOffset" : 41,
      "endOffset" : 150
    }, {
      "referenceID" : 62,
      "context" : "Most relatedly, several concurrent works (Pham et al., 2020; Alleman et al., 2021; Gupta et al., 2021; Sinha et al., 2021; Parthasarathi et al., 2021) investigated the effect of word order permutations on transformer NNs.",
      "startOffset" : 41,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : "models often over-attend to particular words to predict the correct answer (Gururangan et al., 2018; Clark et al., 2019).",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "If performance isn’t affected (or if permutation helps, as we find it does in some cases), it suggests that these state-of-the-art models actually perform somewhat similarly to bag-of-words models (Blei et al., 2003; Mikolov et al., 2013).",
      "startOffset" : 197,
      "endOffset" : 238
    }, {
      "referenceID" : 54,
      "context" : "If performance isn’t affected (or if permutation helps, as we find it does in some cases), it suggests that these state-of-the-art models actually perform somewhat similarly to bag-of-words models (Blei et al., 2003; Mikolov et al., 2013).",
      "startOffset" : 197,
      "endOffset" : 238
    }, {
      "referenceID" : 24,
      "context" : "In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948; Montague, 1970; Chierchia and McConnell-Ginet, 1990; Heim and Kratzer, 1998).",
      "startOffset" : 88,
      "endOffset" : 178
    }, {
      "referenceID" : 56,
      "context" : "In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948; Montague, 1970; Chierchia and McConnell-Ginet, 1990; Heim and Kratzer, 1998).",
      "startOffset" : 88,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948; Montague, 1970; Chierchia and McConnell-Ginet, 1990; Heim and Kratzer, 1998).",
      "startOffset" : 88,
      "endOffset" : 178
    }, {
      "referenceID" : 33,
      "context" : "In the spirit of propositional logic, sentence meaning is taken to be truth-conditional (Frege, 1948; Montague, 1970; Chierchia and McConnell-Ginet, 1990; Heim and Kratzer, 1998).",
      "startOffset" : 88,
      "endOffset" : 178
    }, {
      "referenceID" : 97,
      "context" : "That is to say that understanding a sentence is equivalent to knowing the actual conditions of the world under which the sentences would be (judged) true (Wittgenstein, 1922).",
      "startOffset" : 154,
      "endOffset" : 174
    }, {
      "referenceID" : 56,
      "context" : "proach (Montague, 1970), then permuted sentences should be meaningless.",
      "startOffset" : 7,
      "endOffset" : 23
    }, {
      "referenceID" : 48,
      "context" : "the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 43,
      "context" : ", 2019), BART-Large (Lewis et al., 2020) and DistilBERT (Sanh et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "ferSent (Conneau et al., 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : ", 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al.",
      "startOffset" : 28,
      "endOffset" : 56
    }, {
      "referenceID" : 102,
      "context" : ", 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al., 2015).",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 99,
      "context" : "We independently verify results of (a) using both our fine-tuned model using HuggingFace Transformers (Wolf et al., 2020) and pre-trained checkpoints from FairSeq (Ott et al.",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 61,
      "context" : ", 2020) and pre-trained checkpoints from FairSeq (Ott et al., 2019) (using PyTorch Model Hub).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 96,
      "context" : "Table 2: Statistics for Transformer-based models trained on MNLI corpus (Williams et al., 2018b).",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "All models are trained on OCNLI corpus (Hu et al., 2020a).",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 41,
      "context" : "to the overthinking phenomenon commonly observed in deep neural networks (Kaya et al., 2019) and BERT-based models (Zhou et al.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "verifies findings that showed high word overlap can give hints about the ground truth label (Dasgupta et al., 2018; Poliak et al., 2018; Gururangan et al., 2018; Naik et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 180
    }, {
      "referenceID" : 64,
      "context" : "verifies findings that showed high word overlap can give hints about the ground truth label (Dasgupta et al., 2018; Poliak et al., 2018; Gururangan et al., 2018; Naik et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 180
    }, {
      "referenceID" : 57,
      "context" : "verifies findings that showed high word overlap can give hints about the ground truth label (Dasgupta et al., 2018; Poliak et al., 2018; Gururangan et al., 2018; Naik et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "Neural networks tend to output higher confidence than random for even unknown inputs (Gandhi and Lake, 2020), which might be an underlying cause of the",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "Thus, we train NLI models baking in the principle of mutual exclusivity (Gandhi and Lake, 2020) by maximizing",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "Similar approaches have also proven useful (Gupta et al., 2021) for other tasks as well.",
      "startOffset" : 43,
      "endOffset" : 63
    } ],
    "year" : 2021,
    "abstractText" : "Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random wordorder permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.",
    "creator" : "LaTeX with hyperref"
  }
}