{
  "name" : "2021.acl-long.498.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text",
    "authors" : [ "Philippe Laban", "Tobias Schnabel", "Paul N. Bennett", "Marti A. Hearst" ],
    "emails" : [ "phillab@berkeley.edu,", "hearst@berkeley.edu,", "Tobias.Schnabel@microsoft.com", "Paul.N.Bennett@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6365–6378\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6365"
    }, {
      "heading" : "1 Introduction",
      "text" : "The main objective of text simplification is to make a complex text accessible to a wide audience by increasing its readability. In contrast with text summarization – in which key content is selected to remain in the summary and other content is elided – in text simplification, ideally all relevant content is preserved.\nWe propose that text simplification algorithms need to balance three properties: (1) fluency: the simplified text should use well-formed English sentences, (2) salience: the simplified text should relay the same information as the original, and (3) simplicity: the simplified text should be syntactically and lexically simpler than the original.\nFigure 1 provides intuition for the necessity of each of the three properties. It shows the original text and the output of the full proposed model compared to three reduced versions:\n∗ Author emails: {phillab,hearst}@berkeley.edu, {Tobias.Schnabel,Paul.N.Bennett}@microsoft.com\nWithout Fluency, the generator has no incentive to generate full sentences, and learns it can boost the simplicity score by generating short phrases with excessive punctuation.\nWithout Salience, the generator does not gain by covering facts in the original text, and can improve the simplicity score by learning to remove facts (e.g., not mentioning planet Mars by name).\nWithout Simplicity, the generator is not guided to favor syntactically and lexically simpler rewrites. In Figure 1, Model No Simplicity is in fact more complex than the original according to readability measures.\nAs we show in the related work section (Section 2), there are no high-quality, large datasets publicly released for text simplification. In this work, we build on recent progress of reinforcement learning (RL)-based training of text generators: we\nformulate a reference-free reward for text simplification and directly optimize it, circumventing the need for aligned data.\nOur main contribution is the Keep it Simple (KiS) procedure, a novel unsupervised method for text simplification. Applied to the English news domain, KiS outperforms several supervised models on common simplification metrics such as SARI (Xu et al., 2016) and the Flesch-Kincaid Grade Level (Kincaid et al., 1975).\nA second contribution is a new algorithm for RLbased training of text generators, k-SCST, which is an extension of Self-Critical Sequence Training (Rennie et al., 2017). For each input, we generate k sampled outputs (vs. 2 in SCST), and use the mean population reward as a baseline. We show in Section 4 that in our domain, k-SCST outperforms models trained with SCST.\nA third contribution is a novel evaluation method for text simplification. Based on the assumption that simplified text should enable faster reading with better understanding, we propose a realistic Text Comprehension task. We show that people reading texts simplified by KiS are able to complete comprehension tasks faster than comparison texts.\nAnother departure from previous work is that we work with paragraphs as units of text. Most work in text simplification is done at the sentence level, despite work such as Zhong et al. (2020) showing that common simplification phenomena occur at the level of the paragraph, (e.g., the deletion, insertion or re-ordering of full sentences). Specifically, we train our models to simplify full paragraphs, and evaluate our models in a human evaluation on short documents (i.e., 3-4 paragraphs).\nThrough rigorous empirical evaluation, we demonstrate the strong performance of our approach; automated results show that this unsupervised approach is able to outperform strong supervised models by 4 SARI points or more. We publicly released the code and model checkpoints1."
    }, {
      "heading" : "2 Related Work",
      "text" : "Simplification Datasets. Early datasets were first based on Simple Wikipedia2: WikiSmall (Zhu et al., 2010), later expanded into WikiLarge (Zhang and Lapata, 2017). Xu et al. (2015) show there are quality concerns with Simple Wikipedia datasets,\n1https://github.com/tingofurro/keep_ it_simple\n2https://simple.wikipedia.org/\nand propose Newsela3 as a replacement. Newsela is a project led by educators re-writing news articles targeting different school grade levels. We view Newsela as the gold-standard for our work, and use the public Newsela release of 1,911 groups of articles to design and evaluate our work. Using a coarse paragraph alignment algorithm, we extract 40,000 paired simple/complex paragraphs targeting a separation of 4 grade levels. We call this dataset the paired Newsela dataset, which we use for analysis and baseline training.\nSeq2Seq for Simplification. Text simplification is most commonly framed as a sequence-tosequence (seq2seq) task, leveraging model architectures of other seq2seq tasks, such as natural machine translation (Zhu et al., 2010; Wubben et al., 2012). Martin et al. (2020) introduce ACCESS, a finetuned Transformer model that achieves stateof-the-art performance on WikiLarge. ACCESS can customize simplifications on parameters such as compression rate and paraphrase amount. We directly compare our approach to ACCESS.\nData availability remains one of the main limitations to seq2seq-based text simplification. We side-step this issue entirely by working with unsupervised data, only requiring a small dataset with coarse-level alignments for calibration.\nLexical Simplification focuses on the substitution of single words or phrases with simpler equivalents, with diverse approaches using lexical databases such as WordNet (Thomas and Anderson, 2012), to using contextualized word vectors (Qiang et al., 2020). These methods tend to be limited, as they do not consider syntactic complexity, and have no direct way of modeling deletions and insertions. We incorporate a lexical score (LScore) as one of the rewards in our simplicity component.\nText-edit for Simplification. Recent work (Dong et al., 2019; Stahlberg and Kumar, 2020) has modeled text simplification as a text-edit task, learning sequences of word-edits that transform the input into the output. Text editing offers explainability, at the cost of added model complexity. We find that without explicitly representing edits, the KiS model easily learns to copy (using attention heads) and deviate from the original text. Outputs can be post-processed into edits, if desired.\nUnsupervised Simplification has mostly been limited to lexical simplification. Recently Surya et al. (2019) (Unsup NTS) proposed a system that\n3https://newsela.com/\ncan perform both lexical and syntactic simplification, with a joint encoder, and two decoders (simple and complex). We directly compare our unsupervised approach to Unsup NTS.\nRL for Simplification. Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification. However, in both cases, components of the reward or training procedure involved reference simplifications, requiring an aligned dataset. By designing a reference-free reward, we are able to train our model with RL without supervision.\nEvaluation of Simplification. This usually falls into two categories: automatic offline evaluation, and human evaluation. Automatic evaluations usually involve using n-gram overlap calculations such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016)). SARI was shown to correlate better with human judgements of simplicity than BLEU, and it has since become a standard (Zhang and Lapata, 2017; Surya et al., 2019; Martin et al., 2020). In our experiments, we report both SARI and BLEU.\nHuman evaluation is typically done in an intrinsic way – e.g., by directly rating factors like fluency, simplicity and relevance of model outputs (Surya et al., 2019; Wubben et al., 2012). In this work, we propose an extrinsic, task-based protocol. In our comprehension study, we directly measure how much simplified texts can help a human reader answer questions more efficiently. The closest to our evaluation design is that of Angrosh et al. (2014) with the important difference that we require participants to resubmit after erroneous answers. In pilot studies, we found this step to be crucial for high-quality responses."
    }, {
      "heading" : "3 KiS Components",
      "text" : "In KiS, we approach unsupervised simplification as a (non-differentiable) reward maximization problem. As shown in Figure 2, there are four components to the reward: simplicity, fluency, salience and guardrails which are jointly optimized. This is essential to avoid trivial solutions that only consider subsets. We therefore use the product of all components as the total reward, because the product is sensitive to the sharp decrease of a single component. For example, the triggering of a single guardrail leads to the zeroing of the total reward. Each component is normalized to the [0, 1] range."
    }, {
      "heading" : "3.1 Simplicity",
      "text" : "The simplicity score should establish whether the generator’s output uses simpler language than the original text. We follow prior work (Ferrés et al., 2016) and organize our score into a syntactic score SScore, and a lexical score LScore. Syntactic simplification focuses on reducing the complexity of a sentence, for example by reducing the number of words in a clause, or reducing distant dependencies. In lexical simplification, the objective is to replace complex phrases with simpler synonyms. To produce a single simplicity score, we take the product of SScore and LScore (both in [0, 1]).\n3.1.1 Syntactic Simplicity: SScore We measure syntactic complexity via the FleschKincaid grade level (FKGL) as it is easy to compute and maps to a grade-level which also corresponds to the scale used by Newsela. Other readability metrics such as Dale-Chall formula (Dale and Chall, 1948), or the Gunning-Fog index (Gunning, 1969) could be used, and future work could examine the effect of choosing one readability metric over the\nother. Another viable option is the Lexile score (Smith et al., 2016), however, because its implementation is not publicly released, we cannot use it during training and we report it only for evaluation (done manually on the Lexile Hub4).\nFigure 3 shows the SScore algorithm. We compute the original paragraph’s FKGL (FStart), used to compute a target FKGL (tgt). The score is a linear ramp measuring how close the achieved FKGL (Fend) is to the target, clipped to [0, 1].\nIn the initial design, the target drop was a constant: 4 grade levels, independent of FStart. However, analysis on the paired Newsela corpus revealed that the target FKGL should depend on the initial FKGL. This makes sense intuitively: an already syntactically simple paragraph should not require further simplification, while more complex paragraphs require more simplification. Figure 4 shows the positive correlation between the original paragraph’s FKGL and the drop of FKGL in the simplified text. We fit a piece-wise linear function to calculate the target FKGL drop from the initial paragraph.\n3.1.2 Lexical Simplicity: LScore Lexical simplicity focuses on whether words in the input paragraph (W1) are more complex than ones in the output paragraph (W2). We rely on the observation that word frequency and difficulty are correlated (Breland, 1996), and use word frequency in a large corpus of text (Brysbaert and New, 2009) to determine simplicity.\n4https://hub.lexile.com\nBecause word frequency follows a Zipf power law, we use Speer et al. (2018)’s log normalization, adjusting the frequency on a [0, 8] range, with words at 0 being non-existent in the corpus, and 8 for most common words. As an example, the word vigorous has a frequency of 3.54, while its more common synonym strong obtains 5.23.\nWe compute the average Zipf frequency of the set of inserted words (Z(W2 −W1)), and the set of deleted words (Z(W1 −W2)). The difference\n∆Z(W1,W2) = Z(W2 −W1)− Z(W1 −W2) (1)\nshould be positive. Analysis of the paired Newsela corpus reveals that 91% of pairs have a positive ∆Z(W1,W2), with a median value of 0.4. We use this median as the target Zipf shift in the LScore, and use a ramp shape similar to the SScore, clipped between 0 and 1 (denoted as [·]+):\nLScore(W1,W2) = [ 1− |∆Z(W1,W2)−0.4|0.4 ]+ (2)"
    }, {
      "heading" : "3.2 Fluency",
      "text" : "We use two sub-components for the fluency component: a pre-trained language-model, and a discriminator trained dynamically with the generator."
    }, {
      "heading" : "3.2.1 Language-Model Fluency",
      "text" : "Language models assign a probability to a sequence of words. This probability is often used to measure fluency of generated text (Kann et al., 2018; Salazar et al., 2020). The KiS fluency score is based on a language model in a way similar way to Laban et al. (2020). The language model is used to obtain a likelihood of the original paragraph (LM(p)) and of the generated output LM(q). We use average log-likelihood, for numerical stability. The language model fluency score is then:\nLMScore(p, q) = [ 1− LM(p)− LM(q)\nλ\n]+ (3)\nλ is a tunable hyper-parameter. If the LM(q) is lower than LM(p) by λ or more, LMScore(p, q) = 0. If LM(q) is above or equal to LM(p), then LMScore(p, q) = 1, and otherwise, it is a linear interpolation.\nWe set λ = 1.3 as it is the value for which the paired Newsela dataset achieves an average LMScore of 0.9."
    }, {
      "heading" : "3.2.2 Discriminator Fluency",
      "text" : "The LMScore is static and deterministic, which can be limiting, as the generator can learn during training how to adapt and exploit flaws in the languagemodel (e.g., learning to alter capitalization).\nInspired from the Generative Adversarial Network (GAN) framework (Goodfellow et al., 2014), we create a dynamic discriminator, trained in conjunction with the generator, dynamically adapting the fluency score during training.\nSpecifically, we use a RoBERTa model (Liu et al., 2019) as the basis for the discriminator, a classifier with two labels: 1 for authentic paragraphs, and 0 for generator outputs.\nAs the generator produces outputs, they are assigned a label of 0 and added to a training buffer, while the original paragraphs are assigned a label of 1 and added to the training buffer as well.\nOnce the training buffer reaches a size of 2,000 samples, the discriminator is trained, using 90% of the training buffer. We train the discriminator for 5 epochs (details of training are in Appendix A.1). At the end of each epoch, we checkpoint the discriminator model. We compare the 5 checkpoints in terms of F-1 performance on the remaining 10% of the training buffer, and keep the best checkpoint as the new discriminator.\nThe discriminator’s probability that a paragraph (q) is authentic is the discriminator score:\nDScore(q) = pdisc(Y = 1|X = q) (4)\nAs with GANs, there is an equilibrium between the generator attempting to maximize the probability of generating real outputs (“fooling” the discriminator), and the discriminator succeeding at distinguishing generated and authentic texts."
    }, {
      "heading" : "3.3 Salience",
      "text" : "For the salience component, we use the coverage model introduced in the summary loop (Laban et al., 2020) for the domain of text summarization, and adapt it to the simplification domain.\nThe coverage model is a Transformer-based model trained to look at generated text and answer fill-in-the-blank questions about the original text. The score is based on model accuracy at filling in the blanks: the more is filled in, the more relevant the generated content is, and the higher the score.\nA key element of the coverage model is its masking procedure, which decides which words to mask. In the summary loop, a limited number of extracted\nkeywords (up to 15 words) are masked. By contrast, for simplification, we mask all non-stop words, amounting to a masking rate of about 40%.\nThis change reflects a difference in expectation between summarization and simplification: in summarization, only key components are expected to be recovered from a summary, whereas in simplification most of the original paragraph should be recoverable. Coverage ranges in [0, 1], and reference simplifications in the paired Newsela corpus obtain an average score of 0.76, confirming that manual simplification can achieve high coverage."
    }, {
      "heading" : "3.4 Guardrails",
      "text" : "We use guardrails as simple pattern-based scores to avoid common pathological generation problems that we observed. Unlike the main components, guardrails are binary, giving a score of 1 (pass) unless they trigger (score of 0). We use two guardrails: brevity and inaccuracy."
    }, {
      "heading" : "3.4.1 Brevity guardrail",
      "text" : "The brevity guardrail ensures the length of generated paragraph (L2) falls in a range around the original paragraph’s length (L1). We compute a compression ratio: C = L2/L1. If Cmin ≤ C ≤ Cmax, the guardrail passes, otherwise it triggers.\nWe set [Cmin, Cmax] = [0.6, 1.5], because these values ensure the guardrail is not triggered on 98% of the paired Newsela dataset; this can be adapted depending on the application."
    }, {
      "heading" : "3.4.2 Inaccuracy guardrail",
      "text" : "Modern text generation models are known to hallucinate facts (Huang et al., 2020), which has led the community to create models to detect and correct hallucinations (Cao et al., 2020; Zhang et al., 2020; Wang et al., 2020).\nWe propose a light-weight inaccuracy detector as a guardrail. We use a Named Entity Recognition (NER) model (Honnibal et al., 2020) to extract entities present in the original paragraph (E1) and the model’s output (E2). We trigger the guardrail if an entity present in E2 is not in E1.\nEven though human writers can successfully introduce new entities without creating inaccuracies (e.g., replacing the city La Paz with the country Bolivia), we find that text generators predominantly introduce inaccuracies with novel entities. This simple heuristic can eventually be replaced once inaccuracy detection technology matures."
    }, {
      "heading" : "4 KiS Training",
      "text" : "Rennie et al. (2017) introduced Self-Critical Sequence Training (SCST) as an effective algorithm for reward-based training of text generators, successfully applying it to image captioning. The efficacy of SCST was later confirmed on other text generation tasks such as question generation (Zhang and Bansal, 2019), and summarization (Celikyilmaz et al., 2018; Laban et al., 2020). In SCST, a probabilistic model is used to generate two distinct candidates: CS , a candidate constructed by sampling the word distribution at each step, and Ĉ, by taking the argmax of the word distribution at each step. Each candidate is scored, obtaining rewards of RS and R̂, respectively, and the loss is:\nL = (R̂−RS) N∑ i=0 log p(wSi |wS1 ...wSi−1, P ) (5)\nwhere p(wSi |...) represents the probability of the i-th word conditioned on previously generated sampled sequence according to the model, P is the input paragraph, and N the number of words in the generated sequence. Intuitively, minimizing this loss increases the likelihood of the sampled sequence if RS > R̂, and decreases it otherwise, both increasing the expected total reward.\nOne limitation in SCST occurs when the two sequences achieve comparable rewards (RS ' R̂): the loss nears zero, and the model has little to learn, wasting a training sample. In our experiments with SCST, this can occur with 30% of samples.\nWe propose an extension of SCST, which we call k-SCST. We generate k sampled candidates (k > 2), compute the rewards of each candidate RS1, ..., RSk, as well as the mean reward achieved\nby this sampled population: R̄S = (RS1 + ... + RSk)/k, which we use as the baseline, instead of R̂. The loss L becomes:\nL = k∑ j=1 (R̄S−RSj) N∑ i=0 log p(wSji |w Sj 1 ...w Sj i−1, P )\n(6) We use a GPT2-medium for the generator, initialized with the released pre-trained checkpoint. Experimental details such as data and optimizer used are provided in Appendix A.1.\nIn Figure 5, we show results of a direct comparison of SCST (k = 2) with k-SCST varying k in {4, 6, 8}, while keeping other components of the training fixed. Because of the variance involved in RL training, we recorded six independent training runs for each setting (for a total of 24 runs), and plot the average reward across runs of a setting, as well as the standard error of the mean (SEM).\nWe observe that increasing k leads to higher average reward, and less variation in the reward. In our setting, k-SCST boosts performance and stabilizes training. We use k = 8 in all final models, as increasing k further is impractical due to GPU memory limitations.\nWe believe k-SCST’s advantage stems from two factors: first, obtaining a better estimate of the distribution of rewards by sampling more outputs, second, by using the mean reward as the baseline, saving on computation of a separate baseline generation. We believe k-SCST can also improve learning in other text generation applications and plan to pursue this in future work."
    }, {
      "heading" : "5 Experiments",
      "text" : "We present results experimentally validating the KiS procedure for text simplification. We give results based on automatic metrics, on a novel human comprehension task, and from an ablation study."
    }, {
      "heading" : "5.1 Models Compared",
      "text" : "We compare the KiS Model to three strong supervised models, and an unsupervised approach.\nACCESS from (Martin et al., 2020), is a stateof-the-art Transformer model trained on WikiLarge (300,000 pairs of complex/simple sentences). This model uses default parameters (NBChar=0.95, LevSim=0.75).\nACCESS90 is identical to ACCESS, with different parameters (NBChar=0.90, LevSim=0.75), reducing target compression from 95% to 90%, matching the average compression rate in Newsela.\nFinetune Baseline is a GPT2-medium model finetuned on the paired Newsela dataset. Large pre-trained models often perform competitively in low-resource environments, making this a strong point of comparison.\nUnsup NTS from (Surya et al., 2019) is an unsupervised approach based on successively encoding and denoising text using a GRU architecture.\nTraining details for the KiS Model and Finetune Baseline are in Appendix A.1."
    }, {
      "heading" : "5.2 Automatic Results",
      "text" : "We put aside 500 samples from the paired Newsela dataset as a test set to compare models on automatic metrics. We compare models on SARI and BLEU, report the percentage when readability measures see an improvement in readability: %FKGL, and %Lexile and compute the average compression rate (Comp.), and coverage (Cov.). Results are summarized in Table 1.\nThe KiS model achieves the highest SARI score by a margin of 0.04, even though it is an unsupervised approach.\nFinetune Baseline achieves the highest BLEU and salience scores, but lowest SARI score. We interpret this as showing the model takes the least risk: high salience, with little simplification.\nWe observe that all models are able to increase readability in terms of FKGL and Lexile compared to original paragraphs. We note that for almost all models, the percentage is lower for the Lexile measure than for FKGL, showing that an improvement in Lexile score is more difficult to achieve than FKGL. The KiS model achieves an increase in Lexile readability 72% of the time, the closest figure to 79% of the Newsela human-written reference.\nWe note that the perfect performance of KiS on %FKGL could be explained by the fact that FKGL is a part of a component being optimized (SScore), however Lexile was not.\nIn terms of compression, the KiS model compresses the second most, most likely hurting its coverage. Adjusting the Brevity guardrail could encourage the model to compress less. ACCESS90 has the compression rate closest to Newsela references, but this only leads to a modest improvement in SARI when compared to ACCESS.\nOverall, the Newsela references achieve the best percentage of Lexile readability improvement, while outperforming the KiS model at coverage: there is still a gap between human-written simplifications and model-generated ones."
    }, {
      "heading" : "5.3 Human Comprehension Study",
      "text" : "We propose a human comprehension study to evaluate the usefulness of simplification results. Simplified text should be easier to read than the original text, while retaining accuracy and understanding. We design a task to evaluate how well both manual and automated simplifications achieve this objective. The main idea is to show readers a text and ask them to answer multiple-choice questions, evaluating the texts based on time and retries needed to select the correct answer."
    }, {
      "heading" : "5.3.1 Study Design",
      "text" : "Five different versions of each document were generated as stimuli: the original document, the Newsela reference, and versions from the three best-performing methods from the last section: KiS, Finetune Baseline, and ACCESS. We did not include Unsup NTS in our analysis, because of its low performance on %FKGL and %Lexile metrics. Associated with each document are five manually generated multiple-choice questions, each with one or more correct answers and one to four distractors. The original and the Newsela texts were checked manually by experimenters to ensure that all allow for questions to be answered correctly. Crowdworkers were shown four documents in succession, in a between-participants design. Order of document and stimuli type were randomized. Figure 6 shows two stimuli of a document (original and KiS) along with the comprehension questions. (The entire set of five stimuli can be found in Figure A2 in the Appendix.)\nAfter several rounds of pilot testing, we arrived at the following design choices:\nDocument theme. We chose recent news articles involving complex themes (e.g., trajectory of iceberg) as the source of documents. For news articles, recency seems to engage participants, and\ntechnical terms increase the impact of simplification.\nSection length. We chose document length of 3-4 paragraphs (or 200 words), and five comprehension questions. Document length should not be too short (makes some questions trivial), or too long (adds a retrieval component to the task).\nSelection of questions. Questions were generated via a GPT2 question generation model finetuned on the NewsQA dataset (Trischler et al., 2017). We select questions answerable by both the original and Newsela references, attempting to have both factoid (answer is entity) and reasoning questions.\nRe-submission until correct. When submitting answers, participants received feedback on which were incorrect, and were required to re-submit until all answers were correct. This aligns the objective of the participant (i.e., finishing the task rapidly), with the task’s objective (i.e., measuring participant’s efficiency at understanding). This also gives a way to discourage participants from “bruteforcing” the task, re-submitting many combinations until one works.\nWe note that some components of the study such as the choice of document themes and the selection of comprehension questions are elements that create variability in the results. We release the models used in the study, as well all generated texts that were evaluated to enable follow-up research and to aid reproducibility."
    }, {
      "heading" : "5.3.2 Study Results",
      "text" : "We ran the study on Mechanical Turk, accepting crowd-workers with 1700+ completed tasks, and an acceptance rate of 97%+. The study was active for two weeks in December 2020, and remunerated participants completing all four sections at a rate of $10/hour. (Appendix A.2 shows crowd-worker instructions and the document/version distributions.) When removing “brute-forced” submissions (10+ re-submissions), we are left with 244 submissions, used for result analysis reported in Table 2, (A more detailed results table is included in Appendix A.4.)\nWe measure two outcomes: question completion time (in seconds), and number of submissions to correctness. We performed a Kruskal-Wallis test (Kruskal and Wallis, 1952) with a Dunn posthoc test (Dunn, 1964) for statistical significance between pairs of conditions.\nIn line with study objectives, simplified texts\nhelp participants complete the task faster than reading original texts, with three of the four simplified versions leading to improvements in completion times. Participants were fastest with KiS simplifications (18% faster). The KiS model led to a statistically significant speed-up compared to the originals, Newsela references, and ACCESS simplifications. ACCESS simplifications surprisingly led to a non-significant slow-down, which we attribute to a potential loss in fluency that might have confused participants.\nOne important factor we consider is that shorter passages (i.e., smaller compression) might lead to a speed-up regardless of simplicity. We confirm this by finding a small positive correlation between passage length and completion time of 0.09. We compute a compression-adjusted speed-up (CASpeed) ratio by: (1) computing the passage length of each simplified version, (2) linearly extrapolating the expected completion time for this passage length for original paragraphs, and (3) computing the ratio of the extrapolation to the observed completion time. If CASpeed > 1, participants were faster than expected for the passage length. Newsela reference paragraphs achieve the best CASpeed, followed by the KiS model. This suggests that good simplification can involve making texts longer."
    }, {
      "heading" : "5.4 Ablation Study",
      "text" : "We train three ablated models, each missing a reward component to gain understanding in the value of each component of the KiS procedure.\nFigure 1 gives a qualitative perspective on each ablation. Without fluency, the generator learns to generate incomplete sentences, without salience, it omits important information, and without simplicity, it can sometimes “complexify”.\nWe computed complete automatic results for the ablated models, and find that each ablation leads to a decrease on an evaluation metric, confirming that all three components are necessary to generate highquality simplifications (details in Appendix A.5)."
    }, {
      "heading" : "6 Limitations and Future Work",
      "text" : "Improved Accuracy Scoring. The current guardrail for inaccuracy is rudimentary; trained models still generate non-factual simplifications. Recent work in fact-checking for the summarization domain (Kryscinski et al., 2020; Li et al., 2018) could be adapted to the simplification domain to improve this.\nInclusion of Supervised Signal. In this work, we establish that text simplification can be approached in an unsupervised manner. In future work, Keep it Simple could be used as a pretraining strategy, or used jointly with supervised training.\nReproducibility of Human Evaluation. Even though we release the models, stimuli and comprehension questions used in the human evaluation, some elements of the procedure introduce randomness. Participating crowd-workers differ in literacy level which may have an effect on their performance at the task (Alonzo et al., 2021).\nNew Settings, Domains and Languages. We limited our experiments to the simplification of English news articles following prior work, but plan to pursue other languages in the future. Similarly, because Keep it Simple does not require labeled data, it can be applied to new settings (e.g., rewriting to inverse the effects of simplification), or to new domains (e.g., legal texts)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have shown that text simplification can be approached in an unsupervised manner via KiS. By optimizing a reward comprised of simplicity, fluency and salience components, KiS is able to outperform strong supervised models on automatic metrics (+0.04 in SARI). We propose a human comprehension task to evaluate the usefulness of simplification and show that simplifications tend to lead to a measurable speed-up in task completion, with KiS texts producing the best speed-up of 18% on average. These are first steps for unsupervised text simplification, and we suggest that future work should focus on adapting the methodology to new domains (i.e., legal), non-English languages, and refining optimized rewards to take factuality into account."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Katie Stasaski, Dongyeop Kang, and the ACL reviewers for their helpful comments, as well as Newsela for providing a version of their simplified news corpus. This work was supported by a Microsoft BAIR Commons grant as well as a Microsoft Azure Sponsorship."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Training Details\nWe detail the model architecture size, data, optimizer of the models we train in the paper. All models were trained using Pytorch and HuggingFace’s Transformers library5. We use the Apex6 library to enable half-precision training. The KiS procedure was trained on a single GPU, either an Nvidia V-100 (16Gb memory) or a Quadro RTX 8000 (48 Gb memory). We ran a total of around 200 experiments, with an average run-time of one week.\nBecause the procedure is unsupervised, the model was trained using a large unreleased corpus of news articles, containing 7 million news articles in English.\nKiS Model is initialized with a GPT2-medium model. We used the Adam optimizer, with a learning rate of 10−6, a batch-size of 1, using k-SCST with k = 8.\nFinetune Baseline is initialized with a GPT2medium model. We train using using standard teacher forcing on the 40,000 samples in the paired Newsela dataset, reserving 2,000 samples for validation. We use the Adam optimizer, and use the\n5https://github.com/huggingface/transformers 6https://github.com/nvidia/apex\nvalidation set to choose a learning rate of 10−5, and a batch-size of 8, and run for 3 epochs before seeing a plateau in the validation loss.\nDiscriminator Model is initialized with a Roberta-base, and retrained every time the training buffer reaches 2,000 samples. The discriminator is reset to the original Roberta-base each time the training buffer is full. We use a standard cross-entropy loss, the ADAM optimizer with a learning rate of 10−5 and a batch size of 8. Each time we retrain, we run for 5 epochs, and checkpoint one model after each epoch. The checkpoint that achieves the highest performance on a validation set becomes the new discriminator for the next round.\nA.2 Human Evaluation Instructions Figure A1 shows the instructions given to crowdworker participants for the manual evaluation.\n• The entire HIT should take no more than 15 minutes: (1) You will answer a pre-questionnaire. (2) Read 4 short news stories and answer comprehension questions about each. • If you believe the answer is not in the document, you can select the option “Answer not in document”. • There is no time limit for each individual document or question. • You can leave at any point but will not complete the HIT. • You can complete this task at most once. • If you have a question/problem, contact us at email.\nFigure A1: Instructions given to participants of the comprehension evaluation. Participants were recruited on Amazon Mechanical Turk (MTurk), on which jobs are named “HIT”.\nA.3 Full Example of Generated Texts Figure A2 is a complement to Figure 6, with the five stimuli that were shown for the Covid Libraries document.\nA.4 Detailed of Human Evaluation Results Table A1 details the timing and number of participants for each combination of document and stimuli.\nORIGINAL [Lexile Grade 11] Each summer, libraries in St. Louis, Missouri, host many types of free camps — yoga, chess and even a Harry Potter “Sorting Hat Camp.” In 2020, camp dreams seemed far-fetched given the global coronavirus pandemic. That didn’t stop St. Louis libraries, though. Instead of canceling, they brought camp into kids’ homes. So children who signed up for ukulele camp got a beginner’s guidebook, instructional DVD and an actual ukulele in the mail. It was all free. In addition, camp sessions still occurred. Advisers met with kids using virtual formats. Joe Monahan, manager of youth services for the St. Louis library system, says that of the 70 camps originally scheduled, 54 were held virtually. Paula Langsam, a youth services manager at the soon-to-reopen Martin Luther King Junior Memorial Library in Washington, D.C., says, “In a way, our work has changed a lot. We didn’t used to do videos a lot.”\nWho manages the St Louis library kids programs? Joe Monahan, Paula Langsam, St. Louis Camp Leaders\nWere any camps in St. Louis cancelled? Yes, No\nHow many camps were scheduled, how many were run? 54 and 70, 70 and 54, 70 and 0, 54 and 0\nHow did the Ukulele camp meet? In the park, Virtually, Did not meet\nWhat camps did the libraries host? Yoga, Chess, Pottery, Ukulele\nKIS MODEL [Lexile Grade 9] In the summer months, St. Louis has many free classes for kids, including yoga, chess and a Harry Potter “Sorting Hat Camp.” In 2020, camp dreams again seemed far-fetched given the crisis. That didn’t stop St. Louis libraries, though. They brought camp in. So kids who signed up for ukulele camp got a beginner’s guidebook, a lesson DVD and a real ukulele in the mailbox. It was all free. In addition, camp sessions continued. Advisers tried out a virtual format. Joe Monahan, the manager of youth services for the St. Louis library system, says that of the 70 camps originally scheduled, 54 were held mostly. Paula Langsam, a youth services manager at the Martin Luther King Junior library, says, “In a way, our work changed a lot. We didn’t do videos a lot.”\nNEWSELA [Lexile Grade 7] In St. Louis, Missouri, libraries hold many types of free camps in the summer. They have yoga, chess and even a Harry Potter camp. In 2020, camp ideas seemed unlikely due to the spread of COVID-19. That did not stop St. Louis libraries, though. They did not cancel their programs. They brought camp into kids' homes. Children who signed up for ukulele camp got a beginner's book, instructional video and a ukulele in the mail. It was all free. In addition, camp sessions still went on. Camp leaders met with kids over video calls. Joe Monahan runs the kids programs for the St. Louis libraries. He said 70 camps were scheduled at first. Some were dropped but 54 were still held online. Paula Langsam works at the Martin Luther King Junior Memorial Library in Washington, D.C. It reopened on September 24. She said, \"Our work has changed a lot. We didn't used to do videos a lot.\"\nFINETUNE BASELINE [Lexile Grade 9] Each summer, St. Louis libraries host many types of free camps — yoga, chess and even a Harry Potter \"Sorting Hat Camp.\" In 2020, camp dreams seemed far-fetched given the global cough . However , the future looked bright for libraries in St. Louis. Instead of canceling, they brought camp into kids' homes. This is called ukulele camp. Kids signed up for it in the mail. They got a beginner's guidebook, video and an actual ukulele to play. It was all free. Advisers met with kids using virtual formats. Joe Monahan, manager of youth services for the St. Louis library system, says that 70 camps were originally planned. Of those, 54 were held mostly. Paula Langsam is a youth services manager who works at the Martin Luther King Junior Memorial Library in Washington, D.C. She says, \"In a way, our work has changed a lot. We didn't used to do videos a lot.\"\nACCESS [Lexile Grade 11] Each summer, libraries in St. Louis, Missouri, has many different types of free camps that are yoga, chess and even a Harry Potter gang Sorting Hat Camp. In 2020, camp dreams seemed far-fetched that there was the global coronavirus pandemic. That did not stop St. Louis libraries, though. Instead of being canceled, they brought camp into children's homes. So children who signed up for ukulele camp got a guidebook. They also had an actual ukulelele in the mail. It was all free. In addition, camp meetings still happened. Advisers met with new children using virtual formats. Joe Monahan, also known as Joe Monahan, has youth services for the St. Louis library system says that of the 70 camps first started, 54 were held. Paula Langsam, also known as Paula Langsam, is a youth services manager at the soon-to-reopen Martin Luther King Junior Library in Washington, D. We did not use to do many videos a lot.\nFigure A2: Complement to Figure 6. Example Task for the Comprehension Study. Participants were assigned to one of five settings: original, Newsela, KiS, Finetune Baseline, and ACCESS. Participants were instructed to answer the five comprehension questions.\nSimplification Model Document Id Original Newsela Sup. Base. ACCESS KiS Marvel Show 152 (12) 209 (11) 140 (11) 209 (14) 126 (13) Covid Libraries 167 (14) 180 (12) 182 (10) 190 (13) 171 (12) Sustainable Food 163 (13) 144 (10) 181 (13) 242 (13) 154 (12) Iceberg Collision 208 (14) 116 (11) 139 (12) 104 (12) 119 (12) Version Aggregate 174 (53) 163 (44) 161 (46) 188 (52) 143 (49)\nTable A1: Average time taken and number of participants in each of the document/stimuli combinations. Also shown are aggregates (mean time taken and total number of participants).\nModel SARI BLEU %FKGL %Lexile Comp. Cov. KiS Full 0.709 0.526 100 72 0.85 0.636 KiS No Fluency 0.718 0.611 99 95 1.02 0.901 KiS No Salience 0.695 0.591 100 65 1.01 0.701 KiS No Simplicity 0.672 0.617 51 23 0.92 0.809\nTable A2: Automatic results of the three ablation models. SARI and BLEU are reference-based metrics. % FKGL and % Lexile are the percentage of simplified paragraphs with a lower FKGL and Lexile score than the original paragraph. Comp. is the average compression ratio (# of words), and Cov. is the average coverage score of the simplifications.\nA.5 Detail of Ablation Study Results Table A2 details the metric results of the three ablated models, an extension to Table 1. An example output of each ablated model, illustrating the limitation when a score component is missing, is given in Figure 1.\nOne surprising element is that the model trained without fluency achieves higher scores on almost all metrics, compared to the full model. This surprising fact is due to the fact that without fluency, the model does not learn to generate full sentences (see the example in Figure 1). Instead, the model learns to concatenate high-scoring phrases together, which can boost automatic metrics artificially. In fact, the strong performance of a model generating incomplete sentences reveals a limitation of current automatic metrics, such as BLEU and SARI."
    } ],
    "references" : [ {
      "title" : "Comparison of methods for evaluating complexity of simplified texts among deaf and hard-of-hearing adults at different literacy levels",
      "author" : [ "Oliver Alonzo", "Jessica Trussell", "Becca Dingman", "Matt Huenerfauth." ],
      "venue" : "Proceedings of the 2021 CHI Conference",
      "citeRegEx" : "Alonzo et al\\.,? 2021",
      "shortCiteRegEx" : "Alonzo et al\\.",
      "year" : 2021
    }, {
      "title" : "Lexico-syntactic text simplification and compression with typed dependencies",
      "author" : [ "Mandya Angrosh", "Tadashi Nomoto", "Advaith Siddharthan." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tech-",
      "citeRegEx" : "Angrosh et al\\.,? 2014",
      "shortCiteRegEx" : "Angrosh et al\\.",
      "year" : 2014
    }, {
      "title" : "Word frequency and word difficulty: A comparison of counts in four corpora",
      "author" : [ "H. Breland." ],
      "venue" : "Psychological Science, 7:96 – 99.",
      "citeRegEx" : "Breland.,? 1996",
      "shortCiteRegEx" : "Breland.",
      "year" : 1996
    }, {
      "title" : "Moving beyond kucera and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english",
      "author" : [ "M. Brysbaert", "B. New." ],
      "venue" : "Behavior Research Methods, 41:977–990.",
      "citeRegEx" : "Brysbaert and New.,? 2009",
      "shortCiteRegEx" : "Brysbaert and New.",
      "year" : 2009
    }, {
      "title" : "Factual error correction for abstractive summarization models",
      "author" : [ "Meng Cao", "Yue Dong", "Jiapeng Wu", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251–6258.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep communicating agents for abstractive summarization",
      "author" : [ "Asli Celikyilmaz", "Antoine Bosselut", "Xiaodong He", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Celikyilmaz et al\\.,? 2018",
      "shortCiteRegEx" : "Celikyilmaz et al\\.",
      "year" : 2018
    }, {
      "title" : "A formula for predicting readability: Instructions",
      "author" : [ "Edgar Dale", "Jeanne S Chall." ],
      "venue" : "Educational research bulletin, pages 37–54.",
      "citeRegEx" : "Dale and Chall.,? 1948",
      "shortCiteRegEx" : "Dale and Chall.",
      "year" : 1948
    }, {
      "title" : "Editnts: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "author" : [ "Yue Dong", "Zichao Li", "Mehdi Rezagholizadeh", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Multiple comparisons using rank sums",
      "author" : [ "Olive Jean Dunn." ],
      "venue" : "Technometrics, 6(3):241–252.",
      "citeRegEx" : "Dunn.,? 1964",
      "shortCiteRegEx" : "Dunn.",
      "year" : 1964
    }, {
      "title" : "Yats: yet another text simplifier",
      "author" : [ "Daniel Ferrés", "Montserrat Marimon", "Horacio Saggion" ],
      "venue" : "In International Conference on Applications of Natural Language to Information Systems,",
      "citeRegEx" : "Ferrés et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ferrés et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Courville", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 27.",
      "citeRegEx" : "Courville and Bengio.,? 2014",
      "shortCiteRegEx" : "Courville and Bengio.",
      "year" : 2014
    }, {
      "title" : "The fog index after twenty years",
      "author" : [ "Robert Gunning." ],
      "venue" : "Journal of Business Communication, 6(2):3–",
      "citeRegEx" : "Gunning.,? 1969",
      "shortCiteRegEx" : "Gunning.",
      "year" : 1969
    }, {
      "title" : "Dynamic multi-level multi-task learning for sentence simplification",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 462–476.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "spaCy: Industrial-strength Natural Language Processing in Python",
      "author" : [ "Matthew Honnibal", "Ines Montani", "Sofie Van Landeghem", "Adriane Boyd." ],
      "venue" : "Doi.org/10.5281/zenodo.1212303.",
      "citeRegEx" : "Honnibal et al\\.,? 2020",
      "shortCiteRegEx" : "Honnibal et al\\.",
      "year" : 2020
    }, {
      "title" : "What have we achieved on text summarization",
      "author" : [ "Dandan Huang", "Leyang Cui", "Sen Yang", "Guangsheng Bao", "Kun Wang", "Jun Xie", "Yue Zhang" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Huang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence-level fluency evaluation: References help, but can be spared",
      "author" : [ "Katharina Kann", "Sascha Rothe", "Katja Filippova" ],
      "venue" : "In Proceedings of the 22nd Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Kann et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2018
    }, {
      "title" : "Librarians find creative ways to serve kids when buildings are closed for browsing",
      "author" : [ "Haben Kelati." ],
      "venue" : "The Washington Post.",
      "citeRegEx" : "Kelati.,? 2020",
      "shortCiteRegEx" : "Kelati.",
      "year" : 2020
    }, {
      "title" : "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "author" : [ "J. Peter Kincaid", "Robert P. Fishburne Jr.", "Richard L. Rogers", "Brad S. Chissom." ],
      "venue" : "Technical report, Naval",
      "citeRegEx" : "Kincaid et al\\.,? 1975",
      "shortCiteRegEx" : "Kincaid et al\\.",
      "year" : 1975
    }, {
      "title" : "Use of ranks in one-criterion variance analysis",
      "author" : [ "William H Kruskal", "W Allen Wallis." ],
      "venue" : "Journal of the American statistical Association, 47(260):583– 621.",
      "citeRegEx" : "Kruskal and Wallis.,? 1952",
      "shortCiteRegEx" : "Kruskal and Wallis.",
      "year" : 1952
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "The summary loop: Learning to write abstractive summaries without examples",
      "author" : [ "Philippe Laban", "Andrew Hsi", "John Canny", "Marti A. Hearst." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5135–",
      "citeRegEx" : "Laban et al\\.,? 2020",
      "shortCiteRegEx" : "Laban et al\\.",
      "year" : 2020
    }, {
      "title" : "Nasa curiosity rover celebrates 3,000th day on mars with stunning panorama of planet",
      "author" : [ "Sophie Lewis." ],
      "venue" : "CBS News.",
      "citeRegEx" : "Lewis.,? 2021",
      "shortCiteRegEx" : "Lewis.",
      "year" : 2021
    }, {
      "title" : "Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization",
      "author" : [ "Haoran Li", "Junnan Zhu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguis-",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Y. Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable sentence simplification",
      "author" : [ "Louis Martin", "Éric Villemonte de la Clergerie", "Benoı̂t Sagot", "Antoine Bordes" ],
      "venue" : "In Proceedings of The 12th Language Resources and Evaluation Conference,",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Lexical simplification with pretrained encoders",
      "author" : [ "Jipeng Qiang", "Yun Li", "Yi Zhu", "Yunhao Yuan", "Xindong Wu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8649–8656.",
      "citeRegEx" : "Qiang et al\\.,? 2020",
      "shortCiteRegEx" : "Qiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Self-critical sequence training for image captioning",
      "author" : [ "Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jerret Ross", "Vaibhava Goel." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008–7024.",
      "citeRegEx" : "Rennie et al\\.,? 2017",
      "shortCiteRegEx" : "Rennie et al\\.",
      "year" : 2017
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712.",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "The lexile framework for reading: An introduction to what it is and how to use",
      "author" : [ "Malbert Smith", "J. Turner", "Eleanor E. Sanford-Moore", "Heather H. Koons" ],
      "venue" : null,
      "citeRegEx" : "Smith et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2016
    }, {
      "title" : "Seq2edits: Sequence transduction using span-level edit operations",
      "author" : [ "Felix Stahlberg", "Shankar Kumar." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5147–5159.",
      "citeRegEx" : "Stahlberg and Kumar.,? 2020",
      "shortCiteRegEx" : "Stahlberg and Kumar.",
      "year" : 2020
    }, {
      "title" : "Unsupervised neural text simplification",
      "author" : [ "Sai Surya", "Abhijit Mishra", "Anirban Laha", "Parag Jain", "Karthik Sankaranarayanan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2058–2068.",
      "citeRegEx" : "Surya et al\\.,? 2019",
      "shortCiteRegEx" : "Surya et al\\.",
      "year" : 2019
    }, {
      "title" : "Wordnet-based lexical simplification of a document",
      "author" : [ "S Rebecca Thomas", "Sven Anderson." ],
      "venue" : "KONVENS, pages 80–88.",
      "citeRegEx" : "Thomas and Anderson.,? 2012",
      "shortCiteRegEx" : "Thomas and Anderson.",
      "year" : 2012
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages",
      "citeRegEx" : "Trischler et al\\.,? 2017",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2017
    }, {
      "title" : "Asking and answering questions to evaluate the factual consistency of summaries",
      "author" : [ "Alex Wang", "Kyunghyun Cho", "Mike Lewis." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence simplification by monolingual machine translation",
      "author" : [ "Sander Wubben", "Antal van den Bosch", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1015–",
      "citeRegEx" : "Wubben et al\\.,? 2012",
      "shortCiteRegEx" : "Wubben et al\\.",
      "year" : 2012
    }, {
      "title" : "Problems in current text simplification research: New data can help",
      "author" : [ "W. Xu", "Chris Callison-Burch", "Courtney Napoles." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:283–297.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:401–415.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Addressing semantic drift in question generation for semisupervised question answering",
      "author" : [ "Shiyue Zhang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Zhang and Bansal.,? 2019",
      "shortCiteRegEx" : "Zhang and Bansal.",
      "year" : 2019
    }, {
      "title" : "Sentence simplification with deep reinforcement learning",
      "author" : [ "Xingxing Zhang", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594.",
      "citeRegEx" : "Zhang and Lapata.,? 2017",
      "shortCiteRegEx" : "Zhang and Lapata.",
      "year" : 2017
    }, {
      "title" : "Optimizing the factual correctness of a summary: A study of summarizing radiology reports",
      "author" : [ "Yuhao Zhang", "Derek Merck", "Emily Tsai", "Christopher D Manning", "Curtis Langlotz." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Discourse level factors for sentence deletion in text simplification",
      "author" : [ "Yang Zhong", "Chao Jiang", "Wei Xu", "Junyi Jessy Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9709–9716.",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "A monolingual tree-based translation model for sentence simplification",
      "author" : [ "Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361.",
      "citeRegEx" : "Zhu et al\\.,? 2010",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2010
    }, {
      "title" : "Missouri, libraries hold many types of free camps in the summer. They have yoga, chess and even a Harry Potter camp",
      "author" : [ "St. Louis" ],
      "venue" : "NEWSELA [Lexile Grade",
      "citeRegEx" : "Louis,? \\Q2020\\E",
      "shortCiteRegEx" : "Louis",
      "year" : 2020
    }, {
      "title" : "camp dreams seemed far-fetched given the global cough",
      "author" : [ "Harry Potter \"Sorting Hat Camp" ],
      "venue" : "Of those,",
      "citeRegEx" : "Camp.,? \\Q2020\\E",
      "shortCiteRegEx" : "Camp.",
      "year" : 2020
    }, {
      "title" : "Langsam is a youth services manager who works at the Martin Luther King Junior Memorial Library in Washington, D.C. She says, \"In a way, our work has changed a lot. We didn't used to do videos a lot.\" ACCESS [Lexile Grade 11] Each summer, libraries in St. Louis, Missouri, has many different types of free camps that are yoga, chess and even a Harry Potter gang Sorting Hat Camp",
      "author" : [ "mostly. Paula" ],
      "venue" : null,
      "citeRegEx" : "Paula,? \\Q2020\\E",
      "shortCiteRegEx" : "Paula",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Figure 1: Motivating example for the KiS method, based on a CBS article (Lewis, 2021).",
      "startOffset" : 72,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : "(Xu et al., 2016) and the Flesch-Kincaid Grade Level (Kincaid et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 17,
      "context" : ", 2016) and the Flesch-Kincaid Grade Level (Kincaid et al., 1975).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "A second contribution is a new algorithm for RLbased training of text generators, k-SCST, which is an extension of Self-Critical Sequence Training (Rennie et al., 2017).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 42,
      "context" : "Early datasets were first based on Simple Wikipedia2: WikiSmall (Zhu et al., 2010), later expanded into WikiLarge (Zhang and Lapata, 2017).",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 39,
      "context" : ", 2010), later expanded into WikiLarge (Zhang and Lapata, 2017).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 42,
      "context" : "Text simplification is most commonly framed as a sequence-tosequence (seq2seq) task, leveraging model architectures of other seq2seq tasks, such as natural machine translation (Zhu et al., 2010; Wubben et al., 2012).",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 35,
      "context" : "Text simplification is most commonly framed as a sequence-tosequence (seq2seq) task, leveraging model architectures of other seq2seq tasks, such as natural machine translation (Zhu et al., 2010; Wubben et al., 2012).",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : "Lexical Simplification focuses on the substitution of single words or phrases with simpler equivalents, with diverse approaches using lexical databases such as WordNet (Thomas and Anderson, 2012), to using contextualized word vectors (Qiang et al.",
      "startOffset" : 168,
      "endOffset" : 195
    }, {
      "referenceID" : 26,
      "context" : "Lexical Simplification focuses on the substitution of single words or phrases with simpler equivalents, with diverse approaches using lexical databases such as WordNet (Thomas and Anderson, 2012), to using contextualized word vectors (Qiang et al., 2020).",
      "startOffset" : 234,
      "endOffset" : 254
    }, {
      "referenceID" : 7,
      "context" : "Recent work (Dong et al., 2019; Stahlberg and Kumar, 2020) has modeled text simplification as a text-edit task, learning sequences of word-edits that transform the input into the output.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 30,
      "context" : "Recent work (Dong et al., 2019; Stahlberg and Kumar, 2020) has modeled text simplification as a text-edit task, learning sequences of word-edits that transform the input into the output.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 39,
      "context" : "Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "Automatic evaluations usually involve using n-gram overlap calculations such as BLEU (Papineni et al., 2002) and SARI (Xu et al.",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 39,
      "context" : "SARI was shown to correlate better with human judgements of simplicity than BLEU, and it has since become a standard (Zhang and Lapata, 2017; Surya et al., 2019; Martin et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 31,
      "context" : "SARI was shown to correlate better with human judgements of simplicity than BLEU, and it has since become a standard (Zhang and Lapata, 2017; Surya et al., 2019; Martin et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 24,
      "context" : "SARI was shown to correlate better with human judgements of simplicity than BLEU, and it has since become a standard (Zhang and Lapata, 2017; Surya et al., 2019; Martin et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 182
    }, {
      "referenceID" : 31,
      "context" : ", by directly rating factors like fluency, simplicity and relevance of model outputs (Surya et al., 2019; Wubben et al., 2012).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 35,
      "context" : ", by directly rating factors like fluency, simplicity and relevance of model outputs (Surya et al., 2019; Wubben et al., 2012).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "We follow prior work (Ferrés et al., 2016) and organize our score into a syntactic score SScore, and a lexical score LScore.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Other readability metrics such as Dale-Chall formula (Dale and Chall, 1948), or the Gunning-Fog index (Gunning, 1969) could be used, and future work could examine the effect of choosing one readability metric over the",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Other readability metrics such as Dale-Chall formula (Dale and Chall, 1948), or the Gunning-Fog index (Gunning, 1969) could be used, and future work could examine the effect of choosing one readability metric over the",
      "startOffset" : 102,
      "endOffset" : 117
    }, {
      "referenceID" : 29,
      "context" : "Another viable option is the Lexile score (Smith et al., 2016), however, because its implementation is not publicly released, we cannot use it during training and we report it only for evaluation (done manually on the Lexile Hub4).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "We rely on the observation that word frequency and difficulty are correlated (Breland, 1996), and use word frequency in a large corpus of text (Brysbaert and New, 2009) to determine simplicity.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "We rely on the observation that word frequency and difficulty are correlated (Breland, 1996), and use word frequency in a large corpus of text (Brysbaert and New, 2009) to determine simplicity.",
      "startOffset" : 143,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "This probability is often used to measure fluency of generated text (Kann et al., 2018; Salazar et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "This probability is often used to measure fluency of generated text (Kann et al., 2018; Salazar et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "Specifically, we use a RoBERTa model (Liu et al., 2019) as the basis for the discriminator, a classifier with two labels: 1 for authentic paragraphs, and 0 for generator outputs.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "For the salience component, we use the coverage model introduced in the summary loop (Laban et al., 2020) for the domain of text summarization, and adapt it to the simplification domain.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "Modern text generation models are known to hallucinate facts (Huang et al., 2020), which has led the community to create models to detect and correct hallucinations (Cao et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : ", 2020), which has led the community to create models to detect and correct hallucinations (Cao et al., 2020; Zhang et al., 2020; Wang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 148
    }, {
      "referenceID" : 40,
      "context" : ", 2020), which has led the community to create models to detect and correct hallucinations (Cao et al., 2020; Zhang et al., 2020; Wang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 148
    }, {
      "referenceID" : 34,
      "context" : ", 2020), which has led the community to create models to detect and correct hallucinations (Cao et al., 2020; Zhang et al., 2020; Wang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "We use a Named Entity Recognition (NER) model (Honnibal et al., 2020) to extract entities present in the original paragraph (E1) and the model’s output (E2).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : "The efficacy of SCST was later confirmed on other text generation tasks such as question generation (Zhang and Bansal, 2019), and summarization (Celikyilmaz et al.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "The efficacy of SCST was later confirmed on other text generation tasks such as question generation (Zhang and Bansal, 2019), and summarization (Celikyilmaz et al., 2018; Laban et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "The efficacy of SCST was later confirmed on other text generation tasks such as question generation (Zhang and Bansal, 2019), and summarization (Celikyilmaz et al., 2018; Laban et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "ACCESS from (Martin et al., 2020), is a stateof-the-art Transformer model trained on WikiLarge (300,000 pairs of complex/simple sentences).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 31,
      "context" : "Unsup NTS from (Surya et al., 2019) is an unsupervised approach based on successively encoding and denoising text using a GRU architecture.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Figure 6: Example Task (from a Washington Post article (Kelati, 2020)) for the Comprehension Study.",
      "startOffset" : 55,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : "Questions were generated via a GPT2 question generation model finetuned on the NewsQA dataset (Trischler et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "We performed a Kruskal-Wallis test (Kruskal and Wallis, 1952) with a Dunn posthoc test (Dunn, 1964) for statistical significance between pairs of conditions.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "We performed a Kruskal-Wallis test (Kruskal and Wallis, 1952) with a Dunn posthoc test (Dunn, 1964) for statistical significance between pairs of conditions.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Recent work in fact-checking for the summarization domain (Kryscinski et al., 2020; Li et al., 2018) could be adapted to the simplification domain to improve this.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Recent work in fact-checking for the summarization domain (Kryscinski et al., 2020; Li et al., 2018) could be adapted to the simplification domain to improve this.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Participating crowd-workers differ in literacy level which may have an effect on their performance at the task (Alonzo et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 132
    } ],
    "year" : 2021,
    "abstractText" : "This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate’s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text.",
    "creator" : "LaTeX with hyperref"
  }
}