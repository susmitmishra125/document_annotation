{
  "name" : "2021.acl-long.159.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World",
    "authors" : [ "Rowan Zellers", "Ari Holtzman", "Matthew Peters", "Roozbeh Mottaghi", "Aniruddha Kembhavi", "Ali Farhadi", "Yejin Choi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2040–2050\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2040\nExperimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast “what happens next” given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work."
    }, {
      "heading" : "1 Introduction",
      "text" : "As humans, our use of language is linked to the physical world. To process a sentence like “the robot turns on the stove, with a pan on it” (Figure 1) we might imagine a physical Pan object. This meaning representation in our heads can be seen as a part of our commonsense world knowledge, about what a Pan is and does. We might reasonably predict that the Pan will become Hot – and if there’s an Egg on it, it would become cooked .\nAs humans, we learn such a commonsense world model through interaction. Young children learn to reason physically about basic objects by manipulating them: observing the properties they have,\nand how they change if an action is applied on them (Smith and Gasser, 2005). This process is hypothesized to be crucial to how children learn language: the names of these elementary objects become their first “real words” upon which other language is scaffolded (Yu and Smith, 2012).\nIn contrast, the dominant paradigm today is to train large language or vision models on static data, such as language and photos from the web. Yet such a setting is fundamentally limiting, as suggested empirically by psychologists’ failed attempts to get kittens to learn passively (Held and Hein, 1963). More recently, though large Transformers have made initial progress on benchmarks, they also have frequently revealed biases in those same datasets, suggesting they might not be solving underlying tasks (Zellers et al., 2019b). This has been argued philosophically by a flurry of re-\ncent work arguing that no amount of language form could ever specify language meaning (McClelland et al., 2019; Bender and Koller, 2020; Bisk et al., 2020); connecting back to the Symbol Grounding Problem of Harnad (1990).\nIn this paper, we investigate an alternate strategy for learning physical commonsense through interaction, and then transferring that into language. We introduce a model named PIGLeT, short for Physical Interaction as Grounding for Language Transformers. We factorize an embodied agent into an explicit model of world dynamics, and a model of language form. We learn the dynamics model through interaction. Given an action heatUp applied to the Pan in Figure 1, the model learns that the Egg on the pan becomes Hot and Cooked , and that other attributes do not change.\nWe integrate our dynamics model with a pretrained language model, giving us a joint model of linguistic form and meaning. The combined PIGLeT can then reason about the physical dynamics implied by English sentences describing actions, predicting literally what might happen next. It can then communicate that result either symbolically or through natural language, generating a sentence like ‘The egg becomes hot and cooked.” Our separation between physical dynamics and language allows the model to learn about physical commonsense from the physical world itself, while also avoiding recurring problems of artifacts and biases that arise when we try to model physical world understanding solely through language.\nWe study this through a new environment and evaluation setup called PIGPeN, short for Physical Interaction Grounding Paired with Natural Language. In PIGPeN, a model is given unlimited access to an environment for pretraining, but only 500 examples with paired English annotations. Models in our setup must additionally generalize to novel ‘unseen’ objects for which we intentionally do not provide paired language-environment supervision. We build this on top of the THOR environment\n(Kolve et al., 2017), a physics engine that enables agents to perform contextual interactions (Fig 2) on everyday objects.\nExperiments confirm that PIGLeT performs well at grounding language with meaning. Given a sentence describing an action, our model predicts the resulting object states correctly over 80% of the time, outperforming even a 100x larger model (T511B) by over 10%. Likewise, its generated natural language is rated by humans as being more correct than equivalently-sized language models. Last, it can generalize in a ‘zero-shot’ way to objects that it has never read about before in language.\nIn summary, we make three key contributions. First, we introduce PIGLeT, a model decoupling physical and linguistic reasoning. Second, we introduce PIGPeN, to learn and evaluate the transfer of physical knowledge to the world of language. Third, we perform experiments and analysis suggesting promising avenues for future work.\n2 PIGPeN: A Resource for Neuro-Symbolic Language Grounding\nWe introduce PIGPeN as a setting for learning and evaluating physically grounded language understanding. An overview is shown in Figure 2. The idea is that an agent gets access to an interactive 3D environment, where it can learn about the world through interaction – for example, that objects such as a Vase can become Broken if thrown. The goal for a model is to learn natural language meaning grounded in these interactions.\nTask definition. Through interaction, an agent observes the interplay between objects o 2 O (represented by their attributes) and actions a 2 A through the following transition:\n{o1, . . . , oN}| {z } ~o, state pre-action\n⇥a ! {o01, . . . , o 0 N}| {z }\n~o0, state post-action\n. (1)\nActions change the state of a subset of objects: turning on a Faucet affects a nearby Sink , but it will not change a Mirror on the wall.\nTo encourage learning from interaction, and not just language, an agent is given a small number of natural language annotations of transitions. We denote these sentences as s~o, describing the state pre-action, sa the action, and s~o0 the state postaction respectively. During evaluation, an agent will sometimes encounter new objects o that were not part of the paired training data.\nWe evaluate the model’s transfer in two ways: a. PIGPeN-NLU. A model is given object states\n~o, and an English sentence sa describing an action. It must predict the grounded object states ~o0 that result after the action is taken.\nb. PIGPeN-NLG. A model is given object states ~o and a literal action a. It must generate a sentence s~o0 describing the state post-action.\nWe next describe our environment, feature representation, and language annotation process."
    }, {
      "heading" : "2.1 Environment: THOR",
      "text" : "We use AI2-THOR as an environment for this task (Kolve et al., 2017). In THOR, a robotic agent can navigate around and perform rich contextual interactions with objects in a house. For instance, it can grab an Apple , slice it, put it in a Fridge , drop it, and so on. The state of the Apple , such as whether it is sliced or cold, changes accordingly; this is not possible in many other environments.\nIn this work, we use the underlying THOR simulator as a proxy for grounded meaning. Within THOR, it can be seen as a ‘complete’ meaning representation (Artzi et al., 2013), as it fully specifies the kind of grounding a model can expect in its perception within THOR.\nObjects. The underlying THOR representation of each object o is in terms of 42 attributes; we provide a list in Appendix B. We treat these attributes as words specific to an attribute-level dictionary; for example, the temperature Hot is one of three possible values for an object’s temperature; the others being Cold and RoomTemp .\nActions. An action a in THOR is a function that takes up to two objects as arguments. Actions are highly contextual, affecting not only the arguments but potentially other objects in the scene (Figure 2). We also treat action names as words in a dictionary.\nFiltering out background objects. Most actions change the state of only a few objects, yet there can be many objects in a scene. We keep annotation and computation tractable by having models predict (and humans annotate) possible changes\nof at most two key objects in the scene. As knowing when an object doesn’t change is also important, we include non-changing objects if fewer than two change.\nExploration. Any way of exploring the environment is valid for our task, however, we found that exploring intentionally was needed to yield good coverage of interesting states. Similar to prior work for instruction following (Shridhar et al., 2020), we designed an oracle to collect diverse and interesting trajectories {~o, a, ~o0}. Our oracle randomly selects one of ten high level tasks, see Appendix B for the list. These in turn require randomly choosing objects in the scene; e.g. a Vase and a Laptop in Figure 2. We randomize the manner in which the oracle performs the task to discover diverse situations.\nIn total, we sampled 20k trajectories. From these we extracted 280k transitions (Eqn 1’s) where at least one object changes state, for training."
    }, {
      "heading" : "2.2 Annotating Interactions with Language",
      "text" : ""
    }, {
      "heading" : "2.2.1 Data Selection for Annotation",
      "text" : "We select 2k action state-changes from trajectories held out from the training set. We select them while also balancing the distribution of action types to ensure broad coverage in the final dataset. We are also interested in a model’s ability to generalize to new object categories – beyond what it has read about, or observed in a training set. We thus select 30 objects to be “unseen,” and exclude these from paired environment-language training data. We sample 500 state transitions, containing only “seen” objects to be the training set; we use 500 for validation and 1000 for testing."
    }, {
      "heading" : "2.2.2 Natural Language Annotation",
      "text" : "Workers on Mechanical Turk were shown an environment in THOR before and after a given action a. Each view contains the THOR attributes of the two key objects. Workers then wrote three English sentences, corresponding to s~o, sa, and s~o0 respectively. Workers were instructed to write at a particular level of detail: enough so that a reader could infer “what happens next” from s~o and sa, yet without mentioning redundant attributes.We provide more details in Appendix C.\n3 Modeling PIGLeT\nIn this section, we describe our PIGLeT model. First, we learn a neural physical dynamics model\nfrom interactions, and second, integrate with a pretrained model of language form."
    }, {
      "heading" : "3.1 Modeling Physical Dynamics",
      "text" : "We take a neural, auto-encoder style approach to model world dynamics. An object o gets encoded as a vector ho 2 Rdo . The model likewise encodes an action a as a vector ha 2 Rda , using it to manipulate the hidden states of all objects. The model can then decode any object hidden representation back into a symbolic form."
    }, {
      "heading" : "3.1.1 Object Encoder and Decoder",
      "text" : "We use a Transformer (Vaswani et al., 2017) to encode objects into vectors o 2 Rdo , and then another to decode from this representation.\nEncoder. Objects o are provided to the encoder as a set of attributes, with categories c1,..., cn. Each attribute c has its own vocabulary and embedding Ec. For each object o, we first embed all the attributes separately and feed the result into a Transformer encoder Tenc. This gives us (with position embeddings omitted for clarity):\nho = Tenc ⇣ E1(o1), . . . ,Ecn(ocn) ⌘ (2)\nDecoder. We can then convert back into the original symbolic representation through a left-to-right Transformer decoder, which predicts attributes oneby-one from c1 to cn. This captures the inherent correlation between attributes, while making no independence assumptions, we discuss our ordering in Appendix A.2. The probability of predicting the next attribute oci+1 is then given by: p(oci+1|ho, o:ci)=Tdec ⇣ ho,E1(o1),...,Eci(oci) ⌘ (3)"
    }, {
      "heading" : "3.1.2 Modeling actions as functions",
      "text" : "We treat actions a as functions that transform the state of all objects in the scene. Actions in our environment take at most two arguments, so we embed the action a and the names of its arguments, concatenate them, and pass the result through a multilayer perceptron; yielding a vector representation ha.\nApplying Actions. We use the encoded action ha to transform all objects in the scene, obtaining updated representations ĥo0 for each one. We take a global approach, jointly transforming all objects. This takes into account that interactions are contextual: turning on a Faucet might fill up a Cup if and only if there is one beneath it.\nLetting the observed objects in the interaction be o1 and o2, with encodings ho1 and ho2 respectively, we model the transformation via the following multilayer perceptron:\n[ĥo01 , ĥo02 ] = MLPapply ⇣⇥ ha,ho1 ,ho2 ⇤⌘ . (4)\nThe result can be decoded into symbolic form using the object decoder (Equation 3)."
    }, {
      "heading" : "3.1.3 Loss function and training",
      "text" : "We train our dynamics model on (~o,a,~o0) transitions. The model primarily learns by running ~o,a through the model, predicting the updated output state ĥo0 , and minimizing the cross-entropy of generating attributes of the real changed object ~o0. We also regularize the model by encoding objects ~o, ~o0 and having the model learn to reconstruct them. We weight all these cross-entropy losses equally. We discuss our architecture in Appendix A.1; it uses 3-layer Transformers, totalling 17M parameters."
    }, {
      "heading" : "3.2 Language Grounding",
      "text" : "After pretraining our physical dynamics model, we integrate it with a Transformer Language Model (LM). In our framework, the role of the LM will be to both encode natural language sentences of actions into a hidden state approximating ha, as well as summarizing the result of an interaction (~o,a,~o0) in natural language.\nChoice of LM. Our framework is compatible with any language model. However, to explore the impact of pretraining data on grounding later in this paper, we pretrain our own with an identical architecture to the smallest GPT2 (Radford et al. (2019); 117M). To handle both classification and generation well, we mask only part of the attention weights out, allowing the model to encode a “prefix” bidirectionally; it generates subsequent tokens leftto-right (Dong et al., 2019). We pretrain the model on Wikipedia and books; details in Appendix D.\nWe next discuss architectural details of performing the language transfer, along with optimization."
    }, {
      "heading" : "3.2.1 Transfer Architecture",
      "text" : "English actions to vector form. Given a natural language description sa of an action a, like “The robot throws the vase,” for PIGPeN-NLU, our model will learn to parse this sentence into a neural representation ha, so the dynamics model can simulate the result. We do this by encoding sa through our language model, TLM , with a learned linear transformation over the resulting (bidirectional) encoding. The resulting vector hsa can then be used by Equation 4.\nSummarizing the result of an action. For PIGPeN-NLG, our model simulates the result of an action a neurally, resulting in a predicted hidden state ĥo for each object in the scene o. To write an English summary describing “what changed,” we first learn a lightweight fused representation of the transition, aggregating the initial and final states, along with the action, through a multilayer perceptron. For each object oi we have:\nh oi = MLP ([hoi , ĥo0i ,ha]). (5)\nWe then use the sequence [h o1 ,h o2 ] as bidirectional context for our our LM to decode from. Additionally, since our test set includes novel objects not seen in training, we provide the names of the objects as additional context for the LM generator (e.g. ‘Vase, Laptop’); this allows a LM to copy those names over rather than hallucinate wrong\nones. Importantly we only provide the surfaceform names, not underlying information about these objects or their usage as with few-shot scenarios in the recent GPT-3 experiments (Brown et al., 2020) – necessitating that PIGLeT learns what these names mean through interaction."
    }, {
      "heading" : "3.2.2 Loss functions and training.",
      "text" : "Modeling text generation allows us to incorporate a new loss function, that of minimizing the loglikelihood of generating each s~o0 given previous words and the result of Equation 5:\np(sposti+1|s~o0,1:i) = TLM(h o1 ,h o2 , s~o0,1:i). (6)\nWe do the same for the object states s~o pre-action, using hoi as the corresponding hidden states.\nFor PIGPeN-NLU, where no generation is needed, optimizing Equation 5 is not strictly necessary. However, as we will show later, it helps provide additional signal to the model, improving overall accuracy by several percentage points."
    }, {
      "heading" : "4 Experiments",
      "text" : "We test our model’s ability to encode language into a grounded form (PIGPeN-NLU), and decode that grounded form into language (PIGPeN-NLG).\n4.1 PIGPeN-NLU Results. We first evaluate models by their performance on PIGPeN-NLU: given objects ~o, and a sentence sa describing an action, a model must predict the resulting state of objects ~o0. We primarily evaluate models by accuracy; scoring how many objects for which they got all attributes correct. We compare with the following strong baselines: a. No Change: this baseline copies the initial state\nof all objects ~o as the final state ~o0. b. GPT3-175B (Brown et al., 2020), a very large\nlanguage model for ‘few-shot’ learning using a prompt. For GPT3, and other text-to-text models, we encode and decode the symbolic object states in a JSON-style dictionary format, discussed in Appendix A.4.\nc. T5 (Raffel et al., 2019). With this model, we use the same ‘text-to-text’ format, however here we train it on the paired data from PIGPeN. We consider varying sizes of T5, from T5-Small – the closest in size to PIGLeT, up until T5-11B, roughly 100x the size.\nd. (Alberti et al., 2019)-style. This paper originally proposed a model for VCR (Zellers et al.,\n2019a), where grounded visual information is fed into a BERT model as tokens; the transformer performs the grounded reasoning. We adapt it for our task by using our base LM and feeding in object representations from our pretrained object encoder, also as tokens. Our object decoder predicts the object, given the LM’s pooled hidden state. This is “pretrained dynamics,” we also consider a version without a randomly initialized dynamics model.\ne. (Gupta and Durrett, 2019)-style. Thiso paper proposes using Transformers to model physical state, for tasks like entity tracking in recipes. Here, the authors propose decoding a physical state attribute (like isCooked ) by feeding the model a label-specific [CLS] token, and then mapping the result through a hidden layer. We do this and use a similar object encoder as our (Alberti et al., 2019)-style baseline. We discuss hyperparameters in Appendix A.3. Results. From the results (Table 1), we can draw several patterns. Our model, PIGLeT performs best at getting all attributes correct; doing so over 80% on both validation and test sets, even for novel objects not seen during training. The next closest model is T5-11B, which scores 68% on validation. Though when evaluated on objects ‘seen’ during training it gets 77%, that number drops by over 18% for unseen objects. On the other hand, PIGLeT has a modest gap of 3%. This suggests that our approach is particularly effective at connecting unpaired language and world representations. At\nthe other extreme, GPT3 does poorly in its ‘fewshot’ setting, suggesting that size is no replacement for grounded supervision. PIGLeT also outperforms ‘BERT style’ approaches that control for the same language model architecture, but perform the physical reasoning inside the language transformer rather than as a separate model. Performance drops when the physical decoder must be learned from few paired examples (as in Gupta and Durrett (2019)); it drops even further when neither model is given access to our pretrained dynamics model, with both baselines then underperforming ‘No Change.’ This suggests that our approach of having a physical reasoning model outside of an LM is a good inductive bias."
    }, {
      "heading" : "4.1.1 Ablation study",
      "text" : "In Table 2 we present an ablation study of PIGLeT’s components. Of note, by using a global representation of objects in the world (Equation 4), we get\nover 6% improvement over a local representation where objects are manipulated independently. We get another 3% boost by adding a generation loss, suggesting that learning to generate summaries helps the model better connect the world to language. Last, we benchmark how much headroom there is on PIGPeN-NLU by evaluating model performance on a ‘symbols only’ version of the task, where the symbolic action a is given explicitly to our dynamics model. This upper bound is roughly 7% higher than PIGLeT, suggesting space for future work.\n4.2 PIGPeN-NLG Results\nNext, we turn to PIGPeN-NLG: given objects ~o, and the literal next action a, a model must generate a sentence s~o0 describing what will change in the scene. We compare with the following baselines: a. T5. We use a T5 model that is given a JSON-\nstyle dictionary representation of both ~o and a, it is finetuned to generate summaries s~o0 . b. LM Baseline. We feed our LM hidden states ho from our pretrained encoder, along with its representation of a. The key difference between it and PIGLeT is that we do not allow it to simulate neurally what might happen next – MLPapply is never used here. Size matters. Arguably the most important factor controlling the fluency of a language generator is its size (Kaplan et al., 2020). Since our LM could also be scaled up to arbitrary size, we control for size in our experiments and only consider models the size of GPT2-base (117M) or smaller; we thus compare against T5-small as T5-Base has 220M parameters. We discuss optimization and sampling hyperparameters in Appendix A.3.\nEvaluation metrics. We evaluate models over the validation and test sets. We consider three main evaluation metrics: BLEU (Papineni et al., 2002) with two references, the recently proposed BERTScore (Zhang et al., 2020), and conduct a human evaluation. Humans rate both the fluency of post-action text, as well as its faithfulness to true action result, on a scale from 1 to 1.\nResults. We show our results in Table 3. Of note, PIGLeT is competitive with T5 and significantly outperforms the pure LM baseline, which uses a pretrained encoder for object states, yet has the physical simulation piece MLPapply removed. This suggests that simulating world dynamics not only allows the model to predict what might happen\nnext, it leads to more faithful generation as well."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Qualitative examples.",
      "text" : "We show two qualitative examples in Figure 4, covering both PIGPeN-NLU as well as PIGPeN-NLG. In the first row, the robot empties a held Mug that is filled with water. PIGLeT gets the state, and generates a faithful sentence summarizing that the mug becomes empty. T5 struggles somewhat, emptying the water from both the Mug and the (irrelevant) Sink . It also generates text saying that the Sink becomes empty, instead of the Mug.\nIn the second row, PIGLeT correctly predicts the next object states, but its generated text is incomplete – it should also write that the mug becomes filled wtih Coffee. T5 makes the same mistake in generation, and it also underpredicts the state changes, omitting all changes to the Mug .\nWe suspect that T5 struggles here in part because Mug is an unseen object. T5 only experiences it through language-only pretraining, but this might not be enough for a fully grounded representation."
    }, {
      "heading" : "5.2 Representing novel words",
      "text" : "The language models that perform best today are trained on massive datasets of text. However, this has unintended consequences (Bender et al., 2021) and it is unlike how children learn language, with children learning novel words from experience (Carey and Bartlett, 1978). The large scale of our pretraining datasets might allow models to learn to perform physical-commonsense like tasks for wrong reasons, overfitting to surface patterns rather than learning meaningful grounding.\nWe investigate the extent of this by training a ‘zero-shot’ version of our backbone LM on Wikipedia and books – the only difference is that\nunseen objects like Mug that are excluded from the training set. T5 struggles to predict these changes, for example, it seems to suggest that emptying the Mug causes all containers in the scene to become empty.\nwe explicitly exclude all mentioned sentences containing one of our “unseen” object categories. In this setting, not only must PIGLeT learn to ground words like ‘mug,’ it must do so without having seen the word ‘mug’ during pretraining. This is significant because we count over 20k instances of ‘Mug’ words (including morphology) in our dataset.\nWe show results in Figure 5. A version of PIGLeT with the zero-shot LM does surprisingly well – achieving 80% accuracy at predicting the state changes for “Mug” – despite never having been pretrained on one before. This even outperforms T5 at the overall task. Nevertheless, PIGLeT outperforms it by roughly 7% at unseen objects, with notable gains of over 10% on highly dynamic objects like Toasters and Sinks."
    }, {
      "heading" : "6 Related Work",
      "text" : "Grounded commonsense reasoning. In this work, we study language grounding and common-\nsense reasoning at the representation and concept level. The aim is to train models that learn to acquire concepts more like humans, rather than performing well on a downstream task that (for humans) requires commonsense reasoning. Thus, this work is somewhat different versus other 3D embodied tasks like QA (Gordon et al., 2018; Das et al., 2018), along with past work for measuring such grounded commonsense reasoning, like SWAG, HellaSWAG, and VCR (Zellers et al., 2018, 2019b,a). The knowledge covered is different, as it is self-contained within THOR. While VCR, for instance, includes lots of visual situations about what people are doing, this paper focuses on learning the physical properties of objects.\nZero-shot generalization. There has been a lot of past work involved with learning ‘zero-shot’: often learning about the grounded world in language, and transferring that knowledge to vision. Techniques for this include looking at word embeddings (Frome et al., 2013) and dictionary definitions (Zellers and Choi, 2017). In this work, we propose the inverse. This approach was used to learn better word embeddings (Gupta et al., 2019) or semantic tuples (Yatskar et al., 2016), but we consider learning a component to be plugged into a deep Transformer language model.\nPast work evaluating these types of zero-shot generalization have also looked into how well models can compose concepts in language together (Lake and Baroni, 2018; Ruis et al., 2020). Our work considers elements of compositionality through grounded transfer. For example, in\nPIGPeN-NLG, models must generate sentences about the equivalent of dropping a ‘dax’, despite never having seen one before. However, our work is also contextual, in that the outcome of ‘dropping a dax’ might depend on external attributes (like how high we’re dropping it from).\nStructured Models for Attributes and Objects. The idea of modeling actions as functions that transform objects has been explored in the computer vision space (Wang et al., 2016). Past work has also built formal structured models for connecting vision and language (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), we take a neural approach and connect today’s best models of language form to similarly neural models of a simulated environment."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we presented an approach PIGLeT for jointly modeling language form and meaning. We presented a testbed PIGPeN for evaluating our model, which performs well at grounding language to the (simulated) world."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their helpful feedback, and the Mechanical Turk workers for doing a great job in annotating our data. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the DARPA CwC program through ARO (W911NF-15-1-0543), the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI."
    } ],
    "references" : [ {
      "title" : "Fusion of detected objects in text for visual question answering",
      "author" : [ "Chris Alberti", "Jeffrey Ling", "Michael Collins", "David Reitter." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic parsing with combinatory categorial grammars",
      "author" : [ "Yoav Artzi", "Nicholas FitzGerald", "Luke S Zettlemoyer." ],
      "venue" : "ACL (Tutorial Abstracts), 3.",
      "citeRegEx" : "Artzi et al\\.,? 2013",
      "shortCiteRegEx" : "Artzi et al\\.",
      "year" : 2013
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big",
      "author" : [ "Emily M Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "Proceedings of FAccT.",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. As-",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Acquiring a single new word",
      "author" : [ "S. Carey", "E. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Carey and Bartlett.,? \\Q1978\\E",
      "shortCiteRegEx" : "Carey and Bartlett.",
      "year" : 1978
    }, {
      "title" : "Embodied question answering",
      "author" : [ "Abhishek Das", "Samyak Datta", "Georgia Gkioxari", "Stefan Lee", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–10.",
      "citeRegEx" : "Das et al\\.,? 2018",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Devise: A deep visualsemantic embedding model",
      "author" : [ "Andrea Frome", "Greg Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc’Aurelio Ranzato", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Frome et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "Iqa: Visual question answering in interactive environments",
      "author" : [ "Daniel Gordon", "Aniruddha Kembhavi", "Mohammad Rastegari", "Joseph Redmon", "Dieter Fox", "Ali Farhadi." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Gordon et al\\.,? 2018",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2018
    }, {
      "title" : "Effective use of transformer networks for entity tracking",
      "author" : [ "Aditya Gupta", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Gupta and Durrett.,? 2019",
      "shortCiteRegEx" : "Gupta and Durrett.",
      "year" : 2019
    }, {
      "title" : "Vico: Word embeddings from visual cooccurrences",
      "author" : [ "Tanmay Gupta", "Alexander Schwing", "Derek Hoiem." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 7425– 7434.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "The symbol grounding problem",
      "author" : [ "Stevan Harnad." ],
      "venue" : "Physica D: Nonlinear Phenomena, 42(13):335–346.",
      "citeRegEx" : "Harnad.,? 1990",
      "shortCiteRegEx" : "Harnad.",
      "year" : 1990
    }, {
      "title" : "Movementproduced stimulation in the development of visually guided behavior",
      "author" : [ "Richard Held", "Alan Hein." ],
      "venue" : "Journal of comparative and physiological psychology, 56(5):872.",
      "citeRegEx" : "Held and Hein.,? 1963",
      "shortCiteRegEx" : "Held and Hein.",
      "year" : 1963
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "arXiv preprint arXiv:2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Ai2-thor: An interactive 3d environment for visual ai",
      "author" : [ "Eric Kolve", "Roozbeh Mottaghi", "Winson Han", "Eli VanderBilt", "Luca Weihs", "Alvaro Herrasti", "Daniel Gordon", "Yuke Zhu", "Abhinav Gupta", "Ali Farhadi." ],
      "venue" : "arXiv preprint arXiv:1712.05474.",
      "citeRegEx" : "Kolve et al\\.,? 2017",
      "shortCiteRegEx" : "Kolve et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly learning to parse and perceive: Connecting natural language to the physical world",
      "author" : [ "Jayant Krishnamurthy", "Thomas Kollar." ],
      "venue" : "Transac-",
      "citeRegEx" : "Krishnamurthy and Kollar.,? 2013",
      "shortCiteRegEx" : "Krishnamurthy and Kollar.",
      "year" : 2013
    }, {
      "title" : "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
      "author" : [ "Brenden Lake", "Marco Baroni." ],
      "venue" : "International Conference on Machine Learning, pages 2873–2882. PMLR.",
      "citeRegEx" : "Lake and Baroni.,? 2018",
      "shortCiteRegEx" : "Lake and Baroni.",
      "year" : 2018
    }, {
      "title" : "A joint model of language and perception for grounded attribute learning",
      "author" : [ "Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox." ],
      "venue" : "Proceedings of the 29th Inter-",
      "citeRegEx" : "Matuszek et al\\.,? 2012",
      "shortCiteRegEx" : "Matuszek et al\\.",
      "year" : 2012
    }, {
      "title" : "Extending machine language models toward humanlevel language understanding",
      "author" : [ "James L McClelland", "Felix Hill", "Maja Rudolph", "Jason Baldridge", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:1912.05877.",
      "citeRegEx" : "McClelland et al\\.,? 2019",
      "shortCiteRegEx" : "McClelland et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "Technical report, OpenAI.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "arXiv e-prints.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "A benchmark for systematic generalization in grounded language understanding",
      "author" : [ "Laura Ruis", "Jacob Andreas", "Marco Baroni", "Diane Bouchacourt", "Brenden M Lake." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Ruis et al\\.,? 2020",
      "shortCiteRegEx" : "Ruis et al\\.",
      "year" : 2020
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "International Conference on Machine Learning, pages 4603–4611.",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
      "author" : [ "Mohit Shridhar", "Jesse Thomason", "Daniel Gordon", "Yonatan Bisk", "Winson Han", "Roozbeh Mottaghi", "Luke Zettlemoyer", "Dieter Fox." ],
      "venue" : "Proceedings of the IEEE/CVF",
      "citeRegEx" : "Shridhar et al\\.,? 2020",
      "shortCiteRegEx" : "Shridhar et al\\.",
      "year" : 2020
    }, {
      "title" : "The development of embodied cognition: Six lessons from babies",
      "author" : [ "Linda Smith", "Michael Gasser." ],
      "venue" : "Artificial life, 11(1-2):13–29.",
      "citeRegEx" : "Smith and Gasser.,? 2005",
      "shortCiteRegEx" : "Smith and Gasser.",
      "year" : 2005
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Actions ̃ transformations",
      "author" : [ "Xiaolong Wang", "Ali Farhadi", "Abhinav Gupta." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Stating the obvious: Extracting visual common sense knowledge",
      "author" : [ "Mark Yatskar", "Vicente Ordonez", "Ali Farhadi." ],
      "venue" : "Proceedings of the 2016 Con-",
      "citeRegEx" : "Yatskar et al\\.,? 2016",
      "shortCiteRegEx" : "Yatskar et al\\.",
      "year" : 2016
    }, {
      "title" : "Embodied attention and word learning by toddlers",
      "author" : [ "Chen Yu", "Linda B Smith." ],
      "venue" : "Cognition, 125(2):244–262.",
      "citeRegEx" : "Yu and Smith.,? 2012",
      "shortCiteRegEx" : "Yu and Smith.",
      "year" : 2012
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "The IEEE Confer-",
      "citeRegEx" : "Zellers et al\\.,? 2019a",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "In",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot activity recognition with verb attribute induction",
      "author" : [ "Rowan Zellers", "Yejin Choi." ],
      "venue" : "Pro-",
      "citeRegEx" : "Zellers and Choi.,? 2017",
      "shortCiteRegEx" : "Zellers and Choi.",
      "year" : 2017
    }, {
      "title" : "2019b. HellaSwag: Can a machine really finish your sentence",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "V. Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "ArXiv, abs/1904.09675.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "and how they change if an action is applied on them (Smith and Gasser, 2005).",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 33,
      "context" : "This process is hypothesized to be crucial to how children learn language: the names of these elementary objects become their first “real words” upon which other language is scaffolded (Yu and Smith, 2012).",
      "startOffset" : 185,
      "endOffset" : 205
    }, {
      "referenceID" : 14,
      "context" : "Yet such a setting is fundamentally limiting, as suggested empirically by psychologists’ failed attempts to get kittens to learn passively (Held and Hein, 1963).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 22,
      "context" : "cent work arguing that no amount of language form could ever specify language meaning (McClelland et al., 2019; Bender and Koller, 2020; Bisk et al., 2020); connecting back to the Symbol Grounding Problem of Harnad (1990).",
      "startOffset" : 86,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "cent work arguing that no amount of language form could ever specify language meaning (McClelland et al., 2019; Bender and Koller, 2020; Bisk et al., 2020); connecting back to the Symbol Grounding Problem of Harnad (1990).",
      "startOffset" : 86,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "We build this on top of the THOR environment (Kolve et al., 2017), a physics engine that enables agents to perform contextual interactions (Fig 2) on everyday objects.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "We use AI2-THOR as an environment for this task (Kolve et al., 2017).",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "Within THOR, it can be seen as a ‘complete’ meaning representation (Artzi et al., 2013), as it fully specifies the kind of grounding a model can expect in its perception within THOR.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "Similar to prior work for instruction following (Shridhar et al., 2020), we designed an oracle to collect diverse and interesting trajectories {~ o, a, ~ o0}.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "We use a Transformer (Vaswani et al., 2017) to encode objects into vectors o 2 Rdo , and then another to decode from this representation.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "To handle both classification and generation well, we mask only part of the attention weights out, allowing the model to encode a “prefix” bidirectionally; it generates subsequent tokens leftto-right (Dong et al., 2019).",
      "startOffset" : 200,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : "Importantly we only provide the surfaceform names, not underlying information about these objects or their usage as with few-shot scenarios in the recent GPT-3 experiments (Brown et al., 2020) – necessitating that PIGLeT learns what these names mean through interaction.",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "GPT3-175B (Brown et al., 2020), a very large language model for ‘few-shot’ learning using a prompt.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We do this and use a similar object encoder as our (Alberti et al., 2019)-style baseline.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "Arguably the most important factor controlling the fluency of a language generator is its size (Kaplan et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "We consider three main evaluation metrics: BLEU (Papineni et al., 2002) with two references, the recently proposed BERTScore (Zhang et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 38,
      "context" : ", 2002) with two references, the recently proposed BERTScore (Zhang et al., 2020), and conduct a human evaluation.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "However, this has unintended consequences (Bender et al., 2021) and it is unlike how children learn language, with children learning novel words from experience (Carey and Bartlett, 1978).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : ", 2021) and it is unlike how children learn language, with children learning novel words from experience (Carey and Bartlett, 1978).",
      "startOffset" : 105,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Thus, this work is somewhat different versus other 3D embodied tasks like QA (Gordon et al., 2018; Das et al., 2018), along with past work for measuring such grounded commonsense reasoning, like SWAG, HellaSWAG, and VCR (Zellers et al.",
      "startOffset" : 77,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "Thus, this work is somewhat different versus other 3D embodied tasks like QA (Gordon et al., 2018; Das et al., 2018), along with past work for measuring such grounded commonsense reasoning, like SWAG, HellaSWAG, and VCR (Zellers et al.",
      "startOffset" : 77,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "Techniques for this include looking at word embeddings (Frome et al., 2013) and dictionary definitions (Zellers and Choi, 2017).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 36,
      "context" : ", 2013) and dictionary definitions (Zellers and Choi, 2017).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "This approach was used to learn better word embeddings (Gupta et al., 2019) or semantic tuples (Yatskar et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : ", 2019) or semantic tuples (Yatskar et al., 2016), but we consider learning a component to be plugged into a deep Transformer language model.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Past work evaluating these types of zero-shot generalization have also looked into how well models can compose concepts in language together (Lake and Baroni, 2018; Ruis et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "Past work evaluating these types of zero-shot generalization have also looked into how well models can compose concepts in language together (Lake and Baroni, 2018; Ruis et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 183
    }, {
      "referenceID" : 31,
      "context" : "The idea of modeling actions as functions that transform objects has been explored in the computer vision space (Wang et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "Past work has also built formal structured models for connecting vision and language (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), we take a neural approach and connect today’s best models of language form to similarly neural models of a simulated environment.",
      "startOffset" : 85,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "Past work has also built formal structured models for connecting vision and language (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), we take a neural approach and connect today’s best models of language form to similarly neural models of a simulated environment.",
      "startOffset" : 85,
      "endOffset" : 140
    } ],
    "year" : 2021,
    "abstractText" : "We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast “what happens next” given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.",
    "creator" : "LaTeX with hyperref"
  }
}