{
  "name" : "2021.acl-long.69.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Faithful Representations of Causal Graphs",
    "authors" : [ "Ananth Balashankar", "Lakshminarayanan Subramanian" ],
    "emails" : [ "ananth@nyu.edu", "lakshmi@nyu.edu", "(P@1)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 839–850\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n839"
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning distributed word representations that capture causal relationships are useful for real-world natural language processing tasks (Roberts et al., 2020; Veitch et al., 2020; Gao et al., 2018, 2019). Approximating the notion of causality with a similarity-based distance metric using separate vector representations for cause and effect tokens has led to significant improvement in the performance of downstream tasks like Question Answering, but can be too restrictive to generalize over unobserved edges in larger causal graphs (Sharp et al., 2016). In downstream causal reasoning based tasks like dialog systems (Ning et al., 2018), explanation generation (Grimsley et al., 2020), question answering (Sharp et al., 2016), it is important to align the\nmodels with the corresponding causal graph. However, words that have low cosine similarity capture various semantic similarities, like relatedness, synonyms, replaceability, or complementarity, but not directionality (Hamilton et al., 2017). Hence, any symmetric distance in an embedding space cannot convey the directed causal semantics for a downstream task (Mémoli et al., 2016). In this paper, we overcome these two shortcomings and propose to optimize for directed faithfulness (Spirtes et al., 1993) that word embeddings have to satisfy towards a causal graph.\nPrior work on capturing sufficient information for causal inference tasks from embeddings aims to directly use them for average treatment effect estimation (Veitch et al., 2020). We are, however, interested in a complementary question: “Can we learn word embeddings based on a distance measure that maps the directed distance between nodes in a causal graph to that in the embedding space?”. Unlike prior work, which aims to learn a causal aware embedding restricted to direct link prediction (Hamilton et al., 2017), we propose faithfulness constraints so that causal word embeddings aims to preserve the partial ordering over pairwise distances in the directed causal graph. In this paper, to achieve the goal of learning faithful word embeddings with a vocabulary of more than 100K tokens, we minimize faithfulness violations over pairwise samples of nodes in the causal graph. Through this constrained optimization, we learn an embedding that can be applied directly for causal inference tasks but also generalizes to emergent causal links. It has been shown that NLP models need to understand such causal links that persist in the real world for safe deployment (Gao et al., 2018; Mishra et al., 2019). Embeddings that violate the faithfulness property, can lead to spurious correlations based on co-location in the embedding space. For example, in a Yahoo! causal question-answering task’s\nexample: “What causes nosebleed?”: the answers were “dry air”, “heavy dust”, “damaged nasal cells” and “liver problems”. If we were to only rely on an undirected association based embeddings, the causes “dry air” and “liver problems” might be nearby (with distance of 2), but would be appropriately placed far in a directed causality based embedding space. To capture such asymmetric properties, we aim to preserve alignment with the causal graph by mapping causal links to an asymmetric quasi-pseudo distance measure during training to capture directionality of the causal graph as per Figure 1. Since human validated causal graphs can be used directly to answer questions of the type “What causes X?”, we demonstrate the utility of learning faithful representations by using our distance-based features to solve the Yahoo! causal question-answering (QA) task. A causal QA task, unlike a standard QA task, can directly benefit from incorporating a causal graph into word embeddings to answer anti-causal queries. Our key contributions are:\n• We define a faithfulness property for word embeddings over a causal graph, that captures geometric properties of the causal graph, beyond the direct link prediction by ensuring global proximity preservation.\n• We propose a methodology to learn faithful embeddings through violation minimization which improves neighborhood detection by 31.3%, uniformity by 42.6%, and distance correlation by 54.2% using a quasi-pseudo distance metric.\n• The faithful BERT and RoBERTa-based embeddings we learn, when used as inputs to a causal QA task, increases the precision of the first ranked answer (P@1) over existing baselines by 10.2%."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Causal Model Representations",
      "text" : "Causal Inference, as outlined in (Pearl, 2009) formalizes cause and effects discovered through intervention based experiments and communicates them via directed acyclic graphs. With the availability of large observational datasets for machine learning, various methods and assumptions have been proposed for learning causal graphs (Schölkopf, 2019), data fusion and transportability properties\n(Bareinboim and Pearl, 2016; Bonner and Vasile, 2017). Specifically, our work closely aligns with the assumption of faithfulness (Spirtes et al., 1993), which requires that the observed probability distributions of nodes in a causal graph are conditionally independent as per the links in the graph. In our work, we use the probability distributions as modeled in a natural language model (Kuhn and De Mori, 1990) and align it with the causal links in a graphical causal model. We extend the faithfulness assumption to be reflected in embeddings learnt by a masked language model (Devlin et al., 2019; Liu et al., 2019b) for downstream tasks. This definition of faithfulness is different from the one proposed by (Jacovi and Goldberg, 2020) used to evaluate models for interpretability of models used for downstream tasks. Instead, our work builds on embeddings learnt in (Sharp et al., 2016), given a causal model and learn embeddings that are bootstrapped using a small set of cause-effect seeds. Causal models have also been used to learn auxiliary tasks (Feder et al., 2020) using adversarial training to ensure that a language model learns causal-inspired representations. Such approaches use causal models to learn counterfactual embeddings invariant to the presence of confounding concepts in a sentence, while we encode the geometrical properties of causal graphs into the embeddings and the distance measure to maintain their faithfulness. In principle, we adopt a similar approach to (Veitch et al., 2020) of fine-tuning towards a causal link prediction task. This is in contrast with approaches that use energy-based transition vectors used to represent the cause-to-effect and effectto-cause links (Zhao et al., 2017). Our approach uses regularization constraints similar to the ones\nproposed for information bottlenecks in word embeddings (Li and Eisner, 2019; Goyal and Durrett, 2019), text-based games (Narasimhan et al., 2015), activation links in neuroscience (Chalupka et al., 2016), causal consistency with ordinary differential equations (Rubenstein et al., 2017) and temporal Granger Causality (Tank et al., 2018). For an extensive survey of using text for causal inference tasks, we refer to (Keith et al., 2020)."
    }, {
      "heading" : "2.2 Graph Representation Learning",
      "text" : "Learning asymmetric transitive graph representations which generalize the causal graph have been studied extensively in Information Retrieval (Chen et al., 2007; Epasto and Perozzi, 2019; Li et al., 2019; Grover and Leskovec, 2016). They either utilize a random walk learning technique (Perozzi et al., 2014) or matrix factorization techniques (Lee and Seung, 2000; Tenenbaum et al., 2000; Wang et al., 2017; Mikolov et al., 2013) to incorporate priors such as the stationary transition probability matrix, community structure, etc. More recently, (Liu et al., 2019a; Ostendorff et al., 2019; Lu et al., 2020) have incorporated knowledge graphs in BERT and shown increased accuracy in knowledgecentric NLP tasks. (Zhou et al., 2017; Gordo and Perronnin, 2011; Ou et al., 2016; Sun et al., 2018; Tang et al., 2015) propose asymmetric higher order proximity preserving graph embedding methods by learning separate source and target embeddings. While we can learn faithful 3-dimension embeddings for any fixed finite undirected graph deterministically (Cohen et al., 1995), fine-tuning pretrained word embeddings such that they generalize over all sub-graphs in a directed graph is known to be a hard graph kernel design problem that scales cubically with the number of nodes (Vishwanathan et al., 2010). Our approach builds on efforts to incorporate graph-like structure in BERT, but overcomes the issue of learning dual embeddings for cause-effect edges by learning unified embeddings for both cause and effect roles of words. Through such embeddings, we can further aid causal discovery that is not yet captured in a graphical notation (Chen et al., 2014)."
    }, {
      "heading" : "2.3 Graph Neural Networks",
      "text" : "Recently, Graph neural networks that capture the graph neighborhood structure have been employed in link prediction (Zhu et al., 2020; Abu-El-Haija et al., 2017). In (You et al., 2018), the problem is reduced to that of sequence prediction by reducing\nthe graph to breadth-first search based deterministic sequence. In (Li et al., 2018), node embeddings are updated after several rounds of message passing, while in (Tu et al., 2016) a variant of the random walk is incorporated with a max-margin discriminative constraint. In (Velikovi et al., 2018), models are learned by attending over the neighborhood of nodes for context, while (Kipf and Welling, 2016) apply spectral graph convolutions for a selfsupervised learning task. We adopt the incremental approach proposed in (Velikovi et al., 2018) which does not rely on knowing the entire graph structure apriori and fine-tune on cause-effect pairs for the link prediction task on a pre-trained BERT-based language model."
    }, {
      "heading" : "3 Learning Faithful Embeddings",
      "text" : ""
    }, {
      "heading" : "3.1 Background",
      "text" : "Causal inference (Pearl, 2009) aims to understand the cause and effect relationships between events. Learning purely based on correlations in observational data can lead to spurious causal links and can severely impact downstream tasks. Hence, intervention-based studies are conducted which carefully study the impact of a cause using controlled randomized experiments and other criterion to learn if links between causes and effects exist using observed data under specific assumptions. The findings of such studies are formalized using frameworks like Rubin Causal Models (Rubin, 1974), Structural Causal Models (Pearl, 2009), etc. While there are differences in abstractions between them, there is formal equivalence (Galles and Pearl, 1998) in modeling counterfactuals (“What is the effect when the cause is intervened?”) and we refer the reader to (Pearl and Mackenzie, 2018) for a primer in causal modeling.\nIn this paper, we assume a graphical structural causal modelC (Pearl, 2009) is given, whose nodes are linked with directed edges that denote the causeeffect relationship. For example, the cause-effect of “smoking” causes “cancer”, references to the real world action of “smoking” in individuals that leads to the development of “cancer” kind of disease in those individuals. While causal models have a close relationship to the knowledge graph, the links of the causal graph have a well-defined causal interpretation that can be validated through counterfactual experiments. In this work, we assume the availability of such a causal graph and we do not aim to build one. Instead, we rely on hu-\nman annotators who with the help of web crawlers (Heindorf et al., 2020a) and other information retrieval tools (Sharp et al., 2016) produce a directed graphical causal model as shown in Figure 1."
    }, {
      "heading" : "3.2 Faithfulness",
      "text" : "Given a graphical causal model C, we now present a faithfulness property an embedding that aims to closely align with the causal model has to satisfy. The faithfulness property was first proposed for any two causal spaces in (Bombelli et al., 2013) in the domain of quantum physics with the space-time dimension. Inspired by this, we propose an instantiation for word embeddings and a corresponding graphical causal model.\nDefinition 1 (Faithfulness). An embedding f : C → M from a causal set (C, dC) to a vector space (M,dM ) is faithful if:\n• ∃λ,∀x, y ∈ C, dC(x, y) = 1 ⇔ dM (f(x), f(y)) ≤ λ\n• f(C) is distributed uniformly\n• ∀x, y, w, z ∈ C, dC(x, y) ≤ dC(w, z) ⇔ dM (f(x), f(y)) ≤ dM (f(w), f(z))\nNote that we use the causal set (C, dC) as a tuple of the graphical causal model C and a distance measure dC which is used to measure the directed distance between nodes in the graph. The vector space in which we map our embeddings is also characterized by a tuple (M,dM ), where M is the multidimensional real number space Rm, and a distance measure dM which identifies nearby words in that vector space. The three conditions posed by the faithfulness property, more concretely specify that there needs to be a real threshold, within the embedding space, which can cover all the neighboring nodes of a word, the embedding space needs to be uniformly distributed, and finally, any inequality relationships between two distance measures in the causal graph needs to hold in the embedding space too. An embedding that satisfies this property can then be used to sufficiently represent the causal graph in downstream tasks."
    }, {
      "heading" : "3.2.1 Distance Measures",
      "text" : "The definition of faithfulness is dependent on the distance measure used in both the causal graph and the embedding domains. In this work, we assume that the causal graph is a directed acyclic graph, and hence we measure dC as the shortest directed\ndistance (number of edges in an unweighted graph) between two nodes. If no such path exists between two nodes, we consider the distance to be a large number, which in the case of an unweighted graph, can be set to > n, where n is the number of nodes in the acyclic graph. Note that weighted graphs can also be incorporated with minor changes based on the maximum path in the graph.\nHowever, the distance measure in the embedding space faces challenges in evaluation of simple supervised tasks (Jastrzebski et al., 2017). To overcome these, we chose a distance measure that is closely tied to our faithfulness definition. We chose a unified set of embeddings for both the cause u and effect v, and, if there exists a causal edge from u → v, then we would expect that dM (f(u), f(v)) << dM (f(v), f(u)). For this reason, symmetric distance choices like Euclidean distance, cosine similarity are not suitable. Our chosen distance measure, hence should follow the properties of quasi-pseudo metrics, defined as follows in (Moshokoa, 2005):\nDefinition 2 (Quasi-Pseudo Metric). A measure dM : X ×X → [0,∞) is a quasi-pseudo metric if ∀x, y, z ∈ X ,\n• dM (x, y) ≥ 0\n• dM (x, x) = 0, but dM (x, y) = 0 is possible for x 6= y\n• dM (x, z) ≤ dM (x, y) + dM (y, z)\nHence, quasi-psuedo metrics, which do not satisfy the symmetry property are best suited to measure the distance between any two embeddings. We can generate such metrics, given a measure d. If the cause phrase u has pword tokens, and the effect phrase v has q word tokens, we choose the MaxMatching method given in (Xie and Mu, 2019) in our definition of dM by iterating through all pairs of words (vb, ua) : vb 6= ua. Note that the measure d computes the difference between v to u over the total m number of dimensions in f(vb), f(ua).\nd(u, v) = min a=1..p b=1..q vb 6=ua\nm∑ j=1 (fj(vb)− fj(ua)) (1)\ndM (f(u), f(v)) = { d(u, v), if d(u, v) > 0 10−d(u,v) − 1, otherwise (2)\nWe chose this definition, as it is differentiable (except at 0, where we choose the gradient to be\n0). Also, for each point u in the embedding space, there is a corresponding hyperplane that passes through it that defines the half-space which separates the reachable nodes v : d(u, v) > 0 - nodes which have either an indirect or direct causal link and the unreachable nodes v : d(u, v) < 0. Also, by the property of d(u, v) = −d(v, u), we see that if v is reachable from u, then u is not reachable from v, thus affirming that this is suitable to represent a causal graph that is directed and acyclic."
    }, {
      "heading" : "3.3 Causal Graph Link Prediction",
      "text" : "There are currently many approaches to learning causal representations, one which uses a masked language modeling approach where the word tokens in the cause are paired with word tokens in the effect using a skip-gram technique in an unsupervised setting. In the supervised setting, models align the cause-effect embeddings to solve either a sequence-to-sequence translation task or logistic classification task. Since we aim to capture all the nodes of the causal graph into a single set of word embeddings, we choose this approach. Further, in the supervised setting, we make explicit the causal relationship between cause and effect, thereby capturing the directionality of the linkage. Thus, a supervised model could translate a cause to an effect or predict the link that exists from a cause to an effect. Among these supervised modeling choices, we choose the binary classification task of predicting if a directed edge exists between two nodes in the causal graph. This supervised learning is achieved by following the technique of fine-tuning as proposed in (Veitch et al., 2020). Formally, given a cause phrase u, an effect phrase v, let an i(u, v) be an edge indicator variable i(u, v) = 1u→v that takes binary values of {0, 1} based on the existence of an edge from u→ v in the causal graph.\nPre-trained Contextual Models: Pre-trained models based on transformers like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) learn contextual embeddings of words or tokens by optimizing for the self-supervision task of predicting randomly masked tokens in a sentence. These pretrained embeddings for word tokens have been used extensively for fine-tuning. Here, we use such finetuned models denoted as g̃ to predict the existence of an edge between the cause and effect u, v, by embedding them into f(u), f(v) respectively and further optimizing them in the fine-tuning stage on the following cross-entropy classification loss\nLs = Eu,v∼C CrossEnt(i(u, v), g̃(u, v)) (3)"
    }, {
      "heading" : "3.4 Violation Minimization",
      "text" : "Given the faithfulness definition, our goal is to learn an embedding that minimizes the number of violations of the faithfulness property. For each of the 3 conditions present in the faithfulness property, we define how we measure their adherence and incorporate it in the loss function. In addition to the causal graph link prediction task, we now present how the faithfulness properties are incorporated through regularization constraints."
    }, {
      "heading" : "3.4.1 Neighborhood",
      "text" : "Since we expect a single embedding distance threshold that perfectly encapsulates the neighborhood of a node, we can measure this by varying distance thresholds for neighborhood detection and compute the area under the curve of the precisionrecall curve. Since we aim to retain all the neighbors of a node in the causal graph within an upper bound of the distance in the embedding space, we add the sum of the distance between the nodes and their neighbors as an L1 regularization loss.\nLn = E u∼C v∈Neigh(u) |dM (f(u), f(v))| (4)"
    }, {
      "heading" : "3.4.2 Uniformity",
      "text" : "Since checking for true uniformity can be computationally intractable, we approximate by computing the per-dimension aggregate of all the word embeddings and compute the Wasserstein distance (Olkin and Pukelsheim, 1982) between the observed distribution and the expected uniform distribution centered around zero (0m). Since, in the uniformity constraint, we would expect that the embeddings are centered around zero, the mean of the embeddings should be close to zero. We measure the distance from this expected centroid and penalize the model for a high distance. If Cb denote the set of nodes chosen in a batch b, with size |b|, and fj(p) denote the jth dimension of the embedding of node p, then we present the uniformity regularization loss:\nLu = m∑\nj=1\n1 |b| ∑ p∈Cb fj(p) (5)"
    }, {
      "heading" : "3.4.3 Distance Correlation",
      "text" : "To measure if inequalities between two distances in the causal graph hold in the embedding space, we measure the Pearson correlation coefficient between samples of distances between words in the causal graph and that of the embeddings. To ensure that any two distances sampled from the causal graph maintain the same inequality in the embedding space, we sample random nodes from the causal graph and compute the empirical Pearson Correlation Coefficient of their distances in the embedding space. A perfect correlation would lead to a coefficient of +1, so we penalize any deviation from that ideal correlation and present the distance correlation loss:\nLc = 1− ρdC ,dM\n= 1− cov(dC , dM ) σdCσdM\n(6)\nNote that all the above constraints are at a batch level and hence is added on to the batch crossentropy loss during every back-propagation step. Since the losses are differentiable, we have used the auto-diff capability available in Tensorflow. The contribution of each of the above losses are combined using the Augmented Lagrangian method (Hestenes, 1969) and controlled using 3 parameters α, β, γ as follows:\nL = (1− α− β − γ)Ls + αLn + βLu + γLc (7)\nThe values of these hyperparameters were chosen to be 0.1, 0.15, 0.1 respectively after crossvalidation to optimize causal link prediction accuracy and faithfulness metrics. A summary of our approach is outlined in Algorithm 1.\nThe learning rate a = 0.01,Lu,Lc are computed per batch by maintaining the required variables f(u), f(v), dC(u, v), dM (f(u), f(v)) in memory. These are implemented using Tensorflow’s eager execution framework."
    }, {
      "heading" : "4 Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Causal Evidence Graphs",
      "text" : "The causal evidence graphs we use contain phrases like “heavy rainfall” as causes and effects, which require us to learn the combined embeddings of the phrases. Restricting ourselves to just individual words would leave out the context required to understand the context to understand the causeeffect pairs. For example, the kind of effects “heavy\nAlgorithm 1 Faithful Embedding Training 1: Input: Pre-trained BERT based model g̃, causal\ngraph C, distance measures: dC , dM , 2: for e=1..epochs do 3: L = 0 4: for j=1..b do 5: u, v ∼ C : ∑ 1i(u,v)=0 = ∑ 1i(u,v)=1 6: Ls += CrossEnt(i(u, v), g̃(u, v)) 7: Ln += ∑ w∈Neigh(u) dM (f(u), f(w)) 8: Store f(u), f(v) to update Lu 9: Store dC(u, v), dM (f(u), f(v)) to up-\ndate Lc 10: end for 11: Update Lu,Lc and compute L (Eqn 7) 12: Backprop g̃ ← g̃ − a(∂L∂g̃ ) 13: end for\nrainfall” might have could be different from just “rainfall”. We thus utilize the contextual embedding framework used to learn language models in BERT (Devlin et al., 2019), as a way to learn contextual embeddings that align with a given graphical causal model. Note that there may be more than one causal model provided by experts based on their domains, and it is important to view our contribution as a way to align with domain expertise (for example, medical, legal, privacy, etc) with their respective causal models as a common mechanism to represent the said domain knowledge.\nWe use two causal graphs to construct their respective faithful embeddings, and demonstrate the utility of the embeddings in downstream tasks. The first causal graph we use is identical to the one used in (Sharp et al., 2016), which uses the 815,233 cause-effect pairs extracted from the Annotated Gigaword and Wikipedia dataset, and an equal number of random relation pairs that are not causal as negative samples. The second causal graph is extracted from the web by (Heindorf et al., 2020b), who use a bootstrapping approach with the initial pattern of “A causes B” and apply it to the ClueWeb12 web crawl dataset with 733,019,372 English web pages, between February and May 2012. From this web crawl, they provide a causal graph with 80,223 concept nodes and 199,803 causal links between the nodes. This graph has been sampled and validated by human annotators with over 96% precision. For our indirect evaluation based on downstream question answering tasks, we use the 3031 causal questions from Yahoo! Answers corpus (Sharp et al., 2016). These\nquestions are of the form “What causes X?”, and we use our faithful embeddings as a drop-in replacement for this causal QA task."
    }, {
      "heading" : "4.2 Metrics",
      "text" : "Evaluating embeddings intrinsically has often led to varying leaderboards (Jastrzebski et al., 2017), hence we evaluate our embeddings based on their ability to map to the cause-effect relationship directly. We measure the faithfulness of the trained embeddings, using 3 metrics, one per property as per Eqns 4, 5, 6. For the neighborhood condition, we measure the area under the precision-recall curve as we choose multiple thresholds to define the neighborhood in the embedding space to correspondingly identify the relevant neighbors in the causal graph. For the uniformity condition, we measure the means of the per-dimension values of the word embeddings and compute the 1st Wasserstein (Olkin and Pukelsheim, 1982) distance from the expected centroid of zero. We also perform a statistical test for uniform distribution, which measures the mean Kolmogorov-Smirnov (K-S) test statistic (Daniel, 1990) by bucketing embedding each dimension into 10 buckets. Since each dimension’s test statistic can either pass or fail the test based on the significance level, we present the total number of dimensions that pass the test at α = 0.05 significance level. Finally, to measure the distance correlation property, we report the Pearson correlation coefficient between distances in the causal graph and the embeddings on a held-out part of the causal graph. For the QA task, we report the precision-at-one (P@1), the fraction of test samples where the highest ranked answer is relevant and the mean reciprocal rank (MRR) (Manning et al., 2008), the inverse of the position of the correct answer in our ranking on the held-out question set provided by (Sharp et al., 2015)."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We evaluate our faithful embeddings by comparing them against two state-of-the-art approaches described in (Sharp et al., 2016) and (Veitch et al., 2020). cEmbedBi uses a bi-directional model, with the task of predicting the masked cause and effect word tokens. This approach uses separate embeddings for words used as causes and effects. Causal{BERT,RoBERTa} (Veitch et al., 2020) uses the fine-tuning technique for the binary classification of edge detection, similar to ours, on the pre-trained large-uncased model. We can thus compare the\ngains we get by incorporating faithfulness conditions on the embeddings in downstream tasks."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Faithfulness",
      "text" : "As shown in Tables 1 and 2, our Faithful-RoBERTa model outperforms Causal-{BERT, RoBERTa} and cEmbedBi (Sharp et al., 2016) on each of the three properties of faithfulness, namely the neighborhood, uniformity, and distance correlation, by more than 30%. Additionally, we report the correlation for Euclidean and Cosine similarity, despite not using it to optimize at training time. Faithful versions of the BERT and RoBERTa models increase the area under the curve of the precision-recall curve in detecting neighboring nodes of the Gigaword and CauseNet causal graphs by 21-23% and 17-20% respectively. In Figure 2, we present the precision-recall curve when we use the models for ranking causal pairs above non-causal pairs on the SemEval Task 8 tuples (Hendrickx et al., 2007) by varying the distance threshold in the embedding space which outlines the boundary of the neighboring nodes in the causal graph. This increase in accuracy for neighborhood detection indicates that incorporating the constraints during training time with our asymmetric causal embedding distance provides benefits in aligning the contextual embeddings as per the causal graph."
    }, {
      "heading" : "5.2 QA task",
      "text" : "To evaluate if learning faithful embeddings is useful for causal aligned downstream tasks, we evaluate the fine-tuned embeddings to be directly used for question answering. As used in (Fried et al., 2015), we use the maximum, minimum, average distance between words of the question and answer words and the overall distance between the composite question and answer vectors from the embedding. Note that since both cEmbedBi and Causal-{BERT, RoBERTa} are trained with cosine similarity in mind, we use the cosine similarity, but for our Faithful-{BERT, RoBERTa} models, the distance measure used to rank is the quasi-pseudo metric defined in Def 2. We use these 4 features to train an SVM ranker to re-rank candidate answers provided by the candidate retrieval tool (Jansen et al., 2014). We see in Table 3 that Faithful-RoBERTa increases both the precision of the first answer predicted by 10.2%, and the mean reciprocal rank by 10.8%. This means that not only is the first ranked answer more causally correct, but the retrieval of the correct answer in the top-k positions has improved. This improvement in an out-of-domain QA task by aligning the embeddings to an externally available causal graph demonstrates that benefits of faithfulness transfer to downstream tasks."
    }, {
      "heading" : "5.3 Re-alignment towards causation",
      "text" : "To understand the reason behind the improved performance, we performed a qualitative inspection of 100 randomly sampled word pairs from the Gigaword causal graph 1 that are at varying distances in the original pre-trained embedding and trace\n1https://github.com/ananthnyu/faithful-causal-rep/\nhow they have re-aligned after fine-tuning with the faithfulness objective. We annotate each of these word-pairs as being either causal or not as shown in the confusion matrix with examples in Table 4. In Figure 3, we see re-alignment of these word pairs from association based RoBERTa embeddings to the causally aligned Faithful-RoBERTa embedding space, that is, causal word pairs (blue and orange) move closer, and non-causal word pairs (green and red) move further based on the quasi-pseudo metric dM . Specifically, the associative but non-causal word pairs (green) have moved further in FaithfulRoBERTa, while the non-associative but causal word pairs (orange) have moved closer. We see that in the cosine-similarity based RoBERTa, the causal word pairs had a mean distance of 0.48, while in the quasi-pseudo metric based FaithfulRoBERTa, the mean distance between the causal word pairs reduced to 0.28. The distances are normalized between 0 and 1 based on the maximum and minimum values of distances (cosine or dM ) in the sampled word-pairs.\nWe further analyzed how these associative and causal re-alignments impacted the causal QA task by categorizing the word pairs into three types of variables - mediators, colliders and confounders. Mediators: For the question, “What causes a tornado?”, the answer involves “thunderstorms”, which is a mediator caused by “high pressure”. We see that “high pressure” is now much closer to “tornado” in Faithful-RoBERTa than baseline embeddings. Colliders: For the question, “What\ncauses persistent cough?”, the colliders “smoking” and “asthma” have moved further based on dM in Faithful-RoBERTa. Confounders: For questions with confounders like, “What causes indigestion?”, the confounding links “anxiety→ indigestion”, and “anxiety→ insomnia” are near, but “insomnia→ indigestion”, is far. This further demonstrates the utility of incorporating faithfulness over multiple nodes of the graph, in addition to pairwise causal link prediction."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We show that the faithfulness of text embeddings to a causal graph is important for causal inferencealigned downstream tasks. By incorporating the three faithfulness properties of neighborhood, uniformity, and distance correlation through regularization constraints while learning embeddings, we improve the precision of the first ranked answer in the causal QA task by 10.2%. We show that this is due to causal re-alignment of embeddings as per an asymmetric pseudo-distance metric."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Sam Bowman for his feedback to the draft version of this manuscript."
    } ],
    "references" : [ {
      "title" : "Learning edge representations via low-rank asymmetric projections",
      "author" : [ "Sami Abu-El-Haija", "Bryan Perozzi", "Rami Al-Rfou." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management.",
      "citeRegEx" : "Abu.El.Haija et al\\.,? 2017",
      "shortCiteRegEx" : "Abu.El.Haija et al\\.",
      "year" : 2017
    }, {
      "title" : "Causal inference and the data-fusion problem",
      "author" : [ "Elias Bareinboim", "Judea Pearl." ],
      "venue" : "Proceedings of the National Academy of Sciences, 113(27):7345– 7352.",
      "citeRegEx" : "Bareinboim and Pearl.,? 2016",
      "shortCiteRegEx" : "Bareinboim and Pearl.",
      "year" : 2016
    }, {
      "title" : "Lorentzian manifolds and causal sets as partially ordered measure spaces",
      "author" : [ "Luca Bombelli", "Johan Noldus", "Julio Tafoya" ],
      "venue" : null,
      "citeRegEx" : "Bombelli et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bombelli et al\\.",
      "year" : 2013
    }, {
      "title" : "Causal embeddings for recommendation",
      "author" : [ "Stephen Bonner", "Flavian Vasile." ],
      "venue" : "CoRR, abs/1706.07639.",
      "citeRegEx" : "Bonner and Vasile.,? 2017",
      "shortCiteRegEx" : "Bonner and Vasile.",
      "year" : 2017
    }, {
      "title" : "Multi-level cause-effect systems",
      "author" : [ "Krzysztof Chalupka", "Frederick Eberhardt", "Pietro Perona." ],
      "venue" : "Artificial Intelligence and Statistics, pages 361–369.",
      "citeRegEx" : "Chalupka et al\\.,? 2016",
      "shortCiteRegEx" : "Chalupka et al\\.",
      "year" : 2016
    }, {
      "title" : "Directed graph embedding",
      "author" : [ "Mo Chen", "Qiong Yang", "Xiaoou Tang" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Chen et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2007
    }, {
      "title" : "Causal discovery via reproducing kernel hilbert space embeddings",
      "author" : [ "Zhitang Chen", "Kun Zhang", "Laiwan Chan", "Bernhard Schölkopf." ],
      "venue" : "Neural computation, 26(7):1484–1517.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Three-dimensional graph drawing",
      "author" : [ "Robert F. Cohen", "Peter Eades", "Tao Lin", "Frank Ruskey." ],
      "venue" : "Graph Drawing, pages 1–11, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Cohen et al\\.,? 1995",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 1995
    }, {
      "title" : "Kolmogorov-smirnov onesample test",
      "author" : [ "Wayne W. Daniel." ],
      "venue" : "Applied Nonparametric Statistics (2nd ed.), pages 319–330.",
      "citeRegEx" : "Daniel.,? 1990",
      "shortCiteRegEx" : "Daniel.",
      "year" : 1990
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Is a single embedding enough? learning node representations that capture multiple social contexts",
      "author" : [ "Alessandro Epasto", "Bryan Perozzi." ],
      "venue" : "CoRR, abs/1905.02138.",
      "citeRegEx" : "Epasto and Perozzi.,? 2019",
      "shortCiteRegEx" : "Epasto and Perozzi.",
      "year" : 2019
    }, {
      "title" : "Causalm: Causal model explanation through counterfactual language models",
      "author" : [ "Amir Feder", "Nadav Oved", "Uri Shalit", "Roi Reichart." ],
      "venue" : "CoRR, abs/2005.13407.",
      "citeRegEx" : "Feder et al\\.,? 2020",
      "shortCiteRegEx" : "Feder et al\\.",
      "year" : 2020
    }, {
      "title" : "Low-rank tensors for verbs in compositional distributional semantics",
      "author" : [ "Daniel Fried", "Tamara Polajnar", "Stephen Clark." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Fried et al\\.,? 2015",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2015
    }, {
      "title" : "An axiomatic characterization of causal counterfactuals",
      "author" : [ "David Galles", "Judea Pearl." ],
      "venue" : "Foundations of Science, 3(1):151–182.",
      "citeRegEx" : "Galles and Pearl.,? 1998",
      "shortCiteRegEx" : "Galles and Pearl.",
      "year" : 1998
    }, {
      "title" : "Modeling document-level causal structures for event causal relation identification",
      "author" : [ "Lei Gao", "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "In",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "What action causes this? towards naive physical action-effect prediction",
      "author" : [ "Qiaozi Gao", "Shaohua Yang", "Joyce Chai", "Lucy Vanderwende." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Asymmetric distances for binary embeddings",
      "author" : [ "A. Gordo", "F. Perronnin." ],
      "venue" : "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’11, page 729736, USA. IEEE Computer Society.",
      "citeRegEx" : "Gordo and Perronnin.,? 2011",
      "shortCiteRegEx" : "Gordo and Perronnin.",
      "year" : 2011
    }, {
      "title" : "Embedding time expressions for deep temporal ordering models",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4400– 4406, Florence, Italy. Association for Computational",
      "citeRegEx" : "Goyal and Durrett.,? 2019",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2019
    }, {
      "title" : "Why attention is not explanation: Surgical intervention and causal reasoning about neural models",
      "author" : [ "Christopher Grimsley", "Elijah Mayfield", "Julia R.S. Bursten." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference,",
      "citeRegEx" : "Grimsley et al\\.,? 2020",
      "shortCiteRegEx" : "Grimsley et al\\.",
      "year" : 2020
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "Aditya Grover", "Jure Leskovec" ],
      "venue" : null,
      "citeRegEx" : "Grover and Leskovec.,? \\Q2016\\E",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "Representation learning on graphs: Methods and applications",
      "author" : [ "William L. Hamilton", "Rex Ying", "Jure Leskovec." ],
      "venue" : "CoRR, abs/1709.05584.",
      "citeRegEx" : "Hamilton et al\\.,? 2017",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2017
    }, {
      "title" : "Causenet: Towards a causality graph extracted from the web",
      "author" : [ "Stefan Heindorf", "Yan Scholten", "Henning Wachsmuth", "Axel-Cyrille Ngonga Ngomo", "Martin Potthast." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information",
      "citeRegEx" : "Heindorf et al\\.,? 2020a",
      "shortCiteRegEx" : "Heindorf et al\\.",
      "year" : 2020
    }, {
      "title" : "Causenet: Towards a causality graph extracted from the web",
      "author" : [ "Stefan Heindorf", "Yan Scholten", "Henning Wachsmuth", "Axel-Cyrille Ngonga Ngomo", "Martin Potthast." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information",
      "citeRegEx" : "Heindorf et al\\.,? 2020b",
      "shortCiteRegEx" : "Heindorf et al\\.",
      "year" : 2020
    }, {
      "title" : "ILK: Machine learning of semantic relations with shallow features and almost no data",
      "author" : [ "Iris Hendrickx", "Roser Morante", "Caroline Sporleder", "Antal van den Bosch." ],
      "venue" : "Proceedings of the Fourth",
      "citeRegEx" : "Hendrickx et al\\.,? 2007",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2007
    }, {
      "title" : "Multiplier and gradient methods",
      "author" : [ "M.R. Hestenes." ],
      "venue" : "Journal of Optimization Theory and Applications, 4:303–320.",
      "citeRegEx" : "Hestenes.,? 1969",
      "shortCiteRegEx" : "Hestenes.",
      "year" : 1969
    }, {
      "title" : "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198–4205, Online",
      "author" : [ "Alon Jacovi", "Yoav Goldberg." ],
      "venue" : "As-",
      "citeRegEx" : "Jacovi and Goldberg.,? 2020",
      "shortCiteRegEx" : "Jacovi and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Discourse complements lexical semantics for nonfactoid answer reranking",
      "author" : [ "Peter Jansen", "Mihai Surdeanu", "Peter Clark." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Jansen et al\\.,? 2014",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2014
    }, {
      "title" : "How to evaluate word embeddings? on importance of data efficiency and simple supervised tasks",
      "author" : [ "Stanisaw Jastrzebski", "Damian Leniak", "Wojciech Marian Czarnecki" ],
      "venue" : null,
      "citeRegEx" : "Jastrzebski et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jastrzebski et al\\.",
      "year" : 2017
    }, {
      "title" : "Text and causal inference: A review of using text to remove confounding from causal estimates",
      "author" : [ "Katherine Keith", "David Jensen", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Keith et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Keith et al\\.",
      "year" : 2020
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Cache-based natural language model for speech recognition",
      "author" : [ "Roland Kuhn", "Renato De Mori." ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 12:570–583.",
      "citeRegEx" : "Kuhn and Mori.,? 1990",
      "shortCiteRegEx" : "Kuhn and Mori.",
      "year" : 1990
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "Daniel D. Lee", "H. Sebastian Seung." ],
      "venue" : "Proceedings of the 13th International Conference on Neural Information Processing Systems, NIPS’00, page 535541, Cambridge, MA, USA. MIT Press.",
      "citeRegEx" : "Lee and Seung.,? 2000",
      "shortCiteRegEx" : "Lee and Seung.",
      "year" : 2000
    }, {
      "title" : "Specializing word embeddings (for parsing) by information bottleneck",
      "author" : [ "Xiang Lisa Li", "Jason Eisner." ],
      "venue" : "CoRR, abs/1910.00163.",
      "citeRegEx" : "Li and Eisner.,? 2019",
      "shortCiteRegEx" : "Li and Eisner.",
      "year" : 2019
    }, {
      "title" : "Learning network embedding with community structural information",
      "author" : [ "Yu Li", "Ying Wang", "Tingting Zhang", "Jiawei Zhang", "Yi Chang." ],
      "venue" : "IJCAI, pages 2937–2943.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning deep generative models of graphs",
      "author" : [ "Yujia Li", "Oriol Vinyals", "Chris Dyer", "Razvan Pascanu", "Peter Battaglia" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "K-BERT: enabling language representation with knowledge graph",
      "author" : [ "Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Qi Ju", "Haotang Deng", "Ping Wang." ],
      "venue" : "CoRR, abs/1909.07606.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Vgcn-bert: Augmenting bert with graph embedding for text classification",
      "author" : [ "Zhibin Lu", "Pan Du", "Jian-Yun Nie" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to information retrieval",
      "author" : [ "Christopher D Manning", "Hinrich Schütze", "Prabhakar Raghavan." ],
      "venue" : "Cambridge university press.",
      "citeRegEx" : "Manning et al\\.,? 2008",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Quasimetric embeddings and their applications",
      "author" : [ "Facundo Mémoli", "Anastasios Sidiropoulos", "Vijay Sridhar." ],
      "venue" : "CoRR, abs/1608.01396.",
      "citeRegEx" : "Mémoli et al\\.,? 2016",
      "shortCiteRegEx" : "Mémoli et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Everything happens for a reason: Discovering the purpose of actions in procedural text",
      "author" : [ "Bhavana Dalvi Mishra", "Niket Tandon", "Antoine Bosselut", "Wen tau Yih", "Peter Clark" ],
      "venue" : null,
      "citeRegEx" : "Mishra et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2019
    }, {
      "title" : "On completeness of quasipseudometric spaces",
      "author" : [ "Seithuti P Moshokoa." ],
      "venue" : "International journal of mathematics and mathematical sciences, 2005(18):2933– 2943.",
      "citeRegEx" : "Moshokoa.,? 2005",
      "shortCiteRegEx" : "Moshokoa.",
      "year" : 2005
    }, {
      "title" : "Language understanding for text-based games using deep reinforcement learning",
      "author" : [ "Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay." ],
      "venue" : "arXiv preprint arXiv:1506.08941.",
      "citeRegEx" : "Narasimhan et al\\.,? 2015",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint reasoning for temporal and causal relations",
      "author" : [ "Qiang Ning", "Zhili Feng", "Hao Wu", "Dan Roth." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2278–2288, Melbourne, Aus-",
      "citeRegEx" : "Ning et al\\.,? 2018",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "The distance between two random vectors with given dispersion matrices",
      "author" : [ "I. Olkin", "F. Pukelsheim." ],
      "venue" : "Linear Algebra and its Applications, 48:257 – 263.",
      "citeRegEx" : "Olkin and Pukelsheim.,? 1982",
      "shortCiteRegEx" : "Olkin and Pukelsheim.",
      "year" : 1982
    }, {
      "title" : "Enriching bert with knowledge graph embeddings for document classification",
      "author" : [ "Malte Ostendorff", "Peter Bourgonje", "Maria Berger", "Julian Moreno-Schneider", "Georg Rehm", "Bela Gipp" ],
      "venue" : null,
      "citeRegEx" : "Ostendorff et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ostendorff et al\\.",
      "year" : 2019
    }, {
      "title" : "Asymmetric transitivity preserving graph embedding",
      "author" : [ "Mingdong Ou", "Peng Cui", "Jian Pei", "Ziwei Zhang", "Wenwu Zhu." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, page",
      "citeRegEx" : "Ou et al\\.,? 2016",
      "shortCiteRegEx" : "Ou et al\\.",
      "year" : 2016
    }, {
      "title" : "Causal inference in statistics: An overview",
      "author" : [ "Judea Pearl." ],
      "venue" : "Statistics Surveys.",
      "citeRegEx" : "Pearl.,? 2009",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2009
    }, {
      "title" : "The book of why: the new science of cause and effect",
      "author" : [ "Judea Pearl", "Dana Mackenzie." ],
      "venue" : "Basic books.",
      "citeRegEx" : "Pearl and Mackenzie.,? 2018",
      "shortCiteRegEx" : "Pearl and Mackenzie.",
      "year" : 2018
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena." ],
      "venue" : "CoRR, abs/1403.6652.",
      "citeRegEx" : "Perozzi et al\\.,? 2014",
      "shortCiteRegEx" : "Perozzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Adjusting for confounding with text matching",
      "author" : [ "Margaret E. Roberts", "Brandon M. Stewart", "Richard A. Nielsen." ],
      "venue" : "American Journal of Political Science, 64(4):887–903.",
      "citeRegEx" : "Roberts et al\\.,? 2020",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Causal consistency of structural equation models",
      "author" : [ "Paul K Rubenstein", "Sebastian Weichwald", "Stephan Bongers", "Joris M Mooij", "Dominik Janzing", "Moritz Grosse-Wentrup", "Bernhard Schölkopf." ],
      "venue" : "arXiv preprint arXiv:1707.00819.",
      "citeRegEx" : "Rubenstein et al\\.,? 2017",
      "shortCiteRegEx" : "Rubenstein et al\\.",
      "year" : 2017
    }, {
      "title" : "Estimating causal effects of treatments in randomized and nonrandomized studies",
      "author" : [ "Donald B Rubin." ],
      "venue" : "Journal of educational Psychology, 66(5):688.",
      "citeRegEx" : "Rubin.,? 1974",
      "shortCiteRegEx" : "Rubin.",
      "year" : 1974
    }, {
      "title" : "Causality for machine learning",
      "author" : [ "Bernhard Schölkopf." ],
      "venue" : "CoRR, abs/1911.10500.",
      "citeRegEx" : "Schölkopf.,? 2019",
      "shortCiteRegEx" : "Schölkopf.",
      "year" : 2019
    }, {
      "title" : "Spinning straw into gold: Using free text to train monolingual alignment models for non-factoid question answering",
      "author" : [ "Rebecca Sharp", "Peter Jansen", "Mihai Surdeanu", "Peter Clark." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chap-",
      "citeRegEx" : "Sharp et al\\.,? 2015",
      "shortCiteRegEx" : "Sharp et al\\.",
      "year" : 2015
    }, {
      "title" : "Creating causal embeddings for question answering with minimal supervision",
      "author" : [ "Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond" ],
      "venue" : null,
      "citeRegEx" : "Sharp et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sharp et al\\.",
      "year" : 2016
    }, {
      "title" : "Causation, Prediction, and Search, volume 81",
      "author" : [ "Peter Spirtes", "Clark Glymour", "Richard Scheines" ],
      "venue" : null,
      "citeRegEx" : "Spirtes et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 1993
    }, {
      "title" : "ATP: directed graph embedding with asymmetric transitivity preservation",
      "author" : [ "Jiankai Sun", "Bortik Bandyopadhyay", "Armin Bashizade", "Jiongqian Liang", "P. Sadayappan", "Srinivasan Parthasarathy." ],
      "venue" : "CoRR, abs/1811.00839.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "LINE: largescale information network embedding",
      "author" : [ "Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei." ],
      "venue" : "CoRR, abs/1503.03578.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural granger causality for nonlinear time series",
      "author" : [ "Alex Tank", "Ian Covert", "Nicholas Foti", "Ali Shojaie", "Emily Fox" ],
      "venue" : null,
      "citeRegEx" : "Tank et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Tank et al\\.",
      "year" : 2018
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "Joshua B Tenenbaum", "Vin De Silva", "John C Langford." ],
      "venue" : "science, 290(5500):2319–2323.",
      "citeRegEx" : "Tenenbaum et al\\.,? 2000",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "Max-margin deepwalk: Discriminative learning of network representation",
      "author" : [ "Cunchao Tu", "Weicheng Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16,",
      "citeRegEx" : "Tu et al\\.,? 2016",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "Adapting text embeddings for causal inference",
      "author" : [ "Victor Veitch", "Dhanya Sridhar", "David M. Blei" ],
      "venue" : null,
      "citeRegEx" : "Veitch et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Veitch et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Velikovi", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Li", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Velikovi et al\\.,? 2018",
      "shortCiteRegEx" : "Velikovi et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph kernels",
      "author" : [ "S.V.N. Vishwanathan", "Nicol N. Schraudolph", "Risi Kondor", "Karsten M. Borgwardt." ],
      "venue" : "Journal of Machine Learning Research, 11(40):1201–1242.",
      "citeRegEx" : "Vishwanathan et al\\.,? 2010",
      "shortCiteRegEx" : "Vishwanathan et al\\.",
      "year" : 2010
    }, {
      "title" : "Community preserving network embedding",
      "author" : [ "Xiao Wang", "Peng Cui", "Jing Wang", "Jian Pei", "Wenwu Zhu", "Shiqiang Yang." ],
      "venue" : "AAAI, volume 17, pages 3298239–3298270.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Distributed representation of words in cause and effect spaces",
      "author" : [ "Zhipeng Xie", "Feiteng Mu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7330–7337.",
      "citeRegEx" : "Xie and Mu.,? 2019",
      "shortCiteRegEx" : "Xie and Mu.",
      "year" : 2019
    }, {
      "title" : "Graphrnn: A deep generative model for graphs",
      "author" : [ "Jiaxuan You", "Rex Ying", "Xiang Ren", "William L. Hamilton", "Jure Leskovec." ],
      "venue" : "CoRR, abs/1802.08773.",
      "citeRegEx" : "You et al\\.,? 2018",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2018
    }, {
      "title" : "Constructing and embedding abstract event causality networks from text snippets",
      "author" : [ "Sendong Zhao", "Quan Wang", "Sean Massung", "Bing Qin", "Ting Liu", "Bin Wang", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the Tenth ACM International Conference on Web Search",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    }, {
      "title" : "Scalable graph embedding for asymmetric proximity",
      "author" : [ "Chang Zhou", "Yuqiong Liu", "Xiaofei Liu", "Zhongyi Liu", "Jun Gao" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial directed graph embedding",
      "author" : [ "Shijie Zhu", "Jianxin Li", "Hao Peng", "Senzhang Wang", "Philip S. Yu", "Lifang He" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 56,
      "context" : "Approximating the notion of causality with a similarity-based distance metric using separate vector representations for cause and effect tokens has led to significant improvement in the performance of downstream tasks like Question Answering, but can be too restrictive to generalize over unobserved edges in larger causal graphs (Sharp et al., 2016).",
      "startOffset" : 330,
      "endOffset" : 350
    }, {
      "referenceID" : 44,
      "context" : "In downstream causal reasoning based tasks like dialog systems (Ning et al., 2018), explanation generation (Grimsley et al.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : ", 2018), explanation generation (Grimsley et al., 2020), question answering (Sharp et al.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 56,
      "context" : ", 2020), question answering (Sharp et al., 2016), it is important to align the models with the corresponding causal graph.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "However, words that have low cosine similarity capture various semantic similarities, like relatedness, synonyms, replaceability, or complementarity, but not directionality (Hamilton et al., 2017).",
      "startOffset" : 173,
      "endOffset" : 196
    }, {
      "referenceID" : 39,
      "context" : "symmetric distance in an embedding space cannot convey the directed causal semantics for a downstream task (Mémoli et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 63,
      "context" : "Prior work on capturing sufficient information for causal inference tasks from embeddings aims to directly use them for average treatment effect estimation (Veitch et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "Unlike prior work, which aims to learn a causal aware embedding restricted to direct link prediction (Hamilton et al., 2017), we propose faithfulness constraints so that causal word embeddings aims to preserve the partial ordering over pairwise distances in the directed causal graph.",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "It has been shown that NLP models need to understand such causal links that persist in the real world for safe deployment (Gao et al., 2018; Mishra et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 161
    }, {
      "referenceID" : 41,
      "context" : "It has been shown that NLP models need to understand such causal links that persist in the real world for safe deployment (Gao et al., 2018; Mishra et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 161
    }, {
      "referenceID" : 48,
      "context" : "Causal Inference, as outlined in (Pearl, 2009) formalizes cause and effects discovered through intervention based experiments and communicates them via directed acyclic graphs.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 54,
      "context" : "With the availability of large observational datasets for machine learning, various methods and assumptions have been proposed for learning causal graphs (Schölkopf, 2019), data fusion and transportability properties C: causal graph M: uniform manifold",
      "startOffset" : 154,
      "endOffset" : 171
    }, {
      "referenceID" : 57,
      "context" : "Specifically, our work closely aligns with the assumption of faithfulness (Spirtes et al., 1993),",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "We extend the faithfulness assumption to be reflected in embeddings learnt by a masked language model (Devlin et al., 2019; Liu et al., 2019b) for downstream tasks.",
      "startOffset" : 102,
      "endOffset" : 142
    }, {
      "referenceID" : 36,
      "context" : "We extend the faithfulness assumption to be reflected in embeddings learnt by a masked language model (Devlin et al., 2019; Liu et al., 2019b) for downstream tasks.",
      "startOffset" : 102,
      "endOffset" : 142
    }, {
      "referenceID" : 25,
      "context" : "proposed by (Jacovi and Goldberg, 2020) used to evaluate models for interpretability of models used for downstream tasks.",
      "startOffset" : 12,
      "endOffset" : 39
    }, {
      "referenceID" : 56,
      "context" : "Instead, our work builds on embeddings learnt in (Sharp et al., 2016), given a causal model and learn embeddings that are bootstrapped using a small set of cause-effect seeds.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Causal models have also been used to learn auxiliary tasks (Feder et al., 2020) using adversarial training to ensure that a language model learns causal-inspired representations.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 63,
      "context" : "In principle, we adopt a similar approach to (Veitch et al., 2020) of fine-tuning towards a causal link prediction task.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 69,
      "context" : "This is in contrast with approaches that use energy-based transition vectors used to represent the cause-to-effect and effectto-cause links (Zhao et al., 2017).",
      "startOffset" : 140,
      "endOffset" : 159
    }, {
      "referenceID" : 32,
      "context" : "841 proposed for information bottlenecks in word embeddings (Li and Eisner, 2019; Goyal and Durrett, 2019), text-based games (Narasimhan et al.",
      "startOffset" : 60,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "841 proposed for information bottlenecks in word embeddings (Li and Eisner, 2019; Goyal and Durrett, 2019), text-based games (Narasimhan et al.",
      "startOffset" : 60,
      "endOffset" : 106
    }, {
      "referenceID" : 43,
      "context" : "841 proposed for information bottlenecks in word embeddings (Li and Eisner, 2019; Goyal and Durrett, 2019), text-based games (Narasimhan et al., 2015), activation links in neuroscience (Chalupka et al.",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : ", 2015), activation links in neuroscience (Chalupka et al., 2016), causal consistency with ordinary differential equations (Rubenstein et al.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 52,
      "context" : ", 2016), causal consistency with ordinary differential equations (Rubenstein et al., 2017) and temporal Granger Causality (Tank et al.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 60,
      "context" : ", 2017) and temporal Granger Causality (Tank et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "For an extensive survey of using text for causal inference tasks, we refer to (Keith et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Learning asymmetric transitive graph representations which generalize the causal graph have been studied extensively in Information Retrieval (Chen et al., 2007; Epasto and Perozzi, 2019; Li et al., 2019; Grover and Leskovec, 2016).",
      "startOffset" : 142,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Learning asymmetric transitive graph representations which generalize the causal graph have been studied extensively in Information Retrieval (Chen et al., 2007; Epasto and Perozzi, 2019; Li et al., 2019; Grover and Leskovec, 2016).",
      "startOffset" : 142,
      "endOffset" : 231
    }, {
      "referenceID" : 33,
      "context" : "Learning asymmetric transitive graph representations which generalize the causal graph have been studied extensively in Information Retrieval (Chen et al., 2007; Epasto and Perozzi, 2019; Li et al., 2019; Grover and Leskovec, 2016).",
      "startOffset" : 142,
      "endOffset" : 231
    }, {
      "referenceID" : 19,
      "context" : "Learning asymmetric transitive graph representations which generalize the causal graph have been studied extensively in Information Retrieval (Chen et al., 2007; Epasto and Perozzi, 2019; Li et al., 2019; Grover and Leskovec, 2016).",
      "startOffset" : 142,
      "endOffset" : 231
    }, {
      "referenceID" : 50,
      "context" : "They either utilize a random walk learning technique (Perozzi et al., 2014) or matrix factorization techniques (Lee",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "While we can learn faithful 3-dimension embeddings for any fixed finite undirected graph deterministically (Cohen et al., 1995), fine-tuning pre-",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 65,
      "context" : "trained word embeddings such that they generalize over all sub-graphs in a directed graph is known to be a hard graph kernel design problem that scales cubically with the number of nodes (Vishwanathan et al., 2010).",
      "startOffset" : 187,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "ery that is not yet captured in a graphical notation (Chen et al., 2014).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 71,
      "context" : "Recently, Graph neural networks that capture the graph neighborhood structure have been employed in link prediction (Zhu et al., 2020; Abu-El-Haija et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Recently, Graph neural networks that capture the graph neighborhood structure have been employed in link prediction (Zhu et al., 2020; Abu-El-Haija et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 68,
      "context" : "In (You et al., 2018), the problem is reduced to that of sequence prediction by reducing the graph to breadth-first search based deterministic sequence.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 34,
      "context" : "In (Li et al., 2018), node embeddings are updated after several rounds of message passing, while in (Tu et al.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 62,
      "context" : ", 2018), node embeddings are updated after several rounds of message passing, while in (Tu et al., 2016) a variant of the random walk is incorporated with a max-margin discriminative constraint.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 64,
      "context" : "In (Velikovi et al., 2018), models are learned by attending over the neighborhood of nodes for context, while (Kipf and Welling, 2016) apply spectral graph convolutions for a selfsupervised learning task.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : ", 2018), models are learned by attending over the neighborhood of nodes for context, while (Kipf and Welling, 2016) apply spectral graph convolutions for a selfsupervised learning task.",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 64,
      "context" : "We adopt the incremental approach proposed in (Velikovi et al., 2018) which does not rely on knowing the entire graph structure apriori and fine-tune on cause-effect pairs for the link prediction task on a pre-trained BERT-based language model.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 48,
      "context" : "Causal inference (Pearl, 2009) aims to understand the cause and effect relationships between events.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 53,
      "context" : "The findings of such studies are formalized using frameworks like Rubin Causal Models (Rubin, 1974), Structural Causal Models (Pearl, 2009), etc.",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 48,
      "context" : "The findings of such studies are formalized using frameworks like Rubin Causal Models (Rubin, 1974), Structural Causal Models (Pearl, 2009), etc.",
      "startOffset" : 126,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "there are differences in abstractions between them, there is formal equivalence (Galles and Pearl, 1998) in modeling counterfactuals (“What is the effect when the cause is intervened?”) and we refer the reader to (Pearl and Mackenzie, 2018) for a primer in causal modeling.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 49,
      "context" : "there are differences in abstractions between them, there is formal equivalence (Galles and Pearl, 1998) in modeling counterfactuals (“What is the effect when the cause is intervened?”) and we refer the reader to (Pearl and Mackenzie, 2018) for a primer in causal modeling.",
      "startOffset" : 213,
      "endOffset" : 240
    }, {
      "referenceID" : 48,
      "context" : "In this paper, we assume a graphical structural causal modelC (Pearl, 2009) is given, whose nodes",
      "startOffset" : 62,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "842 man annotators who with the help of web crawlers (Heindorf et al., 2020a) and other information retrieval tools (Sharp et al.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 56,
      "context" : ", 2020a) and other information retrieval tools (Sharp et al., 2016) produce a directed graphical causal model as shown in Figure 1.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "The faithfulness property was first proposed for any two causal spaces in (Bombelli et al., 2013) in the domain of quantum physics with the space-time dimension.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "However, the distance measure in the embedding space faces challenges in evaluation of simple supervised tasks (Jastrzebski et al., 2017).",
      "startOffset" : 111,
      "endOffset" : 137
    }, {
      "referenceID" : 42,
      "context" : "Our chosen distance measure, hence should follow the properties of quasi-pseudo metrics, defined as follows in (Moshokoa, 2005):",
      "startOffset" : 111,
      "endOffset" : 127
    }, {
      "referenceID" : 67,
      "context" : "If the cause phrase u has pword tokens, and the effect phrase v has q word tokens, we choose the MaxMatching method given in (Xie and Mu, 2019) in our definition of dM by iterating through all pairs of words (vb, ua) : vb 6= ua.",
      "startOffset" : 125,
      "endOffset" : 143
    }, {
      "referenceID" : 63,
      "context" : "This supervised learning is achieved by following the technique of fine-tuning as proposed in (Veitch et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "models based on transformers like BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : ", 2019), RoBERTa (Liu et al., 2019b) learn contextual embeddings of words or tokens by optimizing for the self-supervision task of predicting randomly masked tokens in a sentence.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 45,
      "context" : "Since checking for true uniformity can be computationally intractable, we approximate by computing the per-dimension aggregate of all the word embeddings and compute the Wasserstein distance (Olkin and Pukelsheim, 1982) between the observed distribution and the expected uniform distribution centered around zero (0m).",
      "startOffset" : 191,
      "endOffset" : 219
    }, {
      "referenceID" : 24,
      "context" : "The contribution of each of the above losses are combined using the Augmented Lagrangian method (Hestenes, 1969) and controlled using 3 parameters α, β, γ as follows:",
      "startOffset" : 96,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "(Devlin et al., 2019), as a way to learn contextual embeddings that align with a given graphical causal model.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 56,
      "context" : "The first causal graph we use is identical to the one used in (Sharp et al., 2016), which uses the 815,233 cause-effect pairs extracted from the Annotated Gigaword and Wikipedia dataset, and an equal number of random relation pairs that are not causal as negative samples.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "The second causal graph is extracted from the web by (Heindorf et al., 2020b),",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 56,
      "context" : "For our indirect evaluation based on downstream question answering tasks, we use the 3031 causal questions from Yahoo! Answers corpus (Sharp et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "Evaluating embeddings intrinsically has often led to varying leaderboards (Jastrzebski et al., 2017), hence we evaluate our embeddings based on their",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 45,
      "context" : "For the uniformity condition, we measure the means of the per-dimension values of the word embeddings and compute the 1st Wasserstein (Olkin and Pukelsheim, 1982) distance from the expected centroid of zero.",
      "startOffset" : 134,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "statistic (Daniel, 1990) by bucketing embedding each dimension into 10 buckets.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 38,
      "context" : "precision-at-one (P@1), the fraction of test samples where the highest ranked answer is relevant and the mean reciprocal rank (MRR) (Manning et al., 2008), the inverse of the position of the correct answer in our ranking on the held-out question set provided by (Sharp et al.",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 55,
      "context" : ", 2008), the inverse of the position of the correct answer in our ranking on the held-out question set provided by (Sharp et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 56,
      "context" : "We evaluate our faithful embeddings by comparing them against two state-of-the-art approaches described in (Sharp et al., 2016) and (Veitch et al.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 63,
      "context" : "Causal{BERT,RoBERTa} (Veitch et al., 2020) uses the fine-tuning technique for the binary classification of edge detection, similar to ours, on the pre-trained large-uncased model.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 56,
      "context" : "cEmbedBi (Sharp et al., 2016) on each of the three properties of faithfulness, namely the neighborhood, uniformity, and distance correlation, by more than 30%.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "In Figure 2, we present the precision-recall curve when we use the models for ranking causal pairs above non-causal pairs on the SemEval Task 8 tuples (Hendrickx et al., 2007) by varying the distance threshold in the embedding space which outlines the boundary of the neighboring nodes in the causal graph.",
      "startOffset" : 151,
      "endOffset" : 175
    }, {
      "referenceID" : 26,
      "context" : "vided by the candidate retrieval tool (Jansen et al., 2014).",
      "startOffset" : 38,
      "endOffset" : 59
    } ],
    "year" : 2021,
    "abstractText" : "Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation. However, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks. In this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs. By incorporating these faithfulness properties, we learn text embeddings that are 31.3% more faithful to human validated causal graphs with about 800K and 200K causal links and achieve 21.1% better Precision-Recall AUC in a link prediction fine-tuning task. Further, in a crowdsourced causal question-answering task on Yahoo! Answers with questions of the form “What causes X?”, our faithful embeddings achieved a precision of the first ranked answer (P@1) of 41.07%, outperforming the existing baseline by 10.2%.",
    "creator" : "LaTeX with hyperref package"
  }
}