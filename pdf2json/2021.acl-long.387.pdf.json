{
  "name" : "2021.acl-long.387.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation",
    "authors" : [ "Xingyi Yang", "Fenglong Ma" ],
    "emails" : [ "x3yang@eng.ucsd.edu", "muchao@psu.edu", "quyou@microsoft.com", "fenglong@psu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5000–5009\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5000"
    }, {
      "heading" : "1 Introduction",
      "text" : "Medical report generation is the task of generating reports based on medical images, such as radiology and pathology images. Given that this task is timeconsuming and cumbersome, researchers endeavor to relieve the burden of physicians by automatically generating the findings and descriptions from medical images with machine learning techniques.\nExisting studies can be roughly divided into two categories, i.e., generation-based and retrievalbased approaches. Generation-based methods, including LRCN (Donahue et al., 2015), CoAtt (Jing\n∗This work was done when Xingyi Yang remotely worked with Dr. Fenglong Ma.\n†Corresponding author\net al., 2018), and MvH+AttL (Yuan et al., 2019), focus on generating image captions with a encoderdecoder model that leverage image features. However, they are unable to produce linguistically diverse descriptions and depict rare but prominent medical findings. On the other hand, Retrievalbased methods such as HRGR-Agent (Li et al., 2018) and KEPP (Li et al., 2019), pay attention to memorizing templates to generate standardized reports from a predefined retrieval database. However, the quality of generated reports significantly depends on the manually curated template database. Besides, they only use sentence-level templates for the generation but ignore to learn the report-level templates, which prevent them from generating more accurate reports.\nTo address the aforementioned issues, we propose a new framework called MedWriter as shown in Figure 1. MedWriter introduces a novel hierarchical retrieval mechanism working with a hierarchical language decoder to automatically learn the dynamic report and sentence templates from the data for generating accurate and professional medical reports. MedWriter is inspired by the process of how physicians write medical reports in real life. They keep report templates in mind and then generate reports for new images by using the key information that they find in the medical images to update the templates sentence by sentence.\nIn particular, we use three modules to mimic this process. First, MedWriter generates reportlevel templates from the Visual-Language Retrieval (VLR) module using the visual features as the queries. To generate accurate reports, MedWriter also predicts disease labels based on the visual features and extracts medical keywords from the retrieved reports. We propose a multiquery attention mechanism to learn the reportlevel template representations. Second, to make the generated reports more coherent and fluent, we propose a Language-Language Retrieval (LLR) module, which aims to learn sentence-level templates for the next sentence generation by analyzing between-sentence correlation in the retrieved reports. Finally, a hierarchical language decoder is adopted to generate the full report using visual features, report-level and sentence-level template representations. The designed two-level retrieval mechanism for memorization is helpful in generating accurate and diverse medical reports. To sum up, our contributions are:\n• To the best of our knowledge, we are the first to model the memory retrieval mechanism in both report and sentence levels. By imitating the standardized medical report generation in real life, our memory retrieval mechanism effectively utilizes existing templates in the twolayer hierarchy in medical texts. This design allows MedWriter to generate more clinically accurate and standardized reports. • On top of the retrieval modules, we design\na new multi-query attention mechanism to fuse the retrieved information for medical report generation. The fused information can be well incorporated with the existing image and report-level information, which can improve the quality of generated report . • Experiments conducted on two large-scale\nmedical report generation datasets, i.e., Openi and MIMIC-CXR show that MedWriter achieves better performance compared with state-of-the-art baselines measured by CIDEr, ROUGE-L, and BLEUs. Besides, case studies show that MedWriter provides more accurate and natural descriptions for medical images through domain expert evaluation."
    }, {
      "heading" : "2 Related work",
      "text" : "Generation-based report generation Visual captioning is the process of generating a textual de-\nscription given an image or a video. The dominant neural network architecture of the captioning task is based on the encoder-decoder framework (Bahdanau et al., 2014; Vinyals et al., 2015; Mao et al., 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019). As a sub-task in the medical domain, early studies directly apply state-of-theart encoder-decoder models as CNN-RNN (Vinyals et al., 2015), LRCN (Donahue et al., 2015) and AdaAtt (Lu et al., 2017) to medical report generation task. To further improve long text generation with domain-specific knowledge, later generationbased methods introduce hierarchical LSTM with co-attention (Jing et al., 2018) or use the medical concept features (Yuan et al., 2019) to attentively guide the report generation. On the other hand, the concept of reinforcement learning (Liu et al., 2019) is utilized to ensure the generated radiology reports correctly describe the clinical findings.\nTo avoid generating clinically non-informative reports, external domain knowledge like knowledge graphs (Zhang et al., 2020; Li et al., 2019) and anchor words (Biswal et al., 2020) are utilized to promote the medical values of diagnostic reports. CLARA (Biswal et al., 2020) also provides an interactive solution that integrates the doctors’ judgment into the generation process.\nRetrieval-based report generation Retrievalbased approaches are usually hybridized with generation-based ones to improve the readability of generated medical reports. For example, KERP (Li et al., 2019) uses abnormality graphs to retrieve most related sentence templates during the generation. HRGR-Agent(Li et al., 2018) incorporates retrieved sentences in a reinforcement learning framework for medical report generation. However, they all require a template database as the model input. Different from these models, MedWriter is able to automatically learn both report-level and sentence-level templates from the data, which significantly enhances the model applicability."
    }, {
      "heading" : "3 Method",
      "text" : "As shown in Figure 2, we propose a new framework called MedWriter, which consists of three modules. The Visual-Language Retrieval (VLR) module works on the report level and uses visual features to find the most relevant template reports based on a multi-view image query. The Language-Language Retrieval (LLR) module\nworks on the sentence level and retrieves a series of candidates that are most likely to be the next sentence from the retrieval pool given the generated language context. Finally, MedWriter generates accurate, diverse, and disease-specified medical reports by a hierarchical language decoder that fuses the visual, linguistics and pathological information obtained by VLR and LLR modules. To improve the effectiveness and efficiency of retrieval, we first pretrain VLR and LLR modules to build up a retrieval pool for medical report generation as follows."
    }, {
      "heading" : "3.1 VLR module pretraining",
      "text" : "The VLR module aims to retrieve the most relevant medical reports from the training report corpus for the given medical images. The retrieved reports are further used to learn an abstract template for generating new high-quality reports. Towards this goal, we introduce a self-supervised pretraining task by judging whether an image-report pair come from the same subject, i.e., image-report matching. It is based on an intuitive assumption that an imagereport pair from the same subject shares certain common semantics. More importantly, the disease types associated with images and the report should be similar. Thus, in the pretraining task, we also take disease categories into consideration."
    }, {
      "heading" : "3.1.1 Disease classification",
      "text" : "The input of the VLR module is a series of multi-modal images and the corresponding report ({Ii}bi=1, r) where the set {Ii}bi=1 consists of b images, and r denotes the report. We employ a Convolutional Neural Network (CNN) fv(·) as the image encoder to obtain the feature of a given im-\nage Ii, i.e., vi = fv(Ii), where vi ∈ Rk×k×d is the visual feature for the i-th image Ii.\nWith all the extracted features {vi}bi=1, we add them together as the inputs of the disease classification task, which is further used to learn the disease type representation as follows,\ncpred = Wcls( b∑\ni=1\nAvgPool(vi)) + bcls, (1)\nwhere Wcls ∈ Rc×d and bcls ∈ Rc are the weight and bias terms of a linear model, AvgPool is the operation of average pooling, c is the number of disease classes, and cpred ∈ Rc can be used to compute disease probabilities as a multi-label classification task with a sigmoid function, i.e., pdc = sigmoid(cpred)."
    }, {
      "heading" : "3.1.2 Image-report matching",
      "text" : "The next training task for VLR is to predict whether an image-report pair belongs to the same subject. In this subtask, after obtaining the image features {vi}bi=1 and the disease type representation cpred, we extract a context visual vector v by the pathological attention.\nFirst, for each image feature vi, we use the disease type representation cpred to learn the spatial attention score through a linear transformation,\nav = Watanh(Wvvi +Wccpred) (2)\nwhere av ∈ Rk×k, Wa, Wv and Wc are the linear transformation matrices. After that, we use the normalized spatial attention score αv = softmax(av) to add visual features over all locations (x, y) across the feature map,\nv′i = ∑ ∀x,y αv(x, y)vi(x, y). (3)\nThen, we compute the context vector v of the input image set {Ii}bi=1 using a linear layer on the concatenation of all the representation v′i, v = concat(v′1, · · · ,v′b)Wf , where Wf ∈ Rbd×d is the learnable parameter.\nFor the image-report matching task, we also need a language representation, which is extracted by a BERT (Devlin et al., 2018) model fl(·) as the language encoder. fl(·) converts the medical report r into a semantic vector r = fl(r) ∈ Rd. Finally, the probability of the input pair ({Ii}bi=1, r) coming from the same subject can be computed as\npvl = sigmoid(rTv). (4)\nGiven these two sub-tasks, we simultaneously optimize the cross-entropy losses for both disease classification and image-report matching to train the VLR module."
    }, {
      "heading" : "3.2 LLR module pretraining",
      "text" : "A medical report usually has some logical characteristics such as describing the patient’s medical images in a from-top-to-bottom order. Besides, the preceding and following sentences in a medical report may provide explanations for the same object or concept, or they may have certain juxtaposition, transition and progressive relations. Automatically learning such characteristics should be helpful for MedWriter to generate high-quality medical reports. Towards this end, we propose to pretrain a language-language retrieval (LLR) module to search for the most relevant sentences for the next sentence generation. In particular, we introduce a self-supervised pretraining task for LLR to determine if two sentences {si, sj} come from the same report, i.e., sentence-sentence matching.\nSimilar to the VLR module, we use a BERT model fs(·) as the sentence encoder to embed the sentence inputs {si, sj} into feature vectors si = fs(si), sj = fs(sj). Then the probability that two sentences {si, sj} come from the same medical report is measure by\npll = sigmoid(sTi sj). (5)\nAgain, the cross-entropy loss is used to optimize the learning objective given probability pll and the ground-truth label of whether s1 and s2 belong to the same medical report or not."
    }, {
      "heading" : "3.3 Retrieval-based report generation",
      "text" : "Using the pretrained VLR and LLR modules, MedWriter generates a medical report given a\nsequence of input images {Ii}bi=1 using a novel hierarchical retrieval mechanism with a hierarchical language decoder."
    }, {
      "heading" : "3.3.1 VLR module for report-level retrieval",
      "text" : "Report retrieval Let D(tr)r = {rj}Ntrj=1 denote the set of all the training reports, where Ntr is the number of reports in the training dataset. For each report rj , MedWriter first obtain its vector representation using fr(·) in the VLR module, which is denoted as rj = fr(rj). Let Pr = {rj}Ntrj=1 denote the set of training report representations. Given the multi-modal medical images {Ii}bi=1 of a subject, the VLR module aims to return the top kr medical reports {r′j} kr j=1 as well as medical keywords within in the retrieved reports. Specifically, MedWriter extracts the image feature v for {Ii}bi=1 using the pathological attention mechanism as described in Section 3.1. According to Eq. (4), MedWriter then computes a image-report matching sore pvl between v and each r ∈ Pr. The top kr reports {r′j} kr j=1 with the largest scores pvl are considered as the most relevant medical reports corresponding to the images, and they are selected as the template descriptions. From these templates, we identify n medical keywords {wi}ni=1 using a dictionary as a summarization of the template information. The medical keyword dictionary includes disease phenotype, human organ, and tissue, which consists of 36 medical keywords extracted from the training data with the highest frequency.\nReport template representation learning The retrieved reports are highly related to the given images, which should be helpful for the report generation. To make full use of them, we need to learn a report template representation using the image feature v, the features of retrieved reports {r′j} kr j=1, medical keywords embeddings {wi}ni=1 for {wi}ni=1 learned from the pretrained word embeddings, and the disease embeddings {ck}mk=1 from predicted disease labels {ck}mk=1 using Disease Classification in Section 3.1.1.\nWe propose a new multi-query attention mechanism to learn the report template representation. To specify, we use the image features v as the key vector K, the retrieved report features {r′j} kr j=1 as the value matrix V , and the embeddings of both medical keywords {wi}ni=1 and disease labels {ck}mk=1 as the query vectors Q. We modify the original self-attention (Vaswani et al., 2017) into\na multi-query attention. For each query vectorQi inQ, we first get a corresponding attended feature and then transform them into the report template vector rs after concatenation,\nrs = MultiQuery({Qi}ni=1,K,V ) = concat(attn1, · · · , attnn)WO,\n(6)\nwhere attni = Attention(Qi,KWK ,VW V ), and WK , W V and WO are the transformation matrices. Generally, the Attention function is calculated by\nAttention(Qg,Kg,Vg) = softmax( QgKg T√ dg )Vg,\nwhere Q,K,V are queries, keys and values in general case, and dg is the dimension of the query vector."
    }, {
      "heading" : "3.3.2 LLR module for sentence-level retrieval",
      "text" : "Since retrieved reports {rtj} kr j=1 are highly associated with the input images, the sentence within those reports must contain some instructive pathological information that is helpful for sentencelevel generation. Towards this end, we first select sentences from the retrieved reports and then learn sentence-level template representation.\nSentence retrieval We first divide the retrieved reports into L candidate sentences {sj}Lj=1 as the retrieval pool in the LLR module. Given the pretrained LLR language encoder fs(·), we can obtain the sentence-level feature pool, which is Ps = {fs(sj)}Lj=1 = {sj}Lj=1. Assume that the generated sentence at time t is denoted as ot, and its embedding is ot = fs(ot), which is used to find ks sentences {s′j} ks j=1 with the highest probabilities pll from the candidate sentence pool using Eq. (5) in Section 3.2.\nSentence template representation learning Similar to the report template representation, we still use the multi-query attention mechanism. From the retrieved ks sentences, we extract the medical keywords {w′i}ni=1. Besides, we have the predicted disease labels {ck}mk=1. Their embeddings are considered as the query vectors. The embeddings of the extracted sentence, i.e., {fs(s′j)} ks j=1 = {s′j} ks j=1, are treated as the value vectors. The key vector is the current sentence (word) hidden state hst (h w i ), which will be introduced in Section 3.3.3. According to Eq. (6), we can obtain the sentence template representation at time t, which is denoted as ut (uwi used for word-level generation)."
    }, {
      "heading" : "3.3.3 Hierarchical language decoder",
      "text" : "With the extracted features by the retrieval mechanism described above, we apply a hierarchical decoder to generate radiology reports according to the hierarchical linguistics structure of the medical reports. The decoder contains two layers, i.e., a sentence LSTM decoder that outputs sentence hidden states, and a word LSTM decoder which decodes the sentence hidden states into natural languages. In this way, reports are generated sentence by sentence.\nSentence-level LSTM For generating the t-th sentence, MedWriter first uses the previous t− 1 sentences to learn the sentence-level hidden state hst . Specifically, MedWriter learns the image feature vs based on Eq. (3). When calculating the attention score with Eq. (2), we consider both the information obtained from the previous t− 1 sentences (the hidden state hst−1) and the predicted disease representation from Eq. (1), i.e., replacing cpred with concat(ht−1, cpred). Then the concatenation of the image feature vs, the report template representation rs from Eq. (6), and the sentence template representation ust−1 is used as the input of the sentence LSTM to learn the hidden state hst\nhst = LSTMs(concat(v s,ust−1, rs),h s t−1), (7)\nwhere ust−1 is obtained using the multi-query attention, the key vector is the hidden state hst−1, the value vectors are the representations of the retrieved sentences according to the (t− 1)-th sentence, and the query vectors are the embeddings of both medical keywords extracted from the retrieved sentences and the predicted disease labels.\nWord-level LSTM Based on the learned hst , MedWriter conducts the word-by-word generation using a word-level LSTM. For generating the (i+1)-th word, MedWriter first learns the image feature vw using Eq. (2) by replacing cpred with hwi in Eq. (2), where h w i is the hidden state of the i-th word. MedWriter then learns the sentence template representation uwi using the multi-query attention, where the key vector is the hidden state hwi , value and query vectors are the same as those used for calculating ust−1. Finally, the concatenation of hst , u w i , v\nw, and rs is taken as the input of the word-level LSTM to generate the (i+ 1)-th word as follows:\nhwi = LSTMw(concat(h s t ,u w i ,v w, rs),h w i−1), wi+1 = argmax(softmax(FFN(hwi ))), (8)\nwhere FFN(·) is the feed-forward network. Note that for the first sentence generation, we set u0 as 0, and h0 is the randomly initialized vector, to learn the sentence-level hidden state hs1. When generating the words of the first sentence, we set uwi as the 0 vector."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and baselines",
      "text" : "Datasets Open-i1 (Demner-Fushman et al., 2016) (a.k.a IU X-Ray) provides 7,470 chest Xrays with 3,955 radiology reports. In our experiments, we only utilize samples with both frontal and lateral views, and with complete findings and impression sections in the reports. This results in totally 2,902 cases and 5,804 images. MIMICCXR2 (Johnson et al., 2019) contains 377,110 chest X-rays associated with 227,827 radiology reports, divided into subsets. We use the same criterion to select samples, which results in 71,386 reports and 142,772 images.\nFor both datasets, we tokenize all words with more than 3 occurrences and obtain 1,252 tokens on the Open-i dataset and 4,073 tokens on the MIMIC-CXR dataset, including four special tokens 〈PAD〉, 〈START〉, 〈END〉, and 〈UNK〉. The findings and impression sections are concatenated as the ground-truth reports. We randomly divide the whole datasets into train/validation/test sets with a ratio of 0.7/0.1/0.2. To conduct the disease classification task, we include 20 most frequent finding keywords extracted from MeSH tags as disease categories on the Open-i dataset and 14 CheXpert categories on the MIMIC-CXR dataset.\nBaselines On both datasets, we compare with four state-of-the-art image captioning models: CNN-RNN (Vinyals et al., 2015), CoAttn (Jing et al., 2018), MvH+AttL (Yuan et al., 2019), and V-L Retrieval. V-L Retrieval only uses the retrieved report templates with the highest probability as prediction without the generation part based on our pretrained VLR module. Due to the lack of the opensource code for (Wang et al., 2018; Li et al., 2019, 2018; Donahue et al., 2015) and the template databases for (Li et al., 2019, 2018), we only include the reported results on the Open-i dataset in our experiments.\n1https://openi.nlm.nih.gov/faq# collection\n2https://physionet.org/content/ mimic-cxr/2.0.0/"
    }, {
      "heading" : "4.2 Experimental setup",
      "text" : "All input images are resized to 512 × 512, and the feature map from DenseNet-121 (Huang et al., 2017) is 1024× 16× 16. During training, we use random cropping and color histogram equalization for data augmentation.\nTo pretrain the VLR module, the maximum length of the report is restricted to 128 words. We train VLR module for 100 epochs with an Adam (Kingma and Ba, 2014) optimizer with 1e-5 as the initial learning rate, 1e-5 for L2 regularization, and 16 as the mini-batch size. To pretrain the LLR module, the maximum length of each sentence is set to 32 words. We optimize the LLR module for 100 epochs with an Adam (Kingma and Ba, 2014) optimizer with the initial learning rate of 1e-5 and a mini-batch size of 64. The learning rate is multiplied by 0.2 every 20 epochs.\nTo train the full model for MedWriter, we set the retrieved reports number kr = 5 and sentences number ks = 5. Extracting n = 5 medical keywords and predicting m = 5 disease labels are used for report generation. Both sentence and word LSTM have 512 hidden units. We freeze the weights for the pretrained VLR and LLR modules and only optimize on the language decoder. We set the initial learning rate as 3e-4 and mini-batch size as 32. MedWriter takes 10 hours to train on the Open-i dataset and 3 days on the MIMIC-CXR dataset with four GeForce GTX 1080 Ti GPUs."
    }, {
      "heading" : "4.3 Quantitative and qualitative results",
      "text" : "Table 1 shows the CIDEr, ROUGE-L, BLUE, and AUC scores achieved by different methods on the test sets of Open-i and MIMIC-CXR.\nLanguage evaluation From Table 1, we make the following observations. First, compared with Generation-based model, Retrieval-based model that uses the template reports as results has set up a relatively strong baseline for medical report generation. Second, compared with V-L retrieval, other Retrieval-based approaches perform much better in terms of all the metrics. This again shows that that by integrating the information retrieval method into the deep sequence generation framework, we can not only use the retrieved language information as templates to help generate long sentences, but also overcome the monotony of only using the templates as the generations. Finally, we see that the proposed MedWriter achieves the highest language scores on 5/6 metrics on Open-i\ndatasets and all metrics on MIMIC-CXR among all methods. MedWriter not only improves current SOTA model CoAttn (Jing et al., 2018) by 5% and MvH+AttL (Yuan et al., 2019) by 4% on Open-i in average, but also goes beyond SOAT retrieval-based approaches like KERP (Li et al., 2019) and HRGR-Agent (Li et al., 2018) and significantly improves the performance, even without using manually curated template databases. This illustrates the effectiveness of automatically learning templates and adopting hierarchical retrieval in writing medical reports.\nClinical evaluation We train two report classification BERT models on both datasets and use it to judge whether the generated reports correctly reflect the ground-truth findings. We show the mean ROC-AUC scores achieved by generated reports from different baselines in the last column of Table 1. We can observe that MedWriter achieves the highest AUC scores compared with other baselines. In addition, our method achieves the AUC scores that are very close to those of professional doctors’ reports, with 0.814/0.915 and 0.833/0.923 on two datasets. This shows that the generation performance of MedWriter has approached the level of human domain experts, and it embraces great medical potentials in identifying disease-related medical findings.\nHuman evaluation We also qualitatively evaluate the quality of the generated reports via a user study. We randomly select 50 samples from the Open-i test set and collect ground-truth reports and the generated reports from both MvH+AttL (Yuan et al., 2019) and MedWriter to conduct the human evaluation. Two experienced radiologists were asked to give ratings for each selected report, in terms of whether the generated reports are realistic and relevant to the X-ray images. The ratings are\nintegers from one to five. The higher, the better. Table 2 shows average human evaluation results on MedWriter compared with Ground Truth reports and generations of MvH+AttL (Yuan et al., 2019) on Open-i, evaluated in terms of realistic scores and relevant scores. MedWriter achieves much higher human preference than the baseline model, even approaching the performance of Ground Truth reports that wrote by experienced radiologists. It shows that MedWriter is able to generate accurate clinical reports that are comparable to domain experts.\nQualitative analysis Figure 3 shows qualitative results of MedWriter and baseline models on the Open-i dataset. MedWriter not only produces longer reports compared with MvH+AttL but also accurately detects the medical findings in the images (marked in red and bold). On the other hand, we find that MedWriter is able to put forward some supplementary suggestions (marked in blue) and descriptions, which are not in the original report but have diagnostic value. The underlying reason for this merit comes from the memory retrieval mechanism that introduces prior medical knowledge to facilitate the generation process."
    }, {
      "heading" : "4.4 Ablation study",
      "text" : "We perform ablation studies on the Open-i and MIMIC-CXR datasets to investigate the effectiveness of each module in MedWriter. In each of the following studies, we change one module with other modules intact.\nRemoving the VLR module In this experiment, global report feature rs is neglected in Eqs. (7)\nand (8), and the first sentence is generated only based on image features. The LLR module keeps its functionality. However, instead of looking for sentence-level templates from the retrieved reports, it searches for most relevant sentences from all the reports. As can be seen from Table 3, removing VLR module (“w/o VLRM”) leads to performance reduction by 2% on average. This demonstrates that visual-language retrieval is capable in sketching out the linguistic structure of the whole report. The rest of the language generation is largely influenced by report-level context information.\nRemoving the LLR module The generation of (t+1)-th sentence is based on the global report feature rs and the image feature v, without using the retrieved sentences information in Eq. (8). Table 3 shows that removing LLR module (“w/o LLRM”) results in the decease of average evaluation scores by 4% compared with the full model. This verifies that the LLR module plays an essential role in generating long and coherent clinical reports.\nReplacing hierarchical language decoder We use a single layer LSTM that treats the whole report\nas a long sentence and conduct the generation wordby-word. Table 3 shows that replacing hierarchical language decoder with a single-layer LSTM (“w/o HLD”) introduces dramatic performance reduction. This phenomenon shows that the hierarchical generative model can effectively and greatly improve the performance of long text generation tasks."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Automatically generating accurate reports from medical images is a key challenge in medical image analysis. In this paper, we propose a novel model named MedWriter to solve this problem based on hierarchical retrieval techniques. In particular, MedWriter consists of three main modules, which are the visual-language retrieval (VLR) module, the language-language retrieval (LLR) module, and the hierarchical language decoder. These three modules tightly work with each other to automatically generate medical reports. Experimental results on two datasets demonstrate the effectiveness of the proposed MedWriter. Besides, qualitative studies show that MedWriter is able to generate meaningful and realistic medical reports."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Clara: Clinical report auto-completion",
      "author" : [ "Siddharth Biswal", "Cao Xiao", "Lucas Glass", "Brandon Westover", "Jimeng Sun." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 541–550.",
      "citeRegEx" : "Biswal et al\\.,? 2020",
      "shortCiteRegEx" : "Biswal et al\\.",
      "year" : 2020
    }, {
      "title" : "Preparing a collection of radiology examinations for distribution and retrieval",
      "author" : [ "Dina Demner-Fushman", "Marc D Kohli", "Marc B Rosenman", "Sonya E Shooshan", "Laritza Rodriguez", "Sameer Antani", "George R Thoma", "Clement J McDonald." ],
      "venue" : "Journal",
      "citeRegEx" : "Demner.Fushman et al\\.,? 2016",
      "shortCiteRegEx" : "Demner.Fushman et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Laurens Van Der Maaten", "Kilian Q Weinberger." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "On the automatic generation of medical imaging reports",
      "author" : [ "Baoyu Jing", "Pengtao Xie", "Eric Xing." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2577–2586.",
      "citeRegEx" : "Jing et al\\.,? 2018",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2018
    }, {
      "title" : "Mimic-cxr-jpg, a large publicly available database of labeled chest",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Nathaniel R Greenbaum", "Matthew P Lungren", "Chih-ying Deng", "Yifan Peng", "Zhiyong Lu", "Roger G Mark", "Seth J Berkowitz", "Steven Horng" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Knowledge-driven encode, retrieve, paraphrase for medical image report generation",
      "author" : [ "Christy Y Li", "Xiaodan Liang", "Zhiting Hu", "Eric P Xing." ],
      "venue" : "In",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Hybrid retrieval-generation reinforced agent for medical image report generation",
      "author" : [ "Yuan Li", "Xiaodan Liang", "Zhiting Hu", "Eric P Xing." ],
      "venue" : "Advances in neural information processing systems, pages 1530– 1540.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Clinically accurate chest x-ray report generation",
      "author" : [ "Guanxiong Liu", "Tzu-Ming Harry Hsu", "Matthew McDermott", "Willie Boag", "Wei-Hung Weng", "Peter Szolovits", "Marzyeh Ghassemi." ],
      "venue" : "arXiv preprint arXiv:1904.02633.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "author" : [ "Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375–383.",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-rnn)",
      "author" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille." ],
      "venue" : "arXiv preprint arXiv:1412.6632.",
      "citeRegEx" : "Mao et al\\.,? 2014",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical attention network for image captioning",
      "author" : [ "Weixuan Wang", "Zhihong Chen", "Haifeng Hu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8957–8964.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays",
      "author" : [ "Xiaosong Wang", "Yifan Peng", "Le Lu", "Zhiyong Lu", "Ronald M Summers." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pat-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "International conference on machine learn-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Image captioning with semantic attention",
      "author" : [ "Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651–4659.",
      "citeRegEx" : "You et al\\.,? 2016",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic radiology report generation",
      "author" : [ "Jianbo Yuan", "Haofu Liao", "Rui Luo", "Jiebo Luo" ],
      "venue" : null,
      "citeRegEx" : "Yuan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2019
    }, {
      "title" : "When radiology report generation meets knowledge graph",
      "author" : [ "Yixiao Zhang", "Xiaosong Wang", "Ziyue Xu", "Qihang Yu", "Alan L. Yuille", "Daguang Xu." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 12910–12917.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Generation-based methods, including LRCN (Donahue et al., 2015), CoAtt (Jing",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : ", 2018), and MvH+AttL (Yuan et al., 2019), focus on generating image captions with a encoder-",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "2018) and KEPP (Li et al., 2019), pay attention to memorizing templates to generate standardized reports from a predefined retrieval database.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "The dominant neural network architecture of the captioning task is based on the encoder-decoder framework (Bahdanau et al., 2014; Vinyals et al., 2015; Mao et al., 2014), with attention mechanism (Xu et al.",
      "startOffset" : 106,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "The dominant neural network architecture of the captioning task is based on the encoder-decoder framework (Bahdanau et al., 2014; Vinyals et al., 2015; Mao et al., 2014), with attention mechanism (Xu et al.",
      "startOffset" : 106,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "The dominant neural network architecture of the captioning task is based on the encoder-decoder framework (Bahdanau et al., 2014; Vinyals et al., 2015; Mao et al., 2014), with attention mechanism (Xu et al.",
      "startOffset" : 106,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : ", 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : ", 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : ", 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : ", 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : ", 2014), with attention mechanism (Xu et al., 2015; You et al., 2016; Lu et al., 2017; Anderson et al., 2018; Wang et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "As a sub-task in the medical domain, early studies directly apply state-of-theart encoder-decoder models as CNN-RNN (Vinyals et al., 2015), LRCN (Donahue et al.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : ", 2015), LRCN (Donahue et al., 2015) and AdaAtt (Lu et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : ", 2015) and AdaAtt (Lu et al., 2017) to medical report generation task.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "To further improve long text generation with domain-specific knowledge, later generationbased methods introduce hierarchical LSTM with co-attention (Jing et al., 2018) or use the medical concept features (Yuan et al.",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : ", 2018) or use the medical concept features (Yuan et al., 2019) to attentively guide the report generation.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "concept of reinforcement learning (Liu et al., 2019) is utilized to ensure the generated radiology reports correctly describe the clinical findings.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "reports, external domain knowledge like knowledge graphs (Zhang et al., 2020; Li et al., 2019) and anchor words (Biswal et al.",
      "startOffset" : 57,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "reports, external domain knowledge like knowledge graphs (Zhang et al., 2020; Li et al., 2019) and anchor words (Biswal et al.",
      "startOffset" : 57,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : ", 2019) and anchor words (Biswal et al., 2020) are utilized to promote the medical values of diagnostic reports.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "CLARA (Biswal et al., 2020) also provides an interactive solution that integrates the doctors’",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "For example, KERP (Li et al., 2019) uses abnormality graphs to retrieve most related sentence templates during the generation.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "HRGR-Agent(Li et al., 2018) incorporates retrieved sentences in a reinforcement learning framework for medical report generation.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "For the image-report matching task, we also need a language representation, which is extracted by a BERT (Devlin et al., 2018) model fl(·) as the language encoder.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "We modify the original self-attention (Vaswani et al., 2017) into",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "MIMICCXR2 (Johnson et al., 2019) contains 377,110 chest X-rays associated with 227,827 radiology reports, divided into subsets.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "Baselines On both datasets, we compare with four state-of-the-art image captioning models: CNN-RNN (Vinyals et al., 2015), CoAttn (Jing et al.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : ", 2015), CoAttn (Jing et al., 2018), MvH+AttL (Yuan et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : ", 2018), MvH+AttL (Yuan et al., 2019), and V-L Retrieval.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : "Due to the lack of the opensource code for (Wang et al., 2018; Li et al., 2019, 2018; Donahue et al., 2015) and the template databases for (Li et al.",
      "startOffset" : 43,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Due to the lack of the opensource code for (Wang et al., 2018; Li et al., 2019, 2018; Donahue et al., 2015) and the template databases for (Li et al.",
      "startOffset" : 43,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "All input images are resized to 512 × 512, and the feature map from DenseNet-121 (Huang et al., 2017) is 1024× 16× 16.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "We train VLR module for 100 epochs with an Adam (Kingma and Ba, 2014) optimizer with 1e-5 as the initial learning rate, 1e-5 for L2 regularization, and 16 as the mini-batch size.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "We optimize the LLR module for 100 epochs with an Adam (Kingma and Ba, 2014) optimizer with the initial learning rate of 1e-5 and a mini-batch size of 64.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "* indicates the results reported in (Li et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "MedWriter not only improves current SOTA model CoAttn (Jing et al., 2018) by 5% and MvH+AttL (Yuan et al.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : ", 2018) by 5% and MvH+AttL (Yuan et al., 2019) by 4% on Open-i in average, but also goes beyond SOAT retrieval-based approaches like KERP (Li et al.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "2019) and HRGR-Agent (Li et al., 2018) and significantly improves the performance, even without using manually curated template databases.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "We randomly select 50 samples from the Open-i test set and collect ground-truth reports and the generated reports from both MvH+AttL (Yuan et al., 2019) and MedWriter to conduct the human evaluation.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "Table 2 shows average human evaluation results on MedWriter compared with Ground Truth reports and generations of MvH+AttL (Yuan et al., 2019) on Open-i, evaluated in terms of realistic scores and relevant scores.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "Figure 3: Examples of ground-truth and generated reports by MvH+AttL (Yuan et al., 2019) and MedWriter.",
      "startOffset" : 69,
      "endOffset" : 88
    } ],
    "year" : 2021,
    "abstractText" : "Medical report generation is one of the most challenging tasks in medical image analysis. Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentencelevel templates for clinically accurate report generation. MedWriter first employs the Visual-Language Retrieval (VLR) module to retrieve the most relevant reports for the given images. To guarantee the logical coherence between sentences, the Language-Language Retrieval (LLR) module is introduced to retrieve relevant sentences based on the previous generated description. At last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports. We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., Open-I and MIMIC-CXR.",
    "creator" : "LaTeX with hyperref"
  }
}