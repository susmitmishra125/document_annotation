{
  "name" : "2021.acl-long.225.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning",
    "authors" : [ "Cheonbok Park", "Yunwon Tae", "Taehee", "Kim", "Soyoung Yang", "Mohammad Azam Khan", "Lucy Park", "Jaegul Choo" ],
    "emails" : [ "Corp@navercorp.com,", "cbok.park@navercorp.com,", "tyj204@korea.ac.kr", "jchoo}@kaist.com", "azam@dpdc.org.bd,", "lucy@upstage.ai" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2888–2901\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2888"
    }, {
      "heading" : "1 Introduction",
      "text" : "Unsupervised neural machine translation (UNMT) leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus. Recently, the state of the art in UNMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019) has achieved comparable performances against supervised neural machine translation (NMT) approaches. In contrast to supervised NMT, which uses a parallel corpus, training the UNMT model requires a significant number of monolingual sentences (e.g., 1M-3M sentences). However, the prerequisite limits UNMT’s applicability to low-resource domains, especially for\n∗ equal contributions † This work done in NAVER Corp. Our code is avilable at https://github.com/\npapago-lab/MetaGUMT\ndomain-specific document translation tasks. Since gathering or creating those documents requires domain specific knowledge, the monolingual data themselves are scarce and expensive. In addition, the minority languages (e.g., Uzbek and Nepali) make the problem of data scarcity even worse.\nYet, UNMT for low-resource domains is not an actively explored field. One naive approach is to train a model on high-resource domains (e.g., economy and sports) while hoping the model will generalize on an unseen low-resource domain (e.g., medicine). However, recent studies have shown that non-trivial domain mismatch can significantly cause low translation accuracy on supervised NMT tasks (Koehn and Knowles, 2017).\nAnother reasonable approach is transfer learning—particularly, domain adaptation—which has shown performance improvements in the supervised NMT literature (Freitag and Al-Onaizan, 2016; Zeng et al., 2019). In this approach, the model is first pretrained using data from existing domains and then finetuned on a new domain. However, this approach can suffer from overfitting and catastrophic forgetting due to a small amount of training data and a large domain gap.\nAs an effective method for handling a small amount of training data, meta-learning has shown its superiority in various NLP studies such as dialog generation, machine translation, and natural language understanding (Qian and Yu, 2019; Gu et al., 2018; Dou et al., 2019). In general, the metalearning approach is strongly affected by the number of different tasks where tasks are defined as languages or domains from the aforementioned studies. However, in practice, the previous studies may struggle to gather data to define tasks because they rely on a supervised model that requires labeled corpora. In this respect, we argue that applying a meta-learning approach to the unsupervised model is more feasible and achievable than the supervised\nmodel because it can define multiple different tasks with unlabeled corpora. Therefore, we introduce a new meta-learning approach for UNMT, called MetaUMT, for low-resource domains by defining each task as a domain.\nThe objective of MetaUMT is to find the optimal initialization for the model parameters that can quickly adapt to a new domain even with only a small amount of monolingual data. As shown in Fig. 1 (a), we define two different training phases, a meta-train and a meta-test phase, and simulate the domain adaption process to obtain optimally initialized parameters. Specifically, the meta-train phase adapts model parameters to a domain while the meta-test phase optimizes the parameters obtained from the meta-train phase. After obtaining optimally initialized parameters through these two phases, we fine-tune the model using a target domain (i.e., a low-resource domain).\nAlthough the initial parameters optimized through MetaUMT are suitable for adapting to a low-resource domain, these parameters may not fully maintain the knowledge of high-resource domains. Concretely, in the meta-test phase, MetaUMT optimizes initial parameters using the adapted parameters; however, it discards meta-train knowledge used to update adapted parameters in the meta-train phase. Therefore, instead of validating the same domain used in the meta-train phase, we intend to inject generalizable knowledge into the initial parameters by utilizing another domain in the meta-test phase. This prevents overfitting from the data scarcity issue.\nAs shown in Fig. 1 (b), we propose an improved meta-learning approach called MetaGUMT for lowresource UNMT by explicitly infusing common knowledge across multiple source domains as well as generalizable knowledge from one particular domain to another. In other words, we do not only encourage the model to find the optimally initialized parameters that can quickly adapt to a target domain with low-resource data, but also encourage the model to maintain common knowledge (e.g., general words such as determiners, conjunctions, and pronouns) which is obtainable from multiple source domains. Furthermore, due to a small amount of training data in a low-resource domain, the model can suffer from overfitting; however, we attempt to handle overfitting by leveraging generalizable knowledge that is available from one domain to another. Our proposed meta-learning approach\ndemonstrates consistent improvements over other baseline models.\nOverall, our contributions can be summarized as follows:\n• We apply a meta-learning approach for UNMT. To the best of our knowledge, this is the first study to use a meta-learning approach for UNMT, where this approach is more suitable to a UNMT task than a supervised one.\n• We empirically demonstrate that our enhanced method, MetaGUMT, shows fast convergence on both pre-training (i.e., meta-learning with source domains) and finetuning (i.e., adapting to a target domain).\n• The model trained with MetaGUMT consistently outperforms all baseline models including MetaUMT. This demonstrates that finding optimally initialized parameters that incorporate high-resource domain knowledge and generalizable knowledge is significant in handling a low-resource domain."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our study leverages two components from the natural language processing (NLP) domain: lowresource NMT and meta-learning. In this section, we discuss previous studies by concentrating on these two main components."
    }, {
      "heading" : "2.1 Low-Resource Neural Machine Translation",
      "text" : "Based on the success of attention-based models (Luong et al., 2015; Vaswani et al., 2017), NMT obtains significant improvement in numerous language datasets, even showing promising results (Wu et al.) in different datasets. However, the performance of NMT models depends on the size of the parallel dataset (Koehn and Knowles, 2017). To address this problem, one conventional approach is utilizing monolingual datasets.\nRecent studies point out the difficulty of gathering parallel data, whereas the monolingual datasets are relatively easy to collect. To facilitate monolingual corpora, several studies apply dual learning (He et al., 2016), back-translation (Sennrich et al., 2016b), and pretraining the model with bilingual corpora (Hu et al., 2019; Wei et al., 2020). Furthermore, as a challenging scenario, recent studies propose the UNMT methods without using any\nparallel corpora (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018). The UNMT models show comparable performances by extending the back-translation method (Conneau et al., 2018) and incorporating methods such as shared Byte Pair Encoding (BPE) (Lample et al., 2018b) and cross-lingual representations (Conneau and Lample, 2019), following those of the supervised NMT. However, since these approaches require plenty of monolingual datasets, they suffer in a low-resource domain.\nTransferring the knowledge from high-resource domains to a low-resource domain is one alternative way to address this challenge. A few studies concentrate on transferring the knowledge from the rich-resource corpora into the low-resource one. Several models (Chu and Wang, 2018; Hu et al., 2019) show better performances than when trained with the low-resource corpora only. However, these approaches are applicable in specific scenarios where one or both of the source and target domains consist of a parallel corpus.\nTo address the issues, we define a new task as the unsupervised domain adaptation on the lowresource dataset. Our work is more challenging than any other previous studies, since we assume that both the low-resource target domain and the source domain corpora are monolingual."
    }, {
      "heading" : "2.2 Meta Learning",
      "text" : "Given a small amount of training data, most of the machine learning models are prone to overfitting, thus failing to find a generalizable solution. To handle this issue, meta-learning approaches seek for how to adapt quickly and accurately to a lowresource task, and show impressive results in various domains (Finn et al., 2017; Javed and White,\n2019). The meta-learning approaches aim to find the optimal initialization of the model parameters that adapts the model to a low-resource dataset in a few iterations of training (Finn et al., 2017; Ravi and Larochelle, 2016). Owing to the success of the meta learning, recent studies apply the meta learning to low-resource NMT tasks, including multilingual NMT (Gu et al., 2018) and the domain adaptation (Li et al., 2020). These studies assume that all the training corpora consist of the parallel sentences. However, a recent work (Li et al., 2018) utilizes the meta learning approach to find a generalized model for multiple target tasks. However, it is not focused on adapting a specific target task since its main goal is to handle the target task without using any low-resource data.\nOur study attempts to address the low-resource UNMT by exploiting meta-learning approaches. Moreover, we present two novel losses that encourage incorporating high-resource knowledge and generalizable knowledge into the model parameters. Our proposed approaches show significant performance improvements in adapting to a lowresource target domain."
    }, {
      "heading" : "3 Unsupervised Neural Machine Translation",
      "text" : "In this section, we first introduce the notation of the general UNMT models. We then describe the three steps for the UNMT task: initialization, language modeling, and back-translation. On these three steps, we illustrate how each step contributes to improving the performance of UNMT.\nNotations. We denote S and T as a source and a target monolingual language dataset. x and y represent the source and the target sentences from S and T . We assume the NMT model is parame-\nterized by θ. We also denote Ms→s and Mt→t as language models in a source and a target language, respectively, while denotingMs→t andMt→s as the machine translation models from the source to the target language and vice versa.\nInitialization. A recent UNMT model (Lample et al., 2018b) is based on a shared encoder and decoder architecture for the source and the target language. Due to the shared encoder and decoder for each language, initializing the model parameters of the shared encoder and decoder is an important step for competitive performances (Conneau et al., 2018; Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018). Conneau and Lample (2019) propose the XLM (cross-lingual language model) to initialize parameters, showing significantly improved performances for UNMT. Among various initialization methods, we leverage the XLM as our initialization method.\nLanguage modeling. We use a denoising autoencoder (Vincent et al., 2008) to train the UNMT model, reconstructing an original sentence from a noisy one in a given language. The objective function is defined as follows:\nLlm =Ex∼S [− logMs→s(x|C(x))]+ Ey∼T [− logMt→t(y|C(y))],\n(1)\nwhere C is a noise function described in (Lample et al., 2018b), which randomly drops or swaps words in a given sentence. By reconstructing the sentence from the noisy sentence, the model learns the language modeling in each language.\nBack-translation. Back-translation helps the model learn the mapping functions between the source and the target language by using only the monolingual sentences. For example, we sample a sentence x and y from source language S and target language T . To make pseudo-pair sentences from the sampled source sentence, we deduce the target sentence from the source sentence, such that y′ = Ms→t (x), resulting in the pseudo parallel sentence, i.e., (x, y′). Similarly, we obtain (x′, y), where x′ is the translation of a target sentence, i.e., Mt→s (y). We do not back-propagate when we generate the pseudo-parallel sentence pairs. In short, the back-translation objective function is\nLbt =Ey∼T [−logMs→t ( y | x′ ) ]+\nEx∼S [−logMt→s ( x | y′ ) ].\n(2)"
    }, {
      "heading" : "4 Proposed Approach",
      "text" : "This section first explains our formulation of a lowresource unsupervised machine translation task where we can apply a meta-learning approach. Afterwards, we elaborate our proposed methods, MetaUMT and MetaGUMT. We utilize the metalearning approach to address a low-resource challenge for unsupervised machine translation. Moreover, we extend MetaUMT into MetaGUMT to explicitly incorporate learned knowledge from multiple domains."
    }, {
      "heading" : "4.1 Problem Setup",
      "text" : "Finn et al. (2017) assume multiple different tasks to find the proper initial parameters that can quickly adapt to a new task using only a few training\nexamples. In this paper, we consider tasks in the meta-learning as domains, where Dout = {D0out, ...,Dnout} represents n out-domain datasets (i.e., source domain datasets), and Din indicates an in-domain dataset (i.e., a target domain dataset), which can be the dataset in an arbitrary domain not included in Dout. Each domain in both Dout and Din is assumed to be composed of unpaired language corpora, and we create Din as a lowresource monolingual dataset 1. To adapt our model to the low-resource in-domain data, we finetune the UNMT model by minimizing both the losses described in Eqs. (1) and (2) with Din."
    }, {
      "heading" : "4.2 MetaUMT",
      "text" : "In order to obtain an optimal initialization of the model parameters, allowing the model to quickly adapt to a new domain with only a small number of monolingual training data, MetaUMT uses two training phases, the meta-train phase and the metatest phase. During the meta-train phase, the model first learns domain-specific knowledge by updating initial model parameters θ to temporary model parameters φi, i.e., adapted parameters. Then, in the meta-test phase, the model learns the adaptation by optimizing θ with respect to φi. From the domain adaption perspective, two phases simulate the domain adaption process. The model first adapts to a specific domain through the meta-train phase, and this adaption is evaluated in the meta-test phase.\nMeta-train phase. We obtain φi for each i-th out-domain dataset by using one-step gradient descent, i.e.,\nφi= θ − α∇θLsDiout(θ), (3)\nwhere LsDiout is represented as\nLsDiout= L lm Diout (θ) + LbtDiout(θ). (4)\nDiout is the i-th out-domain dataset, and α is the learning rate for the meta-train phase. As previously discussed in Section 3, the language modeling and back-translation losses are essential in facilitating the unsupervised machine translation. Hence, Ls consists of Llm and Lbt, where each loss function is computed with Diout.\nMeta-test phase. The objective of the meta-test phase is to update θ using each φi learned from the\n1We randomly sample the 5,000 tokens (∼ 300 sentences) from the in-domain training dataset.\nmeta-train phase by using each LsDi out′ 2. We call this update as a meta-update, defined as\nθ ← θ − β∇θ n∑ i=0 LsDi out′ (φi), (5)\nwhere β is another learning rate in the meta-test phase. Since Eq. (5) requires the second-order gradient, the equation is simplified with the first-order gradient by replacing the second-order term. Finn et al. (2017) showed that the first-order approximation of the meta-learning maintains the performance while minimizing the computational cost."
    }, {
      "heading" : "4.3 MetaGUMT",
      "text" : "To handle a data scarcity issue from a meta-learning perspective, it is critical to be able to make the initialized model to adapt to a data-scarce domain. However, since a small amount of training data in the new domain may cause the model to overfit and prevent utilizing high-resource domain knowledge, it is important to incorporate high-resource domain knowledge and generalizable knowledge into the model parameters. To address this issue, we extend the existing meta-learning approach via two novel losses, which we call an aggregated meta-train loss and a cross-domain loss. The former contributes to incorporating high-resource domain knowledge into the model parameters, while the latter encourages our model, after trained using a particular domain, to still generalize well to another domain, i.e., cross-domain generalization.\nMeta-train phase. As shown in Fig. 2 (C), via Eqs. (3) and (4), we obtain φi from each i-th outdomain datasets. Since this phase is exactly same with the meta-train phase of MetaUMT, we leave out the details.\nMeta-test phase. The aggregated meta-train loss, which refers to Fig. 2 (D), is computed using all out-domain datasets, i.e.,\nLag = n∑ i=0 LsDiout(θ). (6)\nThis loss term allows the model to learn the source domain knowledge that is potentially applicable to a target domain. Moreover, to alleviate the overfitting after adapting to the low-resource domain, we\n2LsDiout and L s Di out′ indicate different batch sampled data\nfrom same Di.\nintroduce a cross-domain loss, which is in Fig. 2 (D), as\nLcd = n∑ i=0 LsDicd(φ i), (7)\nwhere LsDicd = L s Di out′ (φi)+LsDiother(φ i), i.e., computing the cross-domain loss with the data from Diout′ as well as those from other domains Diother .\nTo obtain the optimal initialization θ for model parameters, we define our total loss function, which is Fig. 2 (E), as the sum of the two of our losses, i.e.,\nθ ← θ − β∇θ(Lcd + Lag). (8)\nIn summary, our aggregated meta-train and crossdomain losses encourage our model to accurately and quickly adapt to an unseen target domain. The overall procedure is described in Algorithm A.1."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section first introduces experiment settings and training details. Afterwards, we show empirical results in various scenarios."
    }, {
      "heading" : "5.1 Dataset and Preprocessing",
      "text" : "We conduct our experiments on eight different domains 3(Appendix T.2). Each domain dataset is publicly available on OPUS 4 (Tiedemann, 2012). We utilize the eight domains for out-domain (Dout) and in-domain datasets (Din). To build the monolingual corpora of in-domain and out-domain datasets, we sample data from the parallel corpus. We made sure to include at most one sentence from each pair of parallel sentences. For instance, we sample the first half of the sentences as unpaired\n3Acquis (Law), EMEA (Medical), IT, Tanzil (Koran), Subtitles, EUbookshop (EUB), Europarl, and GlobalVoices (GV)\n4http://opus.nlpl.eu/\nsource data and the other half as truly unpaired target data. Consequently, the sampled monolingual corpora contain no translated sentence in each language. Each of the two monolingual corpora contains the equal number of sentences for each language (e.g., English and German). For our lowresource scenarios, we sample 5,000 tokens from a selected in-domain corpus for each language. Note that the out-domain dataset represents the full monolingual corpora."
    }, {
      "heading" : "5.2 Experimental Settings",
      "text" : "As our base model, we use a Transformer (Vaswani et al., 2017), which is initialized by a masked language model from XLM (Conneau and Lample, 2019) using our out-domain datasets. All the models consist of 6 layers, 1,024 units, and 8 heads.\nWe establish and evaluate various baseline models as follows:\n• UNMT model is trained with only the indomain monolingual data, composed of 5,000 words for each language.\n• Supervised neural machine translation model (NMT) is trained with in-domain parallel datasets, which we arrange in parallel with the two in-domain monolingual corpora.\n• Unadapted model is pretrained with only the out-domain datasets and evaluated on the indomain datasets.\n• Transfer learning model is a finetuned model, which is pretrained with the outdomain datasets and then finetuned with a low-resource in-domain dataset.\n• Mixed finetuned model (Chu et al., 2017) is similar to the transfer learning model, but\nit utilizes both in-domain and out-domain datasets for finetuning. That is, the training batch is sampled evenly from in-domain and out-of-domain datasets."
    }, {
      "heading" : "5.3 Experimental Results",
      "text" : "In order to verify that leveraging the high-resource domains (i.e., the source domains) effects to handle the low-resource domains (i.e., the target domain), we compare the unsupervised and supervised models with ours and other baseline models.\nAs shown in Table 1, the unsupervised model trained on in-domain data suffers from data scarcity because it only uses low-resource in-domain data. Although the unsupervised and supervised models are initialized by XLM, those models show the worst performance in all the cases. This result indicates that when the small size of an in-domain corpus is given, it is appropriate to utilize the outdomain datasets rather than to train only with lowresource data. In addition, the performance of the unadapted model is far behind compared to other models, such as the mixed finetuned model, transfer learning model, MetaUMT, and MetaGUMT. This implies that we need an adequate strategy of leveraging the high-resource domains to improve the performance.\nWe further compare the performance between our proposed approaches (i.e., MetaUMT and MetaGUMT) and the other two finetuning models (i.e., the transfer learning and the mixed finetuned ones). Our methods exhibit the leading performances in both directions of translation (en↔ de), and consistently achieve improvements of 2- 3 BLEU score in most of settings. Furthermore, MetaGUMT consistently obtains better BLEU scores and converges faster than MetaUMT. We assert that our proposed losses (i.e., the aggregated\nmeta-train and the cross-domain losses) help the model not only to perform well even on the unseen in-domain dataset but also to accelerate the convergence speed."
    }, {
      "heading" : "5.4 Performances and Adaptation Speed in Finetuning Stage",
      "text" : "As shown in Fig. 3 (A), we compare our proposed methods with the transfer learning approach by varying the sizes of an in-domain monolingual corpus. The smaller the size of training data is, the wider the performance gap between the two approaches and the transfer learning model becomes. It means that meta-learning is an effective approach to alleviate the performance degradation, preventing the model from overfitting to the low-resource data.\nCompared to the transfer learning model, MetaUMT demonstrates a better performance than other methods in various settings. However, MetaGUMT exhibits even better performances consistently in all settings owing to our proposed losses (Eq. (8)). The transfer learning approach shows the worst performance except for the unadapted model, even though it exploits the in-domain corpus after being pretrained with the out-domain datasets.\nAdditionally, we analyze the number of iterations required for a model to converge given an in-domain dataset. As shown in Fig. 3 (B), the metalearning approaches rapidly converge after only a few iterations, even faster than the transfer learning one does. As the number of in-domain training words increases, the transfer learning approach requires a much larger number of iterations until convergence than our meta-learning approaches. It can be seen that MetaUMT and MetaGUMT rapidly adapt to an unseen domain. Moreover, owing to the encapsulated knowledge from the high-resource do-\nmains, MetaGUMT converges within a relatively earlier iteration than MetaUMT does.\nIn summary, the meta-learning-based methods quickly converge in the low-resource domain, improving the performances over the transfer learning method in various low-resource settings. This indicates that the meta-learning-based approaches are suitable to alleviate the data deficiency issue in scarce domains. Furthermore, our losses in Eq. (8) enhance the capabilities of aggregating domain general knowledge and finding adequate initialization."
    }, {
      "heading" : "5.5 Number of Iterations until Convergence in Pretraining Stage",
      "text" : "An advantage of our meta-learning approaches is that they can find an optimal initialization point from which the model can quickly adapt to a lowresource in-domain dataset. The transfer learning model requires twice more iterations until convergence than ours does. As shown in Fig. 3 (C), MetaUMT and MetaGUMT not only converge quickly but also outperform the other baseline methods. Specifically, compared to MetaUMT, MetaGUMT is effective in achieving an optimized initialization at an earlier iteration. These results indicate that our additional losses (i.e., the crossdomain and aggregated meta-train losses) are beneficial in boosting up the ability for finding an optimal initialization point when training the model with the out-domain datasets."
    }, {
      "heading" : "5.6 Analysis of MetaGUMT losses",
      "text" : "We assume that the domain generalization ability and high-resource domain knowledge are helpful for the UNMT model to translate the low-resource domain sentences. First, to identify whether the model encapsulates the high-resource knowledge from multiple sources, we evaluate our model on out-domain datasets (i.e., Dout) with initial θ. As shown in Table. 2, MetaGUMT shows remarkable performances over MetaUMT in all domains, even better than the transfer learning models. In other words, MetaUMT demonstrates poor performances\nin Dout, compared to MetaGUMT. This can be explained as MetaGUMT uses an aggregated metatrain loss such that MetaGUMT is able to encapsulate the high-resource domain knowledge. As shown in Table. 1, MetaGUMT achieves superior performances, showing that MetaGUMT is capable of leveraging the encapsulated knowledge when finetuning the low-resource target domain.\nSecondly, our cross-domain loss encourages the model to have a generalization capability after adapting to the low-resource target domain. As shown in ”Unseen” column in Table. 2, MetaGUMT outperforms the other models. It can be seen that our model has the domain generalization ability after the finetuning stage due to the cross-domain loss in the meta-test phase."
    }, {
      "heading" : "5.7 Performance of Unbalanced Monolingual Data in Finetuing Stage",
      "text" : "In UNMT, data unbalancing is often the case in that source language (e.g., English) data are abundant and the target language (e.g., Nepali) data are scarce (Kim et al., 2020). We extend our experiment to the unbalanced scenarios to examine whether our proposed model shows the same tendency. In this scenario, the low-resource target domain dataset consists of monolingual sentences from one side with two times more tokens than the monolingual sentences from the other. As shown in Table. 4, MetaGUMT outperforms in all unbalanced data cases. It shows that MetaGUMT is feasible to a practical UNMT scenario where the number of sentences is different in the source and target languages. The only difference against the main experiment setting 5.1 is the condition that the indomain corpus is unbalanced. We also include the\nresult of the transfer learning model in Table. T.4."
    }, {
      "heading" : "5.8 Ablation Study",
      "text" : "We empirically show the effectiveness of the crossdomain and aggregated meta-train losses, as shown in Table 3 5. First, compared to MetaUMT which does not use any of the two losses, incorporating the cross-domain loss improves the average BLEU score by 0.21. The cross-domain loss acts as a regularization function that prevents the model from overfitting during the finetuning stage. Second, the aggregated meta-train loss, another critical component of our model, allows the model to utilize the high-resource domain knowledge in the finetuning stage. This also improves the average BLEU score by 0.37 from MetaUMT. Lastly, combining both cross-domain and aggregated meta-train losses significantly enhances the result in both directions of translation (En ↔ De), indicating that they are complementary to each other."
    }, {
      "heading" : "5.9 Impact of the Number of Source Domains",
      "text" : "We examine how the performances change against the different number of source domains for each approach. As shown in Table. 5 6, MetaGUMT consistently outperforms the transfer, the mixedfinetune, and MetaUMT approaches. As the size of the source domains increases, so does the performance gap between ours and the transferring based models, i.e., transferring and mixed-finetune models. This indicates that the meta-learning based approaches are highly effected by the size of the domains in the meta-train phase, and also, if the number of source domains is large enough to capture the general knowledge, the meta-learning based approaches are suitable to handle the low-resource target task (i.e., machine translation in a low-resource domain).\n5The models are pretrained on Subtitles, Law, EUB, Europarl, IT, and GV and then finetuned on the Medical data.\n6The 4 case contains the Medical, Law, Koran and EUB domains. 5 and 6 additionally utilize one more domain(i.e., IT) and two more domains(i.e.,IT and GV), respectively."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper proposes a novel meta-learning approach for low-resource UNMT, called MetaUMT, which leverages multiple source domains to quickly and effectively adapt the model to the target domain even with a small amount of training data. Moreover, we introduce an improved method called MetaGUMT, which enhances cross-domain generalization and maintains high-resource domain knowledge. We empirically show that our proposed approach consistently outperforms the baseline methods with a nontrivial margin. We believe that our proposed methods can be extended to semisupervised machine translation as well. In the future, we will further analyze other languages, such as Uzbek and Nepali, instead of languages like English and German."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST) and No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques) and by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420)"
    }, {
      "heading" : "B Additional Results on Different Domain Combinations",
      "text" : "Since the combination of Dout and Din can consist differently, this section provides additional results. As shown in Table. T.1, our proposed approaches\n7https://github.com/glample/fastBPE 8https://pytorch.org/ 9https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/ generic/multi-bleu.perl\nstill significantly outperform other baseline models in different domain combination settings."
    }, {
      "heading" : "C Perfomances and Adaptation Speed in Finetuning Stage for a Law Domain",
      "text" : "As shown in Fig C.1 and Fig C.2, MetaGUMT consistently outperforms other methods even though the number words are increasing. Through this experiment, we attempt to show the robustness of our methods (i.e., MetaUMT and MetaGUMT) against others (i.e., transferring and mixed-finetune models). The models are pretrained on Subtitles, EUbookshop, Europarl, GlobalVoices, Medical, and Koran datasets and then finetuned on a Law dataset."
    }, {
      "heading" : "D Comparison between MetaGUMT and MetaUMT Algorithms",
      "text" : "As shown in Algorithms. A.1, we provide an overall algorithm of MetaGUMT. The only difference\nModel Dout\nMedical Law Koran Medical Law Koran GV Europarl EUB Subtitles EUB Europarl Subtitles EUB Europarl Subtitles Medical Koran\nDin IT GV IT\nDe-En En-De epoch De-En En-De epoch De-En En-De epoch Unadapted 18.62 14.89 - 19.27 16.65 - 16.10 15.30 - Transfer 19.80 16.35 4 19.99 16.90 3 19.31 16.13 5 Mixed 19.75 16.49 7 20.03 16.95 5 19.39 16.18 8 MetaUMT 21.08 18.05 4 22.36 18.91 3 20.5 17.06 4 MetaGUMT 21.37 18.42 3 22.76 19.24 2 20.74 17.74 4 Supervised NMT 3.48 3.33 15 0.97 0.85 14 3.53 3.59 10 Unsupervised NMT 1.83 0.86 22 0.51 0.18 20 0.51 0.55 7\nTable T.1: Extended results on various domain settings. The column ‘epoch’ indicates the converged number of epochs for each in-domain dataset. Since the unadapted model does not involve an additional finetuning step, we leave the epoch column as blank.\nAlgorithm A.1 MetaGUMT Require: α, β: step sizes\n1: Pretrain θ by using XLM 2: while not done do 3: for all Diout do 4: Evaluate ∇θLlmDiout(θ) with respect to\nsource and target language sentences from Diout\n5: Back-translation generates source and target language sentences using the current translation model 6: Evaluate ∇θLbtDiout(θ) with using pseudogenerated sentences 7: Sum each gradient: ∇θLsDiout = ∇θL lm Diout\n(θ) +∇θLbtDiout(θ) 8: Compute adapted parameters with one-\nstep gradient descent: φi = θ − α∇θLsDiout(θ)\n9: end for 10: Update θ ← θ − β∇θ(Lcd + Lag) 11: end while\nbetween MetaUMT and MetaGUMT is the metatest phase in line 10. While MetaUMT computes the loss using Eq. (5), MetaGUMT utilizes Eq. (8)."
    }, {
      "heading" : "E Performance of Semi-Superivsed Machine Translation in Finetuning Stage",
      "text" : "The proposed algorithms, MetaUMT and MetaGUMT, show promising results on lowresource monolingual data. However, some may argue that creating parallel sentences from a small number of unpaired monolingual sentences (e.g., 5k tokens) is also feasible. Hence, we additionally\nCorpus Words\nSentences W/S\nEN DE EN DE Acquis (Law) 9.2M 8M 0.7M 12.93 11.30 EMEA (Medical) 7.5M 6.3M 1.1M 6.81 5.75 IT 1.7M 1M 0.3M 9.08 5.32 Tanzil (Koran) 5.6M 5.3MS 0.5M 10.66 10.08 Subtitles 92.7M 87.6M 22.5M 4.11 3.89 EUbookshop (EUB) 115.4M 100M 9.3M 12.37 10.72 Europarl 27.3M 25.7M 1.9M 13.99 13.18 GlobalVoices (GV) 0.6M 0.6M 0.05M 10.67 10.88\nTable T.2: Statistics of each corpora.\nconduct an experiment of semi-supervised machine translation in the finetuning stage. For instance, we follow the same pretraining stage, but we utilize both monolingual and parallel sentences while finetuning the model on a low-resource domain. The number of tokens for each monolingual and parallel data is 5k. To finetune the model in the semi-supervised setting, we compute the loss as sum of Lct and Lbt, where Lct is the conventional translation loss in the supervised NMT, i.e.,\nLct =E(x,y)∼P [−logMs→t (y | x)]+ E(x,y)∼P [−logMt→s (x | y)].\n(9)\nAs shown in Table T.3, we observe that MetaGUMT demonstrates the promising performance against others, even if we only utilize the monolingual out-domain datasets to pretrain the model."
    }, {
      "heading" : "F Statistics of Datasets",
      "text" : "As shown in Table. T.2, we present the overall number of sentences and words for each domain, where W/S indicates the number of words per sentence in a domain.\n# tokens Transfer MetaUMT MetaGUMT Parallel Monolingual De-En En-De De-En De-En En-De De-En 5k 5k 28.04 32.74 30.31 34.31 31.21 35.90 8k 8k 28.86 33.14 30.51 35.22 31.78 36.25 16k 16k 29.62 33.88 31.49 36.62 32.51 37.05 32k 32k 30.55 35.35 33.25 37.25 34.60 38.58\nTable T.3: Results of semi-supervised machine translation in the finetuning stage. “# tokens” indicates the number of tokens for both monolingual and parallel datasets. Each model is first pretrained on Medical, Law, EUbookshop, Koran, IT, and GlobalVoices and then finetuned to the low-resource domain (i.e., Law).\n# tokens Transfer Mixed MetaUMT MetaGUMT En De De-En En-De En-De De-En En-De De-En En-De De-En 5k 10k 25.84 31.67 26.04 31.90 28.80 32.65 29.43 34.28 8k 16k 25.95 31.83 26.09 32.01 27.84 32.93 29.62 34.39 16k 32k 25.96 32.03 26.44 32.37 27.92 32.96 30.10 34.44 32k 64k 27.34 32.64 27.39 32.84 28.67 33.52 29.83 34.77\nTable T.4: Results on unbalanced monolingual data. This is the same results of Table 4 but included the additional baseline model, Transfer."
    } ],
    "references" : [ {
      "title" : "Unsupervised neural machine translation",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre", "Kyunghyun Cho." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR). Vancouver, Canada.",
      "citeRegEx" : "Artetxe et al\\.,? 2018",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "An empirical comparison of domain adaptation methods for neural machine translation",
      "author" : [ "Chenhui Chu", "Raj Dabre", "Sadao Kurohashi." ],
      "venue" : "In Proc. the Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 2: Short Papers),",
      "citeRegEx" : "Chu et al\\.,? 2017",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey of domain adaptation for neural machine translation",
      "author" : [ "Chenhui Chu", "Rui Wang." ],
      "venue" : "Proc. the International Conference on Computational Linguistics (COLING), pages 1304–1319, Santa Fe, New Mexico, USA.",
      "citeRegEx" : "Chu and Wang.,? 2018",
      "shortCiteRegEx" : "Chu and Wang.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 7057–7067, Vancouver, Canada.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Word translation without parallel data",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : "In Proc. International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Conneau et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Investigating meta-learning algorithms for low-resource natural language understanding tasks",
      "author" : [ "Zi-Yi Dou", "Keyi Yu", "Antonios Anastasopoulos." ],
      "venue" : "Proc. the Conference on Empirical Methods in Natural Language Processing and the International",
      "citeRegEx" : "Dou et al\\.,? 2019",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2019
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "Proc. the International Conference on Machine Learning (ICML), volume 70, page 1126–1135.",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Fast domain adaptation for neural machine translation",
      "author" : [ "Markus Freitag", "Yaser Al-Onaizan." ],
      "venue" : "CoRR, abs/1612.06897.",
      "citeRegEx" : "Freitag and Al.Onaizan.,? 2016",
      "shortCiteRegEx" : "Freitag and Al.Onaizan.",
      "year" : 2016
    }, {
      "title" : "Meta-learning for low-resource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor O.K. Li", "Kyunghyun Cho." ],
      "venue" : "Proc. the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3622–3631,",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma." ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), volume 29.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain adaptation of neural machine translation by lexicon induction",
      "author" : [ "Junjie Hu", "Mengzhou Xia", "Graham Neubig", "Jaime Carbonell." ],
      "venue" : "Proc. the Annual Meeting of the Association for Computational Linguistics (ACL), pages 2989–3001, Flo-",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Metalearning representations for continual learning",
      "author" : [ "Khurram Javed", "Martha White." ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 32, Vancouver, Canada.",
      "citeRegEx" : "Javed and White.,? 2019",
      "shortCiteRegEx" : "Javed and White.",
      "year" : 2019
    }, {
      "title" : "When and why is unsupervised neural machine translation useless? In Proc",
      "author" : [ "Yunsu Kim", "Miguel Graça", "Hermann Ney." ],
      "venue" : "the Annual Conference of the European Association for Machine Translation (EACL), pages 35–44, Lisboa, Portugal.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "Proc. of the First Workshop on Neural Machine Translation (WMT), pages 28–39, Vancouver, Canada.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "Unsupervised machine translation using monolingual corpora",
      "author" : [ "Guillaume Lample", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "only. International Conference on Learning Representations",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "2018b. Phrase-based & neural unsupervised machine translation",
      "author" : [ "Guillaume Lample", "Myle Ott", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proc. the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to generalize: Metalearning for domain generalization",
      "author" : [ "Da Li", "Yongxin Yang", "Yi-Zhe Song", "Timothy Hospedales." ],
      "venue" : "Proc.the AAAI Conference on Artificial Intelligence (AAAI), volume 32.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "MetaMT, a meta learning method leveraging multiple domain data for low resource machine translation",
      "author" : [ "Rumeng Li", "Xun Wang", "Hong Yu." ],
      "venue" : "Proc. the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 8245–8252.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proc. the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421, Lisbon, Portugal.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Domain adaptive dialog generation via meta learning",
      "author" : [ "Kun Qian", "Zhou Yu." ],
      "venue" : "Proc. the Annual Meeting of the Association for Computational Linguistics (ACL), pages 2639–2649, Florence, Italy.",
      "citeRegEx" : "Qian and Yu.,? 2019",
      "shortCiteRegEx" : "Qian and Yu.",
      "year" : 2019
    }, {
      "title" : "Optimization as a model for few-shot learning",
      "author" : [ "Sachin Ravi", "Hugo Larochelle." ],
      "venue" : "Proc. International Conenference on Learning Representations (ICLR). Vancouver, Canada.",
      "citeRegEx" : "Ravi and Larochelle.,? 2016",
      "shortCiteRegEx" : "Ravi and Larochelle.",
      "year" : 2016
    }, {
      "title" : "Explicit cross-lingual pre-training for unsupervised machine translation",
      "author" : [ "Shuo Ren", "Yu Wu", "Shujie Liu", "Ming Zhou", "Shuai Ma." ],
      "venue" : "pages 770–779.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proc. the Annual Meeting of the Association for Computational Linguistics (ACL), pages 86–96, Berlin, Germany.",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proc. the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1715–1725, Berlin, Germany.",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "MASS: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proc. the International Conference on Machine Learning (ICML), volume 97, pages 5926–5936.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proc. the International Conference on Language Resources and Evaluation (LREC), pages 2214–2218, Istanbul, Turkey.",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), pages 5998–6008, Long",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol." ],
      "venue" : "Proc. of the International Conference on Machine Learning (ICML), pages 1096–1103,",
      "citeRegEx" : "Vincent et al\\.,? 2008",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Iterative domain-repaired backtranslation",
      "author" : [ "Hao-Ran Wei", "Zhirui Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "Proc. the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5884–5893, Online. Association for Computa-",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised neural machine translation with weight sharing",
      "author" : [ "Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu." ],
      "venue" : "Proc. the Annual Meeting of the Association for Computational Linguistics (ACL), pages 46–55, Melbourne, Australia.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Iterative dual domain adaptation for neural machine translation",
      "author" : [ "Jiali Zeng", "Yang Liu", "Jinsong Su", "Yubing Ge", "Yaojie Lu", "Yongjing Yin", "Jiebo Luo." ],
      "venue" : "Proc the Conference on Empirical Methods in Natural Language Processing and the Inter-",
      "citeRegEx" : "Zeng et al\\.,? 2019",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recently, the state of the art in UNMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019) has achieved comparable performances against supervised neural machine translation (NMT) approaches.",
      "startOffset" : 39,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "Recently, the state of the art in UNMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019) has achieved comparable performances against supervised neural machine translation (NMT) approaches.",
      "startOffset" : 39,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "Recently, the state of the art in UNMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019) has achieved comparable performances against supervised neural machine translation (NMT) approaches.",
      "startOffset" : 39,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "As an effective method for handling a small amount of training data, meta-learning has shown its superiority in various NLP studies such as dialog generation, machine translation, and natural language understanding (Qian and Yu, 2019; Gu et al., 2018; Dou et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 269
    }, {
      "referenceID" : 8,
      "context" : "As an effective method for handling a small amount of training data, meta-learning has shown its superiority in various NLP studies such as dialog generation, machine translation, and natural language understanding (Qian and Yu, 2019; Gu et al., 2018; Dou et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 269
    }, {
      "referenceID" : 5,
      "context" : "As an effective method for handling a small amount of training data, meta-learning has shown its superiority in various NLP studies such as dialog generation, machine translation, and natural language understanding (Qian and Yu, 2019; Gu et al., 2018; Dou et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 269
    }, {
      "referenceID" : 18,
      "context" : "Based on the success of attention-based models (Luong et al., 2015; Vaswani et al., 2017), NMT obtains significant improvement in numerous language datasets, even showing promising results (Wu et al.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "Based on the success of attention-based models (Luong et al., 2015; Vaswani et al., 2017), NMT obtains significant improvement in numerous language datasets, even showing promising results (Wu et al.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "However, the performance of NMT models depends on the size of the parallel dataset (Koehn and Knowles, 2017).",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "To facilitate monolingual corpora, several studies apply dual learning (He et al., 2016), back-translation (Sennrich et al.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : ", 2016), back-translation (Sennrich et al., 2016b), and pretraining the model with bilingual corpora (Hu et al.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : ", 2016b), and pretraining the model with bilingual corpora (Hu et al., 2019; Wei et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : ", 2016b), and pretraining the model with bilingual corpora (Hu et al., 2019; Wei et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "The UNMT models show comparable performances by extending the back-translation method (Conneau et al., 2018) and incorporating methods such as shared Byte",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : ", 2018b) and cross-lingual representations (Conneau and Lample, 2019), following those of the supervised NMT.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "Several models (Chu and Wang, 2018; Hu et al., 2019) show better performances than when trained with the low-resource corpora only.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Several models (Chu and Wang, 2018; Hu et al., 2019) show better performances than when trained with the low-resource corpora only.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "To handle this issue, meta-learning approaches seek for how to adapt quickly and accurately to a lowresource task, and show impressive results in various domains (Finn et al., 2017; Javed and White, 2019).",
      "startOffset" : 162,
      "endOffset" : 204
    }, {
      "referenceID" : 11,
      "context" : "To handle this issue, meta-learning approaches seek for how to adapt quickly and accurately to a lowresource task, and show impressive results in various domains (Finn et al., 2017; Javed and White, 2019).",
      "startOffset" : 162,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "The meta-learning approaches aim to find the optimal initialization of the model parameters that adapts the model to a low-resource dataset in a few iterations of training (Finn et al., 2017; Ravi and Larochelle, 2016).",
      "startOffset" : 172,
      "endOffset" : 218
    }, {
      "referenceID" : 20,
      "context" : "The meta-learning approaches aim to find the optimal initialization of the model parameters that adapts the model to a low-resource dataset in a few iterations of training (Finn et al., 2017; Ravi and Larochelle, 2016).",
      "startOffset" : 172,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "meta learning, recent studies apply the meta learning to low-resource NMT tasks, including multilingual NMT (Gu et al., 2018) and the domain adaptation (Li et al.",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : ", 2018) and the domain adaptation (Li et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "However, a recent work (Li et al., 2018) utilizes the meta learning approach to find a generalized model for multiple target tasks.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "step for competitive performances (Conneau et al., 2018; Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "step for competitive performances (Conneau et al., 2018; Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 119
    }, {
      "referenceID" : 29,
      "context" : "step for competitive performances (Conneau et al., 2018; Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "We use a denoising autoencoder (Vincent et al., 2008) to train the UNMT model, reconstructing an original sentence from a noisy one in a given language.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "Each domain dataset is publicly available on OPUS 4 (Tiedemann, 2012).",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "As our base model, we use a Transformer (Vaswani et al., 2017), which is initialized by a masked language model from XLM (Conneau and Lample, 2019) using our out-domain datasets.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : ", 2017), which is initialized by a masked language model from XLM (Conneau and Lample, 2019) using our out-domain datasets.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "• Mixed finetuned model (Chu et al., 2017) is similar to the transfer learning model, but",
      "startOffset" : 24,
      "endOffset" : 42
    } ],
    "year" : 2021,
    "abstractText" : "Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling datascarce domains. Hence, we extend the metalearning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.",
    "creator" : "LaTeX with hyperref"
  }
}