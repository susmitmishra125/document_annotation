{
  "name" : "2021.acl-long.158.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Edited Media Understanding Frames: Reasoning About the Intent and Implications of Visual Misinformation",
    "authors" : [ "Jeff Da", "Maxwell Forbes", "Rowan Zellers", "Anthony Zheng", "Jena D. Hwang", "Antoine Bosselut", "Yejin Choi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2026–2039\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2026\nIn this paper, we study Edited Media Understanding Frames, a new conceptual formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, effects on individuals, and the overall implications of disinformation. We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time. At the same time, there is still much work to be done – and we provide analysis that highlights areas for further progress."
    }, {
      "heading" : "1 Introduction",
      "text" : "The modern ubiquity of powerful image-editing software has led to a variety of new disinformation threats. From AI-enabled “deepfakes” to lowskilled “cheapfakes,” attackers edit media to engage in a variety of harmful behaviors, such as spreading disinformation, creating revenge porn, and committing fraud (Paris and Donovan, 2019; Chesney and Citron, 2019; Kietzmann et al., 2020, c.f.). Accordingly, we argue that it is important to develop systems to help spot harmful manipulated media. The rapid growth and virality of social\nmedia requires as such, especially as social media trends towards visual content (Gretzel, 2017).\nIdentifying whether an image or video has been digitally altered (i.e., “digital forgery detection”) has been a long-standing problem in the computer vision and media forensics communities. This has enabled the development of a suite of detection approaches, such as analyzing pixel-level statistics and compression artifacts (Farid, 2009; Bianchi and Piva, 2012; Bappy et al., 2017) or identifying\n“what” the edit was (Tan et al., 2019). However, little work has been done on “why” an edit is made, which is necessary for identifying harm. Darkening someone’s skin in a family photo because background light made them seem quite pale is generally harmless. While such color rebalancing is common, darkening Barack Obama’s (or Rafael Warnock’s) skin in campaign ads was clearly meant as a harmful edit by the editor that did it.1 We choose to focus on the “why” – we define a schema for approaching the problem of intent and provide a rich set of natural language responses. We also make a significant contribution towards the “what:” we include a physical-change question, provide rationales based in physical changes, and give structured annotations (bounding boxes) on what was changed in the edit.\nWe introduce Edited Media Understanding Frames (EMU), a new conceptual formalism that captures the notions of “why” and “what” in image editing for language and vision systems (Figure 1). Following literature on pragmatic frames (Sap et al., 2017, 2020; Forbes et al., 2020)—derived from frame semantics (Baker et al., 1998)— we formalize EMU frames along six dimensions that cover a diverse range of inferences necessary to fully capture the scope of visual disinformation. We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content. For every dimension we collect both a classification label and a free-form text explanation. For example, for frame intent, a model must classify the intent of the edit, and describe why this classification is selected.\nWe then introduce a new dataset for our task, EMU, with 56k annotations over 8k image pairs. To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverages recent progress in pretrained multimodal representations of images and text (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2019). We compare our model to a suite of strong baselines, including a standard VLP model (Zhou et al., 2019), and show key improvement in terms of ability to reason about co-referent subjects in the edit. Nevertheless, our task is far from solved: a significant gap remains\n1How Georgia’s Senate race pits the Old South against the New South. https://www.politico.com/news/2020/12/05 /georgia-senate-old-new-south-442423\nbetween the best machine and human accuracy. Our contributions are thus as follows. First, we introduce a new task of Edited Media Understanding Frames, which requires a deep understanding of why an image was edited, and a corresponding dataset, EMU, with 56k captions that cover diverse inferences. In addition, we introduce a new model, PELICAN, improving over competitive languageand-vision transformer baselines. Our empirical study demonstrates promising results, but significant headroom remains. We release our dataset at jeffda.com/edited-media-understanding to encourage further study in discovering pragmatic markers of disinformation."
    }, {
      "heading" : "2 Defining Edited Media Understanding Frames",
      "text" : "Through an edit e on source image i (e.g. “e = x is edited into a room full of drugs”), an editor can cause harm to the subject x’s mental state (mental state: “x is angry about e”) and effect x’s image (effect: “e makes x seem dishonest”) (Rashkin et al., 2016). The editor does this through the intention of the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019).\nTo this end, we collect edits e and source images i from Reddit’s r/photoshopbattles community. There is no readily available (large) central database of harmful image edits, but r/photoshopbattles is replete with suitable complex and culturally implicative edits (e.g., reference to politics or pop culture). This provides us with relevant image edits at a reasonable cost without advocation for dangerous training on real harmful image edits. Keeping the source image i in the task allows us to sustain the tractability of the image edit problem (Tan et al., 2019; Jhamtani and Berg-Kirkpatrick, 2018)."
    }, {
      "heading" : "2.1 Edited Media Understanding Frames: Task Summary",
      "text" : "Given an edit e: IS → IE , we define an edited media understanding frame F (∗) as a collection of typed dimensions and their polarity assignments: (i) physical P(IS → IE): the changes from IS → IE , (ii) intent N(E → IE): whether the Editor E implied malicious intent in IS → IE , (iii) implication M(E → IE): how E might use IE to\nmislead, (iv) mental state S (IE → si): whether the predicate IE impacts the emotion of a role si, (v) effect E(IE → si): the effect of IE on si. We assume frames can be categorized as harmful or not harmful with polarity l ∈ {+,−}. Each polarity l can be interpreted with reason y, and that each reason can be supported with rationale r.\nTechnically, a model is given the following as input:\n• A source image IS , and an edited image IE . • A list of important subjects: expressed as bound-\ning boxes bi for each subject. • An open-ended question q associated with F (∗);\ne.g., “How might subject3 feel upon seeing this edit?” • A list of annotated boxes ai ∈ IE marking the\nobjects in the image that were introduced and modified, and a true/false label denoting if the background was changed.\nA model must produce the polarity classification l′ ∈ {+,−}, interpretation of the polarity (response y′) and rationale for interpolation r′. (For the physical frame, only y needs to be generated). Figure 2 shows an example of our task configuration. The lexicon of the label is fixed for each F(∗) (e.g. for N(∗), − → harmful, +→ harmless)."
    }, {
      "heading" : "3 EMU: A Corpus of Edited Media Understanding Frames",
      "text" : "Sourcing Image Edits We source our image edits from the r/photoshopbattles community on Reddit which hosts regular Photoshop competitions, where given a source photo, members submit a comment with their own edited photo.\nWe collect 8K image edit pairs (source and edited photo pairs) from this community by, first, manually curating a list of more than 100 terms describing people frequently appearing in Photoshop battles posts. Then, we screen over 100k posts for titles that contain one or more of these search terms resulting in 20k collected image pairs. Additionally, we run an object detector (He et al., 2017) to ensure that is at least one person present in each image as a means for ensuring that annotators do not see image pairs without any subjects.\nAnnotating Image Edits We ask a group of vetted crowd workers to identify the main subjects in an image edit and answer open-ended questions in natural language. Each image is annotated by 3 independent crowd workers.\nCrowd workers are first presented with a numbered set of people bounding boxes (produced by Mask R-CNN (He et al., 2017)) over the edited\nFrame Notation Related Question Physical P(IS → IE) What changed in this image edit? Intent N(E → IE) Why would someone create this edit? Implication M(E → IE) How might this edit be used to mislead? Mental State [of subjectX] S (IE → si) How might this image edit make subjectX feel? Effect [on subjectX] E(IE → si) How could this edit mislead public perception of subjectX?\nTable 1: Questions for each of the frames in Edited Media Understanding Frames. Each frame is associated with a question that allows human annotators to address the frame, and models to generate l, y, r for the given frame.\nimage and are asked to select subjects that are significant to the edits (as opposed, say, a crowd in the background). Once subjects are selected, the annotators are asked to assign classification labels for each of the five possible question types and provide free-form text answers for each question (when applicable). For the classification label, we retain the majority vote (Fleiss κ = 0.67). In a separate and final pass, we explicitly identify which portions of the modified image is introduced or altered by asking the workers to to label the most important sections of the modified image and selecting one of the two labels. The statistics of the dataset are shown in Figure 3."
    }, {
      "heading" : "4 Modeling Edited Media Understanding Frames",
      "text" : "In this section, we present a new model for Edited Media Understanding Frames, with a goal of kickstarting research on this challenging problem. As described in Section 2, our task differs from many standard vision-and-language tasks both in terms of format and required reasoning: a model must take as input two images (a source image and its edit), with a significant change of implication added by the editor. A model must be able to answer questions, grounded in the main subjects of the image, describing these changes. The answers are either boolean labels, or open-ended natural language – including explainable rationales."
    }, {
      "heading" : "4.1 Our model: PELICAN",
      "text" : "For Edited Media Understanding Frames, not all image regions are created equal. Not only is the subject referred to in the question (e.g. subject1) likely important, so too are all of the regions in\nthe image edit that are introduced or altered. We propose to use the annotations that collected for these regions as additional signal for the model to highlight where to attend.2 Not only should a model likely attend to these important regions, it should prioritize attending to regions nearby (such as objects that an edited person is interacting with).\nWe propose to model the (likely) importance of an image region through graph propagation. We will build a directed graph with all regions of the image, rooted at a subject mentioned by the question (e.g. subject1). We will then topologically sort this graph; each region is then given an embedding corresponding to its sorted position – similar to the position embedding in a Transformer. This will allow the model to selectively attend to important image regions in the image edit. We use a different position embedding for the image source, and do not perform the graph propagation here (as we do not have introduced or altered annotations); this separate embedding captures the inductive bias that the edited is more important than the source.\n2These annotations are collected from workers, but in theory, it would be possible to train a model to annotate regions as such. To make our task as accessible and easy-to-study as possible, however, we use the provided labels in place of a separate model however."
    }, {
      "heading" : "4.2 Model details and Transformer integration",
      "text" : "In this section, we describe integrating our importance embeddings with a multimodal transformer.\nLet the source image be IS and IE . We use the backbone feature extractor φ ( Faster-RCNN feature extractor (Ren et al., 2015; Anderson et al., 2018) to extract N regions of interest for each region:\n[s1, ... , sN] = φ(IS ) [e1, ... , eN] = φ(IE). (1)\nWe note that some of these regions in e1, ... , eN are provided to the model (as annotated regions in the image); the rest are detected by φ. These, plus the language representation of the question, are passed to the Transformer backbone T :\n[z1 ... zN+L] = T ([s1 ... sN], [e1, ... , eN] , [x1 ... xL]) (2)\nImportant for EMU, z2N+1, ... , z2N+L serve as language representations. Training under a left-toright language modeling objective, we can predict the next next token xL+1 using the representation zN+L."
    }, {
      "heading" : "4.2.1 Prioritization Embeddings from Topological Sort",
      "text" : "Transformers require position embeddings to be added to each image region and word – enabling it to distinguish which region is which. We supplement the position embeddings of the regions\n{e1...eN} in the edited image IE with the result of a topological sort.\nGraph definition. We define the graph over image regions in the edited image as follows. We begin by sourcing a seed region s ∈ {e1...eN}. Let G = (V, E), where each v ∈ V represents metadata of some ri ∈ φ(IE), defined as vi ∈ m(IE) for simplicity, s.t.:\nvi = {x1, y1, x2, y2, si, li} (3)\nwhere x1, y1, x2, y1 represents the bounding box of ri, si ∈ {1, 0} denoting if ri is a subject of IE , and li ∈ {introduced, altered} denoting the label of ri.\nWe build the graph iteratively: for each iteration, we define an edge e = {v, u}; u ∈ V s.t.:\n∀v ∈ m(IE),∀u ∈ V, E = E ∪ (u, v) ∈ E′ (4)\nWe define E′ as the set of edges (u, v) in which u and v are notationally similar. We define three cases in which this is true: if si ∈ ui ∧ s j ∈ v j, if li ∈ ui = l j ∈ v j, and if x1, y1, x2, y2 ∈ ui and x3, y3, x4, y4 ∈ ui overlaps, in which the percentage overlap is defined by standard intersection-overunion:\nmin{x4, x2} −max{x3, x1} min{y4, y2} −max{y3, y1}\n(5)\nWe cap the number of outgoing edges at 3, and prevent cycles by allowing edges only to unseen image regions. In cases where there are more than\nthree possible edges, we add edges in the order defined in the previous paragraph, and break overlap ties via maximum overlap.\nTo produce embeddings, we run topological sort over the directed graph to assign each image region an embedding, then assign an embedding to each image region based on the ordered index. The embedding is zeroed out for image regions that are missing from the DAG, and from the source image (which are unlabeled). We include bounding box and class labels. To generate text and classification labels, we attach the embeddings onto the input for an encoder-decoder structure."
    }, {
      "heading" : "5 Experimental Results on EMU",
      "text" : "In this section, we evaluate a variety of strong vision-and-language generators on EMU. Similar to past work on VQA, we rebalance our test set split ensuring a 50/50 split per question type of maliciously labeled captions. We provide two human evaluation metrics – head-to-head, in which generated responses are compared to human responses, and accuracy, in which humans are asked to label if generated responses are accurate in regards to the given edit."
    }, {
      "heading" : "5.1 Baselines",
      "text" : "In addition to evaluating PELICAN, we compare and evaluate the performance of various potentially high-performing baselines on our task.\na. Retrieval. For a retrieval baseline, which generally performs well for generation-based tasks, we use features from ResNet-158 (He et al., 2016), defined as φ, to generate vectors for each IE in the test set. We then find the most similar edited image IT in the training set T via cosine similarity:\nargmax IT∈T φ(IE) · φ(IT ) ‖φ(IE)‖ × ‖φ(IT )‖\n(6)\nWe use the captions associated with the most similar image in the training set.\nb. GPT-2 (Radford et al., 2019). As a text-only baseline, we use the 117M parameter model from GPT-2, fine-tuned on the captions from our dataset. Since the images are not taken into consideration, we generate from the seeds associated with each question type and use the same captions for all images in the test set.\nc. Cross-Modality GPT-2. We test a unified language-and-vision model on our dataset. Similar to (Alberti et al., 2019), we append the visual\nfeatures φ(IS ) and φ(IE) to the beginning of the token embeddings from GPT-2 (117M). For the questions involving a subject, we append an additional vector φ(r), where r is the region defined by the bounding box for that subject.\nd. Dynamic Relational Attention (Tan et al., 2019). We test the best model from previous work on image edits on our task, Dynamic Relational Attention. We train the model from scratch on our dataset, using the same procedure as (Tan et al., 2019). We seed each caption with the relevant question.\ne. VLP (Zhou et al., 2019). We test VLP, a pre-trained vision-and-language transformer model. For image captioning, VLP takes a single image as input and uses an off-the-shelf object detector to extract regions, generation a caption using sequenceto-sequence decoding and treating the regions as a sequence of input tokens.\nTo generate a caption for a particular question type, we fix the first few generated tokens to match the prefix for that question type. We fine-tune VLP starting from weights pre-trained on Conceptual Captions (3.3m image-caption pairs) (Sharma et al., 2018) and then further trained on COCO Captions (413k image-caption pairs) (Lin et al., 2014)."
    }, {
      "heading" : "5.2 Quantitative Results and Ablation Study",
      "text" : "We present our results in Table 2. We calculate generative metrics (e.g. METEOR) by appending the rationale to the response. Generations from PELICAN are preferred over human generations 14.0% of the time, with a 0.86 drop in perplexity compared to the next best model. To investigate the performance of the model, we run an ablation study on various modeling attributes, detailed in Table 3. First, we investigate the effect of pretraining (on Conceptual Captions (Sharma et al., 2018; Zhou et al., 2019)). We find that performance drops without pretraining (53.47%), but surprisingly still beats other baselines. This suggests that the task requires more pragmatic inferences than the semantic learning typically gained from pre-training tasks. Second, we ablate the importance of including annotated (ai) features from the dataset when creating the directed graph, relying on a seed from a random R-CNN region (54.44%). We also ablate our use of topological sort and a directed graph by suggesting a simple (but consistent) order for image regions (54.91%). Finally, we ablate including the visual regions from the source image. The performance is\nsimilar (55.35%), suggesting that PELICAN would be able to perform in real-world settings in which only the edited image is present (e.g. social media posts)."
    }, {
      "heading" : "5.3 Qualitative Results",
      "text" : "Last, we present qualitative examples in Figure 5. PELICAN is able to correctly understand image pairs which require mostly surface level understanding - for example, in the top example, it is able to identify that the gun and action implies negative context, but misunderstands the response with regards to the situation. In the bottom example, we show that PELICAN is able to refer to subject1 correctly, but misinterprets the situation to be non-negative."
    }, {
      "heading" : "6 Future Implications",
      "text" : ""
    }, {
      "heading" : "6.1 EMU in the Real World",
      "text" : "To study if EMU is helpful in real-world settings, we train a model of PELICAN on EMU with only the edited image. In this setting, the model must hypothesize which parts of the image were edited and discern the main subjects in the image. At test time, we generate captions for each of the 5 intentionbased question types. Results of this version of PELICAN is in Table 2.\nWhile this evaluation scheme is crude, we find that this version of PELICAN is still able to outperform previous models without usage of the source image. This suggests potential for generations from EMU-trained models in human-assisted settings. In an initial human study (given PELICAN REAL captions, classify the edit as disinformation – were the captions helpful in your decision?) we find that annotators label as helpful 71.5% of the time. Additionally, annotators tended more often to pick the gold label (89.1%→ 95.2%)."
    }, {
      "heading" : "6.2 Failure Cases in Current Models and Avenues for Future Research",
      "text" : "EMU also helps us understand what current visionand-language models are missing for use on disinformation , by analyzing the reasons and rationales generated. We ask annotators to compare PELICAN-generated captions marked as “worse”\nand human captions. Category details are included in the appendix. Figure 6 shows our results. Overall, current models primarily lack the commonsense (event-based and social) to accurately describe disinformation. Geographical (locationbased) and political (e.g. knowledge about the job of a president) external knowledge is also a missing component.\nPELICAN also still makes mistakes in description-related attributes: describing something other than the important change and an inaccuracy (e.g. wrong color) are the most common. Specific information – such as information relating to a specific person in the image (i.e. requiring a model to identify the person in the image), and information about a past event – are the least critical, suggesting that efforts should be focused first on general intelligence rather than named-entity lookup."
    }, {
      "heading" : "7 Related Work",
      "text" : "Language-and-Vision Datasets Datasets involving images and languages cover a variety of tasks, including visual question answering (Agrawal et al., 2015; Goyal et al., 2017), image caption generation (Lin et al., 2014; Young et al., 2014; Krishna et al., 2016), visual storytelling (Park and Kim, 2015; Bosselut et al., 2016), machine translation (Elliott et al., 2016), visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019), and visual common sense (Zellers et al., 2019).\nTwo-image tasks Though most computer vision tasks involve single images, some work has been done on exploring image pairs. The NLVR2 dataset (Suhr et al., 2019) involves yes-no question answering over image pairs. Neural Naturalist (Forbes et al., 2019) tests fine-grained captioning of bird pairs; (Jhamtani and Berg-Kirkpatrick, 2018) identifies the difference between two similar images.\nImage Edits There has been some computer vision research studying image edits. Unlike our EMU dataset, however, much of this work has focused on modeling lower-level image edits wherein the cultural implications do not change significantly between images. For example, (Tan et al., 2019) predicts image editing requests (generate ‘change the background to blue’ from a pair of images). Past work has also studied learning to perform image adjustments (like colorization and enhancement) from a language query (Chen et al., 2017; Wang et al., 2018). Hateful Meme Challenge\n(Kiela et al., 2020) is a recent work challenging models to classify a meme as hateful or not."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We present Edited Media Understanding Frames– a language-and-vision task requiring models to answer open-ended questions that capture the intent and implications of an image edit. Our model, PELICAN, kickstarts progress on our dataset – beating all previous models and with humans rating its answers as accurate 48.2% of the time. At the same time, there is still much work to be done – and we provide analysis that highlights areas for further progress."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Ryan Qiu for help with analysis, and the Amazon Mechanical Turk community for help with annotation. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE1256082, and in part by NSF (IIS-1714566), DARPA CwC through ARO (W911NF15-1-0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF (IIS-1714566), and the Allen Institute for AI."
    }, {
      "heading" : "9 Ethical Considerations",
      "text" : "In constructed the EMU dataset, great care was taken to ensure that crowd-workers are compensated fairly for their efforts. To this end, we monitored median HIT completion times for each published batch, adjusting the monetary reward such that at least 80% of workers always received >$15/hour, which is roughly double the minimum wage in the United States (the country of residence for most Amazon Mechanical Turk workers). This included the qualification and evaluation rounds. The following data sheet summarized relevant aspects of the data collection process (Bender and Friedman, 2018):\nA. Curation Rationale: Selection criteria for the edits included in the presented dataset are discussed Section 3. We selected the highest-rated posts on Reddit, and collected metadata data from annotators marking if the edit is NSFW or offensive.\nB. Language Variety: The dataset is available in English, with mainstream US Englishes being the dominant variety, as per the demographic of Amazon Mechanical Turk workers.\nC. Speaker Demographic: N/A\nD. Annotator Demographic: N/A E. Speech Situation: All frames were collected and validated over a period of about 12 weeks, between November and January 2020, through the Amazon AMT platform. Workers were given regular, detailed feedback regarding the quality of their submissions and were able to address any questions or comments to the study’s main author via Email or Slack.\nF. Text Characteristics: In line with the intended purpose of the dataset, the included edits describe social interactions related (but not limited to) platonic and romantic relationships, political situations, as well as cultural and social contexts.\nG. Recording Quality: N/A H. Other: N/A Lastly, we want to emphasize that our work is strictly scientific in nature, and serves the exploration of machine reasoning alone. It was not developed to offer guidance on misinformation or to train models to classify social posts as misinformation. Consequently, the inclusion of malicious image edits could allow adversaries to train malicious agents to produce visual misinformation. We are aware of this risk, but also want to emphasize that the utility of these agents allow useful negative training signal for minimizing harm that may be cased by agents operating in visual information. It is, therefore, necessary for future work that uses our dataset to specify how the collected examples of both negative and positive misinformation are used, and for what purpose."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Reproducibility of Experiments\nWe provide downloadable source code of all scripts, and experiments, at to-be-provided. We use two Titan X GPUs to train and evaluate all models, except Dynamic Relational Attention (Tan et al., 2019), which was trained on a single Titan Xp GPU. For GPT-2 (Radford et al., 2019), we use the 117M parameter model, taking 5 hours to train. Our configuration of VLP (Zhou et al., 2019) has 138,208,324 parameters, taking 6 hours to train. Our model, PELICAN, has 138,208,324 parameters, taking 6 hours to train. Our Dynamic Relational Attention model has 55,165,687 parameters, taking 10 hours to train.\nA.2 Reproducibility of Hyperparameters\nFor models using GPT-2 as their underlying infrastructure, we use a maximum sequence length of 1024, 12 hidden layers, 12 heads for each attention layer, and 0.1 dropout in all fully connected layers. For Dynamic Relational Attention (Tan et al., 2019), we use a batch size of 95, hidden dimension size of 512, embedding dimension size of 256, 0.5 dropout, Adam optimizer, and a 1e-4 learning rate. We used early stopping based on the BLEU score on the validation set at the end of every epoch; the test scores reported are for a model trained for 63 epochs. For all models relying on VLP as their underlying infrastructure, we use 30 training epochs, 0.1 warmup proportion, 0.01 weight decay, 64 batch size.\nA.3 Reproducibility of Datasets\nOur dataset has 39338 examples in the training set and 4268 and 3992 examples in the development and test sets respectively. All training on additional datasets (e.g. (Zhou et al., 2019)) matches their implementation exactly. Our train/val/test splits were chosen at random, during the annotation period. No data was excluded, and no additional pre-processing was done. A downloadable link is available at to-be-provided after publication.\nA.4 Data Collection\nFor reference and reproducibility, we show the full template used to collect data in Figure 9.\nWe also show our human evaluation process in Figure 10.\nA.5 Additional Annotation Details\nFor an image pair (consisting of an image edit and a source image), we 1) ask the annotator to identify and index the main subjects in the image, 2) prime the annotator by asking them to describe the physical change in the image, 3) ask a series of questions for each main person they identified, and 4) ask a series of questions about the image as a whole. For each question we require annotators to provide both an answer to the question and a rationale (e.g. the physical change in the image edit that alludes to their answer). This is critical, as the rationales prevent models from guessing a response such as “would be harmful” without providing the proper reasoning for their response. We ask annotators to explicitly separate the rationale from the response by using the word “because” or “since” (however, we find that the vast majority of annotators naturally do this, without being explicitly prompted). For the main subjects, we limit the number of subjects to 3. This also mitigates a large variation in workload between image pairs, which was gathered as potentially problematic from annotator feedback. We limit the number of captions per type to 3. We find that a worker chooses to provide more than one label for a type in only a small proportion of cases, suggesting that usually, one caption is needed to convey all the information about the image edit relating to that type .\nA.6 Lexical Analysis\nWord-Level Statistics We analyze the lexical statistics of this dataset. We remove stop words as words such as “him”. We show that different types require different language in their response. In addition, we highlight that many of the rationales involve people, suggesting that understanding social implications is critical to solving this task.\nA.7 Motivation for EMU Task Definition\nWe begin by motivating and contextualising our problem. A key insight is that we need to think into the future – since the task is important but difficult, we aim to structure EMU such that it can help models learn how to understand misinformation (by providing the source image, grounding captions, and additional annotations) without oversimplifying the task.\nFrames. We ask models a series of questions about the what and why of the image edit. We arrived on these questions by first asking annotators to explain the image edits without prompting. Then, we bucketed the responses into similar categories, motivating us to create questions based on the parts of edits humans naturally focused on. In our task, we consider six open-ended question types – physical, intent, implication, emotion [of SubjectX], attack [on SubjectX], and disinformation. Descriptions of each are in Figure 2. Each type focuses on a different aspect of the image edit, and is related one-to-one with an open-ended question q. Each question type may also reference a specific entity b. In these cases, the answer to the question would differ based on the main subject\nreferred. Labels. For each q, we ask models to provide both a classification label l and a generated answer (response y and rationale r) for a given image edit. Visual misinformation is not a closed form problem – the potential label-space and responses for an malicious edit are ever-changing with recent events. Thus, we suggest that models need to produce a generated answer. However, we also want models to go beyond simple answering – we want them to answer for the right reasons, in an explainable way. Thus, we require models to generate a rationale explaining why its answer is true. For example, a good rationale explains that the perception of subject1 could be injured because a gun was added to subject1’s hand. Our evaluation recruits human raters to compare generated answers and rationales y/r to those written by annotators. To account for the current difficulty of evaluating generation, we\ninclude a binary classification label l for each of the “why” answers to allow for a simple checkpoint evaluation metric of model progress.\nGrounding. Each explanation is grounded to bounding boxes ai of the people in the edited image. Similar to past work in vision-and-language (Zellers et al., 2019), annotators write captions that refer to the bounding box (for example, subject1would be angry). This allows precise reference in visually complex edits.\nAdditional annotations. Finally, we provide annotators for bounding boxes of introduced and modified regions in edited images. These bounding boxes provide the syntax of the change in a machine digestible format (bounding boxes + labels). We conduct initial exploration of the empirical benefit\nof these labels in our modeling section."
    } ],
    "references" : [ {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "International Journal of Computer Vision, 123:4–31.",
      "citeRegEx" : "Agrawal et al\\.,? 2015",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2015
    }, {
      "title" : "Fusion of detected objects in text for visual question answering",
      "author" : [ "Chris Alberti", "Jeffrey Ling", "Michael Collins", "David Reitter." ],
      "venue" : "ArXiv, abs/1908.05054.",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F. Baker", "C. Fillmore", "J. Lowe." ],
      "venue" : "COLING-ACL.",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Exploiting spatial structure for localizing manipulated image regions",
      "author" : [ "Jawadul H Bappy", "Amit K Roy-Chowdhury", "Jason Bunk", "Lakshmanan Nataraj", "BS Manjunath." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages",
      "citeRegEx" : "Bappy et al\\.,? 2017",
      "shortCiteRegEx" : "Bappy et al\\.",
      "year" : 2017
    }, {
      "title" : "Data statements for nlp: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "B. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Bender and Friedman.,? \\Q2018\\E",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Image forgery localization via block-grained analysis of jpeg artifacts",
      "author" : [ "Tiziano Bianchi", "Alessandro Piva." ],
      "venue" : "IEEE Transactions on Information Forensics and Security, 7(3):1003–1017.",
      "citeRegEx" : "Bianchi and Piva.,? 2012",
      "shortCiteRegEx" : "Bianchi and Piva.",
      "year" : 2012
    }, {
      "title" : "Learning prototypical event structure from photo albums",
      "author" : [ "Antoine Bosselut", "Jianfu Chen", "David Warren", "Hannaneh Hajishirzi", "Yejin Choi." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Bosselut et al\\.,? 2016",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2016
    }, {
      "title" : "Language-based image editing with recurrent attentive models",
      "author" : [ "Jianbo Chen", "Yelong Shen", "Jianfeng Gao", "Jingjing Liu", "Xiaodong Liu." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8721–8729.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep fakes: A looming challenge for privacy, democracy, and national security",
      "author" : [ "Bobby Chesney", "Danielle Citron." ],
      "venue" : "Calif. L. Rev., 107:1753.",
      "citeRegEx" : "Chesney and Citron.,? 2019",
      "shortCiteRegEx" : "Chesney and Citron.",
      "year" : 2019
    }, {
      "title" : "Multi30k: Multilingual englishgerman image descriptions. ArXiv, abs/1605.00459",
      "author" : [ "Desmond Elliott", "Stella Frank", "Khalil Sima’an", "Lucia Specia" ],
      "venue" : null,
      "citeRegEx" : "Elliott et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey of image forgery detection",
      "author" : [ "Hany Farid." ],
      "venue" : "IEEE Signal Processing Magazine, 26(2):16–25.",
      "citeRegEx" : "Farid.,? 2009",
      "shortCiteRegEx" : "Farid.",
      "year" : 2009
    }, {
      "title" : "Social chemistry 101: Learning to reason about social and moral norms",
      "author" : [ "M. Forbes", "Jena D. Hwang", "Vered Shwartz", "Maarten Sap", "Yejin Choi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Forbes et al\\.,? 2020",
      "shortCiteRegEx" : "Forbes et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural naturalist: Generating fine-grained image comparisons",
      "author" : [ "Maxwell Forbes", "Christine Kaeser-Chen", "Piyush Sharma", "Serge J. Belongie." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Forbes et al\\.,? 2019",
      "shortCiteRegEx" : "Forbes et al\\.",
      "year" : 2019
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "The visual turn in social media marketing",
      "author" : [ "Ulrike Gretzel" ],
      "venue" : null,
      "citeRegEx" : "Gretzel.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gretzel.",
      "year" : 2017
    }, {
      "title" : "Mask r-cnn",
      "author" : [ "Kaiming He", "Georgia Gkioxari", "Piotr Dollár", "Ross B. Girshick." ],
      "venue" : "2017 IEEE International Conference on Computer Vision (ICCV), pages 2980–2988.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6693–6702.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Learning to describe differences between pairs of similar images",
      "author" : [ "Harsh Jhamtani", "Taylor Berg-Kirkpatrick." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Jhamtani and Berg.Kirkpatrick.,? 2018",
      "shortCiteRegEx" : "Jhamtani and Berg.Kirkpatrick.",
      "year" : 2018
    }, {
      "title" : "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "Johanna E. Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross B. Girshick." ],
      "venue" : "2017 IEEE Conference on Com-",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "author" : [ "Douwe Kiela", "Hamed Firooz", "Aravind Mohan", "Vedanuj Goswami", "Amanpreet Singh", "Pratik Ringshia", "Davide Testuggine." ],
      "venue" : "ArXiv, abs/2005.04790.",
      "citeRegEx" : "Kiela et al\\.,? 2020",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2020
    }, {
      "title" : "Deepfakes: Trick or treat",
      "author" : [ "Jan Kietzmann", "Linda W Lee", "Ian P McCarthy", "Tim C Kietzmann" ],
      "venue" : "Business Horizons,",
      "citeRegEx" : "Kietzmann et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kietzmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual genome: Connecting language and vision",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "ArXiv, abs/1405.0312.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Deepfakes and cheap fakes",
      "author" : [ "Britt S. Paris", "Joan M. Donovan." ],
      "venue" : "Technical report, Data and Society.",
      "citeRegEx" : "Paris and Donovan.,? 2019",
      "shortCiteRegEx" : "Paris and Donovan.",
      "year" : 2019
    }, {
      "title" : "Expressing an image stream with a sequence of natural sentences",
      "author" : [ "Cesc C. Park", "Gunhee Kim." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Park and Kim.,? 2015",
      "shortCiteRegEx" : "Park and Kim.",
      "year" : 2015
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Connotation frames: A data-driven investigation",
      "author" : [ "Hannah Rashkin", "Sameer Singh", "Yejin Choi." ],
      "venue" : "arXiv: Computation and Language.",
      "citeRegEx" : "Rashkin et al\\.,? 2016",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2016
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in neural information processing systems, pages 91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Connotation frames of power and agency in modern films",
      "author" : [ "Maarten Sap", "Marcella Cindy Prasettio", "Ari Holtzman", "Hannah Rashkin", "Yejin Choi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Sap et al\\.,? 2017",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2017
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "Fake news detection on social media: A data mining perspective",
      "author" : [ "Kai Shu", "Amy Sliva", "Suhang Wang", "Jiliang Tang", "Huan Liu." ],
      "venue" : "ACM SIGKDD explorations newsletter, 19(1):22–36.",
      "citeRegEx" : "Shu et al\\.,? 2017",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2017
    }, {
      "title" : "A corpus for reasoning about natural language grounded in photographs",
      "author" : [ "Alane Suhr", "Stephanie Zhou", "Iris D. Zhang", "Huajun Bai", "Yoav Artzi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Suhr et al\\.,? 2019",
      "shortCiteRegEx" : "Suhr et al\\.",
      "year" : 2019
    }, {
      "title" : "Lxmert: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Expressing visual relationships via language",
      "author" : [ "Hao Tan", "Franck Dernoncourt", "Zhe Lin", "Trung Bui", "Mohit Bansal." ],
      "venue" : "ACL.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to globally edit images with textual description",
      "author" : [ "Hai Wang", "Jason D. Williams", "SingBing Kang." ],
      "venue" : "ArXiv, abs/1810.05786.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6713–6724.",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified vision-language pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lefei Zhang", "Houdong Hu", "Jason J. Corso", "Jianfeng Gao." ],
      "venue" : "ArXiv, abs/1909.11059.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey of fake news: Fundamental theories, detection methods, and opportunities",
      "author" : [ "Xinyi Zhou", "Reza Zafarani." ],
      "venue" : "ACM Computing Surveys (CSUR), 53(5):1–40.",
      "citeRegEx" : "Zhou and Zafarani.,? 2020",
      "shortCiteRegEx" : "Zhou and Zafarani.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "media requires as such, especially as social media trends towards visual content (Gretzel, 2017).",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "This has enabled the development of a suite of detection approaches, such as analyzing pixel-level statistics and compression artifacts (Farid, 2009; Bianchi and Piva, 2012; Bappy et al., 2017) or identifying",
      "startOffset" : 136,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "This has enabled the development of a suite of detection approaches, such as analyzing pixel-level statistics and compression artifacts (Farid, 2009; Bianchi and Piva, 2012; Bappy et al., 2017) or identifying",
      "startOffset" : 136,
      "endOffset" : 193
    }, {
      "referenceID" : 4,
      "context" : "This has enabled the development of a suite of detection approaches, such as analyzing pixel-level statistics and compression artifacts (Farid, 2009; Bianchi and Piva, 2012; Bappy et al., 2017) or identifying",
      "startOffset" : 136,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "Following literature on pragmatic frames (Sap et al., 2017, 2020; Forbes et al., 2020)—derived from frame semantics (Baker et al.",
      "startOffset" : 41,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : ", 2020)—derived from frame semantics (Baker et al., 1998)— we formalize EMU frames along six dimensions that cover a diverse range of inferences necessary to fully capture the scope of visual disinformation.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : "We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 36,
      "context" : "We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 44,
      "context" : "We delve into the concept of intention as discussed by the fake news literature (Rashkin et al., 2017; Shu et al., 2017; Zhou and Zafarani, 2020) to capture editor’s intent such as motivation for edit and intent to deceive, as well as the resulting implications of the edited content.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : "To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverages recent progress in pretrained multimodal representations of images and text (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 26,
      "context" : "To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverages recent progress in pretrained multimodal representations of images and text (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 24,
      "context" : "To kickstart progress on our task, we introduce a new language and vision model, PELICAN, that leverages recent progress in pretrained multimodal representations of images and text (Tan and Bansal, 2019; Lu et al., 2019; Li et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 43,
      "context" : "We compare our model to a suite of strong baselines, including a standard VLP model (Zhou et al., 2019), and show key improvement in terms of ability to reason about co-referent subjects in the edit.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "“e = x is edited into a room full of drugs”), an editor can cause harm to the subject x’s mental state (mental state: “x is angry about e”) and effect x’s image (effect: “e makes x seem dishonest”) (Rashkin et al., 2016).",
      "startOffset" : 198,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019).",
      "startOffset" : 144,
      "endOffset" : 208
    }, {
      "referenceID" : 33,
      "context" : "the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019).",
      "startOffset" : 144,
      "endOffset" : 208
    }, {
      "referenceID" : 27,
      "context" : "the edit (intent: “e intends to harm x’s image”) and changing the implications of the image (implication: “e frames x as a drug cartel member”) (Forbes et al., 2020; Sap et al., 2020; Paris and Donovan, 2019).",
      "startOffset" : 144,
      "endOffset" : 208
    }, {
      "referenceID" : 39,
      "context" : "Keeping the source image i in the task allows us to sustain the tractability of the image edit problem (Tan et al., 2019; Jhamtani and Berg-Kirkpatrick, 2018).",
      "startOffset" : 103,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "Keeping the source image i in the task allows us to sustain the tractability of the image edit problem (Tan et al., 2019; Jhamtani and Berg-Kirkpatrick, 2018).",
      "startOffset" : 103,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we run an object detector (He et al., 2017) to ensure that is at least one person present in each image as a means for ensuring that annotators do not see image pairs without any subjects.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "Crowd workers are first presented with a numbered set of people bounding boxes (produced by Mask R-CNN (He et al., 2017)) over the edited",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : "We use the backbone feature extractor φ ( Faster-RCNN feature extractor (Ren et al., 2015; Anderson et al., 2018) to extract N regions of interest for each region:",
      "startOffset" : 72,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "We use the backbone feature extractor φ ( Faster-RCNN feature extractor (Ren et al., 2015; Anderson et al., 2018) to extract N regions of interest for each region:",
      "startOffset" : 72,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "For a retrieval baseline, which generally performs well for generation-based tasks, we use features from ResNet-158 (He et al., 2016), defined as φ, to generate vectors for each IE in the test set.",
      "startOffset" : 116,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "Similar to (Alberti et al., 2019), we append the visual features φ(IS ) and φ(IE) to the beginning of the token embeddings from GPT-2 (117M).",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 39,
      "context" : "We train the model from scratch on our dataset, using the same procedure as (Tan et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "3m image-caption pairs) (Sharma et al., 2018) and then further trained on COCO Captions (413k image-caption pairs) (Lin et al.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : ", 2018) and then further trained on COCO Captions (413k image-caption pairs) (Lin et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : "First, we investigate the effect of pretraining (on Conceptual Captions (Sharma et al., 2018; Zhou et al., 2019)).",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : "First, we investigate the effect of pretraining (on Conceptual Captions (Sharma et al., 2018; Zhou et al., 2019)).",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "Language-and-Vision Datasets Datasets involving images and languages cover a variety of tasks, including visual question answering (Agrawal et al., 2015; Goyal et al., 2017), image caption generation (Lin et al.",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "Language-and-Vision Datasets Datasets involving images and languages cover a variety of tasks, including visual question answering (Agrawal et al., 2015; Goyal et al., 2017), image caption generation (Lin et al.",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 25,
      "context" : ", 2017), image caption generation (Lin et al., 2014; Young et al., 2014; Krishna et al., 2016), visual storytelling (Park and Kim, 2015; Bosselut et al.",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : ", 2017), image caption generation (Lin et al., 2014; Young et al., 2014; Krishna et al., 2016), visual storytelling (Park and Kim, 2015; Bosselut et al.",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : ", 2017), image caption generation (Lin et al., 2014; Young et al., 2014; Krishna et al., 2016), visual storytelling (Park and Kim, 2015; Bosselut et al.",
      "startOffset" : 34,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : ", 2016), visual storytelling (Park and Kim, 2015; Bosselut et al., 2016), machine translation (Elliott et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : ", 2016), visual storytelling (Park and Kim, 2015; Bosselut et al., 2016), machine translation (Elliott et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : ", 2016), machine translation (Elliott et al., 2016), visual reasoning (Johnson et al.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : ", 2016), visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019), and visual common sense (Zellers et al.",
      "startOffset" : 26,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : ", 2016), visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019), and visual common sense (Zellers et al.",
      "startOffset" : 26,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : ", 2016), visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019), and visual common sense (Zellers et al.",
      "startOffset" : 26,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "The NLVR2 dataset (Suhr et al., 2019) involves yes-no question answering over image pairs.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "Neural Naturalist (Forbes et al., 2019) tests fine-grained captioning of bird pairs; (Jhamtani and Berg-Kirkpatrick, 2018) identifies the difference between two similar images.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : ", 2019) tests fine-grained captioning of bird pairs; (Jhamtani and Berg-Kirkpatrick, 2018) identifies the difference between two similar images.",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 39,
      "context" : "For example, (Tan et al., 2019) predicts image editing requests (generate ‘change the background to blue’ from a pair of images).",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Past work has also studied learning to perform image adjustments (like colorization and enhancement) from a language query (Chen et al., 2017; Wang et al., 2018).",
      "startOffset" : 123,
      "endOffset" : 161
    }, {
      "referenceID" : 40,
      "context" : "Past work has also studied learning to perform image adjustments (like colorization and enhancement) from a language query (Chen et al., 2017; Wang et al., 2018).",
      "startOffset" : 123,
      "endOffset" : 161
    }, {
      "referenceID" : 21,
      "context" : "2034 (Kiela et al., 2020) is a recent work challenging models to classify a meme as hateful or not.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "The following data sheet summarized relevant aspects of the data collection process (Bender and Friedman, 2018): A.",
      "startOffset" : 84,
      "endOffset" : 111
    } ],
    "year" : 2021,
    "abstractText" : "Understanding manipulated media, from automatically generated ‘deepfakes’ to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. In this paper, we study Edited Media Understanding Frames, a new conceptual formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, effects on individuals, and the overall implications of disinformation. We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time. At the same time, there is still much work to be done – and we provide analysis that highlights areas for further progress.",
    "creator" : "LaTeX with hyperref"
  }
}