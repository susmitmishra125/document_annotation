{
  "name" : "2021.acl-long.363.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction",
    "authors" : [ "Zhengbao Jiang", "Jialong Han", "Bunyamin Sisman", "Xin Luna Dong" ],
    "emails" : [ "zhengbaj@cs.cmu.edu", "jialongh@amazon.com", "bunyamis@amazon.com", "lunadong@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4706–4716\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4706"
    }, {
      "heading" : "1 Introduction",
      "text" : "With its large volume, the Web has been a major resource for knowledge extraction. Open information extraction (open IE; Sekine 2006; Banko et al. 2007) is a prominent approach that harvests subject-relation-object extractions in free text without assuming a predefined set of relations. One way to empower downstream applications like question answering is to integrate those free-text extractions into a knowledge graph (KG), e.g., Freebase. Relation integration is the first step to integrate those extractions, where their free-text relations (i.e., source relations) are normalized to relations in the target KG (i.e., target relations). Only after relation integration can entity linking proceed to resolve the\n∗This work was performed while at Amazon.\nfree-text subjects and objects to their canonical entities in the target KG. Local Approaches. Relation integration has been studied by the natural language processing (NLP) community. With exact matching in literal form between entity names in the source graph and target KG, previous methods obtain parallel data, i.e., common entity pairs, between the two graphs as in Fig. 1. Features of the entity pairs (e.g., MaliaBarack) in the source graph and their relations in the target KG (e.g., father) are used to train models to predict target relations for future extractions. A common challenge is the ambiguity of source relations, e.g., “parent” may correspond to father or mother in different contexts. Previous methods exploited contextual features including embeddings of seen entities (e.g., “Malia”; Riedel et al. 2013), middle relations between (e.g., “parent”; Riedel et al. 2013; Toutanova et al. 2015; Verga et al. 2017, 2016; Weston et al. 2013), and neighbor relations around the entity pair (e.g., “gender”; Zhang et al. 2019).\nAssuming rich contexts to address the ambiguity challenge, previous methods may fall short under the evolving and incomplete nature of the source\ngraph. For example, in the lower part of Fig. 1, emerging entities may come from new extractions with sparse contextual information. For the pair Nell-Marie, a conventional model learned on the parallel data may have neither seen entities nor neighborhood information (e.g., “gender”) to depend on, thus failing to disambiguate “parent” and wrongly predicting father. Due to the local nature of previous approaches, i.e., predictions for different entity pairs are made independently of each other, the model is unaware that “Nell” has two fathers in the final predictions. Such predictions are incoherent in common sense that a person is more likely to have one father and one mother, which is indicated by the graph structure around Malia in the target KG part of the parallel data."
    }, {
      "heading" : "1.1 Our Collective Approach",
      "text" : "To alleviate the incoherent prediction issue of local approaches, we propose Collective Relation Integration (CoRI) that exploits the dependency of predictions between adjacent entity pairs to enforce global coherence.\nSpecifically, we follow two stages, i.e., candidate generation and collective inference. In candidate generation, we simply use a local model to make independent predictions as candidates, e.g., father for all the three pairs in the lower part of Fig. 1. In collective inference, we employ a collective model that is aware of the common substructures of the target graph, e.g., Malia. The collective model makes predictions by not only taking as input all contextual features to the local model but also the candidate predictions of the current and all neighbor pairs. For the pair NellMarie, the collective model will have access to the candidate prediction father of Nell-Burton, which helps flip its final prediction to the correct mother. Tab. 1 summarizes CoRI and representative previous work from four aspects. To the best of our knowledge, CoRI is the first to collectively perform relation integration rather than locally.\nBeing responsible to make globally consistent\npredictions, the collective model needs to be trained to encode common structures of the target KG, e.g., Malia having only one father/mother in the parallel data of Fig. 1. To this end, we train the collective model in a stacked manner (Wolpert, 1992). We first train the first-stage local model on the parallel data, then train the second-stage collective model by conditioning on the candidate predictions of neighbor entity pairs from the first stage (e.g., father for Malia-Barrack) to make globally consistent predictions (e.g., mother for Malia-Michelle). Parallel Data Augmentation. The parallel data may be bounded by the low recall of exact name matching or the limited extractions generated by open IE systems. We observe that, even without counterpart extractions, the unmatched part of the target graph (as in Fig. 1) may also have rich common structures to guide the training of the collective model. To this end, we propose augmenting the parallel data by sampling subgraphs from the unmatched KG and creating pseudo parallel data by synthesizing their extractions, so the collective model can benefit from additional training data characterizing the desired global coherence.\nTo summarize, our contributions are three-fold: (1) We propose CoRI, a two-stage framework that improves state-of-the-art methods by making collective predictions with global coherence. (2) We propose using the unmatched target KG to augment the training data. (3) Experimental results on two datasets demonstrate the superiority of our approaches, improving AUC from .677 to .748 and from .716 to .780, respectively."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section, we first formulate the task of relation integration, then describe local methods by exemplifying with the state-of-the-art approach OpenKI (Zhang et al., 2019)."
    }, {
      "heading" : "2.1 Relation Integration",
      "text" : "We treat subject-relation-object extractions from open IE systems as a source graph K(E ,R) = {(s, r, o) | s, o ∈ E , r ∈ R}, where E denotes extracted textual entities, e.g., “Barack Obama”, and R denotes extracted source relations, e.g., “parent”. We denote by (s, o) a source entity pair. For (s, o), Ks,o = {r | (s, r, o) ∈ K} denotes all source relations between them. Similarly, Kr = {s, o | (s, r, o) ∈ K} denotes all entity pairs with relation\nr in between. We use the union KR = ⋃ r∈RKr to refer to all extracted entity pairs.\nDefinition 1 (Relation Integration). Given a source graph K and a target KG K′(E ′,R′) with target entities E ′ and target relations R′, the task of relation integration is to predict all applicable target relations for each extracted entity pair inKR:\nΓ ⊆ KR ×R′,\nwhere (s, r′, o) ∈ Γ is an integrated extraction indicating that a target relation r′ holds for (s, o).\nTo train relation integration models, all methods employ parallel data formalized as follow:\nDefinition 2 (Parallel Data). Parallel data are common entity pairs shared between KR and K′R′ and their ground truth target relations in K′: T = {〈(s, o),K′s,o〉 | (s, o) ∈ KR ∩ K′R′}. For example, 〈(Malia,Barack), {father}〉 is an instance of parallel data in Fig. 1.\nTo obtain parallel data, a widely used approach is to find entities shared by E and E ′ by exact name matching, then generate common entity pairs and their ground truth."
    }, {
      "heading" : "2.2 Local Approaches",
      "text" : "Previous local methods score potential integrated extractions by assuming their independence:\nP (Γ | K) = ∏\n(s,r′,o)∈KR×R′ Pθ(s, r\n′, o | K), (1)\nwhere θ is the parameters of the local model. One representative local model achieving state-of-theart performance is OpenKI (Zhang et al., 2019). It encodes the neighborhood of (s, o) in K by grouping and averaging embeddings of source relations in three parts. Let Ks,· be the set of source relations between s and neighbor entities other than o, and similarly for K·,o. OpenKI represents (s, o) by concatenating the three averaged embeddings into a local representation tl:\ntl = [A(Ks,o);A(Ks,·);A(K·,o)], (2)\nwhere l stands for local, andA(.) takes a set of relations and outputs the average of their embeddings. Then each integrated extraction is scored by:\nPθ(s, r ′, o | K) = σ(MLPl(tl))r′ , (3)\nwhere MLPl is a multi-layer perceptron and σ the sigmoid function. Given a parallel data T =\n{〈(s, o),K′s,o〉}, the loss function per training example trades between maximizing the probabilities of positive target relations and minimizing those of negative target relations: L ( (s, o),K′s,o ) = − ∑ r′∈K′s,o logPθ(s, r\n′, o | K) |K′s,o|\n+ γ ∑ r′∈R′\\K′s,o logPθ(s, r ′, o | K)\n|R′ \\ K′s,o| , (4)\nwhere γ is a hyperparameter to account for the imbanlance between positive and negative relations, because the latter often outnumber the former. The final loss is the sum of all examples."
    }, {
      "heading" : "3 Collective Relation Integration",
      "text" : "As discussed in § 1, the drawback of local methods is that predictions of different entity pairs are independently made. Neglecting their dependency may lead to predictions inconsistent with each other.\nTo address the issue, we propose a collective approach CoRI, which achieves collective relation integration via two stages: candidate generation and collective inference. In this section, we demonstrate the input and output of the two stages, as well as our current implementations."
    }, {
      "heading" : "3.1 Candidate Generation",
      "text" : "As mentioned in § 1.1, candidate generation’s responsibility is to provide candidate predictions to the collective inference stage. Formally, candidate predictions Γl (l means local) are generated by executing a local model on the source graph K:\nΓl = argmax Γ Pθ(Γ | K). (5)\nThe candidate predictions in Γl may be partially wrong, but the other correct ones can help adjust\nwrong predictions of their adjacent entity pairs in the collective inference stage, under the guidance of the collective model.\nFor example, in the upper part of Fig. 2, we have a source graph K with three entity pairs. The input to candidate generation is the entire K. After applying the local model (OpenKI in our case), we have three additional edges as the output Γl in the lower part of Fig. 2. Note that the candidate prediction father for Nell-Marie (denoted by black outline) is incorrect due to insufficient information in its neighborhood in K, i.e., both the relations in between of and around the entity pair (denoted by solid edges) are ambiguous “parent”s.\nFortunately, the entity pair Nell-Burton is relatively easy for the local model to predict as father because it can leverage the neighbor relation “father” between Billy-Burton. Such correct candidate predictions are included in Γl, provided to the collective inference stage as additional signals for later correction of the wrong predictions such as father for Nell-Marie."
    }, {
      "heading" : "3.2 Collective Inference",
      "text" : "Collective inference’s responsibility is to encode the structures of the target graph and use such information to refine the candidate predictions Γl by enforcing coherence among them. To this end, a collective model Pβ (with parameters β) takes both the source graph K and the candidate predictions Γl as input, and outputs the final predictions Γ:\nP (Γ | K) = Pβ(Γ | K,Γl). (6)\nIn the Nell-Marie case of Fig. 2, when making the final prediction, its own candidate predictions and those of the neighbor entity pairs (solid edges in Γl of the lower part in Fig. 2) are used to leverage the dependency among them. We concatenate the embeddings of candidate predictions to the local representation tl obtained in the first stage, and represent each entity pair as follow:\ntc = [tl;A(Γ l s,o);A(Γ l s,·);A(Γ l ·,o)], (7)\nwhere c means collective. Γls,o includes candidate target relations between s and o, and similarly for Γls,· and Γ l ·,o. Then we use another multi-layer perceptron MLPc to convert tc to probabilities\nPβ(s, r ′, o | K,Γl) = σ(MLPc(tc))r′ , (8)\nand minimize the loss function for Pβ similar to that of the local model Pθ in Eq. 4.\nAlgorithm 1: Training collective model. Result: Collective model β. T (1), . . . , T (T ) ← Split training data T into T folds; for fold i = 1, . . . , T do\nθ(i) ← train local model on data folds 1, . . . , i− 1, i+ 1, . . . , T ;\nΓli ← local predictions on T (i) using θ(i); end Γl ← ∪iΓli; β ← train collective model on T with inputK and Γl;"
    }, {
      "heading" : "3.3 Training Collective Model",
      "text" : "According to Eq. 6, we need Γl as features to train the collective model Pβ . This is to ensure that Pβ captures the dependencies among target relations. One may ask why we do not directly use ground truth K′ instead of predictions Γl. At test time, we can only use target relations predicted by Pθ as input to Pβ because the ground truth target relations of neighbor entity pairs might not be available. If we train Pβ using the ground truth, there will be a discrepancy between training and testing, potentially hurting the performance.\nSpecifically, we split the training set T into T folds. We generate Γl by rotating and unioning a temporary local model’s predictions on a heldout fold, where the temporary model is trained on the other folds. Then we train Pβ on the parallel data T with Γl. In this manner, we can use the full dataset to optimize the collective model while avoiding generating candidates on the training data of the local model, which leads to overfitting. The detailed training procedure is given in Alg. 1."
    }, {
      "heading" : "4 Data Augmentation w/ Unmatched KG",
      "text" : "As in Def. 2, the volume of parallel data is limited by the number of shared entity pairs KR ∩ K′R′ of the two graphs. In Fig. 1, the unmatched part of the target KG, containing entity pairs without extraction counterparts (i.e., K′R′ \\ KR) and their target relations, can also indicate common substructures of the target KG, and guide the training of the collective model. To this end, we propose leveraging unmatched KG to generate pseudo parallel data to augment the limited training data. Synthesizing Pseudo Extractions. To leverage the unmatched KG, we need to synthesize pseudo extractions for the target entities and relations to add to K as features. Since we do not use entityspecific parameters, we only synthesize source relations like “parent”, and keep the target entities\nunchanged, as illustrated in Fig. 3. Specifically, for each subject-relation-object tuple (s′, r′, o′) in the unmatched KG, we keep s′ and o′ unchanged, and synthesize source relations r by sampling from:\nP (r | r′) = |Kr ∩ K′r′ | |K′r′ | , (9)\ni.e., the conditional probability of observing r given r′ based on co-occurrences in the parallel data. |Kr ∩K′r′ | is the number of entity pairs with both r and r′ in between, and |K′r′ | is the number of entity pairs with r′ in between. In this way, we obtain a pseudo extraction (s, r, o), as detailed in Alg. 2 Pseudo Data Selection. We regard all pseudo extractions as a graphKp. Similar to Def. 2, we define pseudo parallel data as below.\nDefinition 3 (Pseudo Parallel Data). Pseudo parallel data T p includes common entity pairs between pseudo extractions Kp and the target KG K′, associated with their ground truth target relations, i.e., T p = {〈(s, o),K′s,o〉} | (s, o) ∈ K p R ∩ K′R′}.\nTo make use of pseudo parallel data T p, the most straightforward way is to use them together with parallel data T to train the collective model Pβ . However, not all substructures in the target graph K′ are useful for Pβ . For example, when K′ has other domains irrelevant to the source extraction graph, substructures in those domains may distract Pβ from concentrating on the domains of the source graph. To mitigate this issue, we only use a subset of T p similar to T , as shown by the black-outlined parts in Fig. 3. Specifically, we represent each entity pair (s, o) as a virtual document with surrounding target relations K′s,o ∪K′s,· ∪K′·,o\nAlgorithm 2: Our augmentation approach. Result: Collective model β with data augmentation. (1) Synthesizing Pseudo Extractions Kp Kp ← ∅; T p ← ∅; for (s′, r′, o′) ∈ K′, where (s′, o′) ∈ K′R′ \\ KR do\ns← s′ and o← o′; Sample r ∼ P (r|r′); Kp ← Kp ∪ {(s, r, o)};\nend (2) Pseudo Data Selection for entity pair ∈ KR ∩ K′R′ do\nS ← its top K similar entity pairs in KpR ∩ K ′ R′ ;\nT p ← T p ∪ {〈(s, o),K′s,o〉 | (s, o) ∈ S}; end β ← Train on T ∪ T p with Alg. 1;\nas “tokens”. For each entity pair from the parallel data T , we use BM25 (Robertson and Zaragoza, 2009) to retrieve its top K most similar entity pairs from T p, and add them to the selected pseudo parallel data T p for training, as detailed in Alg. 2."
    }, {
      "heading" : "5 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Evaluation",
      "text" : "We use the ReVerb dataset (Fader et al., 2011) as the source graph, and Freebase1 and Wikidata2 as the target KGs, respectively. We follow the same name matching approach in Zhang et al. (2019) to obtain parallel data. To simulate real scenarios where models are trained on limited labeled data but applied to a large testing set, we use 20% of entity pairs in the parallel data for training and the other 80% for testing, and there is no overlap. We also compare the performance under other ratios in § 6.3. Dataset statistics are listed in Tab. 2.\nWe evaluate by ranking all integrated extractions based on their probabilities, and report area under the curve (AUC). Considering real scenarios where we want to integrate as many extractions as possible while keeping a high precision, we also report Recall and F1 when precision is 0.8, 0.9, or 0.95.\n1https://developers.google.com/ freebase\n2https://www.wikidata.org"
    }, {
      "heading" : "5.2 Compared Methods",
      "text" : "We compare the following methods in experiments. Relation Translation is a simple method that maps source relations to target relations with conditional probability P (r′ | r) similar to Eq. 9. For an entity pair (s, o), the predicted target relations are {arg maxr′ P (r′|r) | r ∈ Ks,o}. Universal Schema (E-model) (Riedel et al., 2013) learns entity and relation embeddings through matrix factorization, which cannot generalize to unseen entities. It is a local model that scores each integrated extraction independently. Rowless Universal Schema (Verga et al., 2017) is a local model which improves over the E-model by eliminating entity-specific parameters, thus generalizing to unseen entities. OpenKI (Zhang et al., 2019) is a local model that addresses the ambiguity of source relations by using neighbor relations for more context. CoRI is our collective two-stage relation integration model trained with Alg. 1. CoRI + DA is our model where the training data is augmented by pseudo parallel data with Alg. 2. To verify the necessity of retrieval-based pseudo data selection, we also compare with a random DA baseline where we select K random entity pairs. CoRI + KGE is another approach to exploit the unmatched KG with KG embeddings (KGE) trained on the entire target KG in an unsupervised manner. We initialize the embeddings of target relations averaged by A(.) in Eq. 7 with TransE (Bordes et al., 2013) embeddings trained on the target graph."
    }, {
      "heading" : "5.3 Implementation Details",
      "text" : "We uniformly use 32-dimension embeddings for all relations, and AdamW (Loshchilov and Hutter, 2019) optimizer with learning rate 0.01 and epsilon 10-8. The ratio γ in Eq. 4 is set to 10. We sample at most 30 neighbor source relations to handle entity pairs with too many neighbor relations. We use T = 5 folds in Alg. 1 to train our collective model. We retrieve top K = 5 entity pairs in pseudo data selection, adding about 20K and 12K entity pairs to the two datasets in Tab. 2, respectively. We use BM25 (Robertson and Zaragoza, 2009) implementation in ElasticSearch3 in pseudo data selection. We use the KGE released by OpenKE.4 Our model is trained with 32 CPU cores and a single 2080Ti GPU, and it takes 1-2 hours to converge.\n3https://www.elastic.co/ 4https://github.com/thunlp/OpenKE"
    }, {
      "heading" : "6 Experimental Results",
      "text" : "We aim to answer the following questions: (1) Is CoRI superior to local models? (2) Is CoRI robust w.r.t. varying size of training and testing data? (3) Is unmatched KG useful for CoRI? Is our parallel data augmentation approach the best choice?"
    }, {
      "heading" : "6.1 Main Results",
      "text" : "In Tab. 3, we show results comparing all methods on both datasets. Our observations are as follows. Collective inference is beneficial. Among the baselines, OpenKI generally performs best because it leverages neighbor relations besides middle relations between entity pairs, without relying on entity parameters. Even without data augmentation, CoRI outperforms OpenKI by a large margin, improving AUC from .677 to .708 and from .716 to .746 on the two datasets, respectively, which demonstrates the effectiveness of collective inference. Data augmentation further improves the performance. By comparing CoRI with CoRI + DA (retrieval), we observe that data augmentation further improves AUC from .708 to .748 and from .746 to .780, respectively, which indicates that using unmatched KG can effectively augment the training of the collective model. We plot the precision-recall curves of the best three approaches in Fig. 4. It demonstrates the superiority of our methods across the whole spectrum. Generalization on unseen entities is necessary. Among the baselines, the E-model uses entityspecific parameters, hindering it from generalizing to unseen entities and making it less competitive."
    }, {
      "heading" : "6.2 Effectiveness of Pseudo Data Selection",
      "text" : "As shown in Tab. 3, both KGE, random, and retrieval-based data augmentation approaches perform better than CoRI (without DA), indicating the effectiveness of using the unmatched KG. Our retrieval-based DA outperforms the random coun-\nterpart, which confirms the superiority of similaritybased data augmentation in choosing substructures that cover domains relevant to the original parallel data. Our DA approach outperforms KGE, demonstrating the necessity of selectively using the unused KG to avoid discrepancies with the parallel data. Different Numbers of Pseudo Data Entity Pairs. In Fig. 5, we compare the performance of DA w.r.t. different numbers of retrieved entity pairs K. We observe that K=5 yields better performance than K=1. However, further increasing K hurts the performance, which is probably due to pseudo entity pairs with lower similarity to the parallel data causing a domain shift. This validates the necessity of selectively using pseudo parallel data."
    }, {
      "heading" : "6.3 Impacts of Data Size on CoRI",
      "text" : "Due to its collective nature, one may wonder about CoRI’s performance w.r.t. other training and testing data sizes. We analyze these factors in this section. Our observations are similar on both datasets, so we only report the results on ReVerb + Freebase. Varying Size of Training Data. In Fig. 6a, we compare CoRI (without DA) with OpenKI by varying the portion of the parallel data for training from 20% (used in our main results in Tab. 3) to 80%. We observe that using more training data improves the performance, as shown by the increasing trends w.r.t. all metrics. Our method outperforms OpenKI in all settings, demonstrating that our method is effective in both high- and low-resource settings. Varying % of Accessible Neighbor Entity Pairs. Our collective framework is special in its collective inference stage, where the collective model refines the candidate prediction of an entity pair by considering its neighbor entity pairs’ candidates. We\nhypothesize that the more neighbor entity pairs the collective model has access to, the better performance it should achieve. For example, if we use a portion of 50%, candidate predictions for only half of the neighbor entity pairs rather than the entire Γl will be used in Eq. 7. We vary the portion from 25% to 100% (used in our main experiments in Tab. 3). As shown in Fig. 6b, even accessing 25% can make CoRI outperform OpenKI. As the percentage increases, CoRI continues to improve, while OpenKI remains the same because it is local, i.e., not using candidate predictions."
    }, {
      "heading" : "6.4 Case Study",
      "text" : "In Fig. 7, we show two cases from ReVerb + Freebase where CoRI corrects the mistakes of OpenKI in the collective inference stage. In the first case, the source relation “is in” between “Iowa” and “Mahaska County” is extracted but in the wrong direction. OpenKI just straightforwardly predicts containedby based on the surface form, but fails to leverage the neighbor relations to infer that Iowa is a larger geographical area. With the collective model, CoRI is able to use the other two candidate predictions of containedby to flip the wrong prediction to contains.\nIn the second case, a prediction is needed between “Bily Joel” and “Columbia”. Here the source relation “was in” and the object entity “Columbia” are both ambiguous, which can refer to geographical containment with a place or membership to a company. OpenKI makes no prediction due to the ambiguity, while CoRI makes the right prediction music label by collectively working on the other entity pairs, where all predictions coherently indicate that “Columbia” is a music company.\n0.3 0.4 0.5 0.6 0.7 0.8\nAUC R@0.8 F@0.8 R@0.9 F@0.9 R@0.95 F@0.95 20% 40% 60% 80%Portion of training split:\n(a) Varying size of training data.\n0.3 0.4 0.5 0.6 0.7 0.8\nAUC R@0.8 F@0.8 R@0.9 F@0.9 R@0.95 F@0.95 25% 50% 75% 100%Accessible neighbor pairs: (b) Varying % of accessible neighbor pairs\nFigure 6: CoRI (bars without filling) vs. OpenKI (solid bars) on ReVerb + Freebase. CoRI consistently outperforms OpenKI by a large margin. Larger improvements are achieved when candidates of more neighbor entity pairs are accessed."
    }, {
      "heading" : "7 Related Work",
      "text" : "Relation integration has been studied by both the database (DB) and the NLP communities. The DB community formulates it as schema matching that aligns the schemas of two tables, e.g., matching columns of an is in table to those of another subarea of table (Rahm and Bernstein, 2001; Cafarella et al., 2008; Kimmig et al., 2017). Such table-level alignment is valid since all rows in an is in table should have the same semantics, i.e., being geographical containment or not. However, in open IE, predictions should be made at the entity pair level because of the ambiguous nature of source relations. Putting all extracted “is in” entity pairs into one table to conduct schema matching is problematic from the first step since the entity pairs may have different ground truths.\nThe NLP community, on the other hand, investigates the problem at the entity pair level. Besides manually designed rules (Soderland et al., 2013), most works leverage the link structure between entities and relations. Universal schema (Riedel et al., 2013) learns embeddings of entities and middle relations between entity pairs through decomposing their co-occurrence matrix. However, the entity embeddings make it not generalize to unseen entities. Other methods (Toutanova et al., 2015; Verga et al., 2016, 2017; Gupta et al., 2019) also exploit\nmiddle relations, but eliminate entity parameters. Zhang et al. (2019) moves one step further by explicitly considering neighbor relations, leveraging more context from the local link structure. Some works (Weston et al., 2013; Angeli et al., 2015) directly minimize the distance between embeddings of relations sharing the same entity pairs. Yu et al. (2017) further leverage compositional representations of entity names instead of using free parameters to deal with unseen entities at test time.\nThere are also works on Open IE canonicalization that cluster source relations. Some use entity pairs as clustering signals (Yates and Etzioni, 2009; Nakashole et al., 2012; Galárraga et al., 2014), while others use lexical features or side information (Min et al., 2012; Vashishth et al., 2018). However, the clusters are not finally aligned to relations in target KGs, different from our problem.\nThe two-stage collective inference framework has been explored in other problems like entity linking (Cucerzan, 2007; Guo et al., 2013; Shen et al., 2012), where candidate entities are generated for each mention independently, and collectively ranked based on their compatibility in the second stage. In machine translation, an effective approach to leverage monolingual corpus in the target language is to back-translate it to the source language to augment the limited parallel corpus (Sennrich et al., 2016). The above works inspired us to use collective inference for relation integration and leverage the unmatched KG for data augmentation. Another approach to perform collective inference is to solve learning problem with constraints, such as integer linear programming (Roth and Yih, 2004), posterior regularization (Ganchev et al., 2010), and conditional random fields (Lafferty et al., 2001). Comparing to our approach, these methods usually involve heavy computation, or are hard to optimize. Examining the perfor-\nmance of these methods is an interesting future direction. Besides, we also adopted ideas of selecting samples from out-domain data similar to in-domain samples (Xu et al., 2020; Du et al., 2020) to select our pseudo parallel data."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we proposed CoRI, a collective inference approach to relation integration. To the best of our knowledge, this is the first work exploring this idea. We devised a two-stage framework, where the candidate generation stage employs existing local models to make candidate predictions, and the collective inference stage refines the candidate predictions by enforcing global coherence. Observing that the target KG is rich in substructures indicating the desired global coherence, we further proposed exploiting the unmatched KG by selectively synthesizing pseudo parallel data to augment the training of our collective model. Our solution significantly outperforms all baselines on two datasets, indicating the effectiveness of our approaches."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Prashant Shiralkar, Hao Wei, Colin Lockard, Binxuan Huang, and all the reviewers for their insightful comments and suggestions."
    } ],
    "references" : [ {
      "title" : "Leveraging linguistic structure for open domain information extraction",
      "author" : [ "Gabor Angeli", "Melvin Jose Johnson Premkumar", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Angeli et al\\.,? 2015",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2015
    }, {
      "title" : "Open information extraction from the web",
      "author" : [ "Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni." ],
      "venue" : "IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, In-",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcı́aDurán", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Webtables: exploring the power of tables on the web",
      "author" : [ "Michael J. Cafarella", "Alon Y. Halevy", "Daisy Zhe Wang", "Eugene Wu", "Yang Zhang." ],
      "venue" : "Proc. VLDB Endow., 1(1):538–549.",
      "citeRegEx" : "Cafarella et al\\.,? 2008",
      "shortCiteRegEx" : "Cafarella et al\\.",
      "year" : 2008
    }, {
      "title" : "Large-scale named entity disambiguation based on wikipedia data",
      "author" : [ "Silviu Cucerzan." ],
      "venue" : "Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL),",
      "citeRegEx" : "Cucerzan.,? 2007",
      "shortCiteRegEx" : "Cucerzan.",
      "year" : 2007
    }, {
      "title" : "Self-training improves pre-training for natural language understanding",
      "author" : [ "Jingfei Du", "Edouard Grave", "Beliz Gunel", "Vishrav Chaudhary", "Onur Celebi", "Michael Auli", "Ves Stoyanov", "Alexis Conneau." ],
      "venue" : "CoRR, abs/2010.02194.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Identifying relations for open information extraction",
      "author" : [ "Anthony Fader", "Stephen Soderland", "Oren Etzioni." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Con-",
      "citeRegEx" : "Fader et al\\.,? 2011",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2011
    }, {
      "title" : "Canonicalizing open knowledge bases",
      "author" : [ "Luis Galárraga", "Geremy Heitz", "Kevin Murphy", "Fabian M. Suchanek." ],
      "venue" : "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014,",
      "citeRegEx" : "Galárraga et al\\.,? 2014",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2014
    }, {
      "title" : "Posterior regularization for structured latent variable models",
      "author" : [ "Kuzman Ganchev", "João Graça", "Jennifer Gillenwater", "Ben Taskar." ],
      "venue" : "J. Mach. Learn. Res., 11:2001–2049.",
      "citeRegEx" : "Ganchev et al\\.,? 2010",
      "shortCiteRegEx" : "Ganchev et al\\.",
      "year" : 2010
    }, {
      "title" : "To link or not to link? a study on end-toend tweet entity linking",
      "author" : [ "Stephen Guo", "Ming-Wei Chang", "Emre Kiciman." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Guo et al\\.,? 2013",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2013
    }, {
      "title" : "Care: Open knowledge graph embeddings",
      "author" : [ "Swapnil Gupta", "Sreyash Kenkre", "Partha Talukdar." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "A collective, probabilistic approach to schema mapping",
      "author" : [ "Angelika Kimmig", "Alex Memory", "Renée J. Miller", "Lise Getoor." ],
      "venue" : "33rd IEEE International Conference on Data Engineering, ICDE 2017, San Diego, CA, USA, April 19-22, 2017, pages 921–",
      "citeRegEx" : "Kimmig et al\\.,? 2017",
      "shortCiteRegEx" : "Kimmig et al\\.",
      "year" : 2017
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Ensemble semantics for large-scale unsupervised relation extraction",
      "author" : [ "Bonan Min", "Shuming Shi", "Ralph Grishman", "ChinYew Lin." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
      "citeRegEx" : "Min et al\\.,? 2012",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2012
    }, {
      "title" : "PATTY: A taxonomy of relational patterns with semantic types",
      "author" : [ "Ndapandula Nakashole", "Gerhard Weikum", "Fabian M. Suchanek." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Nakashole et al\\.,? 2012",
      "shortCiteRegEx" : "Nakashole et al\\.",
      "year" : 2012
    }, {
      "title" : "A survey of approaches to automatic schema matching",
      "author" : [ "Erhard Rahm", "Philip A. Bernstein." ],
      "venue" : "VLDB J., 10(4):334–350.",
      "citeRegEx" : "Rahm and Bernstein.,? 2001",
      "shortCiteRegEx" : "Rahm and Bernstein.",
      "year" : 2001
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin." ],
      "venue" : "Human Language Technologies: Conference of the North American Chapter of the Association of",
      "citeRegEx" : "Riedel et al\\.,? 2013",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen E. Robertson", "Hugo Zaragoza." ],
      "venue" : "Found. Trends Inf. Retr., 3(4):333–389.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "A linear programming formulation for global inference in natural language tasks",
      "author" : [ "Dan Roth", "Wen-tau Yih." ],
      "venue" : "Proceedings of the Eighth Conference on Computational Natural Language Learning, CoNLL 2004, Held in cooperation with HLT-NAACL",
      "citeRegEx" : "Roth and Yih.,? 2004",
      "shortCiteRegEx" : "Roth and Yih.",
      "year" : 2004
    }, {
      "title" : "On-demand information extraction",
      "author" : [ "Satoshi Sekine." ],
      "venue" : "ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia,",
      "citeRegEx" : "Sekine.,? 2006",
      "shortCiteRegEx" : "Sekine.",
      "year" : 2006
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Linden: linking named entities with knowledge base via semantic knowledge",
      "author" : [ "Wei Shen", "Jianyong Wang", "Ping Luo", "Min Wang." ],
      "venue" : "Proceedings of the 21st international conference on World Wide Web, pages 449–458.",
      "citeRegEx" : "Shen et al\\.,? 2012",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2012
    }, {
      "title" : "Open information extraction to KBP relations in 3 hours",
      "author" : [ "Stephen Soderland", "John Gilmer", "Robert Bart", "Oren Etzioni", "Daniel S. Weld." ],
      "venue" : "Proceedings of the Sixth Text Analysis Conference, TAC 2013, Gaithersburg, Maryland, USA, November 18-",
      "citeRegEx" : "Soderland et al\\.,? 2013",
      "shortCiteRegEx" : "Soderland et al\\.",
      "year" : 2013
    }, {
      "title" : "Representing text for joint embedding of text and knowledge bases",
      "author" : [ "Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Toutanova et al\\.,? 2015",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2015
    }, {
      "title" : "CESI: canonicalizing open knowledge bases using embeddings and side information",
      "author" : [ "Shikhar Vashishth", "Prince Jain", "Partha P. Talukdar." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France, April",
      "citeRegEx" : "Vashishth et al\\.,? 2018",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual relation extraction using compositional universal schema",
      "author" : [ "Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the",
      "citeRegEx" : "Verga et al\\.,? 2016",
      "shortCiteRegEx" : "Verga et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalizing to unseen entities and entity pairs with row-less universal schema",
      "author" : [ "Patrick Verga", "Arvind Neelakantan", "Andrew McCallum." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Verga et al\\.,? 2017",
      "shortCiteRegEx" : "Verga et al\\.",
      "year" : 2017
    }, {
      "title" : "Connecting language and knowledge bases with embedding models for relation extraction",
      "author" : [ "Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Weston et al\\.,? 2013",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2013
    }, {
      "title" : "Stacked generalization",
      "author" : [ "David H. Wolpert." ],
      "venue" : "Neural Networks, 5(2):241–259.",
      "citeRegEx" : "Wolpert.,? 1992",
      "shortCiteRegEx" : "Wolpert.",
      "year" : 1992
    }, {
      "title" : "Incorporating external knowledge through pre-training for natural language to code generation",
      "author" : [ "Frank F. Xu", "Zhengbao Jiang", "Pengcheng Yin", "Bogdan Vasilescu", "Graham Neubig." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised methods for determining object and relation synonyms on the web",
      "author" : [ "Alexander Yates", "Oren Etzioni." ],
      "venue" : "J. Artif. Intell. Res., 34:255– 296.",
      "citeRegEx" : "Yates and Etzioni.,? 2009",
      "shortCiteRegEx" : "Yates and Etzioni.",
      "year" : 2009
    }, {
      "title" : "Open relation extraction and grounding",
      "author" : [ "Dian Yu", "Lifu Huang", "Heng Ji." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1:",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Openki: Integrating open information extraction and knowledge bases with relation inference",
      "author" : [ "Dongxu Zhang", "Subhabrata Mukherjee", "Colin Lockard", "Xin Luna Dong", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Open information extraction (open IE; Sekine 2006; Banko et al. 2007) is a prominent approach that harvests subject-relation-object extractions in free text without assuming a predefined set of relations.",
      "startOffset" : 28,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "Previous methods exploited contextual features including embeddings of seen entities (e.g., “Malia”; Riedel et al. 2013), middle relations between (e.",
      "startOffset" : 85,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "2013), middle relations between (e.g., “parent”; Riedel et al. 2013; Toutanova et al. 2015; Verga et al. 2017, 2016; Weston et al. 2013), and neighbor relations around the entity pair (e.",
      "startOffset" : 32,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "2013), middle relations between (e.g., “parent”; Riedel et al. 2013; Toutanova et al. 2015; Verga et al. 2017, 2016; Weston et al. 2013), and neighbor relations around the entity pair (e.",
      "startOffset" : 32,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "2013), middle relations between (e.g., “parent”; Riedel et al. 2013; Toutanova et al. 2015; Verga et al. 2017, 2016; Weston et al. 2013), and neighbor relations around the entity pair (e.",
      "startOffset" : 32,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "2013), and neighbor relations around the entity pair (e.g., “gender”; Zhang et al. 2019).",
      "startOffset" : 53,
      "endOffset" : 88
    }, {
      "referenceID" : 33,
      "context" : ", 2017) X X (Zhang et al., 2019) X X X CoRI (ours) X X X X",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "To this end, we train the collective model in a stacked manner (Wolpert, 1992).",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "In this section, we first formulate the task of relation integration, then describe local methods by exemplifying with the state-of-the-art approach OpenKI (Zhang et al., 2019).",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 33,
      "context" : "One representative local model achieving state-of-theart performance is OpenKI (Zhang et al., 2019).",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "For each entity pair from the parallel data T , we use BM25 (Robertson and Zaragoza, 2009) to retrieve its top K most similar entity pairs from T p, and add them to the selected pseudo parallel data T p for training, as detailed in Alg.",
      "startOffset" : 60,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "We use the ReVerb dataset (Fader et al., 2011) as the source graph, and Freebase1 and Wikidata2 as the target KGs, respectively.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "Universal Schema (E-model) (Riedel et al., 2013) learns entity and relation embeddings through ma-",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "Rowless Universal Schema (Verga et al., 2017) is a local model which improves over the E-model by eliminating entity-specific parameters, thus generalizing to unseen entities.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "OpenKI (Zhang et al., 2019) is a local model that addresses the ambiguity of source relations by using neighbor relations for more context.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "7 with TransE (Bordes et al., 2013) embeddings trained on the target graph.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "We uniformly use 32-dimension embeddings for all relations, and AdamW (Loshchilov and Hutter, 2019) optimizer with learning rate 0.",
      "startOffset" : 70,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "We use BM25 (Robertson and Zaragoza, 2009) implementation in ElasticSearch3 in pseudo data selection.",
      "startOffset" : 12,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : ", matching columns of an is in table to those of another subarea of table (Rahm and Bernstein, 2001; Cafarella et al., 2008; Kimmig et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : ", matching columns of an is in table to those of another subarea of table (Rahm and Bernstein, 2001; Cafarella et al., 2008; Kimmig et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : ", matching columns of an is in table to those of another subarea of table (Rahm and Bernstein, 2001; Cafarella et al., 2008; Kimmig et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "Besides manually designed rules (Soderland et al., 2013), most works leverage the link structure between entities and relations.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Universal schema (Riedel et al., 2013) learns embeddings of entities and middle relations between entity pairs through decomposing their co-occurrence matrix.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "Other methods (Toutanova et al., 2015; Verga et al., 2016, 2017; Gupta et al., 2019) also exploit middle relations, but eliminate entity parameters.",
      "startOffset" : 14,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Other methods (Toutanova et al., 2015; Verga et al., 2016, 2017; Gupta et al., 2019) also exploit middle relations, but eliminate entity parameters.",
      "startOffset" : 14,
      "endOffset" : 84
    }, {
      "referenceID" : 28,
      "context" : "Some works (Weston et al., 2013; Angeli et al., 2015) directly minimize the distance between embeddings of relations sharing the same entity pairs.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Some works (Weston et al., 2013; Angeli et al., 2015) directly minimize the distance between embeddings of relations sharing the same entity pairs.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : "Some use entity pairs as clustering signals (Yates and Etzioni, 2009; Nakashole et al., 2012; Galárraga et al., 2014), while others use lexical features or side information (Min et al.",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Some use entity pairs as clustering signals (Yates and Etzioni, 2009; Nakashole et al., 2012; Galárraga et al., 2014), while others use lexical features or side information (Min et al.",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "Some use entity pairs as clustering signals (Yates and Etzioni, 2009; Nakashole et al., 2012; Galárraga et al., 2014), while others use lexical features or side information (Min et al.",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : ", 2014), while others use lexical features or side information (Min et al., 2012; Vashishth et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 25,
      "context" : ", 2014), while others use lexical features or side information (Min et al., 2012; Vashishth et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "The two-stage collective inference framework has been explored in other problems like entity linking (Cucerzan, 2007; Guo et al., 2013; Shen et al., 2012), where candidate entities are generated for each mention independently, and collectively ranked based on their compatibility in the second stage.",
      "startOffset" : 101,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "The two-stage collective inference framework has been explored in other problems like entity linking (Cucerzan, 2007; Guo et al., 2013; Shen et al., 2012), where candidate entities are generated for each mention independently, and collectively ranked based on their compatibility in the second stage.",
      "startOffset" : 101,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "The two-stage collective inference framework has been explored in other problems like entity linking (Cucerzan, 2007; Guo et al., 2013; Shen et al., 2012), where candidate entities are generated for each mention independently, and collectively ranked based on their compatibility in the second stage.",
      "startOffset" : 101,
      "endOffset" : 154
    }, {
      "referenceID" : 21,
      "context" : "In machine translation, an effective approach to leverage monolingual corpus in the target language is to back-translate it to the source language to augment the limited parallel corpus (Sennrich et al., 2016).",
      "startOffset" : 186,
      "endOffset" : 209
    }, {
      "referenceID" : 19,
      "context" : "Another approach to perform collective inference is to solve learning problem with constraints, such as integer linear programming (Roth and Yih, 2004), posterior regularization (Ganchev et al.",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Another approach to perform collective inference is to solve learning problem with constraints, such as integer linear programming (Roth and Yih, 2004), posterior regularization (Ganchev et al., 2010), and conditional random fields (Lafferty et al.",
      "startOffset" : 178,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : ", 2010), and conditional random fields (Lafferty et al., 2001).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 30,
      "context" : "Besides, we also adopted ideas of selecting samples from out-domain data similar to in-domain samples (Xu et al., 2020; Du et al., 2020) to select our pseudo parallel data.",
      "startOffset" : 102,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Besides, we also adopted ideas of selecting samples from out-domain data similar to in-domain samples (Xu et al., 2020; Du et al., 2020) to select our pseudo parallel data.",
      "startOffset" : 102,
      "endOffset" : 136
    } ],
    "year" : 2021,
    "abstractText" : "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.",
    "creator" : "LaTeX with hyperref"
  }
}