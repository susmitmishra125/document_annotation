{
  "name" : "2021.acl-long.235.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3013–3025\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3013\nDeep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment."
    }, {
      "heading" : "1 Introduction",
      "text" : "Readability is the ease with which a reader can understand a written text1. Predicting readability has been widely applied in education (Lennon and Burdick, 2004), book publishing (Pera and Ng, 2014), marketing (Chebat et al., 2003), newspaper readership (Pitler and Nenkova, 2008), and health information communication (Bernstam et al., 2005). Ever since the first study by Lively and Pressey in 1923, many researchers have developed various popular readability formulas including Flesch (Flesch, 1948), Fog (Gunning, 1969) and Lexile (Stenner et al., 1988). These traditional readability formulas are favored by domain applications due to their simplicity even though the formulas are mostly based on shallow features and known to lack accuracy (Bruce et al., 1981; Davison and Kantor, 1982; Graesser et al., 2004).\n* Corresponding authors.\nIts strong reliance on expert knowledge is also a burden to adapt it to a new domain. Machine learning approaches, which incorporate a broader set of morphological, lexical, syntactic, and discourse features, have shown to achieve better accuracy in readability assessment (Si and Callan, 2001; Collins-Thompson and Callan, 2005). Figure 1 (a) describes a generic machine-learning framework for Automatic Readability Assessment (ARA) where manual feature engineering is an important step to extract important linguistic features for building readability classification models. To bypass the necessity of heavy feature engineering, deep learning strategies have been studied to automatically detect patterns or extract features related to readability (Azpiazu and Pera, 2019; Martinc et al., 2019; Mohammadi and Khasteh, 2019). Figure 1 (b) provides a generic neural network structure of deep learning approach to ARA. While neural network models take word embedding as input, they in general discard linguistic features traditionally used in machine learning models (Deutsch et al., 2020). If ever incorporated, linguistic features such as POS and morphological tags are only used to guide attention mechanism for embedding representation of the text (Azpiazu and Pera, 2019). Pre-trained models such as BERT (Devlin et al., 2019) learn dense representations of text by informing the models with semantically neighboring words, sentences, or context. Despite the attempts of recent research to assess BERT’s ability to implicitly capture the structural properties of language (Goldberg, 2019; Jawahar et al., 2019; Kovaleva et al., 2019), it has been observed that BERT “tends to rely more on semantic than structural differences during the\n1 https://en.wikipedia.org/wiki/Readability\nclassification phase and therefore performs better on problems with distinct semantic differences between classes” (Martinc et al., 2019). There is clearly a lack of explicit consideration of syntactic (and structural) features in the current BERT-based models for ARA, which is known to be crucial. In this study, we address the problem of augmenting the ability of BERT with widely used linguistic features in ARA.\nTo best integrate with BERT, we create syntactic dense embedding as shown in Figure 1(d). An important problem we consider in this paper is the possible relationships between different features. Linguistic features defined by linguistic experts may often be related. Table 1 shows three pairs of linguistic features for Chinese readability assessment. In example one, the “percentage of conjunctions” and the “average height of parse tree” may be positively correlated because both reflect the complexity of the sentences. In the second example, “percentage of unique functional words”\nin a document is negatively correlated with the “average number of characters per word” for that document because Chinese functional words are usually short (i.e., one or two characters). Utilizing all these linguistic features as if they were independent may potentially hinder the classifier. We propose to consider the possible relationships among linguistic features when creating their dense embeddings with which we could complement the BERT embedding representations.\nIn this paper, we represent pairwise correlations between features as triplets with linguistic features as nodes and their correlations as edges. Positive correlation implies that two features behave similarly in influencing the readability level of the text and should be represented with similar embeddings. The set of triplets forms a graph (as illustrated in Figure 1(d)). We then learn the dense representations of linguistic features with graphbased models. By encoding the similarity knowledge with dense embeddings, the ARA\nclassifier models will be better informed and gain predictive strength. Our experiments on six datasets will confirm the effectiveness of this approach.\nWe contribute to the research on Automatic Readability Assessment in the following directions: (1) We provide three new data sets of linguistic features for document-level readability assessment of Chinese L1, Chinese L2 and English L2 learning. (2) We verify that the correlation relationships among linguistic features could be utilized to learn syntactic dense embeddings. (3) We propose a Dual-channel neural network model (i.e., DualModel) to combine the syntactic dense embeddings and the BERT semantic dense embeddings for readability predictions. (4) We verify, with six data sets of Chinese and English corpora for L1 and L2 language proficiencies, that the Dual-Model can significantly improve the predictive performances of the BERT-only model. We provide our data and codes at: https://github.com/luv2Lab/ linguistic-feature-embedding."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Automatic Readability Assessment",
      "text" : "Corpora for readability assessment are available for many languages. Among some of the most cited of English readability assessment are the WeeBit corpus by Vajjala and Meurers (2012, 2014) for English L1 learning and the Cambridge exam corpus by Xia et al. (2016) for English L2. For Chinese readability assessment, Sung et al. (2015) evaluated 30 linguistic features and classification models with text books in traditional Chinese. Qiu et al. (2017), Lu et al. (2019), and Zhu et al. (2019) designed features of different categories for machine learning methods for Chinese L1 and L2 readability assessment at document and sentence levels. Similar works on other languages include French (Todirascu et al., 2016), German (Hancke et al., 2012), Swedish (Pilán et al., 2016), and Japanese (Wang and Andersen, 2016). Azpiazu and Pera (2020) analyzed the most common linguistic features for six languages and evaluated multiple classifiers for cross-lingual readability assessment. Most of the current work on applying graphbased methods or neural networks to readability assessment operate with word-level semantic embeddings. For example, Jiang et al. (2018) incorporated word-level difficulty from lexical knowledge sources into knowledge graph and\ntrained enriched word embedding representations. Martinc et al. (2019) applied three types of neural language models at word level for unsupervised assessment. Mohammadi and Khasteh (2019) simplified the process of feature extraction with GloVe model for word embedding and reinforcement learning for English and Persian readability assessment. Azpiazu and Pera (2019) presented a multiattentive recurrent neural network model that considers raw words as input and incorporates attention mechanism with POS and morphological tags. Deutsh et al. (2020) proposed a fusion model by adding the numerical output from transformer to the linguistic features as input into SVM classifiers for readability prediction. We notice that in previous studies, the linguistic features are mostly considered to be independent. Each of them is used as an additional one to another. However, two features can reflect the same type of linguistic phenomenon, and thus are positively correlated in influencing the readability of a text. The correlation relationships among features may help learn dense representations of linguistic features to be utilized by neural network models for better-informed predictions."
    }, {
      "heading" : "2.2 Feature Embedding",
      "text" : "An important question in building neural network models is how to learn embedding representation. Feature binning has been studied to exploit the relatedness between different intervals of feature values in feature vector representation (Sil et al., 2017; Liu et al., 2016). In particular, Maddela and Xu (2018) applied smooth binning and project each numerical feature into a vector representation with multiple Gaussian radial basis functions. The embedding approach captures the nuance relationships between different intervals of feature values.\nMethods similar to word embedding (Mikolov et al., 2013) have been applied to create embeddings of POS tags. Chen and Manning (2014) showed that the POS tag and arc labels exhibit semantic similarity like words and embedding can capture the similarities between POS tags or arc labels. We hypothesize that the pair-wise correlations among the linguistic features for ARA can also be used to learn embedding and we propose to use graphbased model for that purpose.\nThere exists a vast amount of research on graphbased embedding (Nickel et al., 2016; Wang et al., 2017; Cai et al., 2018; Ji et al., 2020). We study\ntwo methodologies in particular: Retrofitting (Faruqui et al., 2014) and TransE (Bordes et al., 2013).\nThe resulting similarities learned from datadriven embedding may not fully reflect the similarities one has in mind for their application (Goldberg, 2017). Retrofitting (Faruqui et al., 2014) used information from WordNet, Framenet and PPDB to improve pre-trained embedding vectors so that related words will have more similar embeddings. The method first constructs a graph (\uD835\uDC49, \uD835\uDC38) where V is the set of word types, and \uD835\uDC38 ⊆ \uD835\uDC49 × \uD835\uDC49 indicates semantic relationships among pairs of words with ontology Ω. Given an original embedding vector \uD835\uDC5E*! , a new embedding \uD835\uDC5E! is learned such that it is closer to \uD835\uDC5E*! and its neighbors \uD835\uDC5E\" , ∀\uD835\uDC57 such that (\uD835\uDC56, \uD835\uDC57) ∈ \uD835\uDC38 and with closeness measured by Euclidean distance. The objective is to minimize Ψ(\uD835\uDC44) :\nΨ(\uD835\uDC44) =&'\uD835\uDEFC!‖\uD835\uDC5E! − \uD835\uDC5E,!‖\" + & \uD835\uDEFD!# (!,#∈')\n/\uD835\uDC5E! − \uD835\uDC5E#/ \" 0\n)\n!*+\nwhere \uD835\uDEFC and \uD835\uDEFD control the importance of a word embedding \uD835\uDC5E! being similar to itself in the original space or to another word in the same space connected by relational information.\nWhile Retrofitting is used to improve entity embedding in a graph, knowledge graph embedding learns representations for both the entities and their relations. TransE is a representative translational distance model where entities and relations are modeled in the same Euclidean space. Given two entity vectors h, t and a translation vector r between them, the model requires \uD835\uDC89 + \uD835\uDC93 ≈ \uD835\uDC95 for the observed triple (h, r, t). Hence, TransE assumes the score function\n\uD835\uDC53#(ℎ, \uD835\uDC61) = ‖\uD835\uDC89 + \uD835\uDC93 − \uD835\uDC95‖$!/$\"\nis low if (h, r, t) holds, and high otherwise. To differentiate between correct and incorrect triples, TransE score difference is minimized using margin based pairwise ranking loss."
    }, {
      "heading" : "3 Methodology",
      "text" : "Let \uD835\uDC39 = @\uD835\uDC53&, … , \uD835\uDC53'#B (where \uD835\uDC41( is the number of features) be a linguistic feature set designed for readability assessment. Let matrix \uD835\uDC9F be a collection of the vector representations of\n\uD835\uDC41) documents with \uD835\uDC51! ∈ \uD835\uDC45'# , where \uD835\uDC51! = (\uD835\uDC65\", … , \uD835\uDC65#!)\n$, and \uD835\uDC65\"(1 ≤ \uD835\uDC57 ≤ \uD835\uDC41() is the value of feature \uD835\uDC53\" in \uD835\uDC51! . To construct the syntactic dense embeddings for document representation, we perform the following steps: (1) We apply Gaussian-binning method (Maddela and Xu, 2018) to \uD835\uDC9F such that each feature value \uD835\uDC65\" of \uD835\uDC53\" in \uD835\uDC51! is projected into a k-dimensional vector \uD835\uDC65* JJJJ⃑ = (\uD835\uDC66&, … , \uD835\uDC66+), where \uD835\uDC66- (1 ≤ \uD835\uDC5B ≤ \uD835\uDC58) is the distance of feature value \uD835\uDC65\" in \uD835\uDC51! to bin n. We concatenate the \uD835\uDC65* JJJJ⃑ for all \uD835\uDC51! to form the initial data-driven embedding of feature \uD835\uDC53\" , with dimension of \uD835\uDC40 = \uD835\uDC58 × \uD835\uDC41), ∀\uD835\uDC57 ∈ \uD835\uDC41(. (2) We form a feature graph \uD835\uDCA2 using positive correlations among the \uD835\uDC41( features by setting a correlation threshold of 0.7. We preserve only the positive correlations in the graph. (3) Let the matrix L ∈ \uD835\uDC45/×'# be the collection of embeddings of \uD835\uDC53\" ∈ \uD835\uDC45/. Given a feature graph \uD835\uDCA2 and matrix L ∈ \uD835\uDC45/×'# , we apply TransE (Bordes et al., 2013) or Retrofitting (Faruqui et al., 2014) to learn optimized feature embeddings for each feature \uD835\uDC53\" . Instead of random initialization, we use the data-driven embedding of \uD835\uDC53\" ∈ \uD835\uDC45/ from Step (1) as the initial entity embedding for optimization. The syntactic latent space of \uD835\uDC45/ is trained by TransE or Retrofitting respectively to encode the relationship knowledge implied by the correlations among linguistic features so that the final dense embedding of linguistic feature \uD835\uDC53\" will be closer to those positively correlated with it in graph \uD835\uDCA2 . We denote the matrix optimized by TransE or Retrofitting with \uD835\uDC3F1 ∈ \uD835\uDC45/×'#. (4) To construct the syntactic dense embeddings of document representation with the embeddings of linguistic features, we perform a linear mapping to project the document feature vectors onto the syntactic latent space \uD835\uDC45/ . Specifically, given a feature vector of document \uD835\uDC51! ∈ \uD835\uDC45'# , and an optimized syntactic matrix \uD835\uDC3F1 ∈ \uD835\uDC45/×'# , the projected document vector \uD835\uDC51S! in the syntactic latent space \uD835\uDC45/ is defined as:\n\uD835\uDC51S! = \uD835\uDC3F1\uD835\uDC51! = (\uD835\uDC59&, … , \uD835\uDC59/), where \uD835\uDC592(1 ≤ \uD835\uDC5D ≤ \uD835\uDC40) is the projected value of the \uD835\uDC41( linguistic features of document \uD835\uDC51! at dimension p of \uD835\uDC45/ . We name \uD835\uDC51S! ∈ \uD835\uDC45/ the “syntactic dense embedding”.\nTo construct semantic dense embeddings for the documents, we learn the BERT average embedding representations following the original procedures as shown in Figure 2, where the final BERT representation is the average over all tokens. An alternative approach is to use the [CLS] token embedding to represent the text and fine-tune it for prediction. In our pilot study, we experimented rigorously with different finetuning strategies for each of the six datasets. The best finetuning results as compared with the original BERT average embeddings are reported in Appendix A. The sizes of our corpora are small ranging from 326 to 2500 as described later in Table 2. The finetuning process for BERT with 110M parameters may fit very well on training set but may not generalize well on test set. In the pilot study, we found that the overall performances of the finetuned BERT are not better than the original BERT. Therefore, we present experimentations with the original average BERT embeddings.\nWith the BERT dense embeddings and the syntactic dense embeddings, we propose a DNN dual channel neural network model (i.e., DualModel) to predict the documents’ readability levels. We first feed the BERT embeddings into a fourlayer network and the syntactic dense embeddings into a two-layer network. We then concatenate the outputs of the two channels into combined syntactic-semantic dense embeddings as input into another two-layer network, with MLP and SoftMax layers for readability classification. The Model architecture is provided in Figure 3."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data Sets",
      "text" : "To evaluate our proposed models, we use six readability data sets as shown in Table 2. We create three data sets for Chinese L1 and L2 and English L2 readability assessment. The Chinese L1 data sets are textbooks for first language learning for primary school, secondary school, and high-\n2 http://www.dzkbw.com\nschool education from three publishers. The Chinese L2 data sets are from 5 grades of 73 textbooks that are most widely used by 7 universities in China for teaching Chinese to international students, as described in Lu et al. (2019). The ENEW data set is of 4 grades of English textbooks from New Concept English series which is one of the most widely used English L2 textbooks in China. We followed the data preparation of ENCT in Jiang et al. (2018) to prepare ENEW corpus. The raw data of Chinese L1and ENEW data sets are publicly available from their textbook websites2,3.\nIn addition, we use three benchmark corpora. We obtain the WeeBit data for English L1 from the authors of Vajjala and Meurers (2012, 2014). We re-extract the text from the HTML files and discard\n3 http://www.xgnyy.com\ndocuments that are fill-in-the-blank tests or duplicate. We take the middle set of 500 documents by document length for each class to form a 2500- document WeeBit corpus. We obtained the Cambridge Exam data set for English L2 readability assessment (Xia et al., 2016) from their website4. We found 5 duplicate documents in class FCE, therefore resulting in a total of 326 documents of five grade levels. We also downloaded the OneStopEnglish data set for English L2 learning from its website5 (Vajjala and Lučić, 2018).\nFollowing the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment. We design 33 features for English L2 referencing Vajjala and Meurers (2012). We use the feature extraction codes provided by Vajjala and Meurers (2012) to recalculate the 46 feature values for the 2500-document WeeBit corpus. We acquire the 155-feature calculation results from the OneStopEnglish corpus. We drop the features that have zero value for all documents and obtain the values of 140 features. In our pilot study with ENEW data set, we found that our 33- feature design was effective and apply these to Cambridge corpus as well. We provide linguistic feature descriptions in Appendix B."
    }, {
      "heading" : "4.2 Model Evaluation",
      "text" : "According to our methodologies, we have two implementations of the Dual-channel model to combine syntactic and semantic dense embeddings for ARA: GFE-TransE+BERT and GFERetrofit+BERT. Both have the same network architecture as in Figure 3. The difference is that Gaussian embedding of features are used in TransE\n4 http://www.ilexir.co.uk/datasets/index.html\nand Retrofitting respectively to learn the optimized feature embedding based on correlation graph and then produce syntactic dense embeddings of documents. We compare our methodology with the following baselines: (1) SVM and LR with document feature vector\n\uD835\uDC51! ∈ \uD835\uDC45'# , which are typical classification methods based on manual features.\n(2) BERT-only DNN: This is a BERT-DNN network which has the same architecture as the right-hand side BERT channel in Figure 3. Using BERT for representation has been found effective (Martinc et al., 2019). (3) Raw+BERT Model: This model concatenates the BERT DNN channel output with raw feature vectors \uD835\uDC51! ∈ \uD835\uDC45'# to form input into neural network for predictions. It is to verify if feature embedding is actually needed or if we could simply augment the BERT embedding with raw feature vectors for prediction. (4) G-Doc+BERT: Following Maddela and Xu (2018), for each feature \uD835\uDC65\"(\uD835\uDC57 ≤ 1 ≤ \uD835\uDC41() in \uD835\uDC51! = (\uD835\uDC65\", … , \uD835\uDC65#!)\n$ , we learn the Gaussian embedding \uD835\uDC65* JJJJ⃑ and concatenate all of them into a document embedding representation. We use this syntactic dense embedding not trained by graph relations as the left-channel input in the Dual-DNN model in Figure 3 to compare with our proposed method. For evaluation of model effectiveness, we use Accuracy and Distance-1 Adjacent Accuracy. Adjacent Accuracy means that predicting a text to be within one level distance of the true label is still considered accurate (Heilman et al., 2008). We perform 5-fold stratified cross-validation and report average Accuracy and Adjacent Accuracy. We provide the hyper parameters of neural network models and the preprocessing procedures in Appendix C, and the test of correlation thresholds in Appendix D.\n5 https://zenodo.org/record/1219041"
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We first present the comparison of BERT-only DNN model with two traditional machine learning models of SVM and Logistic Regression, and three other single channel DNN models. Table 3 shows the Accuracy and Adjacent Accuracy in the first and second row for each data set. We observe that BERT-only DNN performs the best in five out of the six data sets except for ENEW. This indicates that semantic embedding alone is very effective in ARA with neural network models which are better than traditional machine learning models with raw feature vector representations. This result is consistent with previous studies using neural network models (Martinc et al., 2019; Azpiazu and Pera, 2019).\nNext, we compare BERT-only model with the Dual-channel DNN models with Raw+BERT and G-Doc+BERT. We find that augmenting BERT with raw feature value vector or document vector based on Gaussian embedding can slightly improve the performance of BERT, showing that the raw linguistic features contain additional structural information of the text that are marginally but consistently useful to the neural models for all data sets.\nThe performances of our proposed method are presented in the last two columns of Table 3. We observe that the two Dual-Models achieve the best performances among all 10 models in five out of six data sets (except for ENEW) and are better than the BERT-only and the other Dual-channel models. Moreover, except for Chinese L2 where the improvement is relatively smaller, the Dual-Model improvements are significant (with Student t-test at p<0.05 level) in the other four data sets of Chinese L1, WeeBit, OneStopEnglish and Cambridge. These results strongly support our earlier hypothesis that the correlations between linguistic features can provide additional useful information to learn syntactic dense embeddings that complement the semantic dense embeddings.\nComparing the last two columns of Table 3, we can observe generally similar performances in using TransE or Retrofitting on the feature graph. In theory, we impose a strict closeness constraint in Retrofitting, but let TransE learn the embedding for the correlation relation freely. The higher flexibility of TransE did not translate into better effectiveness. We speculate that the limited amount of training data may hinder our model from taking full advantage of the flexibility of TransE.\nFigure 4 presents a comparison of t-SNE visualization of semantic and syntactic dense embeddings, and the concatenated embedding. The figure illustrates that the concatenated embedding can produce more closely clustered data points by grade levels.\nTo investigate how Dual-Model improves over BERT-only model in predicting different readability levels, we present analysis of True Positive Rate (TPR) at each grade level. For each data set, we select from cross validation the best GFE-TransE+BERT model and the BERT-only model and then apply them to the whole data set. We construct confusion matrices and calculate TPR for each grade level as:\n\uD835\uDC47\uD835\uDC43\uD835\uDC45 = \uD835\uDC47\uD835\uDC5F\uD835\uDC62\uD835\uDC52 \uD835\uDC43\uD835\uDC5C\uD835\uDC60\uD835\uDC56\uD835\uDC61\uD835\uDC56\uD835\uDC63\uD835\uDC52 (\uD835\uDC47\uD835\uDC5F\uD835\uDC62\uD835\uDC52 \uD835\uDC43\uD835\uDC5C\uD835\uDC60\uD835\uDC56\uD835\uDC61\uD835\uDC56\uD835\uDC63\uD835\uDC52 + \uD835\uDC39\uD835\uDC4E\uD835\uDC59\uD835\uDC60\uD835\uDC52 \uD835\uDC41\uD835\uDC52\uD835\uDC54\uD835\uDC4E\uD835\uDC61\uD835\uDC56\uD835\uDC63\uD835\uDC52)\nAs shown in Figure 5, for Chinese L1 we observe that the largest improvements by the Dual Model are more spread out at Grade 3, 5, 8, and 10\nthan for Chinese L2 which are at both ends of grade of 1 and 5. In contrast, for the four English corpora, adding syntactic dense embedding improves the BERT-only model more in the middle and the higher grade levels. We also observe from Table 3 that the improvement on Chinese L1 is more pronounced. For example, the GFE-TransE+BERT model for Chinese L1 achieved an improvement of 19.4% over BERT-only (0.4732 vs. 0.3963), while Weebit achieved an improvement of 3.88% over BERT-only (0.8672 vs 0.8348).\nWe may speculate that the differences in the improvement might be caused by two factors among many others: (1) how important the syntactic structure is for building the foundational knowledge in learning a certain language; and (2) how the semantic and syntactic knowledge of a certain language is organized throughout the learning process in order to lead the language learners through grasping the language.\nWe construct the correlation graphs with positive correlation relationship only while we observe that\nthere exist both positive and negative correlations among linguistic features. To investigate the effectiveness of learning embedding by considering negative correlation as well, we define an additional score function for negatively correlated features used in TransE as:\n\uD835\uDC53#(ℎ, \uD835\uDC61) = 1 − ‖\uD835\uDC89 + \uD835\uDC93 − \uD835\uDC95‖$!/$\"\nWe present performance comparisons with GFE-\nTransE+BERT model in Table 4. We find that both models perform similarly, showing that defining positive correlation alone is sufficient in learning dense embeddings. We speculate that in feature embedding, the most important is to make similar features closer in the latent space, while repulsing negatively correlated feature embeddings away may not make a better representation of the features, which could have already been well separated in the latent space."
    }, {
      "heading" : "6 Conclusions",
      "text" : "By combining the semantic dense embeddings and the syntactic dense embedding in a dualchannel neural network model, we propose a new methodology for readability assessment that capture both the semantic and the syntactic knowledge related to readability discrepancies. Experiments with six data sets and two proficiency levels show that our Dual-Model is better than the semantic-alone and the syntactic-alone baselines. We prove that complementing semantic dense embeddings with syntactic dense embeddings learned with correlation graph of linguistic features can produce better-informed representations for readability assessment. We will further improve our research by studying other applicable algorithms and linguistic phenomena that could benefit from learning syntactic latent space and syntactic dense embedding representations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by National Social Science Fund (Grant No. 17BGL068). We thank the anonymous reviewers for their helpful feedback and suggestions."
    }, {
      "heading" : "Appendix A. BERT-finetuning Pilot Experiment Performances in Accuracy",
      "text" : "Compared with BERT original as in Paper"
    }, {
      "heading" : "Appendix B. Chinese L1 and L2 Linguistic Features",
      "text" : "Feature category\nSubcategory Features used in metrics\nShallow Features\nCharacter Common characters, stroke-counts, characters by HSK levels Words n-gram, words by HSK levels Sentence Sentence length\nPOS Features\nAdjective, functional words, verbs, nouns, content words, idioms, adverbs\nSyntactic Features\nPhrases Noun phrases, verbal phrases, prepositional phrases Clauses Punctuation-clause, dependency distance Sentences Parse tree, dependency distance\nDiscourse Features\nEntity density\nEntities, named entities\nCoherence Conjunctions, pronouns (Note: The full descriptions of the Chinese L1 and L2 features cannot be included in the paper due to space limit. Please contact the authors if needed.)\nEnglish 33 Linguistic Features Category ID Linguistic Features\nLexical Features\n1 Lexical Density (LD) 2 Type-Token Ratio (TTR) 3 Corrected TTR 4 Root TTR (RTTR) 5 Bilogarithmic TTR (LogTTR) 6 Uber Index (Uber) 7 Lexical Word Variation (LV) 8 Verb Variation-1 (VV1) 9 Squared VV1 (SVV1) 10 Corrected VV1 (CVV1) 11 Verb Variation 2 (VV2) 12 Noun Variation (NV) 13 Adjective Variation (AdjV) 14 Adverb Variation (AdvV) 15 Modifier Variation (ModV) 16 Proportion of words in AWL (AWL) 17 Avg. Num. Characters per word (NumChar) 18 Avg. Num. Syllables per word (NumSyll)\nSyntactic Features"
    }, {
      "heading" : "19 mean length of a sentence",
      "text" : "20 average number of words per punctuation-clause 21 number of punctuation-clauses per sentence 22 average number of subordinate clauses per punctuation clause 23 average number of subordinate clauses per sentence\nData Set BERT-finetuning-only BERT-only (as in paper)\nChinese L1 0.353 0.3963 Chinese L2 0.5353 0.6777\nENEW 0.8881 0.8425 WeeBit 0.8016 0.8348\nOneStopEng 0.8235 0.8157 Cambridge 0.6687 0.696\n24 average number of co-ordinate phrases per punctuation clause 25 average number of co-ordinate phrases per sentence 26 average number of verb phrases per punctuation clause 27 average number of noun phrases per sentence 28 average number of verbal phrases per sentence 29 average number of prepositional phrases per sentence 30 average length of noun phrases 31 average length of verbal phrases 32 average length of prepositional phrases 33 average height of parse tree"
    }, {
      "heading" : "Appendix C. Neural Network Parameters and Corpus Preprocessing",
      "text" : "Max Length Batch Size Epoch Learning Rate\nChi. L1 512 4 60 0.0001 Chi. L2 512 4 60 0.0001 ENEW 256 4 60 0.0001 WeeBit 256 4 60 0.0001 OneStopEng. 512 4 40 0.0001 Cambridge 1024 4 40 0.0001\nCorpus Preprocessing: To calculate linguistic features, we need to first preprocess the corpus. For Chinese data set preprocessing, we use NLPIR6 for word segmentation, LTP7 for POS tagging and named entity recognition, and Stanford CoreNLP (Manning et al., 2014) for syntactic parsing, grammatical labeling, and clause annotation. For preprocessing of ENEW and Cambridge, we use NLTK 8 for syllable counts and Stanford CoreNLP for all other feature calculations. For WeeBit, we re-extract the documents from the HTML files and use our own procedures to reconstruct the corpus. Then we use the author’s code for feature calculation (Vajjala and Meurers 2012). We use the feature values provided by OneStopEnglish directly (Vajjala and Lučić 2018).\n6 http://ictclas.nlpir.org/ 7 http://www.ltp-cloud.com/"
    }, {
      "heading" : "Appendix D. Test of Correlation Coefficient Thresholds",
      "text" : "Correlation Coefficient Threshold\nAccuracy, Adjacent Accuracy\n0.3 0.4651, 0.8498\n0.4 0.461, 0.8434\n0.5 0.4635, 0.8377\n0.6 0.461, 0.8572\n0.7 0.4732, 0.8555\n0.8 0.4594, 0.8385\nNote: To choose an appropriate correlation coefficient threshold for constructing correlation graph, we test different thresholds on Chinese L1 corpus with GFE-TransE+BERT dual model. The above table shows that threshold 0.7 provides the best performance and therefore is used for all experiments.\n8https://github.com/rlvaugh/Impractical_Python_Projects/bl ob/master/Chapter_8/count_syllables.py"
    } ],
    "references" : [ {
      "title" : "Multiattentive Recurrent Neural Network Architecture for Multilingual",
      "author" : [ "I.M. Azpiazu", "M.S. Pera" ],
      "venue" : null,
      "citeRegEx" : "Azpiazu and Pera,? \\Q2019\\E",
      "shortCiteRegEx" : "Azpiazu and Pera",
      "year" : 2019
    }, {
      "title" : "Is cross‐ lingual readability assessment possible",
      "author" : [ "I.M. Azpiazu", "M.S. Pera" ],
      "venue" : "Journal of the Association for Information Science & Technology,",
      "citeRegEx" : "Azpiazu and Pera,? \\Q2020\\E",
      "shortCiteRegEx" : "Azpiazu and Pera",
      "year" : 2020
    }, {
      "title" : "Instruments to assess the quality of health information on the world wide web: What can our patients actually use",
      "author" : [ "E.V. Bernstam", "D.M. Shelton", "M. Walji", "F. Meric-Bernstam" ],
      "venue" : null,
      "citeRegEx" : "Bernstam et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bernstam et al\\.",
      "year" : 2005
    }, {
      "title" : "December). Translating embeddings for modeling multirelational data",
      "author" : [ "A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko" ],
      "venue" : "In Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Why readability formulas fail",
      "author" : [ "B. Bruce", "A. Rubin", "K.S. Starr" ],
      "venue" : "IEEE Transactions on Professional Communication, PC-24, 50–52.",
      "citeRegEx" : "Bruce et al\\.,? 1981",
      "shortCiteRegEx" : "Bruce et al\\.",
      "year" : 1981
    }, {
      "title" : "A comprehensive survey of graph embedding: Problems, techniques, and applications",
      "author" : [ "H. Cai", "Vincent W. Zheng", "Kevin Chen-Chuan Chang." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 30, 9 (2018), 1616–1637.",
      "citeRegEx" : "Cai et al\\.,? 2018",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "Testing consumers’ motivation and linguistic ability as moderators of advertising readability",
      "author" : [ "Chebat", "J.-C.", "C. Gelinas-Chebat", "S. Hombourger", "A.G. Woodside" ],
      "venue" : "Psychology & Marketing, 20(7), 599–624",
      "citeRegEx" : "Chebat et al\\.,? 2003",
      "shortCiteRegEx" : "Chebat et al\\.",
      "year" : 2003
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "D. Chen", "C.D. Manning" ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) 740-750.",
      "citeRegEx" : "Chen and Manning,? 2014",
      "shortCiteRegEx" : "Chen and Manning",
      "year" : 2014
    }, {
      "title" : "Predicting reading difficulty with statistical language models",
      "author" : [ "Collins-Thompson", "Kevyn", "Jamie Callan." ],
      "venue" : "Journal of the American",
      "citeRegEx" : "Collins.Thompson et al\\.,? 2005",
      "shortCiteRegEx" : "Collins.Thompson et al\\.",
      "year" : 2005
    }, {
      "title" : "On the failure of readability formulas to define readable texts: A case study from adaptations",
      "author" : [ "Davison", "Alice", "Robert N. Kantor." ],
      "venue" : "Reading Research Quarterly, 17(2):187–209.",
      "citeRegEx" : "Davison et al\\.,? 1982",
      "shortCiteRegEx" : "Davison et al\\.",
      "year" : 1982
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "J. Devlin", "M.W. Chang", "K. Lee", "K. Toutanova" ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistic Features for Readability Assessment",
      "author" : [ "T. Deutsch", "M. Jasbi", "S.M. Shieber" ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications. pages 1-17.",
      "citeRegEx" : "Deutsch et al\\.,? 2020",
      "shortCiteRegEx" : "Deutsch et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrofitting Word Vectors to Semantic Lexicons",
      "author" : [ "M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith" ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic readability assessment",
      "author" : [ "L. Feng" ],
      "venue" : "Ph.D Thesis. The City University of New York.",
      "citeRegEx" : "Feng,? 2010",
      "shortCiteRegEx" : "Feng",
      "year" : 2010
    }, {
      "title" : "A new readability yardstick",
      "author" : [ "R. Flesch" ],
      "venue" : "Journal of applied psychology, 32(3): 221.",
      "citeRegEx" : "Flesch,? 1948",
      "shortCiteRegEx" : "Flesch",
      "year" : 1948
    }, {
      "title" : "Neural network methods for natural language processing",
      "author" : [ "Y. Goldberg" ],
      "venue" : "Synthesis lectures on human language technologies, 10(1), 1-309.",
      "citeRegEx" : "Goldberg,? 2017",
      "shortCiteRegEx" : "Goldberg",
      "year" : 2017
    }, {
      "title" : "Assessing BERT's syntactic abilities",
      "author" : [ "Y. Goldberg" ],
      "venue" : "arXiv preprint arXiv:1901.05287.",
      "citeRegEx" : "Goldberg,? 2019",
      "shortCiteRegEx" : "Goldberg",
      "year" : 2019
    }, {
      "title" : "Coh-Metrix: Analysis of text on cohesion and language",
      "author" : [ "A.C. Graesser", "D.S. McNamara", "M.M. Louwerse", "Z. Cai" ],
      "venue" : "Behavior Research Methods, Instruments, & Computers, 36(2), 193–202.",
      "citeRegEx" : "Graesser et al\\.,? 2004",
      "shortCiteRegEx" : "Graesser et al\\.",
      "year" : 2004
    }, {
      "title" : "The fog index after twenty years",
      "author" : [ "R. Gunning" ],
      "venue" : "Journal of Business Communication, 6(2): 3-13",
      "citeRegEx" : "Gunning,? 1969",
      "shortCiteRegEx" : "Gunning",
      "year" : 1969
    }, {
      "title" : "Readability Classification for German using Lexical, Syntactic, and Morphological Features",
      "author" : [ "J. Hancke", "S. Vajjala", "D. Meurers" ],
      "venue" : "Proceedings of 24th International Conference on Computational Linguistics. 1063-1080.",
      "citeRegEx" : "Hancke et al\\.,? 2012",
      "shortCiteRegEx" : "Hancke et al\\.",
      "year" : 2012
    }, {
      "title" : "An analysis of statistical models and features for reading difficulty prediction",
      "author" : [ "M. Heilman", "K. Collins-Thompson", "M. Eskenazi" ],
      "venue" : "Proceedings of the third workshop on innovative use of NLP for building educational applications.",
      "citeRegEx" : "Heilman et al\\.,? 2008",
      "shortCiteRegEx" : "Heilman et al\\.",
      "year" : 2008
    }, {
      "title" : "What does BERT learn about the structure of language? ACL 2019 57th Annual Meeting of the Association for Computational Linguistics",
      "author" : [ "G. Jawahar", "B. Sagot", "D. Seddah" ],
      "venue" : null,
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications",
      "author" : [ "S. Ji", "S. Pan", "E. Cambria", "P. Marttinen", "S.Y. Philip" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems. pages 1-21.",
      "citeRegEx" : "Ji et al\\.,? 2021",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2021
    }, {
      "title" : "An ordinal multi-class classification method for readability assessment of Chinese documents",
      "author" : [ "Z. Jiang", "G. Sun", "Q. Gu", "D. Chen" ],
      "venue" : "Proceedings of International Conference on Knowledge Science, Engineering and",
      "citeRegEx" : "Jiang et al\\.,? 2014",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2014
    }, {
      "title" : "Enriching word embeddings with domain knowledge for readability assessment",
      "author" : [ "Z. Jiang", "Q. Gu", "Y. Yin", "D. Chen" ],
      "venue" : "Proceedings of COLING 2018. pages 366-378.",
      "citeRegEx" : "Jiang et al\\.,? 2018",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Derivation of new readability formulas for navy enlisted personnel",
      "author" : [ "J.P. Kincaid", "R.P. Fishburne Jr", "R.L. Rogers", "B.S. Chissom" ],
      "venue" : "Naval Technical Training Command Millington TN Research Branch.",
      "citeRegEx" : "Kincaid et al\\.,? 1975",
      "shortCiteRegEx" : "Kincaid et al\\.",
      "year" : 1975
    }, {
      "title" : "Revealing the Dark Secrets of BERT",
      "author" : [ "O. Kovaleva", "A. Romanov", "A. Rogers", "A. Rumshisky" ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "The Lexile framework as an approach for reading measurement and success",
      "author" : [ "C. Lennon", "H. Burdick" ],
      "venue" : "Retrieved from http://goo.gl/ 4ifNbZ",
      "citeRegEx" : "Lennon and Burdick,? 2004",
      "shortCiteRegEx" : "Lennon and Burdick",
      "year" : 2004
    }, {
      "title" : "Neural networks models for entity discovery and linking",
      "author" : [ "D. Liu", "W. Lin", "S. Zhang", "S. Wei", "H. Jiang" ],
      "venue" : "arXiv preprint arXiv:1611.03558.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence-Level Readability Assessment for L2 Chinese Learning",
      "author" : [ "D. Lu", "X. Qiu", "Y. Cai" ],
      "venue" : "In:Hong JF., Zhang Y., Liu P.(eds)",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Lecture Notes in Computer Science, vol 11831",
      "author" : [ "Chinese Lexical Semantics. CLSW" ],
      "venue" : "Springer, Cham, pages 381-392.",
      "citeRegEx" : "CLSW,? 2019",
      "shortCiteRegEx" : "CLSW",
      "year" : 2019
    }, {
      "title" : "A WordComplexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification",
      "author" : [ "M. Maddela", "W. Xu" ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Maddela and Xu,? 2018",
      "shortCiteRegEx" : "Maddela and Xu",
      "year" : 2018
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky" ],
      "venue" : "Proceedings of 52nd annual meeting of the association for computational linguistics:",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Supervised and unsupervised neural approaches",
      "author" : [ "M. Martinc", "S. Pollak", "M.R. Šikonja" ],
      "venue" : null,
      "citeRegEx" : "Martinc et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Martinc et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model",
      "author" : [ "H. Mohammadi", "S.H. Khasteh" ],
      "venue" : "arXiv preprint arXiv:1912.05957.",
      "citeRegEx" : "Mohammadi and Khasteh,? 2019",
      "shortCiteRegEx" : "Mohammadi and Khasteh",
      "year" : 2019
    }, {
      "title" : "A review of relational machine learning for knowledge graphs",
      "author" : [ "M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich" ],
      "venue" : "In: Proceedings of the IEEE, 104(1), 11-33.",
      "citeRegEx" : "Nickel et al\\.,? 2016",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "Automating readers' advisory to make book recommendations for K-12 readers",
      "author" : [ "Pera", "Maria Soledad", "Yiu-Kai Ng." ],
      "venue" : "Proceedings of the 8th ACM Conference on Recommender systems (RecSys '14). 9–16.",
      "citeRegEx" : "Pera et al\\.,? 2014",
      "shortCiteRegEx" : "Pera et al\\.",
      "year" : 2014
    }, {
      "title" : "Predicting proficiency levels in learner writings by transferring a linguistic complexity model",
      "author" : [ "I. Pilán", "E. Volodina", "T. Zesch" ],
      "venue" : null,
      "citeRegEx" : "Pilán et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pilán et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting readability: A unified framework for predicting",
      "author" : [ "Pitler", "Emily", "Ani Nenkova" ],
      "venue" : "Proceedings of 26th International Conference on Computational Linguistics,",
      "citeRegEx" : "Pitler et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pitler et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploring the impact of linguistic features for Chinese readability assessment",
      "author" : [ "X. Qiu", "K. Deng", "L. Qiu", "X. Wang" ],
      "venue" : "Proceedings of National CCF Conference on Natural Language Processing and Chinese Computing.",
      "citeRegEx" : "Qiu et al\\.,? 2017",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2017
    }, {
      "title" : "Linguistic Feature Representation with Statistical Relational Learning for Readability Assessment",
      "author" : [ "X. Qiu", "D. Lu", "Y. Shen", "Y. Cai" ],
      "venue" : "Proceedings of CCF International Conference on Natural Language",
      "citeRegEx" : "Qiu et al\\.,? 2019",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2019
    }, {
      "title" : "A statistical model for scientific readability",
      "author" : [ "Si", "Luo", "Jamie Callan." ],
      "venue" : "Proceedings of the 10th International Conference on Information Knowledge Management (ICKM-2001), 574– 576, Atlanta, GA.",
      "citeRegEx" : "Si et al\\.,? 2001",
      "shortCiteRegEx" : "Si et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural cross-lingual entity linking",
      "author" : [ "A. Sil", "G. Kundu", "R. Florian", "W. Hamza" ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).",
      "citeRegEx" : "Sil et al\\.,? 2018",
      "shortCiteRegEx" : "Sil et al\\.",
      "year" : 2018
    }, {
      "title" : "The Lexile Framework",
      "author" : [ "A.J. Stenner", "I Horabin", "D.R. Smith", "R. Smith." ],
      "venue" : "Durham, NC: Metametrics",
      "citeRegEx" : "Stenner et al\\.,? 1988",
      "shortCiteRegEx" : "Stenner et al\\.",
      "year" : 1988
    }, {
      "title" : "Leveling L2 texts through readability: Combining multilevel linguistic features with the CEFR",
      "author" : [ "Y.T. Sung", "W.C. Lin", "S.B. Dyson", "K.E. Chang", "Y.C. Chen" ],
      "venue" : "The Modern Language Journal, 99(2): 371-391.",
      "citeRegEx" : "Sung et al\\.,? 2015",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2015
    }, {
      "title" : "Are Cohesive Features Relevant for Text Readability Evaluation",
      "author" : [ "A. Todirascu", "T. François", "D. Bernhard", "N. Gala", "A.L. Ligozat" ],
      "venue" : "In Proceedings of COLING",
      "citeRegEx" : "Todirascu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Todirascu et al\\.",
      "year" : 2016
    }, {
      "title" : "On improving the accuracy of readability classification using insights from second language acquisition",
      "author" : [ "S. Vajjala", "D. Meurers" ],
      "venue" : "Proceedings of the ACL 2012 BEA 7th Workshop. 163–173.",
      "citeRegEx" : "Vajjala and Meurers,? 2012",
      "shortCiteRegEx" : "Vajjala and Meurers",
      "year" : 2012
    }, {
      "title" : "Readability assessment for text simplification: From analysing documents to identifying sentential simplifications",
      "author" : [ "S. Vajjala", "D. Meurers" ],
      "venue" : "ITL-International Journal of Applied Linguistics, 165(2), 194-222.",
      "citeRegEx" : "Vajjala and Meurers,? 2014",
      "shortCiteRegEx" : "Vajjala and Meurers",
      "year" : 2014
    }, {
      "title" : "OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification",
      "author" : [ "S. Vajjala", "I. Lučić" ],
      "venue" : "Proceedings of the thirteenth workshop on",
      "citeRegEx" : "Vajjala and Lučić,? 2018",
      "shortCiteRegEx" : "Vajjala and Lučić",
      "year" : 2018
    }, {
      "title" : "Grammatical templates: Improving text difficulty evaluation for language learners",
      "author" : [ "Wang S.", "Erik Andersen." ],
      "venue" : "Proceedings of COLING 2016. 1692–1702",
      "citeRegEx" : "S. and Andersen.,? 2016",
      "shortCiteRegEx" : "S. and Andersen.",
      "year" : 2016
    }, {
      "title" : "Knowledge graph embedding: A survey of approaches and applications",
      "author" : [ "Q. Wang", "Z. Mao", "B. Wang", "L. Guo" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 29(12): 2724-2743.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Text Readability Assessment for Second Language Learners",
      "author" : [ "M. Xia", "E. Kochmar", "T. Briscoe" ],
      "venue" : "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications. 12-22.",
      "citeRegEx" : "Xia et al\\.,? 2016",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2016
    }, {
      "title" : "A readability formula for Chinese language",
      "author" : [ "S. Yang" ],
      "venue" : "Ph.D. Thesis. University of Wisconsin-Madison.",
      "citeRegEx" : "Yang,? 1970",
      "shortCiteRegEx" : "Yang",
      "year" : 1970
    }, {
      "title" : "Text Readability Assessment for Chinese Second Language Teaching",
      "author" : [ "S. Zhu", "J. Song", "W. Peng", "D. Guo", "G. Wu" ],
      "venue" : "In: Hong JF., Zhang Y., Liu P. (eds) Chinese Lexical Semantics. CLSW 2019. Lecture Notes in Computer Science,",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Predicting readability has been widely applied in education (Lennon and Burdick, 2004), book publishing (Pera and Ng, 2014), marketing (Chebat et al.",
      "startOffset" : 60,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "Predicting readability has been widely applied in education (Lennon and Burdick, 2004), book publishing (Pera and Ng, 2014), marketing (Chebat et al., 2003), newspaper readership (Pitler and Nenkova, 2008), and health information communication (Bernstam et al.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : ", 2003), newspaper readership (Pitler and Nenkova, 2008), and health information communication (Bernstam et al., 2005).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "Ever since the first study by Lively and Pressey in 1923, many researchers have developed various popular readability formulas including Flesch (Flesch, 1948), Fog (Gunning, 1969) and Lexile (Stenner et al.",
      "startOffset" : 144,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "Ever since the first study by Lively and Pressey in 1923, many researchers have developed various popular readability formulas including Flesch (Flesch, 1948), Fog (Gunning, 1969) and Lexile (Stenner et al.",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 44,
      "context" : "Ever since the first study by Lively and Pressey in 1923, many researchers have developed various popular readability formulas including Flesch (Flesch, 1948), Fog (Gunning, 1969) and Lexile (Stenner et al., 1988).",
      "startOffset" : 191,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "These traditional readability formulas are favored by domain applications due to their simplicity even though the formulas are mostly based on shallow features and known to lack accuracy (Bruce et al., 1981; Davison and Kantor, 1982; Graesser et al., 2004).",
      "startOffset" : 187,
      "endOffset" : 256
    }, {
      "referenceID" : 17,
      "context" : "These traditional readability formulas are favored by domain applications due to their simplicity even though the formulas are mostly based on shallow features and known to lack accuracy (Bruce et al., 1981; Davison and Kantor, 1982; Graesser et al., 2004).",
      "startOffset" : 187,
      "endOffset" : 256
    }, {
      "referenceID" : 0,
      "context" : "To bypass the necessity of heavy feature engineering, deep learning strategies have been studied to automatically detect patterns or extract features related to readability (Azpiazu and Pera, 2019; Martinc et al., 2019; Mohammadi and Khasteh, 2019).",
      "startOffset" : 173,
      "endOffset" : 248
    }, {
      "referenceID" : 33,
      "context" : "To bypass the necessity of heavy feature engineering, deep learning strategies have been studied to automatically detect patterns or extract features related to readability (Azpiazu and Pera, 2019; Martinc et al., 2019; Mohammadi and Khasteh, 2019).",
      "startOffset" : 173,
      "endOffset" : 248
    }, {
      "referenceID" : 35,
      "context" : "To bypass the necessity of heavy feature engineering, deep learning strategies have been studied to automatically detect patterns or extract features related to readability (Azpiazu and Pera, 2019; Martinc et al., 2019; Mohammadi and Khasteh, 2019).",
      "startOffset" : 173,
      "endOffset" : 248
    }, {
      "referenceID" : 11,
      "context" : "While neural network models take word embedding as input, they in general discard linguistic features traditionally used in machine learning models (Deutsch et al., 2020).",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "If ever incorporated, linguistic features such as POS and morphological tags are only used to guide attention mechanism for embedding representation of the text (Azpiazu and Pera, 2019).",
      "startOffset" : 161,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "Pre-trained models such as BERT (Devlin et al., 2019) learn dense representations of text by informing the models with semantically neighboring words, sentences, or context.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "Despite the attempts of recent research to assess BERT’s ability to implicitly capture the structural properties of language (Goldberg, 2019; Jawahar et al., 2019; Kovaleva et al., 2019), it has been observed that BERT “tends to rely more on semantic than structural differences during the",
      "startOffset" : 125,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : "Despite the attempts of recent research to assess BERT’s ability to implicitly capture the structural properties of language (Goldberg, 2019; Jawahar et al., 2019; Kovaleva et al., 2019), it has been observed that BERT “tends to rely more on semantic than structural differences during the",
      "startOffset" : 125,
      "endOffset" : 186
    }, {
      "referenceID" : 26,
      "context" : "Despite the attempts of recent research to assess BERT’s ability to implicitly capture the structural properties of language (Goldberg, 2019; Jawahar et al., 2019; Kovaleva et al., 2019), it has been observed that BERT “tends to rely more on semantic than structural differences during the",
      "startOffset" : 125,
      "endOffset" : 186
    }, {
      "referenceID" : 33,
      "context" : "classification phase and therefore performs better on problems with distinct semantic differences between classes” (Martinc et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 46,
      "context" : "Similar works on other languages include French (Todirascu et al., 2016), German (Hancke et al.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : ", 2016), German (Hancke et al., 2012), Swedish (Pilán et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : ", 2012), Swedish (Pilán et al., 2016), and Japanese (Wang and Andersen, 2016).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "Feature binning has been studied to exploit the relatedness between different intervals of feature values in feature vector representation (Sil et al., 2017; Liu et al., 2016).",
      "startOffset" : 139,
      "endOffset" : 175
    }, {
      "referenceID" : 34,
      "context" : "Methods similar to word embedding (Mikolov et al., 2013) have been applied to create embeddings of POS tags.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 36,
      "context" : "There exists a vast amount of research on graphbased embedding (Nickel et al., 2016; Wang et al., 2017; Cai et al., 2018; Ji et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 138
    }, {
      "referenceID" : 51,
      "context" : "There exists a vast amount of research on graphbased embedding (Nickel et al., 2016; Wang et al., 2017; Cai et al., 2018; Ji et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "There exists a vast amount of research on graphbased embedding (Nickel et al., 2016; Wang et al., 2017; Cai et al., 2018; Ji et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "The resulting similarities learned from datadriven embedding may not fully reflect the similarities one has in mind for their application (Goldberg, 2017).",
      "startOffset" : 138,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Given a feature graph G and matrix L ∈ R/×'# , we apply TransE (Bordes et al., 2013) or Retrofitting (Faruqui et al.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 52,
      "context" : "We obtained the Cambridge Exam data set for English L2 readability assessment (Xia et al., 2016) from their website4.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 49,
      "context" : "We also downloaded the OneStopEnglish data set for English L2 learning from its website5 (Vajjala and Lučić, 2018).",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 25,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 53,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 23,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 45,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 40,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 29,
      "context" : "Following the feature engineering methodology in previous work (Flesch, 1948; Gunning, 1969; Kincaid et al., 1975; Yang, 1970; Feng, 2010; Jiang et al., 2014; Sung et al., 2015; Qiu et al., 2017; Lu et al., 2019), we design 102 linguistic features for Chinese L1 and 111 features for Chinese L2 readability assessment.",
      "startOffset" : 63,
      "endOffset" : 212
    }, {
      "referenceID" : 33,
      "context" : "Using BERT for representation has been found effective (Martinc et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "Adjacent Accuracy means that predicting a text to be within one level distance of the true label is still considered accurate (Heilman et al., 2008).",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "This result is consistent with previous studies using neural network models (Martinc et al., 2019; Azpiazu and Pera, 2019).",
      "startOffset" : 76,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "This result is consistent with previous studies using neural network models (Martinc et al., 2019; Azpiazu and Pera, 2019).",
      "startOffset" : 76,
      "endOffset" : 122
    } ],
    "year" : 2021,
    "abstractText" : "Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment.",
    "creator" : "Word"
  }
}