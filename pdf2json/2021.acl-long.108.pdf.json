{
  "name" : "2021.acl-long.108.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Automated Generation of Storytelling Vocabulary from Photographs for use in AAC",
    "authors" : [ "Mauricio Fontana", "Karyn Moffatt" ],
    "emails" : [ "mauricio.fontanadevargas@mail.mcgill.ca,", "karyn.moffatt@mcgill.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1353–1364\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1353"
    }, {
      "heading" : "1 Introduction",
      "text" : "Augmentative and Alternative Communication (AAC) tools can enhance communication for nonspeaking individuals, thus offering improved social interaction and independence. Well established NLP techniques, such as spell check and word prediction, support those with primarily physical barriers to communication (e.g., adults with ALS) to compose complex and nuanced sentences in orthographic-based systems more effciently. However, those with developmental disabilities (e.g., autism spectrum disorder, ASD) or lexical and semantic processing impairments that limit their ability to spell out words (e.g., adults with aphasia1) must usually rely on less expressive symbol-based systems, for which those techniques offer little sup-\n1a language disorder mostly often caused by a stroke.\nport due to unique characteristics of communication with these systems.\nUsers of symbol-based AAC typically do not construct full, grammatically correct sentences, complete with prepositions and infections, but rather often only need a few key content words (i.e., nouns, adjectives, verbs)—appearing at any part of the sentence— to supplement other forms of communication, including preserved speech, gestures, or drawings. Such scattered use of vocabulary hinders the typical statistical prediction approach, which relies on patterns learnt from a large training corpus.\nNonetheless, there is much opportunity for improving symbol-based AAC, which is often abandoned because it offers too little communication support relative to the effort required to learn and use (Moffatt et al., 2017).\nSelecting and organizing vocabularies able to attend user’s communication needs in a wide variety of contexts and such that they can fnd words quickly is one of the major challenges (van de Sandt-Koenderman, 2004; Bailey et al., 2006). Alphabetical organizations are not useful, and traditional hierarchical schemes based on abstract categories (e.g., food → apple) are diffcult for people with language impairments, making navigation extremely slow for anything but the smallest (least useful) vocabularies. Presenting vocabulary as a fat hierarchy is best (Beukelman et al., 2015; Brock et al., 2017; Wallace and Hux, 2014); however, only a very limited set of options can be displayed, making communication very reliant on having the desired keywords among the available options.\nProviding concise situation-relevant vocabularies currently depends on support from a clinician or caregiver to pre-program the device. But such support is often limited or not available, which consequently limits these devices to supporting generic expressions of wants and needs, i.e., functional communication, and not for social interactions involving spontaneous narratives (Waller, 2019).\nGenerating vocabulary from user’s contextual data through Natural Language Generation (NLG) techniques seems an obvious venue to facilitate social interactions. Although NLG has been successfully applied in the context of task-oriented dialogs (He et al., 2017), question answering (Su et al., 2016), text summarization (See et al., 2017), and story generation from photograph sequences (Hsu et al., 2020), it is unclear how these techniques can be adapted to the specifc needs of AAC support (Tintarev et al., 2014).\nIn this paper, we call for more research in the NLP community devoted to language generation for symbol-based AAC systems. We present an overview of the scarce research on the topic and contribute a method that generates vocabulary automatically from a user’s photographs to support autobiographical storytelling, demonstrating how it performs under different combination of the system’s controllable parameters and a wide range of input photographs."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 NLP on Orthographic AAC Systems",
      "text" : "NLP research on AAC systems has mainly focused on improving the communication rate of orthographic-based tools, primarily via attempts\nto reduce keystrokes with letter, word, or message prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008). Researchers have also explored techniques for improving prediction by including in the language model, some sort of contextual information, such as the topic of conversation (Lesher and Rinkus, 2002; Trnka et al., 2006), the user’s location (Garcia et al., 2015), their past utterances (Kristensson et al., 2020; Copestake, 1997; Wandmacher et al., 2008), or their partner’s speech (Wisenburn and Higginbotham, 2008). Virtually all commercial text-based high tech AAC devices employ some form of n-gram prediction (Higginbotham et al., 2012)."
    }, {
      "heading" : "2.2 The Need for Symbol-based AACs Able to Support Social Interactions",
      "text" : "Many people with developmental (e.g., ASD) or acquired disabilities have diffculty using written language, and therefore need support other than orthographic-based AAC. People with expressive aphasia, for example, present lexical and semantic processing impairments that affect their ability to retrieve the names of objects, combine linguistic elements, and use grammar. Nonetheless, they usually have good receptive communication skills and intellectual abilities preserved, and typically desire the ability to communicate complex ideas and share social stories spontaneously, such as describing a recent activity or experience (Garrett, 2005)2.\nTo support this population, researchers from the clinical community (McKelvey et al., 2010; Dietz et al., 2006; McKelvey et al., 2007; Beukelman et al., 2015) have successfully explored the presentation of vocabulary associated with personally relevant and highly contextualized photographs, where people, objects, and activities are depicted in their naturally occurring contexts (also known as visual scene displays, VSDs). Evidence indicates greater conversational turn-taking with fewer instances of frustration and navigational errors (Brock et al., 2017), and increased lexical retrieval during activity retell (Mooney et al., 2018), for which participants perceived this kind of support as very helpful.\nHowever, the automation of the language production process to support those social narratives is still highly unexplored. For example, Mooney\n2We also witnessed this in interactions observed in conversation groups at a local aphasia institute in which the frst author participated for 9 months.\net al.’s system CoChat (2018) generates keywords from human input simulating social network comments. NLP was used only to clean the input and identify nouns and frequent words. In consequence, available commercial tools3 depend on human effort planning and programming relevant vocabulary, leading to lack of spontaneous and independent communication, and requiring a great amount of time from caregivers (Drager et al., 2019)."
    }, {
      "heading" : "2.3 NLG for AAC Systems",
      "text" : "Generating language for AAC systems is highly different from typical NLG usage, mainly because the goal of AAC is to provide support for communicating users thoughts, and not to replace the user by an automatic communicator (Tintarev et al., 2014).\nThe Compansion system (Demasco and McCoy, 1992; McCoy et al., 1998), was one of the frst attempts to apply NLG towards that goal. It was designed to produce grammatically correct sentences from incomplete user input using a small domain model. Although Compansion was dedicated to functional communication, its concept of using a domain knowledge served as a stepping stone to Dempster et al.’s system aimed at generating conversational utterances (2010). In their prototype, users populated a personal knowledge base by recording where, when, and with whom they performed an activity shortly after its end. Through a template-driven system, users’ knowledge was converted into conversational utterances organized on topics that could be accessed during subsequent conversations. This work showed promising results on how NLG can be able to support social dialogues and increase participation of AAC users. However, their system still required considerable manual linguistic input from users.\nAutomatic generation of storytelling vocabulary has been successfully explored by researchers (Reiter, 2007; Black et al., 2010; Tintarev et al., 2016) to support children with limited memory or with physical and intellectual impairment telling \"how was school today\" to their parents. In their project, raw sensor data from passive RFID tags relating to locations, objects, and people was aggregated into events, and then transformed to coherent personal narratives using a domain knowledge containing the school timetable and the RFID tags mapping.\nTo provide just-in-time vocabularies that attend to emergent needs and are not tied to a specifc\n3e.g., Tobii Dynavox Snap Scene\nscenario (e.g., school), Demmans Epp et al. (2012) explored the use of information retrieval algorithms on internet-accessible corpora such as websites, dictionaries, and Wikipedia pages related to the user’s current location or conversation topic. Although this approach was useful for augmenting a base vocabulary with context-specifc terms, it is limited to locations (e.g., retail locations) for which internet-accessible corpora are likely to exist."
    }, {
      "heading" : "3 Vocabulary Generation Method",
      "text" : "Our method generates a rank of key words and short narrative phrases from a single4 input photo for scaffolding storytelling. It was designed to be used as the back end of interactive AAC systems in which relevant vocabulary is associated with a main photograph, such as Mooney et al.’s CoChat, or as in the example design shown in Fig. 1.\nWe used VIST-TRAIN, a sub-set of the visual storytelling dataset VIST (Huang et al., 2016) as the main source for vocabulary generation. VISTTRAIN encompasses 80% of the entire dataset, and is composed of 65,394 photos of personal events, grouped in 16,168 stories. Each photo is annotated with descriptions and narrative phrases that are part of a story, created by Amazon Mechanical Turk workers. We judged VIST to be a good source of vocabulary because i) photos were extracted from personal Flickr albums on a wide range of “storyable” events, related to 69 topics (e.g., graduation, building a house), ii) associated vocabulary is representative of storytelling and, iii) stories and photo descriptions were constructed by a large number (1907) of workers under a rigorous procedure.\nThe generation process is composed of fve steps, as detailed below and illustrated in Fig 2. We explore different implementations for some of the steps, represented by the system’s controllable parameters emphasized with bold italic formatting throughout the paper. The different combination of those parameters are evaluated in the next section."
    }, {
      "heading" : "3.1 Scene Understanding",
      "text" : "The frst step extracts contextual information from the photograph in the form of a high-level, humanlike description of the scene (i.e., caption) using the computer vision technique from Fang et al. (2015). Captioning was chosen over pure object detection and labelling due to the necessity of communicat-\n4to reduce the requirements on users, who may feel discouraged if multiple photos of the event are needed\ning more abstract concepts such as the actions being performed and the interactions between the objects, people, and environment during storytelling."
    }, {
      "heading" : "3.2 Photo Description Matching",
      "text" : "This step fnds the subset of VIST-TRAIN photos most similar to the user input by calculating the sentence similarity between the input photo description and all VIST-TRAIN photos descriptions. All photos with description similarity higher than the parameter Similarity Threshold are selected for processing in the next step, with an upper limit of 30 photos. Sentence similarity is defned as the soft cosine similarity (Sidorov et al., 2014)5 on a bag-of-words representation of the sentences using Word2Vec embeddings, after removing stop words6. Soft cosine was chosen as similarity measure due to its ability to capture the semantic relatedness between different words. This strategy was motivated by the fact that soft cosine similarity with Word2Vec was effective for fnding similar sentences on question-answering systems, achieving the best performance at the SemEval2017 Task 3 (Charlet and Damnati, 2017). Similarity based on entire documents (e.g., Doc2Vec) was\n5Gensim library implementation 6as defned by the Natural Language Toolkit (NLTK)\nnot used because it would require a much larger (at present, nonexistent) training corpus to create proper document embeddings, and there are no pretrained sentence embeddings trained exclusively on photo descriptions."
    }, {
      "heading" : "3.3 Stories Retrieval",
      "text" : "All narrative sentences associated with the selected photos are retrieved for processing in the next stage. The number of sentences per photo varies from 1 to 5 (µ = 3.1, σ = 1.4)."
    }, {
      "heading" : "3.4 Vocabulary Selection",
      "text" : "This step identifes a group of representative sentences and words from the retrieved set by applying the Affnity Propagation7 clustering (Frey and Dueck, 2007)—able to generate clusters with less error than other exemplar-based algorithms and not requiring a predetermined the number of clusters. The fnal set of generated phrases is formed by these cluster’s exemplars, ranked according to their respective clusters size. By defnition, this strategy results in phrases covering the wide range of semantics present in the set of retrieved phrases, while at the same time removing redundant (i.e., very similar) phrases. In case of\n7damping: 0.5, max. iter: 200, convergence iter.: 15\nnon-convergence (< 3% in our evaluation), the set of recommended phrases is formed by ranking all phrases according to the sum of their soft cosine similarity against all other phrases retrieved. The generated base vocabulary is formed by a rank of the word frequencies after fltering-out stop words and applying a porter stemmer to merge different variations (e.g., worked, working → work). The parameter Selection Method determines whether frequencies are calculated considering all retrieved phrases (ALL_PHRASES) or only clusters’ exemplars (EXEMPLARS)."
    }, {
      "heading" : "3.5 Vocabulary Expansion",
      "text" : "The goal of this step is to diversify the base vocabulary derived from VIST-TRAIN to increase communication fexibility. Thus, to fnd words that are related to, but distinct from the initial concept (e.g., cake → sweet), our method uses a model of the human mental lexicon as a secondary source of vocabulary. In this model, SWOW (De Deyne et al., 2019), words are connected with a certain strength representing their relatedness constructed from data of word-association experiments of over 90,000 participants. Therefore, unlike embeddings, SWOW encodes mental representations free from the basic demands of communication.\nThis strategy was motivated by the fact that word association data was successfully applied in a controlled study to support people with aphasia navigating related words more effectively (Nikolova et al., 2010), and that evidence from cognitive science research indicates that the network formed by associations in SWOW presents a widespread thematic structure, rather than taxonomic, with words strongly associated often occurring in the same situation (e.g., pick-strawberry; candle-church) (De Deyne et al., 2015) . This last step expands the initial set of base vocabulary by adding, for each word, the most strongly associated words in SWOW data. The system parameter Expansion Size determines how many words from SWOW are added for each word in the base vocabulary set. Repeated words are not included."
    }, {
      "heading" : "4 Evaluation Experiment",
      "text" : "The goal of our evaluation is to understand how our design choices, represented by the system controllable parameters, along with uncontrollable factors related to the input photograph (i.e., uncontrollable parameters), affect the system’s performance.\nThus, we compared the relevance of vocabulary generated under different combinations of these parameters to investigate the following specifc research questions:\n1. What combination of controllable system parameters related to the base vocabulary generation optimizes performance?\n2. How does the level of contextual information in the input photo affect performance?\n3. How does the quality of the contextual description inferred from the input photo affect performance?\n4. How does the level of contextual information in the input photo affect the quality of the inferred description?\n5. What is the effect of expanding the base generated vocabulary with words from a mental lexicon model on the system’s performance?"
    }, {
      "heading" : "4.1 Performance Metrics",
      "text" : "Considering the AAC application usage scenario, the performance of vocabulary generation can be conceptualized by the combination of two factors: i) communication fexibility, i.e., whether vocabulary needed for composing messages about a specifc experience is provided, and ii) communication ease, i.e., the diffculty in fnding a particular word among all options generated. These two factors directly map to the information retrieval concepts of precision (P ) and recall (R) as a perfect algorithm would provide all words the user needs to communicate the desired message (R = 1), and would not contain any irrelevant vocabulary (P = 1), thereby minimizing the need for scanning. In contrast, the worst algorithm would provide only irrelevant vocabulary (P = R = 0).\nTherefore, we tackle the vocabulary generation evaluation as an information retrieval problem, where the input photo is treated as the user query, generated words and phrases are treated as retrieved documents, and crowd sourced narrative sentences about the photograph are the relevant documents, i.e., ground truth (as detailed in Section 4.2). For each input photo, diffculty in fnding vocabulary and communication fexibility are operationalized as P and R, respectively:\n|{rel_words} ∩ {Gn}| P (n) =\nn\n|{rel_words} ∩ {Gn}| R(n) =\n|{rel_words}|\nwhere n is the number of words displayed to the user, rel_words are the words in the groundtruth sentences, and Gn are the top n words in the generated vocabulary rank. We also calculated the F1, a common information retrieval measure that captures the trade-off between P and R:\nP (n) × R(n)\nF1(n) = 2× P (n) +R(n)\nWe calculated these metrics for all n ∈ [1, 100], and constructed the P-R curves with the arithmetic mean values of P , R, and F1 across all input photographs under analysis. In contrast to BLEU/METEOR metrics, this analysis allows us to clearly demonstrate trade-offs between the diffculty fnding a word among options and communication fexibility, which is important because the number of displayed items will vary for each user.\nTo obtain a single measure of system performance across this entire interval, considering all input photos, we approximate the area under the P-R curves by calculating the mean average precision:\n100X mAP = P (n)(R(n)−R(n− 1))\nn=1"
    }, {
      "heading" : "4.2 Data",
      "text" : "As input photographs and groundtruth sentences, we used VIST-VAL, a sub-set of VIST not employed in our method that contains 8034 photos aligned with crowd sourced stories. We selected all photos from VIST-VAL containing the maximum number of sentences available (5) to act as our input photographs, resulting in 1946 photos. The ground-truth vocabulary for each photograph was formed by joining the fve associated narrative phrases (9730 in total), after removing stop words."
    }, {
      "heading" : "4.3 Specifc Procedures",
      "text" : "Controllable Parameters - Base Vocab. (RQ1). We defned four confgurations of parameters by crossing two extreme values of Similarity Threshold, i.e., 0 and best (highest similarity score among all VIST-VAL) with the Selection Method all_phrases and exemplars, resulting in four confgurations: 0_ALL, 0_EXEMPLARS, BEST_ALL, BEST_EXEMPLARS. Expansion size was set to 0 in all confgurations. In the absence of similar AAC generation systems to compare our method to, we created a BASELINE generation formed by a\nrank of the most frequent words from the Corpus of Contemporary American English (COCA) (Davies, 2009) without stop words. We adopted this baseline because current AAC tools are commonly built on word usage frequency data (Renvall et al., 2013).\nThe optimal values for the parameters established in this analysis were applied in subsequent analyses.\nContextual Information Level (RQ2, RQ4). To investigate the variability caused by different input photographs, we adopted the concept of context richness from Beukelman et al. (2015). The frst author scored each photo from 0–3 based on the number of contextual categories (environment, people/object, activity) it clearly depicts (0 when ambiguous). To validate these annotations, someone unfamiliar with the study also scored a subset of 514 photos (27.8% of the dataset)8. Krippendorff’s alpha reliability score was 0.82, indicating strong agreement between raters (Krippendorff, 2004).\nContext Description Quality (RQ3, RQ4). The frst author scored each photo description from 0 to 3 as follows: 0) not generated or completely unrelated; 1) misses most important elements OR contains most of important elements and a few unrelated elements; 2) contains most of important elements OR all important elements and a few unrelated elements; 3) contains all important elements in the photo and does not contain any unrelated elements. As for contextual information level, a subset of 514 were scored by someone unfamiliar with the study. Krippendorff’s alpha reliability score was 0.88, confrming strong agreement.\nEffect of Vocabulary Expansion (RQ5). We created 24 pairs of confgurations by combining different base vocabulary sizes (5, 10, 15, 20, 25, 30) with the expansion sizes (0, 1, 2, 3). The confguration [5-2], for example, contains fve base words plus two expanded words per base word, resulting in a maximum of 15 words (or less if expanded words were already in the base set)."
    }, {
      "heading" : "4.4 Results",
      "text" : "RQ1. To better illustrate the differences in performance, Fig. 3 presents the P-R curves, while Table 1 shows the mAP and maximum P and R mean values for the pairs of parameters values under investigation, in comparison to the baseline. Overall, 0_ALL results in the best performance,\n8all annotations are available at https://doi.org/ 10.5683/SP2/NVI701\nwith an mAP 4.6 times greater than the baseline, and 1.8 greater than the the worst confguration, BEST_EXEMPLARS.\nRQ2. In our input dataset, the proportion of photos according to their context richness score was: 8%(0), 54%(1), 30%(2), 8%(3). A MannWhitney U test indicated a signifcant difference on P and R only between photos with context richness 0 and the remaining levels (p < .002). Table 2 shows the mean performance metrics according to level of contextual information.\nRQ3. The distribution of input photos across context description quality scores was: 16%(0), 16%(1), 30%(2), 38%(3). We plot the P-R curves according to the context description quality scores in Fig. 4, and summarize performance metrics in Table 3. A Mann-Whitney U test indicated no signifcant differences between photo quality 1 and 2\n(p > .2). However, photos with description quality 3 signifcantly outperformed the other groups (p < .001), and quality 0 photos performed signifcantly worse than all other groups (p < .001).\nRQ4. Fig. 5 illustrates the relationship between the level of contextual information in the input photos and the quality of the photos descriptions generated using machine-learning.\nAs expected, photos with ambiguous contextual information (level= 0) most often received bad captions (53%). As context richness increased, the relative proportion of photos with good descriptions (scores 2 or 3) also increased (39%, 69%, 72%,\n80%), but the relative proportion of perfect descriptions (quality = 3) decreased (46%, 31%, 19%). Photos depicting only one type of contextual information (location, person/object, activity) resulted in the best descriptions: 46% received perfect descriptions, and 66% of all perfect descriptions were given to them. However, when compared to photos with more contextual information, they presented the highest relative proportion of very bad captions (15% vs 9.1% and 5.7%).\nRQ5. Fig. 6 compares the performance of different combinations of base vocabulary and expansion sizes against base vocabulary only, in function of the number of words displayed n. In general, for a given n, generation without expansion resulted in superior performance. However, on confgurations for which a high proportion of expanded words were already in the base vocabulary (e.g., n = 6, 21, 61), expansion presented similar or even better F1 scores than the base vocabulary on its own.\nTo better understand this phenomenon, we plot\nthe F1 score, averaged across all photos, in function of the proportion of expansion words not present in the base vocabulary during generation (Fig. 7). The mean F1 for generation without word expansion is also plotted for comparison.\nWe found that word expansion is able to bring improvement in performance when less than 60% of the expansion words are included in the fnal generated vocabulary, or in other words, when more than 40% of expansion words is already in the base vocabulary. The tendency is that, the lower the proportion of expansion words not in the base vocabulary, the higher the performance."
    }, {
      "heading" : "5 Discussion",
      "text" : "The design space for generating AAC storytelling vocabulary directly from photographs is vast and under explored. Design decisions for individual system components will impact other components and ultimately the overall system effectiveness, and therefore cannot be arbitrary. Without a rigorous performance evaluation on different confgurations of parameters, users would be at risk of using a fawed or under optimized system, which could lead to user frustration and abandonment, and cause confounds that obscure whether failures are due to the need for algorithmic tuning or mismatch between the intended support and user needs.\nThe study of controllable parameters (RQ1, 5) demonstrated that our method is able to provide relevant vocabulary, and showed how it can be used to optimize the system and identify areas for further improvement. The exploration of uncontrollable parameters (RQ2, 3, 4) helped illustrate the likely variation in system performance during real world usage (i.e., wide variety of input photos), allowing us to better anticipate potential problems or pitfalls and understand requirements for use.\nThe similar performance across photos with different levels of contextual information (RQ2) suggests that our method is robust to variations in the input photograph. Users will not need to be instructed to take photographs following specifc requirements, e.g., “photos should demonstrate an action” or “photos should depict objects only”. The similar levels of performance is explained by the pattern observed in the RQ4 analysis; the more elements a photo contains, the better knowledge the machine learning has to infer the central aspect of the photo, but at the same time, the harder it is to capture each and every element. In addition, an\nelement wrongly identifed will have less impact on the overall scene understanding since other elements complement the description. An example would be a photo of a birthday party, in which the machine-learning platform is able to infer the central concept (birthday) from the several elements depicted (e.g., cake, candles, balloons), but misses some of the details (e.g. drinks). On the other hand, simplistic photos will rarely lead to elements being cut out, but the computer vision technique will have more variability when performing the inferences, leading to erroneous descriptions more often."
    }, {
      "heading" : "On the other hand, the quality of generated",
      "text" : "vocabulary was strongly dependent on the computer vision technique employed to extract contextual information about the scene (RQ3) . When a wrong description is generated, the subsequent steps of the algorithm are misled and therefore generate vocabulary less relevant for retelling the scene depicted in the photograph. Nonetheless, even in this case, an AAC device using our method would provide vocabulary more relevant than if the most frequent English words were provided. Since photos for which the computer vision technique was able to correctly identify all contextual elements resulted in substantial performance gain, we encourage further exploration of this component. An option would be to use a higher number of raw context labels instead of the single human-like description employed in this work.\nOur vocabulary expansion analysis (RQ5) provide valuable insights into how the combination of multiple lexicon sources can generate more relevant vocabulary. The most promising approach was to combine the visual-to-story dataset with strongly associated words from a mental-lexicon model, but only when there was high intersection between the two vocabularies."
    }, {
      "heading" : "5.1 Limitations and Future Work",
      "text" : "Although VIST contains a very large range of events, one limitation is that it is unlikely to cover all possible scenarios, and may not accurately refect AAC communication. However, in the absence of an appropriate AAC-specifc corpora (a known issue in the community), we believe the VIST dataset can meaningfully represent the vocabulary needed for scaffolding storytelling. In addition, we do not expect the performance gains observed will directly translate to the same gains in usability. Our goal was to understand fundamental\nquestions necessary for advancing to a usability study, helping fne-tune system components before introducing them to users, avoiding unnecessary interactions with identifably poor designs. Our approach also enables larger numbers of parameters to be examined. The low level of social participation commonly observed among people with aphasia, combined with the rate-limited nature of AAC, would require feld experiments lasting an impractical amount of time to produce suffcient data to comprehensively explore possible combinations of parameters (Kristensson et al., 2020).\nAs a potential improvement to our method, Sent2Vec trained with BERT may better represent sentence structure and words context for fnding similar photo descriptions in step 2 than our use of soft cosine with Word2Vec. Another option would be the use of query expansion to enrich the descriptions. We encourage the exploration of the vast array of strategies for tackling the vocabulary generation process for AAC."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Developing a photo-to-story vocabulary AAC system presents two challenges; a NLP one in how to generate such vocabularies, and a HumanComputer-Interaction (HCI) one in how to use such vocabulary to offer interactive language support. In this work, we tackle the frst challenge.\nWe demonstrated that our method is able to generate vocabulary with reasonable levels of recall and precision, regardless of the level of contextual information in the input photograph, illustrated the likely variation in system performance during real world usage, and provided meaningful insights for fne tuning the algorithm, enabling us to move to the next phase of designing and evaluating, with AAC users, our mobile interactive application."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was funded by the Fonds de Recherche du Québec - Nature et Technologies (FRQNT), the Natural Sciences and Engineering Research Council of Canada (NSERC) [RGPIN2018-06130], the Canada Research Chairs Program (CRC), and by AGE-WELL NCE, Canada’s technology and aging network."
    } ],
    "references" : [ {
      "title" : "Family members’ perceptions of augmentative and alternative communication device use",
      "author" : [ "Rita L Bailey", "Howard P ParetteJr", "Julia B Stoner", "Maureen E Angell", "Kathleen Carroll." ],
      "venue" : "Language, Speech, and Hearing Services in Schools, 37(1).",
      "citeRegEx" : "Bailey et al\\.,? 2006",
      "shortCiteRegEx" : "Bailey et al\\.",
      "year" : 2006
    }, {
      "title" : "Using visual scene displays as communication support options for people with chronic, severe aphasia: A summary of AAC research and future research",
      "author" : [ "David R Beukelman", "Karen Hux", "Aimee Dietz", "Miechelle McKelvey", "Kristy Weissling" ],
      "venue" : null,
      "citeRegEx" : "Beukelman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Beukelman et al\\.",
      "year" : 2015
    }, {
      "title" : "Using NLG and sensors to support personal narrative for children with complex communication needs",
      "author" : [ "Rolf Black", "Joseph Reddington", "Ehud Reiter", "Nava Tintarev", "Annalu Waller." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on",
      "citeRegEx" : "Black et al\\.,? 2010",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 2010
    }, {
      "title" : "A comparison of visual scene and grid displays for people with chronic aphasia: A pilot study to improve communication using aac",
      "author" : [ "Kris Brock", "Rajinder Koul", "Melinda Corwin", "Ralf Schlosser." ],
      "venue" : "Aphasiology, 31(11):1282–1306.",
      "citeRegEx" : "Brock et al\\.,? 2017",
      "shortCiteRegEx" : "Brock et al\\.",
      "year" : 2017
    }, {
      "title" : "Simbow at semeval-2017 task 3: Soft-cosine semantic similarity between questions for community question answering",
      "author" : [ "Delphine Charlet", "Geraldine Damnati." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Charlet and Damnati.,? 2017",
      "shortCiteRegEx" : "Charlet and Damnati.",
      "year" : 2017
    }, {
      "title" : "Augmented and alternative NLP techniques for augmentative and alternative communication",
      "author" : [ "Ann Copestake." ],
      "venue" : "Natural Language Processing for Communication Aids.",
      "citeRegEx" : "Copestake.,? 1997",
      "shortCiteRegEx" : "Copestake.",
      "year" : 1997
    }, {
      "title" : "The 385+ million word corpus of contemporary american english (1990–2008+): Design, architecture, and linguistic insights",
      "author" : [ "Mark Davies." ],
      "venue" : "International journal of corpus linguistics, 14(2):159–190.",
      "citeRegEx" : "Davies.,? 2009",
      "shortCiteRegEx" : "Davies.",
      "year" : 2009
    }, {
      "title" : "The “small world of words” english word association norms for over 12,000 cue words",
      "author" : [ "Simon De Deyne", "Danielle J Navarro", "Amy Perfors", "Marc Brysbaert", "Gert Storms." ],
      "venue" : "Behavior research methods, 51(3):987–1006.",
      "citeRegEx" : "Deyne et al\\.,? 2019",
      "shortCiteRegEx" : "Deyne et al\\.",
      "year" : 2019
    }, {
      "title" : "Evidence for widespread thematic structure in the mental lexicon",
      "author" : [ "Simon De Deyne", "Steven Verheyen", "Amy Perfors", "Daniel J Navarro." ],
      "venue" : "CogSci.",
      "citeRegEx" : "Deyne et al\\.,? 2015",
      "shortCiteRegEx" : "Deyne et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating text from compressed input: An intelligent interface for people with severe motor impairments",
      "author" : [ "Patrick W Demasco", "Kathleen F McCoy." ],
      "venue" : "Communications of the ACM, 35(5):68–78.",
      "citeRegEx" : "Demasco and McCoy.,? 1992",
      "shortCiteRegEx" : "Demasco and McCoy.",
      "year" : 1992
    }, {
      "title" : "Towards providing just-in-time vocabulary support",
      "author" : [ "Carrie Demmans Epp", "Justin Djordjevic", "Shimu Wu", "Karyn Moffatt", "Ronald M Baecker" ],
      "venue" : null,
      "citeRegEx" : "Epp et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Epp et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic generation of conversational utterances and narrative for augmentative and alternative communication: A prototype system",
      "author" : [ "Martin Dempster", "Norman Alm", "Ehud Reiter." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Speech and Lan-",
      "citeRegEx" : "Dempster et al\\.,? 2010",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 2010
    }, {
      "title" : "Visual scene displays (VSD): New AAC interfaces for persons with aphasia",
      "author" : [ "Aimee Dietz", "Miechelle McKelvey", "David R Beukelman." ],
      "venue" : "Perspectives on Augmentative and Alternative Communication, 15(1):13–17.",
      "citeRegEx" : "Dietz et al\\.,? 2006",
      "shortCiteRegEx" : "Dietz et al\\.",
      "year" : 2006
    }, {
      "title" : "2019. AAC technologies with visual scene displays and “just",
      "author" : [ "Kathryn DR Drager", "Janice Light", "Jessica Currall", "Nimisha Muttiah", "Vanessa Smith", "Danielle Kreis", "Alyssa Nilam-Hall", "Daniel Parratt", "Kaitlin Schuessler", "Kaitlin Shermetta" ],
      "venue" : null,
      "citeRegEx" : "Drager et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Drager et al\\.",
      "year" : 2019
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt" ],
      "venue" : "In Proceedings of the IEEE conference on computer",
      "citeRegEx" : "Fang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Testing the effcacy of part-of-speech information in word completion",
      "author" : [ "Afsaneh Fazly", "Graeme Hirst." ],
      "venue" : "Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods.",
      "citeRegEx" : "Fazly and Hirst.,? 2003",
      "shortCiteRegEx" : "Fazly and Hirst.",
      "year" : 2003
    }, {
      "title" : "Clustering by passing messages between data points",
      "author" : [ "Brendan J Frey", "Delbert Dueck." ],
      "venue" : "science, 315(5814):972–976.",
      "citeRegEx" : "Frey and Dueck.,? 2007",
      "shortCiteRegEx" : "Frey and Dueck.",
      "year" : 2007
    }, {
      "title" : "Text prediction systems: A survey",
      "author" : [ "Nestor Garay-Vitoria", "Julio Abascal." ],
      "venue" : "Universal Access in the Information Society, 4(3):188–203.",
      "citeRegEx" : "Garay.Vitoria and Abascal.,? 2006",
      "shortCiteRegEx" : "Garay.Vitoria and Abascal.",
      "year" : 2006
    }, {
      "title" : "Measuring the performance of a location-aware text prediction system",
      "author" : [ "Luís Filipe Garcia", "Luís Caldas De Oliveira", "David Martins De Matos." ],
      "venue" : "ACM Transactions on Accessible Computing (TACCESS), 7(1):1–29.",
      "citeRegEx" : "Garcia et al\\.,? 2015",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2015
    }, {
      "title" : "Adults with severe aphasia",
      "author" : [ "Kathryn L Garrett." ],
      "venue" : "David R Beukelman and Pat Mirenda, editors, Augmentative and alternative communication for children and adults with complex communication needs, pages 467–504. Paul H. Brookes, Baltimore.",
      "citeRegEx" : "Garrett.,? 2005",
      "shortCiteRegEx" : "Garrett.",
      "year" : 2005
    }, {
      "title" : "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings",
      "author" : [ "He He", "Anusha Balakrishnan", "Mihail Eric", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1704.07130.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "The application of natural language processing to augmentative and alternative communication",
      "author" : [ "D Jeffery Higginbotham", "Gregory W Lesher", "Bryan J Moulton", "Brian Roark." ],
      "venue" : "Assistive Technology, 24(1):14–24.",
      "citeRegEx" : "Higginbotham et al\\.,? 2012",
      "shortCiteRegEx" : "Higginbotham et al\\.",
      "year" : 2012
    }, {
      "title" : "Knowledge-enriched visual storytelling",
      "author" : [ "Chao-Chun Hsu", "Zi-Yuan Chen", "Chi-Yang Hsu", "ChihChia Li", "Tzu-Yuan Lin", "Ting-Hao Huang", "LunWei Ku." ],
      "venue" : "Proceedings of the AAAI Conference on Artifcial Intelligence, volume 34, pages 7952–7960.",
      "citeRegEx" : "Hsu et al\\.,? 2020",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2020
    }, {
      "title" : "Reliability in content analysis: Some common misconceptions and recommendations",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Human communication research, 30(3):411–433.",
      "citeRegEx" : "Krippendorff.,? 2004",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2004
    }, {
      "title" : "A design engineering approach for quantitatively exploring context-aware sentence retrieval for nonspeaking individuals with motor disabilities",
      "author" : [ "Per Ola Kristensson", "James Lilley", "Rolf Black", "Annalu Waller." ],
      "venue" : "Proceedings of the 2020 CHI Con-",
      "citeRegEx" : "Kristensson et al\\.,? 2020",
      "shortCiteRegEx" : "Kristensson et al\\.",
      "year" : 2020
    }, {
      "title" : "Domain-specifc word prediction for augmentative communication",
      "author" : [ "Gregory W Lesher", "Gerard J Rinkus." ],
      "venue" : "Proceedings of the RESNA 2002 Annual Conference.",
      "citeRegEx" : "Lesher and Rinkus.,? 2002",
      "shortCiteRegEx" : "Lesher and Rinkus.",
      "year" : 2002
    }, {
      "title" : "Compansion: From research prototype to practical integration",
      "author" : [ "Kathleen F McCoy", "Christopher A Pennington", "Arlene Luberoff Badman." ],
      "venue" : "Natural Language Engineering, 4(1):73–95.",
      "citeRegEx" : "McCoy et al\\.,? 1998",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 1998
    }, {
      "title" : "Performance of a person with chronic aphasia using personal and contextual pictures in a visual scene display prototype",
      "author" : [ "Miechelle L McKelvey", "Aimee R Dietz", "Karen Hux", "Kristy Weissling", "David R Beukelman." ],
      "venue" : "Journal of Medical Speech Lan-",
      "citeRegEx" : "McKelvey et al\\.,? 2007",
      "shortCiteRegEx" : "McKelvey et al\\.",
      "year" : 2007
    }, {
      "title" : "Impact of personal relevance and contextualization on word-picture matching by people with aphasia",
      "author" : [ "Miechelle L McKelvey", "Karen Hux", "Aimee Dietz", "David R Beukelman." ],
      "venue" : "American Journal of Speech-Language Pathology.",
      "citeRegEx" : "McKelvey et al\\.,? 2010",
      "shortCiteRegEx" : "McKelvey et al\\.",
      "year" : 2010
    }, {
      "title" : "Augmentative and alternative communication devices for aphasia: The emerging role of “smart” mobile devices",
      "author" : [ "Karyn Moffatt", "Golnoosh Pourshahid", "Ronald M Baecker." ],
      "venue" : "Universal Access in the Information Society, 16(1):115–128.",
      "citeRegEx" : "Moffatt et al\\.,? 2017",
      "shortCiteRegEx" : "Moffatt et al\\.",
      "year" : 2017
    }, {
      "title" : "Click on bake to get cookies: Guiding wordfnding with semantic associations",
      "author" : [ "Sonya Nikolova", "Marilyn Tremaine", "Perry R Cook." ],
      "venue" : "Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility, pages 155–",
      "citeRegEx" : "Nikolova et al\\.,? 2010",
      "shortCiteRegEx" : "Nikolova et al\\.",
      "year" : 2010
    }, {
      "title" : "An architecture for data-to-text systems",
      "author" : [ "Ehud Reiter." ],
      "venue" : "Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 07), pages 97–104.",
      "citeRegEx" : "Reiter.,? 2007",
      "shortCiteRegEx" : "Reiter.",
      "year" : 2007
    }, {
      "title" : "Functionally relevant items in the treatment of aphasia (part ii): Further perspectives and specifc tools",
      "author" : [ "Kati Renvall", "Lyndsey Nickels", "Bronwyn Davidson." ],
      "venue" : "Aphasiology, 27(6):651–677.",
      "citeRegEx" : "Renvall et al\\.,? 2013",
      "shortCiteRegEx" : "Renvall et al\\.",
      "year" : 2013
    }, {
      "title" : "High-tech aac and aphasia: Widening horizons? Aphasiology",
      "author" : [ "Mieke van de Sandt-Koenderman" ],
      "venue" : null,
      "citeRegEx" : "Sandt.Koenderman.,? \\Q2004\\E",
      "shortCiteRegEx" : "Sandt.Koenderman.",
      "year" : 2004
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1704.04368.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Soft similarity and soft cosine measure: Similarity of features in vector space model",
      "author" : [ "Grigori Sidorov", "Alexander Gelbukh", "Helena GómezAdorno", "David Pinto." ],
      "venue" : "Computación y Sistemas, 18(3):491– 504.",
      "citeRegEx" : "Sidorov et al\\.,? 2014",
      "shortCiteRegEx" : "Sidorov et al\\.",
      "year" : 2014
    }, {
      "title" : "On generating characteristic-rich question sets for qa evaluation",
      "author" : [ "Yu Su", "Huan Sun", "Brian Sadler", "Mudhakar Srivatsa", "Izzeddin Gür", "Zenghui Yan", "Xifeng Yan." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Su et al\\.,? 2016",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2016
    }, {
      "title" : "PAL: An effort effcient portable communication aid and keyboard emulator",
      "author" : [ "Andrew L Swiffn", "J Adrian Pickering", "John L Arnott", "Alan F Newell." ],
      "venue" : "8th Annual Conference on Rehabilitation Technology, Technology-A Bridge to Indepen-",
      "citeRegEx" : "Swiffn et al\\.,? 1985",
      "shortCiteRegEx" : "Swiffn et al\\.",
      "year" : 1985
    }, {
      "title" : "Natural language generation for augmentative and assistive technologies",
      "author" : [ "Nava Tintarev", "Ehud Reiter", "Rolf Black", "Annalu Waller." ],
      "venue" : "Natural Language Generation in Interactive Systems, pages 252–277. Cambridge University Press.",
      "citeRegEx" : "Tintarev et al\\.,? 2014",
      "shortCiteRegEx" : "Tintarev et al\\.",
      "year" : 2014
    }, {
      "title" : "Personal storytelling: Using natural language generation for children with complex communication needs, in the wild",
      "author" : [ "Nava Tintarev", "Ehud Reiter", "Rolf Black", "Annalu Waller", "Joe Reddington." ],
      "venue" : ". . . International Journal of Human-Computer Studies,",
      "citeRegEx" : "Tintarev et al\\.,? 2016",
      "shortCiteRegEx" : "Tintarev et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluating word prediction: Framing keystroke savings",
      "author" : [ "Keith Trnka", "Kathleen F McCoy." ],
      "venue" : "Proceedings of ACL-08: HLT, Short Papers, pages 261–264.",
      "citeRegEx" : "Trnka and McCoy.,? 2008",
      "shortCiteRegEx" : "Trnka and McCoy.",
      "year" : 2008
    }, {
      "title" : "The effects of word prediction on communication rate for AAC",
      "author" : [ "Keith Trnka", "Debra Yarrington", "John McCaw", "Kathleen F McCoy", "Christopher Pennington." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chap-",
      "citeRegEx" : "Trnka et al\\.,? 2007",
      "shortCiteRegEx" : "Trnka et al\\.",
      "year" : 2007
    }, {
      "title" : "Topic modeling in fringe word prediction for AAC",
      "author" : [ "Keith Trnka", "Debra Yarrington", "Kathleen McCoy", "Christopher Pennington." ],
      "venue" : "Proceedings of the 11th international conference on Intelligent user interfaces, pages 276–278.",
      "citeRegEx" : "Trnka et al\\.,? 2006",
      "shortCiteRegEx" : "Trnka et al\\.",
      "year" : 2006
    }, {
      "title" : "Effect of two layouts on high technology AAC navigation and content location by people with aphasia",
      "author" : [ "Sarah E Wallace", "Karen Hux." ],
      "venue" : "Disability and Rehabilitation: Assistive Technology, 9(2):173–182.",
      "citeRegEx" : "Wallace and Hux.,? 2014",
      "shortCiteRegEx" : "Wallace and Hux.",
      "year" : 2014
    }, {
      "title" : "Telling tales: Unlocking the potential of AAC technologies",
      "author" : [ "Annalu Waller." ],
      "venue" : "International journal of language & communication disorders, 54(2):159– 169.",
      "citeRegEx" : "Waller.,? 2019",
      "shortCiteRegEx" : "Waller.",
      "year" : 2019
    }, {
      "title" : "Sibylle, an assistive communication system adapting to the context and its user",
      "author" : [ "Tonio Wandmacher", "Jean-Yves Antoine", "Franck Poirier", "Jean-Paul Départe." ],
      "venue" : "ACM Transactions on Accessible Computing (TACCESS), 1(1):1–30.",
      "citeRegEx" : "Wandmacher et al\\.,? 2008",
      "shortCiteRegEx" : "Wandmacher et al\\.",
      "year" : 2008
    }, {
      "title" : "An AAC application using speaking partner speech recognition to automatically produce contextually relevant utterances: Objective results",
      "author" : [ "Bruce Wisenburn", "D Jeffery Higginbotham." ],
      "venue" : "Augmentative and alternative communication, 24(2):100–109.",
      "citeRegEx" : "Wisenburn and Higginbotham.,? 2008",
      "shortCiteRegEx" : "Wisenburn and Higginbotham.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "doned because it offers too little communication support relative to the effort required to learn and use (Moffatt et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "attend user’s communication needs in a wide variety of contexts and such that they can fnd words quickly is one of the major challenges (van de Sandt-Koenderman, 2004; Bailey et al., 2006).",
      "startOffset" : 136,
      "endOffset" : 188
    }, {
      "referenceID" : 44,
      "context" : "communication, and not for social interactions involving spontaneous narratives (Waller, 2019).",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Although NLG has been successfully applied in the context of task-oriented dialogs (He et al., 2017), question answering (Su et al.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 36,
      "context" : ", 2017), question answering (Su et al., 2016), text summarization (See et al.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : ", 2016), text summarization (See et al., 2017), and story generation from photograph se-",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "quences (Hsu et al., 2020), it is unclear how these techniques can be adapted to the specifc needs of AAC support (Tintarev et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 38,
      "context" : ", 2020), it is unclear how these techniques can be adapted to the specifc needs of AAC support (Tintarev et al., 2014).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 37,
      "context" : "prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008).",
      "startOffset" : 63,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008).",
      "startOffset" : 63,
      "endOffset" : 183
    }, {
      "referenceID" : 15,
      "context" : "prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008).",
      "startOffset" : 63,
      "endOffset" : 183
    }, {
      "referenceID" : 41,
      "context" : "prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008).",
      "startOffset" : 63,
      "endOffset" : 183
    }, {
      "referenceID" : 40,
      "context" : "prediction, applying n-grams language models on the user input (Swiffn et al., 1985; Garay-Vitoria and Abascal, 2006; Fazly and Hirst, 2003; Trnka et al., 2007; Trnka and McCoy, 2008).",
      "startOffset" : 63,
      "endOffset" : 183
    }, {
      "referenceID" : 25,
      "context" : "Researchers have also explored techniques for improving prediction by including in the language model, some sort of contextual information, such as the topic of conversation (Lesher and Rinkus, 2002; Trnka et al., 2006), the user’s location (Garcia et al.",
      "startOffset" : 174,
      "endOffset" : 219
    }, {
      "referenceID" : 42,
      "context" : "Researchers have also explored techniques for improving prediction by including in the language model, some sort of contextual information, such as the topic of conversation (Lesher and Rinkus, 2002; Trnka et al., 2006), the user’s location (Garcia et al.",
      "startOffset" : 174,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : ", 2006), the user’s location (Garcia et al., 2015), their past utterances (Kristensson et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 24,
      "context" : ", 2015), their past utterances (Kristensson et al., 2020; Copestake, 1997; Wandmacher et al., 2008), or their partner’s",
      "startOffset" : 31,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : ", 2015), their past utterances (Kristensson et al., 2020; Copestake, 1997; Wandmacher et al., 2008), or their partner’s",
      "startOffset" : 31,
      "endOffset" : 99
    }, {
      "referenceID" : 45,
      "context" : ", 2015), their past utterances (Kristensson et al., 2020; Copestake, 1997; Wandmacher et al., 2008), or their partner’s",
      "startOffset" : 31,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "Virtually all commercial text-based high tech AAC devices employ some form of n-gram prediction (Higginbotham et al., 2012).",
      "startOffset" : 96,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "ally have good receptive communication skills and intellectual abilities preserved, and typically desire the ability to communicate complex ideas and share social stories spontaneously, such as describing a recent activity or experience (Garrett, 2005)2.",
      "startOffset" : 237,
      "endOffset" : 252
    }, {
      "referenceID" : 3,
      "context" : "Evidence indicates greater conversational turn-taking with fewer instances of frustration and navigational errors (Brock et al., 2017), and increased lexical retrieval during activity retell (Mooney et al.",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "In consequence, available commercial tools3 depend on human effort planning and programming relevant vocabulary, leading to lack of spontaneous and independent communication, and requiring a great amount of time from caregivers (Drager et al., 2019).",
      "startOffset" : 228,
      "endOffset" : 249
    }, {
      "referenceID" : 9,
      "context" : "The Compansion system (Demasco and McCoy, 1992; McCoy et al., 1998), was one of the frst attempts to apply NLG towards that goal.",
      "startOffset" : 22,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "The Compansion system (Demasco and McCoy, 1992; McCoy et al., 1998), was one of the frst attempts to apply NLG towards that goal.",
      "startOffset" : 22,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : "Automatic generation of storytelling vocabulary has been successfully explored by researchers (Reiter, 2007; Black et al., 2010; Tintarev et al., 2016) to support children with limited memory or with physical and intellectual impairment telling \"how was school today\" to their parents.",
      "startOffset" : 94,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "Automatic generation of storytelling vocabulary has been successfully explored by researchers (Reiter, 2007; Black et al., 2010; Tintarev et al., 2016) to support children with limited memory or with physical and intellectual impairment telling \"how was school today\" to their parents.",
      "startOffset" : 94,
      "endOffset" : 151
    }, {
      "referenceID" : 39,
      "context" : "Automatic generation of storytelling vocabulary has been successfully explored by researchers (Reiter, 2007; Black et al., 2010; Tintarev et al., 2016) to support children with limited memory or with physical and intellectual impairment telling \"how was school today\" to their parents.",
      "startOffset" : 94,
      "endOffset" : 151
    }, {
      "referenceID" : 35,
      "context" : "Sentence similarity is defned as the soft cosine similarity (Sidorov et al., 2014)5 on a bag-of-words representation of the sentences using Word2Vec embeddings, after removing stop words6.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "This strategy was motivated by the fact that soft cosine similarity with Word2Vec was effective for fnding similar sentences on question-answering systems, achieving the best performance at the SemEval2017 Task 3 (Charlet and Damnati, 2017).",
      "startOffset" : 213,
      "endOffset" : 240
    }, {
      "referenceID" : 16,
      "context" : "This step identifes a group of representative sentences and words from the retrieved set by applying the Affnity Propagation7 clustering (Frey and Dueck, 2007)—able to generate clusters with less error than other exemplar-based algorithms and not requiring a predetermined the number of clusters.",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 30,
      "context" : "This strategy was motivated by the fact that word association data was successfully applied in a controlled study to support people with aphasia navigating related words more effectively (Nikolova et al., 2010), and that evidence from cognitive science research indicates that the network formed by associations in SWOW presents a widespread thematic structure, rather than taxonomic, with words strongly associated often occurring in the same situation (e.",
      "startOffset" : 187,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "In the absence of similar AAC generation systems to compare our method to, we created a BASELINE generation formed by a rank of the most frequent words from the Corpus of Contemporary American English (COCA) (Davies, 2009) without stop words.",
      "startOffset" : 208,
      "endOffset" : 222
    }, {
      "referenceID" : 32,
      "context" : "We adopted this baseline because current AAC tools are commonly built on word usage frequency data (Renvall et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "82, indicating strong agreement between raters (Krippendorff, 2004).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "The low level of social participation commonly observed among people with aphasia, combined with the rate-limited nature of AAC, would require feld experiments lasting an impractical amount of time to produce suffcient data to comprehensively explore possible combinations of parameters (Kristensson et al., 2020).",
      "startOffset" : 287,
      "endOffset" : 313
    } ],
    "year" : 2021,
    "abstractText" : "Research on the application of NLP in symbolbased Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in recounting their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs. In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly. We conclude by discussing how our fndings provide insights for system optimization and usage.",
    "creator" : "LaTeX with hyperref"
  }
}