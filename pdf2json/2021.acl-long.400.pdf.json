{
  "name" : "2021.acl-long.400.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models",
    "authors" : [ "Yichun Yin", "Cheng Chen", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Qun Liu" ],
    "emails" : [ "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "qun.liu@huawei.com", "c-chen19@mails.tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5146–5157\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5146"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), have become prevalent in natural language processing. To improve model performance, most PLMs (e.g. ELECTRA (Clark et al., 2019) and GPT-2/3 (Radford et al., 2019;\n*Contribution during internship at Noah’s Ark Lab. 1Our code implementation and pre-trained models are available at https://github.com/huawei-noah/ Pretrained-Language-Model .\nBrown et al., 2020)) follow the default rule of hyper-parameter setting2 in BERT to scale up their model sizes. Due to its simplicity, this rule has been widely used and can help large PLMs obtain promising results (Brown et al., 2020).\nIn many industrial scenarios, we need to deploy PLMs on resource-constrained devices, such as smartphones and servers with limited computation power. Due to the expensive computation and slow inference speed, it is usually difficult to deploy PLMs such as BERT (12/24 layers, 110M/340M parameters) and GPT-2 (48 layers, 1.5B parameters) at their original scales. Therefore, there is an urgent need to develop PLMs with smaller sizes which have lower computation cost and inference latency. In this work, we focus on a specific type of efficient PLMs, which we define to have inference time less than 1/4 of BERT-base.3\n2The default rule is dm = dq|k|v = 1/4df , which means the dimension of hidden vector dm is equal to the dimensions of query/key/value vector dq|k|v and a quarter of the intermediate size df in feed-forward networks.\n3We empirically find that being at least 4x faster is a basic requirement in practical deployment environment.\nAlthough, there have been quite a few work using knowledge distillation to build small PLMs (Sanh et al., 2019; Jiao et al., 2020b; Sun et al., 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al., 2015; Romero et al., 2014) and do not study the effect of architecture hyper-parameter settings on model performance. Recently, neural architecture search and hyper-parameter optimization (Tan and Le, 2019; Han et al., 2020) have been widely explored in machine learning, mostly in computer vision, and have been proven to find better designs than heuristic ones. Inspired by this research, one problem that naturally arises is can we find better settings of hyper-parameters4 for efficient PLMs?\nIn this paper, we argue that the conventional hyper-parameter setting is not best for efficient PLMs (as shown in Figure 1) and introduce a method to automatically search for the optimal hyper-parameters for specific latency constraints. Pre-training efficient PLMs is inevitably resourceconsuming (Turc et al., 2019). Therefore, it is infeasible to directly evaluate millions of architectures. To tackle this challenge, we introduce the one-shot Neural Architecture Search (NAS) (Brock et al., 2018; Cai et al., 2018; Yu et al., 2020) to perform the automatic hyper-parameter optimization on efficient PLMs, named as AutoTinyBERT. Specifically, we first use the one-shot learning to obtain a big SuperPLM, which can act as proxies for all potential sub-architectures. Proxy means that when evaluating an architecture, we only need to extract the corresponding sub-model from the SuperPLM, instead of training the model from scratch. SuperPLM helps avoid the time-consuming pre-training process and makes the search process efficient. To make SuperPLM more effective, we propose practical techniques including the head sub-matrix extraction and efficient batch-wise training, and particularly limit the search space to the models with identical layer structure. Furthermore, by using SuperPLM, we leverage search algorithm (Xie and Yuille, 2017; Wang et al., 2020a) to find hyperparameters for various latency constraints.\nIn the experiments, in addition to the pre-training setting (Devlin et al., 2019), we also consider the setting of task-agnostic BERT distillation (Sun et al., 2020) that pre-trains with the loss of knowledge distillation, to build efficient PLMs. Exten-\n4We abbreviate the phrase architecture hyper-parameter as hyper-parameter in the paper.\nsive results show that in pre-training setting, AutoTinyBERT not only consistently outperforms the BERT with conventional hyper-parameters under different latency constraints, but also outperforms NAS-BERT based on neural architecture search. In task-agnostic BERT distillation, AutoTinyBERT outperforms a series of existing SOTA methods of DistilBERT, TinyBERT and MobileBERT.\nOur contributions are three-fold: (1) we explore the problem of how to design hyper-parameters for efficient PLMs and introduce an effective and efficient method: AutoTinyBERT; (2) we conduct extensive experiments in both scenarios of pretraining and knowledge distillation, and the results show our method consistently outperforms baselines under different latency constraints; (3) we summarize a fast rule and it develops an AutoTinyBERT for a specific constraint with even about 50% of the training time of a conventional PLM."
    }, {
      "heading" : "2 Preliminary",
      "text" : "Before presenting our method, we first provide some details about the Transformer layer (Vaswani et al., 2017) to introduce the conventional hyperparameter setting. Transformer layer includes two sub-structures: the multi-head attention (MHA) and the feed-forward network (FFN).\nFor clarity, we show the MHA as a decomposable structure, where the MHA includes h individual and parallel self-attention modules (called heads). The output of MHA is obtained by summing the output of all heads. Specifically, each head is represented by four main matrices W qi ∈ Rd\nm×dq/h, W ki ∈ Rd m×dk/h, W vi ∈ Rd m×dv/h and W oi ∈ Rd v/h×do , and takes the hidden states5 H ∈ Rl×dm of the previous layer as input. The output of MHA is given by the following formulas:\nQi,Ki,Vi = HW q i ,HW k i ,HW v i\nATTN(Qi,Ki,Vi) = softmax( QiKi T√ dq|k/h )Vi\nHi = ATTN(Qi,Ki,Vi)W o i\nMHA(H) = h∑ i=1 Hi,\n(1) where Qi ∈ Rl×d q/h, Ki ∈ Rl×d k/h, Vi ∈ Rl×d v/h are obtained by the linear transformations of W qi , W k i , W v i respectively. ATTN(·) is the\n5We omitted the batch size for simplicity.\nHuawei Confidential63\nscaled dot-product attention operation. Then output of each head is transformed to Hi ∈ Rl×d o by W oi . Finally, outputs of all heads are summed as the output of MHA. In addition, residual connection and layer normalization are added on top of MHA to get the final output:\nHMHA = LayerNorm(H +MHA(H)). (2)\nIn the conventional setting of the hyper-parameters in BERT, all dimensions of matrices are the same as the dimension of the hidden vector, namely, dq|k|v|o=dm. In fact, there are only two requirements of dq=dk and do=dm that must be satisfied because of the dot-product attention operation in MHA and the residual connection.\nTransformer layer also contains an FFN that is stacked on the MHA, that is:\nHFFN = max(0,HMHAW 1+b1)W 2+b2, (3)\nwhere W 1 ∈ Rdm×df , W 2 ∈ Rdf×dm , b1 ∈ Rd f and b2 ∈ Rd m\n. Similarly, there are modules of residual connection and layer normalization on top of FFN. In the original Transformer, df=4dm is assumed. Thus, we conclude that the conventional hyper-parameter setting follows the rule of {dq|k|v|o=dm, df=4dm}."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Statement",
      "text" : "Given a constraint of inference time, our goal is to find an optimal configuration of architecture hyperparameters αopt built with which PLM can achieve the best performances on downstream tasks. This optimization problem is formulated as:\nαopt =argmax α∈ A Perf(α, θ∗α),\ns.t. θ∗α =argmin θ\nLα(θ), Lat(α) ≤ T, (4)\nwhere T is a specific time constraint, A refers to the set of all possible architectures (i.e., combination of hyper-parameters), Lat(·) is a latency evaluator, Lα(·) denotes the loss function of PLMs with the hyper-parameter α, and θ is the corresponding model parameters. We aim to search an optimal architecture for efficient PLM (Lat(α) < 1/4× Lat(BERTbase))."
    }, {
      "heading" : "3.2 Overview",
      "text" : "A straightforward way to get the optimal architecture is to enumerate all possible architectures. However, it is infeasible because each trial involves a time-consuming pre-training process. Therefore, we introduce one-shot NAS to search αopt, as shown in the Figure 2. The proposed method includes three stages: (1) the one-shot learning to obtain SuperPLM that can be used as the proxy for\nHuawei Confidential63\nvarious architectures; (2) the search process for the optimal hyper-parameters; (3) the further training with the optimal architectures and corresponding sub-models. In the following sections, we first introduce the search space, which is the basis for the one-shot learning and search process. Then we present the three stages respectively."
    }, {
      "heading" : "3.3 Search Space",
      "text" : "From the Section 2, we know that the conventional hyper-parameter setting is: {dq|k|v|o=dm, df=4dm}, which is widely-used in PLMs. The architecture of a PLM is parameterized as: α = {lt, dm, dq, dk, dv, df , do}, which is subjected to the constraints {dq = dk, do = dm}. Let lt denote the layer number and d∗ refer to different dimensions in the Transformer layer. We denote the search space of lt and d∗ as Alt and Ad∗ respectively. The overall search space is: A = Alt ×Adm|o ×Adq|k ×Adv ×Adf .\nIn this work, we only consider the case of identical structure for each Transformer layer, instead of the non-identical Transformer (Wang et al., 2020a) or other heterogeneous modules (Xu et al., 2021) (such as convolution units). It has two advantages: (1) it reduces an exponential search space of O(\n∏ ∗ |Ad∗ ||Alt |) to a linear search space of\nO( ∏ ∗ |Ad∗ ||Alt |), greatly reducing the number of possible architectures in SuperPLM training and the exploration space in the search process. It leads to a more efficient search process. (2) An identical and homogeneous structure is in fact more friendly to hardware and software frameworks, e.g., Hugging Face Transformer (Wolf et al., 2020). With a\nAlgorithm 1 Batch-wise training for SuperPLM Input: All possible candidates A; Training thread\n(GPU) number N ; Large-scale unsupervised dataset D; Training epochs E. Sample times M per batch. SuperPLM parameters (θ).\nOutput: Trained SuperPLM (θ) 1: for t = 1→ E do 2: for batch in D do 3: Divide batch into N sub batches 4: Distribute sub batches to N threads 5: Clear the gradients 6: for m = 1→M do 7: Sample N sub-models from A 8: Distribute sub-models to threads 9: Calculate gradients in each thread 10: end for 11: Update the θ with the average gradients 12: end for 13: end for\nfew changes, we can use the original code to use AutoTinyBERT, as shown in Appendix A."
    }, {
      "heading" : "3.4 One-shot Learning for SuperPLM",
      "text" : "We employ the one-shot learning (Brock et al., 2018; Yu et al., 2020) to obtain a SuperPLM whose sub-models can act as the proxy for PLMs trained from scratch. The configurations of SuperPLM in this work are lt=8, dm|q|k|v|o=768, and df=3072. In each step of the one-shot learning, we train several sub-models randomly sampled from SuperPLM to make their performance close to the models trained from scratch. Although the sampling/search space has been reduced to linear complexity, there are still more than 10M possible substructures in SuperPLM (the details are shown in the Appendix B). Therefore, we introduce an effective batch-wise training method to cover the sub-models as much as possible. Specifically, in parallel training, we first divide each batch into multiple sub-batches and distribute them to different threads as parallel training data. Then, we sample several sub-models on each thread for training and merge the gradients of all threads to update the SuperPLM parameters. We illustrate the training process in the Algorithm 1.\nGiven a specific hyper-parameter setting α = {lt, dm, dq, dk, dv, df , do}, we get a sub-model from SuperPLM by the depth-wise and widthwise extraction. Specifically, we first perform the depth-wise extraction that extracts the first lt Trans-\nformer layers from SuperPLM, and then perform the width-wise extraction that extracts bottom-left sub-matrices from original matrices. For MHA, we apply two strategies illustrated in Figure 3 : (1) keep the dimension of each head same as SuperPLM, and extract some of the heads; (2) keep the head number same as SuperPLM, and extract subdimensions from each head. The first strategy is the standard one and we use it for pre-training and the second strategy is used for task-agnostic distillation because that attention-based distillation (Jiao et al., 2020b) requires the student model to have the same head number as the teacher model."
    }, {
      "heading" : "3.5 Search Process",
      "text" : "In the search process, we adopt an evolutionary algorithm (Xie and Yuille, 2017; Jiao et al., 2020a), where Evolver and Evaluator interact with each other to evolve better architectures. Our search process is efficient, as shown in the Section 4.4.\nSpecifically, Evolver firstly samples a generation of architectures from A. Then Evaluator extracts the corresponding sub-models from SuperPLM and ranks them based on their performance on tasks of SQuAD and MNLI. The architectures with the high performance are chosen as the winning architectures and Evolver performs the mutation Mut(·) operation on the winning ones to produce a new generation of architectures. This process is conducted repeatedly. Finally, we choose several architectures with the best performance for further\ntraining. We use Lat(·) to predict the latency of the candidates to filter out the candidates that do not meet the latency constraint. Lat(·) is built with the method by Wang et al. (2020a), which first samples about 10k architectures from A and collects their inference time on target devices, and then uses a feed-forward network to fit the data. For more details of evolutionary algorithm, please refer to Appendix C. Note that we can use different methods in search process, such as random search and more advanced search, which is left as future work."
    }, {
      "heading" : "3.6 Further Training",
      "text" : "The search process produces top several architectures, with which we extract these corresponding sub-models from SuperPLM and continue training them using the pre-training or KD objectives."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "Dataset and Fine tuning. We conduct the experiments on the GLUE benchmark (Wang et al., 2018) and SQuADv1.1 (Rajpurkar et al., 2016). For GLUE, we set the batch size to 32, choose the learning rate from {1e-5, 2e-5, 3e-5} and choose the epoch number from {4, 5, 10}. For SQuADv1.1, we set the batch size to 16, the learning rate to 3e5 and the epoch number to 4. The details for all datasets are displayed in Appendix D.\nAutoTinyBERT. Both the one-shot and further\ntraining use BooksCorpus (Zhu et al., 2015) and English Wikipedia as training data. The settings for one-shot training are: peak learning rate of 1e-5, warmup rate of 0.1, batch size of 256 and 5 running epochs. Further training follows the same setting as the one-shot training except for the warmup rate of 0. In the batch-wise training algorithm 1, the thread number N is set to 16, the sample times M per batch is set to 3, and epoch number E is set to 5. We train the SuperPLM with an architecture of {lt=8, dm|q|k|v|o=768, df=3072}. In the search process, Evolver performs 4 iterations with a population size of 25 and it chooses top three architectures for further training. For more details of the sampling/search space and evolutionary algorithm, please refer to Appendix B and C.\nWe train AutoTinyBERT in both ways of pretraining (Devlin et al., 2019) and task-agnostic BERT distillation (Sun et al., 2020). For taskagnostic distillation, we follow the first stage of TinyBERT (Jiao et al., 2020b) except that only the last-layer loss is used, and ELECTRAbase (Clark et al., 2019) is used as the teacher model.\nBaselines. For the pre-training baselines, we include PF (Pre-training + Fine-tuning, proposed by Turc et al. (2019)), BERT-S* (BERT under several hyper-parameter configurations), and NASBERT (Xu et al., 2021). Both PF and BERTS* follow the conventional setting rule of hyper-\nparameters. BERT-S* uses the training setting: peak learning rate of 1e-5, warmup rate of 0.1, batch size of 256 and 10 running epochs. NASBERT searches the architecture built on the nonidentical layer and heterogeneous modules. For the distillation baselines, we compare some typical methods, including DistilBERT, BERT-PKD, TinyBERT, MiniLM, and MobileBERT. The first four methods use the conventional architectures. MobileBERT is equipped with a bottleneck structure and a carefully designed balance between MHA and FFN. We also consider BERT-KD-S*, which use the same training setting of BERT-S*, except for the loss of knowledge distillation. BERT-KDS* also uses ELECTRAbase as the teacher model."
    }, {
      "heading" : "4.2 Results and Analysis",
      "text" : "The experiment is conducted under different latency constraints that are from 4× to 30× faster than the inference of BERTbase. The results of pretraining and task-agnostic distillation are shown in the Table 1 and Table 2 respectively.\nWe observe that in the settings of the pre-training and knowledge distillation, the performance gap of different models with similar inference time is obvious, which shows the necessity of architecture optimization for efficient PLMs. In the Table 1, some observations are: (1) the architecture optimization methods of AutoTinyBERT and NAS-BERT outperform both BERT and PF that use the default\narchitecture hyper-parameters; (2) our method outperforms NAS-BERT that is built with the nonidentical layer and heterogeneous modules, which shows that the proposed method is effective for the architecture search of efficient PLMs. In the Table 2, we observe that: (1) our method consistently outperforms the conventional structure in all the speedup constraints; (2) our method outperforms the classical distillation methods (e.g., BERT-PKD, DistilBERT, TinyBERT, and MiniLM) that use the conventional architecture. Moreover, AutoTinyBERT achieves comparable results with MobileBERT, and its inference speed is 1.5× faster."
    }, {
      "heading" : "4.3 Ablation Study of One-shot Learning",
      "text" : "We demonstrate the effectiveness of one-shot learning by comparing the performance of one-shot model and stand-alone trained model on the given architectures. We choose 16 architectures and their corresponding PF models6 as the evaluation benchmark. The pairwise accuracy is used as a metric to indicate the ranking correction between the architectures under one-shot training and the ones under stand-alone full training (Luo et al., 2019) and its formula is described in Appendix E.\nWe do the ablation study to analyze the effect of proposed identical layer structure (ILS), MHA sub-matrix extraction (SME) and effective batchwise learning (EBL) on SuperPLM learning. More-\n6The first 16 models https://github.com/ google-research/bert from 2L128D to 8L768D.\nover, we introduce HAT (Wang et al., 2020a), as a baseline of one-shot learning. HAT focuses on the search space of non-identical layer structures. The results are displayed in Table 3 and Figure 4.\nIt can be seen from the figure that compared with stand-alone trained models, the HAT baseline has a significant performance gap, especially in small sizes. Both ILS and SME benefit the one-shot learning for large and medium-sized models. When further combined with EBL, SuperPLM can obtain similar or even better results than stand-alone trained models of small sizes and perform close to stand-alone trained models of big sizes. The results of the table show that: (1) the proposed techniques have positive effects on SuperPLM learning, and EBL brings a significant improvement on a challenging task of SQuAD; (2) SuperPLM achieves a high pairwise accuracy of 96.7% which indicates that the proposed SuperPLM can be a good proxy model for the search process; (3) the performance of SuperPLM is still a little worse than the standalone trained model and we need to do the further training to boost the performance."
    }, {
      "heading" : "4.4 Fast Development of Efficient PLM",
      "text" : "In this section, we explore an effective setting rule of hyper-parameters based on the obtained architectures and also discuss the computation cost of the development of efficient PLM. The conventional and new architectures are displayed in Table 4. We observe that AutoTinyBERT follows an obvious rule (except the S3 model) in the speedup constraints that are from 4× to 30×. The rule is summarized as: {1.6dm ≤ df ≤ 1.9dm, 0.7dm ≤ dq|k|v ≤ 1.0dm}.\nWith the above rule, we propose a faster way to build efficient PLM, denoted as AutoTinyBERTFast. Specifically, we first obtain the candidates by the rule, and then select αopt from the candidates. We observe the fact that the candidates of the same layer number seem to have similar shapes and we assume that they have similar performance. Therefore, we only need to test one architecture at each layer number and choose the best one as αopt.\nTo demonstrate the effectiveness of the proposed method, we evaluate these methods at a new speedup constraint of about 10× under the pre-training setting. The results are shown in Table 5. We find AutoTinyBERT is efficient and its development time is twice that of the conventional method (BERT) and the result is improved by about 1.8%. AutoTinyBERT-Fast achieves a competitive score of 77.6 by only about 50% of BERT training time. In addition to the proposed search method and fast building rule, one reason for the high efficiency of AutoTinyBERT is that the initialization of SuperPLM helps the model to achieve 2× the convergence speedup, as illustrated in Figure 5."
    }, {
      "heading" : "5 Related Work",
      "text" : "Efficient PLMs with Tiny sizes. There are two widely-used methods for building efficient PLMs:\npre-training and model compression. Knowledge distillation (KD) (Hinton et al., 2015; Romero et al., 2014) is the most widely studied technique in PLM compression, which uses a teacher-student framework. The typical distillation studies include DistilBERT (Sanh et al., 2019), BERT-PKD (Sun et al., 2019), MiniLM (Wang et al., 2020b), MobileBERT (Sun et al., 2020), MiniBERT (Tsai et al., 2019) and ETD (Chen et al., 2021). In addition to KD, the techniques of pruning (Han et al., 2016; Hou et al., 2020), quantization (Shen et al., 2020; Zhang et al., 2020; Wang et al., 2020c) and parameter sharing (Lan et al., 2019) introduced for PLM compression. Our method is orthogonal to the building method of efficient PLM and is trained under the settings of pre-training and task-agnostic BERT distillation, which can be used by direct finetuning.\nNAS for NLP. NAS is extensively studied in computer vision (Tan and Le, 2019; Tan et al., 2020), but relatively little studied in the natural language processing. Evolved Transformer (So et al., 2019) and HAT (Wang et al., 2020a) search architecture for Transformer-based neural machine translation. For BERT distillation, AdaBERT (Chen et al., 2020) focuses on searching the architecture in the fine-tuning stage and relies on data augmentation to improve its performance. schuBERT (Khetan and Karnin, 2020) obtains the optimal structures of PLM by a pruning method. A work similar to ours is NAS-BERT (Xu et al., 2021). It proposes some techniques to tackle the challenging exponential search space of non-identical layer structure and heterogeneous modules. Our method adopts a linear search space and introduces several practical techniques for SuperPLM training. Moreover, our method is efficient in terms of computation cost and the obtained PLMs are easy to use."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose an effective and efficient method AutoTinyBERT to search for the optimal architecture hyper-parameters of efficient PLMs. We evaluate the proposed method in the scenarios of both the pre-training and task-agnostic BERT distillation. The extensive experiments show that AutoTinyBERT can consistently outperform the baselines under different latency constraints. Furthermore, we develop a fast development rule for efficient PLMs which can build an AutoTinyBERT model even with less training time of a conventional one."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all the anonymous reviewers for their valuable comments. We thank MindSpore7 for the partial support of this work, which is a new deep learning computing framework."
    }, {
      "heading" : "A Code Modifications for AutoTinyBERT.",
      "text" : "We modify the original code8 to load AutoTinyBERT model and present the details of code modifications in the Figure B.1. We assume that dq/k = dv, and more complicated setting is that dv can be different with dq/k, we can do corresponding changes based on the given modifications."
    }, {
      "heading" : "B Search Space of Architecture Hyper-parameters.",
      "text" : "We has trained two SuperPLMs with a architecture of {lt=8, dm/q/k/v=768, df=3072} to cover the two scenarios of building efficient PLMs (pretraining and task-agnostic BERT distillation). The sampling space in the SuperPLM training is the same as the search space in the search process, as shown in the Table B.1. It can be inferred from the table that the search spaces of the pre-training setting and the knowledge distillation setting are about 46M and 10M, respectively."
    }, {
      "heading" : "C Evolutionary Algorithm.",
      "text" : "We give a detailed description of evolutionary algorithm in Algorithm 2."
    }, {
      "heading" : "D Hyper-parameters for Fine-Tuning.",
      "text" : "Fine-tuning hyper-parameters of GLUE benchmark and SQuAD are displayed in Table D.1. AutoTiny-\n8https://github.com/huggingface/ transformers\nBERT and baselines follow the same settings."
    }, {
      "heading" : "E Pairwise Accuracy.",
      "text" : "We denote a set of architectures {α1, α2, ..., αn} as Aeva and evaluate SuperPLM on this set. The pairwise accuracy is formulated as bellow: ∑ α1∈ Aeva, α2 ∈ Aeva 1f(α1)≥f(α2)1s(α1)≥s(α2)∑\nα1∈ Aeva, α2 ∈ Aeva 1 ,\n(5) where 1 is the 0-1 indicator function, f(∗) and s(∗) refer to the performance of one-shot model and stand-alone trained model respectively."
    }, {
      "heading" : "F More details for Fast Development of efficient PLM.",
      "text" : "We present the detailed results and architecture hyper-parameters for fast development of efficient PLM in Table F.1.\nAlgorithm 2 The Evolutionary Algorithm 1: Input: the number of generations T = 4, the number of archtectures αs in each generation S = 25,\nthe mutation Mut(∗) probability pm = 1/2, the exploration probability pe = 1/2. 2: Sample first generation G1 from A, and Evoluator produces its performance V1. 3: for t = 2, 3 · · · , T do 4: Gt ← {} 5: while |Gt| < S do 6: Sample one architecture: α with a Russian roulette process on Gt−1 and Vt−1. 7: With probability pm, do Mut(∗) for α. 8: With probability pe, sample a new architecture from A. 9: Append the newly generated architectures into Gt.\n10: end while 11: Evaluator obtains Vt for Gt. 12: end for 13: Output: Output the αopt with best performance in the above process.\nModel Speedup SQuAD SST-2 MNLI MRPC CoLA QNLI QQP STS-B RTE Score\nBERT-S54−384−1536−6−384 9.3× 78.5 86.1 76.8 83.1 35.5 84.6 87.5 86.9 65.7 76.0 AutoTinyBERT-S55−450−636−6−384 10.8× 79.7 89.1 78.3 84.6 39.0 85.9 88.2 87.4 68.7 77.8 AutoTinyBERT-Fast-S55−432−720−6−384 10.3× 80.0 88.2 77.9 84.6 37.7 86.1 88.0 87.3 68.7 77.6\nTable F.1: Detailed results for fast development of efficient PLM."
    } ],
    "references" : [ {
      "title" : "Smash: One-shot model architecture search through hypernetworks",
      "author" : [ "Andrew Brock", "Theo Lim", "JM Ritchie", "Nick Weston." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Brock et al\\.,? 2018",
      "shortCiteRegEx" : "Brock et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are few-shot learners. NeurIPS",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Proxylessnas: Direct neural architecture search on target task and hardware",
      "author" : [ "Han Cai", "Ligeng Zhu", "Song Han." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Cai et al\\.,? 2018",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "Extract then distill: Efficient and effective task-agnostic bert distillation",
      "author" : [ "Cheng Chen", "Yichun Yin", "Lifeng Shang", "Zhi Wang", "Xin Jiang", "Xiao Chen", "Qun Liu" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Adabert: Taskadaptive bert compression with differentiable neural architecture search",
      "author" : [ "Daoyuan Chen", "Yaliang Li", "Minghui Qiu", "Zhen Wang", "Bofang Li", "Bolin Ding", "Hongbo Deng", "Jun Huang", "Wei Lin", "Jingren Zhou." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pre-training text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V Le", "Christopher D Manning." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "J. Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT. 7MindSpore. https://www.mindspore.cn/",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Model rubik’s cube: Twisting resolution, depth and width for tinynets",
      "author" : [ "Kai Han", "Yunhe Wang", "Qiulin Zhang", "Wei Zhang", "Chunjing Xu", "Tong Zhang." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Han et al\\.,? 2016",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "J. Dean." ],
      "venue" : "ArXiv, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynabert: Dynamic bert with adaptive width and depth",
      "author" : [ "Lu Hou", "Zhiqi Huang", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Qun Liu." ],
      "venue" : "NeurIPS, 33.",
      "citeRegEx" : "Hou et al\\.,? 2020",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving task-agnostic bert distillation with layer mapping search",
      "author" : [ "Xiaoqi Jiao", "Huating Chang", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:2012.06153.",
      "citeRegEx" : "Jiao et al\\.,? 2020a",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "EMNLP: Findings.",
      "citeRegEx" : "Jiao et al\\.,? 2020b",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "schubert: Optimizing elements of bert",
      "author" : [ "Ashish Khetan", "Zohar Karnin." ],
      "venue" : "ACL.",
      "citeRegEx" : "Khetan and Karnin.,? 2020",
      "shortCiteRegEx" : "Khetan and Karnin.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Balanced one-shot neural architecture optimization",
      "author" : [ "Renqian Luo", "Tao Qin", "Enhong Chen." ],
      "venue" : "arXiv preprint arXiv:1909.10815.",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.6550.",
      "citeRegEx" : "Romero et al\\.,? 2014",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Q-bert: Hessian based ultra low precision quantization of bert",
      "author" : [ "Sheng Shen", "Zhen Dong", "Jiayu Ye", "Linjian Ma", "Zhewei Yao", "Amir Gholami", "Michael W Mahoney", "Kurt Keutzer." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "The evolved transformer",
      "author" : [ "David So", "Quoc Le", "Chen Liang." ],
      "venue" : "ICML.",
      "citeRegEx" : "So et al\\.,? 2019",
      "shortCiteRegEx" : "So et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "EMNLP-IJCNLP.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "author" : [ "Mingxing Tan", "Quoc Le." ],
      "venue" : "ICML.",
      "citeRegEx" : "Tan and Le.,? 2019",
      "shortCiteRegEx" : "Tan and Le.",
      "year" : 2019
    }, {
      "title" : "Efficientdet: Scalable and efficient object detection",
      "author" : [ "Mingxing Tan", "Ruoming Pang", "Quoc V Le." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Tan et al\\.,? 2020",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2020
    }, {
      "title" : "Small and practical bert models for sequence labeling",
      "author" : [ "Henry Tsai", "Jason Riesa", "Melvin Johnson", "Naveen Arivazhagan", "Xin Li", "Amelia Archer." ],
      "venue" : "EMNLP-IJCNLP.",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Well-read students learn better: On the importance of pre-training compact models",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1908.08962.",
      "citeRegEx" : "Turc et al\\.,? 2019",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hat: Hardware-aware transformers for efficient natural language processing",
      "author" : [ "Hanrui Wang", "Zhanghao Wu", "Zhijian Liu", "Han Cai", "Ligeng Zhu", "Chuang Gan", "Song Han." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Structured pruning of large language models",
      "author" : [ "Ziheng Wang", "Jeremy Wohlwend", "Tao Lei." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2020c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "EMNLP: System Demonstrations.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Genetic cnn",
      "author" : [ "Lingxi Xie", "Alan Yuille." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Xie and Yuille.,? 2017",
      "shortCiteRegEx" : "Xie and Yuille.",
      "year" : 2017
    }, {
      "title" : "Task-agnostic and adaptive-size bert compression",
      "author" : [ "Jin Xu", "Xu Tan", "Renqian Luo", "Kaitao Song", "Li Jian", "Tao Qin", "Tie-Yan Liu." ],
      "venue" : "openreview.",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bignas: Scaling up neural architecture search with big single-stage models",
      "author" : [ "Jiahui Yu", "Pengchong Jin", "Hanxiao Liu", "Gabriel Bender", "Pieter-Jan Kindermans", "Mingxing Tan", "Thomas Huang", "Xiaodan Song", "Ruoming Pang", "Quoc Le." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Ternarybert: Distillation-aware ultra-low bit bert",
      "author" : [ "Wei Zhang", "Lu Hou", "Yichun Yin", "Lifeng Shang", "Xiao Chen", "Xin Jiang", "Qun Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : ", the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT (Devlin et al., 2019).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Pre-trained language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : ", 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 37,
      "context" : ", 2019) and XLNet (Yang et al., 2019), have become prevalent in natural language processing.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "ELECTRA (Clark et al., 2019) and GPT-2/3 (Radford et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "Under the same speedup constraint, our method outperforms both the default hyper-parameter setting of BERT (Devlin et al., 2019), PF (Turc et al.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 28,
      "context" : ", 2019), PF (Turc et al., 2019)) and NASBERT (Xu et al.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Due to its simplicity, this rule has been widely used and can help large PLMs obtain promising results (Brown et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "5147 Although, there have been quite a few work using knowledge distillation to build small PLMs (Sanh et al., 2019; Jiao et al., 2020b; Sun et al., 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al.",
      "startOffset" : 97,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "5147 Although, there have been quite a few work using knowledge distillation to build small PLMs (Sanh et al., 2019; Jiao et al., 2020b; Sun et al., 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al.",
      "startOffset" : 97,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : ", 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al., 2015; Romero et al., 2014) and do not study the effect of architecture hyper-parameter settings on model performance.",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : ", 2019, 2020), all of them focus on the application of distillation techniques (Hinton et al., 2015; Romero et al., 2014) and do not study the effect of architecture hyper-parameter settings on model performance.",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "Recently, neural architecture search and hyper-parameter optimization (Tan and Le, 2019; Han et al., 2020) have been widely explored in machine learning, mostly in computer vision, and have been proven to find better designs than heuristic ones.",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Recently, neural architecture search and hyper-parameter optimization (Tan and Le, 2019; Han et al., 2020) have been widely explored in machine learning, mostly in computer vision, and have been proven to find better designs than heuristic ones.",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "Pre-training efficient PLMs is inevitably resourceconsuming (Turc et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "To tackle this challenge, we introduce the one-shot Neural Architecture Search (NAS) (Brock et al., 2018; Cai et al., 2018; Yu et al., 2020) to perform the automatic hyper-parameter optimization on efficient PLMs, named as AutoTinyBERT.",
      "startOffset" : 85,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "To tackle this challenge, we introduce the one-shot Neural Architecture Search (NAS) (Brock et al., 2018; Cai et al., 2018; Yu et al., 2020) to perform the automatic hyper-parameter optimization on efficient PLMs, named as AutoTinyBERT.",
      "startOffset" : 85,
      "endOffset" : 140
    }, {
      "referenceID" : 38,
      "context" : "To tackle this challenge, we introduce the one-shot Neural Architecture Search (NAS) (Brock et al., 2018; Cai et al., 2018; Yu et al., 2020) to perform the automatic hyper-parameter optimization on efficient PLMs, named as AutoTinyBERT.",
      "startOffset" : 85,
      "endOffset" : 140
    }, {
      "referenceID" : 35,
      "context" : "Furthermore, by using SuperPLM, we leverage search algorithm (Xie and Yuille, 2017; Wang et al., 2020a) to find hyperparameters for various latency constraints.",
      "startOffset" : 61,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Furthermore, by using SuperPLM, we leverage search algorithm (Xie and Yuille, 2017; Wang et al., 2020a) to find hyperparameters for various latency constraints.",
      "startOffset" : 61,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "In the experiments, in addition to the pre-training setting (Devlin et al., 2019), we also consider the setting of task-agnostic BERT distillation (Sun et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : ", 2019), we also consider the setting of task-agnostic BERT distillation (Sun et al., 2020) that pre-trains with the loss of knowledge distillation, to build efficient PLMs.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "Before presenting our method, we first provide some details about the Transformer layer (Vaswani et al., 2017) to introduce the conventional hyperparameter setting.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 31,
      "context" : "In this work, we only consider the case of identical structure for each Transformer layer, instead of the non-identical Transformer (Wang et al., 2020a) or other heterogeneous modules (Xu et al.",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 36,
      "context" : ", 2020a) or other heterogeneous modules (Xu et al., 2021) (such as convolution units).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "We employ the one-shot learning (Brock et al., 2018; Yu et al., 2020) to obtain a SuperPLM whose sub-models can act as the proxy for PLMs trained from scratch.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : "We employ the one-shot learning (Brock et al., 2018; Yu et al., 2020) to obtain a SuperPLM whose sub-models can act as the proxy for PLMs trained from scratch.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : "†denotes that the results are taken from (Xu et al., 2021) and ‡denotes that the results are obtained by fine-tuning the released models.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "The first strategy is the standard one and we use it for pre-training and the second strategy is used for task-agnostic distillation because that attention-based distillation (Jiao et al., 2020b) requires the student model to have the same head number as the teacher model.",
      "startOffset" : 175,
      "endOffset" : 195
    }, {
      "referenceID" : 35,
      "context" : "In the search process, we adopt an evolutionary algorithm (Xie and Yuille, 2017; Jiao et al., 2020a), where Evolver and Evaluator interact with each other to evolve better architectures.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "In the search process, we adopt an evolutionary algorithm (Xie and Yuille, 2017; Jiao et al., 2020a), where Evolver and Evaluator interact with each other to evolve better architectures.",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : "We conduct the experiments on the GLUE benchmark (Wang et al., 2018) and SQuADv1.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "‡ denotes that the results are taken from (Sun et al., 2020) and † means the models trained using the released code or the reimplemented code with ELECTRAbase as the teacher model.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "* means that the speedup is different from the (Sun et al., 2020), because it is evaluated on a Pixel phone and not on server CPUs.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 40,
      "context" : "training use BooksCorpus (Zhu et al., 2015) and English Wikipedia as training data.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "We train AutoTinyBERT in both ways of pretraining (Devlin et al., 2019) and task-agnostic BERT distillation (Sun et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : ", 2019) and task-agnostic BERT distillation (Sun et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "For taskagnostic distillation, we follow the first stage of TinyBERT (Jiao et al., 2020b) except that only the last-layer loss is used, and ELECTRAbase (Clark et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : ", 2020b) except that only the last-layer loss is used, and ELECTRAbase (Clark et al., 2019) is used as the teacher model.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "(2019)), BERT-S* (BERT under several hyper-parameter configurations), and NASBERT (Xu et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "The pairwise accuracy is used as a metric to indicate the ranking correction between the architectures under one-shot training and the ones under stand-alone full training (Luo et al., 2019) and its formula is described in Appendix E.",
      "startOffset" : 172,
      "endOffset" : 190
    }, {
      "referenceID" : 31,
      "context" : "over, we introduce HAT (Wang et al., 2020a), as a baseline of one-shot learning.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Knowledge distillation (KD) (Hinton et al., 2015; Romero et al., 2014) is the most widely studied technique in PLM compression, which uses a teacher-student framework.",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "Knowledge distillation (KD) (Hinton et al., 2015; Romero et al., 2014) is the most widely studied technique in PLM compression, which uses a teacher-student framework.",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "The typical distillation studies include DistilBERT (Sanh et al., 2019), BERT-PKD (Sun et al.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : ", 2019), BERT-PKD (Sun et al., 2019), MiniLM (Wang et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : ", 2020b), MobileBERT (Sun et al., 2020), MiniBERT (Tsai et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : ", 2020), MiniBERT (Tsai et al., 2019) and ETD (Chen et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "In addition to KD, the techniques of pruning (Han et al., 2016; Hou et al., 2020), quantization (Shen et al.",
      "startOffset" : 45,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "In addition to KD, the techniques of pruning (Han et al., 2016; Hou et al., 2020), quantization (Shen et al.",
      "startOffset" : 45,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : ", 2020), quantization (Shen et al., 2020; Zhang et al., 2020; Wang et al., 2020c) and parameter sharing (Lan et al.",
      "startOffset" : 22,
      "endOffset" : 81
    }, {
      "referenceID" : 39,
      "context" : ", 2020), quantization (Shen et al., 2020; Zhang et al., 2020; Wang et al., 2020c) and parameter sharing (Lan et al.",
      "startOffset" : 22,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : ", 2020), quantization (Shen et al., 2020; Zhang et al., 2020; Wang et al., 2020c) and parameter sharing (Lan et al.",
      "startOffset" : 22,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : ", 2020c) and parameter sharing (Lan et al., 2019) introduced for PLM compression.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "NAS is extensively studied in computer vision (Tan and Le, 2019; Tan et al., 2020), but relatively little studied in the natural language processing.",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "NAS is extensively studied in computer vision (Tan and Le, 2019; Tan et al., 2020), but relatively little studied in the natural language processing.",
      "startOffset" : 46,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Evolved Transformer (So et al., 2019) and HAT (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : ", 2019) and HAT (Wang et al., 2020a) search architecture for Transformer-based neural machine translation.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "For BERT distillation, AdaBERT (Chen et al., 2020) focuses on searching the architecture in the fine-tuning stage and relies on data augmentation to improve its performance.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "schuBERT (Khetan and Karnin, 2020) obtains the optimal structures of PLM by a pruning method.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 36,
      "context" : "A work similar to ours is NAS-BERT (Xu et al., 2021).",
      "startOffset" : 35,
      "endOffset" : 52
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT (Devlin et al., 2019). Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT1 and evaluate its effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show that our method outperforms both the SOTA searchbased baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM and MobileBERT). In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM.",
    "creator" : "LaTeX with hyperref"
  }
}