{
  "name" : "2021.acl-long.21.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation",
    "authors" : [ "Xiao Pan", "Mingxuan Wang", "Liwei Wu", "Lei Li" ],
    "emails" : [ "panxiao.94@bytedance.com", "wangmingxuan.89@bytedance.com", "wuliwei.000@bytedance.com", "lileilab@bytedance.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 244–258\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n244"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora. Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual translation models are appealing for two reasons. First, they are model efficient, enabling easier deployment (Johnson et al.,\n2017). Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020).\nDespite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020). Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improv-\ning English-centric1 directions (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020). A few recent exceptions are Zhang et al. (2020) and Fan et al. (2020), who trained many-to-many systems with introducing more non-English corpora, through data mining or back translation.\nIn this work, we take a step towards a unified many-to-many multilingual NMT with only English-centric parallel corpora and additional monolingual corpora. Our key insight is to close the representation gap between different languages to encourage transfer learning as much as possible.\nAs such, many-to-many translations can make the most of the knowledge from all supervised directions and the model can perform well for both English-centric and non-English settings. In this paper, we propose a multilingual COntrastive Learning framework for Translation (mCOLT or mRASP2) to reduce the representation gap of different languages, as shown in Figure 1.\nThe objective of mRASP2 ensures the model to represent similar sentences across languages in a shared space by training the encoder to minimize the representation distance of similar sentences. In addition, we also boost mRASP2 by leveraging monolingual data to further improve multilingual translation quality. We introduce an effective aligned augmentation technique by extending RAS (Lin et al., 2020) – on both parallel and monolingual corpora to create pseudo-pairs. These pseudo-pairs are combined with multilingual parallel corpora in a unified training framework.\nSimple yet effective, mRASP2 achieves consistent translation performance improvements for both English-centric and non-English directions on a wide range of benchmarks. For Englishcentric directions, mRASP2 outperforms a strong multilingual baseline in 20 translation directions on WMT testsets. On 10 WMT translation benchmarks, mRASP2 even obtains better results than the strong bilingual mBART model. For zeroshot and unsupervised directions, mRASP2 obtains surprisingly strong results on 36 translation directions2, with 10+ BLEU improvements on average.\n1“English-centric” means that having English as the source or target language\n26 unsupervised directions + 30 zero-shot directions"
    }, {
      "heading" : "2 Methodology",
      "text" : "mRASP2 unifies both parallel corpora and monolingual corpora with contrastive learning. This section will explain our proposed mRASP2. The overall framework is illustrated in Figure 1"
    }, {
      "heading" : "2.1 Multilingual Transformer",
      "text" : "A multilingual neural machine translation model learns a many-to-many mapping function f to translate from one language to another. To distinguish different languages, we add an additional language identification token preceding each sentence, for both source side and target side. The base architecture of mRASP2 is the state-of-theart Transformer (Vaswani et al., 2017). A little different from previous work, we choose a larger setting with a 12-layer encoder and a 12-layer decoder to increase the model capacity. The model dimension is 1024 on 16 heads. To ease the training of the deep model, we apply Layer Normalization for word embedding and pre-norm residual connection following Wang et al. (2019a) for both encoder and decoder. Therefore, our multilingual NMT baseline is much stronger than that of Transformer big model.\nMore formally, we define L = {L1, . . . , LM} where L is a collection of M languages involving in the training phase. Di,j denotes a parallel dataset of (Li, Lj), and D denotes all parallel datasets. The training loss is cross entropy defined as:\nLce = ∑\nxi,xj∈D\n− logPθ(xi|xj) (1)\nwhere xi represents a sentence in language Li, and θ is the parameter of multilingual Transformer model."
    }, {
      "heading" : "2.2 Multilingual Contrastive Learning",
      "text" : "Multilingual Transformer enables implicitly learning shared representation of different languages. mRASP2 introduces contrastive loss to explicitly bring different languages to map a shared semantic space.\nThe key idea of contrastive learning is to minimize the representation gap of similar sentences and maximize that of irrelevant sentences. Formally, given a bilingual translation pairs (xi,xj) ∈ D, (xi,xj) is the positive example and we randomly choose a sentence yj from language Lj to form a negative example3 (xi,yj).\n3It is possible that Lj = Li\nThe objective of contrastive learning is to minimize the following loss:\nLctr = − ∑\nxi,xj∈D log\nesim +(R(xi),R(xj))/τ∑\nyj e sim−(R(xi),R(yj))/τ\n(2) where sim(·) calculates the similarity of different sentences. + and − denotes positive and negative respectively. R(s) denotes the averagepooled encoded output of an arbitrary sentence s. τ is the temperature, which controls the difficulty of distinguishing between positive and negative examples4. In our experiments, it is set to 0.1. The similarity of two sentences is calculated with the cosine similarity of the averagepooled encoded output. To simplify implementation, the negative samples are sampled from the same training batch. Intuitively, by maximizing the softmax term sim+(R(xi),R(xj)), the contrastive loss forces their semantic representations projected close to each other. In the meantime, the softmax function also minimizes the non-matched pairs sim−(R(xi),R(yj)).\nDuring the training of mRASP2, the model can be optimized by jointly minimizing the contrastive training loss and translation loss:\nL = Lce + λ|s|Lctr (3)\nwhere λ is the coefficient to balance the two training losses. SinceLctr is calculated on the sentencelevel and Lce is calculated on the token-level, thereforeLctr should be multiplied by the averaged sequence length |s|."
    }, {
      "heading" : "2.3 Aligned Augmentation",
      "text" : "We then will introduce how to improve mRASP2 with data augmentation methods, including the introduction of noised bilingual and noised monolingual data for multilingual NMT. The above two\n4Higher temperature increases the difficulty to distinguish positive sample from negative ones.\ntypes of training samples are illustrated in Figure 2.\nLin et al. (2020) propose Random Aligned Substitution technique (or RAS5) that builds codeswitched sentence pairs (C(xi),xj) for multilingual pre-training. In this paper, we extend it to Aligned Augmentation (AA), which can also be applied to monolingual data.\nFor a bilingual or monolingual sentence pair (xi, xj)6, AA creates a perturbed sentence C(xi) by replacing aligned words from a synonym dictionary7. For every word contained in the synonym dictionary, we randomly replace it to one of its synonym with a probability of 90%.\nFor a bilingual sentence pair (xi,xj), AA creates a pseudo-parallel training example (C(xi),xj). For monolingual data, AA takes a sentence xi and generates its perturbed C(xi) to form a pseudo self-parallel example (C(xi),xi). (C(xi),xj) and (C(xi),xi) is then used in the training by calculating both the translation loss and contrastive loss. For a pseudo self-parallel example (C(xi),xi), the contrastive loss is basically the reconstruction loss from the perturbed sentence to the original one."
    }, {
      "heading" : "3 Experiments",
      "text" : "This section shows that mRASP2 can achieve substantial improvements over previous many-tomany multilingual translation on a wide range of benchmarks. Especially, it obtains substantial gains on zero-shot directions."
    }, {
      "heading" : "3.1 Settings and Datasets",
      "text" : "Parallel Dataset PC32 We use the parallel dataset PC32 provided by Lin et al. (2020). It con-\n5They apply RAS only on parallel data 6xi is in language Li and xj is in language Lj , where\ni, j ∈ {L1, . . . , LM} 7We will release our synonym dictionary\ntains a large public parallel corpora of 32 Englishcentric language pairs. The total number of sentence pairs is 97.6 million.\nWe apply AA on PC32 by randomly replacing words in the source side sentences with synonyms from an arbitrary bilingual dictionary provided by (Lample et al., 2018)8. For words in the dictionaries, we replace them into one of the synonyms with a probability of 90% and keep them unchanged otherwise. We apply this augmentation in the pre-processing step before training.\nMonolingual Dataset MC24 We create a dataset MC24 with monolingual text in 24 languages9. It is a subset of the Newscrawl10 dataset by retaining only those languages in PC32, plus three additional languages that are not in PC32 (Nl, Pl, Pt). In order to balance the volume across different languages, we apply temperature sam-\npling ñi = ( ni/ ∑ j nj )1/T with T=5 over the dataset, where ni is the number of sentences in ith language. Then we apply AA on monolingual\n8https://github.com/facebookresearch/MUSE 9Bg, Cs, De, El, En, Es, Et, Fi, Fr, Gu, Hi, It, Ja, Kk, Lt,\nLv, Ro, Ru, Sr, Tr, Zh, Nl, Pl, Pt 10http://data.statmt.org/news-crawl\ndata. The total number of sentences in MC24 is 1.01 billion. The detail of data volume is listed in the Appendix.\nWe apply AA on MC24 by randomly replacing words in the source side sentences with synonyms from a multilingual dictionary. Therefore the source side might contain multiple language tokens (preserving the semantics of the original sentence), and the target is just the original sentence. The replace probability is also set to 90%. We apply this augmentation in the pre-processing step before training. We will release the multilingual dictionary and the script for producing the noised monolingual dataset.\nEvaluation Datasets For supervised directions, most of our evaluation datasets are from WMT and IWSLT benchmarks, for pairs that are not available in WMT or IWSLT, we use OPUS-100 instead.\nFor zero-shot directions, we follow (Zhang et al., 2020) and use their proposed OPUS-100 zero-shot testset. The testset is comprised of 6 languages (Ru, De, Fr, Nl, Ar, Zh), resulting in 15 language pairs and 30 translation directions.\nWe report de-tokenized BLEU with Sacre-\nBLEU (Post, 2018). For tokenized BLEU, we tokenize both reference and hypothesis using Sacremoses11 toolkit then report BLEU using the multi-bleu.pl script12. For Chinese (Zh), BLEU score is calculated on character-level.\nExperiment Details We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as well as a learning rate of 3e-4 with polynomial decay scheduling and a warm-up step of 10000. For optimization, we use Adam optimizer (Kingma and Ba, 2015) with = 1e-6 and β2 = 0.98. To stabilize training, we set the threshold of gradient norm to be 5.0 and clip all gradients with a larger norm. We set the hyper-parameter λ = 1.0 in Eq.3 during training. For multilingual vocabulary, we follow the shared BPE (Sennrich et al., 2016) vocabulary of Lin et al. (2020), which includes 59 languages. The vocabulary contains 64808 tokens. After adding 59 language tokens, the total size of vocabulary is 64867.\n11https://github.com/alvations/sacremoses 12https://github.com/moses-smt/mosesdecoder"
    }, {
      "heading" : "4 Experiment Results",
      "text" : "This section shows that mRASP2 provides consistent performance gains for supervised and unsupervised English-centric translation directions as well as for non-English directions."
    }, {
      "heading" : "4.1 English-Centric Directions",
      "text" : "Supervised Directions As shown in Table 1, mRASP2 clearly improves multilingual baselines by a large margin in 10 translation directions. Previously, multilingual machine translation underperforms bilingual translation in rich-resource scenarios. It is worth noting that our multilingual machine translation baseline is already very competitive. It is even on par with the strong mBART bilingual model, which is fine-tuned on a large scale unlabeled monolingual dataset. mRASP2 further improves the performance.\nWe summarize the key factors for the success training of our baseline13 m-Transformer: a) The batch size plays a crucial role in the suc-\n13many-to-many Transformer trained on PC32 as in Johnson et al. (2017) except that we apply language indicator the same way as Fan et al. (2020)\ncess of training multilingual NMT. We use 8 × 4 NVIDIA V100 with update frequency 50 to train the models and each batch contains about 3 million tokens. b) We enlarge the number of layers from 6 to 12 and observe significant improvements for multilingual NMT. By contrast, the gains from increasing the bilingual model size is not that large. mBART also uses 12 encoder and decoder layers. c) We use gradient norm to stable the training. Without this regularization, the large scale training will collapse sometimes.\nUnsupervised Directions In Table 2, we observe that mRASP2 achieves reasonable results on unsupervised translation directions. The language pairs of En-Nl, En-Pt, and En-Pl are never observed by m-Transformer. m-Transformer sometimes achieves reasonable BLEU for X→En, e.g. 10.7 for Pt→En, since there are many similar languages in PC32, such as Es and Fr. Not surprisingly, it totally fails on En→X directions. By contrast, mRASP2 obtains +14.13 BLEU score on an average without explicitly introducing supervision signals for these directions.\nFurthermore, mRASP2 achieves reasonable BLEU scores on Nl↔Pt directions even though it has only been trained on monolingual data of both sides. This indicates that by simply incorporating monolingual data with parallel data in the unified framework, mRASP2 successfully enables unsupervised translation through its unified multilingual representation."
    }, {
      "heading" : "4.2 Zero-shot Translation for non-English Directions",
      "text" : "Zero-shot Translation has been an intriguing topic in multilingual neural machine translation. Previous work shows that the multilingual NMT model\ncan do zero-shot translation directly. However, the translation quality is quite poor compared with pivot-based model.\nWe evaluate mRASP2 on the OPUS-100 (Zhang et al., 2020) zero-shot test set, which contains 6 languages14 and 30 translation directions in total. To make the comparison clear, we also report the results of several different baselines. mRASP2 w/o AA only adopt contrastive learning on the basis of m-Transformer. mRASP2 w/o MC24 excludes monolingual data from mRASP2.\nThe evaluation results are listed in Appendix and we summarize them in Table 3. We find that our mRASP2 significantly outperforms mTransformer and substantially narrows the gap with pivot-based model. This is in line with our intuition that bridging the representation gap of different languages can improve the zero-shot translation.\nThe main reason is that contrastive loss, aligned augmentation and additional monolingual data enable a better language-agnostic sentence representation. It is worth noting that, Zhang et al. (2020) achieves BLEU score improvements on zero-shot translations at sacrifice of about 0.5 BLEU score loss on English-centric directions. By contrast, mRASP2 improves zero-shot translation by a large margin without losing performance on English-Centric directions. Therefore, mRASP2 has a great potential to serve many-to-many translations, including both English-centric and nonEnglish directions."
    }, {
      "heading" : "5 Analysis",
      "text" : "To understand what contributes to the performance gain, we conduct analytical experiments in this\n14Arabic, Chinese, Dutch, French, German, Russian\nsection. First we summarize and analysis the performance of mRASP2 in different scenarios. Second we adopt the sentence representation of mRASP2 to retrieve similar sentences across languages. This is to verify our argument that the improvements come from the universal language representation learned by mRASP2. Finally we visualize the sentence representations, mRASP2 indeed draws the representations closer."
    }, {
      "heading" : "5.1 Ablation Study",
      "text" : "To make a better understanding of the effectiveness of mRASP2, we evaluate models of different settings. We summarize the experiment results in Table 4:\n• 1 v.s. 3 : 3 performs comparably with m-Transformer in supervised and unsupervised scenarios, whereas achieves a substantial BLEU improvement for zero-shot translation. This indicates that by introducing contrastive loss, we can improve zero-shot translation quality without harming other directions.\n• 2 v.s. 4 : 2 performs poorly for zero-shot directions. This means contrastive loss is crucial for the performance in zero-shot directions.\n• 5 : mRASP2 further improves BLEU in all of the three scenarios, especially in unsupervised directions. Therefore it is safe to conjecture that by accomplishing with monolingual data, mRASP2 learns a better representation space."
    }, {
      "heading" : "5.2 Similarity Search",
      "text" : "In order to verify whether mRASP2 learns a better representation space, we conduct a set of similarity search experiments. Similarity search is a task to find the nearest neighbor of each sentence in another language according to cosine similarity. We argue that mRASP2 benefits this task in the sense that it bridges the representation gap across languages. Therefore we use the accuracy of similarity search tasks as a quantitative indicator of cross-lingual representation alignment.\nWe conducted comprehensive experiments to support our argument and experiment on mRASP2 and mRASP2 w/o AA .We divide the experiments into two scenarios: First we evaluate our method\non Tatoeba dataset (Artetxe and Schwenk, 2019), which is English-centric. Then we conduct similar similarity search task on non-English language pairs. Following Tran et al. (2020), we construct a multi-way parallel testset (Ted-M) of 2284 samples by filtering the test split of ted15 that have translations for all 15 languages16.\nUnder both settings, we follow the same strategy: We use the average-pooled encoded output as the sentence representation. For each sentence from the source language, we search the closest sentence in the target set according to cosine similarity.\nEnglish-Centric: Tatoeba We display the evaluation results in Table 5. We detect two trends: (i) The overall accuracy follows the rule: mTransformer < mRASP2 w/o AA < mRASP2. (ii) mRASP2 brings more significant improvements for languages with less data volume in PC32. The two trends mean that mRASP2 increases translation BLEU score in a sense that it bridges the representation gap across languages.\nNon-English: Ted-M It will be more convincing to argue that mRASP2 indeed bridges the representation gap if similarity search accuracy increases on zero-shot directions. We list the averaged top-1 accuracy of 210 non-English directions17 in Table 6. The results show that mRASP2 increases the similarity search accuracy in zeroshot scenario. The results support our argument\n15http://phontron.com/data/ted talks.tar.gz 16Arabic, Czech, German, English, Spanish, French, Italian, Japanese, Korean, Dutch, Romanian, Russian, Turkish, Vietnamese, Chinese\n1715 languages, resulting in 210 directions\nthat our method generally narrows the representation gap across languages.\nTo better understanding the specifics beyond the averaged accuracy, we plot the accuracy improvements in the heat map in Figure 3. mRASP2 w/o AA brings general improvements over mTransformer. mRASP2 especially improves on Dutch(Nl). This is because mRASP2 introduces monolingual data of Dutch while mRASP2 w/o AA includes no Dutch data."
    }, {
      "heading" : "5.3 Visualization",
      "text" : "In order to visualize the sentence representations across languages, we retrieve the sentence representation R(s) for each sentence in Ted-M, resulting in 34260 samples in the high-dimensional space.\nTo facilitate visualization, we apply T-SNE dimension reduction to reduce the 1024-dim representations to 2-dim. Then we select 3 representative languages: English, German, Japanese and depict the bivariate kernel density estimation based on the 2-dim representations. It is clear in Figure 4 that m-Transformer cannot align the 3 languages. By contrast, mRASP2 draws the representations across 3 languages much closer."
    }, {
      "heading" : "6 Related Work",
      "text" : "Multilingual Neural Machine Translation While initial research on NMT starts with build-\ning translation systems between two languages, Dong et al. (2015) extends the bilingual NMT to one-to-many translation with sharing encoders across 4 language pairs. Hence, there has been a massive increase in work on MT systems that involve more than two languages (Chen et al., 2018; Choi et al., 2018; Chu and Dabre, 2019; Dabre et al., 2017). Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages (Bapna and Firat, 2019; Kim et al., 2019; Wang et al., 2019b; Escolano et al., 2020). Another promising thread line is to enlarge the model size with extensive training data to improve the model capability (Arivazhagan et al., 2019; Aharoni et al., 2019; Fan et al., 2020). Different from these approaches, mRASP2 proposes to explicitly close the semantic representation of different languages and make the most of cross lingual transfer.\nZero-shot Machine Translation Typical zeroshot machine translation models rely on a pivot language (e.g. English) to combine the sourcepivot and pivot-target translation models (Chen et al., 2017; Ha et al., 2017; Gu et al., 2019; Currey and Heafield, 2019). Johnson et al. (2017) shows that a multilingual NMT system enables zero-shot translation without explicitly introducing pivot methods. Promising, but the performance still lags behind the pivot competitors. Most following up studies focused on data augmentation methods. Zhang et al. (2020) improved the zero-shot translation with online back translation. Ji et al. (2020); Liu et al. (2020) shows that large scale monolingual data can improve the zero-shot translation with unsupervised pre-training. Fan et al. (2020) proposes a simple and effective data mining method to enlarge the training corpus of zero-shot directions. Some work also attempted to explicitly learn shared semantic representation of different languages to im-\nprove the zero-shot translation. Lu et al. (2018) suggests that by learning an explicit “interlingual” across languages, multilingual NMT model can significantly improve zero-shot translation quality. Al-Shedivat and Parikh (2019) introduces a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. Different from these efforts, mRASP2 attempts to learn a universal many-to-many model, and bridge the cross-lingual representation with contrastive learning and m-RAS. The performance is very competitive both on zero-shot and supervised directions on large scale experiments.\nContrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks (Zhuang et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Misra and van der Maaten, 2020). Researchers in the NLP domain have also explored contrastive Learning for sentence representation. Wu et al. (2020) employed multiple sentence-level augmentation strategies to learn a noise-invariant sentence representation. Fang and Xie (2020) applies the back-translation to create augmentations of original sentences. Inspired by these studies, we apply contrastive learning for multilingual NMT.\nCross-lingual Representation Cross-lingual representation learning has been intensively studied in order to improve cross-lingual understanding (XLU) tasks. Multilingual masked\nlanguage models (MLM), such as mBERT(Devlin et al., 2019) and XLM(Conneau and Lample, 2019), train large Transformer models on multiple languages jointly and have built strong benchmarks on XLU tasks. Most of the previous works on cross-lingual representation learning focus on unsupervised training. For supervised learning, Conneau and Lample (2019) proposes TLM objective that simply concatenates parallel sentences as input. By contrast, mRASP2 leverages the supervision signal by pulling closer the representations of parallel sentences."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We demonstrate that contrastive learning can significantly improve zero-shot machine translation directions. Combined with additional unsupervised monolingual data, we achieve substantial improvements on all translation directions of multilingual NMT. We analyze and visualize our method, and find that contrastive learning tends to close the representation gap of different languages. Our results also show the possibilities of training a true many-to-many Multilingual NMT that works well on any translation direction. In future work, we will scale-up the current training to more languages, e.g. PC150. As such, a single model can handle more than 100 languages and outperforms the corresponding bilingual baseline."
    }, {
      "heading" : "A Case Study",
      "text" : "We plot the location of multi-way parallel sentences in the representation space of mRASP2 in Figure 5 and list sentences number 1 and 100 in Table 7"
    }, {
      "heading" : "B Details of Evaluation Results",
      "text" : "We list detailed results of evaluation on a wide range of test sets.\nB.1 Results on OPUS-100 Detailed results on OPUS-100 zero-shot evaluation set are listed in Table 8\nB.2 Results on WMT Detailed results on WMT evaluation set are listed in Table 9"
    }, {
      "heading" : "C Example of AA",
      "text" : "We show two results of sentences after AA in Figure 6"
    }, {
      "heading" : "D Details of MC24",
      "text" : "We describe the detail of MC24 in Table 10"
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Consistency by agreement in zero-shot neural machine translation",
      "author" : [ "Maruan Al-Shedivat", "Ankur P. Parikh." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Al.Shedivat and Parikh.,? 2019",
      "shortCiteRegEx" : "Al.Shedivat and Parikh.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "A teacher-student framework for zeroresource neural machine translation",
      "author" : [ "Yun Chen", "Yang Liu", "Yong Cheng", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancou-",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Zeroresource neural machine translation with multiagent communication game",
      "author" : [ "Yun Chen", "Yang Liu", "Victor O.K. Li." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving a multi-source neural machine translation model with corpus extension for lowresource languages",
      "author" : [ "Gyu-Hyeon Choi", "Jong-Hun Shin", "Young Kil Kim." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual multidomain adaptation approaches for neural machine translation",
      "author" : [ "Chenhui Chu", "Raj Dabre." ],
      "venue" : "CoRR, abs/1906.07978.",
      "citeRegEx" : "Chu and Dabre.,? 2019",
      "shortCiteRegEx" : "Chu and Dabre.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Zeroresource neural machine translation with monolingual pivot data",
      "author" : [ "Anna Currey", "Kenneth Heafield." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLPIJCNLP 2019, Hong Kong, November 4, 2019,",
      "citeRegEx" : "Currey and Heafield.,? 2019",
      "shortCiteRegEx" : "Currey and Heafield.",
      "year" : 2019
    }, {
      "title" : "Enabling multi-source neural machine translation by concatenating source sentences in multiple languages",
      "author" : [ "Raj Dabre", "Fabien Cromierès", "Sadao Kurohashi." ],
      "venue" : "CoRR, abs/1702.06135.",
      "citeRegEx" : "Dabre et al\\.,? 2017",
      "shortCiteRegEx" : "Dabre et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Training multilingual machine translation by alternately freezing language-specific encoders-decoders",
      "author" : [ "Carlos Escolano", "Marta R. Costa-jussà", "José A.R. Fonollosa", "Mikel Artetxe." ],
      "venue" : "CoRR, abs/2006.01594.",
      "citeRegEx" : "Escolano et al\\.,? 2020",
      "shortCiteRegEx" : "Escolano et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond english-centric multilingual machine translation",
      "author" : [ "Michael Auli", "Armand Joulin." ],
      "venue" : "CoRR, abs/2010.11125.",
      "citeRegEx" : "Auli and Joulin.,? 2020",
      "shortCiteRegEx" : "Auli and Joulin.",
      "year" : 2020
    }, {
      "title" : "CERT: contrastive self-supervised learning for language understanding",
      "author" : [ "Hongchao Fang", "Pengtao Xie." ],
      "venue" : "CoRR, abs/2005.12766.",
      "citeRegEx" : "Fang and Xie.,? 2020",
      "shortCiteRegEx" : "Fang and Xie.",
      "year" : 2020
    }, {
      "title" : "Improved zero-shot neural machine translation via ignoring spurious correlations",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Effective strategies in zero-shot neural machine translation",
      "author" : [ "Thanh-Le Ha", "Jan Niehues", "Alexander H. Waibel." ],
      "venue" : "CoRR, abs/1711.07893.",
      "citeRegEx" : "Ha et al\\.,? 2017",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2017
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross B. Girshick." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual pre-training based transfer for zero-shot neural machine translation",
      "author" : [ "Baijun Ji", "Zhirui Zhang", "Xiangyu Duan", "Min Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
      "citeRegEx" : "Ji et al\\.,? 2020",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective cross-lingual transfer of neural machine translation models without shared vocabularies",
      "author" : [ "Yunsu Kim", "Yingbo Gao", "Hermann Ney." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Unsupervised machine translation using monolingual corpora only",
      "author" : [ "Guillaume Lample", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "In 6th International Conference on Learning Representations,",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretraining multilingual neural machine translation by leveraging alignment information",
      "author" : [ "Zehui Lin", "Xiao Pan", "Mingxuan Wang", "Xipeng Qiu", "Jiangtao Feng", "Hao Zhou", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:726–742.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural interlingua for multilingual machine translation",
      "author" : [ "Yichao Lu", "Phillip Keung", "Faisal Ladhak", "Vikas Bhardwaj", "Shaonan Zhang", "Jason Sun." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 84–92, Brus-",
      "citeRegEx" : "Lu et al\\.,? 2018",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2018
    }, {
      "title" : "Selfsupervised learning of pretext-invariant representations",
      "author" : [ "Ishan Misra", "Laurens van der Maaten." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 6706–6716.",
      "citeRegEx" : "Misra and Maaten.,? 2020",
      "shortCiteRegEx" : "Misra and Maaten.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186–191. Association for Computational Lin-",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th ACL (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguis-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Leveraging monolingual data with self-supervision for multilingual neural machine translation",
      "author" : [ "Aditya Siddhant", "Ankur Bapna", "Yuan Cao", "Orhan Firat", "Mia Xu Chen", "Sneha Reddy Kudugunta", "Naveen Arivazhagan", "Yonghui Wu." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Siddhant et al\\.,? 2020",
      "shortCiteRegEx" : "Siddhant et al\\.",
      "year" : 2020
    }, {
      "title" : "MASS: masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual neural machine translation with knowledge distillation",
      "author" : [ "Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive multiview coding",
      "author" : [ "Yonglong Tian", "Dilip Krishnan", "Phillip Isola." ],
      "venue" : "Computer Vision ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, volume 12356 of Lecture Notes in Computer Science,",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual retrieval for iterative self-supervised training",
      "author" : [ "Chau Tran", "Yuqing Tang", "Xian Li", "Jiatao Gu." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is All you Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning deep transformer models for machine translation",
      "author" : [ "Qiang Wang", "Bei Li", "Tong Xiao", "Jingbo Zhu", "Changliang Li", "Derek F. Wong", "Lidia S. Chao." ],
      "venue" : "Proceedings of the 57th ACL, pages 1810–1822, Florence, Italy. Association for",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A compact and language-sensitive multilingual translation method",
      "author" : [ "Yining Wang", "Long Zhou", "Jiajun Zhang", "Feifei Zhai", "Jingfang Xu", "Chengqing Zong." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "CLEAR: contrastive learning for sentence representation",
      "author" : [ "Zhuofeng Wu", "Sinong Wang", "Jiatao Gu", "Madian Khabsa", "Fei Sun", "Hao Ma." ],
      "venue" : "CoRR, abs/2012.15466.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving massively multilingual neural machine translation and zero-shot translation",
      "author" : [ "Biao Zhang", "Philip Williams", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "Proceedings of the 58th ACL, pages 1628–1639, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Local aggregation for unsupervised learning of visual embeddings",
      "author" : [ "Chengxu Zhuang", "Alex Lin Zhai", "Daniel Yamins." ],
      "venue" : "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,",
      "citeRegEx" : "Zhuang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 219
    }, {
      "referenceID" : 39,
      "context" : "Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 219
    }, {
      "referenceID" : 30,
      "context" : "Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : "direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : "First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020).",
      "startOffset" : 155,
      "endOffset" : 211
    }, {
      "referenceID" : 39,
      "context" : "First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020).",
      "startOffset" : 155,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "245 ing English-centric1 directions (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 100
    }, {
      "referenceID" : 39,
      "context" : "245 ing English-centric1 directions (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "We introduce an effective aligned augmentation technique by extending RAS (Lin et al., 2020) – on both parallel and monolingual corpora to create pseudo-pairs.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "The base architecture of mRASP2 is the state-of-theart Transformer (Vaswani et al., 2017).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "pre-train & fine-tuned Adapter (Bapna and Firat, 2019) - - - - 35.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : "Multi-Distillation (Tan et al., 2019) - - - - - - 31.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "We apply AA on PC32 by randomly replacing words in the source side sentences with synonyms from an arbitrary bilingual dictionary provided by (Lample et al., 2018)8.",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 39,
      "context" : "For zero-shot directions, we follow (Zhang et al., 2020) and use their proposed OPUS-100 zero-shot testset.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "For optimization, we use Adam optimizer (Kingma and Ba, 2015) with = 1e-6 and β2 = 0.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "For multilingual vocabulary, we follow the shared BPE (Sennrich et al., 2016) vocabulary of Lin et al.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 39,
      "context" : "We evaluate mRASP2 on the OPUS-100 (Zhang et al., 2020) zero-shot test set, which contains 6 languages14 and 30 translation directions in total.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "on Tatoeba dataset (Artetxe and Schwenk, 2019), which is English-centric.",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "Hence, there has been a massive increase in work on MT systems that involve more than two languages (Chen et al., 2018; Choi et al., 2018; Chu and Dabre, 2019; Dabre et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "Hence, there has been a massive increase in work on MT systems that involve more than two languages (Chen et al., 2018; Choi et al., 2018; Chu and Dabre, 2019; Dabre et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "Hence, there has been a massive increase in work on MT systems that involve more than two languages (Chen et al., 2018; Choi et al., 2018; Chu and Dabre, 2019; Dabre et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "Hence, there has been a massive increase in work on MT systems that involve more than two languages (Chen et al., 2018; Choi et al., 2018; Chu and Dabre, 2019; Dabre et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages (Bapna and Firat, 2019; Kim et al., 2019; Wang et al., 2019b; Escolano et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 237
    }, {
      "referenceID" : 21,
      "context" : "Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages (Bapna and Firat, 2019; Kim et al., 2019; Wang et al., 2019b; Escolano et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 237
    }, {
      "referenceID" : 37,
      "context" : "Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages (Bapna and Firat, 2019; Kim et al., 2019; Wang et al., 2019b; Escolano et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 237
    }, {
      "referenceID" : 14,
      "context" : "Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages (Bapna and Firat, 2019; Kim et al., 2019; Wang et al., 2019b; Escolano et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 237
    }, {
      "referenceID" : 0,
      "context" : "Another promising thread line is to enlarge the model size with extensive training data to improve the model capability (Arivazhagan et al., 2019; Aharoni et al., 2019; Fan et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "English) to combine the sourcepivot and pivot-target translation models (Chen et al., 2017; Ha et al., 2017; Gu et al., 2019; Currey and Heafield, 2019).",
      "startOffset" : 72,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "English) to combine the sourcepivot and pivot-target translation models (Chen et al., 2017; Ha et al., 2017; Gu et al., 2019; Currey and Heafield, 2019).",
      "startOffset" : 72,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "English) to combine the sourcepivot and pivot-target translation models (Chen et al., 2017; Ha et al., 2017; Gu et al., 2019; Currey and Heafield, 2019).",
      "startOffset" : 72,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "English) to combine the sourcepivot and pivot-target translation models (Chen et al., 2017; Ha et al., 2017; Gu et al., 2019; Currey and Heafield, 2019).",
      "startOffset" : 72,
      "endOffset" : 152
    }, {
      "referenceID" : 40,
      "context" : "Contrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks (Zhuang et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Misra and van der Maaten, 2020).",
      "startOffset" : 135,
      "endOffset" : 243
    }, {
      "referenceID" : 33,
      "context" : "Contrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks (Zhuang et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Misra and van der Maaten, 2020).",
      "startOffset" : 135,
      "endOffset" : 243
    }, {
      "referenceID" : 19,
      "context" : "Contrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks (Zhuang et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Misra and van der Maaten, 2020).",
      "startOffset" : 135,
      "endOffset" : 243
    }, {
      "referenceID" : 4,
      "context" : "Contrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks (Zhuang et al., 2019; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Misra and van der Maaten, 2020).",
      "startOffset" : 135,
      "endOffset" : 243
    }, {
      "referenceID" : 12,
      "context" : "Multilingual masked language models (MLM), such as mBERT(Devlin et al., 2019) and XLM(Conneau and Lample, 2019), train large Transformer models on multiple languages jointly and have built strong benchmarks on XLU tasks.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and XLM(Conneau and Lample, 2019), train large Transformer models on multiple languages jointly and have built strong benchmarks on XLU tasks.",
      "startOffset" : 15,
      "endOffset" : 41
    } ],
    "year" : 2021,
    "abstractText" : "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 outperforms existing best unified model and achieves competitive or even better performance than the pre-trained and fine-tuned model mBART on tens of WMT’s translation directions. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual Transformer baseline. Code, data and trained models are available at https://github. com/PANXiao1994/mRASP2.",
    "creator" : "LaTeX with hyperref"
  }
}