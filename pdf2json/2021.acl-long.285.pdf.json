{
  "name" : "2021.acl-long.285.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding",
    "authors" : [ "Jia-Chen Gu", "Chongyang Tao", "Zhen-Hua Ling", "Can Xu", "Xiubo Geng", "Daxin Jiang" ],
    "emails" : [ "gujc@mail.ustc.edu.cn,", "zhling@ustc.edu.cn,", "chotao@microsoft.com", "caxu@microsoft.com", "xigeng@microsoft.com", "djiang@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3682–3692\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3682"
    }, {
      "heading" : "1 Introduction",
      "text" : "Building a conversational agent with intelligence has drawn significant attention from both academia and industry. Most of existing methods have studied understanding conversations between two participants, aiming to return an appropriate response either in a generation-based (Shang et al.,\n∗Work done during the internship at Microsoft. †Corresponding author.\n2015; Serban et al., 2016, 2017; Zhang et al., 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020). Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019). Table 1 shows an MPC example in the Ubuntu Internet Relay Chat (IRC) channel, which is composed of a sequence of (speaker, utterance, addressee) triples. In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019) are unique and important issues in MPC.\nAn instance of MPC always contains complicated interactions between interlocutors, between utterances and between an interlocutor and an utterance. Therefore, it is challenging to model the conversation flow and fully understand the dialogue content. Existing studies on MPC learn the representations of interlocutors and utterances with neural networks, and their representation\nspaces are either separate (Ouchi and Tsuboi, 2016) or interactive (Zhang et al., 2018a). However, the semantics contained in the interlocutor and utterance representations may not be effectively captured as they are from two different representation spaces. Recently, to take advantage of the breakthrough in pre-training language models (PLMs) for natural language understanding, some studies proposed to integrate the speaker (Gu et al., 2020) or topic (Wang et al., 2020) information into PLMs. Despite of the performance improvement on response selection, these models still overlook the inherent relationships between utterances and interlocutors, such as “address-to”. Furthermore, most existing studies design models for each individual task in MPC (e.g., addressee recognition, speaker identification and response prediction) separately. Intuitively, these tasks are complementary among each other. Making use of these tasks simultaneously may produce better contextualized representations of interlocutors and utterances, and would enhance the conversation understanding, but is neglected in previous studies.\nOn account of above issues, we propose MPCBERT which jointly learns who says what to whom in MPC by designing self-supervised tasks for PLMs, so as to improve the ability of PLMs on MPC understanding. Specifically, the five designed tasks includes reply-to utterance recognition, identical speaker searching, pointer consistency distinction, masked shared utterance restoration and shared node detection. The first three tasks are designed to model the interlocutor structure in MPC in a semantics-to-structure manner. In the output of MPC-BERT, an interlocutor is described through the encoded representations of the utterances it says. Thus, the representations of utterance semantics are utilized to construct the conversation structure in these three tasks. On the other hand, the last two tasks are designed to model the utterance semantics in a structure-to-semantics manner. Intuitively, the conversation structure influences the information flow in MPC. Thus, the structure information can also be used to strengthen the representations of utterance semantics in return. In general, these five self-supervised tasks are employed to jointly train the MPC-BERT in a multi-task learning framework, which helps the model to learn the complementary information among interlocutors and utterances, and that between structure and semantics. By this means,\nMPC-BERT can produce better interlocutor and utterance representations which can be effectively generalized to multiple downstream tasks of MPC.\nTo measure the effectiveness of these selfsupervised tasks and to test the generalization ability of MPC-BERT, we evaluate it on three downstream tasks including addressee recognition, speaker identification and response selection, which are three core research issues of MPC. Two benchmarks based on Ubuntu IRC channel are employed for evaluation. One was released by Hu et al. (2019). The other was released by Ouchi and Tsuboi (2016) and has three experimental settings according to session lengths. Experimental results show that MPC-BERT outperforms the current state-of-the-art models by margins of 3.51%, 2.86%, 3.28% and 5.36% on the test sets of these two benchmarks respectively in terms of the session accuracy of addressee recognition, by margins of 7.66%, 2.60%, 3.38% and 4.24% respectively in terms of the utterance precision of speaker identification, and by margins of 3.82%, 2.71%, 2.55% and 3.22% respectively in terms of the response recall of response selection.\nIn summary, our contributions in this paper are three-fold: (1) MPC-BERT, a PLM for MPC understanding, is proposed by designing five selfsupervised tasks based on the interactions among utterances and interlocutors. (2) Three downstream tasks are employed to comprehensively evaluate the effectiveness of our designed self-supervised tasks and the generalization ability of MPC-BERT. (3) Our proposed MPC-BERT achieves new state-ofthe-art performance on all three downstream tasks at two benchmarks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Existing methods on building dialogue systems can be generally categorized into studying twoparty conversations and multi-party conversations (MPC). In this paper, we study MPC. In addition to predicting utterances, identifying the speaker and recognizing the addressee of an utterance are also important tasks for MPC. Ouchi and Tsuboi (2016) first proposed the task of addressee and response selection and created an MPC corpus for studying this task. Zhang et al. (2018a) proposed SI-RNN, which updated speaker embeddings role-sensitively for addressee and response selection. Meng et al. (2018) proposed a task of speaker classification as a surrogate task for speaker modeling. Le et al.\n(2019) proposed a who-to-whom (W2W) model to recognize the addressees of all utterances. Hu et al. (2019) proposed a graph-structured network (GSN) to model the graphical information flow for response generation. Wang et al. (2020) proposed to track the dynamic topic for response selection.\nGenerally speaking, previous studies on MPC cannot unify the representations of interlocutors and utterances effectively. Also, they are limited to each individual task, ignoring the complementary information among different tasks. To the best of our knowledge, this paper makes the first attempt to design various self-supervised tasks for building PLMs aiming at MPC understanding, and to evaluate the performance of PLMs on three downstream tasks as comprehensively as possible."
    }, {
      "heading" : "3 MPC-BERT and Self-Supervised Tasks",
      "text" : "An MPC instance is composed of a sequence of (speaker, utterance, addressee) triples, denoted as {(sn, un, an)}Nn=1, where N is the number of turns in the conversation. Our goal is to build a pre-trained language model for universal MPC understanding. Given a conversation, this model is expected to produce embedding vectors for all utterances which contain not only the semantic information of each utterance, but also the speaker and addressee structure of the whole conversation. Thus, it can be effectively adapted to various downstream tasks by fine-tuning model parameters."
    }, {
      "heading" : "3.1 Model Overview",
      "text" : "In this paper, BERT (Devlin et al., 2019) is chosen as the backbone of our PLM for MPC. Thus, we name it MPC-BERT. It is worth noting that our proposed self-supervised tasks for training MPCBERT can also be applied to other types of PLMs.\nWe first give an overview of the input representations and the overall architectures of MPC-BERT. When constructing the input representations, in order to consider the speaker information of each utterance, speaker embeddings (Gu et al., 2020) are introduced as shown in Figure 1. Considering that the set of interlocutors are inconsistent in different conversations, a position-based interlocutor embedding table is initialized randomly at first and updated during pre-training, which means each interlocutor in a conversation is assigned with an embedding vector according to the order it appears in the conversation. Then, the speaker embeddings for each utterance can be derived by\nlooking up this embedding table. The speaker embeddings are combined with standard token, position and segmentation embeddings and are then encoded by BERT. The output embeddings of BERT corresponding to different input tokens are utilized by different self-supervised tasks for further calculation."
    }, {
      "heading" : "3.2 Tasks of Interlocutor Structure Modeling",
      "text" : "The first three tasks follow the semantics-tostructure manner. In MPC-BERT, each interlocutor is described through the encoded representations of the utterances it says. Thus, the representations of utterance semantics are utilized to construct the conversation structure. Figure 1 shows the input representations and the model architectures of these three tasks. A [CLS] token is inserted at the start of each utterance, denoting its utterancelevel representation. Then, all utterances in a conversation are concatenated and a [SEP] token is inserted at the end of the whole sequence. It is notable that these three tasks share the same form of input data. Thus, the input only needs to be encoded once by BERT while the output can be fed into three tasks, which is computation-efficient. As shown in Figure 1, a task-dependent non-linear transformation layer is placed on top of BERT in order to adapt the output of BERT to different tasks. We will describe the details of these tasks as follows."
    }, {
      "heading" : "3.2.1 Reply-to Utterance Recognition",
      "text" : "To enable the model to recognize the addressee of each utterance, a self-supervised task named replyto utterance recognition (RUR) is proposed to learn which preceding utterance the current utterance replies to. After encoded by BERT, we extract the contextualized representations for each [CLS] token representing individual utterances. Next, a non-linear transformation followed by a layer normalization are performed to derive the utterance representations for this specific task {ururi }Ni=1, where ururi ∈ Rd and d = 768. Then, for a specific utterance Ui, its matching scores with all its preceding utterances are calculated as\nmij = softmax(urur>i · Arur · ururj ), (1)\nwhere Arur ∈ Rd×d is a linear transformation, mij denotes the matching degree of Uj being the replyto utterance of Ui, and 1 ≤ j < i. We construct a set S by sampling a certain number of utterances\nin a conversation and this recognition operation is performed for each utterance in S. Meanwhile, a dynamic sampling strategy is adopted so that models can see more samples. Finally, the pretraining objective of this self-supervised task is to minimize the cross-entropy loss as\nLrur = − ∑ i∈S i−1∑ j=1 yij log(mij), (2)\nwhere yij = 1 if Uj is the reply-to utterance of Ui and yij = 0 otherwise."
    }, {
      "heading" : "3.2.2 Identical Speaker Searching",
      "text" : "Having knowledge of who is the speaker of an utterance is also important for MPC. The task of identical speaker searching (ISS) is designed by masking the speaker embedding of a specific utterance in the input representation, and aims to predict its speaker given the conversation. Since the set of interlocutors vary across conversations, the task of predicting the speaker of an utterance is reformulated as searching for the utterances sharing the identical speaker.\nFirst, for a specific utterance, its speaker embedding is masked with a special [Mask] interlocutor embedding to avoid information leakage. Given the utterance representations for this specific task {uissi }Ni=1 where uissi ∈ Rd, the matching scores of Ui with all its preceding utterances are calculated similarly with Eq. (1). Here, mij denotes the\nmatching degree of Uj sharing the same speaker with Ui. For each instance in the dynamic sampling set S, there must be an utterance in previous turns sharing the same speaker. Otherwise, it is removed out of the set. Finally, the pre-training objective of this task is to minimize the cross-entropy loss similarly with Eq. (2). Here, yij = 1 if Uj shares the same speaker with Ui and yij = 0 otherwise."
    }, {
      "heading" : "3.2.3 Pointer Consistency Distinction",
      "text" : "We design a task named pointer consistency distinction (PCD) to jointly model speakers and addressees in MPC. In this task, a pair of utterances representing the “reply-to” relationship is defined as a speaker-to-addressee pointer. Here, we assume that the representations of two pointers directing from the same speaker to the same addressee should be consistent. As illustrated in Figure 2 (a), speaker Sm speaks Ui and Uj which reply to Ui′ and Uj′ from speaker Sn respectively. Thus, the utterance tuples (Ui, Ui′) and (Uj , Uj′) both represent the pointer of Sm-to-Sn and their pointer representations should be consistent..\nGiven the utterance representations for this specific task {upcdi }Ni=1 where u pcd i ∈ Rd, we first capture the pointer information contained in each utterance tuple. The element-wise difference and multiplication between an utterance tuple (Ui, Ui′) are computed and are concatenated as\npii′ = [u pcd i − u pcd i′ ; u pcd i u pcd i′ ], (3)\nwhere pii′ ∈ R2d. Then, we compress pii′ and obtain the pointer representation p̄ii′ as\np̄ii′ = ReLU(pii′ ·Wpcd + bpcd), (4)\nwhere Wpcd ∈ R2d×d and bpcd ∈ Rd are parameters. Identically, a consistent pointer representations p̄jj′ and an inconsistent one p̄kk′ sampled from this conversation are obtained. The similarities between every two pointers are calculated as\nmij = sigmoid(p̄>ii′ · Apcd · p̄jj′), (5)\nwhere mij denotes the matching degree of pointer p̄ii′ being consistent with pointer p̄jj′ . mik can be derived accordingly. Finally, the pre-training objective of this task is to minimize the hinge loss which enforces mij to be larger than mik by at least a margin ∆ as\nLpcd = max{0,∆−mij + mik}. (6)"
    }, {
      "heading" : "3.3 Tasks of Utterance Semantics Modeling",
      "text" : "Intuitively, the conversation structure might influence the information flow, so that it can be used to strengthen the representations of utterance semantics. Thus, two self-supervised tasks following the structure-to-semantics manner are designed."
    }, {
      "heading" : "3.3.1 Masked Shared Utterance Restoration",
      "text" : "There are usually several utterances replying-to a shared utterance in MPC. Intuitively, a shared utterance is semantically relevant to more utterances in the context than non-shared ones. Based on this characteristic, we design a task named masked shared utterance restoration (MSUR). We first randomly sample an utterance from all shared utterances in a conversation and all tokens in this sampled utterance are masked with a [MASK]\ntoken. Then the model is enforced to restore the masked utterance given the rest conversation.\nFormally, assuming Ui as the masked shared utterance and li as the number of tokens in Ui. Given the token representations for this task {umsuri,t } li t=1 where umsuri,t ∈ Rd, the probability distribution of each masked token can be calculated as\npui,t = softmax(u msur i,t ·Wmsur + bmsur), (7)\nwhere Wmsur ∈ Rd×V is the token embedding table, V denotes the vocabulary size, and bmsur ∈ RV is a bias vector. Finally, the pre-training objective of this self-supervised task is to minimize the negative log-likelihood loss as\nLmsur = − 1\nli li∑ t=1 log pui,t , (8)\nwhere pui,t is the element in pui,t corresponding to the original token."
    }, {
      "heading" : "3.3.2 Shared Node Detection",
      "text" : "A full MPC instance can be divided into several sub-conversations and we assume that the representations of sub-conversations under the same parent node tend to be similar. As illustrated in Figure 2 (b), two sub-conversations {U3, U5, U7, U8} and {U4, U6, U9} share the same parent node U2. Thus, they should be semantically relevant. Under this assumption, we design a self-supervised task named shared node detection (SND), which utilizes the conversation structure to strengthen the capability of models on measuring the semantic relevance of two sub-conversations.\nWe first construct the pre-training samples for this task. Empirically, only the sub-conversations under the top shared node in a conversation are collected in order to filter out the sub-conversations with few utterances. Given a full MPC, the two sub-conversations with the most utterances form a positive pair. For each positive pair, we replace one of its elements with another sub-conversation randomly sampled from the training corpus to form a negative pair.\nFormally, given two sub-conversations ci and cj , utterances in each sub-conversation are first concatenated respectively to form two segments. Then, the two segments are concatenated with a [SEP] token and a [CLS] token is inserted at the beginning of the whole sequence. This sequence are encoded by BERT to derive the contextualized\nrepresentation for the [CLS] token. A non-linear transformation with sigmoid activation is further applied to this representation for calculating the matching score mij , i.e., the probability of ci and cj sharing the same parent node. Finally, the pretraining objective of this task is to minimize the cross-entropy loss as\nLsnd = −[yijlog(mij) + (1− yij)log(1−mij)], (9) where yij = 1 if ci and cj share the same parent node and yij = 0 otherwise."
    }, {
      "heading" : "3.4 Multi-task Learning",
      "text" : "In addition, we also adopt the tasks of masked language model (MLM) and next sentence prediction (NSP) in original BERT pre-training (Devlin et al., 2019), which have been proven effective for incorporating domain knowledge (Gu et al., 2020; Gururangan et al., 2020). Finally, MPCBERT is trained by performing multi-task learning that minimizes the sum of all loss functions as\nL = Lrur + Liss + Lpcd + Lmsur + Lsnd + Lmlm + Lnsp.\n(10)"
    }, {
      "heading" : "4 Downstream Tasks",
      "text" : ""
    }, {
      "heading" : "4.1 Addressee Recognition",
      "text" : "Given a multi-party conversation where part of the addressees are unknown, Ouchi and Tsuboi (2016) and Zhang et al. (2018a) recognized an addressee of the last utterance. Le et al. (2019) recognized addressees of all utterances in a conversation. In this paper, we follow the more challenging setting in Le et al. (2019).\nFormally, models are asked to predict {ân}Nn=1 given {(sn, un, an)}Nn=1\\{an}Nn=1, where ân is selected from the interlocutor set in this conversation and \\ denotes exclusion. When applying MPC-BERT, this task is reformulated as finding a preceding utterance from the same addressee. Its RUR matching scores with all preceding utterances are calculated following Eq. (1). Then, the utterance with the highest score is selected and the speaker of the selected utterance is considered as the recognized addressee. Finally, the fine-tuning objective of this task is to minimize the crossentropy loss as\nLar = − N∑ i=2 i−1∑ j=1 yij log(mij), (11)\nwhere mij is defined in Eq. (1), yij = 1 if the speaker of Uj is the addressee of Ui and yij = 0 otherwise."
    }, {
      "heading" : "4.2 Speaker Identification",
      "text" : "This task aims to identify the speaker of the last utterance in a conversation. Formally, models are asked to predict ŝN given {(sn, un, an)}Nn=1\\sN , where ŝN is selected from the interlocutor set in this conversation. When applying MPC-BERT, this task is reformulated as identifying the utterances sharing the same speaker. For the last utterance UN , its speaker embedding is masked and its ISS matching scores mNj with all preceding utterances are calculated following Section 3.2.2. The finetuning objective of this task is to minimize the cross-entropy loss as\nLsi = − N−1∑ j=1 yNj log(mNj), (12)\nwhere yNj = 1 if Uj shares the same speaker with UN and yNj = 0 otherwise."
    }, {
      "heading" : "4.3 Response Selection",
      "text" : "This task asks models to select ûN from a set of response candidates given the conversation context {(sn, un, an)}Nn=1\\uN . The key is to measure the similarity between two segments of context and response. We concatenate each response candidate with the context and extract the contextualized representation e[CLS] for the first [CLS] token using MPC-BERT. Then, e[CLS] is fed into a nonlinear transformation with sigmoid activation to obtain the matching score between the context and the response. Finally, the fine-tuning objective of this task is to minimize the cross-entropy loss according to the true/false labels of responses in the training set as\nLrs = −[ylog(mcr)+(1−y)log(1−mcr)], (13)\nwhere y = 1 if the response r is a proper one for the context c; otherwise y = 0."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We evaluated our proposed methods on two Ubuntu IRC benchmarks. One was released by Hu et al. (2019), in which both speaker and addressee labels was provided for each utterance. The other benchmark was released by Ouchi and Tsuboi\n(2016). Here, we adopted the version shared in Le et al. (2019) for fair comparison. The conversation sessions were separated into three categories according to the session length (Len5, Len-10 and Len-15) following the splitting strategy of previous studies (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019). Table 2 presents the statistics of the two benchmarks evaluated in our experiments."
    }, {
      "heading" : "5.2 Baseline Models",
      "text" : "Non-pre-training-based models Ouchi and Tsuboi (2016) proposed a dynamic model DRNN which updated speaker embeddings with the conversation flow. Zhang et al. (2018a) improved DRNN to SI-RNN which updated speaker embeddings role-sensitively. Le et al. (2019) proposed W2W which jointly modeled interlocutors and utterances in a uniform framework, and predicted all addressees.\nPre-training-based models BERT (Devlin et al., 2019) was pre-trained to learn general language representations with MLM and NSP tasks. SABERT (Gu et al., 2020) added speaker embeddings and further pre-trained BERT on a domain-specific corpus to incorporate domain knowledge. We re-implemented SA-BERT with the pre-training corpus used in this paper to ensure fair comparison."
    }, {
      "heading" : "5.3 Implementation Details",
      "text" : "The version of BERT-base-uncased was adopted for all our experiments. For pre-training, GELU (Hendrycks and Gimpel, 2016) was employed as the activation for all non-linear transformations. The Adam method (Kingma and Ba, 2015) was employed for optimization. The learning rate was initialized as 0.00005 and the warmup proportion was set to 0.1. We pre-trained BERT for 10 epochs. The training set of the dateset used in Hu et al. (2019) was employed for pre-training. The maximum utterance number was set to 7. The maximum sequence length was set to 230. The maximum sampling numbers for each example\nwere set to 4 for RUR, 2 for ISS and 2 for PCD. ∆ in Eq. (6) was set to 0.4, achieving the best performance out of {0.2, 0.4, 0.6, 0.8} on the validation set. The pre-training was performed using a GeForce RTX 2080 Ti GPU and the batch size was set to 4.\nFor fine-tuning, some configurations were different according to the characteristics of these datasets. For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230. For the three experimental settings in Ouchi and Tsuboi (2016), the maximum utterance numbers were set to 5, 10 and 15, and the maximum sequence lengths were set to 120, 220 and 320. All parameters in PLMs were updated. The learning rate was initialized as 0.00002 and the warmup proportion was set to 0.1. For Hu et al. (2019), the fine-tuning process was performed for 10 epochs for addressee recognition, 10 epochs for speaker identification, and 5 epochs for response selection. For Ouchi and Tsuboi (2016), the fine-tuning epochs were set to 5, 5 and 3 respectively. The fine-tuning was also performed using a GeForce RTX 2080 Ti GPU. The batch sizes were set to 16 for Hu et al. (2019), and 40, 20, and 12 for the three experimental settings in Ouchi and Tsuboi (2016) respectively. The validation set was used to select the best model for testing.\nAll codes were implemented in the TensorFlow framework (Abadi et al., 2016) and are published to help replicate our results. 1"
    }, {
      "heading" : "5.4 Metrics and Results",
      "text" : "Addressee recognition We followed the metrics of previous work (Le et al., 2019) by employing precision@1 (P@1) to evaluate each utterance with ground truth. Also, a session is marked as positive if the addressees of all its utterances are correctly recognized, which is calculated as accuracy (Acc.).\nTable 3 presents the results of addressee recognition. It shows that MPC-BERT outperforms the best performing model, i.e., SA-BERT, by margins of 3.51%, 2.86%, 3.28% and 5.36% on these test sets respectively in terms of Acc., verifying the effectiveness of the proposed five selfsupervised tasks as a whole. To further illustrate the effectiveness of each task, ablation tests were performed as shown in the last five rows of Table 3. We can observe that all self-supervised tasks are useful as removing any of them causes performance\n1https://github.com/JasonForJoy/MPC-BERT\ndrop. Among the five tasks, RUR plays the most important role, and the tasks focusing on modeling interlocutor structure contribute more than those for utterance semantics.\nSpeaker identification Similarly, P@1 was employed as the evaluation metric of speaker identification for the last utterance of a conversation and the results are shown in Table 4. It shows that MPC-BERT outperforms SA-BERT by margins of 7.66%, 2.60%, 3.38% and 4.24% respectively in terms of P@1. Besides, from the ablation results we find that all tasks are useful for improving the performance of speaker identification and ISS and RUR contribute the most. In particular, removing PCD, MSUR and SND only leads to slight performance drop. The reason might be\nthat the information conveyed by these tasks is redundant.\nResponse selection The Rn@k metrics adopted by previous studies (Ouchi and Tsuboi, 2016; Zhang et al., 2018a) were used here. Each model was tasked with selecting k best-matched responses from n available candidates, and we calculated the recall as Rn@k. Two settings were followed in which k was set to 1 and n was set to 2 or 10.\nTable 5 presents the results of response selection. It shows that MPC-BERT outperforms SABERT by margins of 3.82%, 2.71%, 2.55% and 3.22% respectively in terms of R10@1. Ablation tests show that SND is the most useful task for response selection and the two tasks focusing on the utterance semantics contribute more than those\nfocusing on the interlocutor structures."
    }, {
      "heading" : "5.5 Discussions",
      "text" : "Figure 3 illustrates how the performance of BERT, SA-BERT and MPC-BERT changed with respect to different session lengths on the test sets of Ouchi and Tsuboi (2016). It can be seen that the performance of addressee recognition and speaker identification dropped as the session length increased. The reason might be that longer sessions always contain more interlocutors which increase the difficulties of predicting interlocutors. Meanwhile, the performance of response selection was significantly improved as the session length increased. It can be attributed to that longer sessions enrich the representations of contexts with more details which benefit response selection. Furthermore, as the session length increased, the performance of MPC-BERT dropped more slightly than that of SA-BERT on addressee recognition and\nspeaker identification, and the R10@1 gap between MPC-BERT and SA-BERT on response selection enlarged from 2.71% to 3.22%. These results imply the superiority of MPC-BERT over SA-BERT on modeling long MPCs with complicated structures."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we present MPC-BERT, a pre-trained language model with five self-supervised tasks for MPC understanding. These tasks jointly learn who says what to whom in MPCs. Experimental results on three downstream tasks show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on two benchmarks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for their valuable comments."
    } ],
    "references" : [ {
      "title" : "2019. BERT: pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Dually interactive matching network",
      "author" : [ "Liu. 2019b" ],
      "venue" : null,
      "citeRegEx" : "2019b.,? \\Q2019\\E",
      "shortCiteRegEx" : "2019b.",
      "year" : 2019
    }, {
      "title" : "Don’t stop pretraining",
      "author" : [ "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Smith.,? \\Q2020\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2020
    }, {
      "title" : "GSN: A graph-structured network for multi-party dialogues",
      "author" : [ "Wenpeng Hu", "Zhangming Chan", "Bing Liu", "Dongyan Zhao", "Jinwen Ma", "Rui Yan." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Who is speaking to whom? learning to identify utterance addressee in multi-party conversations",
      "author" : [ "Ran Le", "Wenpeng Hu", "Mingyue Shang", "Zhenjun You", "Lidong Bing", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Le et al\\.,? 2019",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards neural speaker modeling in multi-party conversation: The task, dataset, and models",
      "author" : [ "Zhao Meng", "Lili Mou", "Zhi Jin." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki,",
      "citeRegEx" : "Meng et al\\.,? 2018",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2018
    }, {
      "title" : "Addressee and response selection for multi-party conversation",
      "author" : [ "Hiroki Ouchi", "Yuta Tsuboi." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016,",
      "citeRegEx" : "Ouchi and Tsuboi.,? 2016",
      "shortCiteRegEx" : "Ouchi and Tsuboi.",
      "year" : 2016
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Thirty-First AAAI",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "2019b. One time",
      "author" : [ "Dongyan Zhao", "Rui Yan" ],
      "venue" : null,
      "citeRegEx" : "Zhao and Yan.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhao and Yan.",
      "year" : 2019
    }, {
      "title" : "Sequential matching network",
      "author" : [ "Zhoujun Li" ],
      "venue" : null,
      "citeRegEx" : "Li.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2017
    }, {
      "title" : "Addressee and response",
      "author" : [ "Dragomir R. Radev" ],
      "venue" : null,
      "citeRegEx" : "Radev.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radev.",
      "year" : 2018
    }, {
      "title" : "DIALOGPT : Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-turn response selection for chatbots with deep attention matching network",
      "author" : [ "Xiangyang Zhou", "Lu Li", "Daxiang Dong", "Yi Liu", "Ying Chen", "Wayne Xin Zhao", "Dianhai Yu", "Hua Wu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : ", 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020).",
      "startOffset" : 41,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : ", 2018b, 2020) or retrieval-based manner (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao et al., 2019a,b; Gu et al., 2019a,b, 2020).",
      "startOffset" : 41,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019).",
      "startOffset" : 183,
      "endOffset" : 262
    }, {
      "referenceID" : 5,
      "context" : "Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019).",
      "startOffset" : 183,
      "endOffset" : 262
    }, {
      "referenceID" : 3,
      "context" : "Recently, researchers have paid more attention to a more practical and challenging scenario involving more than two participants, which is well known as multiparty conversation (MPC) (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019; Hu et al., 2019).",
      "startOffset" : 183,
      "endOffset" : 262
    }, {
      "referenceID" : 7,
      "context" : "In addition to returning an appropriate response, predicting who will be the next speaker (Meng et al., 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : ", 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019) are unique and important issues in MPC.",
      "startOffset" : 49,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and who is the addressee of an utterance (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019) are unique and important issues in MPC.",
      "startOffset" : 49,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "3683 spaces are either separate (Ouchi and Tsuboi, 2016) or interactive (Zhang et al.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "One was released by Hu et al. (2019). The other was released by Ouchi and Tsuboi (2016) and has three experimental settings according to session lengths.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "One was released by Hu et al. (2019). The other was released by Ouchi and Tsuboi (2016) and has three experimental settings according to session lengths.",
      "startOffset" : 20,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "Ouchi and Tsuboi (2016) first proposed the task of addressee and response selection and created an MPC corpus for studying this task.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Ouchi and Tsuboi (2016) first proposed the task of addressee and response selection and created an MPC corpus for studying this task. Zhang et al. (2018a) proposed SI-RNN, which updated speaker embeddings role-sensitively for addressee and response selection.",
      "startOffset" : 0,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "Meng et al. (2018) proposed a task of speaker classification as a surrogate task for speaker modeling.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Hu et al. (2019) proposed a graph-structured network (GSN) to model the graphical information flow for response generation.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "Hu et al. (2019) proposed a graph-structured network (GSN) to model the graphical information flow for response generation. Wang et al. (2020) proposed to track the dynamic topic for response selection.",
      "startOffset" : 0,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "Given a multi-party conversation where part of the addressees are unknown, Ouchi and Tsuboi (2016) and Zhang et al.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "Given a multi-party conversation where part of the addressees are unknown, Ouchi and Tsuboi (2016) and Zhang et al. (2018a) recognized an addressee of the last utterance.",
      "startOffset" : 75,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Le et al. (2019) recognized addressees of all utterances in a conversation.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "One was released by Hu et al. (2019), in which both speaker and addressee labels was provided for each utterance.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "3688 Datasets Train Valid Test Hu et al. (2019) 311,725 5,000 5,000",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "The conversation sessions were separated into three categories according to the session length (Len5, Len-10 and Len-15) following the splitting strategy of previous studies (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019).",
      "startOffset" : 174,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "The conversation sessions were separated into three categories according to the session length (Len5, Len-10 and Len-15) following the splitting strategy of previous studies (Ouchi and Tsuboi, 2016; Zhang et al., 2018a; Le et al., 2019).",
      "startOffset" : 174,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "Here, we adopted the version shared in Le et al. (2019) for fair comparison.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Non-pre-training-based models Ouchi and Tsuboi (2016) proposed a dynamic model DRNN which updated speaker embeddings with the conversation flow.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Non-pre-training-based models Ouchi and Tsuboi (2016) proposed a dynamic model DRNN which updated speaker embeddings with the conversation flow. Zhang et al. (2018a) improved DRNN to SI-RNN which updated speaker embeddings role-sensitively.",
      "startOffset" : 30,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "Le et al. (2019) proposed W2W which jointly modeled interlocutors and utterances in a uniform framework, and predicted all addressees.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "The Adam method (Kingma and Ba, 2015) was employed for optimization.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "The training set of the dateset used in Hu et al. (2019) was employed for pre-training.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230. For the three experimental settings in Ouchi and Tsuboi (2016), the maximum utterance numbers were set to 5, 10 and 15, and the maximum sequence lengths were set to 120, 220 and 320.",
      "startOffset" : 4,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230. For the three experimental settings in Ouchi and Tsuboi (2016), the maximum utterance numbers were set to 5, 10 and 15, and the maximum sequence lengths were set to 120, 220 and 320. All parameters in PLMs were updated. The learning rate was initialized as 0.00002 and the warmup proportion was set to 0.1. For Hu et al. (2019), the fine-tuning process was performed for 10 epochs for addressee recognition, 10 epochs for speaker identification, and 5 epochs for response selection.",
      "startOffset" : 4,
      "endOffset" : 440
    }, {
      "referenceID" : 3,
      "context" : "For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230. For the three experimental settings in Ouchi and Tsuboi (2016), the maximum utterance numbers were set to 5, 10 and 15, and the maximum sequence lengths were set to 120, 220 and 320. All parameters in PLMs were updated. The learning rate was initialized as 0.00002 and the warmup proportion was set to 0.1. For Hu et al. (2019), the fine-tuning process was performed for 10 epochs for addressee recognition, 10 epochs for speaker identification, and 5 epochs for response selection. For Ouchi and Tsuboi (2016), the fine-tuning epochs were set to 5, 5 and 3 respectively.",
      "startOffset" : 4,
      "endOffset" : 623
    }, {
      "referenceID" : 3,
      "context" : "For Hu et al. (2019), the maximum utterance number was set to 7 and the maximum sequence length was set to 230. For the three experimental settings in Ouchi and Tsuboi (2016), the maximum utterance numbers were set to 5, 10 and 15, and the maximum sequence lengths were set to 120, 220 and 320. All parameters in PLMs were updated. The learning rate was initialized as 0.00002 and the warmup proportion was set to 0.1. For Hu et al. (2019), the fine-tuning process was performed for 10 epochs for addressee recognition, 10 epochs for speaker identification, and 5 epochs for response selection. For Ouchi and Tsuboi (2016), the fine-tuning epochs were set to 5, 5 and 3 respectively. The fine-tuning was also performed using a GeForce RTX 2080 Ti GPU. The batch sizes were set to 16 for Hu et al. (2019), and 40, 20, and 12 for the three experimental settings in Ouchi and Tsuboi (2016) respectively.",
      "startOffset" : 4,
      "endOffset" : 804
    }, {
      "referenceID" : 5,
      "context" : "Addressee recognition We followed the metrics of previous work (Le et al., 2019) by employing precision@1 (P@1) to evaluate each utterance with ground truth.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "3689 Hu et al. (2019) Ouchi and Tsuboi (2016) Len-5 Len-10 Len-15 P@1 Acc.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Results except ours are cited from Le et al. (2019). Numbers in bold denote that the improvement over the best performing baseline is statistically significant (t-test with p-value < 0.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "Response selection The Rn@k metrics adopted by previous studies (Ouchi and Tsuboi, 2016; Zhang et al., 2018a) were used here.",
      "startOffset" : 64,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "(2019) Ouchi and Tsuboi (2016) Len-5 Len-10 Len-15 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 DRNN (Ouchi and Tsuboi, 2016) - - 76.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "3690 Hu et al. (2019) Ouchi and Tsuboi (2016) Len-5 Len-10 Len-15 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 DRNN (Ouchi and Tsuboi, 2016) - - 76.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "3690 Hu et al. (2019) Ouchi and Tsuboi (2016) Len-5 Len-10 Len-15 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1 DRNN (Ouchi and Tsuboi, 2016) - - 76.",
      "startOffset" : 5,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Results except ours are cited from Ouchi and Tsuboi (2016) and Zhang et al.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Results except ours are cited from Ouchi and Tsuboi (2016) and Zhang et al. (2018a). Numbers in bold denote that the improvement over the best performing baseline is statistically significant (t-test with p-value < 0.",
      "startOffset" : 35,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "Figure 3: Performance of models under different session lengths on the test sets of Ouchi and Tsuboi (2016) on the tasks of (a) addressee recognition, (b) speaker identification and (c) response selection.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Figure 3 illustrates how the performance of BERT, SA-BERT and MPC-BERT changed with respect to different session lengths on the test sets of Ouchi and Tsuboi (2016). It can be seen that the performance of addressee recognition and speaker identification dropped as the session length increased.",
      "startOffset" : 141,
      "endOffset" : 165
    } ],
    "year" : 2021,
    "abstractText" : "Recently, various neural models for multiparty conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPCBERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.",
    "creator" : "LaTeX with hyperref"
  }
}