{
  "name" : "2021.acl-long.267.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "G-Transformer for Document-level Machine Translation",
    "authors" : [ "Guangsheng Bao", "Yue Zhang", "Zhiyang Teng", "Boxing Chen", "Weihua Luo" ],
    "emails" : [ "tengzhiyang}@westlake.edu.cn", "weihua.luowh}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3442–3455\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3442"
    }, {
      "heading" : "1 Introduction",
      "text" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; Läubli et al., 2018). Despite that neural models achieve competitive performances on sentence-\n∗* Corresponding author.\nContext Encoder Source Encoder Target Decoder\nSource\nTranslation\nContext\n(a) Sentence-by-sentence Translation\nSource Encoder Target Decoder\nSource1\nTranslation1\nSource2 …\nTranslation2 …\n(b) Multi-sentence Translation\nlevel MT, the performance of document-level MT is still far from satisfactory.\nExisting methods can be mainly classified into two categories. The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020). Document-level context is integrated into sentence-translation by introducing additional context encoder. The structure of such a model is shown in Figure 1(a). These methods suffer from two limitations. First, the context needs to be encoded separately for translating each sentence, which adds to the runtime complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module.\nThe second category extends the translation unit from a single sentence to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al.,\n2018; Zhang et al., 2020) and the whole document (Junczys-Dowmunt, 2019; Liu et al., 2020). Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019). However, when the whole document is encoded as a single unit for sequence to sequence translation, direct supervised training has been shown to fail (Liu et al., 2020). As a solution, either large-scale pre-training (Liu et al., 2020) or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance. These methods are shown in Figure 1(b). One limitation of such methods is that they require much more training time due to the necessity of data augmentation.\nIntuitively, encoding the whole input document as a single unit allows the best integration of context information when translating the current sentence. However, little work has been done investigating the underlying reason why it is difficult to train such a document-level NMT model. One remote clue is that as the input sequence grows larger, the input becomes more sparse (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017). To gain more understanding, we make dedicated experiments on the influence of input length, data scale and model size for Transformer (Section 3), finding that a Transformer model can fail to converge when training with long sequences, small datasets, or big model size. We further find that for the failed cases, the model gets stuck at local minima during training. In such situation, the attention weights from the decoder to the encoder are flat, with large entropy values. This can be because that larger input sequences increase the challenge for focusing on a local span to translate when generating each target word. In other words, the hypothesis space for target-to-source attention is increased.\nGiven the above observations, we investigate a novel extension of Transformer, by restricting selfattention and target-to-source attention to a local context using a guidance mechanism. As shown in Figure 1(c), while we still encode the input document as a single unit, group tags 1© 2© 3© are assigned to sentences to differentiate their positions. Target-to-source attention is guided by matching the tag of target sentence to the tags of source sentences when translating each sentence, so that the hypothesis space of attention is reduced. Intuitively, the group tags serve as a constraint on attention,\nwhich is useful for differentiating the current sentence and its context sentences. Our model, named G-Transformer, can be thus viewed as a combination of the method in Figure 1(a) and Figure 1(b), which fully separate and fully integrates a sentence being translated with its document level context, respectively.\nWe evaluate our model on three commonly used document-level MT datasets for EnglishGerman translation, covering domains of TED talks, News, and Europarl from small to large. Experiments show that G-Transformer converges faster and more stably than Transformer on different settings, obtaining the state-of-the-art results under both non-pretraining and pre-training settings. To our knowledge, we are the first to realize a truly document-by-document translation model. We release our code and model at https://github.com/baoguangsheng/g-transformer."
    }, {
      "heading" : "2 Experimental Settings",
      "text" : "We evaluate Transformer and G-Transformer on the widely adopted benchmark datasets (Maruf et al., 2019), including three domains for EnglishGerman (En-De) translation.\nTED. The corpus is transcriptions of TED talks from IWSLT 2017. Each talk is used as a document, aligned at the sentence level. tst2016-2017 is used for testing, and the rest for development.\nNews. This corpus uses News Commentary v11 for training, which is document-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing.\nEuroparl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus.\nThe detailed statistics of these corpora are shown in Table 1. We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES (Koehn et al., 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations.\nWe consider three standard model configurations.\nBase Model. Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048\n-5\n0\n5\n10\n15\n20\n25\n30\n35\nd -B LE U\nTokens"
    }, {
      "heading" : "64 128 256 512 1024",
      "text" : "(a) Input Length (Base model with filtered data.)\n-5\n0\n5\n10\n15\n20\n25\n30\n35\nd -B LE U\nInstances 1.25K 2.5K 5K 10K 20K 40K 80K 160K\n(b) Data Scale (Base model with 512 tokens input.)\nFigure 2: Transformer on various input length and data scale.\ndimension hidden vectors. Big Model. We follow the standard Transformer big model (Vaswani et al., 2017), using 6 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors.\nLarge Model. We use the same settings of BART large model (Lewis et al., 2020), which involves 12 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors.\nWe use s-BLEU and d-BLEU (Liu et al., 2020) as the metrics. The detailed descriptions are in Appendix A."
    }, {
      "heading" : "3 Transformer and Long Inputs",
      "text" : "We empirically study Transformer (see Appendix B) on the datasets. We run each experiment five times using different random seeds, reporting the average score for comparison."
    }, {
      "heading" : "3.1 Failure Reproduction",
      "text" : "Input Length. We use the Base model and fixed dataset for this comparison. We split both the training and testing documents from Europarl dataset into instances with input length of 64, 128, 256, 512, and 1024 tokens, respectively. For fair comparison, we remove the training documents with a length of less than 768 tokens, which may favour small input length. The results are shown in Figure 2a. When the input length increases from 256 tokens to 512 tokens, the BLEU score drops dramatically from 30.5 to 2.3, indicating failed training with 512 and 1024 tokens. It demonstrates the difficulty when dealing with long inputs of Trans-\nformer.\nData Scale. We use the Base model and a fixed input length of 512 tokens. For each setting, we randomly sample a training dataset of the expected size from the full dataset of Europarl. The results are shown in Figure 2b. The performance increases sharply when the data scale increases from 20K to 40K. When data scale is equal or less than 20K, the BLEU scores are under 3, which is unreasonably low, indicating that with a fixed model size and input length, the smaller dataset can also cause the failure of the training process. For data scale more than 40K, the BLEU scores show a wide dynamic range, suggesting that the training process is unstable.\nModel Size. We test Transformer with different model sizes, using the full dataset of Europarl and a fixed input length of 512 tokens. Transformer-Base can be trained successfully, giving a reasonable BLEU score. However, the training of the Big and Large models failed, resulting in very low BLEU scores under 3. It demonstrates that the increased model size can also cause the failure with a fixed input length and data scale.\nThe results confirm the intuition that the performance will drop with longer inputs, smaller datasets, or bigger models. However, the BLEU scores show a strong discontinuity with the change of input length, data scale, or model size, falling into two discrete clusters. One is successfully trained cases with d-BLEU scores above 10, and the other is failed cases with d-BLEU scores under 3."
    }, {
      "heading" : "3.2 Failure Analysis",
      "text" : "Training Convergence. Looking into the failed models, we find that they have a similar pattern on loss curves. As an example of the model trained on 20K instances shown in Figure 3a, although the training loss continually decreases during training process, the validation loss sticks at the level of 7, reaching a minimum value at around 9K training steps. In comparison, the successfully trained models share another pattern. Taking the model trained on 40K instances as an example, the loss curves demonstrate two stages, which is shown in Figure 3b. In the first stage, the validation loss similar to the failed cases has a converging trend to the level of 7. In the second stage, after 13K training steps, the validation loss falls suddenly, indicating that the model may escape successfully from local minima. From the two stages of the learning curve, we conclude that the real problem, contradicting our first intuition, is not about overfitting, but about local minima.\nAttention Distribution. We further look into the attention distribution of the failed models, observing that the attentions from target to source are widely spread over all tokens. As Figure 4a shows, the distribution entropy is high for about 8.14 bits on validation. In contrast, as shown in Figure 4b, the successfully trained model has a much lower attention entropy of about 6.0 bits on validation. Furthermore, we can see that before 13K training\nsteps, the entropy sticks at a plateau, confirming with the observation of the local minima in Figure 3b. It indicates that the early stage of the training process for Transformer is difficult.\nFigure 5 shows the self-attention distributions of the successfully trained models. The attention entropy of both the encoder and the decoder drops fast at the beginning, leading to a shrinkage of the attention range. But then the attention entropy gradually increases, indicating an expansion of the attention range. Such back-and-forth oscillation of the attention range may also result in unstable training and slow down the training process."
    }, {
      "heading" : "3.3 Conclusion",
      "text" : "The above experiments show that training failure on Transformer can be caused by local minima. Additionally, the oscillation of attention range may make it worse. During training process, the attention module needs to identify relevant tokens from whole sequence to attend to. Assuming that the sequence length is N , the complexity of the attention distribution increases when N grows from sentence-level to document-level.\nWe propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method."
    }, {
      "heading" : "4 G-Transformer",
      "text" : "An example of G-Transformer is shown in Figure 6, where the input document contains more than 3 sentences. As can be seen from the figure, G-Transformer extends Transformer by augmenting the input and output with group tags (Bao and Zhang, 2021). In particular, each token is assigned a group tag, indicating its sentential index. While\nsource group tags can be assigned deterministically, target tags are assigned dynamically according to whether a generated sentence is complete. Starting from 1, target words copy group tags from its predecessor unless the previous token is </s>, in which case the tag increases by 1. The tags serve as a locality constraint, encouraging target-to-source attention to concentrate on the current source sentence being translated.\nFormally, for a source document X and a target document Y , the probability model of Transformer can be written as\nŶ = argmax Y\nP (Y |X), (1)\nand G-Transformer extends it by having\nŶ = argY max Y,GY\nP (Y,GY |X,GX), (2)\nwhere GX and GY denotes the two sequences of group tags\nGX = {gi = k if wi ∈ sentXk else 0}| |X| i=1, GY = {gj = k if wj ∈ sentYk else 0}| |Y | j=1,\n(3)\nwhere sentk represents the k-th sentence of X or Y . For the example shown in Figure 6, GX = {1, ..., 1, 2, ..., 2, 3, ..., 3, 4, ...} and GY = {1, ..., 1, 2, ..., 2, 3, ..., 3, 4, ...}.\nGroup tags influence the auto-regressive translation process by interfering with the attention mechanism, which we show in the next section. In GTransformer, we use the group-tag sequence GX and GY for representing the alignment between X and Y , and for generating the localized contextual representation of X and Y ."
    }, {
      "heading" : "4.1 Group Attention",
      "text" : "An attention module can be seen as a function mapping a query and a set of key-value pairs to an output (Vaswani et al., 2017). The query, key, value, and output are all vectors. The output is computed by summing the values with corresponding attention weights, which are calculated by matching the query and the keys. Formally, given a set of queries, keys, and values, we pack them into matrix Q, K, and V , respectively. We compute the matrix outputs\nAttention(Q,K, V ) = softmax ( QKT√ dk ) V, (4)\nwhere dk is the dimensions of the key vector. Attention allows a model to focus on different positions. Further, multi-head attention (MHA)\nallows a model to gather information from different representation subspaces\nMHA(Q,K, V ) = Concat(head1, ..., headh)WO,\nheadi = Attention(QWQi ,KW K i , V W V i ),\n(5)\nwhere the projections of WO, WQi , W K i , and W V i are parameter matrices. We update Eq 4 using group-tags, naming it group attention (GroupAttn). In addition to inputs Q, K, and V , two sequences of group-tag inputs are involved, where GQ corresponds to Q and GK corresponds to K. We have\nargs = (Q,K, V,GQ, GK), GroupAttn(args) = softmax ( QKT√ dk +M(GQ, GK) ) V,\n(6)\nwhere function M(·) works as an attention mask, excluding all tokens outside the sentence. Specifically, M(·) gives a big negative number γ to make softmax close to 0 for the tokens with a different group tag compared to current token\nM(GQ, GK) = min(1, abs(GQITK − IQGTK)) ∗ γ, (7)\nwhere IK and IQ are constant vectors with value 1 on all dimensions, that IK has dimensions equal to the length of GK and IQ has dimensions equal to the length of GQ. The constant value γ can typically be −1e8.\nSimilar to Eq 5, we use group multi-head attention\nargs = (Q,K, V,GQ, GK),\nGroupMHA(args) = Concat(head1, ..., headh)WO, (8)\nwhere\nheadi = GroupAttn(QWQi ,KW K i , V W V i , GQ, GK),\n(9)\nand the projections of WO, WQi , W K i , and W V i are parameter matrices. Encoder. For each layer a group multi-head attention module is used for self-attention, assigning the same group-tag sequence for the key and the value that GQ = GK = GX .\nDecoder. We use one group multi-head attention module for self-attention and another group multihead attention module for cross-attention. Similar to the encoder, we assign the same group-tag sequence to the key and value of the self-attention, that GQ = GK = GY , but use different group-tag sequences for cross-attention that GQ = GY and GK = GX .\nComplexity. Consider a document with M sentences and N tokens, where each sentence contains N/M tokens on average. The complexities of both the self-attention and cross-attention in Transformer are O(N2). In contrast, the complexity of group attention in G-Transformer is O(N2/M) given the fact that the attention is restricted to a local sentence. Theoretically, since the average length N/M of sentences tends to be constant, the time and memory complexities of group attention are approximately O(N), making training and inference on very long inputs feasible."
    }, {
      "heading" : "4.2 Combined Attention",
      "text" : "We use only group attention on lower layers for local sentence representation, and combined attention on top layers for integrating local and global context information. We use the standard multihead attention in Eq 5 for global context, naming it global multi-head attention (GlobalMHA). Group multi-head attention in Eq 8 and global multi-head attention are combined using a gate-sum module (Zhang et al., 2016; Tu et al., 2017)\nHL = GroupMHA(Q,K, V,GQ, GK), HG = GlobalMHA(Q,K, V ), g = sigmoid([HL, HG]W + b), H = HL g +HG (1− g),\n(10)\nwhere W and b are linear projection parameters, and denotes element-wise multiplication.\nPrevious study (Jawahar et al., 2019) shows that the lower layers of Transformer catch more local syntactic relations, while the higher layers represent longer distance relations. Based on these findings, we use combined attention only on the top\nlayers for integrating local and global context. By this design, on lower layers, the sentences are isolated from each other, while on top layers, the crosssentence interactions are enabled. Our experiments show that the top 2 layers with global attention are sufficient for document-level NMT, and more layers neither help nor harm the performance."
    }, {
      "heading" : "4.3 Inference",
      "text" : "During decoding, we generate group-tag sequence GY according to the predicted token, starting with 1 at the first <s> and increasing 1 after each </s>. We use beam search and apply the maximum length constraint on each sentence. We generate the whole document from start to end in one beam search process, using a default beam size of 5."
    }, {
      "heading" : "5 G-Transformer Results",
      "text" : "We compare G-Transformer with Transformer baselines and previous document-level NMT models on both non-pretraining and pre-training settings. The detailed descriptions about these training settings are in Appendix C.1. We make statistical significance test according to Collins et al. (2005)."
    }, {
      "heading" : "5.1 Results on Non-pretraining Settings",
      "text" : "As shown in Table 2, the sentence-level Transformer outperforms previous document-level models on News and Europarl. Compared to this strong baseline, our randomly initialized model of G-Transformer improves the s-BLEU by 0.81 point on the large dataset Europarl. The results on the small datasets TED and News are worse, indicating overfitting with long inputs. When GTransformer is trained by fine-tuning the sentence-\nlevel Transformer, the performance improves on the three datasets by 0.3, 0.33, and 1.02 s-BLEU points, respectively.\nDifferent from the baseline of document-level Transformer, G-Transformer can be successfully trained on small TED and News. On Europarl, G-Transformer outperforms Transformer by 0.77 d-BLEU point, and G-Transformer fine-tuned on sentence-level Transformer enlarges the gap to 0.98 d-BLEU point.\nG-Transformer outperforms previous documentlevel MT models on News and Europarl with a significant margin. Compared to the best recent model Hyrbid-Context, G-Transformer improves the s-BLEU on Europarl by 1.99. These results suggest that in contrast to previous short-context models, sequence-to-sequence model taking the whole document as input is a promising direction."
    }, {
      "heading" : "5.2 Results on Pre-training Settings",
      "text" : "There is relatively little existing work about document-level MT using pre-training. Although Flat-Transformer+BERT gives a state-of-the-art scores on TED and Europarl, the score on News is worse than previous non-pretraining model HAN (Miculicich et al., 2018b). G-Transformer+BERT improves the scores by margin of 0.20, 1.62, and 0.47 s-BLEU points on TED, News, and Europarl, respectively. It shows that with a better contextual representation, we can further improve documentlevel MT on pretraining settings.\nWe further build much stronger Transformer baselines by fine-tuning on mBART25 (Liu et al., 2020). Taking advantage of sequence-to-sequence pre-training, the sentence-level Transformer gives much better s-BLEUs of 27.78, 29.90, and 31.87, respectively. G-Transformer fine-tuned on mBART25 improves the performance by 0.28, 0.44, and 0.87 s-BLEU, respectively. Compared to the document-level Transformer baseline, GTransformer gives 1.74, 1.22, and 0.31 higher d-BLEU points, respectively. It demonstrates that even with well-trained sequence-to-sequence model, the locality bias can still enhance the performance."
    }, {
      "heading" : "5.3 Convergence",
      "text" : "We evaluate G-Transformer ad Transformer on various input length, data scale, and model size to better understand that to what extent it has solved the convergence problem of Transformer.\nInput Length. The results are shown in Figure 7a. Unlike Transformer, which fails to train on long input, G-Transformer shows stable scores for inputs containing 512 and 1024 tokens, suggesting that with the help of locality bias, a long input does not impact the performance obviously.\nData Scale. As shown in Figure 7b, overall GTransformer has a smooth curve of performance on the data scale from 1.25K to 160K. The variances of the scores are much lower than Transformer, indicating stable training of G-Transformer. Additionally, G-Transformer outperforms Transformer by a large margin on all the settings.\nModel Size. Unlike Transformer, which fails to train on Big and Large model settings, GTransformer shows stable scores on different model sizes. As shown in Appendix C.2, although performance on small datasets TED and News drops largely for Big and Large model, the performance on large dataset Europarl only decreases by 0.10 d-BLEU points for the Big model and 0.66 for the Large model.\nLoss. Looking into the training process of the above experiments, we see that both the training and validation losses of G-Transformer converge much faster than Transformer, using almost half time to reach the same level of loss. Furthermore, the validation loss of G-Transformer converges to much lower values. These observations demonstrate that G-Transformer converges faster and better.\nAttention Distribution. Benefiting from the separate group attention and global attention, GTransformer avoids the oscillation of attention\nrange, which happens to Transformer. As shown in Figure 8a, Transformer sticks at the plateau area for about 13K training steps, but G-Transformer shows a quick and monotonic convergence, reaching the stable level using about 1/4 of the time that Transformer takes. Through Figure 8b, we can find that G-Transformer also has a smooth and stable curve for the convergence of self-attention distribution. These observations imply that the potential conflict of local sentence and document context can be mitigated by G-Transformer."
    }, {
      "heading" : "5.4 Discussion of G-Transformer",
      "text" : "Document Context. We study the contribution of the source-side and target-side context by removing the cross-sentential attention in Eq 10 from the encoder and the decoder gradually. The results are shown in Table 3. We take the G-Transformer fine-tuned on the sentence-level Transformer as our starting point. When we disable the targetside context, the performance decreases by 0.14 s-BLEU point on average, which indicates that the target-side context does impact translation performance significantly. When we further remove the source-side context, the performance decrease by 0.49, 0.83, and 0.77 s-BLEU point on TED, News, and Europarl, respectively, which indicates that the source-side context is relatively more important for document-level MT.\nTo further understand the impact of the sourceside context, we conduct an experiment on automatic evaluation on discourse phenomena which rely on source context. We use the human labeled evaluation set (Voita et al., 2019b) on English-\nRussion (En-Ru) for deixis and ellipsis. We follow the Transformer concat baseline (Voita et al., 2019b) and use both 6M sentence pairs and 1.5M document pairs from OpenSubtitles2018 (Lison et al., 2018) to train our model. The results are shown in Table 4. G-Transformer outperforms Transformer baseline concat (Voita et al., 2019b) with a large margin on three discourse features, indicating a better leverage of the source-side context. When compared to previous model LSTM-T, G-Transformer achieves a better ellipsis on both infl. and VP. However, the score on deixis is still lower, which indicates a potential direction that we can investigate in further study.\nWord-dropout. As shown in Table 5, worddropout (Appendix C.1) contributes about 0.37 dBLEU on average. Its contribution to TED and News is obvious in 0.35 and 0.58 d-BLEU, respectively. However, for large dataset Europarl, the contribution drops to 0.17, suggesting that with sufficient data, word-dropout may not be necessary.\nLocality Bias. In G-Transformer, we introduce locality bias to the language modeling of source and target, and locality bias to the translation between source and target. We try to understand these biases by removing them from G-Transformer. When all the biases removed, the model downgrades to a document-level Transformer. The results are shown in Table 5. Relatively speaking, the contribution of language locality bias is about 1.78 d-BLEU on average. While the translation locality bias contributes for about 14.68 d-BLEU on average, showing critical impact on the model convergence on small datasets. These results suggest that the locality bias may be the key to train\nwhole-document MT models, especially when the data is insufficient.\nCombined Attention. In G-Transformer, we enable only the top K layers with combined attention. On Europarl7, G-Transformer gives 33.75, 33.87, and 33.84 d-BLEU with top 1, 2, and 3 layers with combined attention, respectively, showing that K = 2 is sufficient. Furthermore, we study the effect of group and global attention separately. As shown in Table 6, when we replace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information."
    }, {
      "heading" : "6 Related Work",
      "text" : "The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality.\nA line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models. Liu et al. (2020) shows that Transformer trained directly on documentlevel dataset can fail, resulting in unreasonably low BLEU scores. Following these studies, we also model translation on the whole document. We solve the training challenge using a novel locality bias with group tags.\nAnother line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019). Different from these approaches, G-Transformer uses a generic design for both\nsource and context, translating whole document in one beam search instead of sentence-by-sentence. Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020). In contrast, G-Transformer uses a single model, which reduces the complexity for both training and inference.\nThe locality bias we introduce to G-Transformer is different from the ones in Longformer (Beltagy et al., 2020) and Reformer (Kitaev et al., 2020) in the sense that we discuss locality in the context of representing the alignment between source sentences and target sentences in document-level MT. Specifically, Longformer introduces locality only to self-attention, while G-Transformer also introduces locality to cross-attention, which is shown to be the key for the success of G-Transformer. Reformer, basically same as Transformer, searches for attention targets in the whole sequence, while G-Transformer mainly restricts the attention inside a local sentence. In addition, the motivations are different. While Longformer and Reformer focus on the time and memory complexities, we focus on attention patterns in cases where a translation model fails to converge during training."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We investigated the main reasons for Transformer training failure in document-level MT, finding that target-to-source attention is a key factor. According to the observation, we designed a simple extension of the standard Transformer architecture, using group tags for attention guiding. Experiments show that the resulting G-Transformer converges fast and stably on small and large data, giving the state-of-the-art results compared to existing models under both pre-training and random initialization settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their valuable feedback. We thank Westlake University High-Performance Computing Center for supporting on GPU resources. This work is supported by grants from Alibaba Group Inc. and Sichuan Lan-bridge Information Technology Co.,Ltd."
    }, {
      "heading" : "A Evaluation Metrics",
      "text" : "Following Liu et al. (2020), we use sentence-level BLEU score (s-BLEU) as the major metric for our evaluation. However, when document-level Transformer is compared, we use document-level BLEU score (d-BLEU) since the sentence-to-sentence alignment is not available.\ns-BLEU. To calculate sentence-level BLEU score on document translations, we first split the translations into sentences, mapping to the corresponding source sentences. Then we calculate the BLEU score on pairs of translation and reference of the same source sentence.\nd-BLEU. When the alignments between translation and source sentences are not available, we calculate the BLEU score on document-level, matching n-grams in the whole document."
    }, {
      "heading" : "B Transformer",
      "text" : "B.1 Model\nTransformer (Vaswani et al., 2017) has an encoderdecoder structure, using multi-head attention and feed-forward network as basic modules. In this paper, we mainly concern about the attention module.\nAttention. An attention module works as a function, mapping a query and a set of key-value pairs to an output, that the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a matching function of the query with the corresponding key. Formally, for matrix inputs of query Q, key K, and value V ,\nAttention(Q,K, V ) = softmax ( QKT√ dk ) V, (11)\nwhere dk is the dimensions of the key vector. Multi-Head Attention. Build upon single-head attention module, multi-head attention allows the model to attend to different positions of a sequence, gathering information from different representation subspaces by heads.\nMultiHead(Q,K, V ) = Concat(head1, ..., headh)WO, (12)\nwhere\nheadi = Attention(QWQi ,KW K i , V W V i ), (13)\nthat the projections of WO, WQi , W K i , and W V i are parameter matrices.\nEncoder. The encoder consists of a stack of N identical layers. Each layer has a multi-head selfattention, stacked with a feed-forward network. A residual connection is applied to each of them.\nDecoder. Similar as the encoder, the decoder also consists of a stack of N identical layers. For each layer, a multi-head self-attention is used to represent the target itself, and a multi-head crossattention is used to attend to the encoder outputs. The same structure of feed-forward network and residual connection as the encoder is used.\nB.2 Training Settings\nWe build our experiments based on Transformer implemented by Fairseq (Ott et al., 2019). We use shared dictionary between source and target, and use a shared embedding table between the encoder and the decoder. We use the default setting proposed by Transformer (Vaswani et al., 2017), which uses Adam optimizer with β1 = 0.9 and β2 = 0.98, a learning rate of 5e−4, and an inversesquare schedule with warmup steps of 4000. We apply label-smoothing of 0.1 and dropout of 0.3 on all settings. To study the impact of input length, data scale, and model size, we take the learning rate and other settings as controlled variables that are fixed for all experiments. We determine the number of updates/steps automatically by early stop on validation set. We train base and big models on 4 GPUs of Navidia 2080ti, and large model on 4 GPUs of v100."
    }, {
      "heading" : "C G-Transformer",
      "text" : "C.1 Training Settings\nWe generate the corresponding group tag sequence dynamically in the model according to the special sentence-mark tokens <s> and </s>. Taking a document “<s> there is no public transport . </s> <s> local people struggle to commute . </s>” as an example, a group-tag sequence G = {1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2} is generated according to Eq 3, where 1 starts on the first <s> and ends on the first </s>, 2 the second, and so on. The model can be trained either randomly initialized or fine-tuned.\nRandomly Initialized. We use the same settings as Transformer to train G-Transformer, using label-smoothing of 0.1, dropout of 0.3, Adam optimizer, and a learning rate of 5e − 4 with 4000 warmup steps. To encourage inferencing the translation from the context, we apply a word-dropout\n(Bowman et al., 2016) with a probability of 0.3 on both the source and the target inputs.\nFine-tuned on Sentence-Level Transformer. We use the parameters of an existing sentencelevel Transformer to initialize G-Transformer. We copy the parameters of the multi-head attention in Transformer to the group multi-head attention in G-Transformer, leaving the global multi-head attention and the gates randomly initialized. For the global multi-head attention and the gates, we use a learning rate of 5e−4, while for other components, we use a smaller learning rate of 1e− 4. All the parameters are jointly trained using Adam optimizer with 4000 warmup steps. We apply a word-dropout with a probability of 0.1 on both the source and the target inputs.\nFine-tuned on mBART25. Similar as the finetuning on sentence-level Transformer, we also copy parameters from mBART25 (Liu et al., 2020) to G-Transformer, leaving the global multi-head attention and the gates randomly initialized. We following the settings (Liu et al., 2020) to train the model, using Adam optimizer with a learning rate of 3e − 5 and 2500 warmup steps. Here, we do not apply word-dropout, which empirically shows a damage to the performance.\nC.2 Results on Model Size As shown in Table 7, G-Transformer has a relatively stable performance on different model size. When increasing the model size from Base to Big, the performance drops for about 0.24, 1.33, and 0.14 s-BLEU points, respectively. Further to Large model, the performance drops further for about 17.06, 8.54, and 0.53 s-BLEU points, respectively. Although the performance drop on small dataset is large since overfitting on larger model, the drop on large dataset Europarl is relatively small, indicating a stable training on different model size."
    } ],
    "references" : [ {
      "title" : "Contextual handling in neural machine translation: Look behind, ahead and on both sides",
      "author" : [ "Ruchit Rajeshkumar Agrawal", "Marco Turchi", "Matteo Negri." ],
      "venue" : "21st Annual Conference of the European Association for Machine Translation, pages 11–20.",
      "citeRegEx" : "Agrawal et al\\.,? 2018",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextualized rewriting for text summarization",
      "author" : [ "Guangsheng Bao", "Yue Zhang." ],
      "venue" : "The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021.",
      "citeRegEx" : "Bao and Zhang.,? 2021",
      "shortCiteRegEx" : "Bao and Zhang.",
      "year" : 2021
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learn-",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "The mathematics of statistical machine translation: Parameter estimation",
      "author" : [ "Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer." ],
      "venue" : "Computational Linguistics, 19(2):263– 311.",
      "citeRegEx" : "Brown et al\\.,? 1993",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "A hierarchical phrase-based model for statistical machine translation",
      "author" : [ "David Chiang." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan. Association",
      "citeRegEx" : "Chiang.,? 2005",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2005
    }, {
      "title" : "Hierarchical phrase-based translation",
      "author" : [ "David Chiang." ],
      "venue" : "Computational Linguistics, 33(2):201–228.",
      "citeRegEx" : "Chiang.,? 2007",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2007
    }, {
      "title" : "Clause restructuring for statistical machine translation",
      "author" : [ "Michael Collins", "Philipp Koehn", "Ivona Kučerová." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 531–540.",
      "citeRegEx" : "Collins et al\\.,? 2005",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2005
    }, {
      "title" : "Document-level machine translation with word vector models",
      "author" : [ "Eva Martı́nez Garcia", "Cristina España-Bonet", "Lluı́s Màrquez" ],
      "venue" : "In Proceedings of the 18th Annual Conference of the European Association for Machine Translation,",
      "citeRegEx" : "Garcia et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2015
    }, {
      "title" : "Cache-based document-level statistical machine translation",
      "author" : [ "Zhengxian Gong", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland,",
      "citeRegEx" : "Gong et al\\.,? 2011",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2011
    }, {
      "title" : "Discourse in statistical machine translation",
      "author" : [ "Christian Hardmeier." ],
      "venue" : "Ph.D. thesis, Acta Universitatis Upsaliensis.",
      "citeRegEx" : "Hardmeier.,? 2014",
      "shortCiteRegEx" : "Hardmeier.",
      "year" : 2014
    }, {
      "title" : "Docent: A document-level decoder for phrase-based statistical machine translation",
      "author" : [ "Christian Hardmeier", "Sara Stymne", "Jörg Tiedemann", "Joakim Nivre." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics:",
      "citeRegEx" : "Hardmeier et al\\.,? 2013",
      "shortCiteRegEx" : "Hardmeier et al\\.",
      "year" : 2013
    }, {
      "title" : "What does BERT learn about the structure of language",
      "author" : [ "Ganesh Jawahar", "Benoı̂t Sagot", "Djamé Seddah" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft translator at WMT 2019: Towards large-scale document-level neural machine translation",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 225–233, Flo-",
      "citeRegEx" : "Junczys.Dowmunt.,? 2019",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2019
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA. Association for Computational",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for Computational Linguistics.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz J. Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–133.",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "OpenSubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora",
      "author" : [ "Pierre Lison", "Jörg Tiedemann", "Milen Kouylekov." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Lison et al\\.,? 2018",
      "shortCiteRegEx" : "Lison et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple and effective unified encoder for documentlevel machine translation",
      "author" : [ "Shuming Ma", "Dongdong Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3505–3511, Online. As-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1275–",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Selective attention for context-aware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Miculicich et al\\.,? 2018a",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference",
      "citeRegEx" : "Miculicich et al\\.,? 2018b",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
      "author" : [ "Jean Pouget-Abadie", "Dzmitry Bahdanau", "Bart van Merriënboer", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of SSST-8, Eighth Workshop",
      "citeRegEx" : "Pouget.Abadie et al\\.,? 2014",
      "shortCiteRegEx" : "Pouget.Abadie et al\\.",
      "year" : 2014
    }, {
      "title" : "Locality",
      "author" : [ "Luigi Rizzi." ],
      "venue" : "Lingua, 130:169–186.",
      "citeRegEx" : "Rizzi.,? 2013",
      "shortCiteRegEx" : "Rizzi.",
      "year" : 2013
    }, {
      "title" : "Analysing concatenation approaches to document-level NMT in two different domains",
      "author" : [ "Yves Scherrer", "Jörg Tiedemann", "Sharid Loáiciga." ],
      "venue" : "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61,",
      "citeRegEx" : "Scherrer et al\\.,? 2019",
      "shortCiteRegEx" : "Scherrer et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Context gates for neural machine translation",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:87–99.",
      "citeRegEx" : "Tu et al\\.,? 2017",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "HMM-based word alignment in statistical translation",
      "author" : [ "Stephan Vogel", "Hermann Ney", "Christoph Tillmann." ],
      "venue" : "COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics.",
      "citeRegEx" : "Vogel et al\\.,? 1996",
      "shortCiteRegEx" : "Vogel et al\\.",
      "year" : 1996
    }, {
      "title" : "Context-aware monolingual repair for neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Voita et al\\.,? 2019a",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Voita et al\\.,? 2019b",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing context modeling with a query-guided capsule network for document-level translation",
      "author" : [ "Zhengxin Yang", "Jinchao Zhang", "Fandong Meng", "Shuhao Gu", "Yang Feng", "Jie Zhou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Better document-level machine translation with Bayes’ rule",
      "author" : [ "Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:346–360.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Gated neural networks for targeted sentiment analysis",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Duy-Tin Vo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation",
      "author" : [ "Pei Zhang", "Boxing Chen", "Niyu Ge", "Kai Fan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards making the most of context in neural machine translation",
      "author" : [ "Zaixiang Zheng", "Xiang Yue", "Shujian Huang", "Jiajun Chen", "Alexandra Birch." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 26,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 25,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 22,
      "context" : "Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 211
    }, {
      "referenceID" : 11,
      "context" : "Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; Läubli et al., 2018).",
      "startOffset" : 96,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; Läubli et al., 2018).",
      "startOffset" : 96,
      "endOffset" : 134
    }, {
      "referenceID" : 42,
      "context" : "The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 189
    }, {
      "referenceID" : 27,
      "context" : "The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 189
    }, {
      "referenceID" : 45,
      "context" : "The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 189
    }, {
      "referenceID" : 44,
      "context" : "Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019).",
      "startOffset" : 131,
      "endOffset" : 174
    }, {
      "referenceID" : 31,
      "context" : "Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019).",
      "startOffset" : 131,
      "endOffset" : 174
    }, {
      "referenceID" : 22,
      "context" : "However, when the whole document is encoded as a single unit for sequence to sequence translation, direct supervised training has been shown to fail (Liu et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "As a solution, either large-scale pre-training (Liu et al., 2020) or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : ", 2020) or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "One remote clue is that as the input sequence grows larger, the input becomes more sparse (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017).",
      "startOffset" : 90,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "One remote clue is that as the input sequence grows larger, the input becomes more sparse (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017).",
      "startOffset" : 90,
      "endOffset" : 143
    }, {
      "referenceID" : 25,
      "context" : "We evaluate Transformer and G-Transformer on the widely adopted benchmark datasets (Maruf et al., 2019), including three domains for EnglishGerman (En-De) translation.",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : ", 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 36,
      "context" : "Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "We follow the standard Transformer big model (Vaswani et al., 2017), using 6 layers, 16",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "BART large model (Lewis et al., 2020), which involves 12 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "We use s-BLEU and d-BLEU (Liu et al., 2020) as the metrics.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "We propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method.",
      "startOffset" : 38,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "We propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method.",
      "startOffset" : 38,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "We propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method.",
      "startOffset" : 38,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "As can be seen from the figure, G-Transformer extends Transformer by augmenting the input and output with group tags (Bao and Zhang, 2021).",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "An attention module can be seen as a function mapping a query and a set of key-value pairs to an output (Vaswani et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 36,
      "context" : "3447 Method TED News Europarl s-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU SENTNMT (Vaswani et al., 2017) 23.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "08* Fine-tuning on Pre-trained Model Flat-Transformer+BERT (Ma et al., 2020) 26.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 43,
      "context" : "Group multi-head attention in Eq 8 and global multi-head attention are combined using a gate-sum module (Zhang et al., 2016; Tu et al., 2017)",
      "startOffset" : 104,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "Group multi-head attention in Eq 8 and global multi-head attention are combined using a gate-sum module (Zhang et al., 2016; Tu et al., 2017)",
      "startOffset" : 104,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "Previous study (Jawahar et al., 2019) shows that the lower layers of Transformer catch more local syntactic relations, while the higher layers represent longer distance relations.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "Although Flat-Transformer+BERT gives a state-of-the-art scores on TED and Europarl, the score on News is worse than previous non-pretraining model HAN (Miculicich et al., 2018b).",
      "startOffset" : 151,
      "endOffset" : 177
    }, {
      "referenceID" : 39,
      "context" : "We use the human labeled evaluation set (Voita et al., 2019b) on EnglishMethod TED News Europarl Drop G-Transformer (rnd.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : "We follow the Transformer concat baseline (Voita et al., 2019b) and use both 6M sentence pairs and 1.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "5M document pairs from OpenSubtitles2018 (Lison et al., 2018) to train our model.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : "Transformer baseline concat (Voita et al., 2019b) with a large margin on three discourse features, indicating a better leverage of the source-side context.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al.",
      "startOffset" : 46,
      "endOffset" : 86
    }, {
      "referenceID" : 37,
      "context" : "The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al.",
      "startOffset" : 46,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : ", 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al.",
      "startOffset" : 18,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : ", 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature.",
      "startOffset" : 52,
      "endOffset" : 131
    }, {
      "referenceID" : 33,
      "context" : ", 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature.",
      "startOffset" : 52,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : ", 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature.",
      "startOffset" : 52,
      "endOffset" : 131
    }, {
      "referenceID" : 34,
      "context" : "A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 164
    }, {
      "referenceID" : 44,
      "context" : "A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models.",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models.",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 45,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 42,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 27,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 25,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 40,
      "context" : "Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 266
    }, {
      "referenceID" : 38,
      "context" : "Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 176
    }, {
      "referenceID" : 41,
      "context" : "Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "The locality bias we introduce to G-Transformer is different from the ones in Longformer (Beltagy et al., 2020) and Reformer (Kitaev et al.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : ", 2020) and Reformer (Kitaev et al., 2020) in the sense that we discuss locality in the context of representing the alignment between source sentences and target sentences in document-level MT.",
      "startOffset" : 21,
      "endOffset" : 42
    } ],
    "year" : 2021,
    "abstractText" : "Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.",
    "creator" : "LaTeX with hyperref"
  }
}