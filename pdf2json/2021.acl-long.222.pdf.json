{
  "name" : "2021.acl-long.222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training",
    "authors" : [ "Linqing Chen", "Junhui Li", "Zhengxian Gong", "Boxing Chen", "Weihua Luo", "Min Zhang", "Guodong Zhou" ],
    "emails" : [ "gdzhou}@suda.edu.cn,", "lqchen21@gmail.com,", "weihua.luowh}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2851–2861\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2851"
    }, {
      "heading" : "1 Introduction",
      "text" : "Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike\n∗Corresponding Author: Junhui Li. 1If not specified, monolingual documents are all for source-\nside through this paper.\nsentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract useful context from the whole document.2 On the contrary, large-scale of parallel sentence corpora, and especially monolingual document corpora are much easier to find. In this paper, our goal is to break the corpus bottleneck for context-aware NMT by leveraging both largescale sentence-level parallel dataset and monolingual documents. Specifically, we aim to use the former to boost the performance of translation models while employ the latter to enhance the context encoders’ capability of capturing useful context information.\nThere have been several attempts to boost context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model.\nIn order to effectively and simultaneously model\n2We note that not all, but many context-aware NMT models contain a context encoder to extract global context information from the document.\nboth sentence-level parallel dataset and monolingual documents, in this paper we propose a novel cross-task pre-training approach. As shown in Figure 1, we define two pre-training tasks. One learns to translate a sentence from source language to target language while the other learns to translate a document from deliberately noised to original. Importantly, the two pre-training tasks are jointly learned via the same model synchronously. Then we use document-level parallel dataset to fine-tune the properly pre-trained models. Similarly to the pre-training, we can fine-tune the models from both sentence-level and document-level perspectives. Experimental results on four document-level translation tasks show that our approach significantly improves translation performance, suggesting the effectiveness of our approach in modeling both sentence-level parallel dataset and monolingual documents. One nice property of our approach is that the fine-tuned models can be used to translate both sentences and documents."
    }, {
      "heading" : "2 Cross-Task Pre-training",
      "text" : "In the following, we first describe our pre-training tasks defined upon sentence-level parallel dataset and large-scale monolingual documents (Section 2.1). Then we detail our model which caters such pre-training tasks (Section 2.2). Finally, we present our joint pre-training (Section 2.3)."
    }, {
      "heading" : "2.1 Pre-training Tasks",
      "text" : "We define two pre-training tasks in our pre-training. One is on sentence-level parallel dataset while the other is on monolingual documents.\nSentence-level Translation Given large-scale sentence-level parallel dataset, our pre-training task is quite straight, i.e., sentence-level translation.\nDocument-level Restoration Given monolingual documents, our pre-training task is to restore a document from a noised version. To this end, we deliberately corrupt documents by following the two pre-training objectives, which are inspired by both gap sentence objective (Zhang et al., 2020) and masked language model objective (Devlin et al., 2019).\n• Context-Aware Gap Sentence Restoration (CA-GSR). Given a document S with N sentences, we randomly select M sentences as gap sentences and replace them with a mask token [MASK1] to inform the model. The gap sentence ratio is, therefore M/N . For each selected gap sentence, we use its left and right neighbours as input while the gap sentence serves as output. To mimic documentlevel translation task, in the selection the first and the last sentences are always not selected while any two consequent sentences are not both selected.\n• Context-Aware Masked Sentence Restoration (CA-MSR). Given a sentence X , we follow BERT and randomly select 15% tokens in it. The selected tokens are (1) 80% of time replaced by a mask token [MASK2], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged. For a sentence, we use its masked X̂ as input while the original X serves as output.\nBoth CA-GSR and CA-MSR are applied simultaneously with the noised document as context. For convenience of presentation, we use a concrete example to illustrate the input and output of our document-level restoration task. As shown in Figure 2, let assume that a document X contains 6 sentences and the third and fifth sentences (i.e., X3 and X5) are selected as gap sentences while the others are not. On the one hand, for a sentence which is not selected as gap sentence, e.g., X1, we use its masked version (e.g., X̂1) as input while try to predict its original sentence (e.g., X1). On the other hand, for a gap sentence, e.g., X3, we concatenate its left and right neighbouring sentences with separator [MASK1] and try to predict the gap sentence (e.g., X3). As shown in Figure 2, sentences from S1 to S6 constitute document-level input S while sentences from T1 to T6 make up output T . Note that we do not include either gap sentences themselves or their masked version in S, in case the document\ncontext contains obvious hints for generating gap sentences.\nOverall, the pre-training task of document-level restoration is to predict target output T by giving source input S, which is the same as the task of document-level translation, except that in the restoration S and T are in the same language while in the latter the two are in different languages."
    }, {
      "heading" : "2.2 Joint Modeling of Pre-training Tasks",
      "text" : "We use the same model to cater the above two pre-training tasks. Since the task of documentlevel restoration is more complicated than the task of sentence-level translation, we first describe the model for document-level restoration (Section 2.2.1). Then we apply the model for sentencelevel translation (Section 2.2.2)."
    }, {
      "heading" : "2.2.1 Context-Aware Modeling for Document-Level Restoration",
      "text" : "We define some notations before describing our model. Given a document-level source input S = (S1, · · · , SN ) and target output T = (T1, · · · , TN ) with N sentence pairs, we assume each source sentence Si = (si,1, · · · , si,n) consists of n words. We use dm as the size of embedding and hidden state throughout the entire model.\nFigure 3 shows our context-aware model. It contains two parts, namely a global context encoder and a seq2seq model augmented by context representation. Note that for document-level restoration, we take documents as input units.\nGlobal Context Encoder For the i-th input sentence Si in document S, the global context encoder aims to extract useful global context for every word si,j in it. As shown in Figure 3(a), the encoder consists of a stack of Ng identical encoder layers. Each encoder layer consists of four major sub-layers: a self-attention sub-layer, a sentence representation\nsub-layer, a global context attention sub-layer and a feed-forward sub-layer.\nIn the k-th encoder layer, the self-attention sublayer takes A(k)i ∈ Rn×dm as input and computes a new sequence B(k)i with the same length via multihead attention function:\nB (k) i = MultiHead\n( q = A\n(k) i , k = A (k) i , v = A (k) i\n) , (1)\nwhere the output B(k)i is in the shape of Rn×dm ,3 and q, k, v represent the query and key-value pairs in attention mechanism respectively. For the first encoder layer, A(1)i is the addition of Si’s word embedding and its position embedding while for other layers, A(k)i is the output of the proceeding encoder layer.\nIn the k-th encoder layer, the sentence representation sub-layer takes B(k)i as input and computes a vector to represent the sentence through a linear combination with a vector of weights as:\nα (k) i = softmax\n( W 2 tanh ( W 1 ( B\n(k) i\n)T)) (2)\nwhere W 1 ∈ Rdm×dm and W 2 ∈ Rdm are model parameters. The output α(k)i is a n-sized vector. Then the representation vector of sentence Si is the weighted sum of its hidden states:\nC (k) i = α (k) i B (k) i , (3)\nwhere C(k)i is a dm-sized vector. We then stack vectors of all sentences in S into C(k), i.e., C(k) =[ C\n(k) 1 , · · · , C (k) N\n] . Note that C(k) ∈ RN×dm is at\ndocument-level and represents the global context. In the k-th encoder layer, the global context attention sub-layer extracts useful global context for si,j in Si. This is also done via multi-head attention function:\nD (k) i = MultiHead\n( q = B\n(k) i , k = C\n(k), v = C(k) ) , (4)\nwhere the output D(k)i is in the shape of Rn×dm . In the k-th encoder layer, the Feed forward sublayer is applied to each position separately and\n3The actual output of this sub-layer is LayerNorm(B(k)i + A\n(k) i ), where LayerNorm is the layer normalization function. For simplicity, we do not include the residual addition and layer normalization functions in our sub-layers. Note that the sentence representation sub-layer is the only exception which does not have residual addition and layer normalization.\nidentically by two linear transformations with a ReLU activation in between.\nE (k) i = max\n( 0, D\n(k) i W\nF1 + bF1 ) WF2 + bF2, (5)\nwhere WF1,WF2 ∈ Rdm×dm , and bF1, bF2 ∈ Rdm are model parameters.\nWe denote Gi ∈ Rn×dm as the final output of the global context encoder, i.e., Gi = E (Ng) i . That is to say, Gi represents the context representation for sentence Si.\nContext-Aware Model As shown in Figure 3 (b), the seq2seq model is very similar to the standard Transformer, except that it is now equipped with context representation obtained by the global context encoder. For sentence Si, we denote the sentence encoder output as Hi ∈ Rn×dm . To leverage its context representation Gi, we define a gate to linearly combine the two kinds of representation via:\nH ′i = λHi + (1− λ)Gi, (6)\nwhere the gating weight is computed by\nλ = sigmoid ( [Hi;Gi]W G ) , (7)\nwhere WG ∈ R2dm×dm are model parameters. Then we use H ′i to replace Hi as the input to the decoder. We point out that in the global context encoder and sentence encoder, we share the\nself-attention sub-layer and the feed forward sublayer. That is to say, compared to the standard Transformer, we introduce new parameters to cater the sentence representation sub-layers, the global context sub-layers, and the gate mechanism to combine the two kinds of representation in Eq. 6."
    }, {
      "heading" : "2.2.2 Adapting Context-Aware Model to Sentence-Level Translation",
      "text" : "In the first pre-training task, sentence-level translation is context-agnostic and does not require the global context encoder. Therefore, it only uses the sentence encoder and decoder, as shown in Figure 3 (b). Moreover, we turn off the gate mechanism by setting H ′i = Hi. Since we share the two sub-layers of self-attention and feed forward between the sentence encoder and the global context encoder, updating the model by sentence-level translation will have direct impact on the global context encoder too."
    }, {
      "heading" : "2.3 Joint Pre-training Process",
      "text" : "As shown in our experimentation, we share the same vocabulary for pre-training tasks. To train the above two pre-training tasks with a single model, we follow the strategy used in Johnson et al. (2017) and add a preceding language tag to each source and target sentence.\nOur joint pre-training on two tasks falls into the paradigm of multi-task learning (MTL). In training\nstage, we take turns to load the training data of these pre-training tasks. For example, we update model parameters on a batch of training instances from the first task, and then update parameters on a batch of training instances of the other, and the process repeats."
    }, {
      "heading" : "3 Fine-tuning on Document-Level Parallel Dataset",
      "text" : ""
    }, {
      "heading" : "3.1 Fine-tuning Tasks",
      "text" : "Similar to pre-training tasks, we define the following two different fine-tuning tasks from both sentence-level and document-level.\nSentence-level Translation We first extract sentence-level parallel sentence pairs from the document-level parallel dataset for fine-tuning. This fine-tuning task enables the fine-tuned model to translate sentences. In fine-tuning, this task is processed as same as the sentence-level translation task in pre-training.\nDocument-level Translation Given a parallel document (X ,Y) with N sentence pairs (Xi, Yi) |N1 . This fine-tune task is to translate source document X into target document Y. In fine-tuning, this task takes parallel documents as input units and is processed as same as the document-level restoration task in pre-training."
    }, {
      "heading" : "3.2 Fine-tuning Process",
      "text" : "The fine-tuning process is quite similar as the pretraining process in Section 2.3. Specifically, we add a preceding language tag to each sentence. Meanwhile in fine-tuning, we alternatively load batches of the two fine-tuning tasks."
    }, {
      "heading" : "4 Experimentation",
      "text" : "To test the effect of our approach in leveraging sentence-level parallel dataset and monolingual documents, we carry out experiments on Chineseto-English (ZH-EN) and English-to-German (ENDE) translation."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words.4 We use WMT14 EN-DE\n4It consists of LDC2002E18, LDC2003E07, LDC2003E14, news part of LDC2004T08, LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02,\ntranslation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs.5\nWe use Chinese Gigaword (LDC2009T27) and English Gigaword (LDC2012T21) as monolingual document dataset for ZH-EN and En-DE translation, respectively. For efficient training, we split long documents into sub-documents with at most 30 sentences. We have 2.6M (7.3M) subdocuments with 24M (102M) sentences in total for Chinese (English). Upon the monolingual documents, we prepare training instances for the document-level restoration task and set gap sentence ratio to 20%.\nAll Chinese sentences are segmented by Jieba6\nwhile all English and German sentences are tokenized by Moses scripts (Koehn et al., 2007).7 For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) operations (Sennrich et al., 2016).\nFine-tuning data settings. For ZH-EN, we have one translation task on news domain. The document-level parallel corpus of training set include 41K documents with 780K sentence pairs.8 We use the NIST MT 2006 dataset as the development set, and combine the NIST MT 2002, 2003, 2004, 2005, 2008 datasets as test set..\nFor EN-DE, we test three translation tasks in domains of TED talks, News-Commentary and Europarl.\n• TED, which is from IWSLT 2017 MT track (Cettolo et al., 2012). We combine test2016 and test2017 as our test set while the rest as the development set.\n• News, which is from News Commentary v11 corpus.9 We use news-test2015 and newstest2016 as the development set and test set, respectively.\nLDC2009T15, LDC2010T03. 5https://www.statmt.org/wmt14/transla tion-task.html 6https://github.com/messense/jieba-rs 7As related studies, we lowercase English sentences in ZHEN while truecase English and German sentences in EN-DE. 8It consists of LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02, LDC2009T15, LDC2010T03. Note that they are also included in ZH-EN parallel dataset.\n9http://www.casmacat.eu/corpus/news-co mmentary.html\n• Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus.\nAll above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets.\nModel settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it.11 For all translation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., Ng, Ne, and Nd in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respectively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 for optimization, and learning rate as 1, the warm-up step as 16K. In fine-tuning, we fine-tune the models for 200K steps on a single V100 GPU with batch-size 8192, learning rate 0.3, and warm-up step 4K. In inferring, we set the beam size to 5.\n10https://github.com/sameenmaruf/selec tive-attn/tree/master/data\n11Our code is available at https://github.com/str awberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nEvaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sentences or documents as input units, respectively. In the scenario where both sentence-level parallel dataset and monolingual documents are not used, we directly train our models from scratch with the two fine-tuning tasks on the fine-tuning datasets. #2 and #3 in the table show that our model is capable of translating both sentences and documents. Interestingly, when we use sentences as translation units, our models (i.e., #2 Ours-sent) outperform sentence-level Transformer baseline (i.e., #1 who uses sentences as input units in both training and inferring) over all translation tasks with improvement of averaged 1.36 BLEU and 1.72 Meteor. Moreover, when we use documents as translation units, our models (i.e., #3 Ours-doc) achieve further improvement by modeling document-level context. Compared to previous studies, it also shows that our approach surpasses all context-aware baselines on ZH-EN and EN-DE (TED) tasks and achieves the state-ofthe-art on average.\nIn the scenario where both sentence-level parallel dataset and monolingual documents are used,12 similar performance trends also hold. For example, #5 Ours-sent significantly exceeds Transformer\n12For Transformer baseline (i.e., #4 in the table), the two pre-training objectives in document-level restoration are context-agnostic.\nbaseline with 1.85 BLEU and 1.78 Meteor on average while #6 Outs-doc further achieves the best performance.\nAblation study. We take ZH-EN and EN-DE (News) translations as representatives to study the effect of leveraging sentence-level parallel dataset and monolingual documents.\nTable 2 compares the performance on the the test sets of ZH-EN and EN-DE (News) translations in different scenarios. From it, we have the following observations.\n• Using either sentence-level parallel dataset or monolingual documents helps translation for both Transformer baselines and our contextaware models. However, in the presence of sentence-level parallel dataset, the Transformer baselines fail to achieve higher performance with monolingual documents, as we observe performance drops from 46.99 BLEU to 46.30 on Zh-EN, and from 26.89 to 26.80 on EN-DE. In contrary, our models achieve the highest performance by leveraging the two resources. This suggests the effectiveness of our approach in employing the two resources.\n• It is not surprising to find out that the improvement is mainly contributed by using sentencelevel parallel dataset, as translation model is more important than context encoder\n• Finally, our approach consistently outperforms sentence-level Transformer in all scenarios. Encouraging, the performance gap becomes even larger on ZH-EN when more resources are used."
    }, {
      "heading" : "5 Discussion",
      "text" : "Next we use ZH-EN translation to analyze more on how our approach affects translation performance. See Appendix B for parameter analysis and statistics of the pre-trained models."
    }, {
      "heading" : "5.1 Effect of Joint Fine-tuning",
      "text" : "In Section 3 we alternate sentence-level translation and document-level translation in fine-tuning. We investigate the effect of including sentencelevel translation as a fine-tuning task. Table 3 compares the performance with respect to different fine-tuning strategies and different input units in inferring. When we use documents as input units in inferring, the joint fine-tuning strategy provides no advantage. However, when the input units are sentences, the joint fine-tuning strategy outperforms the one not including sentence-level translation in fine-tuning."
    }, {
      "heading" : "5.2 Analysis of Discourse Phenomena",
      "text" : "We also want to examine whether the proposed approach actually learns to utilize document context to resolve discourse inconsistencies. Following Voita et al. (2019b) and Zheng et al. (2020), we use the same datasets to train model and contrastive test set for the evaluation of discourse phenomena for English-Russian by Voita et al. (2019b). There are four test sets in the suite regarding deixis, lexicon consistency, ellipsis (inflection and verb phrase). Each testset contains groups of contrastive examples consisting of a positive translation with correct discourse phenomenon and negative translations with incorrect phenomena. The goal is to figure out if a model is more likely to generate a cor-\nrect translation compared to the incorrect variation. We summarize the results in Table 4, which shows that in different scenarios our models are better at resolving discourse consistencies than contextagnostic baselines."
    }, {
      "heading" : "5.3 Pronoun Translation",
      "text" : "We follow Miculicich et al. (2018) and Tan et al. (2019) to evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Werlen and Popescu-Belis, 2017).\nTable 5 lists the performance of pronoun translation. From it we observe that our proposed approach can well improve the performance of pronoun translations."
    }, {
      "heading" : "5.4 Effect of Gap Sentence Ratio",
      "text" : "A significant hyper-parameter in the pre-training task of document-level restoration is the gap sentence ratio. A low ratio makes the document-level restoration less challenging while choosing gap sentences at a high ratio makes the global context have more overlapped. Table 6 shows that we achieve the best performance when the ratio is set as 20%."
    }, {
      "heading" : "5.5 Effect of Pre-training Objectives",
      "text" : "As shown in Figure 2, we include two pre-training objectives in document-level restoration, i.e, CAGSR and CA-MSR. To investigate the effect of CA-GSR, we use CA-MSR as the only objective in this pre-training task. In this way, the S3 and S5 in Figure 2 (a), for example, will be X̂3 and X̂5, respectively. Table 7 compares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It\nshows the combining objective achieves better performance than using CA-MSR alone."
    }, {
      "heading" : "6 Related Work",
      "text" : "We describe related studies in the following two perspectives."
    }, {
      "heading" : "6.1 Context-Aware NMT",
      "text" : "Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020).\nBesides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach."
    }, {
      "heading" : "6.2 Pre-training for Document-Level NMT",
      "text" : "While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which learns to reconstruct the original document from a noised version. Inspired by BART, mBART (Liu et al.,\n2020) is a model trained on a mixed corpus containing monolingual documents of different languages. Both BART and mBART concatenate sentences in one document into a long sequence, and thus fall into a standard sequence-to-sequence (seq2seq) framework. This is very different from our cross-task pre-training, in which we combine both context-agnostic learning and context-aware learning in a single model."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In order to leverage both large-scale sentence-level parallel dataset and source-side monolingual documents for context-aware NMT, in this paper, we have proposed a novel cross-task pre-training approach, which simultaneously learns to translate a sentence from source language to target language while denoising a document from deliberately noised to original. Upon the pre-trained models, we fine-tune them with document-level parallel dataset from both sentence-level and documentlevel perspectives. Experimental results on multiple document-level translation tasks have demonstrate the effectiveness of our approach. Finally, we also provide insights on how context-aware NMT benefits from our approach."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Natural Science Foundation of China (Grant No. 62036004 and 61876120)."
    }, {
      "heading" : "A Experimental Datasets",
      "text" : "Table 8 summarizes statistics of the four translation tasks. Note that we split long documents into subdocuments with at most 30 sentences for efficient training."
    }, {
      "heading" : "B More Result Analysis",
      "text" : "B.1 Model Parameters Table 9 presents the numbers of parameters for ZHEN and EN-DE translations. Note that for all ENDE translation tasks, the numbers of parameters are same as the vocabulary for them are shared. The table shows that our models introduce very limited parameters to encode document-level context.\nB.2 Statistics on Our Pre-trained models Table 10 presents statistics on our two pre-trained models for ZH-EN and EN-DE translations. With 500K training steps, and within 120 (130) hours we complete 3.0 (1.2) and 35 (20) passes over the sentence-level parallel dataset and monolingual document dataset for Chinese (English), respectively."
    } ],
    "references" : [ {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of NAACL, pages 1304–1313.",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Wit3: Web inventory of transcribed and translated talks",
      "author" : [ "Mauro Cettolo", "Christian Girardi", "Marcello Federico." ],
      "venue" : "Proceedings of EAMT, pages 261–268.",
      "citeRegEx" : "Cettolo et al\\.,? 2012",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2012
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Diving deep into context-aware neural machine translation",
      "author" : [ "Jingjing Huo", "Christian Herold", "Yingbo Gao", "Leonard Dahlmann", "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "Proceedings of WMT, pages 604–616.",
      "citeRegEx" : "Huo et al\\.,? 2020",
      "shortCiteRegEx" : "Huo et al\\.",
      "year" : 2020
    }, {
      "title" : "Does neural machinetranslation benefit from larger context",
      "author" : [ "Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho" ],
      "venue" : "Computing Research Repository,",
      "citeRegEx" : "Jean et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "Google’s multilingual neural machine translation system: Enabling zeroshot translation",
      "author" : [ "Melvin Johnson", "Mike Schuster", "Quoc V. Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Viégas." ],
      "venue" : "TACL, 5:339–351.",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Microsoft translator at wmt 2019: Towards large-scale document-level neural machine translation",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of WMT, pages 225–233.",
      "citeRegEx" : "Junczys.Dowmunt.,? 2019",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2019
    }, {
      "title" : "Dynamic context selection for document-level neural machine translation via reinforcement learning",
      "author" : [ "Xiaomian Kang", "Yang Zhao", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of EMNLP, pages 2242–2254.",
      "citeRegEx" : "Kang et al\\.,? 2020",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "OpenNMT: Opensource toolkit for neural machine translation",
      "author" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush." ],
      "venue" : "Proceedings of ACL 2017, System Demonstrations, pages 67–72.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling coherence for neural machine translation with dynamic and topic caches",
      "author" : [ "Shaohui Kuang", "Deyi Xiong", "Weihua Luo", "Guodong Zhou." ],
      "venue" : "Proceedings of COLING, pages 596–606.",
      "citeRegEx" : "Kuang et al\\.,? 2018",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2018
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of WMT, pages 228–231.",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TACL, 8:726–742.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Using whole document context in neural machine translation",
      "author" : [ "Valentin Mace", "Christophe Servan." ],
      "venue" : "Proceedings of IWSLT.",
      "citeRegEx" : "Mace and Servan.,? 2019",
      "shortCiteRegEx" : "Mace and Servan.",
      "year" : 2019
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of ACL, pages 1275– 1284.",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Selective attention for contextaware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of NAACL, pages 3092–3102.",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of EMNLP, pages 2947– 2954.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Ward Todd", "WeiJing Zhu." ],
      "venue" : "Proceedings of ACL, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of ACL, pages 1715– 1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical modeling of global context for document-level neural machine translation",
      "author" : [ "Xin Tan", "Longyin Zhang", "Deyi Xiong", "Guodong Zhou." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 1576– 1585.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "TACL, 6:407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Context-aware monolingual repair for neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 877–886.",
      "citeRegEx" : "Voita et al\\.,? 2019a",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of ACL, pages 1198–1212.",
      "citeRegEx" : "Voita et al\\.,? 2019b",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of ACL, pages 1264–1274.",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting cross-sentence context for neural machine translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of EMNLP, pages 2826–2831.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Validation of an automatic metric for the accuracy of pronoun translation (apt)",
      "author" : [ "Lesly Miculicich Werlen", "Andrei Popescu-Belis." ],
      "venue" : "Proceedings of Workshop on Discourse in Machine Translation, pages 17–25.",
      "citeRegEx" : "Werlen and Popescu.Belis.,? 2017",
      "shortCiteRegEx" : "Werlen and Popescu.Belis.",
      "year" : 2017
    }, {
      "title" : "Modeling coherence for discourse neural machine translation",
      "author" : [ "Hao Xiong", "Zhongjun He", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of AAAI, pages 7338–7345.",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing context modeling with a query-guided capsule network for document-level translation",
      "author" : [ "Zhengxin Yang", "Jinchao Zhang", "Fandong Meng", "Shuhao Gu", "Yang Feng", "Jie Zhou." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 1527–1537.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Better document-level machine translation with bayes rule",
      "author" : [ "Lei Yu", "Laurent Sartran", "Wojciech Stokowiec", "Wang Ling", "Lingpeng Kong", "Phil Blunsom", "Chris Dyer." ],
      "venue" : "TACL, 8:346–360.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of EMNLP, pages 533–542.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards making the most of context in neural machine translation",
      "author" : [ "Zaixiang Zheng", "Xiang Yue", "Shujian Huang", "Jiajun Chen", "Alexandra Birch." ],
      "venue" : "Proceedings of IJCAI, pages 3983–3989.",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few).",
      "startOffset" : 141,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few).",
      "startOffset" : 141,
      "endOffset" : 216
    }, {
      "referenceID" : 16,
      "context" : "Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few).",
      "startOffset" : 141,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder.",
      "startOffset" : 234,
      "endOffset" : 255
    }, {
      "referenceID" : 32,
      "context" : "To this end, we deliberately corrupt documents by following the two pre-training objectives, which are inspired by both gap sentence objective (Zhang et al., 2020) and masked language model objective (Devlin et al.",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and masked language model objective (Devlin et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "7 For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) operations (Sennrich et al., 2016).",
      "startOffset" : 201,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : "• TED, which is from IWSLT 2017 MT track (Cettolo et al., 2012).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 31,
      "context" : "BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor DocT (Zhang et al., 2018) 7 7 40.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : ", 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "(2019) to evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Werlen and Popescu-Belis, 2017).",
      "startOffset" : 110,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation.",
      "startOffset" : 30,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation.",
      "startOffset" : 30,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation.",
      "startOffset" : 30,
      "endOffset" : 111
    }, {
      "referenceID" : 26,
      "context" : "Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation.",
      "startOffset" : 30,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 26,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 31,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 29,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : ", previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al.",
      "startOffset" : 21,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "BART (Lewis et al., 2020) is a denoising auto-encoder model which learns to reconstruct the original document from a noised version.",
      "startOffset" : 5,
      "endOffset" : 25
    } ],
    "year" : 2021,
    "abstractText" : "Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel dataset. To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents.1 To this end, we propose two pre-training tasks. One learns to translate a sentence from source language to target language on the sentencelevel parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scalelimited parallel documents from both sentencelevel and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents.",
    "creator" : "LaTeX with hyperref"
  }
}