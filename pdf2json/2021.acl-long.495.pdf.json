{
  "name" : "2021.acl-long.495.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-Label Few-Shot Learning for Aspect Category Detection",
    "authors" : [ "Mengting Hu", "Shiwan Zhao", "Honglei Guo", "Chao Xue", "Hang Gao", "Tiegang Gao", "Renhong Cheng", "Zhong Su" ],
    "emails" : [ "knimet}@mail.nankai.edu.cn,", "suzhong}@cn.ibm.com", "xuechao19@jd.com,", "chengrh}@nankai.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6330–6340\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6330"
    }, {
      "heading" : "1 Introduction",
      "text" : "Aspect category detection (ACD) (Pontiki et al., 2014, 2015) is an important task in sentiment analysis. It aims to identify the aspect categories mentioned in a given sentence from a predefined set of aspect categories. For example, in the sentence “the cheesecake is tasty and the staffs are friendly”, two aspect categories, i.e. food and service, are mentioned. The performance of existing approaches for the ACD task (Zhou et al., 2015; Schouten et al., 2018; Hu et al., 2019) relies heavily on the scale of the labeled dataset. They usually suffer from limited data and fail to generalize well to novel aspect categories with only a few labeled\n∗Shiwan Zhao is the corresponding author. †The work was (partially) done in IBM.\ninstances. On the one hand, it is time-consuming and labor-intensive to annotate large-scale datasets. On the other hand, given a large dataset, many long-tail aspects still suffer from data sparsity.\nFew-shot learning (FSL) provides a solution to address the above challenges. FSL learns like a human, identifying novel classes with limited supervised information by exploiting prior knowledge. Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019). Among these methods, the prototypical network (Snell et al., 2017) is a promising approach, which is simple but effective. It follows the meta-learning paradigm by building a collection of N -way Kshot meta-tasks. A meta-task aims to infer a query set with the help of a small labeled support set. It first learns a prototype for each class in the support set. Then the query instance is predicted by measuring the distance with N prototypes in the embedding space.\nIn this paper, we formulate ACD in the FSL\nscenario, which aims to detect aspect categories accurately with limited training instances. However, ACD is a multi-label classification problem since a sentence may contain multiple aspect categories. Most FSL works learn a single-label classifier and can not work well to address the ACD task. The reasons are two-fold. Firstly, the sentences of each class (i.e., aspect category) in the support set are diverse and contain noise from irrelevant aspects. As displayed in Figure 1, there are three classes in the support set, and each class has two instances. The aspect categories food and salon tend to be noise for this meta-task, making it hard to learn a good prototype for each class in the support set. Secondly, the query set is also noisy. Figure 1 demonstrates three different cases. The first sentence mentions two aspects hotel and room cleanliness out of the support set. We need to detect both aspects accurately as multi-label classification. When detecting each of them, the other aspect acts as noise and makes the task hard. The second sentence is an easy case with a single aspect staff owner. The third sentence mentions the aspect staff owner out of the support set, while the aspect service is noise for this meta-task. In summary, the noise from both the support set and query set makes the few-shot ACD a challenging task.\nTo this end, we propose a multi-label FSL method based on the prototypical network (Snell et al., 2017). We alleviate the noise in the support set and query set by two effective attention mechanisms. Concretely, the support-set attention tries to extract the common aspect of each class. By removing the noise (i.e., irrelevant aspects), the supportset attention can yield better prototypes. Then for a query instance, the query-set attention utilizes the prototypes to compute multiple prototype-specific query representations, in which the irrelevant aspects are removed. Given the better prototypes and the corresponding prototype-specific query representations, we can compute accurate distances between the query instance and the prototypes in the embedding space. We detect the aspect categories in the query instance by ranking the distances. To select the positive aspects from the ranking, we design a policy network (Williams, 1992) to learn a dynamic threshold for each instance. The threshold is modeled as the action of the policy network with continuous action space.\nThe main contributions of our work are as follows:\n• We formulate ACD as a multi-label FSL problem and design a multi-label FSL method based on the prototypical network to solve the problem. To the best of our knowledge, we are the first to address ACD in the few-shot scenario.\n• To alleviate the noise from the support set and query set, we design two effective attention mechanisms, i.e., support-set attention and query-set attention.\n• Experimental results on the three datasets demonstrate that our method outperforms strong baselines significantly."
    }, {
      "heading" : "2 Related Work",
      "text" : "Aspect Category Detection Previous works for ACD can mainly be divided into two types: unsupervised and supervised methods. Unsupervised approaches extract aspects by mining semantic association (Su et al., 2006) or co-occurrence frequency (Hai et al., 2011; Schouten et al., 2018). These methods require a large corpus to mine aspect knowledge and have limited performance. Supervised methods address this task via hand-crafted features (Kiritchenko et al., 2014), automatically learning useful representations (Zhou et al., 2015), multi-task learning (Xue et al., 2017; Hu et al., 2019), or topic-attention model (Movahedi et al., 2019). The above methods detect aspect categories out of a pre-defined set, which cannot handle the unseen classes. These challenges motivate us to investigate this task in the few-shot scenario. Few-Shot Learning Few-shot learning (FSL) (Fe-Fei et al., 2003; Fei-Fei et al., 2006) is close to real artificial intelligence, which borrows the learning process from the human. By incorporating the prior knowledge, it obtains new knowledge fast with limited supervised information. Many works have been proposed for FSL, which can be mainly divided into four research directions.\nOne promising direction is distance-based methods. These methods measure the distance between instances in the feature embedding space. The siamese network (Koch et al., 2015) infers the similarity score between an instance pair. Others compare the cosine similarity (Vinyals et al., 2016) or Euclidean distance (Snell et al., 2017). The relation network (Sung et al., 2018) exploits a neural network to learn the distance metric. Afterward,\nGarcia and Bruna (2018) utilize graph convolution network to extract the structural information of classes. The second direction focuses on the optimization of networks. Model-agnostic metalearning (MAML) algorithm (Finn et al., 2017) learns a good initialization of the model and updates the model by a few labeled examples. Meta networks (Munkhdalai and Yu, 2017) achieve rapid generalization via fast parameterization. The third type is based on hallucination (Wang et al., 2018; Li et al., 2020). This research line directly deals with data deficiency by “learning to augment”, which designs a generator on the base classes and then hallucinates novel class data to augment few-shot samples. The last direction introduces a weight generator to predict classification weight given a few novel class samples, either based on attention mechanism (Gidaris and Komodakis, 2018) or Gaussian distribution (Guo and Cheung, 2020).\nA recent work Proto-HATT (Gao et al., 2019) is similar to ours. Proto-HATT is based on the prototypical network (Snell et al., 2017), which deals with the text noise in the relation classification task by employing hybrid attention at both the instancelevel and the feature-level. This method is designed for single-label FSL. Compared with it, our method designs two attention mechanisms to alleviate the noise on the support set and query set, respectively. The collaboration of two attentions helps compute accurate distances between the query instance and prototypes, and then improves multi-label FSL.\nMulti-Label Few-Shot Learning Compared\nwith single-label FSL, the multi-label FSL has been underexplored. Previous works focus on image synthesis (Alfassy et al., 2019) and signal processing (Cheng et al., 2019). Rios and Kavuluru (2018) develop few-shot and zero-shot methods for multilabel text classification when there is a known structure over the label space. Their approach relies on label descriptors and the hierarchical structure of the label spaces, which limits its application in practice. Hou et al. (2020) propose to address the multi-label intent detection task in the FSL scenario. It calibrates the threshold by kernel regression. Different from this work, we learn a dynamic threshold per instance in a reinforced manner."
    }, {
      "heading" : "3 Methodology",
      "text" : "In the few-shot ACD scenario, each meta-task contains a support set S and a query set Q. The metatask is to assign the query instance to the class(es) of the support set. An instance may be a multiaspect sentence. Thus a query sentence may describe more than one class out of the support set1. Therefore, we define the few-shot ACD as a multilabel few-shot classification problem."
    }, {
      "heading" : "3.1 Overview",
      "text" : "Suppose in an N -way K-shot meta-task, the support set is S = {(xi1, ...xiK), yi}Ni=1, where each xi\n1We found that the probability of a query instance belonging to more than one class is around 4.5% in the ACD dataset, i.e. FewAsp, by randomly sampling 10,000 5-way 5-shot meta-tasks with 5 query sentences for each class.\nis a sentence and (xi1, ..., x i K) all contain the aspect category yi. A query instance is (xq,yq), where yq is a binary label vector indicating the aspects in xq out of N classes.\nFigure 2 presents the main network by an example 3-way 2-shot meta-task. It is composed of three modules, i.e., encoder, support-set attention (SA) and query-set attention (QA). Each class in the support set contains K instances, which are fed into the encoder to obtain K encoded sequences. Next, SA module extracts a prototype for this class from the encoded sequences. After obtaining N prototypes, we feed a query instance into the QA module to compute multiple prototype-specific query representations, which are then used to compute the Euclidean distances with the corresponding prototypes. Finally, we normalize the negative distances to obtain the ranking of prototypes and then select the positive predictions (i.e., aspect categories) by a dynamic threshold. Next, we will introduce the modules of our method in detail."
    }, {
      "heading" : "3.2 Encoder",
      "text" : "Given an input sentence x = {w1, w2, ..., wn}, we first map it into an embedding sequence {e1, e2, ..., en} by looking up the pre-trained GloVe embeddings (Pennington et al., 2014). Then we encode the embedding sequence by a convolutional neural network (CNN) (Zeng et al., 2014; Gao et al., 2019). The convolution kernel slides with the window size m over the embedding sequence. We gain the contextual sequence H = {h1,h2, ...,hn}, H ∈ Rn×d:\nhi = CNN(ei−m−1 2 , ..., ei+m−1 2 ) (1)\nwhere CNN(·) is a convolution operation. The advantages of CNN are two-fold: first, the convolution kernel can extract n-gram features on the receptive field. For example, the bi-gram feature of hot dog could help detect the aspect category food; second, CNN enables parallel computing over inputs, which is more efficient (Xue and Li, 2018)."
    }, {
      "heading" : "3.3 Support-set Attention (SA)",
      "text" : "In each class of the support set, the K-shot instances describe a common aspect, i.e., the target aspect of interest2. As shown in Figure 1, two\n2In almost all cases, there is only one common aspect in the K instances. We randomly sample 10,000 5-way 5-shot meta-tasks, and found that the probability of containing more than one common aspect in each class is less than 0.086%. The probability will be much lower in the 10-way scenario.\nsentences, “Cleanliness was great, and the food was really good” and “People have mentioned, bed bugs on yelp!!”, share the common aspect room cleanliness. The former contains two aspect categories room cleanliness and food. In this example meta-task, it is an instance of the class room cleanliness. However, when sampling other meta-tasks, the instance may be used to represent the class food. This leads to confusion and makes learning a good prototype difficult. To deal with the issue brought by multi-aspect sentences, we first need to identify the common aspect. As depicted in the right part of Figure 2, we compute the common aspect vector by the combination of the K-shot instances. We then regard the vector as a condition and inject it into the attention mechanism to make our attention mechanism aspect-wise. Common Aspect Vector The encodedK-shot instances of a class contain one common aspect and some irrelevant aspects. Among these aspects, the common aspect is the majority. Thus, we simply conduct a word-level average to extract the common aspect vector vi ∈ Rd.\nvi = avg(H i1, H i 2, ...,H i K) (2)\nThe average operation highlights the common aspect, but cannot completely eliminate noisy aspects. To further reduce the noise of irrelevant aspects in each instance, we use the common aspect as the condition in the attention mechanism. Aspect-Wise Attention To make the attention mechanism adapt to the condition, we have two designs. First, we directly use the common aspect vector to compute the attention with each instance (see Eq. 4), which filters out the irrelevant aspects of each instance to some extent. Second, we exploit the idea of dynamic conditional network, which has been demonstrated effective in FSL (Zhao et al., 2018). By predicting a dynamic attention matrix with the common aspect vector, our attention mechanism can further adapt to the condition, i.e., the common aspect vector of the class. Specifically, we learn different perspectives of the condition by simply repeating the common aspect vector (Vaswani et al., 2017). Then it is fed into a linear layer to obtain the attention matrix W i for class i.\nW i =W (vi ⊗ eM ) + b (3)\nwhere (vi ⊗ eM ) ∈ ReM×d is the operation repeatedly concatenating vi for eM times. The linear layer has parameter matrix W ∈ Rd×eM and bias\nb ∈ Rd. This layer is shared in the classes of all meta-tasks, which is learned to be class-agnostic. Thus in the testing phase, it can generate aspectwise attention for a novel class.\nThen in class i of the support set, we exploit the common aspect vector and attention matrix to calculate a denoised representation for every instance. The denoised representation rij for the j-th instance is computed as below.\nβ = softmax(vitanh(H ijW i))\nrij = βH i j\n(4)\nIn this way, the support-set attention is adapted to the condition and is also class-specific. Thus it tends to focus on the correct aspect even for a multiaspect sentence representing different classes.\nFinally, the average of denoised representations for K-shot instances is the prototype of this class.\nri = avg(ri1, r i 2, ..., r i K) (5)\nAfter processing all classes in the support set, we obtain N prototypes {r1, r2, ..., rN}."
    }, {
      "heading" : "3.4 Query-set Attention (QA)",
      "text" : "A query instance may also contain multiple aspects, making the sentence noisy. To deal with the noise in a query instance, we select the relevant aspects from the query instance by the QA module. Specifically, we first process the query instance by the encoder and obtain the encoded instance Hq. Then we feed Hq into the QA module to obtain multiple prototype-specific query representations riq by the N prototypes.\nρi = softmax(ritanh(Hq))\nriq = ρ iHq\n(6)\nThe QA module tries to focus on the aspect category which is similar to the prototype. In Eq. 6, the attention is non-parametric. It can reduce the dependence on parameters and can accelerate the adaptation to unseen classes."
    }, {
      "heading" : "3.5 Training Objective",
      "text" : "For a query instance, we compute the Euclidean distance (ED) between each prototype and its prototype-specific query representation, and we obtain N distances. Next, we normalize the negative distances as the final prediction, which is a ranking of the prototypes.\nŷ = softmax(−ED(ri, riq)), i ∈ [1, N ] (7)\nThe training objective is the mean square error (MSE) loss:\nL = ∑ (ŷ − yq)2 (8)\nwhere yq is the ground-truth. We also normalize yq to ensure the consistency between the prediction and the ground-truth. Learning Dynamic Threshold (DT) To select the positive aspects from the ranking (see Eq. 7) for a query instance, we further learn a dynamic threshold. The threshold is modeled by a policy network (Williams, 1992), which has a continuous action space following Beta distribution (Chou et al., 2017). Given a query instance, we define the state as [(r1 − r1q)2; ...; (rN − rNq )2; ŷ]. We feed the state into the policy network and obtain the parameters a and b of a Beta distribution. Then we sample a threshold τ from Beta(τ |a, b). The reward score is the F1 score for this instance based on τ . We also introduce a reference score∗, which is the F1 score based on a baseline action, i.e., the mode of Beta(τ |a, b): a−1a+b−2 . The training objective is defined as below to minimize the negative expected reward.\nLt = −(score− score∗)logP (τ) (9)\nwhere P (τ) is the probability of τ in the Beta distribution. During inference, we select the positive aspects in ŷ with the baseline action."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We construct three few-shot ACD datasets from Yelp aspect (Bauman et al., 2017), which is a largescale multi-domain dataset for aspect recommendation. We group all instances by aspects and choose 100 aspect categories. Following Han et al. (2018), we split the 100 aspects without intersection into 64 aspects for training, 16 aspects for validation, and 20 aspects for testing.\nAccording to the sentence type, i.e., singleaspect or multi-aspect3, we sample different types of sentences from each group and construct three datasets: FewAsp(single), FewAsp(multi), and FewAsp, which are composed of single-aspect, multiaspect, and both types of sentences, respectively. Note that FewAsp is randomly sampled from the original set of each class, which can better reflect the data distribution in real applications. The statistics of the three datasets are shown in Table 1."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "Evaluation Metrics Previous single-label FSL (Snell et al., 2017) usually evaluates performance by accuracy. In the multi-label setting, we choose AUC (Area Under Curve) and macro-f1 as the evaluation metrics. AUC is utilized for model selection and macro-f1 is computed with a threshold. In our experiments, we found that for all methods in three datasets, the overall best thresholds are 0.3 in the 5-way setting and 0.2 in the 10-way setting. Thus we choose them for evaluating the baselines. Training Details We first train the main network with MSE loss L (Eq. 8). Then we initialize the main network with the learned parameters and jointly train the policy network with Lt (Eq. 9). The implementation details are described in the appendix.\n3A sentence contains a single aspect or multiple aspects."
    }, {
      "heading" : "4.3 Compared Methods",
      "text" : "Our approach is named as Proto-AWATT (aspectwise attention). We validate the effectiveness of the proposed method by comparing with the following popular approaches.\n• Matching Network (Vinyals et al., 2016): It is a metric-based attention method, where distance is measured by cosine similarity.\n• Prototypical Network (Snell et al., 2017): It computes the average of embedded support examples for each class as the prototype, and then measures the distance between the embedded query instance and each prototype.\n• Relation Network (Sung et al., 2018): It utilizes a neural network to learn the relation metric.\n• Graph Network (Garcia and Bruna, 2018): It casts FSL as a supervised message passing task by graph neural network.\n• IMP (Allen et al., 2019): It proposes infinite mixture prototypes to represent each class by a set of clusters, with the number of clusters determined directly from the data.\n• Proto-HATT (Gao et al., 2019): It is based on the prototypical network, which deals with the\nnoise with hybrid instance-level and featurelevel attention mechanisms."
    }, {
      "heading" : "4.4 Experimental Analysis",
      "text" : "We report the experimental results of various methods in Table 2, Table 3, Table 4 and Table 5. The best scores on each metric are marked in bold. The experimental results demonstrate the effectiveness of our method. Overall Performance AUC and macro-f1 scores of all the methods are shown in Table 2, Table 3 and Table 4. Firstly, we observe that our method Proto-AWATT achieves the best results on almost all evaluation metrics of the three datasets. This reveals the effectiveness of the proposed method. Secondly, compared to Proto-HATT, Proto-AWATT achieves significant improvement. It is worth noting that the average improvement of macro-f1 on three datasets is 4.99%. This exhibits that the SA and QA modules successfully reduce noise for fewshot ACD. Meanwhile, accurate distance measurement between prototypes and the prototype-specific query representations can facilitate the detection of multiple aspects in the query instance.\nThen we found that all methods on FewAsp(multi) perform consistently worse than the counterparts on FewAsp(single) and FewAsp. This is because more aspects increase the complexity of the dataset. On FewAsp(multi), Proto-AWATT still outperforms other methods in most settings,\nwhich demonstrates the robustness of our model on various data distributions.\nIn general, the 10-way scenario contains much more noise than the 5-way. We observe that compared to Proto-HATT, Proto-AWATT achieves more significant improvements in the 10-way scenario than the 5-way. The results further indicate that Proto-AWATT can really alleviate the noise. Ablation Study Table 5 depicts the results of ablation study. Firstly, without the SA module, the performances of Proto-AWATT drop a lot. In particular, AUC drops by 3.43%, and macro-f1 drops by 17.23% relatively. This verifies that the SA module helps reduce noise and extract better prototypes. We can also see that without attention matrix W i in SA causes consistent decreases on all metrics. This suggests that predicting dynamic attention matrix for each class is effective, which makes the SA module extract better prototypes. Then we found that without the QA module, Proto-AWATT significantly performs worse. This validates that for a query instance, computing multiple prototypespecific query representations helps obtain accurate distances for ranking, which facilitates the multilabel predictions.\nFinally, when removing DT and using a static threshold (τ = 0.2 in the 10-way setting), it causes a slight decrease. This shows that learning dynamic threshold is effective. We further compare DT with two alternative dynamic threshold methods: (1) MS (mean ± standard deviation of the threshold by cross-validation); (2) a kernel regression (KR)\napproach which is proposed by Hou et al. (2020) to calibrate the threshold. Comparing with MS and KR, our method slightly outperforms them. This is because DT benefits from reinforcement learning and directly optimizes the evaluation metrics. Different Encoders We also compare the performances of our method with a strong baseline ProtoHATT when using different encoders to obtain the contextual sequence H . The results are reported in Table 6. The output of pre-trained encoders, i.e., BERT (Devlin et al., 2019) or DistilBERT (Sanh et al., 2019), are directly used as the contextual sequence. We observe that Proto-AWATT significantly outperforms the strong baseline Proto-HATT on all encoders. Effects of Thresholds As depicted in Figure 3, we analyze the impact of different thresholds on the macro-f1 score during inference. We can see that Proto-AWATT without DT consistently outperforms Proto-HATT in various thresholds. Macro-f1 scores of the two methods are getting worse as τ grows. However, the declines in Proto-HATT are more significant. At τ = 0.9, the macro-f1 of Proto-HATT drops nearly to 0. Proto-AWATT without DT still achieves much higher macro-f1. This indicates that the proposed two attention mechanisms help extract an accurate ranking of prototypes. The ranking is less sensitive to the threshold, which makes our method robust and stable. We also found that learning threshold by DT benefits from a reinforced way, which slightly outperforms KR and the best static threshold."
    }, {
      "heading" : "4.5 Visualizations",
      "text" : "We further analyze Proto-AWATT by visualizing the extracted representations from the support set\nand query set, respectively. The representations are visualized by t-SNE (Maaten and Hinton, 2008). To observe the performance in a challenging situation, we choose the testing set from FewAsp(multi) as an example. Support Set Figure 4 presents the visualization of extracted prototypes from two methods. We randomly sample 5 classes and then sample 50 times of 5-way 5-shot meta-tasks for the five classes. Then for each class, we have 50 prototype vectors. We observe that prototype vectors from our approach are more separable than those from Proto-HATT. This further indicates that the SA module can alleviate noise and thus yield better prototypes. Query Set We randomly sample 5 classes and then sample 20 times of 5-way 5-shot meta-tasks for these classes. Each meta-task has 5 query instances per class. Thus we have 25 × 20 = 500 query instances. It is worth noting that our model learns N prototype-specific query representations for each query instance. We choose the representations according to the ground-truth label. However, Proto-HATT only outputs a single representation for a query instance. As depicted in Figure 5, we can see that the representations learned by our method are obviously more separable than those by Proto-HATT. This further reveals that ProtoAWATT can obtain accurate prototype-specific query representations, which contributes to com-\nputing accurate distances."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we formulate the aspect category detection (ACD) task in the few-shot learning (FSL) scenario. Existing FSL methods mainly focus on single-label predictions. They can not work well for the ACD task since a sentence may contain multiple aspect categories. Therefore, we propose a multi-label FSL method based on the prototypical network. Specifically, we design two effective attention mechanisms for the support set and query set to alleviate the noise from both sets. To achieve multi-label inference, we further learn a dynamic threshold per instance by a policy network with continuous action space. Extensive experimental results in three datasets demonstrate that our method outperforms strong baselines significantly."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We sincerely thank all the anonymous reviewers for providing valuable feedback. This work is supported by the National Science and Technology Major Project, China (Grant No. 2018YFB0204304)."
    }, {
      "heading" : "B Experimental Results",
      "text" : "Ablation Study We display the results of ablation study on three datasets in Table 7. Effects of Attention Matrix To explore the effects of the condition on the attention matrix, we compare the performances of Proto-AWATT by setting different repeat times eM in Eq. 3. The results are displayed in Figure 6. We can see that by repeating more times of the common aspect vector, the AUC and macro-f1 score both outperform the results of setting eM = 1. As eM grows, the performances are improved. However, when setting eM as 25 or even 50, the performances decline. A possible reason is that the model tends to overfit the training classes."
    } ],
    "references" : [ {
      "title" : "Laso: Label-set operations networks for multi-label few-shot learning",
      "author" : [ "Amit Alfassy", "Leonid Karlinsky", "Amit Aides", "Joseph Shtok", "Sivan Harary", "Rogerio Feris", "Raja Giryes", "Alex M Bronstein." ],
      "venue" : "(CVPR), pages 6548–6557.",
      "citeRegEx" : "Alfassy et al\\.,? 2019",
      "shortCiteRegEx" : "Alfassy et al\\.",
      "year" : 2019
    }, {
      "title" : "Infinite mixture prototypes for few-shot learning",
      "author" : [ "Kelsey Allen", "Evan Shelhamer", "Hanul Shin", "Joshua Tenenbaum." ],
      "venue" : "(ICML), pages 232–241.",
      "citeRegEx" : "Allen et al\\.,? 2019",
      "shortCiteRegEx" : "Allen et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect based recommendations: Recommending items with the most valuable aspects based on user reviews",
      "author" : [ "Konstantin Bauman", "Bing Liu", "Alexander Tuzhilin." ],
      "venue" : "(KDD), page 717–725.",
      "citeRegEx" : "Bauman et al\\.,? 2017",
      "shortCiteRegEx" : "Bauman et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-label few-shot learning for sound event recognition",
      "author" : [ "Kai-Hsiang Cheng", "Szu-Yu Chou", "Yi-Hsuan Yang." ],
      "venue" : "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP), pages 1–5. IEEE.",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution",
      "author" : [ "Po-Wei Chou", "Daniel Maturana", "Sebastian Scherer." ],
      "venue" : "(ICML), pages 834–843.",
      "citeRegEx" : "Chou et al\\.,? 2017",
      "shortCiteRegEx" : "Chou et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "(NAACL-HLT), pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A bayesian approach to unsupervised one-shot learning of object categories",
      "author" : [ "Li Fe-Fei" ],
      "venue" : "(ICCV), pages 1134–1141.",
      "citeRegEx" : "Fe.Fei,? 2003",
      "shortCiteRegEx" : "Fe.Fei",
      "year" : 2003
    }, {
      "title" : "One-shot learning of object categories",
      "author" : [ "Li Fei-Fei", "Rob Fergus", "Pietro Perona." ],
      "venue" : "(TPAMI), 28(4):594–611.",
      "citeRegEx" : "Fei.Fei et al\\.,? 2006",
      "shortCiteRegEx" : "Fei.Fei et al\\.",
      "year" : 2006
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "(ICML), pages 1126–1135.",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "(AAAI), pages 6407–6414.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot learning with graph neural networks",
      "author" : [ "Victor Garcia", "Joan Bruna." ],
      "venue" : "(ICLR).",
      "citeRegEx" : "Garcia and Bruna.,? 2018",
      "shortCiteRegEx" : "Garcia and Bruna.",
      "year" : 2018
    }, {
      "title" : "Dynamic few-shot visual learning without forgetting",
      "author" : [ "Spyros Gidaris", "Nikos Komodakis." ],
      "venue" : "(CVPR), pages 4367–4375.",
      "citeRegEx" : "Gidaris and Komodakis.,? 2018",
      "shortCiteRegEx" : "Gidaris and Komodakis.",
      "year" : 2018
    }, {
      "title" : "Attentive weights generation for few shot learning via information maximization",
      "author" : [ "Yiluan Guo", "Ngai-Man Cheung." ],
      "venue" : "(CVPR), pages 13499–13508.",
      "citeRegEx" : "Guo and Cheung.,? 2020",
      "shortCiteRegEx" : "Guo and Cheung.",
      "year" : 2020
    }, {
      "title" : "Implicit feature identification via co-occurrence association rule mining",
      "author" : [ "Zhen Hai", "Kuiyu Chang", "Jung-jae Kim." ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Linguistics, pages 393–404.",
      "citeRegEx" : "Hai et al\\.,? 2011",
      "shortCiteRegEx" : "Hai et al\\.",
      "year" : 2011
    }, {
      "title" : "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "author" : [ "Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "(EMNLP), pages 4803–4809.",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "Few-shot learning for multi-label intent detection",
      "author" : [ "Yutai Hou", "Yongkui Lai", "Yushan Wu", "Wanxiang Che", "Ting Liu." ],
      "venue" : "arXiv preprint arXiv:2010.05256.",
      "citeRegEx" : "Hou et al\\.,? 2020",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "CAN: Constrained attention networks for multi-aspect sentiment analysis",
      "author" : [ "Mengting Hu", "Shiwan Zhao", "Li Zhang", "Keke Cai", "Zhong Su", "Renhong Cheng", "Xiaowei Shen." ],
      "venue" : "(EMNLPIJCNLP), pages 4601–4610.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Nrc-canada-2014: Detecting aspects and sentiment in customer reviews",
      "author" : [ "Svetlana Kiritchenko", "Xiaodan Zhu", "Colin Cherry", "Saif Mohammad." ],
      "venue" : "Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), pages 437–442.",
      "citeRegEx" : "Kiritchenko et al\\.,? 2014",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2014
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov." ],
      "venue" : "ICML deep learning workshop.",
      "citeRegEx" : "Koch et al\\.,? 2015",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial feature hallucination networks for fewshot learning",
      "author" : [ "Kai Li", "Yulun Zhang", "Kunpeng Li", "Yun Fu." ],
      "venue" : "(CVPR), pages 13470–13479.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Aspect category detection via topic-attention network",
      "author" : [ "Sajad Movahedi", "Erfan Ghadery", "Heshaam Faili", "Azadeh Shakery." ],
      "venue" : "arXiv preprint arXiv:1901.01183.",
      "citeRegEx" : "Movahedi et al\\.,? 2019",
      "shortCiteRegEx" : "Movahedi et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta networks",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "(ICML), pages 2554–2563.",
      "citeRegEx" : "Munkhdalai and Yu.,? 2017",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2017
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "(EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "SemEval-2015 task 12: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos." ],
      "venue" : "(SemEval 2015), pages 486–495.",
      "citeRegEx" : "Pontiki et al\\.,? 2015",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "(SemEval 2014), pages 27–35.",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimization as a model for few-shot learning",
      "author" : [ "Sachin Ravi", "Hugo Larochelle." ],
      "venue" : "(ICLR), pages 1–11.",
      "citeRegEx" : "Ravi and Larochelle.,? 2017",
      "shortCiteRegEx" : "Ravi and Larochelle.",
      "year" : 2017
    }, {
      "title" : "Fewshot and zero-shot multi-label learning for structured label spaces",
      "author" : [ "Anthony Rios", "Ramakanth Kavuluru." ],
      "venue" : "(EMNLP), pages 3132–3142.",
      "citeRegEx" : "Rios and Kavuluru.,? 2018",
      "shortCiteRegEx" : "Rios and Kavuluru.",
      "year" : 2018
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Supervised and unsupervised aspect category detection for sentiment analysis with co-occurrence data",
      "author" : [ "Kim Schouten", "Onne Van Der Weijde", "Flavius Frasincar", "Rommert Dekker." ],
      "venue" : "IEEE Transactions on Cybernetics, 48(4):1263–1275.",
      "citeRegEx" : "Schouten et al\\.,? 2018",
      "shortCiteRegEx" : "Schouten et al\\.",
      "year" : 2018
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "(NeurIPS), pages 4077–4087.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Using pointwise mutual information to identify implicit features in customer reviews",
      "author" : [ "Qi Su", "Kun Xiang", "Houfeng Wang", "Bin Sun", "Shiwen Yu." ],
      "venue" : "International Conference on Computer Processing of Oriental Languages, pages 22–30.",
      "citeRegEx" : "Su et al\\.,? 2006",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning to compare: Relation network for few-shot learning",
      "author" : [ "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip HS Torr", "Timothy M Hospedales." ],
      "venue" : "(CVPR), pages 1199–1208.",
      "citeRegEx" : "Sung et al\\.,? 2018",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "(NeurIPS), pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Daan Wierstra" ],
      "venue" : "In (NeurIPS),",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Low-shot learning from imaginary data",
      "author" : [ "Yu-Xiong Wang", "Ross Girshick", "Martial Hebert", "Bharath Hariharan." ],
      "venue" : "(CVPR), pages 7278–7286.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning, 8(3-4):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Aspect based sentiment analysis with gated convolutional networks",
      "author" : [ "Wei Xue", "Tao Li." ],
      "venue" : "(ACL), pages 2514–2523.",
      "citeRegEx" : "Xue and Li.,? 2018",
      "shortCiteRegEx" : "Xue and Li.",
      "year" : 2018
    }, {
      "title" : "MTNA: A neural multi-task model for aspect category classification and aspect term extraction on restaurant reviews",
      "author" : [ "Wei Xue", "Wubai Zhou", "Tao Li", "Qing Wang." ],
      "venue" : "(IJCNLP), pages 151–156.",
      "citeRegEx" : "Xue et al\\.,? 2017",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "(COLING), pages 2335–2344.",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamic conditional networks for few-shot learning",
      "author" : [ "Fang Zhao", "Jian Zhao", "Shuicheng Yan", "Jiashi Feng." ],
      "venue" : "(ECCV), pages 19–35.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Representation learning for aspect category detection in online reviews",
      "author" : [ "Xinjie Zhou", "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "(AAAI), page 417–423.",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "The performance of existing approaches for the ACD task (Zhou et al., 2015; Schouten et al., 2018; Hu et al., 2019) relies heavily on the scale of the labeled dataset.",
      "startOffset" : 56,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "The performance of existing approaches for the ACD task (Zhou et al., 2015; Schouten et al., 2018; Hu et al., 2019) relies heavily on the scale of the labeled dataset.",
      "startOffset" : 56,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "The performance of existing approaches for the ACD task (Zhou et al., 2015; Schouten et al., 2018; Hu et al., 2019) relies heavily on the scale of the labeled dataset.",
      "startOffset" : 56,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Many efforts have been devoted to FSL (Ravi and Larochelle, 2017; Finn et al., 2017; Snell et al., 2017; Wang et al., 2018; Gao et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "Among these methods, the prototypical network (Snell et al., 2017) is a promising approach, which is simple but effective.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "To this end, we propose a multi-label FSL method based on the prototypical network (Snell et al., 2017).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 36,
      "context" : "To select the positive aspects from the ranking, we design a policy network (Williams, 1992) to learn a dynamic threshold for each instance.",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "Unsupervised approaches extract aspects by mining semantic association (Su et al., 2006) or co-occurrence frequency (Hai et al.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : ", 2006) or co-occurrence frequency (Hai et al., 2011; Schouten et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : ", 2006) or co-occurrence frequency (Hai et al., 2011; Schouten et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Supervised methods address this task via hand-crafted features (Kiritchenko et al., 2014), automatically learning useful representations (Zhou et al.",
      "startOffset" : 63,
      "endOffset" : 89
    }, {
      "referenceID" : 41,
      "context" : ", 2014), automatically learning useful representations (Zhou et al., 2015), multi-task learning (Xue et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 38,
      "context" : ", 2015), multi-task learning (Xue et al., 2017; Hu et al., 2019), or topic-attention model (Movahedi et al.",
      "startOffset" : 29,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : ", 2015), multi-task learning (Xue et al., 2017; Hu et al., 2019), or topic-attention model (Movahedi et al.",
      "startOffset" : 29,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : ", 2019), or topic-attention model (Movahedi et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Few-Shot Learning Few-shot learning (FSL) (Fe-Fei et al., 2003; Fei-Fei et al., 2006) is close to real artificial intelligence, which borrows the learning process from the human.",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "The siamese network (Koch et al., 2015) infers the similarity score between an instance pair.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : "Others compare the cosine similarity (Vinyals et al., 2016) or Euclidean distance (Snell et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "The relation network (Sung et al., 2018) exploits a neural network to learn the distance metric.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "Model-agnostic metalearning (MAML) algorithm (Finn et al., 2017) learns a good initialization of the model and updates the model by a few labeled examples.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Meta networks (Munkhdalai and Yu, 2017) achieve rapid generalization via fast parameterization.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 35,
      "context" : "The third type is based on hallucination (Wang et al., 2018; Li et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "The third type is based on hallucination (Wang et al., 2018; Li et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "The last direction introduces a weight generator to predict classification weight given a few novel class samples, either based on attention mechanism (Gidaris and Komodakis, 2018) or Gaussian distribution (Guo and Cheung, 2020).",
      "startOffset" : 151,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "The last direction introduces a weight generator to predict classification weight given a few novel class samples, either based on attention mechanism (Gidaris and Komodakis, 2018) or Gaussian distribution (Guo and Cheung, 2020).",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "A recent work Proto-HATT (Gao et al., 2019) is similar to ours.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "Proto-HATT is based on the prototypical network (Snell et al., 2017), which deals with the text noise in the relation classification task by employing hybrid attention at both the instancelevel and the feature-level.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Previous works focus on image synthesis (Alfassy et al., 2019) and signal processing (Cheng et al.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : ", en} by looking up the pre-trained GloVe embeddings (Pennington et al., 2014).",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 39,
      "context" : "Then we encode the embedding sequence by a convolutional neural network (CNN) (Zeng et al., 2014; Gao et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "Then we encode the embedding sequence by a convolutional neural network (CNN) (Zeng et al., 2014; Gao et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 37,
      "context" : "For example, the bi-gram feature of hot dog could help detect the aspect category food; second, CNN enables parallel computing over inputs, which is more efficient (Xue and Li, 2018).",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 40,
      "context" : "Second, we exploit the idea of dynamic conditional network, which has been demonstrated effective in FSL (Zhao et al., 2018).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : "Specifically, we learn different perspectives of the condition by simply repeating the common aspect vector (Vaswani et al., 2017).",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 36,
      "context" : "The threshold is modeled by a policy network (Williams, 1992), which has a continuous action space following Beta distribution (Chou et al.",
      "startOffset" : 45,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "The threshold is modeled by a policy network (Williams, 1992), which has a continuous action space following Beta distribution (Chou et al., 2017).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "We construct three few-shot ACD datasets from Yelp aspect (Bauman et al., 2017), which is a largescale multi-domain dataset for aspect recommendation.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "Evaluation Metrics Previous single-label FSL (Snell et al., 2017) usually evaluates performance by accuracy.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 34,
      "context" : "• Matching Network (Vinyals et al., 2016): It is a metric-based attention method, where distance is measured by cosine similarity.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : "• Relation Network (Sung et al., 2018): It utilizes a neural network to learn the relation metric.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "• Graph Network (Garcia and Bruna, 2018): It casts FSL as a supervised message passing task by graph neural network.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "• IMP (Allen et al., 2019): It proposes infinite mixture prototypes to represent each class by a set of clusters, with the number of clusters determined directly from the data.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "• Proto-HATT (Gao et al., 2019): It is based on the prototypical network, which deals with the",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 28,
      "context" : ", 2019) or DistilBERT (Sanh et al., 2019), are directly used as the contextual sequence.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "The representations are visualized by t-SNE (Maaten and Hinton, 2008).",
      "startOffset" : 44,
      "endOffset" : 69
    } ],
    "year" : 2021,
    "abstractText" : "Aspect category detection (ACD) in sentiment analysis aims to identify the aspect categories mentioned in a sentence. In this paper, we formulate ACD in the few-shot learning scenario. However, existing few-shot learning approaches mainly focus on single-label predictions. These methods can not work well for the ACD task since a sentence may contain multiple aspect categories. Therefore, we propose a multi-label few-shot learning method based on the prototypical network. To alleviate the noise, we design two effective attention mechanisms. The support-set attention aims to extract better prototypes by removing irrelevant aspects. The query-set attention computes multiple prototype-specific representations for each query instance, which are then used to compute accurate distances with the corresponding prototypes. To achieve multilabel inference, we further learn a dynamic threshold per instance by a policy network. Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines.",
    "creator" : "LaTeX with hyperref"
  }
}