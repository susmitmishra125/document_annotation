{
  "name" : "2021.acl-long.240.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "UnitedQA: A Hybrid Approach for Open Domain Question Answering",
    "authors" : [ "Hao Cheng", "Yelong Shen", "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao" ],
    "emails" : [ "chehao@microsoft.com", "yeshe@microsoft.com", "xiaodl@microsoft.com", "penhe@microsoft.com", "wzchen@microsoft.com", "jfgao@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3080–3090\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3080"
    }, {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages.\n∗Equal Contribution\nRecent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively. We hypothesize that extractive and generative readers adopt different answer inference strategies, thus a hybrid extractive/generative reader can be a better option for open-domain QA tasks. As shown in Figure 1, compared with prediction agreement among only generative or extractive readers (topleft and bottom-right), the cross prediction agreement between extractive and generative readers (bottom-left) is relatively low (<50%). It indicates that answers produced by those two types of models are different and they can be complementary to each other. Therefore, we propose a hybrid reader approach, UnitedQA, which is a simple ensemble approach to combine the predictions from extractive and generative readers. It achieves state-of-theart results on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).\nIn UnitedQA, the extractive reader (UnitedQAE) and generative reader (UnitedQA-G) are built upon the pretrained language models, ELECTRA (Clark et al., 2020) and T5 (Raffel et al., 2020), respectively. For the UnitedQA-E, we adopt a weakly-supervised training objective to address the noisy supervision issue caused by the heuristicsbased labeling and incorporate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve the model robustness. The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al., 2019; Jiang et al., 2020; Pereira et al., 2021) to improve the model generalization ability.\nThe experimental results highlight the effec-\ntiveness of the simple hybrid approach of UnitedQA. With both improved extractive and generative readers, UnitedQA sets new state-of-the-art results on two popular open-domain QA datasets, i.e. 54.7 and 70.3 in exact match on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), respectively. It is worth noting that our UnitedQA model not only outperforms each single model but also brings more pronounced improvements over homogeneous ensembles of either extractive or generative readers. Last, based on our analyses, UnitedQA-E and UnitedQA-G have advantages in different cases, suggesting they may use different reasoning strategies."
    }, {
      "heading" : "2 Method",
      "text" : "In this section, we present the overall pipeline of the UnitedQA system, which consists of three components: Retrieval, Reading, and Re-ranking. First, the retrieval module fetches a list of relevant passages from a Wikipedia dump for a given question. Then, the module of hybrid readers produces answer candidates from the set of retrieved passages. Last, the re-ranking module combines the answer candidates with linear interpolation and produce the final answer. Retrieval Following Karpukhin et al. (2020), we consider two methods, BM25 and dense passage retrieval (DPR), for retrieving the support passages\nfor a given question. For BM25, passages are encoded as bag of words (BOW), and inverse document frequencies are used as the ranking function. For DPR, passages and questions are represented as dense vectors based on two BERT (Devlin et al., 2019) models. The relevance score is then computed based on the dot production between the query and passage vectors. In this paper, we adopt the same implementation as Karpukhin et al. (2020) for retrieving passages. Specifically, the English Wikipedia dump from Dec. 20, 2018 is used as the source documents for retrieval, with the removal of semi-structured data, such as tables or lists. Each document is split into disjoint 100-word passages as the basic retrieval unit. The top-100 passages are then passed for reading. Reading We combine the generative reader and the extractive reader to produce answer candidates over the retrieved passages. Here, we only give a highlevel description of our approach. More details regarding our improved extractive and generative models are presented in §2.1 and §2.2 respectively.\nThe generative reader is based on a sequenceto-sequence model pre-trained in a forwardgeneration fashion on a large corpus, i.e. T5 (Raffel et al., 2020). Similar to Izacard and Grave (2021), the model takes the question and its relevant passages as input, and then generates the answer string token by token. Specifically, the concatenation of all retrieved passages and the corresponding question is used as the encoder input. Then, the decoder performs reasoning over the concatenation of all evidence through an attention mechanism.\nFollowing state-of-the-art extractive QA models (Devlin et al., 2019; Karpukhin et al., 2020), our extractive reader is based on a Transformer neural network pre-trained with a cloze style selfsupervised objective, i.e. ELECTRA (Clark et al., 2020). Here, a pair of a given question and a support passage is jointly encoded into neural text representations. These representations are then used to define scores or probabilities of possible answer begin and end positions, which are in turn used to define probabilities over possible answer spans. Finally, the answer string probabilities are based on the aggregation over all possible answer spans from the entire set of support passages."
    }, {
      "heading" : "2.1 UnitedQA-E",
      "text" : "In §2.1.2, we give the problem definition of opendomain QA for extractive reader. Then, we detail\nthe improvements of UnitedQA-E in §2.1.2."
    }, {
      "heading" : "2.1.1 Extractive Reader",
      "text" : "Given a question q and a set of K retrieved passages p1, . . . , pK , a text encoder produces contextualized representations: hk1, ...h k T ∈ Rn for the question-passage pair (q, pk) in the form of “[CLS]question [SEP]passage [SEP]”, where [CLS]and [SEP]are special tokens for encoding inputs, T is the maximum sequence length of the input text, and hki indicates the contextualized embedding of the i-th token in (q, pk).\nThe extractive reader computes the span-begin score of the i-th token as sb(ik) = wTb h k i using a weight vector wb ∈ Rd. The span-end score se(j\nk) is defined in the same way. Thus, the probabilities of a start position ik and an end position jk are Pb(ik) = exp(sb(i k))\nZb , Pe(j\nk) = exp(se(j k))\nZe ,\nwhere Zb, Ze are normalizing factors defined by the corresponding probability space. The probability of an answer span from ik to jk is defined as Ps(i k, jk) = Pb(i k)Pe(j\nk). Here, we consider two probability spaces, passage level and multi-passage level, with the only difference in the computing of Zb, Ze. Specifically, the passage-level probability of each answer begin and end is computed by normalizing all possible positions in the respective passage, i.e. Zb = Z k b = ∑ Ik∪NULL exp(sb(i)), Ze = Z k e =∑\nIk∪NULL exp(se(j)), where Ik is the set of all possible positions from the k-th passage and NULL indicates special positions if pk does not support answering the question. Similarly, the multi-passage level probability is computed by normalizing over each answer positions across all K relevant passages, i.e. Zb = Z∗b = ∑ k ∑ Ik exp(sb(i)), Ze =\nZ∗e = ∑ k ∑ Ik exp(se(j)), respectively.\nSince there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019). During inference, the prediction is made based on the candidate answer string score, obtaining as Pa(y) =∑\n(i,j)∈Y Ps(i, j), where Y is the set of spans corresponding to the answer string y."
    }, {
      "heading" : "2.1.2 Improvement Method",
      "text" : "In addition to better text representations from Clark et al. (2020), we consider two methods for improving the training of the extractive reader.\nMulti-objective for Weakly-supervised QA The multi-objective formulation is introduced in Cheng et al. (2020) for improving weakly supervised document-level QA. Different from Cheng et al. (2020) where only MML is considered for the multi-objective formulation, we found combining HardEM with MML is more effective for open-domain QA based on our experiments (§4.1). Specifically, we combine a multi-passage HardEM loss withK passage-level MML losses over a batch of K passages\nLEXT = log max (i,j) PMs (i, j) +\n1\nK ∑ k log ∑ (ik,jk) PPs (i k, jk), (1)\nwhere PMs , P P s is the multi-passage level and passage level span probabilities respectively. Posterior Differential Regularization Due to the noisy supervision for open-domain QA (Chen et al., 2017), we investigate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve the robustness of the extractive reader. Different from Cheng et al. (2021) where only clean supervision setting is considered, in this work, we apply PDR to the weakly supervised open-domain QA scenario. Given it is computationally expensive to enumerate all possible spans, we apply two separate regularization terms for the begin and end probabilities at the multi-passage level, respectively,\nLPDR = D(Pb(i)|P ′b(i)) +D(Pe(j)|P ′e(j)), (2)\nwhere D(·|·) is the squared Hellinger distance, and P ′b, P ′ e are the probabilities of start and end positions with additive input noise to the token embeddings. Specifically, we sample noise vectors 1, . . . , T from N (0, c2I), and add them to the token embeddings as the noisy input, i.e. v1 + 1, . . . ,vT + T , where c is fixed to 1e−3 throughout our experiments.\nBased on this, the overall training objective for the extractive reader is\nL1 = LEXT + γLPDR, (3)\nwhere γ is a regularization scalar hyperparameter."
    }, {
      "heading" : "2.2 UnitedQA-G",
      "text" : "Here, we first formally define the setup of generative reader for open-domain QA in § 2.2.1 and then present our improvements in § 2.2.2."
    }, {
      "heading" : "2.2.1 Generative Reader",
      "text" : "Given a question q and a set of K retrieved passages p1, . . . , pK , the encoder model encodes each (q, pk) pair independently, and produces contextualized representation for each token: hki ∈ Rd for the i-th token of the k-th pair. The decoder then performs attention over the concatenation of the representations of all the retrieved passages, and generates the answer string.\nLet x denote the input of the question and all retrieved passages x = ( (q, p1), ..., (q, pK) ) , and y the answer string with its tokens as (y1, ..., yN ). The generative reader is trained to maximize a sequence-to-sequence objective for a given (x,y),\nL(x,y; θ) = N∑ i logPθ(yi|x, y1:i−1), (4)\nwhere θ is the model parameter. During inference, a greedy decoding is used to produce the answer."
    }, {
      "heading" : "2.2.2 Improvement Method",
      "text" : "Decoder Attention Bias The decoder in the T5 transformer model adopts a cross-attention mechanism to compute attention scores between the decoding answer tokens and all the retrieved passage tokens. Specifically, let yi ∈ Rd be the query vector of the i-th decoding token1, and mkj ∈ Rd be the key vector of the j-th token in ((q), pk). The multi-head cross-attention scores in T5 (Raffel et al., 2020) ski,j is calculated as\nski,j = MultiHeadAtt(yi,m k j ) ∈ R|Head| (5)\nwhere |Head| is the number of attention heads. However, it doesn’t capture the relevance information of retrieved passages into the reader in (5). To add the relevance feature into the attention block, we revise (5) by incorporating the attention bias\nski,j = MultiHeadAtt(yi,m k j ) + bk, (6)\nwhere bk ∈ R|Head| is a trainable attention bias vector for all the tokens in the k-th retrieved passage. In the experiments, the maximum retrieved passages is by default set to 100. Thus, the decoder attention bias introduces additional 100 ∗ |Head| parameters for each layer. Adversarial Training Adversarial training creates adversarial examples by adding small perturbations to the embedding layer. Assuming the word(piece) embedding layer is parameterized by a matrix V ∈ R|V |×d, |V | is the vocabulary size, and d\n1we omit the layer notation for simplification\nis the embed-dimension. The adversarial embedding matrix V̂ can be obtained by\ngV = −∇VL(x,y; θ), (7) V̂ = V + SG( gV/||gV||2), (8)\nwhere SG(·) is the stop-gradient operation. We use the adversarial embedding matrix V̂ to replace the original V in model parameters θ, and obtain θ̂. Thus the adversarial loss can be calculated as\nLAT(x,y; θ) = L(x,y; θ̂). (9)\nTherefore, the overall training objective of the generative reader is\nL2 = αL(x,y; θ) + βLAT(x,y; θ), (10)\nwhere α = 0.5, β = 0.5 in all of the exepriments."
    }, {
      "heading" : "2.3 UnitedQA System",
      "text" : "The UnitedQA system combines outputs from both extractive and generative models for a given question during inference. Since the output spaces of extractive and generative models are different, we use a simple linear interpolation based on best predictions from each model2. Denote the predicted strings from M extractive and N generative models as yE1 , ..., y E M and y G 1 , ..., y G N , respectively. The hybrid prediction y∗ is obtained by\nargmax y∈Y τ M∑ m=1 1(y, yEm) + δ N∑ n=1 1(y, yGn ), (11)\nwhere Y is the set of all predicted strings, 1(y, y′) is an indicator function and τ = 0.6, δ = 0.4."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Experiment Setup",
      "text" : "We use two representative QA datasets and adopt the same training/dev/testing splits as in previous\n2We have also tried a few more complex approaches for combining the extractive and generative models. For example, we first train an extractive model, and then append the top-k answer strings from the extractive model at the end of the input for training a generative model. None of them is as good as the simple ensemble approach.\nmatch score is used for evaluation. The overall best model is in Box , the best single model is in bold, and the best model with the smallest reader size is in underline.\nwork (Lee et al., 2019; Karpukhin et al., 2020). Both datasets (see Table 1 for statistics) have been heavily studied in recent work (Lee et al., 2019; Min et al., 2019; Karpukhin et al., 2020; Guu et al., 2020). We follow the standard evaluation protocol and use exact match (EM) as the evaluation metric.\nNaturalQuestions (Kwiatkowski et al., 2019) is composed of questions by real users to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the NaturualQuestions, the questions are considered to be more information seeking given that the question askers didn’t know the answer beforehand. In addition, we use another evaluation set, i.e. the dev set introduced recently by the EfficientQA competition (Min et al., 2021), which is constructed in the same way as the original NaturalQuestions dataset.\nTriviaQA (Joshi et al., 2017) contains trivia question-answer pairs that were scraped from the web. Different from NaturalQuestions, the questions here are written with known answers in mind. Specifically, the unfiltered set has been used for developing open-domain QA models.\nImplementation details For a fair comparison, we use the same retrieval module as Karpukhin et al. (2020) for NaturalQuestions and TriviaQA to mitigate the impact of retrieval difference. Specifically, we use DPR (single) for NaturalQuestions and BM25+DPR (multi) for TriviaQA because of\ntheir best end-to-end performance (Karpukhin et al. 2020). For all the experiments, we use 8 and 16 V100-32GB for base and large model training respectively. We train our models with Adam optimizer of a linear scheduler with a warmup raito of 0.1. The extractive models are trained for up to 8 epochs with a learning rate of 2e−5 and a batch passage size per question of 16. The generative models are trained for up to 10 epochs with a learning rate of 1e−4, a batch size of 64, and 100 retrieved passages per question for model training. We select γ in {4, 8}. After the best configuration is selected based on the dev set, we run our best models 3 times independently with different random seeds and report the median performance on the test set. We also report ensemble results which are based on the linear interpolation over answer predictions from the 3 models."
    }, {
      "heading" : "3.2 Main results",
      "text" : "Single Model Results: We first compare our models to two recent models, REALM (Guu et al., 2020) and RAG (Lewis et al., 2020), which are first pre-trained with different retrieval augmented objectives and then fine-tuned for open-domain QA. In addition, we include as baselines DPR (Karpukhin et al., 2020) and T5-FID (Izacard and Grave, 2021), both of which are based on the same retriever as ours. As shown in Table 2, both our extractive and generative models achieve new stateof-the-art results for both studied datasets. Compared with the recent state-of-the-art extractive\nmodel (DPR), our base model leads to pronounced 15% relative improvements for both NaturalQuestions (+6.2 absolute improvement) and TriviaQA (+8.4 absolute improvement). More importantly, UnitedQA-Ebase achieves comparable or even better performance with regard to generative models of larger size, i.e. RAG and T5-FIDbase. It highlights the importance of proper training strategies for open-domain QA models.\nHybrid Model Results: In order to evaluate the advantage of the hybrid of the extractive and generative models (UnitedQA), we include two homogeneous ensemble baselines, one consisting of only extractive readers (UnitedQA-E++) and the other ensemble of exclusively generative models (UnitedQA-G++). For homogeneous ensemble cases, the three-way majority prediction is used. For the hybrid of extractive and generative readers, we select a three-model combination from the set of three generative and three extractive models based on the dev set. We observed that combining predictions from two generative models and one extractive model results in the best hybrid model for both datasets. As expected, all ensemble models show an improvement over their single model counterparts. However, the two homogeneous ensemble baselines, UnitedQA-E++ and UnitedQA-G++, only provide marginal gains over the corresponding best single models. The significant improvement brought by our proposed hybrid approach indicates the benefit of combining extractive and generative readers for open-domain QA.\nDiscussion: Although the proposed hybrid approach has been shown to be highly effective for open-domain QA, we point out that the improved performance comes with increased computational cost. The best combination requires approximately three times the computational cost of a single generative model. Therefore, it would be interesting to explore more efficient hybrid methods, such as effective parameter sharing strategies or unified formulations. Another interesting future direction is to explore customized compression approaches for reducing the model size of retriever and reader separately or jointly through pruning (Han et al., 2016), quantization (Hubara et al., 2018), and knowledge distillation (Hinton et al., 2015). Specifically, given that the hybrid model is more effective, it is likely that a student model can learn more effectively from a hybrid teacher model via knowledge distillation for open-domain QA."
    }, {
      "heading" : "4 Analysis",
      "text" : "In this section, we first carry out ablation study on the extractive and generative model improvements. Moreover, we aim to take a deeper look and understand the difference between the two models."
    }, {
      "heading" : "4.1 Ablation Study",
      "text" : "In Table 3, we present ablation experiments on the effectiveness of different textual representations and methods for improving the extractive model UnitedQA-Ebase. Here, we focus on base models, i.e. BERTbase and ELECTRAbase. Note that the row UnitedQA-Ebase is the corresponding base model reported in Table 2. Compared with the MML-based multi-objective (Cheng et al., 2020), we find that a new multi-objective with HardEM at the multi-passage level and MML at the passage level is more effective for open-domain QA. In addition to the multi-objective training, there is a noticeable improvement brought by the regularization method (PDR) which indicates the importance of proper regularization for learning with noisy supervision. Last but not least, the large improvement of ELECTRA over BERT indicates the importance of deriving better text representations for weakly supervised NLP problems. For the UnitedQA-G, we present the ablation study on analyzing the effectiveness of decoder attention bias component and adversarial training mechanism in Table 4. Both techniques contribute to decent improvements over T5-FID with more pronounced gains brought by adversarial training."
    }, {
      "heading" : "4.2 Impact of Retrieval Accuracy",
      "text" : "Here, we vary the number of retrieved passages during inference and report the evaluation results in terms of end-to-end QA exact match score of UnitedQA-E and UnitedQA-G along with the corresponding top-k retrieval accuracy. The results are summarized in Table 5. As expected, when the number of retrieved passages increases, both top-k retrieval accuracy and the end-to-end QA performance improve. However, there is a noticeable gap between the improvement of retrieving more passages (i.e., recall) and that of the corresponding end-to-end QA performance, especially for the extractive reader. This is likely caused by additional noise introduced with improved retrieval recall. Specifically, only half of the retriever improvement can be effectively utilized by the extractive model while the generative model can benefit more from retrieving more passages. This suggests that by concatenating all passages in vector space, the generative model are more effective in de-noising in comparison to the extractive model."
    }, {
      "heading" : "4.3 Breakdown Evaluation",
      "text" : "Following Lewis et al. (2021), we carry out a breakdown evaluation of model performance over the NaturalQuestions and TriviaQA test sets. Given\ntheir superior performance, we again only consider our improved extractive and generative models, i.e. UnitedQA-Elarge and UnitedQA-G respectively. The evaluation is summarized in Table 6. In comparison to their corresponding overall performance, both the extractive and generative models achieve much better performance on the “Overlap” categories (i.e. “Question Overlap” and “Answer Overlap”) for both NaturalQuestions and TrivaQA, which indicates that both models perform well for question and answer memorization. Different from question and answer memorization, there is a pronounced performance drop for both models on the“Answer Overlap Only” category where certain amount of relevance inference capability is required to succeed. Lastly, we see that both extractive and generative models suffer some significant performance degradation for the “No Overlap” column which highlights model’s generalization evaluation. Nevertheless, the extractive model demonstrate a better QA generalization by achieving a better overall performance on the “No Overlap” category for both datasets."
    }, {
      "heading" : "4.4 Error Analysis",
      "text" : "Here, we conduct analyses into prediction errors made by the extractive and generative models based on automatic evaluation. For this study, we use the EfficientQA dev set (Min et al., 2021) which is constructed in the same way as the original NaturalQuestions dataset. Specifically, we group prediction errors into three categorizes: 1) common prediction errors made by both the extractive and generative models, 2) prediction errors made by the extractive model, 3) prediction errors produced by the generative model. In the following, we first carry out a manual inspection into the common errors. Then, we compare the prediction errors made by extractive and generative models, respectively.\nFirst of all, there is an error rate of 29% of those consensus predictions made by both extractive and generative models according to the automatic evaluation. Based on 30 randomly selected examples, we find that around 30% of those predictions are actually valid answers as shown in the top part of Table 7. In addition to predictions that are answers at different granularity or semantically equivalent ones, some of those prediction errors are likely caused by the ambiguity in questions. As the given example in Table 7, based on the specificity, the model prediction is also a valid answer. This high-\nlights the limitation of the current evaluation metric, which does not accurately estimate the existing open-domain QA system capabilities. As shown in the bottom part of Table 7, most of representative errors are due to the confusion of related concepts, entities or events that are mentioned frequently together with the corresponding gold answers.\nNext, all questions from the dev set are categorized based the WH question word, i.e. what, which, when, who, how, where. We then report the relative performance change of each WH category for both extractive and generative models over their corresponding overall prediction accuracy in Figure 2. First, it is easy to see that both extractive and generative models achieve the best performance for entity related who questions, which is likely to be the result of high ratio of samples of this type seen during training. In contrast, the answers to what questions can play a much richer syntactic role in context, making it more difficult for both extractive\nand generative models to perform well. Interestingly, the generative model exhibits the strength for temporal reasoning, whereas the extractive model does not. This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future."
    }, {
      "heading" : "5 Related Work",
      "text" : "Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017). Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021). Generally, the dense representations complement the sparse vector methods for passage retrieval as they can potentially give\nhigh similarity to semantically related text pairs, even without exact lexical overlap. Unlike most work focusing on a pipeline model, Lee et al. (2019) propose a pre-training objective for jointly training both the retrieval encoder and reader. It is further extended by Guu et al. (2020) with a dynamic update of the passage index during the training. Instead, in this work, we focus on a hybrid reader approach for open-domain QA. By simply combing answer predictions from extractive and generative models, our UnitedQA achieves significant improvements over state-of-the-art models. Reading Comprehension with Noisy Labels There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context. Clark and Gardner (2018) propose a paragraphpair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again for documentlevel QA with distant supervision. In our work, we further extend the multi-objective formulation proposed in (Cheng et al., 2020) with the hard EM learning (Min et al., 2019) for enhancing extrac-\ntive open-domain QA, where the input passages are given by a retrieval model and are typically from different documents."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we propose a hybrid model for opendomain QA, called UnitedQA, which combines the strengths of extractive and generative readers. We demonstrate the effectiveness of UnitedQA on two popular open-domain QA benchmarks, NaturalQuestions and TriviaQA. Our results show that the proposed UnitedQA model significantly outperforms single extractive and generative models as well as their corresponding homogeneous ensembles, and sets new state-of-the-art on both benchmarks. We also perform a comprehensive empirical study to investigate the relative contributions of different components of our model and the techniques we use to improve the readers.\nFor future work, it would be interesting to explore model compression approaches for reducing the model size of retriever and reader separately or jointly through pruning, quantization, and knowledge distillation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for valuable suggestions, Yuning Mao for valuable discussions and comments, and Microsoft Research Technology Engineering team for computing support."
    } ],
    "references" : [ {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Open-domain question answering",
      "author" : [ "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 34–37, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Yih.,? 2020",
      "shortCiteRegEx" : "Chen and Yih.",
      "year" : 2020
    }, {
      "title" : "Probabilistic assumptions matter: Improved models for distantlysupervised document-level question answering",
      "author" : [ "Hao Cheng", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Posterior differential regularization with f-divergence for improving model robustness",
      "author" : [ "Hao Cheng", "Xiaodong Liu", "Lis Pereira", "Yaoliang Yu", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Cheng et al\\.,? 2021",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2021
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 845–855. Association",
      "citeRegEx" : "Clark and Gardner.,? 2018",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2018
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval augmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Mingwei Chang." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Ma-",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J. Dally." ],
      "venue" : "4th International Conference on Learning Representations, ICLR 2016, San Juan,",
      "citeRegEx" : "Han et al\\.,? 2016",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Quantized neural networks: Training neural networks with low precision weights and activations",
      "author" : [ "Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio." ],
      "venue" : "Journal of Machine Learning Research, 18(187):1–30.",
      "citeRegEx" : "Hubara et al\\.,? 2018",
      "shortCiteRegEx" : "Hubara et al\\.",
      "year" : 2018
    }, {
      "title" : "Leveraging passage retrieval with generative models for open domain question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Izacard and Grave.,? 2021",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2021
    }, {
      "title" : "SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "Proceedings of the 58th",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:453–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096. Association",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval-augmented generation for knowledge",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Question and answer test-train overlap in open-domain question answering datasets",
      "author" : [ "Patrick Lewis", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "Denoising distantly supervised open-domain question answering",
      "author" : [ "Yankai Lin", "Haozhe Ji", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1736–",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "EfficientQA competition: Systems, analyses and lessons learned",
      "author" : [ "Karpukhin", "Stan Peshterliev", "Dmytro Okhonko", "Michael Schlichtkrull", "Sonal Gupta", "Yashar Mehdad", "Wen tau Yih" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Karpukhin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2021
    }, {
      "title" : "A discrete hard EM approach for weakly supervised question answering",
      "author" : [ "Sewon Min", "Danqi Chen", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Targeted adversarial training for natural language understanding",
      "author" : [ "Lis Pereira", "Xiaodong Liu", "Hao Cheng", "Hoifung Poon", "Jianfeng Gao", "Ichiro Kobayashi." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Pereira et al\\.,? 2021",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2021
    }, {
      "title" : "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
      "author" : [ "Yingqi Qu", "Yuchen Ding", "Jing Liu", "Kai Liu", "Ruiyang Ren", "Wayne Xin Zhao", "Daxiang Dong", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Qu et al\\.,? 2021",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "The TREC-8 question answering track report",
      "author" : [ "Ellen Voorhees" ],
      "venue" : null,
      "citeRegEx" : "Voorhees.,? \\Q2000\\E",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference.",
      "startOffset" : 72,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference.",
      "startOffset" : 72,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference.",
      "startOffset" : 72,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "∗Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively.",
      "startOffset" : 50,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "∗Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively.",
      "startOffset" : 50,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "∗Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively.",
      "startOffset" : 50,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "∗Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively.",
      "startOffset" : 50,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "In UnitedQA, the extractive reader (UnitedQAE) and generative reader (UnitedQA-G) are built upon the pretrained language models, ELECTRA (Clark et al., 2020) and T5 (Raffel et al.",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "noisy supervision issue caused by the heuristicsbased labeling and incorporate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve the model robustness.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al., 2019; Jiang et al., 2020; Pereira et al., 2021) to improve the model generalization ability.",
      "startOffset" : 298,
      "endOffset" : 357
    }, {
      "referenceID" : 22,
      "context" : "The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al., 2019; Jiang et al., 2020; Pereira et al., 2021) to improve the model generalization ability.",
      "startOffset" : 298,
      "endOffset" : 357
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and TriviaQA (Joshi et al., 2017), respectively.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "For DPR, passages and questions are represented as dense vectors based on two BERT (Devlin et al., 2019) models.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Following state-of-the-art extractive QA models (Devlin et al., 2019; Karpukhin et al., 2020), our extractive reader is based on a Transformer neural network pre-trained with a cloze style selfsupervised objective, i.",
      "startOffset" : 48,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Following state-of-the-art extractive QA models (Devlin et al., 2019; Karpukhin et al., 2020), our extractive reader is based on a Transformer neural network pre-trained with a cloze style selfsupervised objective, i.",
      "startOffset" : 48,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Since there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al.",
      "startOffset" : 177,
      "endOffset" : 201
    }, {
      "referenceID" : 21,
      "context" : ", 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Posterior Differential Regularization Due to the noisy supervision for open-domain QA (Chen et al., 2017), we investigate the posterior differential regularization (PDR) (Cheng et al.",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : ", 2017), we investigate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "The open-domain version of NaturalQuestions (Lee et al., 2019) only con-",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "TriviaQA (Joshi et al., 2017) contains trivia question-answer pairs that were scraped from the web.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Specifically, we use DPR (single) for NaturalQuestions and BM25+DPR (multi) for TriviaQA because of their best end-to-end performance (Karpukhin et al. 2020).",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "Single Model Results: We first compare our models to two recent models, REALM (Guu et al., 2020) and RAG (Lewis et al.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and RAG (Lewis et al., 2020), which are first pre-trained with different retrieval augmented objectives and then fine-tuned for open-domain QA.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "In addition, we include as baselines DPR (Karpukhin et al., 2020) and T5-FID (Izacard and Grave, 2021), both of which are based on the same retriever as ours.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : ", 2020) and T5-FID (Izacard and Grave, 2021), both of which are based on the same retriever as ours.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "Another interesting future direction is to explore customized compression approaches for reducing the model size of retriever and reader separately or jointly through pruning (Han et al., 2016), quantization (Hubara et al.",
      "startOffset" : 175,
      "endOffset" : 193
    }, {
      "referenceID" : 10,
      "context" : ", 2016), quantization (Hubara et al., 2018), and knowledge distillation (Hinton et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : ", 2018), and knowledge distillation (Hinton et al., 2015).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Compared with the MML-based multi-objective (Cheng et al., 2020), we find that a new multi-objective with HardEM at the multi-passage level and MML at the passage level is more effective for open-domain QA.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "Table 6: Breakdown evaluation on NaturalQuestions (NQ) and TriviaQA based on test splits defined in (Lewis et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al.",
      "startOffset" : 121,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al.",
      "startOffset" : 121,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : ", 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : ", 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : ", 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : ", 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "In (Lin et al., 2018), a coarse-to-fine model is proposed to han-",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again for documentlevel QA with distant supervision.",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "In our work, we further extend the multi-objective formulation proposed in (Cheng et al., 2020) with the hard EM learning (Min et al.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : ", 2020) with the hard EM learning (Min et al., 2019) for enhancing extractive open-domain QA, where the input passages are",
      "startOffset" : 34,
      "endOffset" : 52
    } ],
    "year" : 2021,
    "abstractText" : "To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models. We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvements over previous state-of-the-art models. We demonstrate that an hybrid approach by combining answers from both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.",
    "creator" : "LaTeX with hyperref"
  }
}