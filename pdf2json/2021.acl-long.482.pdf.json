{
  "name" : "2021.acl-long.482.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition",
    "authors" : [ "Yinghao Li", "Pranav Shetty", "Lucas Liu", "Chao Zhang", "Le Song", "Mohamed bin Zayed" ],
    "emails" : [ "chaozhang}@gatech.edu", "le.song@mbzuai.ac.ae" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6178–6190\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6178"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER), which aims to identify named entities from unstructured text, is an information extraction task fundamental to many downstream applications such as event detection (Li et al., 2012), relationship extraction (Bach and Badaskar, 2007), and question answering (Khalid et al., 2008). Existing NER models are typically supervised by a large number of training sequences, each pre-annotated with token-level labels. In practice, however, obtaining such labels could be prohibitively expensive. On the other hand, many domains have various knowledge resources such as\nknowledge bases, domain-specific dictionaries, or labeling rules provided by domain experts (Farmakiotou et al., 2000; Nadeau and Sekine, 2007). These resources can be used to match a corpus and quickly create large-scale noisy training data for NER from multiple views.\nLearning an NER model from multiple weak supervision sources is a challenging problem. While there are works on distantly supervised NER that use only knowledge bases as weak supervision (Mintz et al., 2009; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020), they cannot leverage complementary information from multiple annotation sources. To handle multi-source weak supervision, several recent works (Nguyen et al., 2017; Safranchik et al., 2020; Lison et al., 2020) leverage the hidden Markov model (HMM), by modeling true labels as hidden variables and inferring them from the observed noisy labels through unsupervised learning. Though principled, these models fall short in capturing token semantics and context information, as they either model input tokens as one-hot observations (Nguyen et al., 2017) or do not model them at all (Safranchik et al., 2020; Lison et al., 2020). Moreover, the flexibility of HMM is limited as its transitions and emissions remain constant over time steps, whereas in practice they should depend on the input words.\nWe propose the conditional hidden Markov model (CHMM) to infer true NER labels from multi-source weak annotations. CHMM conditions the HMM training and inference on BERT by predicting token-wise transition and emission probabilities from the BERT embeddings. These tokenwise probabilities are more flexible than HMM’s constant counterpart in modeling how the true labels should evolve according to the input tokens. The context representation ability they inherit from BERT also relieves the Markov constraint and expands HMM’s context-awareness.\nFurther, we integrate CHMM with a supervised BERT-based NER mode with an alternate-training method (CHMM-ALT). It fine-tunes BERT-NER with the denoised labels generated by CHMM. Taking advantage of the pre-trained knowledge contained in BERT, this process aims to refine the denoised labels by discovering the entity patterns neglected by all of the weak sources. The finetuned BERT-NER serves as an additional supervision source, whose output is combined with other weak labels for the next round of CHMM training. CHMM-ALT trains CHMM and BERT-NER alternately until the result is optimized.\nOur contributions include:\n• A multi-source label aggregator CHMM with token-wise transition and emission probabilities for aggregating multiple sets of NER labels from different weak labeling sources.\n• An alternate-training method CHMM-ALT that trains CHMM and BERT-NER in turn utilizing each other’s outputs for multiple loops to optimize the multi-source weakly supervised NER performance.\n• A comprehensive evaluation on four NER benchmarks from different domains demonstrates that CHMM-ALT achieves a 4.83 average F1 score improvement over the strongest baseline models.\nThe code and data used in this work are available at github.com/Yinghao-Li/CHMM-ALT."
    }, {
      "heading" : "2 Related Work",
      "text" : "Weakly Supervised NER There have been works that train NER models with different weak supervision approaches. Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020). But such a method is limited to one source and falls short of acquiring supplementary annotations from other available resources. Other works adopt multiple additional labeling sources, such as heuristic functions that depend on lexical features, word patterns, or document information (Nadeau and Sekine, 2007; Ratner et al., 2016), and unify their results through multi-source label denoising. Several multi-source weakly supervised learning approaches are designed for sentence classification (Ratner et al.,\n2017, 2019; Ren et al., 2020; Yu et al., 2020). Although these methods can be adapted for sequence labeling tasks such as NER, they tend to overlook the internal dependency relationship between token-level labels during the inference. Fries et al. (2017) target the NER task, but their method first generates candidate named entity spans and then classifies each span independently. This independence makes it suffer from the same drawback as sentence classification models.\nA few works consider label dependency while dealing with multiple supervision sources. Lan et al. (2020) train a BiLSTM-CRF network (Huang et al., 2015) with multiple parallel CRF layers, each for an individual labeling source, and aggregate their transitions with confidence scores predicted by an attention network (Bahdanau et al., 2015; Luong et al., 2015). HMM is a more principled model for multi-source sequential label denoising as the true labels are implicitly inferred through unsupervised learning without deliberately assigning any additional scores. Following this track, Nguyen et al. (2017) and Lison et al. (2020) use a standard HMM with multiple observed variables, each from one labeling source. Safranchik et al. (2020) propose linked HMM, which differs from ordinary HMM by introducing unique linking rules as an adjunct supervision source additional to general token labels. However, these methods fail to utilize the context information embedded in the tokens as effectively as CHMM, and their NER performance is further constrained by the Markov assumption.\nNeuralizing the Hidden Markov Model Some works attempt to neuralize HMM in order to relax the Markov assumption while maintaining its generative property (Kim et al., 2018). For example, Dai et al. (2017) and Liu et al. (2018) incorporate recurrent units into the hidden semiMarkov model (HSMM) to segment and label highdimensional time series; Wiseman et al. (2018) learn discrete template structures for conditional text generation using neuralized HSMM. Wessels and Omlin (2000) and Chiu and Rush (2020) factorize HMM with neural networks to scale it and improve its sequence modeling capacity. The work most related to ours leverages neural HMM for sequence labeling (Tran et al., 2016). CHMM differs from neural HMM in that the tokens are treated as a dependency term in CHMM instead of the observation in neural HMM. Besides, CHMM is trained with generalized EM, whereas neural HMM opti-\nmizes the marginal likelihood of the observations."
    }, {
      "heading" : "3 Problem Setup",
      "text" : "In this section, we formulate the multi-source weakly supervised NER problem. Consider an input sentence that contains T tokens w(1:T ), NER can be formulated as a sequence labeling task that assigns a label to each token in the sentence.1 Assuming the set of target entity types is E and the tagging scheme is BIO (Ramshaw and Marcus, 1995), NER models assign one label from the label set l ∈ L to each token, where the size of the label set is |L| = 2|E| + 1, e.g., if E = {PER,LOC}, then L = {O,B-PER,I-PER,B-LOC,I-LOC}.\nSuppose we have a sequence with K weak sources, each of which can be a heuristic rule, knowledge base, or existing out-of-domain NER model. Each source serves as a labeling function that generates token-level weak labels from the input corpus, as shown in Figure 1. For the input sequence w(1:T ), we use x(1:T )k , k ∈ {1, . . . ,K} to represent the weak labels from the source k, where x (t) k ∈ R\n|L|, t ∈ {1, . . . , T} is a probability distribution over L. Multi-source weakly supervised NER aims to find the underlying true sequence of labels ŷ(1:T ), ŷ(t) ∈ L given {w(1:T ),x(1:T )1:K }."
    }, {
      "heading" : "4 Methodology",
      "text" : "In this section, we describe our proposed method CHMM-ALT. We first sketch the alternate-training procedure (§ 4.1), then explain the CHMM component (§ 4.2) and how BERT-NER is involved (§ 4.3)."
    }, {
      "heading" : "4.1 Alternate-Training Procedure",
      "text" : "The alternate-training method trains two models— a multi-source label aggregator CHMM and a BERT-NER model—in turn with each other’s output. CHMM aggregates multiple sets of labels from different sources into a unified sequence of\n1We represent vectors, matrices or tensors with bold fonts and scalars with regular fonts; 1 : a , {1, 2, . . . , a}.\nlabels, while BERT-NER refines them by its language modeling ability gained from pre-training. The training process is divided into two phases.\n• In phase I, CHMM takes the annotations x (1:T ) 1:K from existing sources and gives a set\nof denoised labels y∗(1:T ), which are used to fine-tune the BERT-NER model. Then, we regard the fine-tuned model as an additional labeling source, whose outputs ỹ(1:T ) are added into the original weak label sets to give the updated observation instances: x (1:T ) 1:K+1 = {x (1:T ) 1:K , ỹ (1:T )}.\n• In phase II, CHMM and BERT-NER mutually improve each other iteratively in several loops. Each loop first trains CHMM with the observation x(1:T )1:K+1 from the previous one. Then, its predictions are adopted to fine-tune BERT-NER, whose output updates x(1:T )K+1 .\nFigure 2 illustrates the alternate-training method. In general, CHMM gives high precision predictions, whereas BERT-NER trades recall with precision. In other words, CHMM can classify named entities with high accuracy but is slightly disadvantaged in discovering all entities. BERT-NER increases the coverage with a certain loss of accuracy. Combined with the alternate-training approach, this complementarity between these models further increases the overall performance."
    }, {
      "heading" : "4.2 Conditional Hidden Markov Model",
      "text" : "The conditional hidden Markov model is an HMM variant for multi-source label denoising. It models true entity labels as hidden variables and infers them from the observed noisy labels. Traditionally, discrete HMM uses one transition matrix to model the probability of hidden label transitioning and one emission matrix to model the probability of the observations from the hidden labels. These two matrices are constant, i.e., their values do not change over time steps. CHMM, on the contrary, conditions both its transition and emission matrices on the BERT embeddings e(1:T ) of the input tokensw(1:T ). This design not only allows CHMM to leverage the rich contextual representations of the BERT embeddings but relieves the constant matrices constraint as well.\nIn phase I, CHMM takes K sets of weak labels from the provided K weak labeling sources. In phase II, in addition to the existing sources, it takes\nanother set of labels from the previously fine-tuned BERT-NER, making the total number of sources K + 1. For convenience, we use K as the number of weak sources below.\nModel Architecture Figure 3 shows a sketch of CHMM’s architecture.2 z(1:T ) denotes the discrete hidden states of CHMM with z(t) ∈ L, representing the underlying true labels to be inferred from multiple weak annotations. Ψ(t) ∈ R|L|×|L| is the transition matrix, whose element Ψ(t)i,j = p(z(t) = j|z(t−1) = i, e(t)), i, j ∈ {1, . . . , |L|} denotes the probability of moving from label i to label j at time step t. Φ(t)k ∈ R\n|L|×|L| is the emission matrix of weak source k, each element in which Φ (t) i,j,k = p(x (t) j,k = 1|z\n(t) = i, e(t)) represents the probability of source k observing label j when the\n2We relax plate notation here to present details.\nhidden label is i at time step t. For each step, e(t) ∈ Rdemb is the output of a pre-trained BERT with demb being its embedding dimension. Ψ(t) and Φ(t)1:K are calculated by applying a multi-layer perceptron (MLP) to e(t):\ns(t) ∈ R|L|2 = MLP(e(t)), (1) h(t) ∈ R|L|·|L|·K = MLP(e(t)). (2)\nSince the MLP outputs are vectors, we need to reshape them to matrices or tensors:\nS(t) ∈ R|L|×|L| = reshape(s(t)), (3) H(t) ∈ R|L|×|L|×K = reshape(h(t)). (4)\nTo achieve the proper probability distributions, we apply the Softmax function along the label axis so that these values are positive and sum up to 1:\nΨ (t) i,1:|L| = σ(S (t) i,1:|L|), Φ (t) i,1:|L|,k = σ(H (t) i,1:|L|,k),\nwhere\nσ(a)i = exp (ai)∑ j exp (aj) . (5)\na is an arbitrary vector. The formulae in the following discussion always depend on e(1:T ), but we will omit the dependency term for simplicity.\nModel Training According to the generative process of CHMM, the joint distribution of the hidden states and the observed weak labels for one sequence p(z(0:T ),x(1:T )|θ) can be factorized as:\np(z(0:T ),x(1:T )|θ) = p(z(0))p(x(1:T )|z(1:T ))\n= p(z(0)) T∏ t=1 p(z(t)|z(t−1)) T∏ t=1 p(x(t)|z(t)),\n(6) where θ represents all the trainable parameters.\nHMM is generally trained with an expectationmaximization (EM, also known as Baum-Welch) algorithm. In the expectation step (E-step), we compute the expected complete data log likelihood:\nQ(θ,θold) , Ez[`c(θ)|θold]. (7)\nθold is the parameters from the previous training step, Ez[·] is the expectation over variable z, and\n`c(θ) , log p(z (0:T ),x(1:T )|θ)\nis the comptelete data log likelihood. Let ϕ(t) ∈ R|L| be the observation likelihood where\nϕ (t) i , p(x (t)|z(t) = i) = K∏ k=1 |L|∑ j=1 Φ (t) i,j,kx (t) j,k. (8)\nCombining (6)–(8) together, we have\nQ(θ,θold) = |L|∑ i=1 γ (0) i log πi+ T∑ t=1 |L|∑ i=1 |L|∑ j=1 ξ (t) i,j log Ψ (t) i,j + T∑ t=1 |L|∑ i=1 γ (t) i logϕ (t) i ,\n(9) where π1 = 1,π2:|L| = 0;3 γ (t) i , p(z (t) = i|x(1:T )) is the smoothed marginal; ξ(t)i,j , p(z(t−1) = i, z(t) = j|x(1:T )) is the expected number of transitions. These parameters are computed using the forward-backward algorithm.4\nIn the maximization step (M-step), traditional HMM updates parameters θHMM = {Ψ,Φ,π} by optimizing (7) with pseudo-statistics.5 However, as the transitions and emissions in CHMM are not standalone parameters, we cannot directly optimize CHMM by this method. Instead, we update the model parameters through gradient descent w.r.t. θCHMM using (9) as the objective function:\n∇θCHMM = ∂Q(θCHMM,θ\nold CHMM)\n∂θCHMM . (10)\nIn practice, the calculation is conducted in the logarithm domain to avoid the loss of precision issue that occurs when the floating-point numbers become too small.\nTo solve the label sparsity issue, i.e., some entities are only observed by a minority of the weak\n3This assumes the initial hidden state is always O. In practice, we set π` = ,∀` ∈ 2 : |L| and π1 = 1 − (|L| − 1) , where is a small value, to avoid getting −∞ from log.\n4Details are presented in appendix A.1. 5Details are presented in appendix A.2.\nsources, we modify the observations x(1:T ) before training. If one source k observes an entity at time step t: x(t)j 6=1,k > 0, the observation of nonobserving sources at t will be modified to x(t)1,κ =\n;x (t) j 6=1,κ = (1 − )/|L|,∀κ ∈ {1, . . . ,K}\\k, where is an arbitrary small value. Note that x(t)1,κ corresponds to the observed label O.\nCHMM Initialization Generally, HMM has its transition and emission probabilities initialized with the statistics Ψ∗ and Φ∗ computed from the observation set. But it is impossible to directly set Ψ(t) and Φ(t) in CHMM to these values, as these matrices are the output of the MLPs rather than standalone parameters. To address this issue, we choose to pre-train the MLPs before starting CHMM’s training by minimizing the mean squared error (MSE) loss between their outputs and the target statistics:\n`MSE = 1\nT ∑ t ‖Ψ∗ − S(t)‖2F + ‖Φ∗ −H(t)‖2F ,\nwhere ‖ · ‖F is the Frobenius norm. Right after initialization, MLPs can only output similar probabilities for all time steps: Ψ(t) ≈ Ψ∗, Φ(t) ≈ Φ∗, ∀t ∈ {1, 2, . . . , T}. But their token-wise prediction divergence will emerge when CHMM has been trained. The initial hidden state z(0) is fixed to O as it has no corresponding token.\nInference Once trained, CHMM can provide the most probable sequence of hidden labels ẑ(1:T ) along with the probabilities of all labels y∗(1:T ).\nẑ(1:T ) = arg max z(1:T ) pθ̂CHMM(z (1:T )|x(1:T )1:K , e (1:T )),\ny∗ (t) i = pθ̂CHMM(z (t) = i|x(1:T )1:K , e (1:T )),\nwhere θ̂CHMM represents the trained parameters. These results can be calculated by either the Viterbi decoding algorithm (Viterbi, 1967) or directly maximizing the smoothed marginal γ(1:T )."
    }, {
      "heading" : "4.3 Improving Denoised Labels with BERT",
      "text" : "The pre-trained BERT model encodes semantic and structural knowledge, which can be distilled to further refine the denoised labels from CHMM. Specifically, we construct the BERT-NER model by stacking a feed-forward layer and a Softmax layer on top of the original BERT to predict the probabilities of the classes that each token belongs\nto (Sun et al., 2019). The probability predictions of CHMM, y∗(1:T ), often referred to as soft labels, are chosen to supervise the fine-tuning procedure. Compared with the hard labels ẑ(1:T ), soft labels lead to a more stable training process and higher model robustness (Thiel, 2008; Liang et al., 2020).\nWe train BERT-NER by minimizing the Kullback-Leibler divergence (KL divergence) between the soft labels y∗ and the model output y:\nθ̂BERT = arg min θBERT\nD[y∗(1:T )‖y(1:T )]\n= arg min θBERT T∑ t=1 |L|∑ i=1 y∗ (t) i log y∗ (t) i y (t) i , (11)\nwhere θBERT denotes all the trainable parameters in the BERT model. BERT-NER does not update the embeddings e(1:T ) that CHMM depends on.\nWe obtain the refined labels ỹ(1:T ) ∈ RT×|L| from the fine-tuned BERT-NER directly through a forward pass. Different from CHMM, we continue BERT-NER’s training with parameter weights from the last loop’s checkpoint so that the model is initialized closer to the optimum. Correspondingly, phase II trains BERT-NER with a smaller learning rate, fewer epoch iterations, and batch gradient descent instead of the mini-batch version.6 This strategy speeds up phase II training without sacrificing the model performance as y∗(1:T ) does not change significantly from loop to loop."
    }, {
      "heading" : "5 Experiments",
      "text" : "We benchmark CHMM-ALT on four datasets against state-of-the-art weakly supervised NER baselines, including both distant learning models and multi-source label aggregation models. We also conduct a series of ablation studies to evaluate the different components in CHMM-ALT’s design."
    }, {
      "heading" : "5.1 Setup",
      "text" : "Datasets We consider four NER datasets covering the general, technological and biomedical domains: 1) CoNLL 2003 (English subset) (Tjong Kim Sang and De Meulder, 2003) is a general domain dataset containing 22,137 sentences manually labelled with 4 entity types. 2) LaptopReview dataset (Pontiki et al., 2014) consists of 3,845 sentences with laptop-related entity mentions. 3) NCBI-Disease dataset (Dogan et al., 2014) contains 793 PubMed abstracts annotated with disease\n6Hyper-parameter values are listed in appendix C.\nmentions. 4) BC5CDR (Li et al., 2016), the dataset accompanies the BioCreative V CDR challenge, consists of 1,500 PubMed articles, annotated with chemical disease mentions.\nTable 1 shows dataset statistics, including the average number of tokens, entities and weak labeling sources. We use the original word tokens in the dataset if provided and use NLTK (Bird and Loper, 2004) otherwise for sentence tokenization.\nFor weak labeling sources, we use the ones from Lison et al. (2020) for CoNLL 2003, and the ones from Safranchik et al. (2020) for LaptopReview, NCBI-Disease and BC5CDR.7\nBaselines We compare our model to the following state-of-the-art baselines: 1) Majority Voting returns the label for a token that has been observed by most of the sources and randomly chooses one if it’s a tie; 2) Snorkel (Ratner et al., 2017) treats each token in a sequence as i.i.d. and conducts the label classification without considering its context; 3) SwellShark (Fries et al., 2017) improves Snorkel by predicting all the target entity spans before classifying them using naı̈ve Bayes; 4) AutoNER (Shang et al., 2018) augments distant supervision by predicting whether two consecutive tokens should be in the same entity span; 5) BOND (Liang et al., 2020) adopts self-training and highconfidence selection to further boost the distant supervision performance. 6) HMM is the multiobservation generative model used in Lison et al. (2020) that does not have the integrated neural network; 7) Linked HMM (Safranchik et al., 2020) uses linking rules to provide additional inter-token structural information to the HMM model.\nFor the ablation study, we modify CHMM to another type of i.i.d. model by taking away its transition matrices. This model, named CHMM-i.i.d.,\n7Details are presented in appendix B.\ndirectly predicts the hidden steps from the BERT embeddings, while otherwise identical to CHMM. We also investigate how CHMM-ALT performs with other aggregators other than CHMM.\nWe also introduce two upper bounds from different aspects: 1) a fully supervised BERT-NER model trained with manually labeled data is regarded as a supervised reference; 2) the best possible consensus of the weak sources. The latter assumes an oracle that always selects the correct annotations from these weak supervision sources. According to the definition, its precision is always 100% and its recall is non-decreasing with the increase of the number of weak sources.\nEvaluation Metrics We evaluate the performance of NER models using entity-level precision, recall, and F1 scores. All scores are presented as percentages. The results come from the average of 5 trials with different random seeds.\nImplementation Details We use BERT pretrained on different domains for different datasets, both for embedding construction and as the component of the supervised BERT-NER model. The original BERT (Devlin et al., 2019) is used for CoNLL 2003 and LaptopReview datasets, bioBERT (Lee et al., 2019) for NCBI-Disease and SciBERT (Beltagy et al., 2019) for BC5CDR. Instances with lengths exceeding BERT’s maximum length limitation (512) are broken into several shorter segments.\nThe only tunable hyper-parameter in CHMM is the learning rate. But its influence is negligible—\nbenefitted from the stability of the generalized EM, the model is guaranteed to converge to a local optimum if the learning rate is small enough. For all the BERT-NER models used in our experiments, the hyper-parameters except the batch size are fixed to the default values (appendix C).\nTo prevent overfitting, we use a two-scale early stopping strategy for model choosing at two scales based on the development set. The micro-scale early stopping chooses the best model parameters for each individual training process of both CHMM and BERT-NER; the macro-scale early stopping selects the best-performing model in phase II iterations, which reports the test results. In our experiments, phase II exits if the macro-scale development score has not increased in 5 loops or the maximum number of loops (10) is reached."
    }, {
      "heading" : "5.2 Main Results",
      "text" : "Table 2 presents the model performance from different domains. We find that our alternate-training framework outperforms all weakly supervised baseline models. In addition, CHMM-ALT approaches or even exceeds the best source consensus, which sufficiently proves the effectiveness of the design. For general HMM-based label aggregators such as CHMM, it is impossible to exceed the best consensus since they can only predict an entity observed by at least one source. Based on this fact, CHMM is designed to select the most accurate observations from the weak sources without shrinking their coverage. In comparison, BERT’s language\nrepresentation ability enables it to generalize the entity patterns and successfully discovers those entities annotated by none of the sources. Comparing CHMM + BERT to CHMM, we can conclude that BERT basically exchanges recall with precision, and its high-recall predictions can improve the result of CHMM in return. The complementary nature of these two models is why CHMM-ALT improves the overall performance of weakly supervised NER."
    }, {
      "heading" : "5.3 Analysis of CHMM",
      "text" : "Looking at Table 2, we notice that CHMM performs the best amongst all generative models including majority voting, HMM and CHMM-i.i.d. The performance of conventional HMM is largely limited by the Markov assumption with the unchanging transition and emission probabilities. The results in the table validate that conditioning the model on BERT embedding alleviates this limitation. However, the transition matrices in HMM are indispensable, implied by CHMM-i.i.d.’s results, as they provide supplemental information about how the underlying true labels should evolve."
    }, {
      "heading" : "5.4 Analysis of Alternate-Training",
      "text" : "Performance Evolution Figure 4 reveals the details of the alternate-training process. For less ambiguous tasks including NCBI-Disease, BC5CDR and LaptopReview with fewer entity types, BERT generally has better performance in phase I but gets surpassed in phase II. Interestingly, BERT’s performance never exceeds that of CHMM on the LaptopReview dataset. This may be because BERT fails to construct sufficiently representative patterns from the denoised labels for this dataset. For CoNLL 2003, where it is harder for the labeling sources to model the language structures, the strength of a pre-trained language model in pattern recognition becomes more prominent. From the re-\nsults it seems that the performance increment of the denoised labels y∗(1:T ) provides marginally extra information to BERT after phase II, as most of the increment comes from the information provided by BERT itself. Even so, keeping phase II is reasonable when we want to get the best out of the weak labeling sources and the pre-trained BERT.\nBERT-NER Initialization CHMM-ALT initializes BERT-NER’s parameters from its previous checkpoint at the beginning of each loop in phase II to reduce training time (§ 4.3). If we instead fine-tune BERT-NER from the initial parameters of the pre-trained BERT model for each loop, CHMM-ALT gets 84.30, 84.71, and 76.68 F1 scores on NCBI-Disease, BC5CDR, and LaptopReview datasets. These scores are close to the results in Table 2, but the training takes much longer. Consequently, our BERT-NER initialization strategy is a more practical choice overall.\nApplying Alternate-Training to Other Methods Table 3 shows the alternate-training performance acquired with different label aggregators. The ac-\ncompanying BERT-NER models are identical to those described in § 5.1. The results in the table suggest that the performance improvement obtained by using alternate-training on the label aggregators is stable and generalizable to any other models yet to be proposed."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we present CHMM-ALT, a multisource weakly supervised approach that does not depend on manually labeled data to learn an accurate NER tagger. It integrates a label aggregator— CHMM and a supervised model—BERT-NER together into an alternate-training procedure. CHMM conditions HMM on BERT embeddings to achieve greater flexibility and stronger context-awareness. Fine-tuned with CHMM’s prediction, BERT-NER discovers patterns unobserved by the weak sources and complements CHMM. Training these models in turn, CHMM-ALT uses the knowledge encoded in both the weak sources and the pre-trained BERT model to improve the final NER performance. In the future, we will consider imposing more constraints on the transition and emission probabilities, or manipulating them according to sophisticated domain knowledge. This technique could be also extended to other sequence labeling tasks such as semantic role labeling or event extraction."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by ONR MURI N0001417-1-2656, NSF III-2008334, Kolon Industries, and research gifts from Google and Amazon. In addition, we would like to thank Yue Yu for his insightful suggestions for this work."
    }, {
      "heading" : "A Technical Details",
      "text" : "A.1 CHMM Training\nFollowing the discussion in § 4.2, we use the forward-backward algorithm to calculate the smoothed marginal γ(t)i , p(z\n(t) = i|x(1:T )), i ∈ {1, 2, . . . , |L|}, t ∈ {1, 2, . . . , T} and the expected number of transitions ξ(t)i,j , p(z (t−1) = i, z(t) = j|x(1:T )), i, j ∈ {1, 2, . . . , |L|}.8 |L| is the number of BIO formatted entity labels, which are regarded as hidden states; T is the total number of hidden steps in a sequence, which equals the number of tokens.\nDefining α(t)i , p(z (t) = i|x(1:t)) and β(t)i , p(x(t+1:T )|z(t) = i), γ(t)i and ξ (t) i,j can be represented by α and β using the Bayes’ rule and Markov assumption:\nγ (t) i , p(z (t) = i|x(1:T ))\n= p(x(t+1:T ), z(t) = i|x(1:t)) p(x(t+1:T )|x(1:T )) ∝ p(z(t) = i|x(1:t))p(x(t+1:T )|z(t) = i)\n= α (t) i β (t) i ,\n(12)\nξ (t) i,j , p(z (t−1) = i, z(t) = j|x(1:T ))\n∝ p(z(t−1) = i|x(1:t−1)) p(z(t) = j|z(t−1) = i,x(t:T )) ∝ p(z(t−1) = i|x(1:t−1))p(x(t)|z(t) = j) p(x(t+1:T )|z(t) = j)p(z(t) = j|z(t−1) = i)\n= α (t−1) i ϕ (t) j β (t) j Ψ (t) i,j .\n(13)\nϕ (t) i ∈ R|L| , p(x(t)|z(t) = i) is the likelihood of the observation when the hidden state is i (§ 4.2). Written in the matrix form, (12) and (13) become:\nγ(t) ∝ α(t) β(t), (14) ξ(t) ∝ Ψ(t) (α(t−1)(ϕ(t) β(t))T), (15)\nwhere is the element-wise product. Note that the elements in both γ(t) and ξ(t) should sum up to 1.\n8Same as § 4.2, we omit the dependency term e(1:T ).\nThe Forward Pass The filtered marginal α(t)i can be computed iteratively:\nα (t) i , p(z (t) = i|x(1:t)) = p(z(t) = i|x(t),x(1:t−1)) ∝ p(x(t)|z(t) = i)p(z(t) = i|x(1:t−1))\n= ∑ j ϕ (t) i Ψ (t) j,iα (t−1) j .\n(16)\nWritten in the matrix form, (16) becomes\nα(t) ∝ ϕ(t) (Ψ(t)Tα(t−1)). (17)\nWe initialize α with α(0) = π (§ 4.2) since we have no observation at time step 0. As α(t) is a probability distribution, the elements in it sum up to 1. The calculation of α is the forward pass.\nThe Backward Pass In the same way, we do the backward pass to compute the conditional future evidence β(t)i , p(x (t+1:T )|z(t) = i):\nβ (t−1) i , p(x (t+1:T )|z(t) = j) = ∑ j p(z(t) = j,x(t),x(t+1:T )|z(t−1) = i)\n= ∑ j [p(x(t+1:T )|z(t) = j)\np(x(t), z(t) = j|z(t−1) = i)] = ∑ j β (t) j ϕ (t) j Ψ (t) i,j .\n(18) In the matrix form, (18) becomes:\nβ(t−1) = Ψ(t)(ϕ(t) β(t)), (19)\nwhose base case is\nβ (T ) i = p(x (T+1:T )|z(T ) = i) = 1, ∀i ∈ {1, . . . , |L|}.\nA.2 The Maximization step for Unsupervised HMM\nFor traditional unsupervised HMM, the expected complete data log likelihood is maximized by updating the matrices with the approximated pseudostatistics. different from CHMM, HMM has constant transition and emission for all time steps, i.e.:\nΨ(1) = Ψ(t); Φ(1) = Φ(t); ∀t ∈ {2, . . . , T}.\nFor simplicity, we remove the term t for the transition and emission matrices. Suppose we are updating HMM based on one instance with t starting from 1:\nπi = γ (1) i ; (20)\nΨi,j =\n∑T t=2 ξ\n(t) i,j∑T\nt=2 ∑|L| `=1 ξ (t) i,` ; (21)\nΦi,j,k =\n∑T t=1 γ (t) i x\n(t) j,k∑T\nt=1 γ t i\n. (22)\nNote that the observation has property 0 ≤ x(t)j,k ≤ 1 and ∑|L| j=1 x (t) j,k = 1, where k ∈ {1, . . .K} is the index of the weak labeling source."
    }, {
      "heading" : "B Labeling Source Performance",
      "text" : "The weak labeling sources of the CoNLL 2003 dataset come from Lison et al. (2020), whereas Safranchik et al. (2020) provide the sources for the LaptopReview, NCBI-Disease and BC5CDR dataset. For Safranchik et al. (2020)’s labeling sources, we apply a majority voting using their tagging results to the spans detected by their linking rules to convert the linking results to token annotations. In consideration of the training time and resource consumption, we only adopt a subset of the labeling sources provided by the authors. The performance of the labeling sources is presented in the tables below.\nPlease refer to Lison et al. (2020) for the information about the construction of the labeling sources on the CoNLL 2003 dataset; please refer to Safranchik et al. (2020) for the labeling sources on other three datasets."
    }, {
      "heading" : "C Hyper-Parameters",
      "text" : "The experiments are conducted on one GeForce RTX 2080 Ti GPU. For NCBI-Disease, BC5CDR and LaptopReview datasets, CHMM is pre-trained for 5 epochs and trained for 20 epochs. The learning rates for these three datasets are 5×10−4, 10−3 and 10−4, respectively, and the batch sizes are 64, 64 and 128. In phase I, BERT-NER is trained with the default learning rate (5× 10−5) for 100 epochs. The batch sizes are 8, 8, and 48, respectively. Note that for LaptopReview, the maximum length limitation of BERT-NER is set to 128 whereas the limitation is 512 for the other two datasets. In phase II, we use half the learning rate with 20 epochs for each loop.\nFor CoNLL 2003, CHMM has the same number of training epochs as for other datasets. The batch size is 32, and the learning rate is 10−5. BERTNER has a maximum sequence length of 256. It is trained for 15 epochs in phase I and 5 epochs in phase II. Other hyper-parameters are identical to other BERT-NER models’."
    } ],
    "references" : [ {
      "title" : "A review of relation extraction",
      "author" : [ "Nguyen Bach", "Sameer Badaskar." ],
      "venue" : "Literature review for Language and Statistics II, 2:1–15.",
      "citeRegEx" : "Bach and Badaskar.,? 2007",
      "shortCiteRegEx" : "Bach and Badaskar.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "SciBERT: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "NLTK: The natural language toolkit",
      "author" : [ "Steven Bird", "Edward Loper." ],
      "venue" : "Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214–217, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Bird and Loper.,? 2004",
      "shortCiteRegEx" : "Bird and Loper.",
      "year" : 2004
    }, {
      "title" : "Low-resource name tagging learned with weakly labeled data",
      "author" : [ "Yixin Cao", "Zikun Hu", "Tat-seng Chua", "Zhiyuan Liu", "Heng Ji." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Scaling hidden Markov language models",
      "author" : [ "Justin Chiu", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1341–1349, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Chiu and Rush.,? 2020",
      "shortCiteRegEx" : "Chiu and Rush.",
      "year" : 2020
    }, {
      "title" : "Recurrent hidden semi-markov model",
      "author" : [ "Hanjun Dai", "Bo Dai", "Yan-Ming Zhang", "Shuang Li", "Le Song." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-",
      "citeRegEx" : "Dai et al\\.,? 2017",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "NCBI disease corpus: A resource for disease name recognition and concept normalization",
      "author" : [ "Rezarta Islamaj Dogan", "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "J. Biomed. Informatics, 47:1–10.",
      "citeRegEx" : "Dogan et al\\.,? 2014",
      "shortCiteRegEx" : "Dogan et al\\.",
      "year" : 2014
    }, {
      "title" : "Rule-based named entity recognition for greek financial texts",
      "author" : [ "Dimitra Farmakiotou", "Vangelis Karkaletsis", "John Koutsias", "George Sigletos", "Constantine D. Spyropoulos", "Panagiotis Stamatopoulos." ],
      "venue" : "In Proceedings of the Workshop on Computational",
      "citeRegEx" : "Farmakiotou et al\\.,? 2000",
      "shortCiteRegEx" : "Farmakiotou et al\\.",
      "year" : 2000
    }, {
      "title" : "Swellshark: A generative model for biomedical named entity recognition without labeled data",
      "author" : [ "Jason A. Fries", "Sen Wu", "Alexander Ratner", "Christopher Ré." ],
      "venue" : "CoRR, abs/1704.06360.",
      "citeRegEx" : "Fries et al\\.,? 2017",
      "shortCiteRegEx" : "Fries et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "CoRR, abs/1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "The impact of named entity normalization on information retrieval for question answering",
      "author" : [ "Mahboob Alam Khalid", "Valentin Jijkoun", "Maarten De Rijke." ],
      "venue" : "Proceedings of the IR Research, 30th European Conference on Advances in Information",
      "citeRegEx" : "Khalid et al\\.,? 2008",
      "shortCiteRegEx" : "Khalid et al\\.",
      "year" : 2008
    }, {
      "title" : "A tutorial on deep latent variable models of natural language",
      "author" : [ "Yoon Kim", "Sam Wiseman", "Alexander M. Rush." ],
      "venue" : "CoRR, abs/1812.06834.",
      "citeRegEx" : "Kim et al\\.,? 2018",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to contextually aggregate multi-source supervision for sequence labeling",
      "author" : [ "Ouyu Lan", "Xiao Huang", "Bill Yuchen Lin", "He Jiang", "Liyuan Liu", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "BioBERT: a pretrained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Twevent: Segment-based event detection from tweets",
      "author" : [ "Chenliang Li", "Aixin Sun", "Anwitaman Datta." ],
      "venue" : "Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 155–164, New York,",
      "citeRegEx" : "Li et al\\.,? 2012",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Biocreative V CDR task corpus: a resource for chemical disease",
      "author" : [ "Jiao Li", "Yueping Sun", "Robin J. Johnson", "Daniela Sciaky", "Chih-Hsuan Wei", "Robert Leaman", "Allan Peter Davis", "Carolyn J. Mattingly", "Thomas C. Wiegers", "Zhiyong Lu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Bond: Bert-assisted open-domain named entity recognition with distant supervision",
      "author" : [ "Chen Liang", "Yue Yu", "Haoming Jiang", "Siawpeng Er", "Ruijia Wang", "Tuo Zhao", "Chao Zhang." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International Conference",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Named entity recognition without labelled data: A weak supervision approach",
      "author" : [ "Pierre Lison", "Jeremy Barnes", "Aliaksandr Hubin", "Samia Touileb." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Lison et al\\.,? 2020",
      "shortCiteRegEx" : "Lison et al\\.",
      "year" : 2020
    }, {
      "title" : "Structured inference for recurrent hidden semi-markov model",
      "author" : [ "Hao Liu", "Lirong He", "Haoli Bai", "Bo Dai", "Kun Bai", "Zenglin Xu." ],
      "venue" : "Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI’18, page 2447–2453. AAAI",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lis-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "A survey of named entity recognition and classification",
      "author" : [ "David Nadeau", "Satoshi Sekine." ],
      "venue" : "Linguisticae Investigationes, 30(1):3–26. Publisher: John Benjamins Publishing Company.",
      "citeRegEx" : "Nadeau and Sekine.,? 2007",
      "shortCiteRegEx" : "Nadeau and Sekine.",
      "year" : 2007
    }, {
      "title" : "Aggregating and predicting sequence labels from crowd annotations",
      "author" : [ "An Thanh Nguyen", "Byron Wallace", "Junyi Jessy Li", "Ani Nenkova", "Matthew Lease." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Nguyen et al\\.,? 2017",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2017
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Text chunking using transformation-based learning",
      "author" : [ "Lance Ramshaw", "Mitch Marcus." ],
      "venue" : "Third Workshop on Very Large Corpora.",
      "citeRegEx" : "Ramshaw and Marcus.,? 1995",
      "shortCiteRegEx" : "Ramshaw and Marcus.",
      "year" : 1995
    }, {
      "title" : "Snorkel: Rapid training data creation with weak supervision",
      "author" : [ "Alexander Ratner", "Stephen H. Bach", "Henry Ehrenberg", "Jason Fries", "Sen Wu", "Christopher Ré." ],
      "venue" : "Proc. VLDB Endow., 11(3):269–282.",
      "citeRegEx" : "Ratner et al\\.,? 2017",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2017
    }, {
      "title" : "Training complex models with multi-task weak supervision",
      "author" : [ "Alexander Ratner", "Braden Hancock", "Jared Dunnmon", "Frederic Sala", "Shreyash Pandey", "Christopher Ré." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33:4763–4771.",
      "citeRegEx" : "Ratner et al\\.,? 2019",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2019
    }, {
      "title" : "Data programming: Creating large training sets, quickly",
      "author" : [ "Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher Ré." ],
      "venue" : "Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16,",
      "citeRegEx" : "Ratner et al\\.,? 2016",
      "shortCiteRegEx" : "Ratner et al\\.",
      "year" : 2016
    }, {
      "title" : "Denoising multi-source weak supervision for neural text classification",
      "author" : [ "Wendi Ren", "Yinghao Li", "Hanting Su", "David Kartchner", "Cassie Mitchell", "Chao Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3739–3754,",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "Weakly supervised sequence tagging from noisy rules",
      "author" : [ "Esteban Safranchik", "Shiying Luo", "Stephen H. Bach." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelli-",
      "citeRegEx" : "Safranchik et al\\.,? 2020",
      "shortCiteRegEx" : "Safranchik et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning named entity tagger using domain-specific dictionary",
      "author" : [ "Jingbo Shang", "Liyuan Liu", "Xiaotao Gu", "Xiang Ren", "Teng Ren", "Jiawei Han." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Shang et al\\.,? 2018",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2018
    }, {
      "title" : "How to fine-tune bert for text classification",
      "author" : [ "Chi Sun", "Xipeng Qiu", "Yige Xu", "Xuanjing Huang" ],
      "venue" : "Chinese Computational Linguistics,",
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Classification on soft labels is robust against label noise",
      "author" : [ "Christian Thiel." ],
      "venue" : "Proceedings of the 12th International Conference on KnowledgeBased Intelligent Information and Engineering Systems, Part I, KES ’08, pages 65–73, Berlin, Heidel-",
      "citeRegEx" : "Thiel.,? 2008",
      "shortCiteRegEx" : "Thiel.",
      "year" : 2008
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Unsupervised neural hidden Markov models",
      "author" : [ "Ke M. Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight." ],
      "venue" : "Proceedings of the Workshop on Structured Prediction for NLP, pages 63–71, Austin, TX. Association for Computational",
      "citeRegEx" : "Tran et al\\.,? 2016",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
      "author" : [ "A. Viterbi." ],
      "venue" : "IEEE Trans. Inf. Theor., 13(2):260–269.",
      "citeRegEx" : "Viterbi.,? 1967",
      "shortCiteRegEx" : "Viterbi.",
      "year" : 1967
    }, {
      "title" : "Refining hidden markov models with recurrent neural networks",
      "author" : [ "T. Wessels", "Christian W. Omlin." ],
      "venue" : "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, IJCNN 2000, Neural Computing: New Challenges and Per-",
      "citeRegEx" : "Wessels and Omlin.,? 2000",
      "shortCiteRegEx" : "Wessels and Omlin.",
      "year" : 2000
    }, {
      "title" : "Learning neural templates for text generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3174–3187, Brussels, Belgium. Association",
      "citeRegEx" : "Wiseman et al\\.,? 2018",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2018
    }, {
      "title" : "Distantly supervised NER with partial annotation learning",
      "author" : [ "Yaosheng Yang", "Wenliang Chen", "Zhenghua Li", "Zhengqiu He", "Min Zhang" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach",
      "author" : [ "Yue Yu", "Simiao Zuo", "Haoming Jiang", "Wendi Ren", "Tuo Zhao", "Chao Zhang" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "2020) provide the sources for the LaptopReview, NCBI-Disease and BC5CDR dataset. For Safranchik et al. (2020)’s labeling sources, we apply a majority voting using their tagging results to the spans detected by their linking",
      "author" : [ "Safranchik" ],
      "venue" : null,
      "citeRegEx" : "Safranchik,? \\Q2020\\E",
      "shortCiteRegEx" : "Safranchik",
      "year" : 2020
    }, {
      "title" : "2020) for the information about the construction of the labeling sources on the CoNLL 2003 dataset; please refer to Safranchik et al",
      "author" : [ "Lison" ],
      "venue" : null,
      "citeRegEx" : "Lison,? \\Q2020\\E",
      "shortCiteRegEx" : "Lison",
      "year" : 2020
    }, {
      "title" : "The experiments are conducted on one GeForce RTX 2080 Ti GPU. For NCBI-Disease, BC5CDR and LaptopReview datasets, CHMM is pre-trained for 5 epochs and trained for 20 epochs",
      "author" : [ "C Hyper-Parameters" ],
      "venue" : null,
      "citeRegEx" : "Hyper.Parameters,? \\Q2080\\E",
      "shortCiteRegEx" : "Hyper.Parameters",
      "year" : 2080
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Named entity recognition (NER), which aims to identify named entities from unstructured text, is an information extraction task fundamental to many downstream applications such as event detection (Li et al., 2012), relationship extraction (Bach and Badaskar, 2007), and question answering (Khalid et al.",
      "startOffset" : 196,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : ", 2012), relationship extraction (Bach and Badaskar, 2007), and question answering (Khalid et al.",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : ", 2012), relationship extraction (Bach and Badaskar, 2007), and question answering (Khalid et al., 2008).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, many domains have various knowledge resources such as knowledge bases, domain-specific dictionaries, or labeling rules provided by domain experts (Farmakiotou et al., 2000; Nadeau and Sekine, 2007).",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, many domains have various knowledge resources such as knowledge bases, domain-specific dictionaries, or labeling rules provided by domain experts (Farmakiotou et al., 2000; Nadeau and Sekine, 2007).",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 22,
      "context" : "While there are works on distantly supervised NER that use only knowledge bases as weak supervision (Mintz et al., 2009; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020), they cannot leverage complementary information from multiple annotation sources.",
      "startOffset" : 100,
      "endOffset" : 178
    }, {
      "referenceID" : 32,
      "context" : "While there are works on distantly supervised NER that use only knowledge bases as weak supervision (Mintz et al., 2009; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020), they cannot leverage complementary information from multiple annotation sources.",
      "startOffset" : 100,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "While there are works on distantly supervised NER that use only knowledge bases as weak supervision (Mintz et al., 2009; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020), they cannot leverage complementary information from multiple annotation sources.",
      "startOffset" : 100,
      "endOffset" : 178
    }, {
      "referenceID" : 18,
      "context" : "While there are works on distantly supervised NER that use only knowledge bases as weak supervision (Mintz et al., 2009; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020), they cannot leverage complementary information from multiple annotation sources.",
      "startOffset" : 100,
      "endOffset" : 178
    }, {
      "referenceID" : 24,
      "context" : "To handle multi-source weak supervision, several recent works (Nguyen et al., 2017; Safranchik et al., 2020; Lison et al., 2020) leverage the hidden Markov model (HMM), by modeling true labels as hidden variables and inferring them from the observed noisy labels through unsupervised learning.",
      "startOffset" : 62,
      "endOffset" : 128
    }, {
      "referenceID" : 31,
      "context" : "To handle multi-source weak supervision, several recent works (Nguyen et al., 2017; Safranchik et al., 2020; Lison et al., 2020) leverage the hidden Markov model (HMM), by modeling true labels as hidden variables and inferring them from the observed noisy labels through unsupervised learning.",
      "startOffset" : 62,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "To handle multi-source weak supervision, several recent works (Nguyen et al., 2017; Safranchik et al., 2020; Lison et al., 2020) leverage the hidden Markov model (HMM), by modeling true labels as hidden variables and inferring them from the observed noisy labels through unsupervised learning.",
      "startOffset" : 62,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "Though principled, these models fall short in capturing token semantics and context information, as they either model input tokens as one-hot observations (Nguyen et al., 2017) or do not model them at all (Safranchik et al.",
      "startOffset" : 155,
      "endOffset" : 176
    }, {
      "referenceID" : 31,
      "context" : ", 2017) or do not model them at all (Safranchik et al., 2020; Lison et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : ", 2017) or do not model them at all (Safranchik et al., 2020; Lison et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 202
    }, {
      "referenceID" : 40,
      "context" : "Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 202
    }, {
      "referenceID" : 32,
      "context" : "Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "Distant supervision, a specific type of weak supervision, generates training labels from knowledge bases (Mintz et al., 2009; Yang et al., 2018; Shang et al., 2018; Cao et al., 2019; Liang et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 202
    }, {
      "referenceID" : 23,
      "context" : "Other works adopt multiple additional labeling sources, such as heuristic functions that depend on lexical features, word patterns, or document information (Nadeau and Sekine, 2007; Ratner et al., 2016), and unify their results through multi-source label denoising.",
      "startOffset" : 156,
      "endOffset" : 202
    }, {
      "referenceID" : 29,
      "context" : "Other works adopt multiple additional labeling sources, such as heuristic functions that depend on lexical features, word patterns, or document information (Nadeau and Sekine, 2007; Ratner et al., 2016), and unify their results through multi-source label denoising.",
      "startOffset" : 156,
      "endOffset" : 202
    }, {
      "referenceID" : 30,
      "context" : "Several multi-source weakly supervised learning approaches are designed for sentence classification (Ratner et al., 2017, 2019; Ren et al., 2020; Yu et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 162
    }, {
      "referenceID" : 41,
      "context" : "Several multi-source weakly supervised learning approaches are designed for sentence classification (Ratner et al., 2017, 2019; Ren et al., 2020; Yu et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "(2020) train a BiLSTM-CRF network (Huang et al., 2015) with multiple parallel CRF layers, each for an individual labeling source, and aggregate their transitions with confidence scores predicted by an attention network (Bahdanau et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : ", 2015) with multiple parallel CRF layers, each for an individual labeling source, and aggregate their transitions with confidence scores predicted by an attention network (Bahdanau et al., 2015; Luong et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : ", 2015) with multiple parallel CRF layers, each for an individual labeling source, and aggregate their transitions with confidence scores predicted by an attention network (Bahdanau et al., 2015; Luong et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 215
    }, {
      "referenceID" : 13,
      "context" : "Neuralizing the Hidden Markov Model Some works attempt to neuralize HMM in order to relax the Markov assumption while maintaining its generative property (Kim et al., 2018).",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "The work most related to ours leverages neural HMM for sequence labeling (Tran et al., 2016).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "1 Assuming the set of target entity types is E and the tagging scheme is BIO (Ramshaw and Marcus, 1995), NER models assign one label from the label set l ∈ L to each token, where the size of the label set is |L| = 2|E| + 1, e.",
      "startOffset" : 77,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "These results can be calculated by either the Viterbi decoding algorithm (Viterbi, 1967) or directly maximizing the smoothed marginal γ(1:T ).",
      "startOffset" : 73,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : "Compared with the hard labels ẑ(1:T ), soft labels lead to a more stable training process and higher model robustness (Thiel, 2008; Liang et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "Compared with the hard labels ẑ(1:T ), soft labels lead to a more stable training process and higher model robustness (Thiel, 2008; Liang et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : "2) LaptopReview dataset (Pontiki et al., 2014) consists of 3,845 sentences with laptop-related entity mentions.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "3) NCBI-Disease dataset (Dogan et al., 2014) contains 793 PubMed abstracts annotated with disease",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "4) BC5CDR (Li et al., 2016), the dataset accompanies the BioCreative V CDR challenge, consists of 1,500 PubMed articles, annotated with chemical disease mentions.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "We use the original word tokens in the dataset if provided and use NLTK (Bird and Loper, 2004) otherwise for sentence tokenization.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "Baselines We compare our model to the following state-of-the-art baselines: 1) Majority Voting returns the label for a token that has been observed by most of the sources and randomly chooses one if it’s a tie; 2) Snorkel (Ratner et al., 2017) treats each token in a sequence as i.",
      "startOffset" : 222,
      "endOffset" : 243
    }, {
      "referenceID" : 10,
      "context" : "and conducts the label classification without considering its context; 3) SwellShark (Fries et al., 2017) improves Snorkel by predicting all the target entity spans before classifying them using naı̈ve Bayes; 4) AutoNER (Shang et al.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : ", 2017) improves Snorkel by predicting all the target entity spans before classifying them using naı̈ve Bayes; 4) AutoNER (Shang et al., 2018) augments distant supervision by predicting whether two consecutive tokens should be in the same entity span; 5) BOND (Liang et al.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : ", 2018) augments distant supervision by predicting whether two consecutive tokens should be in the same entity span; 5) BOND (Liang et al., 2020) adopts self-training and highconfidence selection to further boost the distant supervision performance.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 31,
      "context" : "(2020) that does not have the integrated neural network; 7) Linked HMM (Safranchik et al., 2020) uses linking rules to provide additional inter-token structural information to the HMM model.",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "The original BERT (Devlin et al., 2019) is used for CoNLL 2003 and LaptopReview datasets, bioBERT (Lee et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : ", 2019) is used for CoNLL 2003 and LaptopReview datasets, bioBERT (Lee et al., 2019) for NCBI-Disease and SciBERT (Beltagy et al.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : ", 2019) for NCBI-Disease and SciBERT (Beltagy et al., 2019) for BC5CDR.",
      "startOffset" : 37,
      "endOffset" : 59
    } ],
    "year" : 2021,
    "abstractText" : "We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pretrained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMMALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERTNER’s output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.",
    "creator" : "LaTeX with hyperref"
  }
}