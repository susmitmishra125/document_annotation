{
  "name" : "2021.acl-long.543.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lexical Semantic Change Discovery",
    "authors" : [ "Sinan Kurtyigit", "Maike Park", "Dominik Schlechtweg", "Jonas Kuhn", "Sabine Schulte im Walde" ],
    "emails" : [ "sinan.kurtyigit@gmail.com,", "park@ids-mannheim.de", "schlecdk@ims.uni-stuttgart.de", "jonas.kuhn@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6985–6998\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6985"
    }, {
      "heading" : "1 Introduction",
      "text" : "There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020).\nHowever, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction\nof novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. application represented a well-motivated choice, because the quality of state-of-the-art models had not been established yet, and because no tuning and testing data were available. But with recent advances in evaluation (Basile et al., 2020; Schlechtweg et al., 2020; Kutuzov and Pivovarova, 2021), the field now owns standard corpora and tuning data for different languages. Furthermore, we have gained experience regarding the interaction of model parameters and modelling task (such as binary vs. graded semantic change). This enables the field to more confidently apply models to discover previously unknown semantic changes. Such discoveries may be useful in a range of fields (Hengchen et al., 2019; Jatowt et al., 2021), among which historical semantics and lexicography represent obvious choices (Ljubešić, 2020).\nIn this paper, we tune the most successful models from SemEval-2020 Task 1 (Schlechtweg et al., 2020) on the German task data set in order to obtain high-quality discovery predictions for novel semantic changes. We validate the model predictions in a standardized human annotation procedure and visualize the annotations in an intuitive way supporting further analysis of the semantic structure relating word usages. In this way, we automatically detect previously described semantic changes and at the same time discover novel instances of semantic change which had not been indexed in standard historical dictionaries before. Our approach is largely automated, by relying on unsupervized language models and a publicly available annotation system requiring only a small set of judgments from annotators. We further evaluate the usability of the approach from a lexicographer’s viewpoint and show how intuitive visualizations of human-annotated data can benefit dictionary makers."
    }, {
      "heading" : "2 Related Work",
      "text" : "State-of-the-art semantic change detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Schütze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava and Baroni and Cook et al. evaluate their predictions on the (macro) lemma level. We, however, annotate our predictions on the (micro) usage level, enabling us to better control the criteria for annotation and their inter-subjectivity. In this way, we are also able to build clusters of usages with the same sense and to visualise the annotated data in an intuitive way. The annotation process is designed to not only improve the quality of the annotations, but also lessen the burden on the annotators. We additionally seek the opinion of a professional lexicographer to assess the usefulness of the predictions outside the field of LSCD.\nIn contrast to previous work, we obtain model predictions by fine-tuning static and contextualized embeddings on high-quality data sets (Schlechtweg et al., 2020) that were not available before. We provide a highly automated general framework for\nevaluating models and predicting changing words on all kinds of corpora."
    }, {
      "heading" : "3 Data",
      "text" : "We use the German data set provided by the SemEval-2020 shared task (Schlechtweg et al., 2020, 2021). The data set contains a diachronic corpus pair for two time periods to be compared, a set of carefully selected target words as well as binary and graded gold data for semantic change evaluation and fine-tuning purposes.\nCorpora The DTA corpus (Deutsches Textarchiv, 2017) and a combination of the BZ (Berliner Zeitung, 2018) and ND (Neues Deutschland, 2018) corpora are used. DTA contains texts from different genres spanning the 16th–20th centuries. BZ and ND are newspaper corpora jointly spanning 1945–1993. Schlechtweg et al. (2020) extract two time specific corpora C1 (DTA, 1800–1899) and C2 (BZ+ND 1946–1990) and provide raw and lemmatized versions.\nTarget Words A list of 48 target words, consisting of 32 nouns, 14 verbs and 2 adjectives is provided. These are controlled for word frequency to minimize model biases that may lead to artificially high performance (Dubossarsky et al., 2017; Schlechtweg and Schulte im Walde, 2020)."
    }, {
      "heading" : "4 Models",
      "text" : "Type-based models generate a single vector for each word from a pre-defined vocabulary. In contrast, token-based models generate one vector for each usage of a word. While the former do not take into account that most words have multiple senses, the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020). Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020). Kutuzov and Giulianelli (2020) yet show that the performance of tokenbased models (especially ELMo) can be increased by fine-tuning on the target corpora. Laicher et al. (2020, 2021) drastically improve the performance of BERT by reducing the influence of target word morphology. In this paper, we compare both families of approaches for change discovery."
    }, {
      "heading" : "4.1 Type-based approach",
      "text" : "Most type-based approaches in LSCD combine three sub-systems: (i) creating semantic word representations, (ii) aligning them across corpora, and (iii) measuring differences between the aligned representations (Schlechtweg et al., 2019). Motivated by its wide usage and high performance among participants in SemEval-2020 (Schlechtweg et al., 2020) and DIACR-Ita (Basile et al., 2020), we use the Skip-gram with Negative Sampling model (SGNS, Mikolov et al., 2013a,b) to create static word embeddings. SGNS is a shallow neural language model trained on pairs of word co-occurrences extracted from a corpus with a symmetric window. The optimized parameters can be interpreted as a semantic vector space that contains the word vectors for all words in the vocabulary. In our case, we obtain two separately trained vector spaces, one for each subcorpus (C1 and C2). Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; Pömsl and Lyapin, 2020; Pražák et al., 2020)."
    }, {
      "heading" : "4.2 Token-based approach",
      "text" : "Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019) is a transformer-based neural language model designed to find contextualized representations for text by analyzing left and right contexts. The base version processes text in 12 different layers. In each layer, a contextualized token vector representation is created for every word. A layer, or a combination of multiple layers (we use the average), then serves as a representation for a token. For every target word we extract usages (i.e., sentences in which the word appears) by randomly sub-sampling up to 100 sentences from both subcorpora C1 and C2.1 These are then fed into BERT to create contex1We sub-sample as some words appear in 10,000 or more sentences.\ntualized embeddings, resulting in two sets of up to 100 contextualized vectors for both time periods. To measure the change between these sets we use two different approaches: (i) We calculate the Average Pairwise Distance (APD). The idea is to randomly pick a number of vectors from both sets and measure their mutual distances (Schlechtweg et al., 2018; Kutuzov and Giulianelli, 2020). The change score corresponds to the mean average distance of all comparisons. (ii) We average both vector sets and measure the Cosine Distance (COS) between the two resulting mean vectors (Kutuzov and Giulianelli, 2020)."
    }, {
      "heading" : "5 Discovery",
      "text" : "SemEval-2020 Task 1 consists of two subtasks: (i) binary classification: for a set of target words, decide whether (or not) the words lost or gained sense(s) between C1 and C2, and (ii) graded ranking: rank a set of target words according to their degree of LSC between C1 and C2. These require to detect semantic change in a small pre-selected set of target words. Instead, we are interested in the discovery of changing words from the full vocabulary of the corpus. We define the task of lexical semantic change discovery as follows.\nGiven a diachronic corpus pair C1 and C2, decide for the intersection of their vocabularies which words lost or gained sense(s) between C1 and C2.\nThis task can also be seen as a special case of SemEval’s Subtask 1 where the target words equal the intersection of the corpus vocabularies. Note, however, that discovery introduces additional difficulties for models, e.g. because a large number of predictions is required and the target words are not preselected, balanced or cleaned. Yet, discovery is an important task, with applications such as lexicography where dictionary makers aim to cover the full vocabulary of a language."
    }, {
      "heading" : "5.1 Approach",
      "text" : "We start the discovery process by generating optimized graded value predictions using highperforming parameter configurations following previous work and fine-tuning. Afterwards, we infer binary scores with a thresholding technique (see below). We then tune the threshold to find the best-performing type- and token-based approach\nfor binary classification. These are used to generate two sets of predictions.2\nEvaluation metrics We evaluate the graded rankings in Subtask 2 by computing Spearman’s rank-order correlation coefficient ρ. For the binary classification subtask we compute precision, recall and F0.5. The latter puts a stronger focus on precision than recall because our human evaluation cannot be automated, so we decided to weigh quality (precision) higher than quantity (recall).\nParameter tuning Solving Subtask 2 is straightforward, since both the type-based and token-based approaches output distances between representations for C1 and C2 for every target word. Like many approaches in SemEval-2020 Task 1 and DIACR-Ita we use thresholding to binarise these values. The idea is to define a threshold parameter, where all ranked words with a distance greater or equal to this threshold are labeled as changing words.\nFor cases where no tuning data is available, Kaiser et al. (2020b) propose to choose the threshold according to the population of CDs of all words in the corpus. Kaiser et al. set the threshold to µ+ σ, where µ is the mean and σ is the standard deviation of the population. We slightly modify this approach by changing the threshold to µ + t ∗ σ. In this way, we introduce an additional parameter t, which we tune on the SemEval-2020 test data. We test different values ranging from −2 to 2 in steps of 0.1.\nPopulation Since SGNS generates type-based vectors for every word in the vocabulary, measuring the distances for the full vocabulary comes with low additional computational effort. Unfortunately, this is much more difficult for BERT. Creating up to 100 vectors for every word in the vocabulary drastically increases the computational burden. We choose a population of 500 words for our work allowing us\n2Find the code used for each step of the prediction process at https://github.com/seinan9/ LSCDiscovery.\nto test multiple parameter configurations.3 We sample words from different frequency areas to have predictions not only for low-frequency words. For this, we first compute the frequency range (highest frequency – lowest frequency) of the vocabulary. This range is then split into 5 areas of equal frequency width. Random samples from these areas are taken based on how many words they contain. For example: if the lowest frequency area contains 50% of all words from the vocabulary, then 0.5 ∗ 500 = 250 random samples are taken from this area. The SemEval-2020 target words are excluded from this sampling process. The resulting population is used to create predictions for both models.\nFiltering The predictions contain proper names, foreign language and lemmatization errors, which we aim to filter out, as such cases are usually not considered as semantic changes. We only allow nouns, verbs and adjectives to pass. Words where over 10% of the usages are either non-German or contain more than 25% punctuation are filtered out as well."
    }, {
      "heading" : "6 Annotation",
      "text" : "The model predictions are validated by human annotation. For this, we apply the SemEval-2020 Task 1 procedure, as described in Schlechtweg et al. (2020). Annotators are asked to judge the semantic relatedness of pairs of word usages, such as the two usages of Aufkommen in (1) and (2), on the scale in Table 1.\n(1) Es ist richtig, dass mit dem Aufkommen der Manufaktur im Unterschied zum Handwerk sich Spuren der Kinderexploitation zeigen. ‘It is true that with the emergence of the manufactory, in contrast to the handicraft, traces of child labor are showing.’\n(2) Sie wissen, daß wir für das Vieh mehr Futter aus eigenem Aufkommen brauchen. ‘They know that we need more feed from our own production for the cattle.’\nThe annotated data of a word is represented in a Word Usage Graph (WUG), where vertices represent word usages, and weights on edges represent\n3In a practical setting where predictions have to be generated only once, a much larger number may be chosen. Also, possibilities to scale up BERT performance can be applied (Montariol et al., 2021).\nthe (median) semantic relatedness judgment of a pair of usages such as (1) and (2). The final WUGs are clustered with a variation of correlation clustering (Bansal et al., 2004; Schlechtweg et al., 2020) (see Figure 1, left) and split into two subgraphs representing nodes from subcorpora C1 and C2, respectively (middle and right). Clusters are then interpreted as word senses and changes in clusters over time as lexical semantic change.\nIn contrast to Schlechtweg et al. we use the openly available DURel interface for annotation and visualization.4 This also implies a change in sampling procedure, as the system currently implements only random sampling of use pairs (without SemEval-style optimization). For each target word we sample |U1| = |U2| = 25 usages (sentences) per subcorpus (C1, C2) and upload these to the DURel system, which presents use pairs to annotators in randomized order. We recruit eight German native speakers with university level education as annotators. Five have a background in linguistics, two in German studies, and one has an additional professional background in lexicography. Similar to Schlechtweg et al., we ensure the robustness of the obtained clusterings by continuing the annotation of a target word until all multi-clusters (clusters with more than one usage) in its WUG are connected by at least one judgment. We finally label a target word as changed (binary) if it gained or lost a cluster over time. For instance, Aufkommen in Figure 1 is labeled as change as it gains the orange cluster from C1 to C2. Following Schlechtweg et al. (2020) we use k and n as lower frequency thresholds to avoid that small random fluctuations in sense frequencies caused by sampling variability or annotation error be misclas-\n4https://www.ims.uni-stuttgart.de/ data/durel-tool.\nsified as change. As proposed in Schlechtweg and Schulte im Walde (submitted) for comparability across sample sizes we set k = 1 ≤ 0.01∗|Ui| ≤ 3 and n = 3 ≤ 0.1 ∗ |Ui| ≤ 5, where |Ui| is the number of usages from the respective time period (after removing incomprehensible usages from the graphs). This results in k = 1 and n = 3 for all target words.\nFind an overview over the final set of WUGs in Table 2. We reach a comparably high interannotator agreement (Krippendorf’s α = .58).5"
    }, {
      "heading" : "7 Results",
      "text" : "We now describe the results of the tuning and discovery procedures."
    }, {
      "heading" : "7.1 Tuning",
      "text" : "SGNS is commonly used (Schlechtweg et al., 2020) and also highly optimized (Kaiser et al., 2020a,b, 2021), so it is difficult to further increase the performance. We thus rely on the work of Kaiser et al. (2020a) and test their parameter configurations on the German SemEval-2020 data set.6 We obtain three slightly different parameter configurations (see Table 3 for more details), yielding competitive ρ = .690, ρ = .710 and ρ = .710, respectively.\nIn order to improve the performance of BERT, we test different layer combinations, preprocessings and semantic change measures. Following Laicher et al. (2020, 2021), we are able to drastically increase the performance of BERT\n5We provide WUGs as Python NetworkX graphs, descriptive statistics, inferred clusterings, change values and interactive visualizations for all target words and the respective code at https://www.ims.uni-stuttgart.de/ data/wugs.\n6All configurations use w = 10, d = 300, e = 5 and a minimum frequency count of 39.\non the German SemEval-2020 data. In a preprocessing step, we replace the target word in every usage by its lemma. In combination with layer 12+1, both APD and COS perform competitively well on Subtask 2 (ρ = .690 and ρ = .738).\nAfter applying thresholding as described in Section 5 we obtain F0.5-scores for a large range of thresholds. SGNS achieves peak F0.5-scores of .692, .738 and .685, respectively (see Table 3). Interestingly, the optimal threshold is at t = 1.0 in all three cases. This corresponds to the threshold used in Kaiser et al. (2020b). While the peak F0.5 of BERT+APD is marginally worse (.598 at t = −0.2), BERT+COS is able to outperform the best SGNS configuration with a peak of .741 at t = 0.1.\nIn order to obtain an estimate on the sampling variability that is caused by sampling only up to 100 usages per word for BERT+APD and BERT+COS (see Section 4.2), we repeat the whole procedure 9 times and estimate mean and standard deviation of performance on the tuning data. In the beginning of every run the usages are randomly sampled from the corpora. We observe a mean ρ of .657 for BERT+APD and .743 for BERT+COS with a standard deviation of .015 and .012, respectively, as well as a mean F0.5 of .576 for BERT+APD and .684 for BERT+COS with a standard deviation of .013 and .038, respectively. This shows that the variability caused by sub-sampling word usages is negligible."
    }, {
      "heading" : "7.2 Discovery",
      "text" : "We use the top-performing configurations (see Table 3) to generate two sets of large-scale predictions. While we use the lemmatized corpora for SGNS, in BERT’s case we choose the raw corpora with lemmatized target words instead. The latter choice is motivated by the previously described performance increases. After the filtering as described in Section 6, we obtain 27 and 75 words labeled\nas changing, respectively. We further sample 30 targets from the second set of predictions to obtain a feasible number for annotation. We call the first set SGNS targets and the second one BERT targets, with an overlap of 7 targets. Additionally, we randomly sample 30 words from the population (with an overlap of 5 with the SGNS and BERT targets) in order to have an indication of what the change distribution underlying the corpora is. We call these baseline (BL) targets. This baseline will help us to put the results of the predictions in context and to find out whether the predictions of the two models can be explained by pure randomness. Following the annotation process, binary gold data is generated for all three target sets, in order to validate the quality of the predictions.\nThe evaluation of the predictions is presented in Table 3. We achieve a F0.5-score of .714 for SGNS and .620 for BERT. Out of the 27 words predicted by the SGNS model, 18 (67 %) were actually labeled as changing words by the human annotators. In comparison, only 17 out of the 30 (57 %) BERT predictions were annotated as such. The performance of SGNS for prediction (SGNS targets) is even higher than on the tuning data (SemEval targets). In contrast, BERT’s performance for prediction drops strongly in comparison to the performance on the tuning data (.741 vs. .620). This reproduces previous results and confirms that (off-the-shelf) BERT generalises poorly for LSCD and does not transfer well between data sets (Laicher et al., 2020). If we compare these results to the baseline, we can see that both models perform much better than the random baseline (F0.5 of .349). Only 10 out of the 30 (30 %) randomly sampled words are annotated as changing. This indicates, that the performance of SGNS and BERT is likely not a cause of randomness. Both models considerably increase the chance of finding changing words compared to a random model.\nFigure 2 shows the detailed F0.5 developments\nacross different thresholds on the SemEval targets and the predicted words. Increasing the threshold on the predicted words improves the F0.5 for both the type-based and token-based approach. A new high-score of .783 at t = 1.3 is achievable for SGNS. While BERT’s performance also increases to a peak of .714 at t = 1.0, it is still lower than in the tuning phase."
    }, {
      "heading" : "7.3 Analysis",
      "text" : "For further insights into sources of errors, we take a close look at the false positives, their WUGs and the underlying usages. Most of the wrong predictions can be grouped into one out of two error sources (cf. Kutuzov, 2020, pp. 175–182).\nContext change The first category includes words where the context in the usages shifts between time periods, while the meaning stays the same. The WUG of Angriffswaffe (‘offensive weapon’) (see Figure 5 in Appendix A) shows a single cluster for both C1 and C2. In the first time period Angriffswaffe is used to refer to a hand weapon (such as ‘sword’, ‘spear’). In the second period, however, the context changes to nuclear weaponry. We can see a clear contextual shift, while the meaning did not change. In this case both models are tricked by the change of context. Further false positives in this category are the SGNS targets Ächtung (‘ostracism’) and aussterben (‘to die out’) and the COS targets Königreich (‘kingdom’) and Waffenruhe (‘ceasefire’).\nContext variety Words that can be used in a large variety of contexts form the second group of false positives. SGNS falsely predicts neunjährig as a changing word. We take a closer look at its WUG (see Figure 6 in Appendix A). We observe\nthat there is only one and the same cluster in both time periods, and the meaning of the target does not change, even though a large variety of contexts exists in both C1 and C2. For example: ‘which bears oats at nine years fertilization’, ‘courageously, a nine-year-old Spaniard did something’ and ‘after nine years of work’. Both models are misguided by this large context variety. Examples include the SGNS targets neunjährig (‘9-year-old’) and vorjährig (‘of the previous year’) and the COS targets bemerken (‘to notice’) and durchdenken (‘to think through’)."
    }, {
      "heading" : "8 Lexicographical evaluation",
      "text" : "We now evaluate the usefulness of the proposed semantic change discovery procedure including the annotation system and WUG visualization from a lexicographer’s viewpoint. The advantage of our approach lies in providing lexicographers and dictionary makers the choice to take a look into predictions they consider promising with respect to their research objective (disambiguation of word senses, detection of novel senses, detection of archaisms, describing senses in regard to specific discourses etc.) and the type of dictionary. Visualized predictions for target words may be analyzed in regard to single senses, clusters of senses, the semantic proximity of sense clusters and a stylized representation of frequency. Random sampling of usages also offers the opportunity to judge underrepresented senses in a sample that might be infrequent in a corpus or during a specific period of time (although currently a high number of overall annotations would be required in order to do so). Most importantly, the use of a variable number of human annotators has the potential to ensure a more objective analysis of large amounts of corpus\ndata. In order to evaluate the potential of the approach for assisting lexicographers with extending dictionaries, we analyze statistical measures and predictions of the models provided for the two sets of predictions (SGNS, BERT) and compare them to existing dictionary contents.\nWe consider overall inter-annotator agreement (α >= .5) and annotated binary change label to select 21 target words for lexicographical analysis. In this way, we exclude unclear cases and non-changing words. The target words are analyzed by inspecting cluster visualizations of WUGs (such as in Figure 1) and comparing them to entries in general and specialized dictionaries in order to determine:\n• whether a candidate novel sense is already included in one of the reference dictionaries,\n• whether a candidate novel sense is included in one of the two reference dictionaries that are consulted forC1 (covering the period between 1800–1899) and C2 (covering the period between 1946–1990), indicating the rise of a novel sense, the archaization of older senses or a change in frequency.\nThree dictionaries are consulted throughout the analysis: (i) the Dictionary of the German language (DWB) by Jacob und Wilhelm Grimm (digitized version of the 1st print published between 1854– 1961), (ii) the Dictionary of Contemporary German (WGD), published between 1964–1977, now curated and digitized by the DWDS and (iii) the Duden online dictionary of German language (DUDEN), reflecting usage of Contemporary German\nup until today.7 Additionally, lemma entries in the Wiktionary online dictionary (Wiktionary) are consulted to verify genuinely novel senses described in Section 8.1."
    }, {
      "heading" : "8.1 Records of novel senses",
      "text" : "In the case of 17 target words, all senses identified by the system are included in at least one of the three dictionaries consulted for the analysis. In the four remaining cases, at least one novel sense of a word is neither paraphrased nor given as an example of semantically related senses in the dictionaries:\neinbinden Reference to the integration or embedding of details on a topic, event, person in respect to a chronological order within written text or visual presentation (e.g. for an exhibition on an author) is judged as a novel sense in close semantic proximity to the old sense ‘to bind sth. into sth.’, e.g. flowers into a bundle of flowers. einbinden is also used in technical contexts, meaning ‘to (physically) implement parts of a construction or machine into their intended slots’.\nniederschlagen In cases where the verb niederschlagen co-occurs with the verb particle auf and the noun Flügel, the verb refers to a bird’s action of repeatedly moving its wings up and down in order to fly.\nregelrecht Used as an adverb, regelrecht may refer to something being the usual outcome that ought\n7Only the fully-digitized version of the DWB’s first print was consulted for this evaluation, since a revised version has not been completed yet and is only available for lemmas starting with letters a–f.\nto be expected due to scientific principles, with an emphasis on the actual result of an action (such as the dyeing of fiber of a piece of clothing following the bleaching process), whereas senses included in dictionaries for general language emphasize either the intended accordance with a rule or something usually happening (the latter being colloquial use).\nZehner (see Figure 3 in Appendix A) The meaning ‘a winning sequence of numbers in the national lottery’, predicted to have risen as a novel sense between C1 and C2, is not included in any of the reference dictionaries.\nIn most of these cases, senses identified as novel reflect metaphoric use, indicating that definitions in existing dictionary entries may need to be broadened, or example sentences would have to be added. Some of the senses described in this section might be included in specialized dictionaries, e.g. technical usage of einbinden."
    }, {
      "heading" : "8.2 Records of changes",
      "text" : "For 12 target words, semantic change predicted by the models (innovative, reductive or a salient change of frequency of a sense) correlates with the addition or non-inclusion of senses in dictionary entries consulted for the respective period of time (DWB for C1, WGD for C2). It should be noted though, that lemma lists of the two dictionaries might be lacking lemmas in the headword list, and lemma entries might be lacking paraphrases or examples of senses of the lemma, simply because corpus-based lexicography was not available at the time of their first print and revisions of the dictionaries are currently work in progress.\nAdditionally, we consult a dictionary for Early New High German (FHD) in order to check whether discovered novel senses existed at an earlier stage and may be discovered due to low frequency or sampling error. In two cases, discovered novel senses that are not included in the DWB (for C1) are found to be included in the FHD.\nInterestingly, one sense paraphrased for Ausrufung (‘a loud wording, a shout’) is included in neither of the two dictionaries consulted to judge senses from C1 and C2, but in the FHD (earlier) and DUDEN (as of now). These findings suggest that it might be reasonable to use more than two reference corpora. This would also alleviate the corpus bias stemming from idiosyncratic data sampling procedures."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We used two state-of-the-art approaches to LSC detection in combination with a recently published high-quality data set to automatically discover semantic changes in a German diachronic corpus pair. While both approaches were able to discover various semantic changes with above-random probability, some of them previously undescribed in etymological dictionaries, the type-based approach showed a clearly better performance.\nWe validated model predictions by an optimized human annotation process yielding high inter-annotator agreement and providing convenient ways of visualization. In addition, we evaluated the full discovery process from a lexicographer’s point of view and conclude that we obtained high-quality predictions, useful visualizations and previously unreported changes. On the other hand, we discovered some issues with respect to the reliability of predictions for semantic change and number and composition of reference corpora that are going to be dealt with in the future. The results of the analyses endorse that our approach might aid lexicographers with extending and altering existing dictionary entries."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the three reviewers for their insightful feedback and Pedro González Bascoy for setting up the DURel annotation tool. Dominik Schlechtweg was supported by the Konrad Adenauer Foundation and the CRETA center funded by the German Ministry for Education and Research (BMBF) during the conduct of this study."
    }, {
      "heading" : "A Additional plots",
      "text" : "Please find additional plots of Word Usage Graphs in Figures 3–6."
    } ],
    "references" : [ {
      "title" : "digitalized edition curated by the Wörterbuchnetz at the Trier Center for Digital Humanities. https://www.woerterbuchnetz.de/ DWB",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Grimm,? \\Q2021\\E",
      "shortCiteRegEx" : "Grimm",
      "year" : 2021
    }, {
      "title" : "BOS at SemEval-2020 Task 1: Word Sense Induction via Lexical Substitution for Lexical Semantic Change",
      "author" : [ "Nikolay Arefyev", "Vasily Zhikov" ],
      "venue" : null,
      "citeRegEx" : "Arefyev and Zhikov.,? \\Q2020\\E",
      "shortCiteRegEx" : "Arefyev and Zhikov.",
      "year" : 2020
    }, {
      "title" : "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Artetxe et al\\.,? 2016",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2016
    }, {
      "title" : "Correlation clustering",
      "author" : [ "Nikhil Bansal", "Avrim Blum", "Shuchi Chawla." ],
      "venue" : "Machine Learning, 56(13):89–113.",
      "citeRegEx" : "Bansal et al\\.,? 2004",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2004
    }, {
      "title" : "Overview of the EVALITA 2020 Diachronic Lexical Semantics (DIACR-Ita) Task",
      "author" : [ "Pierpaolo Basile", "Annalina Caputo", "Tommaso Caselli", "Pierluigi Cassotti", "Rossella Varvara." ],
      "venue" : "Proceedings of the 7th evaluation campaign of Natural Language",
      "citeRegEx" : "Basile et al\\.,? 2020",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2020
    }, {
      "title" : "Diachronic Analysis of the Italian Language exploiting Google Ngram",
      "author" : [ "Pierpaolo Basile", "Annalina Caputo", "Roberta Luisi", "Giovanni Semeraro" ],
      "venue" : null,
      "citeRegEx" : "Basile et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploiting the Web for Semantic Change Detection, pages 194–208",
      "author" : [ "Pierpaolo Basile", "Barbara Mcgillivray" ],
      "venue" : null,
      "citeRegEx" : "Basile and Mcgillivray.,? \\Q2018\\E",
      "shortCiteRegEx" : "Basile and Mcgillivray.",
      "year" : 2018
    }, {
      "title" : "Diachronic newspaper corpus published by Staatsbibliothek zu Berlin [online",
      "author" : [ "Berliner Zeitung" ],
      "venue" : null,
      "citeRegEx" : "Zeitung.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zeitung.",
      "year" : 2018
    }, {
      "title" : "A lexicographic appraisal of an automatic approach for detecting new word senses",
      "author" : [ "Paul Cook", "Jey Han Lau", "Michael Rundell", "Diana McCarthy", "Timothy Baldwin." ],
      "venue" : "Proceedings of eLex 2013, pages 49–65.",
      "citeRegEx" : "Cook et al\\.,? 2013",
      "shortCiteRegEx" : "Cook et al\\.",
      "year" : 2013
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change",
      "author" : [ "Haim Dubossarsky", "Simon Hengchen", "Nina Tahmasebi", "Dominik Schlechtweg." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Dubossarsky et al\\.,? 2019",
      "shortCiteRegEx" : "Dubossarsky et al\\.",
      "year" : 2019
    }, {
      "title" : "Outta control: Laws of semantic change and inherent biases in word representation models",
      "author" : [ "Haim Dubossarsky", "Daphna Weinshall", "Eitan Grossman." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Dubossarsky et al\\.,? 2017",
      "shortCiteRegEx" : "Dubossarsky et al\\.",
      "year" : 2017
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "A Bayesian model of diachronic meaning change",
      "author" : [ "Lea Frermann", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:31–45.",
      "citeRegEx" : "Frermann and Lapata.,? 2016",
      "shortCiteRegEx" : "Frermann and Lapata.",
      "year" : 2016
    }, {
      "title" : "Analysing lexical semantic change with contextualised word representations",
      "author" : [ "Mario Giulianelli", "Marco Del Tredici", "Raquel Fernández." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3960–",
      "citeRegEx" : "Giulianelli et al\\.,? 2020",
      "shortCiteRegEx" : "Giulianelli et al\\.",
      "year" : 2020
    }, {
      "title" : "A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus",
      "author" : [ "Kristina Gulordava", "Marco Baroni." ],
      "venue" : "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages",
      "citeRegEx" : "Gulordava and Baroni.,? 2011",
      "shortCiteRegEx" : "Gulordava and Baroni.",
      "year" : 2011
    }, {
      "title" : "Diachronic word embeddings reveal statistical laws of semantic change",
      "author" : [ "William L. Hamilton", "Jure Leskovec", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Hamilton et al\\.,? 2016",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "A data-driven approach to the changing vocabulary of the ’nation’ in English, Dutch, Swedish and Finnish newspapers, 1750-1950",
      "author" : [ "Simon Hengchen", "Ruben Ros", "Jani Marjanen." ],
      "venue" : "Proceedings of the Digital Humanities (DH) conference 2019,",
      "citeRegEx" : "Hengchen et al\\.,? 2019",
      "shortCiteRegEx" : "Hengchen et al\\.",
      "year" : 2019
    }, {
      "title" : "Challenges for Computational Lexical Semantic Change",
      "author" : [ "Simon Hengchen", "Nina Tahmasebi", "Dominik Schlechtweg", "Haim Dubossarsky." ],
      "venue" : "Nina Tahmasebi, Lars Borin, Adam Jatowt, Yang Xu, and Simon Hengchen, editors,",
      "citeRegEx" : "Hengchen et al\\.,? 2021",
      "shortCiteRegEx" : "Hengchen et al\\.",
      "year" : 2021
    }, {
      "title" : "Diachronic sense modeling with deep contextualized word embeddings: An ecological view",
      "author" : [ "Renfen Hu", "Shen Li", "Shichen Liang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3899–3908,",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Computational approaches to lexical semantic change: Visualization systems and novel applications",
      "author" : [ "Adam Jatowt", "Nina Tahmasebi", "Lars Borin." ],
      "venue" : "Nina Tahmasebi, Lars Borin, Adam Jatowt, Yang Xu, and Simon Hengchen, editors,",
      "citeRegEx" : "Jatowt et al\\.,? 2021",
      "shortCiteRegEx" : "Jatowt et al\\.",
      "year" : 2021
    }, {
      "title" : "Effects of Pre- and Post-Processing on type-based Embeddings in Lexical Semantic Change Detection",
      "author" : [ "Jens Kaiser", "Sinan Kurtyigit", "Serge Kotchourko", "Dominik Schlechtweg." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the",
      "citeRegEx" : "Kaiser et al\\.,? 2021",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2021
    }, {
      "title" : "IMS at SemEval2020 Task 1: How low can you go? Dimensionality in Lexical Semantic Change Detection",
      "author" : [ "Jens Kaiser", "Dominik Schlechtweg", "Sean Papay", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the 14th International Workshop on Semantic",
      "citeRegEx" : "Kaiser et al\\.,? 2020a",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2020
    }, {
      "title" : "OP-IMS @ DIACR-Ita: Back to the Roots: SGNS+OP+CD still rocks Semantic Change Detection",
      "author" : [ "Jens Kaiser", "Dominik Schlechtweg", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the 7th evaluation campaign of Natural Language Processing",
      "citeRegEx" : "Kaiser et al\\.,? 2020b",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2020
    }, {
      "title" : "Temporal analysis of language through neural language models",
      "author" : [ "Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov." ],
      "venue" : "LTCSS@ACL, pages 61–65. Association for Computational Linguistics.",
      "citeRegEx" : "Kim et al\\.,? 2014",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistically significant detection of linguistic change",
      "author" : [ "Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web, WWW, pages 625–635, Florence, Italy.",
      "citeRegEx" : "Kulkarni et al\\.,? 2015",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributional word embeddings in modeling diachronic semantic change",
      "author" : [ "Andrey Kutuzov" ],
      "venue" : null,
      "citeRegEx" : "Kutuzov.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kutuzov.",
      "year" : 2020
    }, {
      "title" : "UiOUvA at SemEval-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection",
      "author" : [ "Andrey Kutuzov", "Mario Giulianelli." ],
      "venue" : "Proceedings of the 14th International Workshop on Semantic Evaluation, Barcelona, Spain. Associa-",
      "citeRegEx" : "Kutuzov and Giulianelli.,? 2020",
      "shortCiteRegEx" : "Kutuzov and Giulianelli.",
      "year" : 2020
    }, {
      "title" : "Diachronic word embeddings and semantic shifts: a survey",
      "author" : [ "Andrey Kutuzov", "Lilja Øvrelid", "Terrence Szymanski", "Erik Velldal." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1384–1397, Santa Fe, New",
      "citeRegEx" : "Kutuzov et al\\.,? 2018",
      "shortCiteRegEx" : "Kutuzov et al\\.",
      "year" : 2018
    }, {
      "title" : "Rushifteval: a shared task on semantic shift detection for russian",
      "author" : [ "Andrey Kutuzov", "Lidia Pivovarova." ],
      "venue" : "Komp’yuternaya Lingvistika i Intellektual’nye Tekhnologii: Dialog conference.",
      "citeRegEx" : "Kutuzov and Pivovarova.,? 2021",
      "shortCiteRegEx" : "Kutuzov and Pivovarova.",
      "year" : 2021
    }, {
      "title" : "CL-IMS @ DIACR-Ita: Volente o Nolente: BERT does not outperform SGNS on Semantic Change Detection",
      "author" : [ "Severin Laicher", "Gioia Baldissin", "Enrique Castaneda", "Dominik Schlechtweg", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the 7th",
      "citeRegEx" : "Laicher et al\\.,? 2020",
      "shortCiteRegEx" : "Laicher et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining and Improving BERT Performance on Lexical Semantic Change Detection",
      "author" : [ "Severin Laicher", "Sinan Kurtyigit", "Dominik Schlechtweg", "Jonas Kuhn", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the Student Research",
      "citeRegEx" : "Laicher et al\\.,? 2021",
      "shortCiteRegEx" : "Laicher et al\\.",
      "year" : 2021
    }, {
      "title" : "Capturing evolution in word usage: Just add more clusters? In Companion Proceedings of the Web Conference 2020, WWW ’20, pages 343—-349, New York, NY, USA",
      "author" : [ "Matej Martinc", "Syrielle Montariol", "Elaine Zosa", "Lidia Pivovarova." ],
      "venue" : "Asso-",
      "citeRegEx" : "Martinc et al\\.,? 2020",
      "shortCiteRegEx" : "Martinc et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Scalable and interpretable semantic change detection",
      "author" : [ "Syrielle Montariol", "Matej Martinc", "Lidia Pivovarova." ],
      "venue" : "2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Montariol et al\\.,? 2021",
      "shortCiteRegEx" : "Montariol et al\\.",
      "year" : 2021
    }, {
      "title" : "GASC: Genre-aware semantic change for ancient Greek",
      "author" : [ "Valerio Perrone", "Marco Palma", "Simon Hengchen", "Alessandro Vatri", "Jim Q. Smith", "Barbara McGillivray." ],
      "venue" : "Proceedings of the 1st International Workshop on Computational Ap-",
      "citeRegEx" : "Perrone et al\\.,? 2019",
      "shortCiteRegEx" : "Perrone et al\\.",
      "year" : 2019
    }, {
      "title" : "CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and Context-Dependent Word Representations",
      "author" : [ "Martin Pömsl", "Roman Lyapin." ],
      "venue" : "Proceedings of the 14th International Workshop on Semantic Evaluation, Barcelona, Spain. Association",
      "citeRegEx" : "Pömsl and Lyapin.,? 2020",
      "shortCiteRegEx" : "Pömsl and Lyapin.",
      "year" : 2020
    }, {
      "title" : "UWB @ DIACR-Ita: Lexical Semantic Change Detection with CCA and Orthogonal Transformation",
      "author" : [ "Ondřej Pražák", "Pavel Přibáň", "Stephen Taylor." ],
      "venue" : "Proceedings of the 7th evaluation campaign of Natural Language Processing and Speech tools for",
      "citeRegEx" : "Pražák et al\\.,? 2020",
      "shortCiteRegEx" : "Pražák et al\\.",
      "year" : 2020
    }, {
      "title" : "Rusemshift: a dataset of historical lexical semantic change in russian",
      "author" : [ "Julia Rodina", "Andrey Kutuzov." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020). Association for Computational Linguistics.",
      "citeRegEx" : "Rodina and Kutuzov.,? 2020",
      "shortCiteRegEx" : "Rodina and Kutuzov.",
      "year" : 2020
    }, {
      "title" : "Deep neural models of semantic shift",
      "author" : [ "Alex Rosenfeld", "Katrin Erk." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 474–484, New",
      "citeRegEx" : "Rosenfeld and Erk.,? 2018",
      "shortCiteRegEx" : "Rosenfeld and Erk.",
      "year" : 2018
    }, {
      "title" : "Introduction to Modern Information Retrieval",
      "author" : [ "Gerard Salton", "Michael J McGill." ],
      "venue" : "McGraw-Hill Book Company, New York.",
      "citeRegEx" : "Salton and McGill.,? 1983",
      "shortCiteRegEx" : "Salton and McGill.",
      "year" : 1983
    }, {
      "title" : "A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains",
      "author" : [ "Dominik Schlechtweg", "Anna Hätty", "Marco del Tredici", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Schlechtweg et al\\.,? 2019",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection",
      "author" : [ "Dominik Schlechtweg", "Barbara McGillivray", "Simon Hengchen", "Haim Dubossarsky", "Nina Tahmasebi." ],
      "venue" : "Proceedings of the 14th International Workshop on Semantic Eval-",
      "citeRegEx" : "Schlechtweg et al\\.,? 2020",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2020
    }, {
      "title" : "Diachronic Usage Relatedness (DURel): A framework for the annotation",
      "author" : [ "Dominik Schlechtweg", "Sabine Schulte im Walde", "Stefanie Eckmann" ],
      "venue" : null,
      "citeRegEx" : "Schlechtweg et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2018
    }, {
      "title" : "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages",
      "author" : [ "Dominik Schlechtweg", "Nina Tahmasebi", "Simon Hengchen", "Haim Dubossarsky", "Barbara McGillivray" ],
      "venue" : null,
      "citeRegEx" : "Schlechtweg et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2021
    }, {
      "title" : "Simulating Lexical Semantic Change from Sense-Annotated Data",
      "author" : [ "Dominik Schlechtweg", "Sabine Schulte im Walde." ],
      "venue" : "The Evolution of Language: Proceedings of the 13th International Conference (EvoLang13).",
      "citeRegEx" : "Schlechtweg and Walde.,? 2020",
      "shortCiteRegEx" : "Schlechtweg and Walde.",
      "year" : 2020
    }, {
      "title" : "Automatic word sense discrimination",
      "author" : [ "Hinrich Schütze." ],
      "venue" : "Computational Linguistics, 24(1):97–123.",
      "citeRegEx" : "Schütze.,? 1998",
      "shortCiteRegEx" : "Schütze.",
      "year" : 1998
    }, {
      "title" : "Room to Glo: A systematic comparison of semantic change detection approaches with word embeddings",
      "author" : [ "Philippa Shoemark", "Farhana Ferdousi Liza", "Dong Nguyen", "Scott Hale", "Barbara McGillivray." ],
      "venue" : "Proceedings of the 2019 Conference on",
      "citeRegEx" : "Shoemark et al\\.,? 2019",
      "shortCiteRegEx" : "Shoemark et al\\.",
      "year" : 2019
    }, {
      "title" : "Survey of Computational Approaches to Diachronic Conceptual Change",
      "author" : [ "Nina Tahmasebi", "Lars Borin", "Adam Jatowt." ],
      "venue" : "arXiv e-prints.",
      "citeRegEx" : "Tahmasebi et al\\.,? 2018",
      "shortCiteRegEx" : "Tahmasebi et al\\.",
      "year" : 2018
    }, {
      "title" : "Finding individual word sense changes and their delay in appearance",
      "author" : [ "Nina Tahmasebi", "Thomas Risse." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 741–749, Varna, Bulgaria.",
      "citeRegEx" : "Tahmasebi and Risse.,? 2017",
      "shortCiteRegEx" : "Tahmasebi and Risse.",
      "year" : 2017
    }, {
      "title" : "Analyzing semantic change in Japanese loanwords",
      "author" : [ "Hiroya Takamura", "Ryo Nagata", "Yoshifumi Kawasaki." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long",
      "citeRegEx" : "Takamura et al\\.,? 2017",
      "shortCiteRegEx" : "Takamura et al\\.",
      "year" : 2017
    }, {
      "title" : "Mining the UK web archive for semantic change detection",
      "author" : [ "Adam Tsakalidis", "Marya Bazzi", "Mihai Cucuringu", "Pierpaolo Basile", "Barbara McGillivray." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Process-",
      "citeRegEx" : "Tsakalidis et al\\.,? 2019",
      "shortCiteRegEx" : "Tsakalidis et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequential modelling of the evolution of word representations for semantic change detection",
      "author" : [ "Adam Tsakalidis", "Maria Liakata." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8485–8497,",
      "citeRegEx" : "Tsakalidis and Liakata.,? 2020",
      "shortCiteRegEx" : "Tsakalidis and Liakata.",
      "year" : 2020
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Peter D. Turney", "Patrick Pantel." ],
      "venue" : "J. Artif. Int. Res., 37(1):141–188.",
      "citeRegEx" : "Turney and Pantel.,? 2010",
      "shortCiteRegEx" : "Turney and Pantel.",
      "year" : 2010
    }, {
      "title" : "A Additional plots Please find additional plots of Word Usage Graphs in Figures 3–6",
      "author" : [ "de.wiktionary.org" ],
      "venue" : null,
      "citeRegEx" : "de.wiktionary.org.,? \\Q2021\\E",
      "shortCiteRegEx" : "de.wiktionary.org.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "mantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al.",
      "startOffset" : 47,
      "endOffset" : 116
    }, {
      "referenceID" : 49,
      "context" : "mantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al.",
      "startOffset" : 47,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "mantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al.",
      "startOffset" : 47,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : ", 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al.",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : ", 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al.",
      "startOffset" : 83,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : ", 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al.",
      "startOffset" : 61,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : ", 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al.",
      "startOffset" : 61,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al.",
      "startOffset" : 81,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al.",
      "startOffset" : 81,
      "endOffset" : 124
    }, {
      "referenceID" : 48,
      "context" : ", 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al.",
      "startOffset" : 71,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "But with recent advances in evaluation (Basile et al., 2020; Schlechtweg et al., 2020; Kutuzov and Pivovarova, 2021), the field now owns standard corpora and tuning data for different lan-",
      "startOffset" : 39,
      "endOffset" : 116
    }, {
      "referenceID" : 43,
      "context" : "But with recent advances in evaluation (Basile et al., 2020; Schlechtweg et al., 2020; Kutuzov and Pivovarova, 2021), the field now owns standard corpora and tuning data for different lan-",
      "startOffset" : 39,
      "endOffset" : 116
    }, {
      "referenceID" : 29,
      "context" : "But with recent advances in evaluation (Basile et al., 2020; Schlechtweg et al., 2020; Kutuzov and Pivovarova, 2021), the field now owns standard corpora and tuning data for different lan-",
      "startOffset" : 39,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "Such discoveries may be useful in a range of fields (Hengchen et al., 2019; Jatowt et al., 2021), among which historical semantics and lexicography represent obvious choices (Ljubešić, 2020).",
      "startOffset" : 52,
      "endOffset" : 96
    }, {
      "referenceID" : 20,
      "context" : "Such discoveries may be useful in a range of fields (Hengchen et al., 2019; Jatowt et al., 2021), among which historical semantics and lexicography represent obvious choices (Ljubešić, 2020).",
      "startOffset" : 52,
      "endOffset" : 96
    }, {
      "referenceID" : 43,
      "context" : "In this paper, we tune the most successful models from SemEval-2020 Task 1 (Schlechtweg et al., 2020) on the German task data set in order to obtain high-quality discovery predictions for novel semantic changes.",
      "startOffset" : 75,
      "endOffset" : 101
    }, {
      "referenceID" : 54,
      "context" : "These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Schütze, 1998) approaches.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 47,
      "context" : "These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Schütze, 1998) approaches.",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 43,
      "context" : "In contrast to previous work, we obtain model predictions by fine-tuning static and contextualized embeddings on high-quality data sets (Schlechtweg et al., 2020) that were not available before.",
      "startOffset" : 136,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "These are controlled for word frequency to minimize model biases that may lead to artificially high performance (Dubossarsky et al., 2017; Schlechtweg and Schulte im Walde, 2020).",
      "startOffset" : 112,
      "endOffset" : 178
    }, {
      "referenceID" : 32,
      "context" : "the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 43,
      "context" : "Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020).",
      "startOffset" : 214,
      "endOffset" : 240
    }, {
      "referenceID" : 43,
      "context" : "Motivated by its wide usage and high performance among participants in SemEval-2020 (Schlechtweg et al., 2020) and DIACR-Ita (Basile et al.",
      "startOffset" : 84,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : ", 2020) and DIACR-Ita (Basile et al., 2020), we use the Skip-gram with Negative Sampling",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton",
      "startOffset" : 78,
      "endOffset" : 126
    }, {
      "referenceID" : 42,
      "context" : "Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton",
      "startOffset" : 78,
      "endOffset" : 126
    }, {
      "referenceID" : 41,
      "context" : "The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983).",
      "startOffset" : 102,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "tion ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; Pömsl and Lyapin, 2020; Pražák et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "tion ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; Pömsl and Lyapin, 2020; Pražák et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 133
    }, {
      "referenceID" : 37,
      "context" : "tion ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; Pömsl and Lyapin, 2020; Pražák et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 133
    }, {
      "referenceID" : 38,
      "context" : "tion ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; Pömsl and Lyapin, 2020; Pražák et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 133
    }, {
      "referenceID" : 44,
      "context" : "and measure their mutual distances (Schlechtweg et al., 2018; Kutuzov and Giulianelli, 2020).",
      "startOffset" : 35,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "and measure their mutual distances (Schlechtweg et al., 2018; Kutuzov and Giulianelli, 2020).",
      "startOffset" : 35,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "Also, possibilities to scale up BERT performance can be applied (Montariol et al., 2021).",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "ing (Bansal et al., 2004; Schlechtweg et al., 2020) (see Figure 1, left) and split into two subgraphs representing nodes from subcorpora C1 and C2, respectively (middle and right).",
      "startOffset" : 4,
      "endOffset" : 51
    }, {
      "referenceID" : 43,
      "context" : "ing (Bansal et al., 2004; Schlechtweg et al., 2020) (see Figure 1, left) and split into two subgraphs representing nodes from subcorpora C1 and C2, respectively (middle and right).",
      "startOffset" : 4,
      "endOffset" : 51
    }, {
      "referenceID" : 43,
      "context" : "SGNS is commonly used (Schlechtweg et al., 2020) and also highly optimized (Kaiser et al.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 30,
      "context" : "This reproduces previous results and confirms that (off-the-shelf) BERT generalises poorly for LSCD and does not transfer well between data sets (Laicher et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 167
    } ],
    "year" : 2021,
    "abstractText" : "While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery.",
    "creator" : "LaTeX with hyperref"
  }
}