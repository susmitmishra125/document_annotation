{
  "name" : "2021.acl-long.554.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ReadOnce Transformers: Reusable Representations of Text for Transformers",
    "authors" : [ "Shih-Ting Lin", "Ashish Sabharwal", "Tushar Khot" ],
    "emails" : [ "j0717lin@utexas.edu,", "ashishs@allenai.org", "tushark@allenai.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7129–7141\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7129"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer-based large scale language models (LMs) (Radford et al., 2018; Devlin et al., 2019) are task-independent models that are surprisingly effective when directly fine-tuned on many different end-tasks (Rajpurkar et al., 2016; Wang et al., 2019b,a). However, this approach relies heavily on using end-task supervision to learn to solve two sub-problems simultaneously: extract information1 from an input document D and solve the end-task (e.g., answer a question aboutD). This incentivizes LM-based models to learn to extract only taskspecific—and even example-specific—information when fine-tuned on the end-task. For example, a Question Answering (QA) model may learn to only extract the answer from D given the input question.\n∗The author’s work was primarily done during an internship at the Allen Institute for AI.\n1By “extract information”, we mean implicitly or explicitly compute some representation of the document.\nThis strategy, while effective on many datasets, is also inefficient. First, it requires model’s pretrained weights to be fine-tuned separately for each end-task, even though the sub-problem of gathering the information content of the input document D is shared across tasks. Second, each D must be re-read from scratch in the context of each example (e.g., once for each question) even when many examples share D. Not only is this computational redundancy undesirable, slow inference can quickly become a bottleneck in deployed, real-time systems if models with billions of parameters must re-read D for every input query.\nInspired by humans’ ability to read a document and extract key information from it without having to know the use case in advance, we ask the following question: Can we use transformer-based LMs to build compressed representations of text that are example- and task-independent, and hence reusable? Further, can we extend text-to-text transformer architectures to consume such representa-\ntions in conjunction with text?\nPrior representation learning approaches attempt to capture the meaning of sentences into a continuous vector (Conneau et al., 2017; Kiros et al., 2015; Reimers and Gurevych, 2019). While they have been effective on downstream classification tasks, it is unclear whether they can capture the information content of entire paragraphs. Moreover, these approaches focus on building fixed-length representations that are used as the input features for task-specific classifiers. In contrast, our goal is to (a) use transformer-based LMs to build compressed representations that scale with the document size, and (b) combine them with example-specific text inputs to produce the more general text output.\nTo this end, we propose an approach to convert any encoder-decoder based transformer LM (such as BART (Lewis et al., 2020)) into a new architecture termed READONCE Transformer, with two key parts: (1) a Document Encoder that reads documents only once to create compressed, informationcapturing, reusable representations that we refer to as READONCE Representations (2) a Representation+Text Model that consumes these document representations together with task- and examplespecific plain text (e.g., a question) to produce text output (e.g. an answer). To ensure that our compressed representations capture the key facts, we use supervision from two factoid QA datasets, SQuAD (Rajpurkar et al., 2016) and UnsupervisedQA (Lewis et al., 2019) to train READONCE Transformers. To solve an end-task, we only need to compute the READONCE Representations of the documents once and only train the Representation+Text Model to perform the end-task.\nOur experiments demonstrate that these representations are more effective at capturing information compared to baseline approaches. Our representations also generalize to other tasks such as multihop QA (Yang et al., 2018), abstractive QA (Kociský et al., 2018), and summarization (Narayan et al., 2018). Since READONCE Representations are computed only once, we can train and infer with models 2x-5x faster than standard approaches, with only a marginal drop in accuracy (about 3 F1 points on QA and 4 Rouge-L points on summarization for a 2x speedup). Moreover, the compression ratio parameter K of our representations provides an easy way to trade off computation time with accuracy. Specifically, our analysis suggests that the resulting model has a computation cost of roughly\n1/2R + 3/4K2 of the base LM, where R is the frequency of document reuse.\nAdditionally, our compressed representation enables us to efficiently combine information from long (or multiple) documents enabling more accurate long-document summarization (Cohan et al., 2018) without needing costly pre-training of new LMs (Beltagy et al., 2020; Zaheer et al., 2020)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Representation learning approaches are commonly used to extract fixed-length sentence embeddings (Conneau et al., 2017; Kiros et al., 2015; Wang et al., 2020) from variable-length text inputs. Such fixed length representations have enabled the development of simpler downstream models that do not have to deal with the variable-lengths of textual inputs. However, these representations have mainly been used for simple classification tasks on short input texts (Bowman et al., 2015; Wang et al., 2019b). The word-level representations from RNNs or transformers are also variable-length, but uncompressed. While such representations have been re-used with RNNs (Peters et al., 2018) and are easy to combine with text input, it is not immediately clear how to combine representations from transformers with text, which is what we propose.\nRecent work (Reimers and Gurevych, 2019; He et al., 2020; Artetxe and Schwenk, 2019; Karpukhin et al., 2020) has tried building document-embedding using large-scale language models as well. However these fixed-length representations have mostly been built to identify similar documents (Reimers and Gurevych, 2019; Karpukhin et al., 2020) and are not used directly for QA. QuASE (He et al., 2020), also used questionanswering supervision for transfer learning but do not produce re-usable representations. Artetxe and Schwenk (2019) learned multi-lingual sentence embeddings that may be able to capture the knowledge present in a sentence but they were designed for BiLSTMs. Some large-scale LMs have been especially designed to handle long documents (Yang et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) too but need to be pre-trained on large corpora, whereas we can use any pre-trained LM.\nAspects of our work also bears resemblance to domain adaptation (Daume III and Marcu, 2006), transfer learning (Pan and Yang, 2010) and multi-task learning (Caruana, 1993) but focuses on learning information-capturing represen-\ntations from transformer-based models that has not been explored by prior work. While model distillation (Hinton et al., 2015) can also result in speedups, these techniques are orthogonal and can be easily incorporated in our framework (as we show in our experiments)."
    }, {
      "heading" : "3 READONCE Transformers",
      "text" : "Our goal in this work is to identify the optimal architecture to extract information-capturing reusable representations. At the same time, we also need to find the optimal architecture to use such representation in conjunction with text inputs. So at a high level (as shown in Fig. 1), we need to develop two systems: (1) A model to compute the representation, Document Encoder and (2) A general model for tasks that can consume vector representations and text, Representation+Text Model. Given the recent success and generality of encoder-decoder models (Radford et al., 2018; Raffel et al., 2020; Lewis et al., 2020), we focus on developing models for such an architecture. We present the potential choices for each model, with the final model used in our system indicated by a *."
    }, {
      "heading" : "3.1 Document Encoder",
      "text" : "Given an encoder-decoder model, there are different ways to compute representations for a document d with tokens {t1, . . . , tn}. We focus on using the output representation generated by the encoder, represented with hi for each token ti.\nFixed Length Aggregation. The most common approach is to extract a single representation from a sequence of vector (Kiros et al., 2015; Conneau et al., 2017). While this can be a very compact representation of a document, it tends to be very lossy, especially when dealing with large documents. As a result, these representations are mainly used for classification (Conneau et al., 2017; Reimers and Gurevych, 2019) or retrieval (Karpukhin et al., 2020), and have not been shown to capture the content of the document. E.g, InferSent (Conneau et al., 2017) presented a self-attentive approach to extract sentence embedding using:\nr = ∑ i Uθ(hi)hi (1)\nwhere Uθ is a function that computes a scalar attention over each hi. To reduce information loss, we extend these models to produce M representation vectors by learning M sets of parameters θj for\nj ∈ {1, . . . ,M}, i.e., rj = ∑\ni Uθj (hi)hi where Uθj (hi) = e θjhi/ ∑ i e θjhi .\nSpecial Token Representations. With the advent of transformer models, another common approach is adding a special [CLS] (Radford et al., 2018; Devlin et al., 2019) or <s> (Liu et al., 2019) token to the context. The output representation of this special token can then be used as inputs to classifiers and other down-stream models. Again, a single representation can be lossy, so we generate M representations by inserting multiple special tokens. We can dynamically adjust the number of special tokens based on the input length to produce a variable-length representation. To achieve a compression-ratio of 1k , we insert N k special tokens and use their representations. We consider two ways2 of inserting special tokens into the context: (1) Suffix: Add them at the end of the context3 (2) Interleave: Add them after every k tokens. While the first approach preserves context continuity, the latter might more directly incentivize the model to capture local context.\nSliding Window Aggregation*. We apply the idea of aggregating single-vector representations to generate a variable-length representation. We apply an aggregation function F over sliding windows of size W tokens to capture the local context of the window (akin to CNNs). For a stride length of S, this would result in representation vectors:\nrj = F ({hS·j , · · · , hS·j+W }) (2) where F ∈ {µ, α, ω} corresponds to mean-pooling, linear weighting (as described in Eqn. (1)), and max-pooling, respectively.\nFigure 2 shows how we would compute these representations using a window-size of W=2 with no overlap (i.e. S=2) and the linear weighting function. The resulting READONCE Representations would have M = N/2 vectors where N is the number of tokens in the input.\nSentenceBERT Baseline. For completeness, we also use an existing transformer-based SentenceBert model (Reimers and Gurevych, 2019)4 to compute the representation of each sentence in the document. Since the space of these representation might\n2More complex designs such as special token embeddings, position embeddings, and indicator features are left as future work.\n3Prefixing special tokens generally worsened performance. 4We use the BERT-Large NLI tokens which performed better than the NLI-STSB representations in our experiments\nbe different, we learn a single-layer feedforward network to project the representations into the right space. For fair comparison to models with variable compression ratio k, we also use SentenceBERT representations for a sliding window of k tokens."
    }, {
      "heading" : "3.2 Representation+Text Model",
      "text" : "Next, we present our modification to downstream task models to use both text and our generated READONCE Representations. Since most NLP tasks can be re-formulated as a text-to-text problem (Radford et al., 2018; Raffel et al., 2020), we focus on extending text-to-text encoder-decoder models to a (vec+text)-to-text model.\nAppend to Encoder*. Since the transformer block in an encoder can handle any input length in each layer, one possible approach is to append the representations to the Lth layer of the encoder. This allows the model to focus on parsing the input example text(e.g., question) in the L-1 layers followed by focusing on answering the question in the remaining layers. We show this model in Figure 3 where the encoder only processes the Q tokens of the question for the first L layers. Once the M READONCE Representations are added to the Lth layer, all the subsequent layers produce M + Q vectors by attending over both the representations and text. Finally an unmodified decoder produces the output answer.\nModify Transformer Block Attention. Rather than just modifying the input, we consider an alternate approach of modifying the transformer block itself. Similar to PlotMachines (Rashkin et al., 2020), we view the representation as a memory that the self-attention block can attend over (in addition\nto the input text). We modify the self-attention blocks in both the encoder and the decoder5 to use two separate attention modules for both of these input types and averages the vectors.6 With this design, ideally the Representation+Text Model will gain extra capacity to model the interaction between the representation and the input text."
    }, {
      "heading" : "3.3 Training READONCE via QA",
      "text" : "Given the overall architecture of such a system (shown in Fig. 4), we next focus on training this model to produce READONCE Representations that capture the information present in the document. While prior representation learning models have often focused on classification tasks, we instead use the reading comprehension QA task to ensure this information-capturing property. If a model is able to use just the READONCE Representations to answer the questions grounded in the document, the representations would contain the information needed to answer such questions.\nThe key question here is: Which QA datasets are most suitable for training a compact yet information-capturing document representation?\nLow-level semantic QA datasets (Michael et al., 2018; He et al., 2015) don’t allow for any compression as the questions require the knowledge about every word in the input sentence. More complex multi-hop QA datasets such as HotpotQA (Yang et al., 2018) are also not appropriate, as they focus on learning to reason in addition to capturing the information. Shallow reading comprehension tasks provide a sweet spot between these two extremes, as extracting key information from the given document is sufficient to answer the questions. Further, unlike semantic QA tasks, the questions only focus on the key facts mentioned in a document, which can be captured in a compressed representation. We\n5Only modifying the encoder or decoder resulted in slightly lower performance.\n6See App. A.1 for the detailed formulas.\nas Document Encoder to compute the READONCE Representations. We append these representations to theLth layer of the encoder in our Representation+Text Model (Fig. 3). This end-to-end model is fine-tuned on QA tasks to train the Document Encoder to extract information-capturing representations.\nuse two such datasets to train our models: SQuAD and Unsupervised QA."
    }, {
      "heading" : "3.4 Downstream Usage of READONCE",
      "text" : "To verify the generality of the READONCE Representations, we train models to perform multi-hop reasoning, abstractive QA and summarization using our learned representations. Specifically, we freeze the Document Encoder model and use it to generate the representations for documents. We further fine-tune the Representation+Text Model on the downstream task to produce the output label given the READONCE Representations and any example-specific input."
    }, {
      "heading" : "4 Representation Learning Experiments",
      "text" : "We first evaluate the different potential architectural choices for extracting and using document representations discussed in §3.1 and §3.2, respectively. While our main interest is in learning effective representations, we also need to find the optimal Representation+Text Model architecture that can consume the representation."
    }, {
      "heading" : "4.1 Training Setup",
      "text" : "We train the entire model on the factoid QA task to ensure that the document representations do capture factual knowledge. We primarily use the SQuAD reading-comprehension dataset (Rajpurkar et al., 2016) containing more than 100,000 crowdsourced factoid questions. We further augment this dataset with about 500,000 rule-based questions from the UnsupervisedQA (UQA) dataset (Lewis\net al., 2019). This increases the size of the training dataset while also introducing question diversity. To avoid these automatically generated questions overwhelming training, we ensure that the same number of questions are selected from both the datasets in each batch (by duplicating SQuAD questions). In the same vein, we evaluate each model based on their performance on the SQuAD task.7\nUnless otherwise mentioned, we use the BARTLarge model in all our experiments, and optimize the model with cross-entropy loss. We set the learning rate to 1e-5 for the weights initialized from the BART model, and to 1e-4 for randomly initialized newly added weights, which is shown beneficial in Peters et al. (2019). For other hyper-parameters, we follow Lewis et al. (2020). We ran all the experiments on RTX 8000 with 48GB GPU memory. All experiments did not use the complete GPU memory, e.g. experim We kept the batch size and gradient accumulation steps constant (both at 8) across different compression ratios."
    }, {
      "heading" : "4.2 Architecture Evaluation",
      "text" : "To be able to evaluate the representations, we need to first select the architecture of the model consuming these representations."
    }, {
      "heading" : "4.2.1 Representation+Text Model",
      "text" : "We explore the different choices for the Representation+Text Model model discussed in §3.2, assuming the representation is generated by a simple Document Encoder model: Mean aggregation over a Sliding Window with both window size and stride being 8 tokens. The results are shown in Table 1.\nWe see that appending READONCE representations too early (L=1) or too late (L=12) in the encoder stack is not as effective as appending about half-way (L=6).8 We suspect that appending too\n7The scores on UQA correlate well with the scores on SQuAD, with close to 90 F1 for most models.\n8We also experimented with L=3 and L=9, and didn’t find any significant gains.\nearly does not allow the model to focus on understanding the question, whereas appending too late does not leave enough room for cross-attention between the question and representations.\nModifying the transformer block to attend over these representations results in a reasonable F1 score on SQuAD, but it is still outperformed by our simple Append architecture. Hence, for the rest of this work, we stick to the simpler architecture of appending the representation at the 6th layer, denoted Append(L=6)."
    }, {
      "heading" : "4.2.2 Document Encoder",
      "text" : "Given the Representation+Text Model model architecture chosen above, we now explore potential Document Encoder architectures to extract READONCE Representations. For a fair comparison, we ensure that all our evaluated representations use, on average across a dataset, the same number of vectors to represent documents. Table 2 presents EM and F1 scores on SQuAD for the various architectural choices discussed in §3.1.\nThe top 3 rows explore the sliding window architecture with both window size and stride length of 8 (i.e., no overlap between windows), with the three different aggregation functions mentioned earlier. We see that both the mean µ and the learned weighted sum α have comparable performance on this task, and outperform the max-pooling function ω. We also evaluate the impact of increasing the overlap between windows by increasing the window size (not changing the stride length keeps the average number of vectors constant). For the\nlearned weighted sum function, this results in a 5 point F1 drop, possibly due to the aggregation function having to operate over a larger window.9\nWe next evaluate the approaches inspired by prior work where we add special tokens and use the representations of these tokens. For the BART model, we use a newly added [CLS] token as our special token. We see from Table 2 that neither appending these tokens at the end nor interleaving them in the input results in representations comparable to the sliding window based approaches.10 The sliding window representations outperform the pre-trained sentence-based representations from SentenceBERT irrespective of the number of vectors used.11 Finally, if we fix the representation length to 21 vectors (computed based on the average token length of SQuAD: 163.7), the learned representations are still not as effective."
    }, {
      "heading" : "4.3 Final READONCE Architecture",
      "text" : "Based on this set of experiments, we use the sliding window architecture for the Document Encoder with learned weighted sum as the aggregation function, and append these representations to the 6th layer in the final task-dependent Representation+Text Model."
    }, {
      "heading" : "5 Downstream Task Experiments",
      "text" : "Next, we evaluate the quality of our representations by using them on three downstream tasks, different from the tasks READONCE Transformers are trained on, demonstrating faster training and inference. We then show the benefit of using our representation when documents are much longer than the token limit of the underlying LM."
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Tasks: We consider three end-tasks, extractive QA, summarization, and abstractive QA, to evaluate our system using the following datasets: (1) HotpotQA (Yang et al., 2018), a multi-hop reasoning extractive QA dataset. (2) XSUM (Narayan et al., 2018), an abstractive News summarization dataset (3) NarrativeQA (Kociský et al., 2018), an abstractive QA dataset where answers are not spans\n9We also compared W=8, S=2 with W=2, S=2 in our early experiments and notice a similar trend—the smaller sliding window performs better.\n10Special token prefix scored similar to the Suffix model. 11Even when the SlidingWindow approach is limited to M = N/32 vectors, it achieves a higher F1 score (52.4) than SentenceBERT.\nfrom the input document. More details about these datasets and metrics provided in App. B\nBaselines: We compare READONCE Transformers to BART-based QA models that use the document text directly to answer the given question. Since these models use text directly without any lossy compression, their score is best viewed as an upper bound for any representation-based BART model, including ours. We train the BART model to generate the answer given the entire document and question (we use “Summary” as question for XSUM). In addition to BART-Large, we evaluate two smaller models: BART-Base and DistilBART (Shleifer and Rush, 2020). Since our representations were trained on SQuAD and UQA, we also first fine-tune all our BART models on the same datasets.\nREADONCE Models: We freeze the parameters of the Document Encoder to generate the representations for all the documents in the datasets. We then use these representations with our Representation+Text Model, which is further fine-tuned on each end-task. To evaluate the impact of our pretraining on QA datasets, we compare our model to the READONCE architecture initialized with the BART model weights, READONCEφ. To illustrate the architecture-independence of our approach and orthogonality to traditional compression methods, we also train and evaluate READONCE models using the BART-Base and DistilBART models. These models were also first trained on SQuAD +UQA datasets to learn the document representation. See App. C for more details.\nSince our Representation+Text Model can handle a variable number of representation vectors, we can change this compression ratio, on-the-fly, without having to change the model architecture. Specifically, we can use a stride-length of K in our Document Encoder to generate representations that are 1/Kth of the input length, and then feed them to a downstream model. By reducing K, we can reduce the compression ratio and improve the model accuracy, at the cost of increased runtime.\nInterestingly, we discovered that we don’t even need to re-train Document Encoder for each value ofK. We can achieve a performance comparable to encoders trained individually for each value of K, by using the Document Encoder trained on K = 8 and only varying K during the fine-tuning step."
    }, {
      "heading" : "5.2 Representation Quality",
      "text" : "First, we assess the ability of READONCE Representations to capture document information as compared to using the original document text. As shown in Table 3, our framework at K=2 is about 2x faster than BART-Large while being only 3 F1 and 4 Rouge-L points behind this model with full access to the text. This demonstrates that READONCE Representations do capture most of the relevant information in the document. The different compressed models can also result in smaller (DistilBART) or comparable (BART-Base) speed-ups, but (1) our accuracy vs speed trade-off is more easily controllable via K and (2) we can apply our framework on these models to achieve similar speedups.12\nLastly, we note that the READONCEφ system, which simply uses the BART model parameters, is about 6 F1 and 14 Rouge-L points behind our model with learned representations. This shows that our model does utilize the factoid questions to learn to extract meaningful representations — without our training, the representations obtained from the pre-trained models are not as effective.13\n12While more recent LMs can outperform BART (e.g. Pegasus (Zhang et al., 2020) for summarization), we believe similar tradeoffs can be achieved by applying our framework on these newer models.\n13We also observe drops in score when using the BART model parameters in only the Document Encoder or only the Representation+Text Model."
    }, {
      "heading" : "5.3 Model Efficiency",
      "text" : "One key advantage of READONCE Representations is that the model needs to read the document only once, and can reuse pre-computed representations for multiple examples or even multiple tasks. Specifically, if a document is repeated across R examples (the replication factor) and we use a compression ratio of K, our computation cost per question is roughly only (1/2R+ 3/4K2) relative to a baseline seq2seq model (cf. App. C.3 for an analysis). In other words, the higher the replication factor R or the compression ratio K, the higher the speedup achieved via READONCE Representations.\nOur model exhibits a speedup of 2x-5x in training time compared to the different BART architectures (Figure 5). Similarly, we observe a 2x-3x speedup in the inference time (as shown in Figure 6), which again plateaus out at K=8.\nNote that the time reported for our model includes the cost of reading READONCE Representations from disk as well as some fixed costs. These costs form a larger fraction of the overall time for faster models. Hence, while our speedups do not exactly match up to the theoretical analysis, the empirical trends are as expected: we see larger speedups on the NarrativeQA dataset which has a higher replication factor R. In general, the R value for our datasets (e.g., R=29.7 for NarrativeQA) is within the range of other datasets (e.g., R=9.4 for NewsQA and R=13.9 for DROP). Note that even when R=1 (e.g., XSUM), we observe a speedup due to the compression ratio K."
    }, {
      "heading" : "5.4 Efficiency-Accuracy Tradeoff",
      "text" : "We also perform a more fine-grained analysis of the efficiency-accuracy tradeoff in READONCE by\nvarying the values of the compression ratio K. As shown in Figure 7, across all three of our datasets, as the value of K increases, the model’s accuracy goes down due to increased compression but so does the training time. As compared to the upperbound BART-Large model, we see a large gain in speed when K=2 with diminishing gains as K reaches 8."
    }, {
      "heading" : "5.5 Handling Long Documents",
      "text" : "Compressing document representations also enables the downstream model to reason over documents longer than its maximum token length limit T. For example, we can compute representations of document chunks with upto T tokens each and concatenate them together. Since these representations do not rely on any position embeddings in Representation+Text Model, theoretically we can use as many representation vectors as needed.\nGiven GPU memory limits, lets assume we can only accommodate documents upto length T. Given\na compression ratio K, we can compute READONCE Representations for K such length-T chunks, increasing the capacity of our downstream model to T*K.14 For simplicity, we ignore the question as it tends to be much shorter than T.\nTo assess the impact of increased model capacity, we evaluate our learned representations on the long document summarization task PubMed (Cohan et al., 2018).15 We follow Cohan et al. (2018) and only include the first 4 sections from each document (average length=2270 tokens). We vary the memory budget from T=512 to T=256 and compare our approach to two BART seq2seq baselines: a simple truncation baseline with T/4 tokens from each section, and a sliding-window baseline often used in QA models for summarization extended here by concatenating summaries from length-T chunks of the input document. For the READONCE Transformer with a compression ratio of K, we can accommodate K*T/4 tokens per section, resulting in a total of T representations from the 4 sections. We choose to obtain these T representations using K/2 chunks from each section, with each chunk containing T/2 tokens.16\nROUGE-L scores of these models are depicted in Figure 8. As we reduce T for the underlying transformer model from 512 to 256, the score of the baseline BART model drops to 35.5 ROUGE-L. When used with the sliding window technique, the performance is even worse, likely due to the naive aggregation of the summaries. Our approach, on\n14If we allow overlap of O tokens between chunks, the capacity changes to T*K – O*(K–1)\n15We also evaluate NarrativeQA, see App. C.2 16See Appendix C.1 for more details.\nthe other hand, concatenates document representations, allowing the downstream model to build a coherent summary. We see the ROUGE-L score only drops to 36.6 when K=2 (with model capacity dropping from 1024 to 512 tokens) and a much smaller drop from 37.0 to 36.5 when K=8 (with model capacity dropping from 3520 to 1472 tokens). This simulation shows that concatenating READONCE Representations is a simple yet effective way to increase the capacity of existing models."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This work introduced READONCE Transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations. Akin to humans’ ability to read a document and extract useful information without knowing the enduse, READONCE Representations are compact, information-capturing document representations that can be pre-computed once, in a task- and example-independent fashion.\nOur results on extractive QA, summarization, and abstractive QA tasks demonstrate that using READONCE Representations, in lieu of re-reading document text in the context of every example, results in substantially faster training and inference, at a modest cost in accuracy. The READONCE framework also offers an easy way to control the trade off between speed and accuracy (via the compression ratio parameter), and enables the use of standard transformer architectures on long documents beyond the model’s token limit.\nIdentifying the ideal compact document representations in our controlled setting opens up the possibility of efficient open-domain QA, where models retrieve and reason directly over these representations. We leave an exploration of the training of the retrieval function, often with only answer supervision and ideally in an end-to-end setting, to future work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Dirk Groeneveld for providing the output of the Quark system for HotpotQA and the Beaker team for their support with the experiments."
    }, {
      "heading" : "A Model Details",
      "text" : "A.1 The Detailed Architecture of Modified Transformer Block Attention\nIn this variation of Representation+Text Model, the modified self-attention block uses two separate attention modules for both of these input types and averages the vectors. Specifically, let HLenc be the matrix of hidden states generated from the Lth layer of a standard transformer:\nHLenc = Attn(H L−1 enc ,H L−1 enc ,H L−1 enc ) (3)\nwhere Attn(Q,K,V) is the attention module used in the transformer that takes Q,K,V as the query, key, and value matrices. To take extra READONCE Representations as an input, we instead compute HLenc as:\nHLenc =(Attn(H L−1 enc ,H L−1 enc ,H L−1 enc )+\nAttn ′ (HL−1enc ,R,R))/2\n(4)\nwhere Attn ′ (·) is a separate attention module to include the READONCE Representations R in our Representation+Text Model, whose weights are initialized by the corresponding weights in Attn(·) to speed up the training. For the decoder in the Representation+Text Model, we also compute the hidden states of each layer as per Eqn. 4 so that the model can attend over the extracted document information during the decoding process too."
    }, {
      "heading" : "B Dataset Details",
      "text" : "(1) HotpotQA (Yang et al., 2018) is a multi-hop reasoning dataset that requires models to aggregate information from two paragraphs to produce the answer (a span from the input paragraphs). We focus on their distractor setting where they additionally provide models with 8 distractor paragraphs. For efficiency, we use the output of the Quark system (Groeneveld et al., 2020) which selects up to 512 tokens (including the question) from the input paragraphs. We use the answer EM and F1 scores as the metrics.\n(2) XSUM (Narayan et al., 2018) is an abstractive News summarization dataset that requires models to generate summaries that go beyond simply extracting key sentences. We use the Rouge-L Summ. score17 commonly used for summarization datasets, which computes the union-LCS of the longest common subsequences (LCS) between each pair of reference and hypothesis sentences. In contrast, the\n17https://github.com/google-research/ google-research/tree/master/rouge\nstandard Rouge-L score computes LCS between the reference and hypothesis, treating both of them as one sentence.\n(3) NarrativeQA (Kociský et al., 2018) is an abstractive QA dataset where answers may not be extractive spans in the input document. Models would need to understand the content of the document to generate such answers. We use the same Rouge-L Summ. score as for the summarization task.18\n(4) PubMed (Cohan et al., 2018) is an abstractive long-document summarization dataset specifically focusing on the scientific publications. The large number of tokens in each document makes it hard for standard Pretrained Transformers to deal with. We use the same Rouge-L Summ. score as for XSUM."
    }, {
      "heading" : "C Experiment Setting for DistilBART",
      "text" : "We follow Shleifer and Rush (2020) to obtain our DistilBART model used in §5. Specifically, we first create a student model with 12-layer encoder and 6- layer decoder from BART-LargeSQuAD +UQA using the “Shrink and Fine-Tune” distillation described in Shleifer and Rush (2020), which has been shown to be effective for BART model on summarization tasks. We then further finetune the student model on SQuAD+UQA, and exploit the resulting model as our DistilBART.\nC.1 Setup for the Pubmed Dataset in the Long-document Experiment\nWe follow Cohan et al. (2018) and only include 4 sections from document. After the truncation, the average number of tokens in the documents in this dataset is 2270, with 90% of the documents being under 4253 tokens. To include the information from each section, we evenly distribute the length budget T across the sections. This would mean, for the baseline BART seq2seq model, each section is first truncated to T/4 tokens, then the 4 sections are concatenated as the input. As for READONCE Transformers, we first compute representations for K/2 chunks of each section with length T/2 tokens, then aggregate them as the final READONCE Representations for the input document. In this case, even when K equals 2, we are allowed to include one chunk from each section without exceeding the\n18In our experiments, we did not notice any substantial difference between the simple Rouge-L metric and this summarization-based metric.\nlength limit. For the BART + SlidingWindow baseline, we concatenate summaries from 16 chunks of length T with 4 chunks from each section.\nC.2 Handling Long Documents: Narr.QA Aside from Pubmed, we also evaluate the ability of READONCE Transformers to handle long documents on the NarrativeQA dataset. The average number of tokens in the documents in this dataset is 668.6, with 90% of the documents being under 981 tokens. The results are depicted in Figure 9.\nAs we reduce T for the underlying transformer model from 384 to 128, the score of the baseline BART model drops significantly from 59.1 to 38.8. With K=2, our model consistently outperforms the baseline model but exhibits a similar drop in score as the maximum token limit of this model with T=128 is still only 208 tokens (as per the equation above). On the other hand, with K=8 and T=128, we can handle documents up to 688 tokens, thereby requiring no truncation on 50% of the examples even in this extreme scenario. As a result, we only see a 5.2 point drop in score as T is decreased from 384 to 128. These simulations provide strong evidence of the ability of READONCE Transformers to handle long documents more effectively than standard transformer based models.\nC.3 Read-Once Efficiency Gains Let C denote context length (as #tokens), Q the question length, R the repetition factor (i.e., #questions per context), and K the READONCE compression factor. For an input of N tokens, we treat the computational cost of the encoder (forward or\nbackward pass) as TeN2 and that of the decoder as TdN\n2, where Te, Td capture the complexity of the encoder and decoder model respectively and N2 captures the self-attention over the input context. We ignore the self-attention over the decoded text as it is unchanged across models.\nFor any baseline seq2seq model, the computational cost of processing an example (context+question) can be captured by (Te + Td)(C + Q)2. For READONCE Transformers, the cost of computing a context’s representation, amortized over the R questions that share this context, is Te C2\nR . Once the compressed context representation is appended to the question encoder at layer L (out of 12), the rest of the encoder computation costs Te · ( L 12Q 2 + ( 1− L12 ) ( C K +Q )2) . The de-\ncoder’s computation cost is Td · ( C K +Q )2 .\nWhen L = 6 and Q CK , the net computational cost (without caching) simplifies to TeC2\nR + TeC2 2K2 + TdC 2 K2 . Assuming Te ≈ Td = T , this equals TC2 ( 1 R + 3 2K2 ) . In contrast, the baseline model’s cost simplifies to 2TC2. The efficiency gain of READONCE Transformers over the baseline encoder-decoder model is therefore roughly 1 2 ( 1/R+ 3/2K2 ) .\nAdditionally, when we use these representations for multiple training runs, inferences, downstream tasks, etc., the cost of computing the fixed representations is basically amortized to a constant term. As a result, over multiple runs, using READONCE Representations now reduces the cost of the building and using models to just TC2 3\n2K2 . So using\nthese cached representations amortized over multiple epochs/runs, improves the efficiency gains further to 3/4K2."
    } ],
    "references" : [ {
      "title" : "Massively multilingual sentence embeddings for zero-shot crosslingual transfer and beyond",
      "author" : [ "M. Artetxe", "Holger Schwenk." ],
      "venue" : "TACL, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Multitask learning: A knowledgebased source of inductive bias",
      "author" : [ "R. Caruana." ],
      "venue" : "ICML.",
      "citeRegEx" : "Caruana.,? 1993",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1993
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "W. Chang", "Nazli Goharian." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : null,
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Domain adaptation for statistical classifiers",
      "author" : [ "Hal Daume III", "Daniel Marcu." ],
      "venue" : "Journal of artificial Intelligence research, 26:101–126.",
      "citeRegEx" : "III and Marcu.,? 2006",
      "shortCiteRegEx" : "III and Marcu.",
      "year" : 2006
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple yet strong pipeline for HotpotQA",
      "author" : [ "Dirk Groeneveld", "Tushar Khot", "Ashish Sabharwal" ],
      "venue" : null,
      "citeRegEx" : "Groeneveld et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Groeneveld et al\\.",
      "year" : 2020
    }, {
      "title" : "Quase: Question-answer driven sentence encoding",
      "author" : [ "Hangfeng He", "Qiang Ning", "Dan Roth." ],
      "venue" : "ACL.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
      "author" : [ "Luheng He", "M. Lewis", "Luke Zettlemoyer." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NeurIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Dense passage retrieval for opendomain question answering",
      "author" : [ "V. Karpukhin", "Barlas Oğuz", "Sewon Min", "Patrick Lewis", "Ledell Yu Wu", "Sergey Edunov", "Danqi Chen", "Wen tau Yih." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Russ R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "The NarrativeQA reading comprehension challenge",
      "author" : [ "Tomás Kociský", "Jonathan Schwarz", "P. Blunsom", "Chris Dyer", "K. Hermann", "Gábor Melis", "Edward Grefenstette." ],
      "venue" : "TACL, 6:317–328.",
      "citeRegEx" : "Kociský et al\\.,? 2018",
      "shortCiteRegEx" : "Kociský et al\\.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised question answering by cloze translation",
      "author" : [ "Patrick Lewis", "Ludovic Denoyer", "Sebastian Riedel." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lewis et al\\.,? 2019",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Crowdsourcing question-answer meaning representations",
      "author" : [ "Julian Michael", "Gabriel Stanovsky", "Luheng He", "I. Dagan", "Luke Zettlemoyer." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Michael et al\\.,? 2018",
      "shortCiteRegEx" : "Michael et al\\.",
      "year" : 2018
    }, {
      "title" : "Don’t give me the details, just the summary! Topicaware convolutional neural networks for extreme summarization",
      "author" : [ "S. Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 22:1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowledge enhanced contextual word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Robert L Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving language understanding with unsupervised learning",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "Technical report, OpenAI.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "JMLR, 21(140):1–67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Plotmachines: Outline-conditioned generation with dynamic plot state tracking",
      "author" : [ "Hannah Rashkin", "A. Çelikyilmaz", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rashkin et al\\.,? 2020",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2020
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Pre-trained summarization distillation",
      "author" : [ "Sam Shleifer", "Alexander M. Rush." ],
      "venue" : "ArXiv, abs/2010.13002.",
      "citeRegEx" : "Shleifer and Rush.,? 2020",
      "shortCiteRegEx" : "Shleifer and Rush.",
      "year" : 2020
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A Multi-task Benchmark and Analysis Platform for Natural Language Understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-thought for sentence encoder pre-training",
      "author" : [ "Shuohang Wang", "Yuwei Fang", "Siqi Sun", "Zhe Gan", "Yu Cheng", "Jingjing Liu", "Jing Jiang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "ICML.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "2018) is an abstractive QA dataset where answers may not be extractive spans in the input document. Models would need to understand the content of the document",
      "author" : [ "NarrativeQA (Kociský" ],
      "venue" : null,
      "citeRegEx" : ".Kociský,? \\Q2018\\E",
      "shortCiteRegEx" : ".Kociský",
      "year" : 2018
    }, {
      "title" : "2018) and only include 4 sections from document. After the truncation, the average number of tokens in the documents in this dataset is 2270, with 90% of the documents being under 4253",
      "author" : [ "Cohan" ],
      "venue" : null,
      "citeRegEx" : "Cohan,? \\Q2018\\E",
      "shortCiteRegEx" : "Cohan",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Transformer-based large scale language models (LMs) (Radford et al., 2018; Devlin et al., 2019) are task-independent models that are surprisingly effective when directly fine-tuned on many different end-tasks (Rajpurkar et al.",
      "startOffset" : 52,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Transformer-based large scale language models (LMs) (Radford et al., 2018; Devlin et al., 2019) are task-independent models that are surprisingly effective when directly fine-tuned on many different end-tasks (Rajpurkar et al.",
      "startOffset" : 52,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Prior representation learning approaches attempt to capture the meaning of sentences into a continuous vector (Conneau et al., 2017; Kiros et al., 2015; Reimers and Gurevych, 2019).",
      "startOffset" : 110,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "Prior representation learning approaches attempt to capture the meaning of sentences into a continuous vector (Conneau et al., 2017; Kiros et al., 2015; Reimers and Gurevych, 2019).",
      "startOffset" : 110,
      "endOffset" : 180
    }, {
      "referenceID" : 27,
      "context" : "Prior representation learning approaches attempt to capture the meaning of sentences into a continuous vector (Conneau et al., 2017; Kiros et al., 2015; Reimers and Gurevych, 2019).",
      "startOffset" : 110,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : "To this end, we propose an approach to convert any encoder-decoder based transformer LM (such as BART (Lewis et al., 2020)) into a new architecture termed READONCE Transformer, with two key parts: (1) a Document Encoder that reads documents only once to create compressed, informationcapturing, reusable representations that we refer to as READONCE Representations (2) a Representation+Text Model that consumes these document representations together with task- and examplespecific plain text (e.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "To ensure that our compressed representations capture the key facts, we use supervision from two factoid QA datasets, SQuAD (Rajpurkar et al., 2016) and UnsupervisedQA (Lewis et al.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : ", 2016) and UnsupervisedQA (Lewis et al., 2019) to train READONCE Transformers.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 33,
      "context" : "Our representations also generalize to other tasks such as multihop QA (Yang et al., 2018), abstractive QA (Kociský et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : ", 2018), abstractive QA (Kociský et al., 2018), and summarization (Narayan et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Additionally, our compressed representation enables us to efficiently combine information from long (or multiple) documents enabling more accurate long-document summarization (Cohan et al., 2018) without needing costly pre-training of new LMs (Beltagy et al.",
      "startOffset" : 175,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : ", 2018) without needing costly pre-training of new LMs (Beltagy et al., 2020; Zaheer et al., 2020).",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Representation learning approaches are commonly used to extract fixed-length sentence embeddings (Conneau et al., 2017; Kiros et al., 2015; Wang et al., 2020) from variable-length text inputs.",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "Representation learning approaches are commonly used to extract fixed-length sentence embeddings (Conneau et al., 2017; Kiros et al., 2015; Wang et al., 2020) from variable-length text inputs.",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 31,
      "context" : "Representation learning approaches are commonly used to extract fixed-length sentence embeddings (Conneau et al., 2017; Kiros et al., 2015; Wang et al., 2020) from variable-length text inputs.",
      "startOffset" : 97,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "However, these representations have mainly been used for simple classification tasks on short input texts (Bowman et al., 2015; Wang et al., 2019b).",
      "startOffset" : 106,
      "endOffset" : 147
    }, {
      "referenceID" : 30,
      "context" : "However, these representations have mainly been used for simple classification tasks on short input texts (Bowman et al., 2015; Wang et al., 2019b).",
      "startOffset" : 106,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "While such representations have been re-used with RNNs (Peters et al., 2018) and are easy to combine with text input, it is not immediately clear how to combine representations from transformers with text, which is what we propose.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Recent work (Reimers and Gurevych, 2019; He et al., 2020; Artetxe and Schwenk, 2019; Karpukhin et al., 2020) has tried building document-embedding using large-scale language models as well.",
      "startOffset" : 12,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "Recent work (Reimers and Gurevych, 2019; He et al., 2020; Artetxe and Schwenk, 2019; Karpukhin et al., 2020) has tried building document-embedding using large-scale language models as well.",
      "startOffset" : 12,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Recent work (Reimers and Gurevych, 2019; He et al., 2020; Artetxe and Schwenk, 2019; Karpukhin et al., 2020) has tried building document-embedding using large-scale language models as well.",
      "startOffset" : 12,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "Recent work (Reimers and Gurevych, 2019; He et al., 2020; Artetxe and Schwenk, 2019; Karpukhin et al., 2020) has tried building document-embedding using large-scale language models as well.",
      "startOffset" : 12,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "However these fixed-length representations have mostly been built to identify similar documents (Reimers and Gurevych, 2019; Karpukhin et al., 2020) and are not used directly for QA.",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "However these fixed-length representations have mostly been built to identify similar documents (Reimers and Gurevych, 2019; Karpukhin et al., 2020) and are not used directly for QA.",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "QuASE (He et al., 2020), also used questionanswering supervision for transfer learning but do not produce re-usable representations.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 32,
      "context" : "Some large-scale LMs have been especially designed to handle long documents (Yang et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) too but need to be pre-trained on large corpora, whereas we can use any pre-trained LM.",
      "startOffset" : 76,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "Some large-scale LMs have been especially designed to handle long documents (Yang et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) too but need to be pre-trained on large corpora, whereas we can use any pre-trained LM.",
      "startOffset" : 76,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "Aspects of our work also bears resemblance to domain adaptation (Daume III and Marcu, 2006), transfer learning (Pan and Yang, 2010) and multi-task learning (Caruana, 1993) but focuses on learning information-capturing represen-",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Aspects of our work also bears resemblance to domain adaptation (Daume III and Marcu, 2006), transfer learning (Pan and Yang, 2010) and multi-task learning (Caruana, 1993) but focuses on learning information-capturing represen-",
      "startOffset" : 156,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "While model distillation (Hinton et al., 2015) can also result in speedups, these techniques are orthogonal and can be easily incorporated in our framework (as we show in our experiments).",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "Given the recent success and generality of encoder-decoder models (Radford et al., 2018; Raffel et al., 2020; Lewis et al., 2020), we focus on developing models for such an architecture.",
      "startOffset" : 66,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Given the recent success and generality of encoder-decoder models (Radford et al., 2018; Raffel et al., 2020; Lewis et al., 2020), we focus on developing models for such an architecture.",
      "startOffset" : 66,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Given the recent success and generality of encoder-decoder models (Radford et al., 2018; Raffel et al., 2020; Lewis et al., 2020), we focus on developing models for such an architecture.",
      "startOffset" : 66,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "The most common approach is to extract a single representation from a sequence of vector (Kiros et al., 2015; Conneau et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "The most common approach is to extract a single representation from a sequence of vector (Kiros et al., 2015; Conneau et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "As a result, these representations are mainly used for classification (Conneau et al., 2017; Reimers and Gurevych, 2019) or retrieval (Karpukhin et al.",
      "startOffset" : 70,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "As a result, these representations are mainly used for classification (Conneau et al., 2017; Reimers and Gurevych, 2019) or retrieval (Karpukhin et al.",
      "startOffset" : 70,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : ", 2017; Reimers and Gurevych, 2019) or retrieval (Karpukhin et al., 2020), and have not been shown to capture the content of the document.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "g, InferSent (Conneau et al., 2017) presented a self-attentive approach to extract sentence embedding using:",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "With the advent of transformer models, another common approach is adding a special [CLS] (Radford et al., 2018; Devlin et al., 2019) or <s> (Liu et al.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "With the advent of transformer models, another common approach is adding a special [CLS] (Radford et al., 2018; Devlin et al., 2019) or <s> (Liu et al.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : ", 2019) or <s> (Liu et al., 2019) token to the context.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 27,
      "context" : "For completeness, we also use an existing transformer-based SentenceBert model (Reimers and Gurevych, 2019)4 to compute the representation of each sentence in the document.",
      "startOffset" : 79,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "tasks can be re-formulated as a text-to-text problem (Radford et al., 2018; Raffel et al., 2020), we focus on extending text-to-text encoder-decoder models to a (vec+text)-to-text model.",
      "startOffset" : 53,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "tasks can be re-formulated as a text-to-text problem (Radford et al., 2018; Raffel et al., 2020), we focus on extending text-to-text encoder-decoder models to a (vec+text)-to-text model.",
      "startOffset" : 53,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "Similar to PlotMachines (Rashkin et al., 2020), we view the representation as a memory that the self-attention block can attend over (in addition La ye r L",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "Low-level semantic QA datasets (Michael et al., 2018; He et al., 2015) don’t allow for any compression as the questions require the knowledge about every word in the input sentence.",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Low-level semantic QA datasets (Michael et al., 2018; He et al., 2015) don’t allow for any compression as the questions require the knowledge about every word in the input sentence.",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 33,
      "context" : "More complex multi-hop QA datasets such as HotpotQA (Yang et al., 2018) are also not appropriate, as they focus on learning to reason in addition to capturing the information.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "We primarily use the SQuAD reading-comprehension dataset (Rajpurkar et al., 2016) containing more than 100,000 crowdsourced factoid questions.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "We further augment this dataset with about 500,000 rule-based questions from the UnsupervisedQA (UQA) dataset (Lewis et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "Tasks: We consider three end-tasks, extractive QA, summarization, and abstractive QA, to evaluate our system using the following datasets: (1) HotpotQA (Yang et al., 2018), a multi-hop reasoning extractive QA dataset.",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "(2) XSUM (Narayan et al., 2018), an abstractive News summarization dataset (3) NarrativeQA (Kociský et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : ", 2018), an abstractive News summarization dataset (3) NarrativeQA (Kociský et al., 2018), an abstractive QA dataset where answers are not spans",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 28,
      "context" : "In addition to BART-Large, we evaluate two smaller models: BART-Base and DistilBART (Shleifer and Rush, 2020).",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 34,
      "context" : "Pegasus (Zhang et al., 2020) for summarization), we believe similar tradeoffs can be achieved by applying our framework on these newer models.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "To assess the impact of increased model capacity, we evaluate our learned representations on the long document summarization task PubMed (Cohan et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 157
    } ],
    "year" : 2021,
    "abstractText" : "We present READONCE Transformers, an approach to convert a transformer-based model into one that can build an informationcapturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multihop QA, abstractive QA, and long-document summarization. Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.",
    "creator" : "LaTeX with hyperref"
  }
}