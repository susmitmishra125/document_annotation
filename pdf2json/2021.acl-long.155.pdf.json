{
  "name" : "2021.acl-long.155.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
    "authors" : [ "Lihua Qian", "Hao Zhou", "Yu Bao", "Mingxuan Wang", "Lin Qiu", "Weinan Zhang", "Yong Yu", "Lei Li" ],
    "emails" : [ "qianlihua@apex.sjtu.edu.cn", "lqiu@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "yyu@apex.sjtu.edu.cn", "zhouhao.nlp@bytedance.com", "wangmingxuan.89@bytedance.com", "lileilab@bytedance.com", "baoy@smail.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1993–2003\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1993"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer has been the most widely used architecture for machine translation (Vaswani et al., 2017). Despite its strong performance, the decoding of Transformer is inefficient as it adopts the sequential auto-regressive factorization for its probability model (Figure 1a). Recent work such as the non-autoregressive transformer (NAT), aims to decode target tokens in parallel to speed up the generation (Gu et al., 2018). However, the vanilla NAT still lags behind the Transformer in translation quality – with a gap of about 7.0 BLEU points. NAT assumes the conditional independence of the target tokens given the source sentence. We suspect that NAT’s conditional independence assumption prevents learning word interdependency in the target\n∗The work was done when the first author was an intern at Bytedance.\n̂y1\n̂y2\n̂y4 ̂y5\n̂y3\nh1\nh2 h3 h4 h5\nNAT DecodingH H′ Glancing Sampling\nHamming Distance N( ̂Y, Y ) = 3\ny1\ny2 y3 y4 y5\n̂y1\n̂y2 ̂y4 ̂y5 ̂y3\ny1\ny3\ny5\nReplace Inputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\na tt\nen tio\nn\nR an\ndo m\nM\nas ki\nng\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\na tt\nen tio\nn\nan apple in the car\nein Apfel im Auto\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nG la\nnc in g Sa m pl in g\ny3y1 y5\nein Apfel im Auto\nan apple in the car\na tt\nen tio\nn\nein Apfel im Auto ein Apfel im Auto\napple in apple the\nan the car\nan apple in the\nan in car a tt\nen tio\nn\n(a) Sequential LM\n̂y1\n̂y2\n̂y4 ̂y5\n̂y3\nh1\nh2 h3 h4 h5\nNAT DecodingH H′ Glancing Sampling\nHamming Distance N( ̂Y, Y ) = 3\ny1\ny2 y3 y4 y5\n̂y1\n̂y2 ̂y4 ̂y5 ̂y3\ny1\ny3\ny5\nReplace Inputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 4h1 3 5 a tt en\ntio n\nR an\ndo m\nM\nas ki\nng Decoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\na tt\nen tio\nn\nan apple in the car\nein Apfel im Auto\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y54y3\ny2 y4\nG la\nnc in g Sa m pl in g\ny3y1 y5\nein Apfel im Auto\nan apple in the car\na tt\nen tio\nn\nein Apfel im Auto ein Apfel im Auto\napple in apple the\nan the car\nan apple in the\nan in car a tt\nen tio\nn\n(b) Cond. Independent LM\n̂\n̂\n̂ 5̂\n̂\n5\n′ li\ni i t ( ,̂ )\n5\n̂\n̂\n̂ 5̂\n̂\n1\n3\n5\neplace Inputs\n.\n.\n.\n.\n.\n2\n4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\na tt\nen tio\nn\nR an\ndo m\nM\nas ki\nng\nec er\n[ AS ] [ AS ]\ny2 y3y1 y5y4\na tt\nen tio\nn\nan apple in the car\nein pfel i uto\nec er\n2 4\n2 y4\nG la\nnc in g Sa m pl in g\n31 y5\nein pfel i uto\nan apple in the car\na tt\nen tio\nn\ni f l i i t\napple in apple the\nt r\nan apple in the\ni r a tt\nen tio n (c) Masked LM (MLM)\n̂y1\n̂y2\n̂y4 ̂y5\n̂y3\nh1\nh2 h3 h4 h5\nNAT DecodingH H′ Glancing Sampling\nHamming Distance N( ̂Y, Y ) = 3\ny1\ny2 y3 y4 y5\n̂y1\n̂y2 ̂y4 ̂y5 ̂y3\ny1\ny3\ny5\nReplace Inputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5 a tt en\ntio n\nR an\ndo m\nM\nas ki\nng Decoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\na tt\nen tio\nn\nan apple in the car\nein Apfel im Auto\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nG la\nnc in g Sa m pl in g\ny3y1 y5\nein Apfel im Auto\nan apple in the car\na tt\nen tio\nn\nein Apfel im Auto ein Apfel im Auto\napple in apple the\nan the car\nan apple in the\nan in car a tt\nen tio\nn\n(d) Glancing LM (GLM)\nFigure 1: Probabilistic models for machine translation methods. (b) Vanilla NAT uses conditional indepedent LM. (c) Mask-Predict NAT uses MLM and requires multiple passes of decoding. (d) Our proposed GLM leverages the decoder prediction to decide g ancing sampling policy during training and only requires one pass of decoding during inference.\nsentence. Notice that such word interdependency is crucial, as the Transformer explicitly captures that via decoding from left to right (Figure 1a).\nSeveral remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding. Their common idea is to decode the target tokens iteratively while each pass of decoding is trained using the masked language model (Figure 1c). Since these methods require multiple passes of decoding, its generation speed is measurably slower than the vanilla NAT. With single-pass generation only, these methods still largely lag behind the autoregressive Transformer.\nOne open question is whether a complete parallel decoding model can achieve comparable machine translation performance to the Transformer. It should be non-autoregressive and take only one pass of decoding during the inference time.\nTo address the quest, we propose glancing language model (GLM), a new method to train a probabilistic sequence model. Based on GLM, we develop the glancing Transformer (GLAT) for neural machine translation. It achieves parallel text generation with only single decoding. Yet, it outperforms previous NAT methods and achieves comparable performance as the strong Transformer baseline in multiple cases. Intuitively, GLM adopts a adaptive glancing sampling strategy, which glances at some fragments of the reference if the reference is too difficult to fit in the training of GLAT. Correspondingly, when the model is well tuned, it will adaptively reduce the percentage of glancing sampling, making sure that the resulting model could learn to generate the whole sentence in the single-pass fashion. The gradual learning process smooths the learning curve of single-pass parallel generation.\nSpecifically, our proposed GLM differs from MLM in two aspects. Firstly, GLM proposes an adaptive glancing sampling strategy, which enables GLAT to generate sentences in a one-iteration way, working by gradual training instead of iterative inference (see Figure 1d). Generally, GLM is quite similar to curriculum learning (Bengio et al., 2009) in spirit, namely first learning to generate some fragments and gradually moving to learn the whole sentences (from easy to hard). To achieve the adaptive glancing sampling, GLM performs decoding twice in training. The first decoding is the same as the vanilla NAT, and the prediction accuracy indicates whether the current reference is “difficult” for fitting. In the second decoding, GLM gets words of the reference via glancing sampling according to the first decoding, and learn to predict the remaining words that are not sampled. Note that only the second decoding will update the model parameters. Secondly, instead of using the [MASK] token, GLM directly uses representations from the encoder at corresponding positions, which is more natural and could enhance the interactions between sampled words and signals from the encoder.\nNote that GLAT does not modify the network architecture, which is a training method to explicityly learn word interdependency. Experimental results show that GLAT obtains significant improvements\n(about 5 BLEU) on standard benchmarks compared to the vanilla NAT, without losing inference speedup. GLAT achieves competitive results against iterative approaches like Mask-Predict (Ghazvininejad et al., 2019), even outperforming the Mask-Predict model on WMT14 DE-EN and WMT16 RO-EN. Compared to the strong AT baseline, GLAT can still close the performance gap within 0.9 BLEU point while keeping 7.9× speed-up. Empirically, we even find that GLAT outperforms AT when the length of the reference is less than 20 on WMT14 DE-EN. We speculate this is because GLM could capture bidirectional context for generation while its left-to-right counterpart is only unidirectional, which indicates the potential of parallel generation approaches like GLAT."
    }, {
      "heading" : "2 Probability Models of Machine Translation",
      "text" : "We state and compare different probability models for machine translation. A machine translation task can be formally defined as a sequence to sequence generation problem: given the source sentence X = {x1, x2, ..., xN}, to generate the target sentence Y = {y1, y2, ..., yT } according to the conditional probability P (Y |X; θ), where θ denotes the parameter set of a network. Different methods factorize the conditional probability differently.\nThe Transformer uses the autoregressive factorization to maximize the following likelihood: LAT = logP (Y |X; θ) = T∑ t=1 log p(yt|y<t, X; θ),\nwhere y<t = {[BOS], y1, ..., yt−1}. For simplicity, we omit the number of samples in the equation. Note the training of AT adopts left-to-right teacher forcing on the target tokens (Vaswani et al., 2017). The word interdependency is learned in a unidirectional way. During inference, the preceding predicted token is fed into the decoder to generate the next token.\nThe vanilla NAT consists of the same encoder as the Transformer and a parallel decoder with layers of multi-head attention (Gu et al., 2018). During training, it uses the conditional independent factorization for the target sentence:\nLNAT = T∑ t=1 logP (yt|X; θ).\nNAT DecodingH H′ Glancing Sampling y1 y3\ny5\nh2 h4 Hamming Distance N( ̂Y, Y ) = 3 Replace Inputs y1 y2 y3 y4 y5 ̂y1 ̂y2 ̂y4 ̂y5 ̂y3 0.8 0.5 0.7 0.6 0.9\n̂y1 ̂y2\n̂y4 ̂y5\n̂y3 h1 h2 h3 h4 h5\nNotice that, NAT’s log-likelihood is an approximation to the full log-likelihood logP (Y |X; θ). During inference, the encoder representation is copied as the input to the decoder, therefore all tokens on the target side can be generated in parallel. Such a conditional independence assumption does not hold in general, which explains the inferior performance of NAT.\nMulti-pass iterative decoding approaches such as Mask-Predict (Ghazvininejad et al., 2019) extends the vanilla NAT. It still uses the conditional independent factorization, together with the random masking scheme: LMLM = ∑\nyt∈RM(Y )\nlog p ( yt|Φ ( Y,RM(Y ) ) , X; θ ) ,\nwhere RM(Y ) is a set of randomly selected words from Y , and Φ(·) replaces these selected words in Y with the [MASK] token. For example in Figure 1c, RM(Y ) = {y2, y3}, Φ ( Y,RM(Y ) ) = {y1,[MASK],[MASK], y4, y5}. The number of masked tokens distributes uniformly from 1 to the total number of tokens in the target sequence. Such training objective is used to learn a refinement model θ that can predict the masked tokens given the source sentence X and words generated in the previous iteration.\nThe vanilla NAT breaks word interdependency, while MLM requires multiple passes of decoding to re-establish the word interdependency. Our goal in this work is to design a better probability model and a training objective to enable word interdependency learning for single-pass parallel generation."
    }, {
      "heading" : "3 Glancing Transformer",
      "text" : "In this section, we present GLAT in detail. GLAT uses the same encoder-decoder architecture as the vanilla NAT (Gu et al., 2018). GLAT differs from the vanilla NAT in that it explicitly encourages word interdependency via training with glancing language model (GLM). It differs from the iterative NAT with MLM in that it is trained to produce single pass parallel decoding while MLM is used for prediction refinement."
    }, {
      "heading" : "3.1 The Glancing Language Model",
      "text" : "Given the input source sentence X = {x1, x2, ..., xN}, the task is to predict Y = {y1, y2, ..., yT }. The glancing Transformer (GLAT) formulates a glancing language model (GLM) during training. It maximizes the following:\nLGLM = ∑\nyt∈GS(Y,Ŷ )\nlog p(yt|GS(Y, Ŷ ), X; θ)\n(1) Where, Ŷ is the initial predicted tokens, and GS(Y, Ŷ ) is a subset of tokens selected via the glancing sampling strategy (Figure 2, described in detail in the next section). The glancing sampling strategy selects those words from the target sentence by comparing the initial prediction against the ground-truth tokens. It selects more tokens and feeds the embeddings of these tokens into the decoder input if the network’s initial prediction is less accurate. GS(Y, Ŷ ) is the remaining subset of tokens within the target Y but not selected. The training loss above is calculated against these remaining tokens.\nGLAT adopts similar encoder-decoder architecture as the Transformer with some modification (Figure 1d). Its encoder fencis the same multihead attention layers. Its decoder fdec include multiple layers of multi-head attention where each layer attends to the full sequence of both encoder representation and the previous layer of decoder representation.\nDuring the initial prediction, the input to the decoder H = {h1, h2, ..., hT } are copied from the encoder output using either uniform copy or soft copy (Wei et al., 2019). The initial tokens Ŷ are predicted using argmax decoding with fdec(fenc(X; θ), H; θ).\nTo calculate the loss LGLM, we compare the initial prediction Ŷ against the ground-truth to select tokens within the target sentence, i.e. GS(Y, Ŷ ). We then replace those sampled indices of h’s with corresponding target word embeddings, H ′ = RP(Embyt∈GS(Y,Ŷ )(yt), H), where RP replaces the corresponding indices. Namely, if a token in the target is sampled, its word embedding replaces the corresponding h. Here the word embeddings are obtained from the softmax embedding matrix of the decoder. The updated H ′ is then fed into the decoder fdec again to calculate the output token probability. Specifically, the output probabilities of remaining tokens p(yt|GS(Y, Ŷ ), X; θ) are computed with fdec(H ′, fenc(X; θ); θ)."
    }, {
      "heading" : "3.2 The Glancing Sampling Strategy",
      "text" : "One important component of GLM is to adaptively select the positions of tokens from the target sentence. Those selected tokens provide “correct” information from the ground-truth target, therefore it helps training the decoder to predict the rest nonselected tokens. Intuitively, our adaptive sampling strategy guides the model to first learn the generation of fragments and then gradually turn to the whole sentences. Our glancing sampling strategy selects many words at the start of the training, when the model is not yet well tuned. As the model gets better progressively, the sampling strategy will sample fewer words to enable the model to learn the parallel generation of the whole sentence. Note that the sampling strategy is crucial in the training of GLAT.\nAs illustrated in Figure 2, the glancing sampling could be divided into two steps: first deciding a sampling number S, and then randomly selecting S words from the reference. The sampling number\nS will be larger when the model is poorly trained and decreases along the training process. Note that we choose to randomly select the S words from the reference. The random reference word selection is simple and yields good performance empirically.\nFormally, given the input X , its predicted sentence Ŷ and its reference Y , the goal of glancing sampling function GS(Y, Ŷ ) is to obtain a subset of words sampled from Y :\nGS(Y, Ŷ ) = Random(Y, S(Y, Ŷ )) (2)\nHere, Random(Y, S) is randomly selecting S tokens from Y , and S is computed by comparing the difference between Ŷ and Y , S(Y, Ŷ ) = λ · d(Y, Ŷ ). The sampling ratio λ is a hyper-parameter to more flexibly control the number of sampled tokens. d(Y, Ŷ ) is a metric for measuring the differences between Y and Ŷ . We adopt the Hamming distance (Hamming, 1950) as the metric, which is computed as d(Y, Ŷ ) = ∑T t=1(yt 6= ŷt). With d(Y, Ŷ ), the sampling number can be decided adaptively considering the current trained model’s prediction capability. For situations that Y and Ŷ have different lengths, d(Y, Ŷ ) could be other distances such as Levenshtein distance (Levenshtein, 1966).\nAlternative glancing sampling strategy can be adopted as well. For example, one simple alternative strategy is to set the number of sampled tokens to be proportional to the target sentence length, i.e. S = λ ∗ T . We will evaluate the effects of these variations in the experiment."
    }, {
      "heading" : "3.3 Inference",
      "text" : "GLAT only modifies the training procedure. Its inference is fully parallel with only a single pass. For parallel generation, we need to decide the output lengths before decoding. A simple way to decide the output lengths is predicting length with representations from the encoder.\nIn GLAT, the length prediction is implemented as in Ghazvininejad et al. (2019). An additional [LENGTH] token is added to the source input, and the encoder output for the [LENGTH] token is used to predict the length.\nWe also use two more complex methods to better decide the output lengths: noisy parallel decoding (NPD) and connectionist temporal classification (CTC). For NPD (Gu et al., 2018), we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate. Then we use a pre-trained\ntransformer to rank these sequences and identify the best overall output as the final output. For CTC (Graves et al., 2006), following Libovickỳ and Helcl (2018), we first set the max output length to twice the source input length, and remove the blanks and repeated tokens after generation."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines. Ablation studies and further analysis are also included to verify the effects of different components used in GLAT."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Datasets We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs). These datasets are tokenized and segmented into subword units using BPE encodings (Sennrich et al., 2016). We preprocess WMT14 EN-DE by following the data preprocessing in Vaswani et al. (2017). For WMT16 EN-RO\nand IWSLT16 DE-EN, we use the processed data provided in Lee et al. (2018).\nKnowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets. We employ the transformer with the base setting in Vaswani et al. (2017) as the teacher for knowledge distillation. Then, we train our GLAT on distilled data.\nBaselines and Setup We compare our method with the base Transformer and strong representative NAT baselines in Table 1. For all our tasks, we obtain other NAT models’ performance by directly using the performance figures reported in their papers if they are available.\nWe adopt the vanilla model which copies source input uniformly in Gu et al. (2018) as our base model (NAT-base) and replace the UniformCopy with attention mechanism using positions. Note that the output length does not equal the length of reference in models using CTC. Therefore, for GLAT with CTC, we adopt longest common subsequence distance for compar-\ning Y and Ŷ , and the glancing target is the target alignment that maximize the output probability argmaxa∈B−1(Y ) P (a|X; θ). B−1 is the mapping proposed in (Graves et al., 2006), which expand the reference to the length of output by inserting blanks or repeating words.\nFor WMT datasets, we follow the hyperparameters of the base Transformer in Vaswani et al. (2017). And we choose a smaller setting for IWSLT16, as IWSLT16 is a smaller dataset. For IWSLT16, we use 5 layers for encoder and decoder, and set the model size dmodel to 256. Using Nvidia V100 GPUs, We train the model with batches of 64k/8k tokens for WMT/IWSLT datasets, respectively. We set the dropout rate to 0.1 and use Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.999). For WMT datasets, the learning rate warms up to 5e− 4 in 4k steps and gradually decays according to inverse square root schedule in Vaswani et al. (2017). As for IWSLT16 DE-EN, we adopt linear annealing (from 3e− 4 to 1e− 5) as in Lee et al. (2018). For the hyper-parameter λ, we adopt linear annealing from 0.5 to 0.3 for WMT datasets and a fixed value of 0.5 for IWSLT16. The final model is created by averaging the 5 best checkpoints chosen by validation BLEU scores. We report tokenized BLEU for all the datasets used in experiment. We measure the average latency per sentence on a single Nvidia 1080TI GPU."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "The main results on the benchmarks are presented in Table 1. GLAT significantly improves the translation quality and outperforms strong baselines by a large margin. Our method introduces explicit word interdependency modeling for the decoder and gradually learns simultaneous generation of whole sequences, enabling the model to better capture the underlying data structure. Compared to models\nwith iterative decoding, our method completely maintains the inference efficiency advantage of fully non-autoregressive models, since GLAT generate with a single pass. Compared with the baselines, we highlight our empirical advantages:\n• GLAT is highly effective. Compared with the vanilla NAT-base models, GLAT obtains significant improvements (about 5 BLEU) on ENDE/DE-EN. Additionally, GLAT also outperforms other fully non-autoregressive models with a substantial margin (almost +2 BLEU points on average). The results are even very close to those of the AT model, which shows great potential.\n• GLAT is simple and can be applied to other NAT models flexibly, as we only modify the training process by reference glancing while keeping inference unchanged. For comparison, NAT-DCRF utilizes CRF to generate sequentially; NAT-IR and Mask-Predict models need multiple decoding iterations.\n• CTC and NPD use different approaches to determine the best output length, and they have their own advantages and disadvantages. CTC requires the output length to be longer than the exact target length. With longer output lengths, the training will consume more time and GPU memory. As for NPD, with a certain number of length reranking candidates, the inference speed will be slower than models using CTC. Note that NPD can use pretrained AT models or the non-autoregressive model itself to rerank multiple outputs.\nWe also present a scatter plot in Figure 3, displaying the trend of speed-up and BLEU with different NAT models. It is shown that the point of\nGLAT is located on the top-right of the competing methods. Obviously, GLAT outperforms our competitors in BLEU if speed-up is controlled, and in speed-up if BLEU is controlled. This indicates that GLAT outperforms previous NAT methods. Although iterative models like Mask-Predict achieves competitive BLEU scores, they only maintain minor speed advantages over AT. In contrast, fully non-autoregressive models remarkably improve the inference speed."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "Effect of Source Input Length To analyze the effect of source input length on the models’ performance, we split the source sentences into different intervals by length after BPE and compute the BLEU score for each interval. The histogram of results is presented in Figure 4. NAT-base’s performance drops sharply for long sentences, while the gradual learning process enables GLAT to boost the performance by a large margin, especially for long sentences. We also find that GLAT outperforms autoregressive Transformer when the source input length is smaller than 20.\nGLAT Reduces Repetition We also measure the percentage of repeated tokens on test set of WMT14 EN-DE and WMT14 DE-EN. Table 2 presents the token repetition ratio of sentences generated by NAT-base and GLAT. The results show that GLAT significantly reduces the occurrence of repetition, and the repetition ratio can be further\nreduced with NPD. We think an important cause of the improvement is better interdependency modeling. Since GLAT explicitly encourages word interdependency modeling to better capture the dependency between target tokens, wrong generation patterns, such as repetition, can be largely avoided.\nGLAT Achieves Strong Results without Multiple Iterations We conduct experiments of GLAT with more than one decoding iteration in inference. We adopt the inference algorithm in Mask-Predict for multiple-iteration decoding. The results are shown in Figure 5. We find that GLAT can achieve decent performances with only one decoding iteration, while further iterations only obtain minor improvements of 0.2∼0.3 BLEU."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "Effectiveness of the Adaptive Sampling Number To validate the effectiveness of the adaptive sampling strategy for the sampling number S(Y, Ŷ ), we also introduce two fixed approaches for comparison. The first one decides the sampling number with λ∗T , where T is the length of Y , and λ is a constant ratio. The second one is relatively flexible, which sets a start ratio of λs and an end ratio λe, and linearly reduces the sampling number from λs ∗ T to λe ∗ T along the training process.\nAs shown in Table 3 and Table 4, our adaptive approach (Adaptive in the table) outperforms the baseline models with big margins. The results confirm our intuition that the sampling schedule affects the generation performance of our NAT model. The\nsampling strategy, which first offers relatively easy generation problems and then turns harder, benefits the final performance. Besides, even with the simplest constant ratio, GLAT still achieves remarkable results. When set λ = 0.2, it even outperforms the baseline λ = 0.0 by 2.5 BLEU points.\nThe experiments potentially support that it is beneficial to learn the generation of fragments at the start and gradually transfer to the whole sequence. The flexible decreasing ratio method works better than the constant one, and our proposed adaptive approaches achieve the best results.\nInfluence of Reference Word Selection To analyze how the strategies of selecting reference words affect glancing sampling, we conduct experiments with different selection strategies. By default, we assume all the words in the reference are equally important and randomly choose reference words for glancing. Besides the random strategy, we devise four other selection methods considering the prediction of first decoding. For pref and 1−pref, the sampling probability of each reference word is proportional to the output probability for the reference word pref and the probability 1− pref, respectively. Similar to the word selection strategy for masking words during inference in Mask-Predict, we also add two strategies related to the prediction confidence: \"most certain\" and \"most uncertain.\" We choose the positions where predictions have higher confidence for \"most certain\", and vise versa for \"most uncertain.\" The results for different selection methods are listed in Table 5.\nIn comparisons, the model with the selection\nstrategy 1− pref outperforms the one with pref, indicating that words hard to predict are more important for glancing in training. And we find that the random strategy performs a little better than the two confidence-based strategies. We think this indicates that introducing more randomness in sampling enable GLAT to explore more interdependency among target words. We adopt the random strategy for its simplicity and good performance.\nComparison of Different Distances for Glancing Sampling We conduct experiments with two distances for comparing the predictions of the first decoding and references, and the results are presented in Table 6. Experimental results show that both distances can be used to improve the quality of one-iteration generation, and GLAT with Hamming distance is better than GLAT with Levenshtein distance. Especially when there is no target length reranking, GLAT with Hamming distance outperforms GLAT with Levenshtein distance by about 0.7 BLEU and 0.9 BLEU on WMT14 EN-DE and DE-EN respectively. We think Hamming distance is more strict than Levenshtein distance because only the same words on the corresponding positions are regarded as correct, which is more consistent with the training of GLAT.\nAdvantages of GLAT over Mask-Predict To study the effects of sampling strategy and decoder inputs of GLAT, we conduct experiments for replacing these two modules in GLAT with the corresponding part in Mask-Predict, respectively. The results are presented in Table 7. GLAT employs glancing sampling strategy instead of the uniform sampling strategy used in Mask-Predict, and replaces the [MASK] token inputs with source representations from the encoder. The results show that the glancing sampling strategy outperforms the uniform sampling strategy by 5∼6 BLEU points, and feeding representations from the encoder as the decoder input could still improve the strong baseline by 0.2∼0.3 BLEU points after adopting glancing sampling. To sum up, the adaptive glanc-\ning sampling approach contributes the most to the final improvement, and the use of representations from the encoder also helps a bit."
    }, {
      "heading" : "5 Related Work",
      "text" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021). Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; Li et al., 2019; Guo et al., 2020a; Sun and Yang, 2020). Besides, there are also some work that apply different training objectives to train nonautoregressive models (Libovickỳ and Helcl, 2018; Shao et al., 2020; Ghazvininejad et al., 2020a), add regularization terms (Wang et al., 2019; Guo et al., 2019).\nNon-Autoregressive Models with Structured Decoding To model the dependencies between words, Sun et al. (2019) introduces a CRF inference module in NAT and performs additional sequential decoding after the non-autoregressive computation in inference. Deng and Rush (2020) proposes cascaded CRF decoding. Since GLAT only performs single-pass non-autoregressive generation, our approach is orthogonal to the method proposed in Sun et al. (2019). We can also combine our approach with the structured decoding methods.\nNon-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020). Lee et al. (2018) proposed a method of iterative refinement based on denoising autoencoder. Gu et al. (2019) utilized insertion and deletion to refine the outputs in inference. Ghazvininejad et al. (2019) trained the model with the masked language model, and the model iteratively replaces masked tokens with new outputs. (Li et al., 2020) first predict the left token and right token for each position, and decode the final token at the current position conditioned on the left-and-right tokens predicted before. Despite the relatively better accuracy, the multiple decoding iterations reduce the inference efficiency of non-autoregressive models.\nScheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019). Although scheduled sampling also modifies decoder inputs in training, there are mainly two differences between our work and scheduled sampling. Firstly, scheduled sampling mixes up the predicted sequence and the gold target sequence, and our method does not mix predicted sequences into decoder inputs. Besides, GLAT aims to learn word interdependency for single-pass parallel generation and scheduled sampling is designed for alleviating exposure bias."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel generation models. With the glancing language model, the model starts from learning the generation of sequence fragments and gradually moving to whole sequences. Experimental results show that our approach significantly improves the performance of non-autoregressive machine translation with single-pass parallel generation. As GLAT achieves competitive performance compared with autoregressive models, applying our approach to other generation tasks is a promising direction for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all the anonymous reviewers for their valuable comments. Hao Zhou and Lei Li are corresponding authors."
    } ],
    "references" : [ {
      "title" : "Nonautoregressive translation by learning target categorical codes",
      "author" : [ "Yu Bao", "Shujian Huang", "Tong Xiao", "Dongqi Wang", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Bao et al\\.,? 2021",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2021
    }, {
      "title" : "Non-autoregressive transformer by position learning",
      "author" : [ "Yu Bao", "Hao Zhou", "Jiangtao Feng", "Mingxuan Wang", "Shujian Huang", "Jiajun Chen", "Lei Li." ],
      "venue" : "arXiv preprint arXiv:1911.10677.",
      "citeRegEx" : "Bao et al\\.,? 2019",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2019
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer." ],
      "venue" : "Proceedings of the 28th International Conference",
      "citeRegEx" : "Bengio et al\\.,? 2015",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "ICML, pages 41–48.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Cascaded text generation with markov transformers",
      "author" : [ "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Deng and Rush.,? 2020",
      "shortCiteRegEx" : "Deng and Rush.",
      "year" : 2020
    }, {
      "title" : "Aligned cross entropy for non-autoregressive machine translation",
      "author" : [ "Marjan Ghazvininejad", "Vladimir Karpukhin", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "International Conference on Machine Learning, pages 3515–3523. PMLR.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2020a",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2020
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "EMNLP-IJCNLP, pages 6114–6123.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-autoregressive training improves mask-predict decoding",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2001.08785.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2020b",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2020
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "ICML, pages 369–376.",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Nonautoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Levenshtein transformer",
      "author" : [ "Jiatao Gu", "Changhan Wang", "Junbo Zhao." ],
      "venue" : "NeurIPS, pages 11179– 11189.",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Non-autoregressive neural machine translation with enhanced decoder input",
      "author" : [ "Junliang Guo", "Xu Tan", "Di He", "Tao Qin", "Linli Xu", "Tie-Yan Liu." ],
      "venue" : "AAAI, volume 33, pages 3723–3730.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-tuning by curriculum learning for non-autoregressive neural machine translation",
      "author" : [ "Junliang Guo", "Xu Tan", "Linli Xu", "Tao Qin", "Enhong Chen", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
      "citeRegEx" : "Guo et al\\.,? 2020a",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation",
      "author" : [ "Junliang Guo", "Linli Xu", "Enhong Chen." ],
      "venue" : "ACL, pages 376–385.",
      "citeRegEx" : "Guo et al\\.,? 2020b",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Error detecting and error correcting codes",
      "author" : [ "Richard W Hamming." ],
      "venue" : "The Bell system technical journal, 29(2):147–160.",
      "citeRegEx" : "Hamming.,? 1950",
      "shortCiteRegEx" : "Hamming.",
      "year" : 1950
    }, {
      "title" : "Non-autoregressive machine translation with disentangled context transformer",
      "author" : [ "Jungo Kasai", "James Cross", "Marjan Ghazvininejad", "Jiatao Gu." ],
      "venue" : "International Conference on Machine Learning, pages 5144–5155. PMLR.",
      "citeRegEx" : "Kasai et al\\.,? 2020",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
      "author" : [ "Jason Lee", "Elman Mansimov", "Kyunghyun Cho." ],
      "venue" : "EMNLP, pages 1173–1182.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Binary codes capable of correcting deletions, insertions, and reversals",
      "author" : [ "Vladimir I Levenshtein." ],
      "venue" : "Soviet physics doklady, pages 707–710.",
      "citeRegEx" : "Levenshtein.,? 1966",
      "shortCiteRegEx" : "Levenshtein.",
      "year" : 1966
    }, {
      "title" : "Lava nat: A nonautoregressive translation model with look-around decoding and vocabulary attention",
      "author" : [ "Xiaoya Li", "Yuxian Meng", "Arianna Yuan", "Fei Wu", "Jiwei Li." ],
      "venue" : "arXiv preprint arXiv:2002.03084.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Hint-based training for nonautoregressive translation",
      "author" : [ "Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "EMNLP-IJCNLP.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Endto-end non-autoregressive neural machine translation with connectionist temporal classification",
      "author" : [ "Jindřich Libovickỳ", "Jindřich Helcl." ],
      "venue" : "EMNLP, pages 3016–3021.",
      "citeRegEx" : "Libovickỳ and Helcl.,? 2018",
      "shortCiteRegEx" : "Libovickỳ and Helcl.",
      "year" : 2018
    }, {
      "title" : "FlowSeq: Nonautoregressive conditional sequence generation with generative flow",
      "author" : [ "Xuezhe Ma", "Chunting Zhou", "Xian Li", "Graham Neubig", "Eduard Hovy." ],
      "venue" : "EMNLP-IJCNLP, pages 4273– 4283, Hong Kong, China.",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "CGMH: Constrained sentence generation by Metropolis-Hastings sampling",
      "author" : [ "Ning Miao", "Hao Zhou", "Lili Mou", "Rui Yan", "Lei Li." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Miao et al\\.,? 2019",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2019
    }, {
      "title" : "Scheduled sampling for transformers",
      "author" : [ "Tsvetomila Mihaylova", "André FT Martins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 351–356.",
      "citeRegEx" : "Mihaylova and Martins.,? 2019",
      "shortCiteRegEx" : "Mihaylova and Martins.",
      "year" : 2019
    }, {
      "title" : "Guiding non-autoregressive neural machine translation decoding with reordering information",
      "author" : [ "Qiu Ran", "Yankai Lin", "Peng Li", "Jie Zhou." ],
      "venue" : "arXiv preprint arXiv:1911.02215.",
      "citeRegEx" : "Ran et al\\.,? 2019",
      "shortCiteRegEx" : "Ran et al\\.",
      "year" : 2019
    }, {
      "title" : "Non-autoregressive machine translation with latent alignments",
      "author" : [ "Chitwan Saharia", "William Chan", "Saurabh Saxena", "Mohammad Norouzi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Saharia et al\\.,? 2020",
      "shortCiteRegEx" : "Saharia et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL, pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimizing the bag-ofngrams difference for non-autoregressive neural machine translation",
      "author" : [ "Chenze Shao", "Jinchao Zhang", "Yang Feng", "Fandong Meng", "Jie Zhou." ],
      "venue" : "AAAI, pages 198–205.",
      "citeRegEx" : "Shao et al\\.,? 2020",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior",
      "author" : [ "Raphael Shu", "Jason Lee", "Hideki Nakayama", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intel-",
      "citeRegEx" : "Shu et al\\.,? 2020",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2020
    }, {
      "title" : "Fast structured decoding for sequence models",
      "author" : [ "Zhiqing Sun", "Zhuohan Li", "Haoqing Wang", "Di He", "Zi Lin", "Zhihong Deng." ],
      "venue" : "NeurIPS, pages 3016–3026.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "An em approach to non-autoregressive conditional sequence generation",
      "author" : [ "Zhiqing Sun", "Yiming Yang." ],
      "venue" : "International Conference on Machine Learning, pages 9249–9258. PMLR.",
      "citeRegEx" : "Sun and Yang.,? 2020",
      "shortCiteRegEx" : "Sun and Yang.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Non-autoregressive machine translation with auxiliary regularization",
      "author" : [ "Yiren Wang", "Fei Tian", "Di He", "Tao Qin", "ChengXiang Zhai", "Tie-Yan Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Imitation learning for nonautoregressive neural machine translation",
      "author" : [ "Bingzhen Wei", "Mingxuan Wang", "Hao Zhou", "Junyang Lin", "Xu Sun." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wei et al\\.,? 2019",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Transformer has been the most widely used architecture for machine translation (Vaswani et al., 2017).",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Recent work such as the non-autoregressive transformer (NAT), aims to decode target tokens in parallel to speed up the generation (Gu et al., 2018).",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "Several remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding.",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Several remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding.",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Generally, GLM is quite similar to curriculum learning (Bengio et al., 2009) in spirit, namely first learning to generate some fragments and gradually moving to learn the whole sentences (from easy to hard).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "GLAT achieves competitive results against iterative approaches like Mask-Predict (Ghazvininejad et al., 2019), even outperforming the Mask-Predict model on WMT14 DE-EN and WMT16 RO-EN.",
      "startOffset" : 81,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "Note the training of AT adopts left-to-right teacher forcing on the target tokens (Vaswani et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "The vanilla NAT consists of the same encoder as the Transformer and a parallel decoder with layers of multi-head attention (Gu et al., 2018).",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "Multi-pass iterative decoding approaches such as Mask-Predict (Ghazvininejad et al., 2019) extends the vanilla NAT.",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "GLAT uses the same encoder-decoder architecture as the vanilla NAT (Gu et al., 2018).",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : ", hT } are copied from the encoder output using either uniform copy or soft copy (Wei et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "We adopt the Hamming distance (Hamming, 1950) as the metric, which is computed as d(Y, Ŷ ) = ∑T t=1(yt 6= ŷt).",
      "startOffset" : 30,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "For situations that Y and Ŷ have different lengths, d(Y, Ŷ ) could be other distances such as Levenshtein distance (Levenshtein, 1966).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "For NPD (Gu et al., 2018), we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 32,
      "context" : "1997 Models Idec WMT14 WMT16 Speed Up EN-DE DE-EN EN-RO RO-EN AT Models Transformer (Vaswani et al., 2017) T 27.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "For CTC (Graves et al., 2006), following Libovickỳ and Helcl (2018), we first set the max output length to twice the source input length, and remove the blanks and repeated tokens after generation.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "These datasets are tokenized and segmented into subword units using BPE encodings (Sennrich et al., 2016).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets.",
      "startOffset" : 47,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets.",
      "startOffset" : 47,
      "endOffset" : 101
    }, {
      "referenceID" : 33,
      "context" : "Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets.",
      "startOffset" : 47,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "B−1 is the mapping proposed in (Graves et al., 2006), which expand the reference to the length of output by inserting blanks or repeating words.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "1 and use Adam optimizer (Kingma and Ba, 2014) with β = (0.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021).",
      "startOffset" : 169,
      "endOffset" : 257
    }, {
      "referenceID" : 22,
      "context" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021).",
      "startOffset" : 169,
      "endOffset" : 257
    }, {
      "referenceID" : 1,
      "context" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021).",
      "startOffset" : 169,
      "endOffset" : 257
    }, {
      "referenceID" : 25,
      "context" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021).",
      "startOffset" : 169,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "Fully Non-Autoregressive Models A line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (Gu et al., 2018; Ma et al., 2019; Bao et al., 2019; Ran et al., 2019; Bao et al., 2021).",
      "startOffset" : 169,
      "endOffset" : 257
    }, {
      "referenceID" : 34,
      "context" : "Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; Li et al., 2019; Guo et al., 2020a; Sun and Yang, 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; Li et al., 2019; Guo et al., 2020a; Sun and Yang, 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; Li et al., 2019; Guo et al., 2020a; Sun and Yang, 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 31,
      "context" : "Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; Li et al., 2019; Guo et al., 2020a; Sun and Yang, 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 21,
      "context" : "Besides, there are also some work that apply different training objectives to train nonautoregressive models (Libovickỳ and Helcl, 2018; Shao et al., 2020; Ghazvininejad et al., 2020a), add regularization terms (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 184
    }, {
      "referenceID" : 28,
      "context" : "Besides, there are also some work that apply different training objectives to train nonautoregressive models (Libovickỳ and Helcl, 2018; Shao et al., 2020; Ghazvininejad et al., 2020a), add regularization terms (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "Besides, there are also some work that apply different training objectives to train nonautoregressive models (Libovickỳ and Helcl, 2018; Shao et al., 2020; Ghazvininejad et al., 2020a), add regularization terms (Wang et al.",
      "startOffset" : 109,
      "endOffset" : 184
    }, {
      "referenceID" : 33,
      "context" : ", 2020a), add regularization terms (Wang et al., 2019; Guo et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : ", 2020a), add regularization terms (Wang et al., 2019; Guo et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 295
    }, {
      "referenceID" : 23,
      "context" : "Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 295
    }, {
      "referenceID" : 10,
      "context" : "Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 295
    }, {
      "referenceID" : 15,
      "context" : "Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 295
    }, {
      "referenceID" : 19,
      "context" : "Non-Autoregressive Models with Iterative Refinement A series of work are devoted to semiautoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; Miao et al., 2019; Gu et al., 2019; Ghazvininejad et al., 2019, 2020b; Kasai et al., 2020; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 295
    }, {
      "referenceID" : 19,
      "context" : "(Li et al., 2020) first predict the left token and right token for each position, and decode the final token at the current position conditioned on the left-and-right tokens predicted before.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "Scheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019).",
      "startOffset" : 165,
      "endOffset" : 215
    }, {
      "referenceID" : 24,
      "context" : "Scheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019).",
      "startOffset" : 165,
      "endOffset" : 215
    } ],
    "year" : 2021,
    "abstractText" : "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
    "creator" : "LaTeX with hyperref"
  }
}