{
  "name" : "2021.acl-long.428.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "BanditMTL: Bandit-based Multi-task Learning for Text Classification",
    "authors" : [ "Yuren Mao", "Zekai", "Wang", "Weiwei Liu", "Xuemin", "Lin", "Wenbin Hu" ],
    "emails" : [ "yuren.mao@unsw.edu.au,", "wzekai99@gmail.com", "liuweiwei863@gmail.com", "lxue@cse.unsw.edu.au,", "hwb@whu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5506–5516\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5506"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently (Caruana, 1993; Ando and Zhang, 2005). It has achieved great success in various applications, ranging from summary quality estimation (Kriz et al., 2020) to text classification (Liu et al., 2017).\nIn the multi-task text classification context, MTL simultaneously learns the tasks by minimizing their empirical losses together; for example, by minimizing the mean of the empirical losses for the included tasks. However, it is common for these tasks to be competing. Minimizing the losses of some tasks increases the losses of others, which accordingly increases the task variance (variance between the task-specific loss). Large task variance can lead to over-fitting in some tasks and under-fitting in others, which degenerates the generalization performance of an MTL model. To illustrate this issue,\n*Corresponding author.\nit is instructive to consider a case of two-task learning, where task 1 and task 2 are conflicting binary classification tasks. When the task variance is uncontrolled, it is possible that the empirical loss of task 1 will converge to 0, while the empirical loss of task 2 will converge to 0.5. In such a case, although the mean of the empirical losses is decreasing, task 1 overfits and task 2 underfits, which leads to poor generalization performance.\nTo address the problem caused by uncontrolled task variance, it is necessary to implement task variance regularization, which regularizes the variance between the task-specific losses during training. However, existing deep MTL methods, including both adaptive weighting sum methods (Kendall et al., 2018; Chen et al., 2018; Liu et al., 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al., 2020b), ignore the task variance. Overlooking task variance degenerates an MTL model’s generalization ability.\nTo fill this gap and further improve the generalization ability of MTL models, this paper proposes a novel MTL method, dubbed BanditMTL, which jointly minimizes the empirical losses and regularizes the task variance. BanditMTL is proposed based on linear adversarial multi-armed bandit and implemented with a mirror gradient ascent-descent algorithm. Our proposed approach can improve the performance of multi-task text classification.\nMoreover, to verify our theoretical analysis and validate the superiority of BanditMTL in the text classification context, we conduct experiments on two classical text classification problems: sentiment analysis (on reviews) and topic classification (on news). The results demonstrate that applying variance regularization can improve the performance of a MTL model; moreover, BanditMTL is found to outperform several state-of-the-art multitask text classification methods."
    }, {
      "heading" : "2 Related Works",
      "text" : "Multi-task Learning methods jointly minimize taskspecific empirical loss based on multi-objective optimization (Sener and Koltun, 2018; Lin et al., 2019; Mao et al., 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018). The multi-objective optimization based MTL can converge to an arbitrary Pareto stationary point, the task variance of which is also arbitrary. While the weighted sum methods focus on minimizing the weighted average of the task-specific empirical loss, they do not consider the task variance. To fill the gap in existing methods, this paper proposes to regularize the task variance, which will significantly impact the generalization performance of MTL models.\nVariance-based regularization has been used previously in Single-task Learning to balance the tradeoff between approximation and estimation error (Bartlett et al., 2006; Koltchinskii et al., 2006; Namkoong and Duchi, 2017). In the Single-task Learning setting, the goal of variance-based regularization is to regularize the variance between the loss of training samples (Namkoong and Duchi, 2016; Duchi and Namkoong, 2019). While these variance-based regularization methods can improve the generalization ability of Single-task Learning models, they do not fit the Multi-task Learning setting. This paper thus first proposes a novel variancebased regularization method for Multi-task Learning to improve MTL models’ generalization ability by regularizing the between-task loss variance."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "Consider a multi-task learning problem with T tasks over an input space X and a collection of task spaces {Yt}Tt=1. For each task, we have a set of i.i.d. training samples Dt = (Xt, Y t) and (Xt, Y t) = {xti, yti}nti=1, where nt is the number of training samples of task t. In this paper, we focus on the neural network-based multi-task learning setting, in which the tasks are jointly learned by sharing some parameters (hidden layers).\nLet h(·, θ) : {X}Tt=1 → {Yt}Tt=1 be the multitask learning model, where θ ∈ Θ is the vector of the model parameters. θ = (θsh, θ1, ..., θT ) consists of θsh (the parameters shared between tasks) and θt (the task-specific parameters). We denote ht(·, θsh, θt) : X → Yt as the taskspecific map. The task-specific loss function\nis denoted as lt(·, ·) : Yt × Yt → [0, 1]T . The empirical loss of the task t is defined as L̂t(θsh, θt)= 1nt ∑nt i=1 l t(h(xti, θ sh, θt), yti).\nThe transpose of the vector/matrix is represented by the superscript , and the logarithms to base e are denoted by log."
    }, {
      "heading" : "3.1 The Learning Objective of MTL",
      "text" : "Under the Empirical Risk Minimization paradigm, multi-task learning aims to optimize the vector of task-specific empirical losses. The learning objective of multi-task learning is formulated as a vector optimization objective, as in equation (1).\nmin θ\n(L̂1(θsh, θ1), ..., L̂T (θsh, θT )) , (1)\nIn order to optimize the learning objective, existing multi-task learning methods tend to adopt either global criterion optimization strategies (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018; Mao et al., 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al., 2019; Debabrata Mahapatra, 2020). In this paper, we choose to adopt the typical linear-combination strategy, which can achieve proper Pareto Optimality (Miettinen, 2012) and is widely used in the multi-task text classification context (Liu et al., 2017; Yadav et al., 2018; Xiao et al., 2018). The linear-combination strategy is defined in (2):\nmin θ\n1\nT T∑ t=1 L̂t(θsh, θt), (2)"
    }, {
      "heading" : "3.2 Adversarial Multi-armed Bandit",
      "text" : "Adversarial multi-armed bandit, a case in which a player and an adversary simultaneously address the trade-off between exploration and exploitation, is one of the fundamental multi-armed bandit problems (Bubeck and Cesa-Bianchi, 2012). In this paper, we consider the linear multi-armed bandit, which is a generalized adversarial multi-armed bandit. In our linear multi-armed bandit setting, the set of arms is a compact set A ∈ RT . At each time step k = 1, 2, ...,K the player chooses an arm from A while; simultaneously, the adversary chooses a loss vector from [0, 1]T . For linear multi-armed bandit, the Online Mirror Descent (OMD) algorithm is a powerful technology that can be used to achieve proper regret (Srebro et al., 2011)."
    }, {
      "heading" : "3.3 Online Mirror Descent",
      "text" : "The Online Mirror Descent (OMD) algorithm is a generalization of gradient descent for sequential de-\ncision problems. Rather than taking gradient steps in the primal space, the mirror descent approach involves taking gradient steps in the dual space. The bijection ∇Φ and its inverse ∇Φ∗ are used to map back and forth between primal and dual points. To obtain a good regret bound, Φ must be a Legendre function (Definition 1).\nAssume that we update uk with gradient gk using OMD. The OMD algorithm consists of three steps: (1) select a Legendre function Φ; (2) perform a gradient descent step in the dual space vk+1 = ∇Φ∗(∇Φ(uk) − ηgk), where Φ∗ and ∇Φ∗ are as defined in Definition 2 and η is the step length; (3) project back to the primal space according to the Bregman divergence (Definition 3): uk+1 = argminuDΦ(u, v k+1) . Definition 1 (Legendre Function). Let O ⊂ RT be an open convex set, and let O be the closure of O. A continuous function Φ : O → R is Legendre if:\n(i) Φ is strictly convex and admits continuous first partial derivatives on O;\n(ii) limu→O/O ‖ ∇Φ(u) ‖= +∞. Definition 2 (Fenchel Conjugate). The Fenchel conjugate Φ∗ of Φ is Φ∗(u) = supv{〈u, v〉 + Φ(v)}, and ∇Φ∗(u) = argmaxv{〈u, v〉+Φ(v)}. Definition 3 (Bregman Divergence). The Bregman divergence DΦ : O × O → R associated with a Legendre function Φ is defined by DΦ(u, v) = Φ(u)− Φ(v)− (u− v) ∇Φ(v)."
    }, {
      "heading" : "3.4 Hard Parameter-sharing MTL Model",
      "text" : "This paper adopts the most prevalent and efficient hard parameter-sharing MTL model (Kendall et al., 2018; Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2020b) to perform multi-task text classification. As shown in Figure 1, the hard parametersharing MTL model learns multiple related tasks simultaneously by sharing the hidden layers (feature extractor) across all tasks while retaining taskspecific output layers for each task. In multitask text classification, the feature extractor can\nbe LSTM (Hochreiter and Schmidhuber, 1997), TextCNN (Kim, 2014), and so on. The task-specific layers are typically formulated by fully connected layers, ending with a softmax function."
    }, {
      "heading" : "4 Bandit-based Multi-task Learning",
      "text" : "To avoid uncontrolled task variance, we need to develop a learning method that regularizes the task variance during training. Regularized Loss Minimization (RLM) is a learning method that jointly minimizes the empirical risk and a regularization function, and is thus a natural choice. While RLM is widely used in Single-task Learning, it cannot be directly used in Multi-task Learning to regularize the task variance. In this section, we propose a surrogate for RLM in MTL and accordingly develop a novel MTL method, namely BanditMTL."
    }, {
      "heading" : "4.1 Regularizing the Task Variance",
      "text" : "RLM is a natural choice for regularizing the task variance. RLM for task-variance-regularized MTL can be formulated as in equation (3):\nmin θ\n1\nT T∑ t=1 L̂t(θsh, θt) + √\nρV ar(L̂t(θsh, θt)), (3)\nwhere V ar(L̂t(θsh, θt)) = 1T ∑T\nt=1(L̂t(θsh, θt)− 1 T ∑T t=1 L̂t(θsh, θt))2 is the empirical variance between the task-specific losses. However, formulation (3) is generally nonconvex and associated NP-hardness. To handle the non-convexity, we select a convex surrogate for (3) based on its equivalent formulation (4) (Ben-Tal et al., 2013; Bertsimas et al., 2018).\nsup p∈Pρ,T\n1\nT T∑ t=1 ptL̂t(θsh, θt) = 1 T T∑ t=1 L̂t(θsh, θt)\n+ √ ρV ar(L̂t(θsh, θt)) + o(T− 12 ),\n(4) where Pρ,T := {p ∈ RT : ∑T t=1 pt = 1, pt ≥\n0, ∑T\nt=1 pt log(Tpt) ≤ ρ}. supp∈Pρ,T 1 T ∑T t=1 ptL̂t(θsh, θt) is convex and can be used as a convex surrogate for (3). This paper proposes to perform task-variance-regularized multi-task-learning with the following learning objective:\nmin θ sup p∈Pρ,T\n1\nT T∑ t=1 ptL̂t(θsh, θt) (5)\nOptimizing (5) is equivalent to optimizing (3).\nIn the proposed learning objective (5), ρ is the regularization parameter that controls the trade-off between the mean empirical loss and the task variance. Experimental analysis on the influence of ρ is presented in Section 5.6. To learn an MTL model via learning objective (5), we formulate the learning problem as an adversarial multi-armed bandit problem in Section 4.2 and further propose the BanditMTL algorithm in Section 4.3."
    }, {
      "heading" : "4.2 Task-Variance-Regularized MTL as Adversarial Multi-armed Bandit",
      "text" : "In deep multi-task learning, an MTL model is typically learnt by iteratively optimizing the learning objective. To iteratively optimize the proposed learning objective (5), we formulate it as an adversarial multi-armed bandit problem in which the player chooses an arm from Pρ,T and the adversary assigns a loss vector L(θ) = (L̂1(θsh, θ1), ..., L̂T (θsh, θT )) to each arm. In each learning iteration, the player chooses an arm from Pρ,T to increase the weighted sum loss, while the adversary aims to decrease the loss by updating the learning model. Moreover, both the player and the adversary aim to find a trade-off between exploration and exploitation to achieve proper regret.\nWhen lt(·, ·) is convex and Θ is compact, the adversarial multi-armed bandit problem can achieve a saddle point (θ∗, p∗) (Boyd and Vandenberghe, 2014). The saddle point satisfies Lpsup ≤ p∗ L(θ∗) ≤ Lθinf , where Lpsup = sup{p L(θ∗)|p ∈ Pρ,T} and Lθinf = inf{p∗ L(θ)|θ ∈ Θ}.\nTo achieve a proper regret and saddle point, we adopts mirror gradient ascent for the player and mirror gradient descent for the adversary. The mirror gradient ascent-descent algorithm for MTL, namely BanditMTL, is proposed in the next section."
    }, {
      "heading" : "4.3 BanditMTL",
      "text" : "In this paper, the task-variance-regularized multitask learning is formulated as a linear adversarial multi-armed bandit problem. For a problem of this kind, mirror gradient descent (ascent) is a powerful technique for the adversary and the player to achieve proper regret (Bubeck and Cesa-Bianchi, 2012; Namkoong and Duchi, 2016). Moreover, based on the mirror gradient ascent-descent, we can reach the saddle point of the minimax optimization problem when the task-specific loss functions are convex and the parameter space Θ is compact (Boyd and Vandenberghe, 2014).\nAlgorithm 1: BanditMTL Input: data {Dt}Tt=1, the learning rate ηp and ηa, the approximation parameter . Initialization: p1 = ( 1T , 1 T , ..., 1 T )\n, randomly initialize θ1. for k = 1 to K do\nCompute λ with Algorithm 2. Update p: :\npk+1t = e\n1 1+λ (log pkt +ηpL̂t(θksh,θ k t ))\n∑T t=1 e 1 1+λ (log pkt +ηpL̂t(θksh,θ k t ))\nUpdate θ: θk+1 = θk − ηa∇θ 1T ∑T t=1 p k t L̂t(θsh, θt)\nend for return θk with best validation performance.\nAlgorithm 2: Compute λ Input: pk, θk, , β. Initialization: λl = 0, λr = 0. if f(0) ≤ 0 then\nreturn 0. end if while f(λr) ≥ 0 do\nλl = λr. λr = λl + β.\nend while while |f(λ̂)| > do\nλ̂ = λl+λr2 . if f(λ̂) > 0 then\nλl = λ̂. else\nλr = λ̂. end if\nend while return λ̂.\nIn this paper, we propose a task-varianceregularized multi-task learning algorithm based on mirror gradient ascent-descent, dubbed BanditMTL. The proposed method is presented in algorithmic form in Algorithm 1. We assume that the training procedure has K learning iterations. In each learning iteration 1 ≤ k < K, the player and the adversary update via mirror gradient ascent and descent."
    }, {
      "heading" : "4.3.1 Mirror Gradient Ascent for the Player",
      "text" : "For the player, considering the constraint in Pρ,T , we choose the Legendre function Φp(p) =∑T\nt=1 pt log pt. Based on the Legendre function, we propose the update rule of p in (6) (see the\nAppendix for derivations of the update rule).\npk+1t = e\n1 1+λ (log pkt+ηpL̂t(θksh,θkt )) ∑T\nt=1 e 1 1+λ (log pkt+ηpL̂t(θksh,θkt ))\n(6)\nwhere ηp is the step size for the player. Moreover,λ is the solution of equation, where f(λ) is defined in (7). f(λ) is non-increasing and λ ≥ 0.\nf(λ) =\n∑T t=1(log qt)qt 1 1+λ ∑T t=1(1 + λ)qt 1 1+λ − log T∑ t=1 qt 1 1+λ\n+ log T − ρ, (7)\nwhere qt = e(log p k t+ηpL̂t(θksh,θkt )). To solve f(λ) = 0, we propose a bisection search-based algorithm, as outlined in Algorithm 2."
    }, {
      "heading" : "4.3.2 Mirror Gradient Descent for the Adversary",
      "text" : "For the adversary, to simplify calculation, we choose the Legendre function Φθ(θ) = 12 ‖ θ ‖22. By using Φθ(θ), the update rule of mirror gradient descent (presented in (8)) is the same as that of same with the common gradient descent. (see the Appendix for derivations of the update rule).\nθk+1 = θk − ηa∇θ 1 T T∑ t=1 pkt L̂t(θsh, θt), (8)\nwhere ηa is the learning rate for the adversary."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we perform experimental studies on sentiment analysis and topic classification respectively to evaluate the performance of our proposed BanditMTL and verify our theoretical analysis. The implementation is based on PyTorch (Paszke et al., 2019). The code is attached in the supplementary materials."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "Sentiment Analysis . We evaluate our algorithm on product reviews from Amazon. The dataset (Blitzer et al., 2007) contains product reviews from 14 domains, including books, DVDs, electronics, kitchen appliances and so on. We consider each domain as a binary classification task. Reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, reviews with\nhttps://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/\nrating = 3 are discarded as the sentiments were ambiguous and hard to predict.\nTopic Classification . We select 16 newsgroups from the 20 Newsgroup dataset, which is a collection of approximately 20,000 newsgroup documents that is partitioned (nearly) evenly across 20 different newsgroups, then formulate them into four 4-class classification tasks (as shown in Table 1) to evaluate the performance of our algorithm on topic classification."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "We compare BanditMTL with following baselines. Single-Task Learning: learning each task independently. Uniform Scaling: learning the MTL model with learning objective (2), the uniformly weighted sum of task-specific empirical loss.\nUncertainty: using the uncertainty weighting method proposed by (Kendall et al., 2018).\nGradNorm: using the gradient normalization method proposed by (Chen et al., 2018).\nMGDA: using the MGDA-UB method proposed by (Sener and Koltun, 2018).\nAdvMTL: using the adversarial Multi-task Learning method proposed by (Liu et al., 2017).\nTchebycheff: using the Tchebycheff procedure proposed by (Mao et al., 2020b)."
    }, {
      "heading" : "5.3 Experimental Settings",
      "text" : "We adopt the hard parameter-sharing MTL model shown in Fig. 1. The shared feature extractor is formulated via a TextCNN which is structured with three parallel convolutional layers with kernels size of 3, 5, 7 respectively. The task-specific module is formulated by means of one fully connected layer ending with a softmax function. To ensure consistency with the state-of-the-art multi-task classification methods (Liu et al., 2017; Mao et al., 2020b) and ensure fair comparison, we adopt Pre-trained\nhttp://qwone.com/˜jason/20Newsgroups/\nGloVe (Pennington et al., 2014) word embeddings in our experimental analysis.\nWe train the deep MTL network model in line with Algorithm 1. The learning rate for the adversary is 1e − 3 for both sentiment analysis and topic classification. We use the Adam optimizer (Kingma and Ba, 2015) and train over 3000 epochs for both sentiment analysis and topic classification.\nThe batch size is 256. We use dropout with a probability of 0.5 for all task-specific modules."
    }, {
      "heading" : "5.4 Classification Accuracy",
      "text" : "We compare the proposed BanditMTL with the baselines and report the results over 10 runs by plotting the classification accuracy of each task for both sentiment analysis and topic classification. The results are shown in Fig. 2 and 3.\nAll experimental results show that our proposed BanditMTL significantly outperforms Uniform Scaling, which demonstrates that adopting task variance regularization can boost the performance of MTL models. Moreover, BanditMTL can be seen to outperform all baselines and achieve state-of-the-art performance."
    }, {
      "heading" : "5.5 Task Variance",
      "text" : "In this section, we experimentally investigate how BanditMTL regularizes the task variance during training and compare the task variance of BanditMTL with the baselines. The results are plotted in Fig. 4. As the figure shows, all MTL methods have lower task variance than single task learning during training. Moreover, BanditMTL has lower task variance and smoother evolution during train-\ning than other MTL methods. After considering the results obtained in Section 5.4, we conclude that task variance has a significant impact on multi-task text classification performance."
    }, {
      "heading" : "5.6 Impact of ρ",
      "text" : "In BanditMTL, ρ is the regularization parameter. In this section, we experimentally investigate the impact of ρ on task variance and average classification accuracy over the tasks of interest."
    }, {
      "heading" : "5.6.1 Impact on Variance",
      "text" : "Fig. 5 plots how the task variance evolves during training w.r.t different values of ρ. The task variance decreases as ρ increases. It reveals that we can control the task variance by adjusting ρ."
    }, {
      "heading" : "5.6.2 Impact on Average Accuracy",
      "text" : "The changes in BanditMTL’s average classification accuracy w.r.t different values of ρ is illustrated in Fig. 6. In this figure, as ρ increases, the average accuracy of BanditMTL first increases and then decreases. This reveals that ρ significantly impacts the performance of multi-task text classification. As ρ controls the trade-off between the empirical loss and the task variance, we can conclude that this trade-off significantly impacts the multi-task text classification performance. Thus, in the multitask text classification, it is necessary for us to find a proper trade-off between the empirical loss and the task variance rather than focusing only on empirical loss. These results verify the necessary of task variance regularization."
    }, {
      "heading" : "5.7 Sensitivity Study on ηp",
      "text" : "In BanditMTL, ηp is a hyper-parameter. To determine whether the performance of BanditMTL is sensitive to ηp, we conduct experiments on the classification performance of BanditMTL w.r.t different values of ηp. The results of these experiments are presented in Fig. 7. As the figure shows, the performance of our proposed method is not very sensitive to ηp when ηp is within the range of 0.3\nto 0.9 for both sentiment analysis and topic classification. Setting ηp to between 0.3 and 0.9 can generally provide satisfactory results."
    }, {
      "heading" : "5.8 Evolution of pt",
      "text" : "In this section, we observe the changes in pt during training and compare these changes with the task weight adaption process of three weight adaptive MTL methods (i.e., Uncertainty, Gradnorm, and MGDA). The results for topic classification are reported in Fig. 9. Due to space limitations, the sentiment analysis results are presented in the appendix. From the results, we can see that the weight adaption process of BanditMTL is more stable than that of Uncertainty, Gradnorm, and MGDA."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper proposes a novel Multi-task Learning algorithm, dubbed BanditMTL. It fills the task variance regularization gap in the field of MTL and achieves state-of-the-art performance in real-world text classification applications. Moreover, our proposed BanditMTL is model-agnostic; thus, it could potentially be used in other natural language processing applications, such as Multi-task Named Entity Recognition."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the National Natural Science Foundation of China under Grants 61976161 and 61976162."
    }, {
      "heading" : "1 Derivations of the Update Rule for the Player",
      "text" : "Assume the mirror gradient ascent step in the dual space is qk+1 w.r.t the k + 1th learning iteration. Then, the qk+1 can be obtained as the follows.\nAccording to the gradient descent step,\n∇Φp(qk+1) = ∇Φp(pk) + ηpL(θk). (9) For each task, the t-th element of ∇Φp(qk+1),\n∇Φp(qk+1t ) = 1 + log qk+1t . (10) Combining (9) and (10), we have\nqk+1t = e (∇Φp(pkt )+ηpL̂t(θksh,θkt ))−1). (11)\nTo map back to the primal space, we need to solve optimization objective (12).\npk+1 = arg min p∈Pρ,T DΦp(p, q k+1), (12)\nThe Lagrangian for the optimization problem (12) is:\nL(pk+1, α, λ) = T∑ t=1 pk+1t log pk+1t qk+1t\n− α( T∑ t=1 pk+1t − 1)− λ(ρ− T∑ t=1 pk+1t log p k+1 t T ).\n(13) The partial derivative w.r.t pt is:\n∇pk+1t L(p k+1, α, λ) =(1 + λ) log pk+1t − log qk+1t\n− α+ λ log T + 1 + λ. (14)\nUsing the first order conditions w.r.t pk+1t (∇pk+1t L(p k+1, α, λ) = 0), we have\npk+1t = (q k+1 t )\n1 1+λT− λ 1+λ exp( α\n1 + λ − 1). (15)\nCombining with ∑T\nt=1 p k+1 t = 1, we have\npk+1t = (q k+1 t )\n1 1+λ /( T∑ t=1 (qk+1t ) 1 1+λ ). (16)\nPlugging this back into the Lagrangian, we have\nL(λ) =min α\nmax pk+1∈Pρ,T\nL(pk+1, α, λ)\n=λ(log T − ρ)− (1 + λ) log T∑ t=1 (qk+1t ) 1 1+λ .\n(17)\nTaking derivatives, we have\nd\ndλ L(λ) = log T − ρ− log T∑ t=1 (qk+1t ) 1 1+λ\n− ∑T t=1 log(q k+1 t )(q k+1 t ) 1 1+λ\n(1 + λ) ∑T\nt=1(q k+1 t )\n1 1+λ\n.\n(18)\nCombining (11) and (16), we have\npk+1t = e\n1 1+λ (log pkt+ηpL̂t(θksh,θkt )) ∑T\nt=1 e 1 1+λ (log pkt+ηpL̂t(θksh,θkt ))\n(19)\nwhere λ is obtained by solving the equation d dλL(λ) = 0, which is the necessary condition to optimize the Lagrangian function."
    }, {
      "heading" : "2 Derivations of the Update Rule for the Adversary",
      "text" : "Assume the mirror gradient descent step in the dual space is γk+1 w.r.t the k + 1th learning iteration. Then, the γk+1 can be obtained as the follows.\n∇Φθ(γk+1) = ∇Φθ(θk)− ηa 1 T T∑ t=1\npkt L̂t(θsh, θt) (20)\nFor Φθ(θ) = 12 ‖ θ ‖22, we have ∇Φθ(γk+1) = γk+1 and ∇Φθ(θk) = θk. Thus,\nγk+1 = θk − ηa 1 T T∑ t=1 pkt L̂t(θsh, θt). (21)\nMoreover, it is obvious that\nargminDΦθ(Φθ, γ k+1) = γk+1. (22)\nThen,\nθk+1 = θk − ηa 1 T T∑ t=1 pkt L̂t(θsh, θt). (23)\nwhich means that the update rule of the mirror gradient descent is same with the vanilla gradient descent when Legendre function Φθ(θ) = 12 ‖ θ ‖22 is adopted."
    }, {
      "heading" : "3 Weight Adaption Process for Sentiment Analysis",
      "text" : "The results of the change of pt during a training banditMTL model are shown in Fig. 9. Comparing it with the task weights adaption process of three weight adaptive MTL methods (i.e., Uncertainty, Gradnorm, MGDA), we can see that the weights adaption process of banditMTL is more stable."
    }, {
      "heading" : "4 Hardware Specification and Environment",
      "text" : "Our experiments are conducted on a Ubuntu 64- Bit Linux workstation, having 10-core Intel Xeon\nSilver CPU (2.20 GHz) and Nvidia GeForce RTX 2080 Ti GPUs with 11GB graphics memory."
    } ],
    "references" : [ {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "Rie Kubota Ando", "Tong Zhang." ],
      "venue" : "Journal of Machine Learning Research, 6:1817–1853.",
      "citeRegEx" : "Ando and Zhang.,? 2005",
      "shortCiteRegEx" : "Ando and Zhang.",
      "year" : 2005
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe." ],
      "venue" : "Journal of the American Statistical Association, 101(473):138–156.",
      "citeRegEx" : "Bartlett et al\\.,? 2006",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2006
    }, {
      "title" : "Robust solutions of optimization problems affected by uncertain probabilities",
      "author" : [ "Aharon Ben-Tal", "Dick den Hertog", "Anja De Waegenaere", "Bertrand Melenberg", "Gijs Rennen." ],
      "venue" : "Manag. Sci., 59(2):341– 357.",
      "citeRegEx" : "Ben.Tal et al\\.,? 2013",
      "shortCiteRegEx" : "Ben.Tal et al\\.",
      "year" : 2013
    }, {
      "title" : "Robust sample average approximation",
      "author" : [ "Dimitris Bertsimas", "Vishal Gupta", "Nathan Kallus." ],
      "venue" : "Math. Program., 171(1-2):217–282.",
      "citeRegEx" : "Bertsimas et al\\.,? 2018",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 2018
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "ACL.",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen P. Boyd", "Lieven Vandenberghe." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Boyd and Vandenberghe.,? 2014",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2014
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multiarmed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi." ],
      "venue" : "Found. Trends Mach. Learn., 5(1):1–122.",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? 2012",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Multitask learning: A knowledgebased source of inductive bias",
      "author" : [ "Rich Caruana." ],
      "venue" : "ICML.",
      "citeRegEx" : "Caruana.,? 1993",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1993
    }, {
      "title" : "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "author" : [ "Zhao Chen", "Vijay Badrinarayanan", "Chen-Yu Lee", "Andrew Rabinovich." ],
      "venue" : "ICML.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization",
      "author" : [ "Vaibhav Rajan Debabrata Mahapatra." ],
      "venue" : "ICML.",
      "citeRegEx" : "Mahapatra.,? 2020",
      "shortCiteRegEx" : "Mahapatra.",
      "year" : 2020
    }, {
      "title" : "Variance-based regularization with convex objectives",
      "author" : [ "John C. Duchi", "Hongseok Namkoong." ],
      "venue" : "J. Mach. Learn. Res., 20:68:1–68:55.",
      "citeRegEx" : "Duchi and Namkoong.,? 2019",
      "shortCiteRegEx" : "Duchi and Namkoong.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "author" : [ "Alex Kendall", "Yarin Gal", "Roberto Cipolla." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Kendall et al\\.,? 2018",
      "shortCiteRegEx" : "Kendall et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Local rademacher complexities and oracle inequalities in risk minimization",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "The Annals of Statistics, 34(6):2593– 2656.",
      "citeRegEx" : "Koltchinskii,? 2006",
      "shortCiteRegEx" : "Koltchinskii",
      "year" : 2006
    }, {
      "title" : "Simple-qe: Better automatic quality estimation for text simplification",
      "author" : [ "Reno Kriz", "Marianna Apidianaki", "Chris CallisonBurch." ],
      "venue" : "Arxiv.",
      "citeRegEx" : "Kriz et al\\.,? 2020",
      "shortCiteRegEx" : "Kriz et al\\.",
      "year" : 2020
    }, {
      "title" : "Pareto multi-task learning",
      "author" : [ "Xi Lin", "Hui-Ling Zhen", "Zhenhua Li", "Qingfu Zhang", "Sam Kwong." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial multi-task learning for text classification",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Adaptive adversarial multi-task representation learning",
      "author" : [ "Yuren Mao", "Weiwei Liu", "Xuemin Lin." ],
      "venue" : "ICML.",
      "citeRegEx" : "Mao et al\\.,? 2020a",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2020
    }, {
      "title" : "Tchebycheff procedure for multi-task text classification",
      "author" : [ "Yuren Mao", "Shuang Yun", "Weiwei Liu", "Bo Du." ],
      "venue" : "ACL.",
      "citeRegEx" : "Mao et al\\.,? 2020b",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2020
    }, {
      "title" : "Nonlinear multiobjective optimization, volume 12",
      "author" : [ "Kaisa Miettinen." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Miettinen.,? 2012",
      "shortCiteRegEx" : "Miettinen.",
      "year" : 2012
    }, {
      "title" : "Stochastic gradient methods for distributionally robust optimization with f-divergences",
      "author" : [ "Hongseok Namkoong", "John C. Duchi." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Namkoong and Duchi.,? 2016",
      "shortCiteRegEx" : "Namkoong and Duchi.",
      "year" : 2016
    }, {
      "title" : "Variance-based regularization with convex objectives",
      "author" : [ "Hongseok Namkoong", "John C. Duchi." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Namkoong and Duchi.,? 2017",
      "shortCiteRegEx" : "Namkoong and Duchi.",
      "year" : 2017
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Multitask learning as multi-objective optimization",
      "author" : [ "Ozan Sener", "Vladlen Koltun." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Sener and Koltun.,? 2018",
      "shortCiteRegEx" : "Sener and Koltun.",
      "year" : 2018
    }, {
      "title" : "On the universality of online mirror descent",
      "author" : [ "Nati Srebro", "Karthik Sridharan", "Ambuj Tewari." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Srebro et al\\.,? 2011",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2011
    }, {
      "title" : "Gated multi-task network for text classification",
      "author" : [ "Liqiang Xiao", "Honglun Zhang", "Wenqing Chen." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Xiao et al\\.,? 2018",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task learning framework for mining crowd intelligence towards clinical treatment",
      "author" : [ "Shweta Yadav", "Asif Ekbal", "Sriparna Saha", "Pushpak Bhattacharyya", "Amit P. Sheth." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Yadav et al\\.,? 2018",
      "shortCiteRegEx" : "Yadav et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently (Caruana, 1993; Ando and Zhang, 2005).",
      "startOffset" : 156,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently (Caruana, 1993; Ando and Zhang, 2005).",
      "startOffset" : 156,
      "endOffset" : 193
    }, {
      "referenceID" : 16,
      "context" : "It has achieved great success in various applications, ranging from summary quality estimation (Kriz et al., 2020) to text classification (Liu et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "However, existing deep MTL methods, including both adaptive weighting sum methods (Kendall et al., 2018; Chen et al., 2018; Liu et al., 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al.",
      "startOffset" : 82,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "However, existing deep MTL methods, including both adaptive weighting sum methods (Kendall et al., 2018; Chen et al., 2018; Liu et al., 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al.",
      "startOffset" : 82,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "However, existing deep MTL methods, including both adaptive weighting sum methods (Kendall et al., 2018; Chen et al., 2018; Liu et al., 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al.",
      "startOffset" : 82,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : ", 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al., 2020b), ignore the task variance.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : ", 2017) and multi-objective optimization-based methods (Sener and Koltun, 2018; Mao et al., 2020b), ignore the task variance.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "Multi-task Learning methods jointly minimize taskspecific empirical loss based on multi-objective optimization (Sener and Koltun, 2018; Lin et al., 2019; Mao et al., 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al.",
      "startOffset" : 111,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "Multi-task Learning methods jointly minimize taskspecific empirical loss based on multi-objective optimization (Sener and Koltun, 2018; Lin et al., 2019; Mao et al., 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al.",
      "startOffset" : 111,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "Multi-task Learning methods jointly minimize taskspecific empirical loss based on multi-objective optimization (Sener and Koltun, 2018; Lin et al., 2019; Mao et al., 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al.",
      "startOffset" : 111,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : ", 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : ", 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : ", 2020a) or optimizing the weighted sum of the task-specific loss (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Variance-based regularization has been used previously in Single-task Learning to balance the tradeoff between approximation and estimation error (Bartlett et al., 2006; Koltchinskii et al., 2006; Namkoong and Duchi, 2017).",
      "startOffset" : 146,
      "endOffset" : 222
    }, {
      "referenceID" : 23,
      "context" : "Variance-based regularization has been used previously in Single-task Learning to balance the tradeoff between approximation and estimation error (Bartlett et al., 2006; Koltchinskii et al., 2006; Namkoong and Duchi, 2017).",
      "startOffset" : 146,
      "endOffset" : 222
    }, {
      "referenceID" : 22,
      "context" : "In the Single-task Learning setting, the goal of variance-based regularization is to regularize the variance between the loss of training samples (Namkoong and Duchi, 2016; Duchi and Namkoong, 2019).",
      "startOffset" : 146,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "In the Single-task Learning setting, the goal of variance-based regularization is to regularize the variance between the loss of training samples (Namkoong and Duchi, 2016; Duchi and Namkoong, 2019).",
      "startOffset" : 146,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "In order to optimize the learning objective, existing multi-task learning methods tend to adopt either global criterion optimization strategies (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018; Mao et al., 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al.",
      "startOffset" : 144,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "In order to optimize the learning objective, existing multi-task learning methods tend to adopt either global criterion optimization strategies (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018; Mao et al., 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al.",
      "startOffset" : 144,
      "endOffset" : 222
    }, {
      "referenceID" : 8,
      "context" : "In order to optimize the learning objective, existing multi-task learning methods tend to adopt either global criterion optimization strategies (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018; Mao et al., 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al.",
      "startOffset" : 144,
      "endOffset" : 222
    }, {
      "referenceID" : 20,
      "context" : "In order to optimize the learning objective, existing multi-task learning methods tend to adopt either global criterion optimization strategies (Liu et al., 2017; Kendall et al., 2018; Chen et al., 2018; Mao et al., 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al.",
      "startOffset" : 144,
      "endOffset" : 222
    }, {
      "referenceID" : 26,
      "context" : ", 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al., 2019; Debabrata Mahapatra, 2020).",
      "startOffset" : 49,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : ", 2020b) or multiple gradient descent strategies (Sener and Koltun, 2018; Lin et al., 2019; Debabrata Mahapatra, 2020).",
      "startOffset" : 49,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "In this paper, we choose to adopt the typical linear-combination strategy, which can achieve proper Pareto Optimality (Miettinen, 2012) and is widely used in the multi-task text classification context (Liu et al.",
      "startOffset" : 118,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "In this paper, we choose to adopt the typical linear-combination strategy, which can achieve proper Pareto Optimality (Miettinen, 2012) and is widely used in the multi-task text classification context (Liu et al., 2017; Yadav et al., 2018; Xiao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 258
    }, {
      "referenceID" : 29,
      "context" : "In this paper, we choose to adopt the typical linear-combination strategy, which can achieve proper Pareto Optimality (Miettinen, 2012) and is widely used in the multi-task text classification context (Liu et al., 2017; Yadav et al., 2018; Xiao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 258
    }, {
      "referenceID" : 28,
      "context" : "In this paper, we choose to adopt the typical linear-combination strategy, which can achieve proper Pareto Optimality (Miettinen, 2012) and is widely used in the multi-task text classification context (Liu et al., 2017; Yadav et al., 2018; Xiao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 258
    }, {
      "referenceID" : 6,
      "context" : "2 Adversarial Multi-armed Bandit Adversarial multi-armed bandit, a case in which a player and an adversary simultaneously address the trade-off between exploration and exploitation, is one of the fundamental multi-armed bandit problems (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 236,
      "endOffset" : 267
    }, {
      "referenceID" : 27,
      "context" : "For linear multi-armed bandit, the Online Mirror Descent (OMD) algorithm is a powerful technology that can be used to achieve proper regret (Srebro et al., 2011).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "4 Hard Parameter-sharing MTL Model This paper adopts the most prevalent and efficient hard parameter-sharing MTL model (Kendall et al., 2018; Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2020b) to perform multi-task text classification.",
      "startOffset" : 119,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "4 Hard Parameter-sharing MTL Model This paper adopts the most prevalent and efficient hard parameter-sharing MTL model (Kendall et al., 2018; Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2020b) to perform multi-task text classification.",
      "startOffset" : 119,
      "endOffset" : 203
    }, {
      "referenceID" : 26,
      "context" : "4 Hard Parameter-sharing MTL Model This paper adopts the most prevalent and efficient hard parameter-sharing MTL model (Kendall et al., 2018; Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2020b) to perform multi-task text classification.",
      "startOffset" : 119,
      "endOffset" : 203
    }, {
      "referenceID" : 20,
      "context" : "4 Hard Parameter-sharing MTL Model This paper adopts the most prevalent and efficient hard parameter-sharing MTL model (Kendall et al., 2018; Chen et al., 2018; Sener and Koltun, 2018; Mao et al., 2020b) to perform multi-task text classification.",
      "startOffset" : 119,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "In multitask text classification, the feature extractor can be LSTM (Hochreiter and Schmidhuber, 1997), TextCNN (Kim, 2014), and so on.",
      "startOffset" : 68,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "In multitask text classification, the feature extractor can be LSTM (Hochreiter and Schmidhuber, 1997), TextCNN (Kim, 2014), and so on.",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "To handle the non-convexity, we select a convex surrogate for (3) based on its equivalent formulation (4) (Ben-Tal et al., 2013; Bertsimas et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "To handle the non-convexity, we select a convex surrogate for (3) based on its equivalent formulation (4) (Ben-Tal et al., 2013; Bertsimas et al., 2018).",
      "startOffset" : 106,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "When lt(·, ·) is convex and Θ is compact, the adversarial multi-armed bandit problem can achieve a saddle point (θ∗, p∗) (Boyd and Vandenberghe, 2014).",
      "startOffset" : 121,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "For a problem of this kind, mirror gradient descent (ascent) is a powerful technique for the adversary and the player to achieve proper regret (Bubeck and Cesa-Bianchi, 2012; Namkoong and Duchi, 2016).",
      "startOffset" : 143,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "For a problem of this kind, mirror gradient descent (ascent) is a powerful technique for the adversary and the player to achieve proper regret (Bubeck and Cesa-Bianchi, 2012; Namkoong and Duchi, 2016).",
      "startOffset" : 143,
      "endOffset" : 200
    }, {
      "referenceID" : 5,
      "context" : "Moreover, based on the mirror gradient ascent-descent, we can reach the saddle point of the minimax optimization problem when the task-specific loss functions are convex and the parameter space Θ is compact (Boyd and Vandenberghe, 2014).",
      "startOffset" : 207,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "The dataset (Blitzer et al., 2007) contains product reviews from 14 domains, including books, DVDs, electronics, kitchen appliances and so on.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "Uncertainty: using the uncertainty weighting method proposed by (Kendall et al., 2018).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "GradNorm: using the gradient normalization method proposed by (Chen et al., 2018).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "MGDA: using the MGDA-UB method proposed by (Sener and Koltun, 2018).",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "AdvMTL: using the adversarial Multi-task Learning method proposed by (Liu et al., 2017).",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Tchebycheff: using the Tchebycheff procedure proposed by (Mao et al., 2020b).",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "To ensure consistency with the state-of-the-art multi-task classification methods (Liu et al., 2017; Mao et al., 2020b) and ensure fair comparison, we adopt Pre-trained",
      "startOffset" : 82,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "To ensure consistency with the state-of-the-art multi-task classification methods (Liu et al., 2017; Mao et al., 2020b) and ensure fair comparison, we adopt Pre-trained",
      "startOffset" : 82,
      "endOffset" : 119
    }, {
      "referenceID" : 25,
      "context" : "GloVe (Pennington et al., 2014) word embeddings in our experimental analysis.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "We use the Adam optimizer (Kingma and Ba, 2015) and train over 3000 epochs for both sentiment analysis and topic classification.",
      "startOffset" : 26,
      "endOffset" : 47
    } ],
    "year" : 2021,
    "abstractText" : "Task variance regularization, which can be used to improve the generalization of Multitask Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multiarmed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multitask text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.",
    "creator" : "LaTeX with hyperref"
  }
}