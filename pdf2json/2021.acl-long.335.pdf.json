{
  "name" : "2021.acl-long.335.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Are Pre-trained Convolutions Better than Pre-trained Transformers?",
    "authors" : [ "Yi Tay", "Mostafa Dehghani", "Vamsi Aribandi", "Dara Bahri", "Zhen Qin" ],
    "emails" : [ "yitay@google.com", "dehghani@google.com", "jaigupta@google.com", "aribandi@google.com", "dbahri@google.com", "zhenqin@google.com", "metzler@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4349–4359\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4349"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the modern era of pre-training, there appears to be an unbreakable tie between Transformer architectures (Vaswani et al., 2017) and pre-trained language models. Models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2019) have all adopted Transformers as their underlying architecture. As a matter of fact, there are barely any recent pre-trained models not based on Transformers.\nWhile the contextual representation learning has a rich history (Pennington et al., 2014; Dai and Le,\n∗Google AI Resident\n2015; Chidambaram et al., 2018; Liu et al., 2020; Qiu et al., 2020), modern pre-trained language modeling started with models like ELMo (Peters et al., 2018) and CoVE (McCann et al., 2017) which are based on recurrent (e.g. LSTM (Hochreiter and Schmidhuber, 1997)) architectures. Although they were successful, research using these architectures dwindled as Transformers stole the hearts of the NLP community, having, possibly implicitly, been perceived as a unequivocal advancement over its predecessors.\nRecent work demonstrates the promise of entirely convolution-based models (Wu et al., 2019; Gehring et al., 2017) and questions the necessity of self-attentive architectures like Transformers. For example, in (Wu et al., 2019), the proposed convolutional seq2seq models outperform Transformers on a series of canonical benchmarks such as machine translation and language modeling. From these findings emerge a rather natural line of questioning - should we consider pre-trained models beyond Transformers?\nDespite early success, the relevance of convolutional models in the era of pre-trained language models remains an open question. To the best of our knowledge, convolutional architectures have not yet been rigorously evaluated under the pretrain-fine-tune paradigm. This is the primary purpose of this work. Concretely, this paper seeks to empirically validate whether pre-trained convolutions are competitive with pre-trained Transformers across a range of tasks.\nThe interaction between pre-training schemes and model architectures is an under-studied topic. Are only Transformers able to capitalize on the\nbenefits of pre-training? If we use a different architectural inductive bias, would there also be a substantial gain unlocked by pre-training? Are pretrained convolutions better in particular scenarios? This paper investigates these questions.\nThere are a number of obvious benefits of convolution-based models. Firstly, convolutions do not suffer from the quadratic memory complexity of self-attention - a problem significant enough that it spawned the creation of the entirely new category of “efficient” Transformer architectures (Tay et al., 2020b, 2021). Secondly, convolutions operate locally and do not rely on positional encodings as an order signal to the model. That said, convolutions also come with a slew of downsides. For example, being unable to access global information means such models are unable to perform a form of cross-attention across multiple sequences. We dive into the details of this more in subsequent sections.\nIn this paper, we present a pre-trained convolutional sequence-to-sequence, or Seq2Seq, model. We train our convolutional model using span-based sequence-to-sequence denoising objectives similar to those employed in T5 (Raffel et al., 2019). We evaluate a variety of convolutional variants (e.g., dilated, lightweight, dynamic (Wu et al., 2019), etc.) under both raw (no pre-training) and pre-train-finetune paradigms. Our goal is to understand the true competitiveness of convolutional architectures in the era of pre-training.\nWe show that pre-trained convolutions are competitive against pre-trained Transformers via a set of experiments on a potpourri of NLP tasks, like toxicity detection, sentiment classification, news classification, query understanding and semantic parsing/compositional generalization (Kim and Linzen, 2020). Moreover, we find that pretrained convolutions can outperform, in terms of model quality and training speed, state-of-the-art pre-trained Transformers (Raffel et al., 2019) in certain scenarios. However, to provide a balanced perspective, we also describe scenarios where pretrained convolutions do not perform well and may be deemed unsuitable.\nContributions Overall, the main contributions of this paper can be summarized as follows:\n• We perform a comprehensive empirical evaluation of convolutional Seq2Seq models under the pre-train-fine-tune paradigm. To the best of our knowledge, the competitiveness and\nrelevance of pre-trained convolutions still remains an open question.\n• We make several important observations. Specifically, we find that (1) pre-training helps convolutional models just as much as it helps Transformers, and (2) pre-trained convolutions are competitive alternatives in certain scenarios in terms of model quality and training speed.\n• We conduct extensive experiments across 8 datasets spanning a diverse range of tasks and domains. On 7 out of 8 tasks, we find that pre-trained convolutions outperform a recent state-of-the-art transformer (T5 (Raffel et al., 2019)) with and without pre-training. We examine the speed and operation count (FLOPS) of convolutions versus Transformers and find that convolutions are not only faster but also scale better to longer sequence lengths."
    }, {
      "heading" : "2 Related Work",
      "text" : "Pre-training on a large corpus has become the primary method of learning universal language representations to solve different downstream NLP tasks. The first generation of pre-trained models aimed at learning embedding for words, like Skip-Gram (Mikolov et al., 2013) and Glove (Pennington et al., 2014), and quickly developed to learning contextualized representation for words, like ELMO (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2018). This, however, is not the only axis in which pre-trained models have evolved.\nDifferent objective functions and various tasks, both supervised and unsupervised, have been explored for pre-training. For instance, CoVe (McCann et al., 2017) uses machine translation as the pre-training task, ELMO (Peters et al., 2018) and GPT (Radford et al., 2018) use language modeling objectives, BERT (Devlin et al., 2018) uses masked language modeling, T5 (Raffel et al., 2019) and MASS (Song et al., 2019) use Seq2Seq masked language modeling, and XLNet (Yang et al., 2019) utilizes permuted language modeling. In addition to this, BART (Lewis et al., 2019) uses a denoising autoencoder setup during pre-training, where the model takes a partially corrupted input and is trained to recover the original, undistorted input. Some models use a contrastive learning setup during pertaining, like replaced token detection, used\nby ELECTRA (Clark et al., 2020), and sentence order prediction, used by ALBERT (Lan et al., 2019) and StructBERT (Wang et al., 2019).\nAnother axis where pre-trained models in NLP explored different ideas is model architecture. ELMO (Peters et al., 2018) and CoVe (McCann et al., 2017) used LSTMs as the base model. Later, Transformers (Vaswani et al., 2017) became the de facto architecture of pre-trained NLP models. BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) use the Transformer encoder, while GPT (Radford et al., 2018), GPT-2 (Radford et al.), and GPT-3 (Brown et al., 2020) use the Transformer decoder as the backbone. Some pre-trained models are also are based on the encoder-decoder transformer architecture, like T5 (Raffel et al., 2019), MASS (Song et al., 2019), and BART (Lewis et al., 2019). In this paper, we investigate another model architecture variation by studying the power of convolutional neural network as the backbone of pre-trained models for NLP.\nConvolutions have always been an interesting choice for sequence modeling and NLP applications (Kim, 2014; Bai et al., 2018; Kalchbrenner et al., 2016). Convolutions are lightweight and fast and have many interesting use-cases, notably for lightweight classification. In the era when LSTMs were the workhorses of NLP applications, convolutions were positioned nicely on the pareto frontier of the compute-performance curve. They are fast and lightweight, and unlike Transformers, they do not suffer from quadratic complexity. Our work is also well-aligned with the resurgence of interest in convolutions where (Wu et al., 2019) showed that convolutions can outperform self-attention on several sequence transduction tasks. Moreover, the necessity of the self-attention inductive bias in transformers have been also a subject of recent interest. Synthesizer models (Tay et al., 2020a) showed that transformers can still do pretty well without token-token dot product self-attention and a random attention matrix can perform competitively on certain tasks."
    }, {
      "heading" : "3 Pre-Trained Convolution Models",
      "text" : "This section describes the pre-trained Convolution Model. For most of our experiments, we adopt depthwise separable convolutions (Kaiser et al., 2017; Sifre and Mallat, 2014; Chollet, 2017) which have shown to be fast and efficient variants of the\nstandard convolution."
    }, {
      "heading" : "3.1 Lightweight Depthwise Convolution",
      "text" : "This section introduces Lightweight Depthwise Convolutions (Wu et al., 2019) which forms the backbone of our pre-trained convolution model."
    }, {
      "heading" : "3.1.1 Depthwise convolutions",
      "text" : "Depthwise convolutions convolve independently over every channel. Given an input tensor X of dimensions n × d, the depthwise convolution, D(X,Wc,:, i, c) is defined as:\nOi,c = k∑\nj−1 Wc,j ·Xi+j−d k+1 2 e), c (1)\nwhere W ∈ Rd×k are the learnable parameters of the layer. Oi,c is the output at position i and channel c. The overall output is a tensor of n× d of identical shape as the input."
    }, {
      "heading" : "3.1.2 Lightweight Convolutions",
      "text" : "L(.) are depthwise separable convolutions with (1) softmax-normalized kernels and (2) shared output channels and weight tying. Specifically, this is written as:\nOLi,c = k∑ j−1 softmax(Wĉ,j) ·Xi+j−d k+1 2 e), ĉ (2)\nwhere ĉ = cHd . In short, parameters are shared every dH output channels. When H = 1, this is equivalent to sharing all the weights of all channels."
    }, {
      "heading" : "3.1.3 Dynamic Convolutions",
      "text" : "Dynamic Convolutions DY (.) are a new form of lightweight convolutions introduced by (Wu et al., 2019). The key idea is to learn position-specific kernels for performing lightweight convolutions. This can be written as:\nDY = L(X, f(Xi)h,:, i, c), (3)\nwhere f(.) is a linear transformation with parameters WQ ∈ RH×k×d that learns a position dependent kernel."
    }, {
      "heading" : "3.2 Span-based Seq2Seq pre-training",
      "text" : "We adopt span-based sequence-to-sequence pretraining as per (Raffel et al., 2019). Specifically, given an input sequence, we randomly mask spans of lengths L and replace them with a special sentinel token. The pre-training task is then to generate the masked tokens as targets. For example: Inputs: The happy cat sat [mask]. and Outputs: on the mat."
    }, {
      "heading" : "3.2.1 Convolutional Seq2Seq Architecture",
      "text" : "We implement a Seq2Seq (Sutskever et al., 2014) architecture similar to (Wu et al., 2019). The key difference when compared with Transformer architectures is that we replace the multi-headed selfattention with convolutional blocks. Instead of query-key-value transforms, we use gated linear unit projections following (Wu et al., 2019). Each convolution block be written as:\nX1 =W IX sigmoid(WSX), X2 = ConvBlock(X1),\nX3 =WO(X2),\nwhere W I ,WS ,WO are trainable parameters. We experiment with simple lightweight convolutions, dynamic convolutions and dilated convolutions in our experiments. Following (Wu et al., 2019; Gehring et al., 2017), the encoder-decoder attention remains untouched. The convention follows the backbone Transformer model in which we wrap each submodule with layer normalization and residual connectors. Hence, each Conv block is written as:\nXA = LayerNorm(Conv(X)) +X,\nXB = LayerNorm(FFN(XA) +XA,\nwhere Conv is any of the convolution models that we explore in our experiments. FFN(.) is a two layer feed-forward network with ReLU activations in the middle."
    }, {
      "heading" : "3.2.2 Optimization",
      "text" : "The model optimizes the token-wise cross-entropy loss and is trained with teacher forcing.\nL = L∑\nt=1 n∑ i=1 log(πti) + (1− yti) log(1− πti),\nwhere πti is the prediction of class i at time step t and yti is the ground truth label of the class i at time step t."
    }, {
      "heading" : "4 Research Questions and Discussion",
      "text" : "Before we delve into our experiments, we establish a set of research questions and agenda we hope this work aims to bring clarity to.\n• RQ1: Do convolutions benefit from pretraining as much as Transformers?\n• RQ2: Are convolutional models, pre-trained or otherwise, competitive with Transformer models? When do they perform well?\n• RQ3: What are the benefits (if any) of using pre-trained convolution models over pretrained Transformers? Are convolutions faster alternatives to self-attention based Transformers?\n• RQ4: What are the failure modes, caveats and reasons to not use pre-trained convolutions?\n• RQ5: Are certain convolution variants better than others?"
    }, {
      "heading" : "5 Experiments and Analysis",
      "text" : "This section presents our analysis and results."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "Our evaluation is based on the following datasets and tasks.\n• Toxicity Detection - We use the CIVIL COMMENTS (Borkan et al., 2019) and WIKI TOXIC SUBTYPES dataset (Wulczyn et al., 2017). Given a piece of short text (originating from social media or wikipedia), the goal is to determine if the content is toxic, i.e., a binary classification task. For this task, we evaluate on both accuracy and F1 score.\n• Sentiment Classification - This is a binary classification task that determines the polarity of documents, sentences and/or tweets. We use the IMDb reviews dataset (Maas et al., 2011), Stanford Sentiment Treebank (SST2) (Socher et al., 2013) dataset, along with Twitter Sentiment140 (S140) (Go et al., 2009) dataset.\n• News Classification - This is a task of topic categorization for news articles. We use the AGNews dataset (Zhang et al., 2015). This is a four-way classification task.\n• Question Classification We use the TREC fine-grained question classification dataset (Li and Roth, 2002). This task involves classifying questions into 46 fine-grained question categories.\n• Semantic Parsing / Compositional Generalization Compositional generalization is the\nability of models to generalize compositionally outside of the training distribution. To be specific, it needs be able to handle unseen combinations at test time. For this task, we use the COGS dataset (Kim and Linzen, 2020), a task of generating semantic representation of a given English sentence. For example, A cat smiled→ cat(x1) AND smile.agent(x2, x1).\nAll of the datasets, with the exception of the recent COGS dataset (Kim and Linzen, 2020), are Tensorflow datasets1.\nFor each dataset, we evaluate all models with and without pre-training (details in subsequent sections). Table 1 reports the statistics of the datasets used in this paper."
    }, {
      "heading" : "5.2 Experimental Setup",
      "text" : "This section describes our experimental setup."
    }, {
      "heading" : "5.2.1 Models",
      "text" : "Our models are largely based on sequence to sequence models, a paradigm that has demonstrated great success made evident by models such as BART (Lewis et al., 2019) and T5(Raffel et al., 2019). We implement our models in Mesh Tensorflow (MTF) (Shazeer et al., 2018), a library for distributed and efficient parallel model training that has similar API to Tensorflow. We train models that are of base size, which corresponds to 12 layers each in the encoder and decoder, along with 3072 dimensions for the feed-forward layers, a model dimension of 768 and a total of 12 heads. Our Transformer models are largely based on T5 (Raffel et al., 2019), which is considered the current state-of-the-art Transformer model for NLP tasks and hence serves as a strong baseline. For the convolution models, our lightweight convolution\n1https://www.tensorflow.org/datasets/ catalog/overview.\nand dynamic convolution models have a window size2 of 7 across all layers, the number of unique depth filters is 2. For dilated models, we use a filter size of [4, 4, 7, 7, 15, 15, 15, 15, 31, 31, 31] for our 12 layer convolution model."
    }, {
      "heading" : "5.2.2 Pre-training",
      "text" : "We pre-train both our convolutional and Transformer models for 524K steps with a batch size of 128. Given the input sequence length of 512, this corresponds to 65536 tokens per batch. For pre-training, we use the Colossal Cleaned CommonCrawl Corpus (C4) (Raffel et al., 2019) dataset which has demonstrated impressive results on downstream tasks. We use the span based seq2seq objective as the pre-training objective as mentioned in earlier sections. The span size is set to 3 and a corruption rate of 15% is adopted. We use the Adafactor optimizer (Shazeer and Stern, 2018) with an inverse square root learning rate scheduler. Each pre-training run is performed using 16 TPU-v3 chips and takes approximately 12 hours to complete for models of base size."
    }, {
      "heading" : "5.2.3 Downstream Fine-tuning",
      "text" : "We fine-tune the pre-trained models using the following set of hyperparameters: We use a constant learning rate which is tuned amongst {0.001, 0.0005, 0.0001}. The batch size is generally set to 64 but occasionally set to 32 for smaller datasets. Intuitively, sequence length is task dependent but generally approximately the 90th percentile for each task. We fine-tune for a maximum of 100K steps and report peak validation performance. Fine-tuning uses the same Adafactor optimizer as during training. We perform fine-tuning on similar hardware, i.e., typically 16 TPUv3 chips are used per fine-tuning job."
    }, {
      "heading" : "5.3 Experimental Results",
      "text" : "This section describes our experimental setup and results."
    }, {
      "heading" : "5.4 Results on Toxicity Detection",
      "text" : "Table 2 reports results on toxicity detection. On both toxicity detection datasets the pre-trained and no-pre-training (raw) setup, the best models are the dilated convolution models and the dynamic convolution models. In fact, all convolutional models\n2We believe that tuning the hyperparameters of the convolution models can result in even better performance. However, we decided to keep these hyperparameters simple for the start.\noutperform Transformers on both CivilComments and WikiToxic. Before pre-training, convolutions outperform Transformers by approximately 1.5 absolute percentage points. The gap narrows after pretraining where Transformers see a better gain (e.g., +5.1% against +4.3%) from pre-training over convolutions on the CivilComments dataset. However, the converse is true on WikiToxic - the only case of performance degradation after pre-training. Overall, on this task, convolutions are competitive to Transformers and outperform them."
    }, {
      "heading" : "5.5 Results on Sentiment Classification",
      "text" : "Results on Sentiment Classification (IMDb, SST-2 and S140) can be found in Table 2. On the IMDb reviews dataset, the best non-pre-trained model is the lightweight convolution model, outperforming the Transformer model. The best pre-trained model is the Transformer model. However, all convolutional models come in close with less than a percentage point gap difference with pre-trained Transformers. On the SST-2 and S140 tasks, we observe that the best models are convolution-based, regardless of whether the model is pre-trained or not."
    }, {
      "heading" : "5.6 Results on Question Classification",
      "text" : "The best non-pre-trained model is the Lightweight Convolution model. For pre-trained models, convolutional models also outperform the pre-trained Transformer. On this task, while most models benefit significantly from pre-training, Transformers seem to benefit slightly more from pre-training."
    }, {
      "heading" : "5.7 Results on News Classification",
      "text" : "Results on news classification seems to follow similar trends as other benchmarks. Convolutional models outperform Transformers both in non-pretrained and pre-trained setups. The highest gain from pre-training is obtained from the dilated convolution model."
    }, {
      "heading" : "5.8 Results on Compositional Generalization Challenge and Semantic Parsing",
      "text" : "We conduct additional experiments on semantic parsing and compositional generalization. The task is framed as a sequence generation task. We use the recently proposed (Kim and Linzen, 2020) dataset. On the in-distribution test set, Transformers and convolutions have identical performance (95%). On the generalization or out of distribution set, Transformers perform at 77.5% while convolutions\ncome in at 76.9. While convolutions do not exactly outperform Transformers, they come in close enough to be considered competitive."
    }, {
      "heading" : "5.9 Summary of Results",
      "text" : "On the seven tasks across a broad range of domains we find that (1) non-pre-trained convolutions are competitive and frequently outperform non-pretrained Transformers, (2) pre-trained convolutions outperform pre-trained Transformers on six out of seven tasks. This answers RQ2.\nWe also find that convolutions are able to benefit from pre-training, in a similar fashion to self-attention-based models. Hence, the benefits achieved by pre-training are not exclusive to Transformer models. This answers RQ1.\nAmongst the pre-trained convolutional models, we find that dilated convolutions and dynamic convolutions are generally better than lightweight convolutions, thus answering RQ5.\nFinally, we observe that relative performance (i.e., rankings) do change with pre-training. This definitely shows that there is some kind of effect from composing architectures with pre-training. The direct implication of this effect is that a model that performs well (relatively) without pre-training will not necessarily perform the best when pretrained (and vice versa). Hence, aside from conflating architectures with pre-training schemes, we do also need to take note that different architectures may behave differently under pre-training."
    }, {
      "heading" : "6 Discussion and Analysis",
      "text" : "This section expands on the results via a detailed analysis and discussion. We discuss the pros/cons of pretrained convolutions, the impact of pretraining on performance and also recommendations to the broader community.\n6.1 When do we expect pre-trained convolutions to fail?\nIn our experimental section, we observed the potential upsides of convolutional models over wellestablished pre-trained Transformers and observe that we are able to get quality improvements in certain cases. However, it might be good to further understand the drawbacks of convolutions.\nOne obvious weakness of pre-trained convolutions are their lack of cross-attention inductive bias that comes for free with self-attention in the Transformer encoder. For this reason, it is not a\ngood idea to use pre-trained convolutions for tasks that requires modeling the relationship between two or more sequences. To verify this, we run experiments on SQuAD and MultiNLI and find that convolutions do not come close to Transformers just because of this missing inductive bias. This should be clearly distinguished when examining and evaluating models, as how the early SNLI leaderboard3 distinguished between models that used cross-attention and models that did not.\nOur initial evaluations on benchmarks like SQuAD/MNLI (Rajpurkar et al., 2016; Williams et al., 2017) showed that pre-trained convolutions are indeed significantly lackluster. For example, convolutions only achieve ≈ 75% accuracy on MultiNLI, while transformers easily achieve ≈ 84% accuracy. Likewise, while transformers achieve about ≈ 90% F1 on SQuAd, convolutions come in around ≈ 70%. This is entirely expected because there is no way the premise/question can interact with the hypothesis/context. (RQ4). However, our experiments show that this was only because they lack this cross-attention property. When we augment convolutions with a single layer of cross attention at the encoder, we find that pre-trained convolutions come close (a delta of\n3https://nlp.stanford.edu/projects/ snli/\n(≈ 1%)) to pre-trained Transformers on datasets such as MultiNLI (Williams et al., 2017), achieving about ≈ 83% accuracy.\nThat said, we leave it to the practitioner to decide whether the cross-attention inductive bias is actually important for the problem at hand. We also like to emphasize that the pattern of concatenating sentence pairs is not necessary practical when scaling up since this requires inference on every permutation of sentence pairs. For this reason, dual encoder setups that do fast embedding space look-ups are more practical and feasible in practice (Guo et al., 2020). Given the strong performance of convolutions in a series of encoding tasks, we can expect pre-trained convolutions to do well in a dual encoder setup."
    }, {
      "heading" : "6.2 What are the benefits of pre-trained convolutions over Transformers?",
      "text" : "We observed a reasonable quality improvement from using convolutions over Transformers. This section discusses the additional benefit."
    }, {
      "heading" : "6.2.1 Convolutions are faster and scale better to long sequences",
      "text" : "Figure 1 reports training speed of convolution (LightConvs) versus transformers on a sequence to sequence task. The input lengths are varied from {64, 128, 256, 512, 1024, 2048, 4096}. We\nshow that convolutions are not only consistently faster (even at shorter sequences) but scale better than transformers. Convolution scales linearly while transformers are not able to scale to longer sequences."
    }, {
      "heading" : "6.2.2 Convolutions are FLOPs efficient",
      "text" : "We measure the number of FLOPs of convolutions versus transformers as we increase the sequence length. Figure 2 shows the phenomenon while varying sequence length. In general, across all sequence lengths, convolutions are more efficient in the number of floating point operations.\nThe overall findings that convolutions are faster both in wall clock time and in FLOPs answers RQ3.\nMoreover, we find that the FLOP efficiency of convolutions scales better across sequence lengths.\n6.3 Are we suggesting to completely replace Transformers with convolution?\nWhile Transformers have dominated the research landscape in NLP, this paper suggests that there are commonly overlooked benefits to convolutions such as model quality, speed, FLOPs and scalability. Moreover, it is previously unknown to whether convolutions benefit from pre-training. In this paper, we showed that they are competitive on some tasks and also benefit from pre-training in similar fashion to transformer models. However, on the flip side, we also highlighted that they are unable to handle tasks that require cross-attention or when there is a need to model > 1 sentence or documents within the same sequence. We believe that practitioners have good options and it might be worthwhile to explore architectures outside the well-established transformer models."
    }, {
      "heading" : "6.4 On not conflating pre-training with architectural advances",
      "text" : "In this paper, we showed that three other (convolutional-based) architectures (e.g., lightweight, dymamic and dilated) also benefit from pre-training to the same extent as transformer models.\nIn the current research landscape, pre-training has always be tightly coupled and associated with transformers architectures. As a result, the success of BERT, transformers and large language models seem to be pretty conflated. While it is true that, to this date, the only model that large-scale pretraining has been applied to are transformer models, we believe there might be potential in other architectures.\nBased on our empirical findings, we believe there is still significant room for the improving the understanding of the compositional effects of architecture and pre-training. Hence, we believe that the impact of this work extends beyond showing the competitiveness of convolution models in NLP. More concretely, the take home message is that there should be a healthy level of optimism in exploring architectural alternatives."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we conducted an extensive study of the viability and feasibility of pre-trained convolu-\ntions. Our experimental results show that convolutions can outperform Transformers in both pretrain and non-pre-trained setups. Our extensive experiments across 8 datasets spanning a diverse range of tasks, show that convolutions are able to benefit from pre-training to the same (or sometimes greater) extent than Transformers. While pre-trained transformers are the de-facto choice of architecture, our results show that they might not be the best in certain scenarios. Additionally, we discussed the caveats, trade-offs pertaining with runtime, scalability, number of FLOPS and model quality. Finally, we discussed the situations or data types that convolutions are not well equipped to handle and make an empirically informed recommendation for practitioners."
    } ],
    "references" : [ {
      "title" : "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "author" : [ "Shaojie Bai", "J Zico Kolter", "Vladlen Koltun." ],
      "venue" : "arXiv preprint arXiv:1803.01271.",
      "citeRegEx" : "Bai et al\\.,? 2018",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2018
    }, {
      "title" : "Nuanced metrics for measuring unintended bias with real data for text classification",
      "author" : [ "Daniel Borkan", "Lucas Dixon", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "CoRR, abs/1903.04561.",
      "citeRegEx" : "Borkan et al\\.,? 2019",
      "shortCiteRegEx" : "Borkan et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning cross-lingual sentence representations via a multi-task dual-encoder model",
      "author" : [ "Muthuraman Chidambaram", "Yinfei Yang", "Daniel Cer", "Steve Yuan", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "arXiv preprint arXiv:1810.12836.",
      "citeRegEx" : "Chidambaram et al\\.,? 2018",
      "shortCiteRegEx" : "Chidambaram et al\\.",
      "year" : 2018
    }, {
      "title" : "Xception: Deep learning with depthwise separable convolutions",
      "author" : [ "François Chollet." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251–1258.",
      "citeRegEx" : "Chollet.,? 2017",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2017
    }, {
      "title" : "Electra: Pre-training text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V Le", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:2003.10555.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Semisupervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1511.01432.",
      "citeRegEx" : "Dai and Le.,? 2015",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin." ],
      "venue" : "arXiv preprint arXiv:1705.03122.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Twitter sentiment classification using distant supervision",
      "author" : [ "Alec Go", "Richa Bhayani", "Lei Huang." ],
      "venue" : "CS224N project report, Stanford, 1(12):2009.",
      "citeRegEx" : "Go et al\\.,? 2009",
      "shortCiteRegEx" : "Go et al\\.",
      "year" : 2009
    }, {
      "title" : "Accelerating large-scale inference with anisotropic vector quantization",
      "author" : [ "Ruiqi Guo", "Philip Sun", "Erik Lindgren", "Quan Geng", "David Simcha", "Felix Chern", "Sanjiv Kumar." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Depthwise separable convolutions for neural machine translation",
      "author" : [ "Lukasz Kaiser", "Aidan N Gomez", "Francois Chollet." ],
      "venue" : "arXiv preprint arXiv:1706.03059.",
      "citeRegEx" : "Kaiser et al\\.,? 2017",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation in linear time",
      "author" : [ "Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu." ],
      "venue" : "arXiv preprint arXiv:1610.10099.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2016",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Cogs: A compositional generalization challenge based on semantic interpretation",
      "author" : [ "Najoung Kim", "Tal Linzen." ],
      "venue" : "arXiv preprint arXiv:2010.05465.",
      "citeRegEx" : "Kim and Linzen.,? 2020",
      "shortCiteRegEx" : "Kim and Linzen.",
      "year" : 2020
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Lin-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "COLING 2002: The 19th International Conference on Computational Linguistics.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "A survey on contextual embeddings",
      "author" : [ "Qi Liu", "Matt J Kusner", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:2003.07278.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: Human lan-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Learned in translation: Contextualized word vectors",
      "author" : [ "Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Advances in neural information processing systems, pages 6294–6305.",
      "citeRegEx" : "McCann et al\\.,? 2017",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2017
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1310.4546.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1802.05365.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Pre-trained models for natural language processing: A survey",
      "author" : [ "Xipeng Qiu", "Tianxiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang." ],
      "venue" : "Science China Technological Sciences, pages 1–26.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Mesh-tensorflow: Deep learning for supercomputers",
      "author" : [ "Noam Shazeer", "Youlong Cheng", "Niki Parmar", "Dustin Tran", "Ashish Vaswani", "Penporn Koanantakool", "Peter Hawkins", "HyoukJoong Lee", "Mingsheng Hong", "Cliff Young" ],
      "venue" : null,
      "citeRegEx" : "Shazeer et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shazeer et al\\.",
      "year" : 2018
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "arXiv preprint arXiv:1804.04235.",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "Rigidmotion scattering for image classification",
      "author" : [ "Laurent Sifre", "Stéphane Mallat" ],
      "venue" : null,
      "citeRegEx" : "Sifre and Mallat.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sifre and Mallat.",
      "year" : 2014
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "arXiv preprint arXiv:1905.02450.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Synthesizer: Rethinking self-attention in transformer models",
      "author" : [ "Yi Tay", "Dara Bahri", "Donald Metzler", "Da-Cheng Juan", "Zhe Zhao", "Che Zheng." ],
      "venue" : "arXiv preprint arXiv:2005.00743.",
      "citeRegEx" : "Tay et al\\.,? 2020a",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Long range arena : A benchmark for efficient transformers",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Samira Abnar", "Yikang Shen", "Dara Bahri", "Philip Pham", "Jinfeng Rao", "Liu Yang", "Sebastian Ruder", "Donald Metzler." ],
      "venue" : "International Conference on Learning",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "arXiv preprint arXiv:2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020b",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Structbert: Incorporating language structures into pretraining for deep language understanding",
      "author" : [ "Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Zuyi Bao", "Jiangnan Xia", "Liwei Peng", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:1908.04577.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1704.05426.",
      "citeRegEx" : "Williams et al\\.,? 2017",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2017
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann N Dauphin", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1901.10430.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, WWW ’17, pages 1391–1399, Republic and Canton of Geneva, CHE. International",
      "citeRegEx" : "Wulczyn et al\\.,? 2017",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "In the modern era of pre-training, there appears to be an unbreakable tie between Transformer architectures (Vaswani et al., 2017) and pre-trained language models.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "Models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : ", 2018), RoBERTa (Liu et al., 2019), and T5 (Raffel et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : ", 2019), and T5 (Raffel et al., 2019) have all adopted Transformers as their underlying architecture.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : ", 2020), modern pre-trained language modeling started with models like ELMo (Peters et al., 2018) and CoVE (McCann et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : ", 2018) and CoVE (McCann et al., 2017) which are based on recurrent (e.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 42,
      "context" : "Recent work demonstrates the promise of entirely convolution-based models (Wu et al., 2019; Gehring et al., 2017) and questions the necessity of self-attentive architectures like Transformers.",
      "startOffset" : 74,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Recent work demonstrates the promise of entirely convolution-based models (Wu et al., 2019; Gehring et al., 2017) and questions the necessity of self-attentive architectures like Transformers.",
      "startOffset" : 74,
      "endOffset" : 113
    }, {
      "referenceID" : 42,
      "context" : "For example, in (Wu et al., 2019), the proposed convolutional seq2seq models outperform Transformers on a series of canonical benchmarks such as machine translation and language modeling.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 28,
      "context" : "We train our convolutional model using span-based sequence-to-sequence denoising objectives similar to those employed in T5 (Raffel et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 42,
      "context" : ", dilated, lightweight, dynamic (Wu et al., 2019), etc.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "petitive against pre-trained Transformers via a set of experiments on a potpourri of NLP tasks, like toxicity detection, sentiment classification, news classification, query understanding and semantic parsing/compositional generalization (Kim and Linzen, 2020).",
      "startOffset" : 238,
      "endOffset" : 260
    }, {
      "referenceID" : 28,
      "context" : "Moreover, we find that pretrained convolutions can outperform, in terms of model quality and training speed, state-of-the-art pre-trained Transformers (Raffel et al., 2019) in certain scenarios.",
      "startOffset" : 151,
      "endOffset" : 172
    }, {
      "referenceID" : 28,
      "context" : "On 7 out of 8 tasks, we find that pre-trained convolutions outperform a recent state-of-the-art transformer (T5 (Raffel et al., 2019)) with and without pre-training.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "els aimed at learning embedding for words, like Skip-Gram (Mikolov et al., 2013) and Glove (Pennington et al.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : ", 2013) and Glove (Pennington et al., 2014), and quickly developed to learning contextualized representation for words, like ELMO (Peters et al.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : ", 2014), and quickly developed to learning contextualized representation for words, like ELMO (Peters et al., 2018), GPT (Radford",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "For instance, CoVe (McCann et al., 2017) uses machine translation as the pre-training task, ELMO (Peters et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : ", 2017) uses machine translation as the pre-training task, ELMO (Peters et al., 2018) and GPT (Radford et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 27,
      "context" : ", 2018) and GPT (Radford et al., 2018) use language modeling objectives, BERT (Devlin et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ", 2018) use language modeling objectives, BERT (Devlin et al., 2018) uses masked language modeling, T5 (Raffel et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : ", 2018) uses masked language modeling, T5 (Raffel et al., 2019) and MASS (Song et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 34,
      "context" : ", 2019) and MASS (Song et al., 2019) use Seq2Seq masked language modeling, and XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 44,
      "context" : ", 2019) use Seq2Seq masked language modeling, and XLNet (Yang et al., 2019) utilizes permuted language modeling.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "In addition to this, BART (Lewis et al., 2019) uses a denoising autoencoder setup during pre-training, where the model takes a partially corrupted input and is trained to recover the original, undistorted input.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "4351 by ELECTRA (Clark et al., 2020), and sentence order prediction, used by ALBERT (Lan et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : ", 2020), and sentence order prediction, used by ALBERT (Lan et al., 2019) and StructBERT (Wang et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : ", 2018) and CoVe (McCann et al., 2017) used LSTMs as the base model.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 39,
      "context" : "Later, Transformers (Vaswani et al., 2017) became the de facto architecture of pre-trained NLP models.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 44,
      "context" : ", 2018), XLNet (Yang et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019) use the Transformer encoder, while GPT (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2019) use the Transformer encoder, while GPT (Radford et al., 2018), GPT-2 (Radford et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "), and GPT-3 (Brown et al., 2020) use the Transformer decoder as the backbone.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 28,
      "context" : "Some pre-trained models are also are based on the encoder-decoder transformer architecture, like T5 (Raffel et al., 2019), MASS (Song et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 34,
      "context" : ", 2019), MASS (Song et al., 2019), and BART (Lewis et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Convolutions have always been an interesting choice for sequence modeling and NLP applications (Kim, 2014; Bai et al., 2018; Kalchbrenner et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "Convolutions have always been an interesting choice for sequence modeling and NLP applications (Kim, 2014; Bai et al., 2018; Kalchbrenner et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "Convolutions have always been an interesting choice for sequence modeling and NLP applications (Kim, 2014; Bai et al., 2018; Kalchbrenner et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 151
    }, {
      "referenceID" : 42,
      "context" : "Our work is also well-aligned with the resurgence of interest in convolutions where (Wu et al., 2019) showed that convolutions can outperform self-attention on several sequence transduction tasks.",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : "Synthesizer models (Tay et al., 2020a) showed that transformers can still do pretty well without token-token dot product self-attention and a random attention matrix can perform competitively on certain tasks.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "For most of our experiments, we adopt depthwise separable convolutions (Kaiser et al., 2017; Sifre and Mallat, 2014; Chollet, 2017) which have shown to be fast and efficient variants of the standard convolution.",
      "startOffset" : 71,
      "endOffset" : 131
    }, {
      "referenceID" : 32,
      "context" : "For most of our experiments, we adopt depthwise separable convolutions (Kaiser et al., 2017; Sifre and Mallat, 2014; Chollet, 2017) which have shown to be fast and efficient variants of the standard convolution.",
      "startOffset" : 71,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "For most of our experiments, we adopt depthwise separable convolutions (Kaiser et al., 2017; Sifre and Mallat, 2014; Chollet, 2017) which have shown to be fast and efficient variants of the standard convolution.",
      "startOffset" : 71,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "This section introduces Lightweight Depthwise Convolutions (Wu et al., 2019) which forms the backbone of our pre-trained convolution model.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 42,
      "context" : ") are a new form of lightweight convolutions introduced by (Wu et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "We adopt span-based sequence-to-sequence pretraining as per (Raffel et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 35,
      "context" : "We implement a Seq2Seq (Sutskever et al., 2014) architecture similar to (Wu et al.",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 42,
      "context" : "Instead of query-key-value transforms, we use gated linear unit projections following (Wu et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 42,
      "context" : "Following (Wu et al., 2019; Gehring et al., 2017), the encoder-decoder attention remains untouched.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Following (Wu et al., 2019; Gehring et al., 2017), the encoder-decoder attention remains untouched.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "• Toxicity Detection - We use the CIVIL COMMENTS (Borkan et al., 2019) and WIKI TOXIC SUBTYPES dataset (Wulczyn et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 43,
      "context" : ", 2019) and WIKI TOXIC SUBTYPES dataset (Wulczyn et al., 2017).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "use the IMDb reviews dataset (Maas et al., 2011), Stanford Sentiment Treebank (SST2) (Socher et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : ", 2011), Stanford Sentiment Treebank (SST2) (Socher et al., 2013) dataset, along with Twitter Sentiment140 (S140) (Go et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : ", 2013) dataset, along with Twitter Sentiment140 (S140) (Go et al., 2009) dataset.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "• Question Classification We use the TREC fine-grained question classification dataset (Li and Roth, 2002).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "For this task, we use the COGS dataset (Kim and Linzen, 2020), a task of generating semantic representation of a given English sentence.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "All of the datasets, with the exception of the recent COGS dataset (Kim and Linzen, 2020), are Tensorflow datasets1.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Our models are largely based on sequence to sequence models, a paradigm that has demonstrated great success made evident by models such as BART (Lewis et al., 2019) and T5(Raffel et al.",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 30,
      "context" : "We implement our models in Mesh Tensorflow (MTF) (Shazeer et al., 2018), a library for distributed and efficient parallel model training that has similar API to Tensorflow.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "Our Transformer models are largely based on T5 (Raffel et al., 2019), which is considered the current state-of-the-art Transformer model for NLP tasks and hence serves as a strong baseline.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "For pre-training, we use the Colossal Cleaned CommonCrawl Corpus (C4) (Raffel et al., 2019) dataset which has demonstrated impressive results on downstream tasks.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "We use the Adafactor optimizer (Shazeer and Stern, 2018) with an inverse square root learning rate scheduler.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "We use the recently proposed (Kim and Linzen, 2020) dataset.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "Our initial evaluations on benchmarks like SQuAD/MNLI (Rajpurkar et al., 2016; Williams et al., 2017) showed that pre-trained convolutions are indeed significantly lackluster.",
      "startOffset" : 54,
      "endOffset" : 101
    }, {
      "referenceID" : 41,
      "context" : "Our initial evaluations on benchmarks like SQuAD/MNLI (Rajpurkar et al., 2016; Williams et al., 2017) showed that pre-trained convolutions are indeed significantly lackluster.",
      "startOffset" : 54,
      "endOffset" : 101
    }, {
      "referenceID" : 41,
      "context" : "snli/ (≈ 1%)) to pre-trained Transformers on datasets such as MultiNLI (Williams et al., 2017), achieving about ≈ 83% accuracy.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "For this reason, dual encoder setups that do fast embedding space look-ups are more practical and feasible in practice (Guo et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 137
    } ],
    "year" : 2021,
    "abstractText" : "In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.",
    "creator" : "LaTeX with hyperref"
  }
}