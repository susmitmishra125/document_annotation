{
  "name" : "2021.acl-long.183.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning",
    "authors" : [ "Li Du", "Xiao Ding", "Kai Xiong", "Ting Liu", "Bing Qin" ],
    "emails" : [ "tliu@ir.hit.edu.cn", "qinb@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2354–2363\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2354"
    }, {
      "heading" : "1 Introduction",
      "text" : "Causal reasoning aims at understanding the general causal dependency between the cause and effect (Luo et al., 2016). Causality is commonly expressed by humans in the text of natural language, and is of great value for various Artificial Intelligence applications, such as question answering (Oh et al., 2013), event prediction (Li et al., 2018), and decision making (Sun et al., 2018).\nPrevious work mainly learns causal knowledge from manually annotated causal event pairs, and achieves promising performances (Luo et al., 2016;\n∗Corresponding author\nXie and Mu, 2019a; Li et al., 2019). However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018). Specifically, training data may contain exploitable superficial cues that are correlative of the expected output. The main concern is that these works have not learned the underlying mechanism of causation so that their inference models are not stable enough and their results are not explainable.\nWhile we notice that there is plentiful evidence information outside the given corpus that can provide more clues for understanding the logical law of the causality. Figure 1 (a) exemplifies two clues I1 : Excess Liquidity and I2: Invest Demand Increase for explaining how a: Quantitative Easing gradually leads to b: House Price Increases.\nWithout these important evidence information, on the other hand, as illustrated in Figure 1 (b),\nthe causal relationship between 〈a, d〉 and between 〈c, b〉 could not be deducted from the known causation between 〈a, b〉 and between 〈c, d〉. In contrast, with intermediate event I in hand, according to the transitivity of causality (Hall, 2000), the logic chain of 〈a ⇒ i ⇒ d〉 and 〈c ⇒ i ⇒ b〉 could be naturally derived from the observed logic chain 〈a⇒ i⇒ b〉 and 〈c⇒ i⇒ d〉.\nTo fully exploit the potential of the evidence information, we present an Event graph knowledge enhanced explainable CAusal Reasoning (ExCAR) framework. In particular, as illustrated in Figure 1 (c), given an input event pair 〈C,E〉, ExCAR firstly retrieves external evidence events such as I1, I2 from a large-scale causal event graph (CEG, a causal knowledge base constructed by us), and defines the causation between C, I1, I2, E as a set of logical rules (e.g., ri = (Ei ⇒ Ii)), which rules are useful representations for the causal reasoning task because they are interpretable and can provide insight to inference results.\nPearl (2001) pointed out that the underlying logic of causality is a probabilistic logic. The advantage of using a probabilistic logic is that by equipping logical rules with probability, one can better model statistically complex and noisy data. However, learning such probabilistic logical rules in the causal reasoning scenario is quite difficult —- it requires modeling the superimposed causal effect for each logical rule. Different from firstorder logical rules induced from some knowledge graphs, the probability of the logical rule (i.e. the causal strength of the cause-effect pair) in causal reasoning is uncertain, which varies with different antecedents. For example, as shown in Figure 1 (d), with the antecedent A: Catch a cold, a fever can hardly lead to life danger. While if fever is caused by the antecedent B: Septicemia, it can result in life danger with a high probability.\nTo address this issue, we further propose a Conditional Markov Neural Logic Network (CMNLN) for learning the conditional causal dependency of logical rules in an end-to-end fashion. Specifically, CMNLN first decomposes the logical rules set derived from the CEG into several distinct logic chains and learns a distributed representation for each logic chain in an embedding space. Subsequently, CMNLN estimates the conditional probability of each logical rule by an antecedent-aware potential function. Then CMNLN computes the probability of each logic chain by multiplying the\nprobabilities of logical rules in the chain. Finally, CMNLN predicts the causality score of the input event pair based on the disjunction of chain-level causality information.\nExperimental results show that our approach can effectively utilize the event graph information to improve the accuracy of causal reasoning by more than 5%. Adversarial evaluation and human evaluation show that ExCAR can achieve stable and explainable performance. The code is released at https://github.com/sjcfr/ExCAR."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Task Formalization",
      "text" : "In this paper, both the COPA (Luo et al., 2016) and the C-COPA causal reasoning task are defined as a multiple-choice task. Specifically, as the following example shows, given a premise event, one needs to choose a more plausible cause (effect) from two hypothesis events. Example:\nPremise: The company lost money.\nAsk-for: Cause\nHypothesis 1: Its products received favorable comments.\nHypothesis 2: Some of its products were defective.\nTherefore, the causal reasoning task could be formalized as a prediction problem: given a causeeffect event pair 〈C,E〉 composed by the premise event and one of the hypothesis events, the prediction model is required to predict a score measuring the causality of the event pair."
    }, {
      "heading" : "2.2 Causal Event Graph",
      "text" : "CEG is a large-scale causal knowledge base constructed by us, from which we can retrieve a set of additional evidences for a given cause-effect event pair 〈C,E〉. Formally, CEG is a directed acyclic graph and can be denoted as G = {V,R}, where V is the node set, R is the edge set. Each node Vi ∈ V corresponds to an event, while each edge Rij ∈ R denotes that there is a causal relationship between the ith event and jth event."
    }, {
      "heading" : "2.3 Rule-based Reasoning Using Markov Logic Network",
      "text" : "In this paper, to enhance the explainability and stability of causal reasoning, we cast the causal reasoning problem as a rule based reasoning task. Specifically, given an input causal event pair 〈C,E〉, we retrieve a set of evidence events from the CEG. The evidence events together withC andE further form\ninto a set of causal logical rules, where a rule describes the causal relationship between two events. Formally, a rule ri = (ei1 ⇒ ei2), where⇒ is a logical connective indicating the causal relationship between two events ei1 and ei2 . With regard to these causal logical rules, the causal mechanism can be revealed and the causal reasoning can be conducted in an explainable way.\nHowever, the underlying logic is a probabilistic logic. Markov Logic Network (MLN) (Pearl, 1988) can model such uncertainty by assigning each causal rule a causal strength, which measures the probability that this rule holds true. Let P (ri) denote the causal strength of rule ri. MLN estimates P (ri) using a potential function φ(ri). Thereafter, the causality score Y is predicted by simply multiplying the causal strength of obtained rules:\nP (Y ) = 1\nZ ∏ i P (ri) = 1 Z ∏ i φ(ri), (1)\nwhere 1Z is a normalization constant. However, there still remains two challenges for rule-based causal reasoning using MLN: 1) MLN defines potential functions as linear combinations of some hand-crafted features; 2) MLN cannot model the influence of antecedents of rules. Different from MLN, in this paper, we propose a Conditional Markov Neural Logic Network, which works on the embedding space of logic rules to model the conditional causal strength of rules."
    }, {
      "heading" : "3 Method",
      "text" : "As shown in Figure 2, ExCAR consists of two components. Given an event pair 〈C,E〉, ExCAR employs an evidence retrieval module to retrieves evidence events from a prebuilt causal event graph to generate a set of logical rules. Then ExCAR conducts causal reasoning based on the logical rules using a Conditional Markov Neural Logic Network."
    }, {
      "heading" : "3.1 Evidence Events Retrieval",
      "text" : "Given an event pair 〈C,E〉 outside the causal event graph, to obtain the evidences from the CEG, we first locate the cause and effect in the CEG. Intuitively, semantically similar events would have similar causes and effects, and share similar locations in the CEG. To this end, we employ a pretrained language model ELMo (Peters et al., 2018) to derive the semantic representation for events in the CEG, as well as the cause and effect event. Then events in the CEG which are semantically similar to the input cause and effect event can be found using cosine similarity of the semantic representations. These events can serve as anchors for locating the cause and effect event. Then as Figure 2 shows, taking the anchors of the cause event as start points, and taking the anchors of the effect event as end points, the evidence events can be retrieved by a Breadth First Search (BFS) algorithm.\nAfter the retrieving process, the cause, effect and evidence events constitute a causal logical graph (CLG) G∗ = {V ∗, R∗}, where V ∗ and R∗ is the node set and edge set, respectively. Each node ei within V ∗ is an event, each edge rj within R∗ describes the causal relationship between two events. TakingG∗ as the input, the following causal reasoning process is equipped with a set of logical rules for revealing the behind causal mechanism."
    }, {
      "heading" : "3.2 Conditional Markov Neural Logic Network",
      "text" : ""
    }, {
      "heading" : "3.2.1 Overview",
      "text" : "Given the CLG, we can derive a set of causal logical rules for supporting the causal reasoning process. However, as Figure 1 (d) shows, the causal strength of a rule may vary with different antecedents, where the antecedent can be an event, a simple rule or a complex of single rules. For clarity, we denote the antecedent of a rule ri as ANTEi. Influenced by a certain antecedent, the\ncausal strength of a rule can be described by a conditional probability P (ri|ANTEi).\nAs shown in Figure 2, a single rule derived from the CLG can have multiple antecedents, and each of these antecedents can have its own influence on the causal strength of the rule. To address this issue by exploiting the effectiveness of neural models in representation learning, we propose the CMNLN that works on the embeddings of logical rules. To model the superimposed causal effect of rules, CMNLN regards the CLG as a composition of distinct causal logic chains {ρ1, · · · , ρm}, and predicts causality score through combining information of each causal logic chain. Hence, within each causal logic chain, we can estimate a chainspecific causal strength for each rule rjk ∈ ρ\nj , using an antecedent-aware potential function. Then CMNLN aggregates the intra-chain causation information and inter-chain causation information to derive the causality score."
    }, {
      "heading" : "3.2.2 Logic Chain Generation",
      "text" : "For supporting the following reasoning process, we first explore the CLG to generate all possible causal logic chains {ρ1, · · · , ρm}. As shown in Figure 2, ρj = {rj1∧, · · · ,∧r j lj } describes a serial of transitive causal logical rules starting from the cause event C and ending at the effect event E.\nConsidering that each rule rjk ∈ ρ j is composed by two events ejk−1 and e j k , a causal logic chain ρj with lj rules contains totally lj + 1 events {ej0, · · · , e j lj } , where ej0 and ejlj are the cause event C and the effect event E, respectively. Taking C and E as the start and end point respectively, we can enumerate all distinct causal logic chains in the CLG using a Depth First Searching algorithm."
    }, {
      "heading" : "3.2.3 Event Encoding",
      "text" : "A BERT-based encoder (Devlin et al., 2019) is employed to encode all events within each causal logic chain into chain-specific distributed embeddings.\nSpecifically, for a causal logic chain ρj containing lj+1 events {ej0, · · · , ejlj}, we first process the event sequence into the form of: [CLS] ej0 · · · [CLS] e j k · · · [CLS] e j lj .\nAfter that, the processed event sequence is fed into BERT. We define the final hidden state of the [CLS] token before each event as the representation of the corresponding event. In this way, we obtain an event embedding set H = {hj0, · · · ,hjlj}, where hjk ∈ Rdis the embedding of the kth event within the causal logic chain ρj . Note that, hj0 is\nthe representation of the cause event C, and hjlj is the representation of the effect event E."
    }, {
      "heading" : "3.2.4 Chain-specific Conditional Causal Strength Estimation",
      "text" : "Given one of the causal logic chains ρj = (rj1∧, · · · ,∧r j lj ) and corresponding event representations H = {hj0, · · · ,hjlj}, CMNLN estimates the chain-specific causal strength for each rule using an antecedent-aware potential function.\nFor a rule rjk ∈ ρ j , we define the chain-wise\nantecedent of rjk as (r j 1∧r j 2∧, · · · ,∧r j k−1) , and denote it as ANTEjk. Therefore, with regard to ANTE j k, we can derive the chain-specific causal strength using an antecedent-aware potential function as:\nP (rjk|ANTE j k) = φa(r j k,ANTE j k). (2)\nConsidering that each logical rule rjkis composed of two events ejk−1and e j k, the input of φa(·) is the distributed representation of ANTEjk, and the embedding of ejk−1and e j k. We denote the representation of ANTEjkas s j k, and describe the specific process for deriving sjk in the following section. Given sjk, h j k−1 and h j k, to model the influence of ANTEjk, we first derive antecedent-aware representations of ejk−1 and e j k using an MLP:\nh′jk−1 =tanh(Wc[s j k||h j k−1] + bc), (3)\nh′jk =tanh(We[s j k||h j k] + bc), (4)\nwhere ·||· is the concatenate operation, and Wc, We ∈ Rd×2d are two different weight matrix modeling the influence of sjk on e j k−1 and e j k, respectively.\nThen based on the antecedent-aware event representations h′jk−1 and h ′j k, we calculate the conditional causal strength of rjk as:\nφa(r j k,ANTE j k) = σ(h ′j k−1Wcsh ′j k), (5)\nwhere Wcs ∈ Rd×d are trainable parameters, and σ is a sigmoid function.\nAntecedent Representation Along with the estimation of conditional causal strength, the representation of antecedents are also recursively updated. Specifically, at the first reasoning step, we initialize sj0 with h j 0. At the kth reasoning step, s j k is obtained based on sjk−1, the conditional causal strength P (rjk|ANTE j k), and the embedding of events within rjk :\nsjk = tanh(P (r j k|ANTE j k)Wu[h j k−1||h j k]) + s j k−1, (6)\nwhere Wu ∈ Rd×2d is a parameter matrix."
    }, {
      "heading" : "3.2.5 Intra-Chain Information Aggregation",
      "text" : "We aggregate the intra-chain causality information to derive a distributed representation and a chainlevel causal strength for each causal logic chain.\nWe notice that, in the conditional causal strength estimation process, at the lj th reasoning step, ANTEjlj+1 actually includes all the rules within ρ\nj . Hence, we utilize the representation of ANTEjlj+1 as the representation of ρj , which we denote as sj .\nGiven the chain-specific conditional causal strength for each rule within ρj , we can calculate a chain-level causal strength csj for ρj by multiplying the conditional causal strength of the rules:\ncsj = lj∏ k=1 P (rjk|ANTE j k) = lj∏ k=1 φa(r j k,ANTE j k). (7)\nThen we normalize the chain-level causal strengths as:\nĉsj = softmaxj(cs j). (8)"
    }, {
      "heading" : "3.2.6 Aggregating Chain-level Information for Predicting Causality Score",
      "text" : "Finally, we obtain the disjunction of chain-level causality information to predict the causality score Y . Intuitively, a causal logic chain with higher causal strength should have a stronger influence on Y . Therefore, we aggregate the chain-level information through calculating a linear combination of logic chain representations {s1 · · · , sm} using the normalized causal strengths {ĉs1, · · · , ĉsm}:\nu = Σj ĉs j · sj (9)\nwhere u ∈ R1×d is a final state carrying information from the disjunction of {ρ1, · · · , ρm}.\nThe causality score Y is predicted based on u:\nY = softmax(Wyu + by), (10)\nwhere Wy and by are trainable parameters."
    }, {
      "heading" : "3.3 Training",
      "text" : "In the training process, we introduce a causal logic driven negative sampling to improve the reliability of conditional causal strength estimation. In particular, if there exists a rule ri = (ei1 ⇒ ei2) within the CLG, due to the unidirectionality of causality, we can derive a corresponding false rule rF = (ei2 ⇒ ei1). From the CLG, we can also generate a wrong antecedent for the false rule through random sampling. Hence, ideally, the conditional causal strength of these false rules should equal 0. In addition, we also combine the unidirection-\nality of causality with the transitivity of causality to generate false rules with more complex patterns (e.g.: if e1 ⇒ e2 ⇒ e3, then we can induce a rF = (e3 ⇒ e1)). By sampling false rules and training the potential functions of these false rules φa(rF ,ANTEF ) to be zero, the reliability of conditional causal strength estimation can be enhanced.\nWith regard to the causal logic driven negative sampling process, the loss function of CMNLN is defined as:\nL = LCausality Score + λLConditional CS, (11)\nwhere both LCausality Score and LConditional CS are cross entropy loss, measuring the difference between the predicted and ground truth causality score, and between the predicted and the ideal conditional causal strength, respectively; λ is a balance coefficient."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Construction of C-COPA Dataset",
      "text" : "To evaluate the robustness of the ExCAR framework, we build an additional Chinese commonsense causal reasoning dataset C-COPA.\nThe C-COPA dataset is built upon a large-scale web news corpus SogouCS (Wang et al., 2008) by human annotation. We start the annotation process from manually extracting causal event pairs from raw texts within the corpus. Given a causal event pair, we first randomly generate an ask-for indicator, where ask-for ∈ [“effect”, “cause”]. Then the ask-for indicator are used to decide whether the cause or effect event to be the premise or plausible hypothesis. Given the premise, an implausible effect (cause) events is generated by a human annotator. As a result, the same as the COPA dataset, each instance within the C-COPA consists a premise event p, a plausible and an implausible hypothesis event h+ and h−, and an ask-for indicator a.\nThree Chinese volunteers are enlisted for validating the dataset. Agreement between volunteers is high (Cohen’s K = 0.923). Instances with diverged results between volunteers are removed from the dataset. After the annotation process, a total of 3,258 instances are left and we randomly split these instances into two equal-sized parts as the development set and the test set, respectively."
    }, {
      "heading" : "4.2 Construction of Causal Event Graph",
      "text" : "Before constructing the CEG, we have to collect a sufficient number of causal event pairs. To this\nend, we harvest English causal event pairs from the CausalBank Corpus (Li et al., 2020), which contains 314 million commonsense causal event pairs in total. While the Chinese causal event pairs are collected from a raw web text corpus crawled from multiple websites date from 2018 to 2019, and filtered with keywords. More details could be found in the Appendix.\nThen an English and a Chinese CEG are build based on the corresponding causal event pair corpus. To balance the computation burden and coverage of the event graph, we build the English and the Chinese CEG based on 1,500,000 Chinese and 1,5000,000 English causal event pairs randomly sampled from the whole corpus, respectively."
    }, {
      "heading" : "4.3 Experimental Settings",
      "text" : "Given a cause or effect event, we find three most textually similar events from the causal event graph, and employ them as the anchors. In the evidence retrieving process, we limit the maximum searching depth of BFS to 3, and restrict the size of evidence event set to be no more than 8. We employ the pre-trained BERT-base model as the event encoder, which encodes each input event to a 768-dimension vector. On both datasets, for each instance, 5 negative rules are sampled to facilitate the estimation of conditional causal strength. Model is trained with the balance coefficient λ of 0.1."
    }, {
      "heading" : "4.4 Baselines",
      "text" : "Statistical-based Methods These methods estimate words or phrase level causality from large-scale corpora. Then the causality of an input event pair could be obtained through synthesizing the word or phrase level causality. • PMI (Jabeen et al., 2014) measures the wordlevel causality using Point Mutual Information. • PMI EX (Gordon et al., 2011) is an asymmetric word-level PMI which takes the directionality of causal inference into consideration. • CS (Luo et al., 2016) measures word-level causality through integrating both the necessity causality and sufficiency causality. • CS MWP (Sasaki et al., 2017) measures the causality between words and prepositional phrases using the CS score.\nPre-trained-model-based Methods • BERT Wang et al. [2019a] and Li et al. [2019] finetune BERTbase with different hyper parameters to predict the causality of each 〈C,E〉 pair.\nExCAR-based Methods\nWe replace the CMNLN layer of ExCAR framework with different reasoning modules and get: • ExCAR-w/ MLN refers to substitute the CMNLN layer by a classical Markov Logic Network layer. • ExCAR-w/ fix-cs arbitrarily assign a fixed causal strength 0.5 for each logical rule. • ExCAR-concat flattens the causal logical graph into a single event sequence and takes the event sequence as input."
    }, {
      "heading" : "4.5 Quantitative Analysis",
      "text" : "We list the results on both the COPA dataset and C-COPA dataset in Table 1. We find that:\n(1) Statistical-based methods, such as CS (Luo et al., 2016) and CS MWP (Sasaki et al., 2017) achieve comparable performances with BERTbased methods, this is mainly because they harvest causal knowledge with elaborate patterns from large-scale corpus sized up to 10TB. Training BERT with such causal knowledge may provide potential space for improvement, which is left for future work.\n(2) Compared to causal pair based BERT, ExCAR related methods show improved performance. This indicates that incorporating additional evidences from the event graph can be helpful for revealing the causal decision mechanism and then improve the accuracy of causal reasoning.\n(3) ExCAR-w/ MLN and ExCAR -w/ CMNLN outperforms ExCAR-concat, which flats the CLG into an event sequence. This shows that exploiting the complex causal correlation patterns between logical rules can be helpful for the causal reasoning task.\n(4) ExCAR-w/ MLN and ExCAR -w/ CMNLN shows improved performance compared to ExCAR -w/ fixed-cs. This confirms that neuralizing rules to account for the uncertainty of the logical rules is helpful for the causal reasoning task.\n(5) ExCAR-w/ CMNLN further improves the prediction accuracy compared to ExCAR-w/ MLN, suggesting that by incorporating the antecedentaware potential function CMNLN can model the conditional causal strength of logical rules for causal reasoning."
    }, {
      "heading" : "4.6 Stability Analysis",
      "text" : "In this paper, we propose to enhance the stability of our approach through introducing additional evidence information. We investigate the specific influence of these evidences on the stability of our approach through an adversarial evaluation. Following Bekoulis et al. [2018] and Yasunaga et al. [2018], we attack the reasoning systems by adding a perturbation term on the word embedding of inputs. The perturbation term is derived using a gradient-based method FGM (Miyato et al., 2016).\nTable 2 shows the prediction accuracy after adversary attack, and ∆ denotes the change of performance brought by adversary attack. For example, ∆ = -9.9 means a 9.9% decrease of prediction accuracy after the adversary attack. We find that, compared with event pair based BERT, ExCAR can significantly improve the stability of the prediction accuracy. These results show that by incorporating additional evidence events, ExCAR could reveal the behind causal mechanism to increase the stability of prediction results."
    }, {
      "heading" : "4.7 Human Evaluation for Explainability",
      "text" : "We analyze the explainability of our approach quantitatively through human evaluations. In particular, we randomly sample 200 instances from the test set of C-COPA and make prediction using ExCAR. Then we employ three experts to give an explainability score belonging to {0, 1, 2} to evaluate whether the causality strengths derived by our\napproach are reasonable, where 0 stands for unexplainable, 1 stands for moderately explainable and 2 stands for explainable. For comparison, we further introduce two baselines: (1) Markov Logic Network (MLN); (2) Fixed-cs.\nThe average explainability scores are shown in Table 3, from which we can observe that: (1) The average explainability scores of CMNLN and MLN are higher than that of fixed-cs. This is because, through neuralizing the logical rules and equipping the logical rules with probability, CMNLN and MLN can better model the potential noise in the retrieved evidences, as well as the uncertainty of rules. (2) The explainability score of CMNLN is further higher than that of MLN. This indicates that, CMNLN can model the conditional causal strength of logical rules using the antecedent-aware potential function, and then increase the reasonability of causal strength estimation."
    }, {
      "heading" : "4.8 Case Study",
      "text" : "Figure 3 provides an example of causal reasoning made by ExCAR on C-COPA. Given a cause event Reduction of grain production, E: Rise of Inflation Rate is more likely to be the effect of the cause. However, it is difficult to directly infer the effect E: Rise of Inflation Rate directly from the cause event C:Reduction of grain production. Correspondingly, given C and E, ExCAR can obtain evidence events such as I1: Food prices increase and I2: Grain prices out of control from the causal event graph. These results show that ExCAR can obtain relevant evidences and hence choose the correct effect event in an explainable manner.\nWe also examined the estimated causal strengths. As shown in Figure 3, the causal strength between I1 and E is higher in the logic chain ρ2 compared to ρ1. Intuitively, with the additional antecedent I2: Grain prices out of control, I1: Food prices increase could be more likely to lead to E: Rise of Inflation Rate. These results indicate that CMNLN can model the conditional causal strength of rules."
    }, {
      "heading" : "4.9 Effect of the Number of Evidence Events",
      "text" : "We compare the reasoning accuracy of ExCAR on samples with different numbers of evidence events. Experiments are conducted on the test set of CCOPA. Results are shown in Figure 4. We can find that, when the evidence events number increases from 0 to 3, the reasoning accuracy increases in general, since sufficient evidences are helpful for the reasoning task. However, the accuracy starts to decrease when evidence number exceeds 4. This indicates that noisy evidence events may be obtained. The inclusion of noisy evidence events emphasis the necessity of neutralizing the logical rules, as the symbolic logic based systems cannot accommodate for the noise in the rules."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Causal Reasoning",
      "text" : "Causal reasoning remains a challenging problem for today’s AI systems. Statistical-based methods can provide strong baselines, as they can find some useful cues from large-scale causal corpus. For example, Gordon et al. (2011) measured the causality between words using PMI, and estimated the PMI based on a personal story corpus. While Luo et al. (2016) and Sasaki et al. (2017) further introduced direction information into a causal strength index. Then through synthesizing the word-level causality, the causality between events could be inferred.\nCompared to statistical-based methods, deep neural networks enable models to learn the causality between events considering the semantics of events. To this end, Xie and Mu (2019b) devised attention-based models to capture the word-level causal relationships. While Wang et al. and Li et al. (2019) finetuned the pretrained language model BERT on causal event pairs corpus to learn the pairwise causality knowledge between events.\nIn this paper, we argue that in addition to the event pair itself, causal reasoning also needs to involve more evidence information. To address this issue, we propose a novel inference framework ExCAR, which is able to incorporate the additional\nevidence events from an event graph for supporting the causal reasoning task."
    }, {
      "heading" : "5.2 Explainable Textual Inference",
      "text" : "Explainability has been a long-pursued goal for textual inference systems, as it can help to unveil the decision making mechanism of black-box models and enhance the stability of reasoning, which can be crucial for applications in various domains, such as medical and financial domains. To introduce interpretability in textural inference process, previous studies can be mainly divided into two categories: generating explainable information and devising self-explaining mechanism.\nBeyond the task related information, automated generated textual explanations are helpful for justifying the reliability of models. For example, Camburu et al. (2018) and Nie et al. (2019) train multitask learning models to learn to generate explanations for textual entailment inference. On the other hand, the incorporation of relevant external knowledge can not only increase the model performance compared to purely data-driven approaches, but also can be helpful for understanding the model behavior (Niu et al., 2019; Wang et al., 2019b).\nAnother line of work designs self-explaining models to reveal the reasoning process of models. Attention mechanism was devised to explicitly measure the relative importance of input textual features. Hence, it has been widely employed to enhance the interpretability of deep neural models.\nIn this paper, to conduct causal reasoning in an explainable manner, we propose to induce a set of logic rules from a pre-built causal event graph, and explicitly model the conditional causal strength of each logical rule. The probabilistic logical rules can provide clues to explain the prediction results."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We devise a novel explainable causal reasoning framework ExCAR. Given an event pair, ExCAR is able to obtain logical rules from a large-scale causal event graph to provide insight to inference results. To learn the conditional probabilistic of logical rules, we propose a conditional Markov neural logic network that combines the strengths of rulebased and neural models. Empirically, our method outperforms prior work on two causal reasoning datasets, including COPA and C-COPA. Furthermore, ExCAR is interpretable by providing explanations in terms of probabilistic logical rules."
    }, {
      "heading" : "7 Acknowledgments",
      "text" : "We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the Technological Innovation “2030 Megaproject” - New Generation Artificial Intelligence of China (2018AAA0101901), and the National Natural Science Foundation of China (61976073)."
    } ],
    "references" : [ {
      "title" : "Adversarial training for multi-context joint entity and relation extraction",
      "author" : [ "Giannis Bekoulis", "Johannes Deleu", "Thomas Demeester", "Chris Develder." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bekoulis et al\\.,? 2018",
      "shortCiteRegEx" : "Bekoulis et al\\.",
      "year" : 2018
    }, {
      "title" : "e-snli: Natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9539–9549.",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NACCL 2019, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense causal reasoning using millions of personal stories",
      "author" : [ "Andrew S Gordon", "Bejan", "Cosmin A", "Kenji Sagae." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Gordon et al\\.,? 2011",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2011
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R Bowman", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1803.02324.",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Causation and the price of transitivity",
      "author" : [ "Ned Hall." ],
      "venue" : "The Journal of Philosophy, 97(4):198–222.",
      "citeRegEx" : "Hall.,? 2000",
      "shortCiteRegEx" : "Hall.",
      "year" : 2000
    }, {
      "title" : "Using asymmetric associations for commonsense causality detection",
      "author" : [ "Shahida Jabeen", "Xiaoying Gao", "Peter Andreae." ],
      "venue" : "PRICAI.",
      "citeRegEx" : "Jabeen et al\\.,? 2014",
      "shortCiteRegEx" : "Jabeen et al\\.",
      "year" : 2014
    }, {
      "title" : "Constructing narrative event evolutionary graph for script event prediction",
      "author" : [ "Zhongyang Li", "Xiao", "Ding", "Ting Liu" ],
      "venue" : "In Proceedings of the 27th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to rank for plausible plausibility",
      "author" : [ "Zhongyang Li", "Tongfei Chen", "Benjamin", "Van Durme" ],
      "venue" : "arXiv preprint arXiv:1906.02079",
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Guided generation of cause and effect",
      "author" : [ "Zhongyang Li", "Xiao Ding", "Ting Liu", "J Edward Hu", "Benjamin Van Durme" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Commonsense causal reasoning between short texts",
      "author" : [ "Zhiyi Luo", "Yuchen Sha", "Kenny Q Zhu", "Seung-won Hwang", "Zhongyuan Wang." ],
      "venue" : "Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning.",
      "citeRegEx" : "Luo et al\\.,? 2016",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2016
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "R Thomas McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "arXiv preprint arXiv:1902.01007.",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M Dai", "Ian Goodfellow." ],
      "venue" : "arXiv preprint arXiv:1605.07725.",
      "citeRegEx" : "Miyato et al\\.,? 2016",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial nli: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:1910.14599.",
      "citeRegEx" : "Nie et al\\.,? 2019",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge aware conversation generation with explainable reasoning over augmented graphs",
      "author" : [ "Zheng-Yu Niu", "Hua Wu", "Haifeng Wang" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Niu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2019
    }, {
      "title" : "Why-question answering using intra-and intersentential causal relations",
      "author" : [ "Jong-Hoon Oh", "Kentaro Torisawa", "Chikara Hashimoto", "Motoki Sano", "Stijn De Saeger", "Kiyonori Ohtake." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational",
      "citeRegEx" : "Oh et al\\.,? 2013",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2013
    }, {
      "title" : "Probabilistic reasoning in intelligent systems; networks of plausible inference",
      "author" : [ "Judea Pearl." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Pearl.,? 1988",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1988
    }, {
      "title" : "Direct and indirect effects",
      "author" : [ "Judea Pearl." ],
      "venue" : "Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 411–420.",
      "citeRegEx" : "Pearl.,? 2001",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2001
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Collecting diverse natural language inference problems for sentence representation evaluation",
      "author" : [ "Adam Poliak", "Aparajita Haldar", "Rachel Rudinger", "J Edward Hu", "Ellie Pavlick", "Aaron Steven White", "Benjamin Van Durme." ],
      "venue" : "arXiv preprint arXiv:1804.08207.",
      "citeRegEx" : "Poliak et al\\.,? 2018",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2018
    }, {
      "title" : "Handling multiword expressions in causality estimation",
      "author" : [ "Shota Sasaki", "Sho Takase", "Naoya Inoue", "Naoaki Okazaki", "Kentaro Inui." ],
      "venue" : "IWCS 2017.",
      "citeRegEx" : "Sasaki et al\\.,? 2017",
      "shortCiteRegEx" : "Sasaki et al\\.",
      "year" : 2017
    }, {
      "title" : "Reading comprehension with graph-based temporalcasual reasoning",
      "author" : [ "Yawei Sun", "Cheng", "Gong", "Yuzhong Qu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 806–817.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "Superglue: A stickier",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic online news issue construction in web environment",
      "author" : [ "Canhui Wang", "Min Zhang", "Shaoping Ma", "Liyun Ru." ],
      "venue" : "Proceedings of the 17th international conference on World Wide Web, pages 457–466.",
      "citeRegEx" : "Wang et al\\.,? 2008",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2008
    }, {
      "title" : "Explainable reasoning over knowledge graphs for recommendation",
      "author" : [ "Xiang Wang", "Dingxian Wang", "Canran Xu", "Xiangnan He", "Yixin Cao", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5329–5336.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Boosting causal embeddings via potential verb-mediated causal patterns",
      "author" : [ "Zhipeng Xie", "Feiteng Mu." ],
      "venue" : "IJCAI, pages 1921–1927. I Press.",
      "citeRegEx" : "Xie and Mu.,? 2019a",
      "shortCiteRegEx" : "Xie and Mu.",
      "year" : 2019
    }, {
      "title" : "Distributed representation of words in cause and effect spaces",
      "author" : [ "Zhipeng Xie", "Feiteng Mu." ],
      "venue" : "IJCAI, volume 33, pages 7330–7337.",
      "citeRegEx" : "Xie and Mu.,? 2019b",
      "shortCiteRegEx" : "Xie and Mu.",
      "year" : 2019
    }, {
      "title" : "Robust multilingual part-of-speech tagging via adversarial training",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Dragomir Radev." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Yasunaga et al\\.,? 2018",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Causal reasoning aims at understanding the general causal dependency between the cause and effect (Luo et al., 2016).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Causality is commonly expressed by humans in the text of natural language, and is of great value for various Artificial Intelligence applications, such as question answering (Oh et al., 2013), event prediction (Li et al.",
      "startOffset" : 174,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : ", 2013), event prediction (Li et al., 2018), and decision making (Sun et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018).",
      "startOffset" : 97,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "In contrast, with intermediate event I in hand, according to the transitivity of causality (Hall, 2000), the logic chain of 〈a ⇒ i ⇒ d〉 and 〈c ⇒ i ⇒ b〉 could be naturally derived from the observed logic chain 〈a⇒ i⇒ b〉 and 〈c⇒ i⇒ d〉.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "In this paper, both the COPA (Luo et al., 2016) and the C-COPA causal reasoning task are defined as a multiple-choice task.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Markov Logic Network (MLN) (Pearl, 1988) can model such uncertainty by assigning",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "To this end, we employ a pretrained language model ELMo (Peters et al., 2018) to derive the semantic representation for events in the",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "A BERT-based encoder (Devlin et al., 2019) is employed to encode all events within each causal logic chain into chain-specific distributed embeddings.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "The C-COPA dataset is built upon a large-scale web news corpus SogouCS (Wang et al., 2008) by human annotation.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "2359 end, we harvest English causal event pairs from the CausalBank Corpus (Li et al., 2020), which contains 314 million commonsense causal event pairs in total.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "• PMI (Jabeen et al., 2014) measures the wordlevel causality using Point Mutual Information.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "• PMI EX (Gordon et al., 2011) is an asymmetric word-level PMI which takes the directionality of causal inference into consideration.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "• CS (Luo et al., 2016) measures word-level causality through integrating both the necessity causality and sufficiency causality.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 20,
      "context" : "• CS MWP (Sasaki et al., 2017) measures the causality between words and prepositional phrases using the CS score.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "ExCAR-based Methods Methods COPA C-COPA PMI (Jabeen et al., 2014) 58.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "(1) Statistical-based methods, such as CS (Luo et al., 2016) and CS MWP (Sasaki et al.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : ", 2016) and CS MWP (Sasaki et al., 2017) achieve comparable performances with BERTbased methods, this is mainly because they har-",
      "startOffset" : 19,
      "endOffset" : 40
    } ],
    "year" : 2021,
    "abstractText" : "Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of ExCAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance.",
    "creator" : "LaTeX with hyperref package"
  }
}