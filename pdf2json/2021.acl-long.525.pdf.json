{
  "name" : "2021.acl-long.525.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols",
    "authors" : [ "Chaitanya Kulkarni", "Jany Chan", "Eric Fosler-Lussier", "Raghu Machiraju" ],
    "emails" : [ "kulkarni.132@osu.edu", "chan.206@osu.edu", "fosler-lussier.1@osu.edu", "machiraju.1@osu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6737–6750\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6737"
    }, {
      "heading" : "1 Introduction",
      "text" : "Wet laboratory protocols (WLPs) play an integral role in bioscience and biomedical research by serving as a vehicle to communicate experimental instructions that allow for standardization and replication of experiments. These procedures, typically written in natural language, prescribe actions (Figure 1) to be conducted on materials that generally\n1The dataset and code is available on the authors’ websites\nproduce new materials which, in turn, are used by future actions to make newer materials. However, WLPs can be unclear, composed of disconnected and distant parts, and built upon implicit information that were referenced earlier or omitted entirely. Lack of careful documentation has led to a reproducibility crisis (Baker, 2016) in the biosciences and also poses considerable challenges for automation of laboratory procedures: gleaning the effect and semantics of actions requires understanding the underlying experiment, the sentence structure and rationale behind implicitly stated arguments.\nCurrently, there is a dearth of annotated resources for natural language instructions in laboratory protocols. The WLP corpus initially collected by Kulkarni et al. (2018) and later updated by Tabassum et al. (2020) focused solely on relations within sentences. However, actions in WLPs are more complex, containing additional relations between actions (e.g., temporal and causal rela-\ntions). We propose using material state transfer graphs (MSTG), which are a natural extension of Action Graphs (Kulkarni et al., 2018). MSTGs link together several Action Graphs into a larger structure by utilizing global temporal and causal relationships that can span several sentences in order to describe the flow of materials from action to action (Section 3). An example of a MSTG is shown in Figure 1. The action phrase Grow the bacteria overnight in Step 1 consists of an action Grow that Acts-on the reagent bacteria for an amount of time specified as overnight. This Action Graph is then connected to other such graphs (like in Step 5) through temporal and causal relationships (e.g., Grow action’s product is host culture thus we use a Product link to establish a temporal relation between Step 1 and Step 5).\nTo automate the generation of MSTGs, we must overcome two distinct challenges prevalent in WLPs. First, the result of a preceding step may not be immediately used by the next step, resulting in long-range dependencies. Second, an action may involve implicit information, which is either mentioned earlier or omitted entirely. Current models usually fail to make accurate predictions for long-range relations, as seen in Figure 1 when establishing a temporal relation between Step 1 and Step 5. These methods rely on relation propagation (DyGIE++ Wadden et al. (2019)) or use contextual embeddings (spERT Eberts and Ulges (2019)). Furthermore, neither successfully establish complex relations involving implicit arguments. In Step 5, the host culture and viral concentrate must be added to the tube containing soft agar that was removed in Step 4. However, the location tube in Step 5 is implicit and has to be correctly inferred to make the Site relation between Remove and Add.\nWe propose a novel and effective neural network model that: (i): uses a series of relational convolutions to learn from relations within and across multiple action phrases and (ii): iteratively enriches entity representations with learned latent structures using a multi-head R-GCN model. Our model achieves an F1 score of 54.5% for temporal and causal relations, significantly improving upon previous methods DyGIE++ and spERT for such long-range relations by 26.4% and 26.7% respectively. We analyze our model for intra- and intersentence relation extraction and show substantial improvements. Further, we also show the model’s ability in resolving implicit arguments to improve\ntemporal relation extraction over the best baseline method by 23.3%.\nThis paper is organized around two main contributions: (i): the WLP-MSTG Corpus that extends the WLP Corpus (Kulkarni et al., 2018) by including intra- and cross-sentence temporal and causal relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences. In Section 2, we describe related works and in Section 3, we introduce MSTGs highlighting the two challenges. Next, we describe our proposed model in Section 4 and demonstrate its performance in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences.\nCross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences.\nImplicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum,\n2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well."
    }, {
      "heading" : "3 Task Formulation: Material State Transfer Graph",
      "text" : "To construct a MSTG from an input protocol, we define the following four concepts. (i) Action Graphs: Introduced by Kulkarni et al. (2018), they are extracted from action phrases as seen in Figure 1. Forming the fundamental unit of a MSTG, Action Graphs are composed of an Action, 17 types of named entities as explicit arguments (e.g, ”Reagent”, ”Location”, etc.), and 13 local semantic relations (e.g., ”Using”, ”Measure”, ”Acts-on”,\netc.) represented as directed edges, which we shall refer to as inter-Action Phrase (iAP) relations hereafter. (ii) Temporal Relations: Inspired from prior work (Allen, 1984), we define temporality as a relationship between two action phrases such that an action’s product (output) is connected to another action’s source (input), thereby imposing a partial or total order. It is also necessary to determine whether an action is executed before or simultaneously with respect to other actions. We use 5 temporal relations, (namely ”Acts-on”, ”Site”, ”Coreference”, ”Product”, and ”Overlaps”) to capture the flow of materials. (iii) Causal Relations: Following (Barbey and Wolff, 2007), we define causality as the relationship between two actions where one action directly affects the execution of another action (e.g., if a given action enables or prevents2 another action). (iv) Implicit Arguments: We characterize implicit arguments into four cases (Figure 2a) depending on whether the source or product of the connected actions is implicit or explicit. Four of the five temporal relations in WLP-MSTG are defined to handle implicit arguments: ”Acts-on”, ”Site”, ”Coreference”, and ”Product”."
    }, {
      "heading" : "3.1 Corpus for Cross Sentence Relations",
      "text" : "Annotation Process: We annotate six-hundredand-fifteen (615) protocols derived from the WLP Corpus to include the 6 global cross-Action Phrase Temporal and Causal (cAP-TaC) relationships. We split the annotation task into two phases. In the first phase, we worked with 7 expert annotators to develop the guidelines over 8 iterations. Each iteration consisted of 10 protocols that were individually annotated by each expert annotator, and the inter-annotator agreement (IAA) was measured for each of the 10 protocols. At the end of each iter-\n2Due to the limited instances of ”Prevents” relations found in WLPs, we replace these with the relation ”Enables”. E.g., Mix regents carefully to not spill contents, implies a ”Prevents” relation from Mix to spill which is equivalent to an ”Enables” relation from Mix to not spill.\nation, we refined the set of rules to reduce the guidelines’ ambiguity. The agreement measured across all annotators using Krippendorff’s Alpha (Krippendorff, 2004) on the last iteration was 78.23%.\nWith a good IAA attained, we began the second phase to collect the train, dev, and test datasets. To ensure the highest quality of the test data, we employed all 7 annotators to work on the same 128 protocols and merged the resulting annotations based on majority voting. In contrast, individual annotators collected the train and dev sets separately to speed up the annotation process. A typical protocol of 30 steps required 25 minutes on average for an annotator to identify all the cAP-TaC relations.\nComparison with previous corpora: Our corpus, WLP-MSTG, extends the WLP corpus (Kulkarni et al., 2018) which was later updated for a WNUT 2020 shared task (Tabassum et al., 2020). WNUT 2020 was primarily designed to facilitate supervised named entity taggers and withinsentence relation extraction methods. We extend the 615 protocols therein to include intra- and intersentence temporal and causal relations. To ensure a fully connected graph, we exclude entities and relations annotated for spurious descriptive sentences that do not prescribe any actions (e.g., title, notations, definitions, etc.). Table 2 provides a comparison of statistics among the three corpora.\nAnalysis: We conducted a distribution analysis of 90 protocols that would typically serve as the dev set for machine learning models. Actions connected by temporal and causal relations tend to be consecutive (78.4%); however, a non-trivial number are considerably spaced apart (21.6%) with 1.08% of the total at least 8 actions apart. For implicit arguments, we observed: (i) implicit arguments are unusually prevalent in WLPs (88.44%), (ii) a higher percentage (55.98%) of the products of an action are implied, and (iii) temporally connected actions are closer if they contain implicit\narguments; otherwise, they are relatively farther apart Figure 2b. This analysis provides valuable insight about the challenges in the form of long-range relations and implicit arguments that are present in extracting MSTGs from WLPs."
    }, {
      "heading" : "4 A Latent Structure Model for Joint Entity and Relation Extraction",
      "text" : "We develop a latent structure model for jointly learning entity and relations within and across multiple sentences. A schematic of the model is shown in Figure 3. In Section 4.1 we describe construction of span representation (Figure 3A) from protocol text that incorporates critical features necessary for long-range relation extraction. Section 4.2 explains how the transcoder block (Figure 3B) builds upon latent structures (as illustrated in Figure 3D) to improve entity and relation representations. Finally, in Section 4.3 we discuss training and regularization strategies to jointly learn span, entity, and relations through a multi-task loss function derived from span, entity, and relation scores (Figure 3C). We shall use Figure 1 as a running example throughout the model description."
    }, {
      "heading" : "4.1 Span Representation",
      "text" : "Following prior span-based approaches (Wadden et al., 2019; Eberts and Ulges, 2019), our goal is to (i): collect a series of tokens from the protocol text, (ii): enumerate all spans, and (iii): rank topscoring spans for considerations as candidates for entity and relation extraction.\nToken embeddings: We use SciBERT (Beltagy et al., 2019) for learning token representations for a given protocol P . As shown in Figure 3, the input is a protocol P represented as a collection of sentences S = {s1, ..., sP}. Each sentence si is composed of a sequence of tokens {t1, ..., tn}. For example, within the sentence, Add 1.0 mL host culture and either 1.0 or 0.1 mL viral concentrate (Figure 1, Step 5), we identify host, culture, and etc., as the tokens to be passed to the SciBERT model. We batch process sentences in the protocol to generate context-aware embeddings {t1, ..., tn} for each sentence.\nSpan Enumeration: The spans between two tokens ti and tj is represented as sij = {ti, ti+1, ...tj}. We enumerate all possible spans of upto a size of 10 tokens. For each enumerated span, the span representation eij ∈ Rde is derived from\napplying a feed-forward neural network (FFNN) on a concatenation of tokens representations and embeddings:\neij = FFNN([ti; tj ;φsh(sij);φpos(sij);\nφstep(sij);φw(sij)]) (1)\nwhere, ti and tj are the first and last token representation. Note, φsh(sij) is a soft head representation (Bahdanau et al., 2014) and, φw(sij) is a learnt span width embedding respectively. Further, φpos(sij) and φstep(sij) are two positional embeddings, the former for within sentence while the latter defines the step position within the protocol respectively. Hence, host culture and host culture and are two valid spans that are enumerated through this process.\nSpan Pruning: Next, low scoring spans are filtered out during both training and evaluation phases. Following (Lee et al., 2017), the scoring function is implemented as a feed-forward network φs(eij) = w T s FFNNs(eij). We rank and pick a number of top scoring spans per sentence by using a combination of (i): a maximum fraction λp = 0.1 of spans per sentence, and (ii): a minimum score threshold λt = 0.5. Thus, the span host culture receives a significantly higher score than host culture and, indicating that the former is the correct reagent entity in the prescribed step. These span candidates are then passed to the transcoder block."
    }, {
      "heading" : "4.2 Transcoder Block",
      "text" : "In the transcoder block, we propose a novel architecture to improve relation and entity representation from latent structures. The objective is two fold: (i): to leverage localized features at phrase and sentence levels to resolve long range relations through a relation convolutions, and (ii): to learn from latent structures how to resolve implicit arguments through a multi-head relational graph convolution network (multi-head R-GCN).\nEach transcoder block is composed of a Relation Encoder (Section 4.2.1), Convolution (Section 4.2.2) and Decoder (Section 4.2.3) components, to discover local relationships between the input entities. These relations (represented as latent structures A ∈ Rm×m×r) are then passed to the Multi-Head R-GCN (Section 4.2.4) component of the same transcoder block to enrich the entity representation with information about those discovered local relationships. These enriched entities can now be used to predict more complex cross sentence relationships in the next transcoder block. To facilitate deeper networks, we make use of residual connections (He et al., 2016) followed by layer normalization (Ba et al., 2016) as denoted by Add + Norm in Figure 3B.\nWe shall make use of the example (Figure 1), focusing on the long range relationships between Step 1 (i.e., Grow the bacteria overnight.) and Step 5 (i.e., Add 1.0 mL host culture and either 1.0 or\n0.1 mL viral concentrate.) to illustrate the flow of information throughout the transcoder block. The first transcoder block takes as input m high scoring candidate entity span representations (as E(0) ∈ Rm×de) as determined by the pruner 3. For instance, from Step 1 we identify the following high scoring candidate entities grow, bacteria, and overnight and from Step 5 we find add, 1.0 mL, host culture, 0.1 mL, and viral concentrate."
    }, {
      "heading" : "4.2.1 Relation Encoder:",
      "text" : "Following (Nguyen and Verspoor, 2019), we make use of a bi-affine pairwise function to encode relations for every pair of entity span representation. That is, we generate relational embeddings for entity pairs like grow and bacteria, grow and overnight, etc. Each entity span eij ∈ Rde is first projected using two FFNNs to generate the representations ehij ∈ Rdh and etij ∈ Rdt indicating the first (head) and the second (tail) argument of a relation:\nehij = FFNNh(eij); e t ij = FFNNt(eij)\nIn practice, we batch process all entities to generate Eh ∈ Rm×dh and Et ∈ Rm×dt where m is the number of candidate spans. In our experiments, we let dh = dt then use a bi-affine operator to calculate a tensor R̃(l) ∈ Rm×dr×m for relational embeddings: R̃(l) = (EhL)ETt . Here L ∈ Rdh×dr×dt is a learned parameter tensor and dr is the relation embedding size."
    }, {
      "heading" : "4.2.2 Relation Convolutions:",
      "text" : "We enrich the relational embeddings R̃(l) with local relational features within a single phrase (found near the diagonal) and across multiple phrases (found in the upper and lower triangle) using a stack of convolutional layers. We denote Cw(.) to be a 2D convolutional operator applying a kernel width of size w × w. In our model, we make use of a two-layer convolution:\nT(0) = ReLU(C3(R̃(l)))\nR(l) = ReLU(C3(T(0)))\nThe input R̃(l) is reshaped as Rm×m×dr such that the dimensions dr acts as the channel dimension in the convolutions. The dimensions of T(0) is in Rm×m×2dr with the final output R(l) ∈ Rm×m×dr .\n3The entity span representation from the entire subprotocol, (i.e., from steps 1 to 5), are passed as a bag of entities E(0) ∈ Rm×de . However, there aren’t any relations (i.e., R(0)) to be passed to the first transcoder block"
    }, {
      "heading" : "4.2.3 Relation Decoder:",
      "text" : "The relational embeddings R(l) are decoded using a 2-layer FFNN. The decoded scores A ∈ Rm×m×r captures the latent structures (as shown in Figure 3B). This is re-encoded using the multi-head R-GCN to strengthen the model’s ability to predict more complex relations in the next transcoder layer."
    }, {
      "heading" : "4.2.4 Multi-head R-GCN:",
      "text" : "For each predicted relation score Ar ∈ Rm×m, we add self loops and perform Laplacian smoothing (Kipf and Welling, 2017; Li et al., 2018) for normalization following: Âr = D̃− 1 2 ÃrD̃ − 1 2 where\nÃr = Ar + I and D̃ = ∑\nj Ãijr. Then, using Âr as an adjacency matrix, we learn multi-head, direction-specific graph convolution transformations. Each head corresponding to a given relation r performs graph convolutions on the entity representation E(l−1) ∈ Rm×de to generate E(l)r ∈ Rm×(dr/r). A single R-GCN(i)r (.) (Schlichtkrull et al., 2018) operation for a given relation type r and ith GCN layer corresponds to:\nR-GCN(i)r (Âr,E (i−1) r ) = σ(ÂrE (i−1) r W (i) fr )\n+ σ(ÂTr E (i−1) r W (i) br ) + b (i) r (2)\nwhere W(i)fr ∈ R di−1×di , W(i)br ∈ R di−1×di are learnable parameters for incoming and outgoing edge directions respectively and b(i)r is the bias. We use the ReLU activation function σ in our networks. As shown in Figure 3B, the outputs of the individual R-GCN heads are concatenated and passed through a FFNN layer to compute the final output E(l).\nFor instance, suppose we discovered a local relation in Step 1 between grow and bacteria after the Relation Decoder component in the first transcoder block. The Multi-head R-GCN takes in the discovered relation (through the latent structure A) and enriches grow’s entity embeddings, enabling the next transcoder layer to predict a more complex cross sentence relation between grow (Step 1) and host culture (Step 5). Since bacteria and host culture are semantically related, they have similar entity embeddings, and therefore the enriched representation of grow (now containing information about bacteria) allows for establishing the relation between grow and host culture in the next transcoder block."
    }, {
      "heading" : "4.3 Training and Regularization",
      "text" : "The loss function is a linear combination of cross entropy losses for each of the tasks. We additionally apply label smoothing (Szegedy et al., 2016). The relation extraction is trained on gold entity spans. For regularization, we apply dropout (Srivastava et al., 2014) to the output of each FFNN layer. We make use of dropedge (Rong et al., 2019) for the adjacency matrix Ar before it is passed to the multi-head R-GCN model."
    }, {
      "heading" : "5 Experiments",
      "text" : "In contrast to general language models, domainspecific methods have resulted in more competitive baselines and are better suited (Tabassum et al., 2020; Wadden et al., 2019; Eberts and Ulges, 2019) for simultaneously resolving and predicting entities and relations over longer contexts. Thus, we evaluate our model against two state-of-the-art models for jointly predicting entities and relations in scientific-text domain, namely DyGIE++ (Wadden et al., 2019) and spERT (Eberts and Ulges, 2019), on the WLP-MSTG.\nWe conduct five (5) runs with random initializations for each evaluation and report the test set performance on the model that achieved the median relation F1 score on the dev(elopment) set. All models are evaluated end-to-end, where the model takes as input tokenized sentences and predicts all the entities and the relations generating a MSTG. We use the standard precision, recall and F1 metrics. An entity is considered correct if its predicted span and label match the ground truth. Relation extraction is performed on the predicted entity spans. A relation is correct if its relation type and the entity pairs are both correct (in span and type) against the ground truth. We also evaluate our model’s performance on WNUT 2020 (Tabassum et al., 2020) corpus. To fairly evaluate relation extraction, we use gold entities to make relation predictions4 by modifying the loss function to only train on relation scores. We additionally concatenate entity label embeddings to the span representation in Equation (1)."
    }, {
      "heading" : "5.1 Results",
      "text" : "On the WLP-MSTG corpus, Table 3 shows our best model with N = 8 transcoder block layers making\n4The best models on WNUT2020 make direct use of gold entities during the training and inference and only focus on relation extraction task.\nmodest improvement on entity extraction at 82.0% but improving significantly upon the previous stateof-the-art methods (i.e. DyGIE++ and spERT) in predicting relations. Our model outperforms the baselines for relation extraction with an F1 score on predicting inter-Action Phrase (iAP) relations at 68.0% and cross-Action Phrase Temporal and Causal (cAP-TaC) relations at 54.5%. We further enhanced the performance of our model by sharing the relational decoders’ parameters across all layers of the transcoder block (Section 4.2.3). This enables the latent structures to be grounded in output relation types, which also lends itself to be interpretable. The shared relation decoder marginally outperforms the not-shared configuration by 0.5% for iAP relations and 1.1% for cAP-TaC relations.\nShort and Long Range Relations: On the WNUT 2020 corpus, which only includes intrasentence relations, Table 4 shows that our model outperforms the best single model that used the original data by 1.0%. We also report that our model is competitive against the ensemble approach that included models trained on an altered version of the original corpus where they removed duplicate text after clustering. On the WLP-MSTG corpus, we can evaluate both short and long range relations: from Table 3 we see a 3.5% improvement in F1 score over DyGIE++ for iAP relations. This shows that our model leverages the cross-sentence temporal and causal relations that were additionally annotated in WLP-MSTG to improve local iAP relations. Our model outperforms DyGIE++ and spERT on intra-sentence by 4.3% and 26.1% respectively, and significantly improves for intersentence cAP-TaC relations by 45.5% and 21.5% respectively. This is attributed to positional embeddings along with the relational convolutions which enables the model to learn intra and inter action phrase relations effectively. We see spERT performing better for ”Overlaps” which is largely attributed to the ’CLS’ token that spERT embeds to make relation predictions. Figure 4 shows performance on varying the number of sentences in between entities involved in a relation. We observe our model performing the best for all distances between sentences. This is once again attributed to the relational convolution component which is effective in capturing far away relations.\nTemporal and Implicit Arguments: In Table 6 we show our model outperforming the baselines for\ntemporal relations at 53.4% F1 score. We also observe significant improvements across the board for resolving implicit arguments. We see the highest gains (at 55.6%) compared to the baseline models (1.6% for DyGIE++ and 10.2% for spERT) for (E-I) case (Figure 2a) which only contains 169 samples in the test set. Our model is able to correctly resolve the implicit source (input) to an action by utilizing simple relations that is typically connected to explicit arguments.\nCausal relations: The performance for causal relations for our model against DyGIE++ is comparable as seen in Table 6. Causal relations are relatively easier for the baseline models to capture, as they tend to have specific prepositions in be-\ntween action phrases.5 However, more complex causal relations are hard. Still, our model is able to deal with such examples, presenting about 0.7% performance gain compared to DyGIE++ and about 10.9% improvement against spERT. This is primarily attributed to the multi-head R-GCN which builds upon simple relations that provide clues to establish harder causal relations. Cross-sentential ’Enables’ relations (as seen in Table 5) are challenging even for our model as once again we do not encode any contextual features.\nModel Ablation: Table 7 presents the results of the ablation test of our model on the development set of WLP-MSTG. All three components (i.e., positional embeddings, relation convolutions and\n5For instance, in Step Resuspend by vortexing the pellets baseline models can easily identify an ”Enables” relation from vortexing to Resuspend with the help of the preposition ’by’.\nmulti-head R-GCN) play a significant role in improving cAP-TaC performance. Relation convolutions contributes the most to iAP and cAP-TaC relations by about 1.2% and 2.4% respectively. Positional embeddings impacts iAP relations more (by 1.1%) whereas Multi-Head R-GCN only impacts the more complex relations (cAPTaC by 1.1%) and does not help in improving simpler relations.\nHow Many Layers?: Figure 5 shows that more layers generally improve far away relations without improving closer ones. This shows that although our model can build upon simple relations that are typically close by, it cannot do the opposite, i.e.,\nleverage far away relations (which are typically more complicated) to improve more challenging closer relations. Our model discovers those complex, distant relations too deep into the network to be utilized to predict the challenging local relations."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We present the WLP-MSTG corpus, an extension of the WLP corpus that includes cAP-TaC relationships for building MSTGs. This corpus highlights two unique challenges: (i) the implicit argument problem and (ii) long-range relations. To address these issues, our model builds upon latent structures thus outperforming previous state-of-the-art models for predicting iAP and cAP-TaC relations. We also report significant improvements in understanding implicit arguments and identifying long range relationships across multiple sentences. However, our model’s lower absolute performance indicates that we have not fully captured the information needed to facilitate modeling end-to-end workflows, which will have a lasting impact in improving automation in the life sciences and other domains."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    }, {
      "heading" : "7.1 Material State Transfer Graph Example",
      "text" : "We describe a full material state transfer graph (as seen in Figure 7) designed for the protocol in Figure 6. Each action phrase found in the protocol text is converted into an action graph (as seen in grey boxes in Figure 7). For example, the action phrase: Grow the bacteria overnight, we identify an ”Action” Grow and all of its arguments like ”Reagent” bacteria and ”Time” overnight. These actions and entities are interconnected with local relations that we call iAP (inter-Action Phrase) relations. For instance, relations like ”Acts-on” between Grow to bacteria and ”Setting” from Grow to overnight to indicate that is how long we should be growing the bacteria. Then, the action phrases as action graphs are interconnected with cross-Action Phrase Temporal and Causal (cAP-TaC) relations. These relations can connect to any action or entity in the action graph as seen in Figure 7. For example, the ”Product” relation from Grow to host culture indicates two things, (i) that the actual product of the Grow ”Action” is host culture and (ii) the steps involving Grow must take place first before the ”Action” Add. We carefully define each relation used as cAP-TaC relations below:"
    }, {
      "heading" : "7.2 Temporal Relations",
      "text" : "The following four relations behave as ”before” temporal relations (ie ”Acts-on”, ”Site”, ”Product”, ”Coreference”). Whereas the fifth relation ”Overlaps” is used when any two actions have any degree of overlap in time. The four relations also define which implicit argument relation group do they fall under. The four groups are (i)(I-I) as in both the product and the sources are implicit. (ii)(E-I) The product is explicit, but the source is implied. (iii) (I-E) the product is implied but the source is explicit. And, (iv) (E-E) both the source and product are explicit.\nActs-on: Connects to a previous ”Action” if the product of that ”Action” is implicit. Otherwise directly connects to the named entity (which can be a ”Reagent”, ”Location”, ”Device” etc) that the previous Action has a ”Product” relation to. If directly connected to an ”Action”, this would fall under (I-I) case of implicit argument types. If connected to any named entity which is a product of the previous action then it would fall under (E-I) case.\nSite: Similar to ”Acts-on”, this relation links to the previous ”Action” if the product of that ”Action” is implicit, and that product is where the current ”Action” is taking place. Otherwise we directly connect to the appropriate named entity. Once again, similar to ”Acts-on”, if directly connected to ”Action”, it falls under (I-I) case, otherwise its (E-I) case.\nProduct: This relation is used to identify the product of the current ”Action”, either its found in its own action phrase or in some future action phrase. If the product is identified within its action phrase (which is quite rare) it would be considered an iAP relation. Otherwise, this would fall under (I-E) case.\nCoreference: This is used when the objects or arguments are in the same state. We connect the object to the same object referred before only if that object has not undergone any transformations by any actions in between. This relation falls under (E-E) case.\nOverlaps: This relation is used to indicate which two actions are being performed simultaneously or that have any degree of overlap between them in terms of time."
    }, {
      "heading" : "7.3 Causal Relation",
      "text" : "We only make use of one causal relation type ”Enables”. Due to low numbers on ”Prevents” relations, we turn them into an ”Enables” relation by\nsimply negating the ”Action” involved in the relationship. For example, Mix regents carefully to not spill contents, we replace a ”Prevents” relation from Mix to spill with an ”Enables” relation from Mix to not spill. In many elaborate negative words we make use of ”Mod-Link” to connect to the additional descriptors to the relevant action."
    }, {
      "heading" : "7.4 Implementation Details",
      "text" : "In evaluating on WLP-MSTG, we overcome memory limitations in baseline models during training and inferencing, we sub-divide long protocols into overlapping windows of 5 sentences each, with a stride of 2 (i.e., each consecutive window shares 3 sentences). To ensure fair comparison we also incorporate this restriction to our model, although our models is capable of a much larger window size. The final evaluation is done by merging the predictions in the form of sub-graphs into one complete material state transfer graph (MSTG) and resolving duplicate predictions through majority voting. We identify duplicates through exact match of spans boundaries for entities and exact match of entity span and its types for relations.\nHyperparameters We make use of Adam optimizer with a initial learning rate of 2.13 × 10−5. For generating span candidates we only enumerate them upto 10 tokens in width. We set the positional embedding φpos(sij) table size to 100. For step embedding φstep(sij) we only learn embeddings for 5 steps. Both embeddings use embedding dimensions as 50. The span embedding size de = 340, and the relational embedding size dr is set to 100. Label smoothing [symbol] is set to the default value of 0.1. Dropout used in every FFNN has p = 0.2 and the dropedge used right before multi-head RGCN model is set with p = 0.5."
    } ],
    "references" : [ {
      "title" : "Towards a general theory of action and time",
      "author" : [ "James F Allen." ],
      "venue" : "Artificial intelligence, 23(2):123– 154.",
      "citeRegEx" : "Allen.,? 1984",
      "shortCiteRegEx" : "Allen.",
      "year" : 1984
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Reproducibility crisis",
      "author" : [ "Monya Baker." ],
      "venue" : "nature, 533(26):353–66.",
      "citeRegEx" : "Baker.,? 2016",
      "shortCiteRegEx" : "Baker.",
      "year" : 2016
    }, {
      "title" : "Learning causal structure from reasoning",
      "author" : [ "Aron K Barbey", "Philip Wolff." ],
      "venue" : "Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29.",
      "citeRegEx" : "Barbey and Wolff.,? 2007",
      "shortCiteRegEx" : "Barbey and Wolff.",
      "year" : 2007
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning semantic links from a corpus of parallel temporal",
      "author" : [ "Steven Bethard", "James H Martin" ],
      "venue" : null,
      "citeRegEx" : "Bethard and Martin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bethard and Martin.",
      "year" : 2008
    }, {
      "title" : "A minimalist approach to shallow discourse parsing and implicit relation recognition",
      "author" : [ "Christian Chiarcos", "Niko Schenk." ],
      "venue" : "Proceedings of the Nineteenth Conference on Computational Natural Language Learning-Shared Task, pages 42–49.",
      "citeRegEx" : "Chiarcos and Schenk.,? 2015",
      "shortCiteRegEx" : "Chiarcos and Schenk.",
      "year" : 2015
    }, {
      "title" : "Improving implicit semantic role labeling by predicting semantic frame arguments",
      "author" : [ "Quynh Ngoc Thi Do", "Steven Bethard", "Marie Francine Moens." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language",
      "citeRegEx" : "Do et al\\.,? 2017",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2017
    }, {
      "title" : "Span-based joint entity and relation extraction with transformer pre-training",
      "author" : [ "Markus Eberts", "Adrian Ulges." ],
      "venue" : "24th European Conference on Artificial Intelligence.",
      "citeRegEx" : "Eberts and Ulges.,? 2019",
      "shortCiteRegEx" : "Eberts and Ulges.",
      "year" : 2019
    }, {
      "title" : "Wordnet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "The encyclopedia of applied linguistics.",
      "citeRegEx" : "Fellbaum.,? 2012",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 2012
    }, {
      "title" : "Beyond nombank: A study of implicit arguments for nominal predicates",
      "author" : [ "Matthew Gerber", "Joyce Chai." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592.",
      "citeRegEx" : "Gerber and Chai.,? 2010",
      "shortCiteRegEx" : "Gerber and Chai.",
      "year" : 2010
    }, {
      "title" : "Semantic role labeling of implicit arguments for nominal predicates",
      "author" : [ "Matthew Gerber", "Joyce Y Chai." ],
      "venue" : "Computational Linguistics, 38(4):755–798.",
      "citeRegEx" : "Gerber and Chai.,? 2012",
      "shortCiteRegEx" : "Gerber and Chai.",
      "year" : 2012
    }, {
      "title" : "Attention guided graph convolutional networks for relation extraction",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Wei Lu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint event and temporal relation extraction with shared representations and structured prediction",
      "author" : [ "Rujun Han", "Qiang Ning", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Content analysis: An introduction to its methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "SAGE Publications.",
      "citeRegEx" : "Krippendorff.,? 2004",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2004
    }, {
      "title" : "An annotated corpus for machine reading of instructions in wet lab protocols",
      "author" : [ "Chaitanya Kulkarni", "Wei Xu", "Alan Ritter", "Raghu Machiraju." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Kulkarni et al\\.,? 2018",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2018
    }, {
      "title" : "Impar: A deterministic algorithm for implicit semantic role labelling",
      "author" : [ "Egoitz Laparra", "German Rigau." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1180–1189.",
      "citeRegEx" : "Laparra and Rigau.,? 2013",
      "shortCiteRegEx" : "Laparra and Rigau.",
      "year" : 2013
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Higher-order coreference resolution with coarse-tofine inference",
      "author" : [ "Kenton Lee", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Structured learning for temporal relation extraction from clinical records",
      "author" : [ "Artuur Leeuwenberg", "Marie Francine Moens." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long",
      "citeRegEx" : "Leeuwenberg and Moens.,? 2017",
      "shortCiteRegEx" : "Leeuwenberg and Moens.",
      "year" : 2017
    }, {
      "title" : "Knowledge-oriented convolutional neural network for causal relation extraction from natural language texts",
      "author" : [ "Pengfei Li", "Kezhi Mao." ],
      "venue" : "Expert Systems with Applications, 115:512–523.",
      "citeRegEx" : "Li and Mao.,? 2019",
      "shortCiteRegEx" : "Li and Mao.",
      "year" : 2019
    }, {
      "title" : "Deeper insights into graph convolutional networks for semi-supervised learning",
      "author" : [ "Qimai Li", "Zhichao Han", "Xiao-Ming Wu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextaware neural model for temporal information extraction",
      "author" : [ "Yuanliang Meng", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 527–536.",
      "citeRegEx" : "Meng and Rumshisky.,? 2018",
      "shortCiteRegEx" : "Meng and Rumshisky.",
      "year" : 2018
    }, {
      "title" : "Temporal information extraction for question answering using syntactic dependencies in an lstm-based architecture",
      "author" : [ "Yuanliang Meng", "Anna Rumshisky", "Alexey Romanov." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Meng et al\\.,? 2017",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2017
    }, {
      "title" : "Big green at wnut 2020 shared task-1: Relation extraction as contextualized sequence classification",
      "author" : [ "Chris Miller", "Soroush Vosoughi." ],
      "venue" : "arXiv preprint arXiv:2012.04538.",
      "citeRegEx" : "Miller and Vosoughi.,? 2020",
      "shortCiteRegEx" : "Miller and Vosoughi.",
      "year" : 2020
    }, {
      "title" : "End-toend neural relation extraction using deep biaffine attention",
      "author" : [ "Dat Quoc Nguyen", "Karin Verspoor." ],
      "venue" : "European Conference on Information Retrieval, pages 729–738. Springer.",
      "citeRegEx" : "Nguyen and Verspoor.,? 2019",
      "shortCiteRegEx" : "Nguyen and Verspoor.",
      "year" : 2019
    }, {
      "title" : "Relation extraction: Perspective from convolutional neural networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 39–48.",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "A structured learning approach to temporal relation extraction",
      "author" : [ "Qiang Ning", "Zhili Feng", "Dan Roth." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1027–1037.",
      "citeRegEx" : "Ning et al\\.,? 2017",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-sentence n-ary relation extraction with graph lstms",
      "author" : [ "Nanyun Peng", "Hoifung Poon", "Chris Quirk", "Kristina Toutanova", "Wen-tau Yih." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:101–115.",
      "citeRegEx" : "Peng et al\\.,? 2017",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Dropedge: Towards deep graph convolutional networks on node classification",
      "author" : [ "Yu Rong", "Wenbing Huang", "Tingyang Xu", "Junzhou Huang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Rong et al\\.,? 2019",
      "shortCiteRegEx" : "Rong et al\\.",
      "year" : 2019
    }, {
      "title" : "Classifying relations by ranking with convolutional neural networks",
      "author" : [ "Cicero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "arXiv preprint arXiv:1504.06580.",
      "citeRegEx" : "Santos et al\\.,? 2015",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2015
    }, {
      "title" : "Do we really need all those rich linguistic features? a neural network-based approach to implicit sense labeling",
      "author" : [ "Niko Schenk", "Christian Chiarcos", "Kathrin Donandt", "Samuel Rönnqvist", "Evgeny Stepanov", "Giuseppe Riccardi." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Schenk et al\\.,? 2016",
      "shortCiteRegEx" : "Schenk et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "15th International Conference on Extended Semantic Web Conference, ESWC 2018,",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "mgsohrab at wnut 2020 shared task-1: Neural exhaustive approach for entity and relation recognition over wet lab protocols",
      "author" : [ "Mohammad Golam Sohrab", "Anh-Khoa Duong Nguyen", "Makoto Miwa", "Hiroya Takamura." ],
      "venue" : "Proceedings of the Sixth Work-",
      "citeRegEx" : "Sohrab et al\\.,? 2020",
      "shortCiteRegEx" : "Sohrab et al\\.",
      "year" : 2020
    }, {
      "title" : "N-ary relation extraction using graphstate lstm",
      "author" : [ "Linfeng Song", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2226–2235.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Wnut-2020 task 1 overview: Extracting entities and relations from wet lab protocols",
      "author" : [ "Jeniya Tabassum", "Wei Xu", "Alan Ritter." ],
      "venue" : "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020), pages 260–267.",
      "citeRegEx" : "Tabassum et al\\.,? 2020",
      "shortCiteRegEx" : "Tabassum et al\\.",
      "year" : 2020
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi level causal relation identification using extended features",
      "author" : [ "Xuefeng Yang", "Kezhi Mao." ],
      "venue" : "Expert Systems with Applications, 41(16):7171– 7181.",
      "citeRegEx" : "Yang and Mao.,? 2014",
      "shortCiteRegEx" : "Yang and Mao.",
      "year" : 2014
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Lack of careful documentation has led to a reproducibility crisis (Baker, 2016) in the biosciences and also poses considerable challenges for automation of laboratory procedures: gleaning the effect and semantics of actions requires understanding the underlying experiment, the sentence structure and rationale behind implicitly stated arguments.",
      "startOffset" : 66,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "We propose using material state transfer graphs (MSTG), which are a natural extension of Action Graphs (Kulkarni et al., 2018).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "This paper is organized around two main contributions: (i): the WLP-MSTG Corpus that extends the WLP Corpus (Kulkarni et al., 2018) by including intra- and cross-sentence temporal and causal relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017).",
      "startOffset" : 118,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017).",
      "startOffset" : 118,
      "endOffset" : 166
    }, {
      "referenceID" : 26,
      "context" : "based methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "based methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018).",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 43,
      "context" : "Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features.",
      "startOffset" : 29,
      "endOffset" : 96
    }, {
      "referenceID" : 29,
      "context" : "Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features.",
      "startOffset" : 29,
      "endOffset" : 96
    }, {
      "referenceID" : 33,
      "context" : "Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features.",
      "startOffset" : 29,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 162
    }, {
      "referenceID" : 37,
      "context" : "Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "(ii) Temporal Relations: Inspired from prior work (Allen, 1984), we define temporality as a relationship between two action phrases such that an action’s product (output) is connected to another action’s source (input), thereby imposing a partial or total order.",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "(iii) Causal Relations: Following (Barbey and Wolff, 2007), we define causality as the relationship between two actions where one action directly affects the execution of another action (e.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "Comparison with previous corpora: Our corpus, WLP-MSTG, extends the WLP corpus (Kulkarni et al., 2018) which was later updated for a WNUT 2020 shared task (Tabassum et al.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 40,
      "context" : ", 2018) which was later updated for a WNUT 2020 shared task (Tabassum et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : "Following prior span-based approaches (Wadden et al., 2019; Eberts and Ulges, 2019), our goal is to (i): collect a series of tokens from the protocol text, (ii): enumerate all spans, and (iii): rank topscoring spans for considerations as candidates for entity and relation extraction.",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "Following prior span-based approaches (Wadden et al., 2019; Eberts and Ulges, 2019), our goal is to (i): collect a series of tokens from the protocol text, (ii): enumerate all spans, and (iii): rank topscoring spans for considerations as candidates for entity and relation extraction.",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Token embeddings: We use SciBERT (Beltagy et al., 2019) for learning token representations for a given protocol P .",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "Note, φsh(sij) is a soft head representation (Bahdanau et al., 2014) and, φw(sij) is a learnt span width embedding respectively.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Following (Lee et al., 2017), the scoring function is implemented as a feed-forward network φs(eij) = w T s FFNNs(eij).",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "To facilitate deeper networks, we make use of residual connections (He et al., 2016) followed by layer normalization (Ba et al.",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : ", 2016) followed by layer normalization (Ba et al., 2016) as denoted by Add + Norm in Figure 3B.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : "Following (Nguyen and Verspoor, 2019), we make use of a bi-affine pairwise function to encode relations for every pair of entity span representation.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "For each predicted relation score Ar ∈ Rm×m, we add self loops and perform Laplacian smoothing (Kipf and Welling, 2017; Li et al., 2018) for normalization following: Âr = D̃− 1 2 ÃrD̃ − 1 2 where Ãr = Ar + I and D̃ = ∑ j Ãijr.",
      "startOffset" : 95,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "For each predicted relation score Ar ∈ Rm×m, we add self loops and perform Laplacian smoothing (Kipf and Welling, 2017; Li et al., 2018) for normalization following: Âr = D̃− 1 2 ÃrD̃ − 1 2 where Ãr = Ar + I and D̃ = ∑ j Ãijr.",
      "startOffset" : 95,
      "endOffset" : 136
    }, {
      "referenceID" : 35,
      "context" : ") (Schlichtkrull et al., 2018) operation for a given relation type r and ith GCN layer corresponds to:",
      "startOffset" : 2,
      "endOffset" : 30
    }, {
      "referenceID" : 39,
      "context" : "We additionally apply label smoothing (Szegedy et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 38,
      "context" : "For regularization, we apply dropout (Srivastava et al., 2014) to the output of each FFNN layer.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 32,
      "context" : "We make use of dropedge (Rong et al., 2019) for the adjacency matrix Ar before it is passed to the multi-head R-GCN model.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 40,
      "context" : "In contrast to general language models, domainspecific methods have resulted in more competitive baselines and are better suited (Tabassum et al., 2020; Wadden et al., 2019; Eberts and Ulges, 2019) for simultaneously resolving and predicting entities and relations over longer contexts.",
      "startOffset" : 129,
      "endOffset" : 197
    }, {
      "referenceID" : 41,
      "context" : "In contrast to general language models, domainspecific methods have resulted in more competitive baselines and are better suited (Tabassum et al., 2020; Wadden et al., 2019; Eberts and Ulges, 2019) for simultaneously resolving and predicting entities and relations over longer contexts.",
      "startOffset" : 129,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "In contrast to general language models, domainspecific methods have resulted in more competitive baselines and are better suited (Tabassum et al., 2020; Wadden et al., 2019; Eberts and Ulges, 2019) for simultaneously resolving and predicting entities and relations over longer contexts.",
      "startOffset" : 129,
      "endOffset" : 197
    }, {
      "referenceID" : 41,
      "context" : "Thus, we evaluate our model against two state-of-the-art models for jointly predicting entities and relations in scientific-text domain, namely DyGIE++ (Wadden et al., 2019) and spERT (Eberts and Ulges, 2019), on the WLP-MSTG.",
      "startOffset" : 152,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and spERT (Eberts and Ulges, 2019), on the WLP-MSTG.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 40,
      "context" : "We also evaluate our model’s performance on WNUT 2020 (Tabassum et al., 2020) corpus.",
      "startOffset" : 54,
      "endOffset" : 77
    } ],
    "year" : 2021,
    "abstractText" : "Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intrasentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models DyGIE++:28.17%; spERT:27.81%. We make our annotated WLP-MSTG corpus available to the research community. 1",
    "creator" : "LaTeX with hyperref"
  }
}