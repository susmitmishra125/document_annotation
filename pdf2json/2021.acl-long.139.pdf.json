{
  "name" : "2021.acl-long.139.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Systematic Investigation of KB-Text Embedding Alignment at Scale",
    "authors" : [ "Vardaan Pahuja", "Yu Gu", "Wenhu Chen", "Mehdi Bahrami", "Lei Liu", "Wei-Peng Chen", "Yu Su" ],
    "emails" : [ "su.809}@osu.edu,", "wenhuchen@cs.ucsb.edu", "wchen}@fujitsu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1764–1774\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1764"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase (Bollacker et al., 2007), DBPedia (Auer et al., 2007), YAGO (Suchanek et al., 2007) and Wikidata (Vrandečić and Krötzsch, 2014). These KBs store facts about real-world entities (e.g. people, places, and things) in the form of RDF triples, i.e. (subject, predicate, object). Today’s KBs are massive in scale. For instance, Freebase contains over 45 million entities and 3 billion facts involving a large variety of relations. Such large-scale multi-relational knowledge provides a great potential for improving a wide range of tasks, from information retrieval (Castells et al., 2007; Shen et al., 2015),\n1Code and data are available at https://github. com/dki-lab/joint-kb-text-embedding.\nquestion answering (Yao and Van Durme, 2014; Yu et al., 2017) to biological data mining (Zheng et al., 2020b).\nKB embedding models (Bordes et al., 2013; Dong et al., 2014; Lin et al., 2015) embed entities and relations into vector space(s) such that the embeddings capture the symbolic knowledge present in the KB. Similarly, word embedding models (Mikolov et al., 2013b; Pennington et al., 2014) learn continuous vector representations that capture the distributional semantics of words. Experiments on analogical reasoning (Mikolov et al., 2013b; Gladkova et al., 2016) and multilingual word embedding alignment (Mikolov et al., 2013a) have shown that there exists a linear structure in the word embedding space encoding relational information. On the other hand, translation-based KB embedding models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), by construction, also present a linear structure in their embedding space.\nA natural question then is, can we align the two embedding spaces such that they mutually enhance each other? Such alignment could poten-\ntially inject structured knowledge from KBs into text embeddings and inject unstructured but more timely-updated knowledge from text into KB embeddings, leading to more universal and comprehensive embeddings (Figure 1). Several studies have attempted at this. Lao et al. (2012) use the Path-Ranking Algorithm (Lao and Cohen, 2010) on combined text and KB to improve binary relation prediction. Gardner et al. (2014) leverage text data to enhance KB inference and help address the incompleteness of KBs. Toutanova et al. (2015) augment the KB with facts and relations from the text corpus and learn joint embedding for entities, KB relations and textual relations. Enhancement of KB entity embeddings using using Entity Descriptions has been attempted in (Zhong et al., 2015; Xie et al., 2016). Wang et al. (2014) propose to jointly embed entities and words in the same vector space. The alignment of embeddings of words and entities is accomplished using Wikipedia anchors or entity names.\nHowever, existing studies are still ad-hoc and a more systematic investigation of KB-text embedding alignment is needed to answer an array of important open questions: What is the best way to align the KB and text embedding spaces? To what degree can such alignment inject information from one source to another? How to balance the alignment loss with the original embedding losses? In this work, we conduct a systematic investigation of KB-text embedding alignment at scale and seek to answer these questions. Our investigation uses the latest version of the full Wikidata (Vrandečić and Krötzsch, 2014) as the KB, the full Wikipedia as the text corpus, and the shared entities as anchors for alignment. We define two tasks, few-shot link prediction and analogical reasoning, to evaluate the effectiveness of injecting text information into KB embeddings and injecting KB information into text embeddings, respectively, based on which we evaluate and compare an array of embedding alignment methods. The results and discussion present new insights about this important problem. Finally, using COVID-19 as a case study, we also demonstrate that such alignment can effectively inject text information into KB embeddings to complete KBs on emerging entities and events.\nIn summary, our contributions are three-fold:\n1. We conduct the first systematic investigation on KB-text embedding alignment at scale and propose and compare multiple effective align-\nment methods.\n2. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, to facilitate future research on this important problem.\n3. We have also learned joint KB-text embeddings on the largest-scale data to date and will release the embeddings as a valuable resource to the community."
    }, {
      "heading" : "2 Related Work",
      "text" : "KB-KB embedding alignment. Most existing knowledge bases are incomplete. Learning of distributed representations for entities and relations in knowledge bases finds application in the task of link prediction i.e. to infer missing facts in the KB given the known facts. This includes translationbased models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), feed-forward neural network based approaches (Socher et al., 2013; Dong et al., 2014), convolutional neural networks (Dettmers et al., 2018; Nguyen et al., 2018) and models that leverage graph neural networks (Schlichtkrull et al., 2018; Shang et al., 2019; Nathani et al., 2019). Recently, many research works have focused on the alignment of embedding spaces of heterogeneous data sources such as different KBs. JE (Hao et al., 2016) introduces a projection matrix to align the embedding spaces of different KBs. MTransE (Chen et al., 2017) first learns the embeddings of entities and relations in each language independently and then learns the transformation between these embedding spaces. Wang et al. (2018) use Graph Convolutional networks and a set of prealigned entities to learn embeddings of entities in multilingual KBs in a unified vector space. In the present work, we focus on aligning the KB and textual embedding spaces. KB-text joint representation. Many recent approaches have attempted to learn the embeddings of words and knowledge base entities in the same vector space. Wang et al. (2014) propose an alignment technique for KB and text representations using entity names and/or anchors. Wikipedia2Vec (Yamada et al., 2016) extends the skip-gram based model by modeling entity-entity co-occurrences using a link graph and word-entity co-occurrences using KB anchors. However, an entity mention can be ambiguous i.e. it can refer to different entities in different contexts. To resolve this, Cao\net al. (2017) propose Multi-Prototype Entity Mention Embedding model to learn representations for different senses of entity mentions. It includes a mention sense embedding model which uses context words and a set of reference entities to predict the actual entity referred to by the mention. Despite this progress, a comprehensive investigation of the merits of different alignment approaches is missing. Our work takes a step forward in this direction and proposes a novel evaluation framework to compare multiple alignment approaches for KB-Text joint embedding on a large-scale KB and textual corpus."
    }, {
      "heading" : "3 Model",
      "text" : "In this section, we describe the four alignment methods used in our study. At first, we describe the component models used in all alignment methods - the KB embedding model and the skip-gram model."
    }, {
      "heading" : "3.1 Knowledge Base embedding model",
      "text" : "We use the TransE model (Bordes et al., 2013) to learn the KB embeddings. We use the loss function proposed in Sun et al. (2019) as our KB embedding objective.\nLKB =∑ (h,r,t)∈S∪S′ log(1 + exp(y ∗ (−γ + dr(h, t))))\nHere, dr(h, t) = ‖h+ r − t‖2 denotes the score function for the triple (h, r, t), S denotes the set of positive triples and S ′ denotes the set of corrupted triples obtained by replacing the head or tail of a positive triple with a random entity. γ is a hyperparameter which denotes the margin and y denotes the label (+1 for positive triple and -1 for negative triple)."
    }, {
      "heading" : "3.2 Skip-gram model",
      "text" : "The skip-gram model learns the embeddings of words and entities by modeling the word-word, word-entity and entity-entity co-occurrences. We use the skip-gram model proposed in Yamada et al. (2016) for learning the word and entity representations. LetW and E denote the set of all words and entities in the vocabulary respectively and c denote the size of the context window.\n• Word-Word co-occurrence model: The skip-gram model is trained to predict the target word given a context word. Given a se-\nquence ofN wordsw1, w2, · · · , wN , the skipgram model maximizes the following objective:\nLww = N∑\nn=1 ∑ −c≤j≤c;j 6=0 log P (wn+j |wn)\nwhere p(wO|wI) = exp(v\n′ wI T vwO )∑\nw∈W exp(v ′ wI T vw)\n. Here,\nv ′ w and vw denote the input and output representations of the word w respectively. The input representations are used as the final representations for both words and entities.\n• Word-Entity co-occurrence model: In the word-entity co-occurrence model, the model is trained to predict the context words of an entity pointed to by the target anchor. The training objective corresponding to the wordentity co-occurrences is\nLwe = ∑\n(ei,Cei )∈A ∑ wo∈Cei log p(wo|ei)\nHere, A denotes the set of anchors in the corpus. Each anchor consists of an entity ei and its context words (represented by Cei). The conditional probability p(wo|ei) is given by:\np(wO|ei) = exp(v\n′ ei T vwO)∑\nw∈W exp(v ′ ei T vw)\n• Entity-Entity co-occurrence model: The entity-entity co-occurrence model learns to predict incoming links of an entity (denoted by Ce) given an entity e.\nLee = ∑ ei∈E ∑ eo∈Cei ;ei 6=eo log p(eo|ei)\np(eO|ei) = exp(v\n′ ei T veO)∑\ne∈E exp(v ′ ei T ve)\nIn practice, the probabilities involved in the skipgram model are estimated using negative sampling (Mikolov et al., 2013b). The overall objective is the sum of the three objectives for each type of co-occurrence.\nLSG = Lww + Lwe + Lee"
    }, {
      "heading" : "3.3 Alignment methods",
      "text" : "We align the entity pairs in KB and text corpus using a set of seed entity pairs, which are obtained from a mapping between Wikidata and Wikipedia. This mapping is constructed from the metadata associated with the Wikidata entities. The set of entities present in the TransE model and the skip-gram model is denoted by ETE and ESG respectively.\n(a) Alignment using same embedding: In this approach, we use the same embedding for the shared entities in the KB and text corpus. There is no separate alignment loss for this method.\n(b) Alignment using Projection: Inspired by the multilingual word embedding approaches (Mikolov et al., 2013a; Faruqui and Dyer, 2014) which use a linear transformation to map word embeddings from one space to another, we use an affine transformation from the skip-gram vector space to the TransE vector space to align the entity representations.\nThe alignment loss is calculated as a squared L2 norm between the transformed skip-gram entity embeddings and the corresponding TransE entity embeddings. The vectors eTE and eSG denote the TransE and skip-gram\nversions of embeddings of the entity e respectively.\nLalign =∑ e∈ESG∩ETE ‖(WeSG + b)− eTE‖22\n(c) Alignment using Entity Names: In this alignment technique inspired by Wang et al. (2014), for a particular triple (h, r, t) in the KB, if an equivalent entity eh exists in the text corpus, we add an additional triple (eh, r, t) to the KB. Similarly, if an equivalent entity et also exists for the entity t, we add the triples (h, r, et) and (eh, r, et) to the KB. The term “name graph” is used to denote this subgraph of additional triples.\nLalign =∑ (h,r,t) ∈ KB 1[h∈ESG∧t∈ESG]dr(wh,wt)+\n1[t∈ESG]dr(h,wt) + 1[h∈ESG]dr(wh, t)\n(d) Alignment using Wikipedia Anchors This alignment technique is motivated by a similar technique proposed in Wang et al. (2014). Here, we introduce an alignment loss term in which for word-entity co-occurrences, we substitute the textual entity embedding by its KB\ncounterpart in the skip-gram objective. Let eite denote the embedding of the KB entity equivalent to the textual entity ei.\nLalign =∑ (ei,Cei )∈A ∑ wo∈Cei log σ(exp(eite T vwO))+\nk∑ i=1 Ewi∼Pn(W)[log σ(−exp(e i te T vwi))]\nHere, Pn(W) denotes the noise distribution over words and k is the number of negative samples.\nThe final objective for training these models becomes\nL = LKB + LSG + λLalign\nHere, λ denotes the balance parameter which controls the extent of influence of alignment on the embeddings of each of the individual vector spaces. An illustration of the different alignment methods used in our study is given in Figure 2."
    }, {
      "heading" : "4 Dataset",
      "text" : "We use Wikipedia as the text corpus and Wikidata (Vrandečić and Krötzsch, 2014) as the knowledge base. We use the Wikidata version dated 16 December 2020 and the Wikipedia version dated 3 December 2020 for all of our experiments. The term support set (as used in the subsequent sections), denoted by S , is used to refer to the intersection set of Wikidata entities and entities in Wikipedia for which an article is present. Dataset preprocessing. We pre-process the original set of Wikidata triples and filter out entities and relations with frequency less than 10 and 5 respectively. This results in a KB with 14.64 M entities, 1222 relations, and 261 M facts. Similarly, we preprocess Wikipedia and filter out words from the vocabulary with frequency less than 10. However, we utilize the entire entity set of Wikipedia to maximize the size of the support set. After processing, the Wikipedia vocabulary consists of 2.1 M words and 12.3 M entities."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We compare the performance of different alignment methods using two evaluation tasks - few-shot link\nprediction and analogical reasoning. The fewshot link prediction task is designed to test the capability of the alignment model to inject the relational information present in text into the knowledge base embeddings. The train-test set for this task is constructed such that the test set contains triples corresponding to a subset of entities in the support set, but each of these entities is observed only once in the training triples set. Thus, the model is tasked to do link prediction on entities that occur rarely in the training set (hence the term “few-shot”). The training and test sets consist of 260.1 M and 110.8 K triples respectively. For this setting, both entities of each triple in the test set are contained in the support set.\nThe purpose of the analogical reasoning task is to test the information flow from the knowledge-base embeddings to the skip-gram embeddings. This task was first proposed in Mikolov et al. (2013b) to test the syntactic and semantic information present in learned word embeddings. We choose the top 50 relations from the set of one-to-one and manyto-one relations based on the frequency of occurrence and construct a dataset of 1000 analogical reasoning examples for each relation. The 1st pair of entities is randomly chosen from the training triples set, as the pair of entities involved in that relation. The 2nd pair of entities is obtained from the test triples set. More formally, given a pair of entities (h1, t1) and the head entity of the 2nd pair (h2), the task is to predict the tail entity (t2) of the 2nd pair by comparing the cosine similarity between the embedding of candidate entity (et2) and (eh2 + et1 − eh1).\nEvaluation protocol. For link prediction evaluation on a given test triple (h, r, t), we corrupt either the head entity (by generating triplets like (h ′ , r, t)) or the tail entity (by generating triplets like (h, r, t ′ )) of the triple and then rank the score of correct entity amongst all entities in the candidate set. Due to the extremely large entity vocabulary size in Wikidata, we restrict the size of the candidate set to a sample of 1000 entities whose types lie in the set of permissible domain/range types for that relation (Lerer et al., 2019; Krompaß et al., 2015). In cases where the number of such entities is less than 1000, we choose the entire set of those entities. In addition, we filter any positive triplets (triplets that exist in the KB) from the set of negative triplets for this evaluation, also known as filtered evaluation setting. We report results on\nstandard evaluation metrics - Mean Rank (MR), Hits@1, and Hits@10. For this task, we compare the TransE model and the KB-side embeddings of different alignment methods.\nFor the analogical reasoning task, we report Mean Rank (MR), Hits@1, and Hits@10 by ranking the correct entity t2 against the entities in the candidate set. The candidate set for the tail entity t2 is a set of 1K entities sampled from the support set (excluding h1, h2 and t1) according to the node degree. All reported metrics are macro-averaged over the results for different relations. Here, we compare the skip-gram model embeddings with the textual embeddings obtained from different alignment methods."
    }, {
      "heading" : "5.2 Implementation",
      "text" : "The scale of the training data (both the Wikidata Knowledge Base and the Wikipedia corpus) is huge, so the efficient implementation of the model is a key challenge. For efficient implementation of the TransE model, we used the DGL-KE (Zheng et al., 2020a) library. It uses graph partitioning to train across multiple partitions of the knowledge base in parallel and incorporates engineering optimizations like efficient negative sampling to reduce the training time by orders of magnitude compared to naive implementations. The skip-gram model is implemented using PyTorch (Paszke et al., 2019) and Wikipedia2vec (Yamada et al., 2020) libraries.\nFor training, we optimize the parameters of the TransE and skip-gram models alternately in each epoch. We use the Adagrad (Duchi et al., 2011) optimizer for the KBE model and SGD for the skip-gram model. For both models, the training is done by multiple processes asynchronously using the Hogwild (Niu et al., 2011) approach. This introduces additional challenges like synchronizing the weights of parameters among different training processes. We choose the values of balance parameter for each of the two evaluation tasks based on the performance of aligned KB and textual embeddings on a small set of analogy examples (disjoint from the analogy test set used in the main evaluation). Our implementation can serve as a good resource to do a similar large-scale analysis of KBText alignment approaches in the future."
    }, {
      "heading" : "5.3 Overall Results",
      "text" : "The overall results for the two evaluation tasks are given in Table 1. For the few-shot link prediction task, we observe that all the alignment tech-\nniques lead to improved performance of the KB embeddings over the naive TransE baseline. The Same Embedding alignment approach performs the best followed by Entity Name alignment, Projection, and alignment using Wikipedia Anchors. The use of the same embeddings for the shared entities helps in propagating the factual knowledge present in the text to the KB more efficiently, so the Same Embedding alignment performs better than others. The Entity Name alignment approach is worse than the Same embedding alignment approach since the test set entities occur less often in the train set (as the dataset is few-shot). So, the name graph doesn’t make a substantial difference here.\nFor the analogical reasoning task, the results show that all alignment approaches obtain an improvement over the naive skip-gram baseline. The Entity Name alignment approach performs the best followed by Projection, Same Embedding alignment, and alignment using Wikipedia Anchors. The good performance of the Entity Name alignment approach could be explained by the fact that for every test analogy example (eh1 , et1 , eh2 , et2), there is a relation r present between the entity pairs (eh1 , et1) and (eh2 , et2), although that is unobserved. Since eh and et also occur in the KB, due to the extra added triples, the KB reasoning process incorporates the relation r in these embeddings, just like it does for KB entities h and t. The other approaches viz. Same Embedding alignment, Projection, and Wikipedia Anchors don’t have a mechanism for explicit KB reasoning like the Entity Name alignment approach. The Projection technique outperforms the Same Embedding alignment as the embeddings in the two spaces are less tightly coupled in the former, so it can take advantage of the complementary relational information in textual as well as the KB embeddings."
    }, {
      "heading" : "5.4 Fine-grained Analysis",
      "text" : "In this section, we present a fine-grained analysis of the efficacy of the alignment methods w.r.t. changes in training data size and whether the test set entities belong to the support set. We also study the impact of balance parameter on the performance of the two evaluation tasks. Due to resource constraint, we do this analysis on two representative methods of different nature - Projection alignment and Same Embedding alignment. Effect of Training data size. To study and differentiate the impact of entities present in the support\nset on the performance of the few-shot link prediction task, we create two versions of the training set with different sizes:\n(a) Full version: In this version of the training set, we include all triples in Wikidata which don’t violate the few-shot property of the dataset. This is the same as the training set for the evaluation proposed in Section 5.1.\n(b) Support version: In this version of the training set, we exclude triples from the full version whose either head or tail entity isn’t present in the support set.\nNext, we try to analyze the impact of whether the head/tail entity of the test triple is present in the support set S, on the few-shot link prediction performance. To this end, we create two versions of test sets:\n(a) Both in support: Both head and tail entity of the triple lie in the support set.\n(b) Missing support: Atleast one out of the head/tail entity of the triple doesn’t lie in the support set.\nThe statistics for this dataset are given in Table 3. The results for the training data size analysis for different alignment methods on Test set (Both in support) are shown in Table 4. The results show that for both Projection and Same Embedding alignment approach, the performance is significantly better with using the full training set of triples instead of just the support set. This shows that triples involving non-support set entities play a vital role in helping learn better entity and relation representations which in turn helps in injecting textual information to the KB embeddings via alignment. Effect of Support set for Test triples. Here, we investigate the performance of the few-shot link prediction task for triples whose entities may not\nlie in the support set. The results for this evaluation are given in Table 5. We observe that there is no significant gain in performance for any of the alignment methods over the simple TransE baseline. This shows these alignment methods are only effective for triples whose both entities lie in the support set.\nEffect of balance parameter. In this analysis, we study the role of balance parameter for the Projection alignment method. This parameter controls the extent of alignment between the two embedding spaces. The higher the value of the balance parameter, the more the embedding tries to capture the entity information from the other embedding space, rather than its own. The results of this study are shown in Table 2. The peak performance for the few-shot link prediction task is obtained for balance parameter = 1e0 in terms of Hits@1 and Hits@10. Whereas, for the analogical reasoning task, the peak performance is obtained for balance parameter = 1e-3. This difference in the optimal value of the balance parameter can be explained by the fact that the skip-gram objective relies on cosine similarity which is more sensitive to changes in the values of vector embeddings than the TransE model. We show this analytically. Let (h, r, t) be a KB triple and let h, r, and t denote the embeddings of h, r, and t respectively. The partial derivative of score function of the triple w.r.t. h is given by\ndr(h, t) = ‖h+ r − t‖2∥∥∥∥∂dr(h, t)∂h ∥∥∥∥ 2 = ∥∥∥∥ (h+ r − t)‖h+ r − t‖2 ∥∥∥∥ 2 = 1\nSimilarly, let (u, v) be an entity-word pair in the text corpus. Let u and v denote the embeddings of u and v respectively. The partial derivative of the score function for the entity-word pair (u, v) w.r.t.\nu is given by\nd(u,v) = exp(uTv)∥∥∥∥∂d(u,v)∂u ∥∥∥∥ 2 = ∥∥(uTv)v∥∥ 2 = (uTv) ‖v‖2\nThe value of ∥∥∥∂dr(h,t)∂h ∥∥∥2 equals 1 whereas for\nthe skip-gram model, ∥∥∥∂d(u,v)∂u ∥∥∥2 = (uTv) ‖v‖2 which is greater than 1, as seen empirically. This shows that the skip-gram embeddings are more sensitive to delta changes in values of the parameters. For them to be reasonably assigned with their KB counterparts without losing the textual information, thus a lower value of balance parameter is optimal."
    }, {
      "heading" : "5.5 Case study on COVID related triples",
      "text" : "Recently, the COVID pandemic (Fauci et al., 2020) has been responsible for bringing a tremendous change in the lives of people across the globe. Through this case study, we demonstrate that aligning embedding representations can help us do knowledge base completion for recent events like COVID-19. We selected 4 relevant relations (“Risk factor”, “Symptoms”, “Medical Condition” and “Cause of Death”) with atleast 10 triples in the difference between March 2020 and December 2020 snapshots of Wikidata. We use the March 2020 Wikidata and December 2020 Wikipedia to train the alignment models and do link prediction on these triples. For each of the relations, we keep the COVID-19 entity (Entity ID: Q84263196) unchanged and corrupt the other entity in the triple. This would correspond to asking questions like “What are the symptoms of COVID-19?”, “Who died due to COVID-19?” etc. The results are shown in Table 6.\nWe observe that the Projection model obtains a decent improvement over the TransE model on the link prediction task on these triples in terms of Mean Rank. Similarly, the Same Embedding alignment model obtains outperforms the TransE baseline for three out of four relations. This case study gives a real-life use-case of how the text information can be injected into the KB embeddings using alignment in scenarios when such information is not yet curated in the KB in structured form."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we presented a systematic study of different alignment approaches that can be applied to align entity representations in a knowledge base and textual corpora. By evaluating on the few-shot link prediction task and analogical reasoning task, we found that although all approaches have the desired outcome, i.e., to incorporate information from the other modality, some approaches perform better than others on a particular task. We also analyzed the impact of different factors such as the size of the training set, the presence of test set entities in the support set, and the balance parameter on the evaluation task performance. We believe our evaluation framework, as well as jointly trained embeddings can serve as a useful resource for future research and applications."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored by a gift grant from Fujitsu and the Ohio Supercomputer Center (Center, 1987)."
    } ],
    "references" : [ {
      "title" : "Dbpedia: A nucleus for a web of open data",
      "author" : [ "Sören Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives." ],
      "venue" : "The semantic web, pages 722–735. Springer.",
      "citeRegEx" : "Auer et al\\.,? 2007",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "A platform for scalable, collaborative, structured information integration",
      "author" : [ "Kurt Bollacker", "Patrick Tufts", "Tomi Pierce", "Robert Cook." ],
      "venue" : "Intl. Workshop on Information Integration on the Web (IIWeb’07), pages 22–27.",
      "citeRegEx" : "Bollacker et al\\.,? 2007",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2007
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Bridge text and knowledge by learning multi-prototype entity mention embedding",
      "author" : [ "Yixin Cao", "Lifu Huang", "Heng Ji", "Xu Chen", "Juanzi Li." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Cao et al\\.,? 2017",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2017
    }, {
      "title" : "An adaptation of the vector-space model for ontologybased information retrieval",
      "author" : [ "P. Castells", "M. Fernández", "D. Vallet." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 19.",
      "citeRegEx" : "Castells et al\\.,? 2007",
      "shortCiteRegEx" : "Castells et al\\.",
      "year" : 2007
    }, {
      "title" : "Multilingual knowledge graph embeddings for cross-lingual knowledge alignment",
      "author" : [ "Muhao Chen", "Yingtao Tian", "Mohan Yang", "Carlo Zaniolo." ],
      "venue" : "Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI’17, page",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional 2d knowledge graph embeddings",
      "author" : [ "Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Dettmers et al\\.,? 2018",
      "shortCiteRegEx" : "Dettmers et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang." ],
      "venue" : "Proceedings of the 20th ACM",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of machine learning research, 12(7).",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving vector space word representations using multilingual correlation",
      "author" : [ "Manaal Faruqui", "Chris Dyer." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471.",
      "citeRegEx" : "Faruqui and Dyer.,? 2014",
      "shortCiteRegEx" : "Faruqui and Dyer.",
      "year" : 2014
    }, {
      "title" : "Covid-19—navigating the uncharted",
      "author" : [ "Anthony S Fauci", "H Clifford Lane", "Robert R Redfield" ],
      "venue" : null,
      "citeRegEx" : "Fauci et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fauci et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating vector space similarity in random walk inference over knowledge bases",
      "author" : [ "Matt Gardner", "Partha Talukdar", "Jayant Krishnamurthy", "Tom Mitchell." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing",
      "citeRegEx" : "Gardner et al\\.,? 2014",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2014
    }, {
      "title" : "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn’t",
      "author" : [ "Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 8–",
      "citeRegEx" : "Gladkova et al\\.,? 2016",
      "shortCiteRegEx" : "Gladkova et al\\.",
      "year" : 2016
    }, {
      "title" : "A joint embedding method for entity alignment of knowledge bases",
      "author" : [ "Yanchao Hao", "Yuanzhe Zhang", "Shizhu He", "Kang Liu", "Jun Zhao." ],
      "venue" : "China Conference on Knowledge Graph and Semantic Computing, pages 3–14. Springer.",
      "citeRegEx" : "Hao et al\\.,? 2016",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge graph embedding via dynamic mapping matrix",
      "author" : [ "Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Ji et al\\.,? 2015",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2015
    }, {
      "title" : "Type-constrained representation learning in knowledge graphs",
      "author" : [ "Denis Krompaß", "Stephan Baier", "Volker Tresp." ],
      "venue" : "International semantic web conference, pages 640–655. Springer.",
      "citeRegEx" : "Krompaß et al\\.,? 2015",
      "shortCiteRegEx" : "Krompaß et al\\.",
      "year" : 2015
    }, {
      "title" : "Relational retrieval using a combination of path-constrained random walks",
      "author" : [ "Ni Lao", "William W Cohen." ],
      "venue" : "Machine learning, 81(1):53–67.",
      "citeRegEx" : "Lao and Cohen.,? 2010",
      "shortCiteRegEx" : "Lao and Cohen.",
      "year" : 2010
    }, {
      "title" : "Reading the web with learned syntactic-semantic inference rules",
      "author" : [ "Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William Cohen." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
      "citeRegEx" : "Lao et al\\.,? 2012",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2012
    }, {
      "title" : "PyTorch-BigGraph: A Largescale Graph Embedding System",
      "author" : [ "Adam Lerer", "Ledell Wu", "Jiajun Shen", "Timothee Lacroix", "Luca Wehrstedt", "Abhijit Bose", "Alex Peysakhovich." ],
      "venue" : "Proceedings of the 2nd SysML Conference, Palo Alto, CA, USA.",
      "citeRegEx" : "Lerer et al\\.,? 2019",
      "shortCiteRegEx" : "Lerer et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning entity and relation embeddings for knowledge graph completion",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu." ],
      "venue" : "Twenty-ninth AAAI conference on artificial intelligence.",
      "citeRegEx" : "Lin et al\\.,? 2015",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V Le", "Ilya Sutskever." ],
      "venue" : "arXiv preprint arXiv:1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning attention-based embeddings for relation prediction in knowledge graphs",
      "author" : [ "Deepak Nathani", "Jatin Chauhan", "Charu Sharma", "Manohar Kaul." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Nathani et al\\.,? 2019",
      "shortCiteRegEx" : "Nathani et al\\.",
      "year" : 2019
    }, {
      "title" : "A novel embedding model for knowledge base completion based on convolutional neural network",
      "author" : [ "Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dat Quoc Nguyen", "Dinh Phung." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Nguyen et al\\.,? 2018",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2018
    }, {
      "title" : "Hogwild! a lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Feng Niu", "Benjamin Recht", "Christopher Re", "Stephen J. Wright." ],
      "venue" : "Proceedings of the 24th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Niu et al\\.,? 2011",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2011
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Schlichtkrull", "Thomas N Kipf", "Peter Bloem", "Rianne Van Den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "European Semantic Web Conference, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end structure-aware convolutional networks for knowledge base completion",
      "author" : [ "Chao Shang", "Yun Tang", "Jing Huang", "Jinbo Bi", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,",
      "citeRegEx" : "Shang et al\\.,? 2019",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity linking with a knowledge base: Issues, techniques, and solutions",
      "author" : [ "Wei Shen", "Jianyong Wang", "Jiawei Han." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 27:443–460.",
      "citeRegEx" : "Shen et al\\.,? 2015",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2015
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng." ],
      "venue" : "Advances in neural information processing systems, pages 926–934.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Yago: a core of semantic knowledge",
      "author" : [ "Fabian M Suchanek", "Gjergji Kasneci", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web, pages 697–706.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Rotate: Knowledge graph embedding by relational rotation in complex space",
      "author" : [ "Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Representing text for joint embedding of text and knowledge bases",
      "author" : [ "Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural",
      "citeRegEx" : "Toutanova et al\\.,? 2015",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2015
    }, {
      "title" : "Wikidata: a free collaborative knowledgebase",
      "author" : [ "Denny Vrandečić", "Markus Krötzsch." ],
      "venue" : "Communications of the ACM, 57(10):78–85.",
      "citeRegEx" : "Vrandečić and Krötzsch.,? 2014",
      "shortCiteRegEx" : "Vrandečić and Krötzsch.",
      "year" : 2014
    }, {
      "title" : "Knowledge graph and text jointly embedding",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1591–1601.",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Cross-lingual knowledge graph alignment via graph convolutional networks",
      "author" : [ "Zhichun Wang", "Qingsong Lv", "Xiaohan Lan", "Yu Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 349–",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Representation learning of knowledge graphs with entity descriptions",
      "author" : [ "Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Xie et al\\.,? 2016",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2016
    }, {
      "title" : "Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Jin Sakuma", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji", "Yuji Matsumoto." ],
      "venue" : "In",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint learning of the embedding of words and entities for named entity disambiguation",
      "author" : [ "Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language",
      "citeRegEx" : "Yamada et al\\.,? 2016",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2016
    }, {
      "title" : "Information extraction over structured data: Question answering with Freebase",
      "author" : [ "Xuchen Yao", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Yao and Durme.,? 2014",
      "shortCiteRegEx" : "Yao and Durme.",
      "year" : 2014
    }, {
      "title" : "Improved neural relation detection for knowledge base question answering",
      "author" : [ "Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Dgl-ke: Training knowledge graph embeddings at scale",
      "author" : [ "Da Zheng", "Xiang Song", "Chao Ma", "Zeyuan Tan", "Zihao Ye", "Jin Dong", "Hao Xiong", "Zheng Zhang", "George Karypis." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research",
      "citeRegEx" : "Zheng et al\\.,? 2020a",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Pharmkg: a dedicated knowledge graph benchmark for bomedical data mining",
      "author" : [ "Shuangjia Zheng", "J. Rao", "Y. Song", "Jixian Zhang", "Xianglu Xiao", "E. Fang", "Yuedong Yang", "Zhangming Niu." ],
      "venue" : "Briefings in bioinformatics.",
      "citeRegEx" : "Zheng et al\\.,? 2020b",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning knowledge and text embeddings by entity descriptions",
      "author" : [ "Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267–272.",
      "citeRegEx" : "Zhong et al\\.,? 2015",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase (Bollacker et al., 2007), DBPedia (Auer et al.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : ", 2007), DBPedia (Auer et al., 2007), YAGO (Suchanek et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 31,
      "context" : ", 2007), YAGO (Suchanek et al., 2007) and Wikidata (Vrandečić and Krötzsch, 2014).",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Such large-scale multi-relational knowledge provides a great potential for improving a wide range of tasks, from information retrieval (Castells et al., 2007; Shen et al., 2015),",
      "startOffset" : 135,
      "endOffset" : 177
    }, {
      "referenceID" : 29,
      "context" : "Such large-scale multi-relational knowledge provides a great potential for improving a wide range of tasks, from information retrieval (Castells et al., 2007; Shen et al., 2015),",
      "startOffset" : 135,
      "endOffset" : 177
    }, {
      "referenceID" : 41,
      "context" : "question answering (Yao and Van Durme, 2014; Yu et al., 2017) to biological data mining (Zheng et al.",
      "startOffset" : 19,
      "endOffset" : 61
    }, {
      "referenceID" : 43,
      "context" : ", 2017) to biological data mining (Zheng et al., 2020b).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "KB embedding models (Bordes et al., 2013; Dong et al., 2014; Lin et al., 2015) embed entities and relations into vector space(s) such that the embeddings capture the symbolic knowledge present in the KB.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "KB embedding models (Bordes et al., 2013; Dong et al., 2014; Lin et al., 2015) embed entities and relations into vector space(s) such that the embeddings capture the symbolic knowledge present in the KB.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "KB embedding models (Bordes et al., 2013; Dong et al., 2014; Lin et al., 2015) embed entities and relations into vector space(s) such that the embeddings capture the symbolic knowledge present in the KB.",
      "startOffset" : 20,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "Similarly, word embedding models (Mikolov et al., 2013b; Pennington et al., 2014) learn continuous vector representations that capture the distributional semantics of words.",
      "startOffset" : 33,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "Similarly, word embedding models (Mikolov et al., 2013b; Pennington et al., 2014) learn continuous vector representations that capture the distributional semantics of words.",
      "startOffset" : 33,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "Experiments on analogical reasoning (Mikolov et al., 2013b; Gladkova et al., 2016) and multilingual word embedding alignment (Mikolov et al.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "Experiments on analogical reasoning (Mikolov et al., 2013b; Gladkova et al., 2016) and multilingual word embedding alignment (Mikolov et al.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : ", 2016) and multilingual word embedding alignment (Mikolov et al., 2013a) have shown that there exists a linear structure in the word embedding space encoding relational information.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, translation-based KB embedding models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), by construction, also present a linear structure in their embedding space.",
      "startOffset" : 57,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, translation-based KB embedding models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), by construction, also present a linear structure in their embedding space.",
      "startOffset" : 57,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "On the other hand, translation-based KB embedding models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), by construction, also present a linear structure in their embedding space.",
      "startOffset" : 57,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "(2012) use the Path-Ranking Algorithm (Lao and Cohen, 2010) on combined text and KB to improve binary relation prediction.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 44,
      "context" : "Enhancement of KB entity embeddings using using Entity Descriptions has been attempted in (Zhong et al., 2015; Xie et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 128
    }, {
      "referenceID" : 37,
      "context" : "Enhancement of KB entity embeddings using using Entity Descriptions has been attempted in (Zhong et al., 2015; Xie et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 128
    }, {
      "referenceID" : 34,
      "context" : "Our investigation uses the latest version of the full Wikidata (Vrandečić and Krötzsch, 2014) as the KB, the full Wikipedia as the text corpus, and the shared entities as anchors for alignment.",
      "startOffset" : 63,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "This includes translationbased models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), feed-forward neural network based approaches (Socher et al.",
      "startOffset" : 38,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "This includes translationbased models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), feed-forward neural network based approaches (Socher et al.",
      "startOffset" : 38,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "This includes translationbased models (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015), feed-forward neural network based approaches (Socher et al.",
      "startOffset" : 38,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : ", 2015), feed-forward neural network based approaches (Socher et al., 2013; Dong et al., 2014), convolutional neural networks (Dettmers et al.",
      "startOffset" : 54,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : ", 2015), feed-forward neural network based approaches (Socher et al., 2013; Dong et al., 2014), convolutional neural networks (Dettmers et al.",
      "startOffset" : 54,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : ", 2014), convolutional neural networks (Dettmers et al., 2018; Nguyen et al., 2018) and models that lever-",
      "startOffset" : 39,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : ", 2014), convolutional neural networks (Dettmers et al., 2018; Nguyen et al., 2018) and models that lever-",
      "startOffset" : 39,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "JE (Hao et al., 2016) introduces a projection matrix to align the embedding spaces of different KBs.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "MTransE (Chen et al., 2017) first learns the embeddings of entities and relations in each language independently and then learns the transformation between these embedding spaces.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 39,
      "context" : "Wikipedia2Vec (Yamada et al., 2016) extends the skip-gram based model by modeling entity-entity co-occurrences using a link graph and word-entity co-occurrences using KB anchors.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "We use the TransE model (Bordes et al., 2013) to learn the KB embeddings.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "In practice, the probabilities involved in the skipgram model are estimated using negative sampling (Mikolov et al., 2013b).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "(b) Alignment using Projection: Inspired by the multilingual word embedding approaches (Mikolov et al., 2013a; Faruqui and Dyer, 2014) which use a linear transformation to map word embeddings from one space to another, we use an affine transformation from the skip-gram vector space to the TransE vector space to align the entity representations.",
      "startOffset" : 87,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "(b) Alignment using Projection: Inspired by the multilingual word embedding approaches (Mikolov et al., 2013a; Faruqui and Dyer, 2014) which use a linear transformation to map word embeddings from one space to another, we use an affine transformation from the skip-gram vector space to the TransE vector space to align the entity representations.",
      "startOffset" : 87,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : "We use Wikipedia as the text corpus and Wikidata (Vrandečić and Krötzsch, 2014) as the knowledge base.",
      "startOffset" : 49,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "Due to the extremely large entity vocabulary size in Wikidata, we restrict the size of the candidate set to a sample of 1000 entities whose types lie in the set of permissible domain/range types for that relation (Lerer et al., 2019; Krompaß et al., 2015).",
      "startOffset" : 213,
      "endOffset" : 255
    }, {
      "referenceID" : 15,
      "context" : "Due to the extremely large entity vocabulary size in Wikidata, we restrict the size of the candidate set to a sample of 1000 entities whose types lie in the set of permissible domain/range types for that relation (Lerer et al., 2019; Krompaß et al., 2015).",
      "startOffset" : 213,
      "endOffset" : 255
    }, {
      "referenceID" : 42,
      "context" : "For efficient implementation of the TransE model, we used the DGL-KE (Zheng et al., 2020a) library.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : "implemented using PyTorch (Paszke et al., 2019) and Wikipedia2vec (Yamada et al.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 38,
      "context" : ", 2019) and Wikipedia2vec (Yamada et al., 2020) libraries.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "We use the Adagrad (Duchi et al., 2011) optimizer for the KBE model and SGD for the skip-gram model.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "For both models, the training is done by multiple processes asynchronously using the Hogwild (Niu et al., 2011) approach.",
      "startOffset" : 93,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Recently, the COVID pandemic (Fauci et al., 2020) has been responsible for bringing a tremendous change in the lives of people across the globe.",
      "startOffset" : 29,
      "endOffset" : 49
    } ],
    "year" : 2021,
    "abstractText" : "Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support longrange reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods. We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study.1",
    "creator" : "LaTeX with hyperref"
  }
}