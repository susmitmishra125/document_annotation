{
  "name" : "2021.acl-long.430.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation",
    "authors" : [ "Wenqing Chen", "Jidong Tian", "Yitian Li", "Hao He", "Yaohui Jin" ],
    "emails" : [ "jinyh}@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5532–5542\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5532"
    }, {
      "heading" : "1 Introduction",
      "text" : "Data-to-text generation refers to the task of generating descriptive text from non-linguistic inputs. With the different types of inputs, this task can be\n∗ Corresponding Authors\ndefined more specifically, such as abstract meaning representation to text (Zhao et al., 2020; Bai et al., 2020a), infobox with key-value pairs to text (Bai et al., 2020b), graph-to-text (Song et al., 2020), and table-to-text (Wang et al., 2020; Parikh et al., 2020) generation.\nAmong these tasks, we focus on logical tableto-text generation, which aims to generate fluent and logically faithful text from tables (Chen et al., 2020a). And the ability of logical inference is a kind of high-level intelligence, which is nontrivial for text generation systems in reality. The task remains challenging because the reference sentences often convey logically inferred information, which is not explicitly presented in the table. As a consequence, data-driven models often generated linguistically fluent but logically inconsistent text. Recent progress on this task mainly lies in the use of pretrained language models (LMs) like GPT-2 (Radford et al., 2018), which was shown to perform much better than non-pretrained models (Chen et al., 2020a,e).\nHowever, it is still arguable that whether pretrained LMs can correctly capture the logics, as pretrained LMs like BERT would use spurious statistical cues for inference (Niven and Kao, 2019). The substantial difficulty for this task does not lay on whether to use the pretrained models or not. Instead, the difficulty is because the surface-level spurious correlations are easier to capture than the causal relationship between the table and the text. For example, we have observed that a model cooperating with GPT-2 generated a sentence \"The album was released in the United States 2 time\" for a given table. But the country where the album was released twice is \"the United Kingdom\"1. In the training stage, a model may get low training loss by utilizing the surface-level correlations without\n1The details of the table can be found in Section 5.6\nactually focusing on the selected entities. As a result, in the inference stage, the model is possible to produce incorrect facts.\nIn this paper, we view the logical table-totext generation from the perspective of causal inference and propose a de-confounded variational encoder-decoder (DCVED). Firstly, given the table-sentence pair (x,y), we assume confounders zc existed in the latent space and contributing to the surface-level correlations (e.g., \"the United States\" and \"the United Kingdom\"). We estimate zc in the latent space based on variational inference, and cooperate the causal intervention based on Pearl’s do-calculus (Pearl, 2010) to learn the objective p(y|do(x)) instead of p(y|x). Secondly, to make the latent confounder meaningful, we propose a back-prediction process to ensure the latent confounder zc can predict the not-used entities but linguistically similar to the exactly selected ones. We also consider the exactly selected entities as the mediators in our deconfounded architecture models. Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best text for the table. An extensive set of experiments show that our model achieves new stateof-the-art performance on two logical table-to-text datasets in terms of logical fidelity.\nThe main contributions of this work can be summarized as follows:\n• We propose to use variational inference to estimate the confounders in the latent space and cooperated with back-prediction to make the latent variable meaningful.\n• We propose a generate-then-select paradigm jointly considering the surface-level and logical fidelity, which can be considered as an alternative to reinforcement learning.\n• The experiments have shown that our model achieves new state-of-the-art performance on two logical table-to-text datasets with or without pretrained LMs."
    }, {
      "heading" : "2 Related Work",
      "text" : "Table-to-Text Generation. The task of table-totext generation belongs to the data-to-text generation, where a key feature is the structured input data. Lebret et al. (2016) used a seq2seq neural model with a field-infusing strategy that obtains\nfield-position-aware and field-words-aware cell embeddings to generate sentences from Wikipedia tables. A follow-up work proposed to update the cell memory of the LSTM by a field gate to help LSTM identify the boundary between different cells (Liu et al., 2018). Transformerbased (Vaswani et al., 2017) models were also proposed which improved the ability to capture longterm dependencies between cells (Ma et al., 2019; Wang et al., 2020; Chen et al., 2020a). It is worth to mention that the copy mechanism (Luong et al., 2015) is an important part to deal with the outof-vocabulary (OOV) words (Lebret et al., 2016; Gehrmann et al., 2018; Chen et al., 2020a) when not using pretrained language models.\nLogical Table-to-Text Generation. While usually fluent, existing methods often hallucinate phrases that contradict the facts in the table. To benchmark models’ ability to generate logically consistent sentences, recent work proposed a dataset collected from open domain (Chen et al., 2020a), which would score low on those models ignoring logical consistency. Follow-up work further proposed another dataset that involved logical forms as additional supervision information (Chen et al., 2020e), which includes common logic types paired with the underlying logical forms.\nCausal Inference. Machine learning models often suffer from the spurious statistical correlations brought by unmeasured or latent confounders (Keith et al., 2020). To eliminate the confounding bias, one approach is applying the causal intervention based on Pearl’s do-calculus (Pearl, 2010). However, it remains an open problem to choose proper confounders, and the language of text itself could be a confounder (Keith et al., 2020). It is worth noting that high-quality observations of the mediators can also reduce the confounding bias, as the models will reduce the possibility of counting on the confounders (Chen et al., 2020d)."
    }, {
      "heading" : "3 Backgrounds",
      "text" : "Before introducing our models, we briefly review the framework of VAE (Kingma and Welling, 2014), a generative model which allows to generate high-dimensional samples from a continuous space. In the probability model framework, the probability of data x can be computed by:\np(x) = ∫ p(x, z)dz = ∫ p(z)p(x|z)dz (1)\nwhere it is approximated by maximizing the evidence lower bound (ELBO):\nlog pθ(x) ≥ E z∼qϕ(z|x) [log pθ(x|z)]\n− KL(qϕ(z|x)∥p(z)) (2)\nwhere pθ(x|z) denotes the decoder with parameters θ and qϕ(z|x) is obtained by an encoder with parameters ϕ, and p(z) is a prior distribution, for example, a Gaussian distribution. And KL(·||·) denotes the Kullback-Leibler (KL) Divergence between two distributions.\nWhen applied to seq2seq generation where the input and the output are denoted by x and y respectively, the conditional variational autoencoder (CVAE), or often known as variational encoder-decoder (VED), is used with following approximation:\nlog pθ(y|x) ≥ E z∼qϕ(z|x,y) [log pθ(y|x, z)]\n− KL(qϕ(z|x,y)∥p(z|x)) (3)\nIn the vanilla CVAE formulation, such as the ones adopted in (Kingma et al., 2014; Jain et al., 2017), the prior distribution p(z|x) is approximated to p(z), which is independent on x and fixed to a zero-mean unit-variance Gaussian distribution N (0, I). However, this formulation is shown to induce a strong model bias (Tomczak and Welling, 2018) and empirically perform worse than non-variational models (Wang et al., 2017) in multi-modal situation."
    }, {
      "heading" : "4 Methodology",
      "text" : ""
    }, {
      "heading" : "4.1 De-Confounded VED",
      "text" : "From a human perspective, multiple sentences can properly describe a given table, varying with dif-\nferent concerns, different logical types or linguistic realizations. Therefore, given the input data x and the output sentence y, we can assume a latent variable z existed leading to a conditional generation process p(y|x, z) where z contributes to the diversity. It suggests a CVAE framework with Equation 3. However, as discussed in Section 3, the vanilla CVAE will introduce a model bias (Tomczak and Welling, 2018). In this subsection, we re-think the CVAE from the perspective of causal inference. We assume a directed acyclic graph (DAG) existed, which includes a mediator zm and a confounder zc as shown in Figure 1(a). The mediator is determined by x and has causal effects on y, while the confounder has causal effects on both x and y.\nWhen only considering zm, we can compute the probability distribution p(y|x) by:\np(y|x) = ∑ zm p(y|x, zm)p(zm|x)\n= Ezm∼pφ(zm|x)p(y|x, zm) (4)\nwhere φ denotes the parameters of a mediator predictor. An example for zm is the selected entity (e.g., United Kingdom) from the table x and exactly appeared in y. The vanilla CVAE will constrain zm in the continuous space, and further approximate the prior distribution p(zm|x) to p(zm), which produces biased information.\nHowever, it does not mean that removing the approximation between p(zm|x) and p(zm) is enough. We observe that models often rely on spurious statistical cues for prediction, resulting in some linguistically similar but inconsistent expressions in the generated sentences (e.g., using \"The United States\" instead of \"The United Kingdom). The model is possible to minimize the training loss relying on the surface-level correlations between the selected entity and the high-frequency entity. In this case, the high-frequency entity belongs to the confounder zc. In the inference stage, model may infer contradicting facts due to a high posterior probability of q(zc|x).\nTo eliminate the spurious correlations, we apply causal intervention by learning the objective p(y|do(x)) instead of p(y|x), which forces the input to be the observed data x, and removes all the arrows pointing to x as shown in Figure 1(b). When only considering zc, we can compute the in-\ntervened probability distribution by:\np(y|do(x)) = ∑ zc p(y|x, zc)p(zc)\n= Ezc∼p(zc)p(y|x, zc) (5)\nwhere zc is no longer determined by x, making p(zc|do(x)) = p(zc). When applying variational inference to zc, we have:\np(y|do(x)) ≥ Ezc∼qϕ(zc|y)pθ(y|x, zc) − KL(qϕ(zc|y)|p(zc))\n(6)\nIt can be seen that the confounder zc is more suitable than the mediator zm to cooperate with variational inference, as cutting off the link zc → x will naturally make p(zc|do(x)) to p(zc).\nWhen jointly considering zm and zc, we have: p(y|do(x)) = ∑ zm ∫ zc p(y, zm, zc|do(x))dzc\n≥ Ezm∼pφ(zm|x),zc∼qϕ(zc|y)[log pθ(y|x, zm, zc)]− KL(qϕ(zc|y)∥p(zc))\n(7) according to the intervened causal graph in Figure 1(b). The symbols ϕ, φ and θ denote the parameters of three probability modeling networks, respectively. It is worth noting that we do not apply variational inference to zm because finding a proper prior distribution p(zm|x) remains another big topic. Instead, our framework is easy to implement."
    }, {
      "heading" : "4.2 Making Latent Variables Meaningful",
      "text" : "However, there is no guarantee that zm and zc can represent the real mediators and confounders in Equation 7. If we have no other observed variables, the confounder zc would mainly represent the covariate which is naturally independent of x and has causal effects on y.\nTherefore, we further involve proxy variables m and c for zm and zc, respectively, where the full causal graph is shown in Figure 1. Proxy variables are the proxies of hidden or unmeasured variables (Miao et al., 2018). In practice, the mediators and the confounders are often too complex and can not be directly observed. For example, we may not be able to directly measure one’s socioeconomic status but we are probable to get a proxy by the zip code or job type (Louizos et al., 2017). To make the latent variables zm and zc meaning-\nful, we add two additional networks and the learning objective is maximizing:\nEzm∼pφ(zm|x),zc∼qϕ(zc|y)[log pθ(y|x, zm, zc)]− KL(qϕ(zc|y)∥p(zc)) + Ezm∼pφ(zm|x)[log pΦ(m|zm)] + Ezc∼qϕ(zc|y)[log pΨ(c|zc)] (8)\nwhere Φ and Ψ denote the parameters of the two additional networks.\nBack-Prediction from the Confounder. As shown in Figure 1(a), the confounder zc inferred from y also have a causal effect on x. Otherwise, the confounder will collapse into the covariate. The spurious correlations we have observed are that models often generate linguistically similar but logically inconsistent outputs. For example, \"the United Kingdom\" and \"the United State\" instead of \"the United Kingdom\" because the two entities are linguistically similar to each other. Therefore, we assume the proxy confounders c to be the not-mentioned entities in the given table. And we keep those high-frequency entities in the training set (≥ 5 times). Let c = {ci,j} ∈ RNc×Lc where ci,j denotes the j-th token of i-th entity, and Nc and Lc denote the number of entities and maximum length of the entity, respectively. The logprobability log pΨ(c|zc) is computed by:\nlog pΨ(c|zc) = ∑ i,j log pΨ(ci,j |zc, ci,<j) (9)\nwhere ci,<j denotes the tokens preceding to the jth token in the i-th entity. Then we minimize the cross-entropy between pΨ(c|zc) and p(c).\nSupervision for the Mediator. In the logical table-to-text generation task, from the human perspective, the correct mediators may be the selected entities, the logical types, or the logical forms (Chen et al., 2020e). In this paper, we only consider the selected entities as it is relatively easy to extract while the logical types or forms are laborintensive to annotate. We represent the selected entities by m = {mi,j} ∈ RNm×Lm where mi,j denotes the j-th token of i-th entity, and Nm and Lm denote the number of entities and maximum token number of the entity, respectively. The logprobability pΦ(m|zm) is computed by:\nlog pΦ(m|zm) = ∑ i,j log pΦ(mi,j |zm,mi,<j)\n(10) where mi,<j denotes the tokens preceding to the j-th token in the i-th entity."
    }, {
      "heading" : "4.3 Encoders and Decoders Implementation",
      "text" : "Then we introduce the implementations of pθ(y|x, zm, zc), pφ(zm|x), and qϕ(zc|y). We assume that the seq2seq model consists of an encoder Enc(·) and a decoder Dec(·) for pθ(y|x, zm, zc). And a target-oriented encoder T-Enc(·) is used for qϕ(zc|y).\nFirstly, we need to implement pφ(zm|x) and qϕ(zc|y). Let Hx be the hidden states of x encoded via Hx = Enc(x), and Ey be the embeddings of y before fed to the decoder Dec(·). We use a fully-connected neural network (FCNN) to project Hx followed with the average pooling to obtain zm. And we use the target-oriented encoder to encode Ey and obtain Hy = T-Enc(Ey). We apply the mean pooling operation to Hy and obtain hy. To modeling qϕ(zc|y) which is approximated to a Gaussian distribution, we use two FCNNs to process hy and obtain the mean vector µy and the log variance log σ2y which makes:\nqϕ(zc|y) = N (µy, σ2y) (11)\nTo implement pθ(y|x, zm, zc), our model cooperates an non-pretrained model \"FieldInfusing+Trans\" (Chen et al., 2020a) or a pretrained model \"GPT-TabGen\" (Chen et al., 2020a). Specifically, \"Field-Infusing+Trans\" uses an infusing field embedding network to produce header-words-aware and cell-position-aware embeddings Ep, then concatenate Ep with token embeddings to obtain the infused embeddings E = {ei} ∈ RLt×d where ei denotes the embedding of i-th token in the table x, and Lt and d denote the token number and the dimension, respectively. Then the decoder is used to decode y token by token: yt = Dec(H\nx,y≤t, zm, zc). The latent variables zm and zc are concatenated as one latent variable and projected by a FCNN to get a vector zm,c which has the same dimension with Hx. Then we add zm,c with Ey at each decoding step. When cooperated with \"GPT-TabGen\", the difference from \"Field-Infusing+Trans\" is that we use the GPT-2 as the encoder and decoder, and use the table linearization to indicate the cell position instead of the field-infusing method. More details about the table linearization can be found in (Chen et al., 2020a). And the vector zm,c is fed to the last Transformer layer of GPT-2 instead of the first layer, which brings less impact on the pretrained GPT-2."
    }, {
      "heading" : "4.4 Generate-then-Select Paradigm",
      "text" : "By sampling multiple latent variables zc ∼ p(zc), our model can generate multiple candidate sentences Ỹ = (ỹ1, ỹ2, ..., ỹNc) for the table x where Nc is the number of generated sentences. We propose to find out the best sentence by a trained selector. The generator optimized with MLE may focus more on the token-level matching than the sentence-level consistency while the selector will focus on improving the sentence-level scores. Therefore, it can be considered as an alternative of reinforcement learning. The selector scores each candidate sentence by si = Sχ(ỹi,x) where χ denotes the parameters of the selector network. Note that we are not designing a selector si = Sχ(ỹ\ni,y) because the reference sentence y is not available in practice.\nRecent work has provided several selectors including parsing-based and NLI-based models (Chen et al., 2020c). We can directly use these selectors but we aims to develop a more general selector jointly considering surface-level fidelity and logical fidelity. We use a mix of BLEU-3 (Papineni et al., 2002) and NLI-Acc (Chen et al., 2020a) scores to supervise the selector. In the training stage of the selector, we can get the gold scores of each generated candidate with the referenced sentence y by s∗i = S\n∗(ỹi,y). Then, we use BERT to encode x and yi followed with the average pooling layers to produce hs and his. Finally, we score the table-sentence pair represented by (hs,his) as follows:\nhf = hs ⊕ his ⊕ |hs − his| ⊕ hs ⊙ his Sχ(ỹ i,x) = σ(W shf ) (12)\nwhere ⊕ and ⊙ denote the concatenation and the element-wise multiplication operations, respectively. And W s denotes the parameters of the scoring network. The score Sχ(ỹi,x) is between 0 and 1, and better sentences need to be closer to 1. The scores of gold reference are set to 1. Then we use the margin-based triplet loss for the generated sentences in two way: comparing with gold sentences, and comparing between arbitrary two generated sentences. Given Nc generated candidate sentences, we rank the generated sentences according to the mix of BLEU-3 and NLI-Acc scores. The ranked sentences are denoted by Ỹ r = (ỹ1r , ỹ 2 r , ..., ỹ Nc r ) where ỹ 1 r has the highest score. Then the loss is computed as\nfollows: Lχ = max ( 0, Sχ(ỹ i r,x)− S(y,x) + γ1 ) +max ( 0, Sχ(ỹ j r,x)− S(ỹ i r,x) + γ2\n) (13)\nwhere γ1 and γ2 are the hyperparameters representing margin values, and i and j represent the ranked indexes. At the inference stage, we can select the best sentence with the highest score."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We conduct experiments on two datasets: LogicNLG (Chen et al., 2020a) and Logic2Text (Chen et al., 2020e). LogicNLG is constructed based on the positive statements of the Tabfact dataset (Chen et al., 2020c), which contains rich logical inferences in the annotated statements. Logic2Text is a smaller dataset and provides the annotation of logical forms. Since the annotations of logical forms are labor-intensive, we only use the tablesentence pairs, following the task formulation of LogicNLG. The statistics of the two datasets are shown in Table 1."
    }, {
      "heading" : "5.2 Evaluation and Settings",
      "text" : "The models are evaluated on the surface-level consistency and the logical fidelity. In terms of the surface-level consistency, we evaluate models on the sentence-level BLEU scores (Papineni et al., 2002) based on 1-3 grams matching. In terms of logical fidelity, we follow the recent work and apply three metrics including SP-Acc and NLIAcc based on semantic parsing and pretrained NLI model, respectively (Chen et al., 2020a). The metrics are computed with the officially released codes2.\nCompared Models. We compare our models with both non-pretrained and pretrained models. The non-pretrained models include \"Field-Gating\" (Liu et al., 2018) and \"Field-Infusing\" (Lebret et al., 2016) with LSTM decoder or Transformer\n2https://github.com/wenhuchen/LogicNLG\ndecoder, which are strong baselines among nonpretrained models. The pretrained models include \"BERT-TabGen\" and \"GPT-TabGen\" with the base size (Chen et al., 2020a). Moreover, for the LogicNLG dataset, we compare with a two-phrase approach denoted by \"GPT-Coarse-to-Fine\", which first generates a template and then generates the final sentence conditioning on the template (Chen et al., 2020a). For the variational models, we compare with the vanilla CVAE (Kingma et al., 2014) that approximates the prior distribution p(z|x) to p(z).\nHyperparameters. For the non-pretrained models, we set the dimension of LSTM or Transformer to 256. Our model is based on \"FieldInfusing+Trans\" which includes 3-layer Transformers in the encoder and decoder respectively. The posterior network qϕ(zc|y) contains a twolayer Transformer. For the pretrained models, we use the base version of BERT and GPT-2 which have an embedding size of 768. The KL loss is minimized with the annealing trick where the KL weight is set to 0 for 2 epochs and grows to 1.0 in another 5 epochs. The learning rate is initialized to set to 0.0001 and 0.000002 for non-pretrained and pretrained models, respectively. Each model is trained for 15 epochs. A special setting for our model is that we generate 10 candidate sentences for each table, and report the average performance and the best performance based on the selector, respectively. We set the hyperparameters γ1 = 0.2 and γ2 = 0.2 for the selector."
    }, {
      "heading" : "5.3 Main Results",
      "text" : "Table 2 and 3 present the performance of our model as well the compared models on the surfacelevel consistency and the logical fidelity. As shown, without the selector, our model DCVED already outperforms the baseline models \"FieldInfusing\" and \"GPT-TableGen\" on both LogicNLG and Logic2Text datasets. Specifically, when compared with \"Field-Infusing\", our model increases the BLEU-3, SP-Acc, and NLI-Acc scores by 1.4, 3.7, and 3.9 points, respectively on the LogicNLG dataset, and 0.2, 2.4, and 2.8 points on the Logic2Text dataset. When cooperating with GPT-2, our model outperforms \"GPT-TableGen\" by 1.6, 2.2, and 5.2 points of BLEU-3, SP-Acc, and NLI-Acc scores, respectively on the LogicNLG dataset, and 0.2, 1.3, and 5.4 points on the Logic2Text dataset. Moreover, our model\nalso outperforms the recent SOTA model \"GPTCoarse-to-Fine\" which increases the NLI-Acc score from 72.2 to 73.9 points on the Logic2Text dataset. When combining with the trained selector, our model further increases the NLI-Acc scores to 76.9 and 73.8 points on LogicNLG and Logic2Text datasets, respectively. We also show the upper bound of our model on BLEU and NLIAcc scores. Assume that two optimum selectors have access to the ground-truth sentences, and\nwould select the best sentence according to the BLEU-3 and NLI-Acc scores, respectively. As shown, a higher BLEU-3 score does not lead to a higher NLI-Acc score. Similarly, a higher NLI-Acc score does not yield a higher BLEU-3 score. The findings indicate that selecting candidates only by BLEU-3 or only by NLI-Acc is not enough. Instead, our trained selector comprehensively considers the BLEU-3 and NLI-Acc scores."
    }, {
      "heading" : "5.4 Ablation Study",
      "text" : "To analyze which mechanisms are driving the improvements, we present an ablation study in Table 4. We show different ablated models with different combinations of zc, zm, c and m. All these models are based on \"Field-Infusing\". Moreover, the vanilla CVAE is also compared, which can be considered as a baseline making both zm and zc independent from x.\nAs shown, both the mediators and the confounders are influential. The full model achieve the best SP-Acc and NLI-Acc scores with slightly lower BLEU-3 scores than the ablated model, DCVED (zc, zm, m). Eliminating c from the full model leads to a drop of NLI-Acc by 0.6 and 0.4 points on LogicNLG and Logic2Text, respectively. Further eliminating zm and m leads to a drop of NLI-Acc by 0.9 and 2.9 points on LogicNLG and Logic2Text, respectively. An interesting finding is that DCVED (zc, c) performs worse than DCVED (zc) on SP-Acc. The reason may be that predicting c from zc without considering the mediators zm may also lead to a bias, similar to CVAE. However, the ablated models all perform better than CVAE on SP-Acc and NLI-Acc."
    }, {
      "heading" : "5.5 Human Evaluation",
      "text" : "Following recent work (Chen et al., 2020a), we also perform human evaluation on the fluency and\nlogical fidelity. We randomly select 200 tables in the LogicNLG dataset, and generate one sentence per table for each model. Then we present the generated sentences to four raters without telling which model generates them. The raters are all post-graduate students majoring in computer science. We ask the raters to finish two binarydecision tasks: 1) whether a generated sentence is fluent; and 2) whether the fact of a generated sentence can be supported by the given table. We report the averaged results in Table 5, from which we can see that our model \"DCVED + GPTTabGen\" mainly increases the logical fidelity over the baseline model \"GPT-TabGen\" from 19.1% to 25.8%. When cooperated with the trained selector and the oracle NLI selector, our model further increase the logical fidelity to 30.8% and 37.1%, respectively. It is worth noting that the NLI selector can be represented by the scorer PNLI(ỹ,x), which does not require the ground-truth sentence y to be available (Chen et al., 2020a). It means that the setting of using the oracle NLI selector is acceptable."
    }, {
      "heading" : "5.6 Case Study",
      "text" : "To directly see the effect of our model, we present a case study in Figure 2. Several GPT-2 based models generate sentences describing two tables in the LogicNLG test set. The underlined red words represent the facts contradicting the table. As shown, for the first table, CVAE generates the sentence \"The album was released in the United State 2 time\", where the correct entity should be \"the United Kingdom\" according to the table. Instead, our model DCVED acknowledges that \"The album was released in the United Kingdom 2 time\". Moreover, compared with those deterministic models like GPT-TableGen and GPT-Coarseto-Fine, our model can generate sentences with different logical types. For the second table, we can see that many contradicting facts exist in recent models. For example, GPT-TableGen generates an incomplete sentence, which uses superla-\ncountry date Europe 17 october 2008 Australia 18 october 2008 United Kingdom 20 october 2008 United Kingdom 1 december 2008 United States 20 october 2008 Japan 22 october 2008 Germany 5 december 2008 Global ( itunes ) 19 november 2012\nCase 1: Black Ice (Album)\nGPT-TableGen: The album was released in the United State. GPT-Coarse-to-Fine: Black Ice was released in Germany and Japan. CVAE: The album was released in the United State 2 time. DCVED: The album was released in the United Kingdom 2 time. DCVED: The album was released in the United State before the release of the album in Japan.\nCase 2: Green Party of Canada\ntive logic but not mentions a specific year. Instead, our model produces two logically consistent sentences with superlative and comparative logic."
    }, {
      "heading" : "5.7 Limitations",
      "text" : "Although our model can improve the logical fidelity to a certain degree, all the models still get low scores in terms of the logical fidelity in human evaluation, which reflects the challenge of the task. Especially, we find that models do not perform well on certain types of tables: 1) containing and comparing between large numbers, e.g., 18,013 and 29,001 in a table; and 2) containing mixed logics so that models require multi-hop reasoning, e.g., models generating \"there were 3 nations that won 2 gold medals\" while the correct nation number is 4.\nTo deal with these problems, we believe that two directions of work may be workable: 1) enhancing the mediators. For example, the logical forms (Chen et al., 2020e) can be utilized as the mediator. But as mentioned in Section 4.2, it is label-intensive to annotate the logical forms; 2) large-scale knowledge grounded pre-training, which may be a more promising way. This type of work utilized the existing knowledge graphs or crawled data from Wikipedia (Chen et al., 2020b) to help models better encode/represent non-linguistic inputs, such as the numbers, the time, or the scores in the tables."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a de-confounded variational encoder-decoder for the logical table-to-text generation. Firstly, we assume two latent variables existed in the continuous space, representing the mediator and the confounder respectively. And we apply the causal intervention method to reduce the spurious correlations. Secondly, to make the latent variables meaningful, we use the exactly selected entities to supervise the mediator and the not selected but linguistically similar entities to supervise the confounder. Finally, since our model can generate multiple candidates for a table, we train a selector guided by both surface-level and logical fidelity to select the best sentence. The experiments show that our model yields competitive results with recent SOTA models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China under Grant 2018YFC0830400, and Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102."
    } ],
    "references" : [ {
      "title" : "Online back-parsing for amr-to-text generation",
      "author" : [ "Xuefeng Bai", "Linfeng Song", "Yue Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 1206–",
      "citeRegEx" : "Bai et al\\.,? 2020a",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Infobox-to-text generation",
      "author" : [ "Yang Bai", "Ziran Li", "Ning Ding", "Ying Shen", "HaiTao Zheng" ],
      "venue" : null,
      "citeRegEx" : "Bai et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Logical natural language generation from open-domain tables",
      "author" : [ "Wenhu Chen", "Jianshu Chen", "Yu Su", "Zhiyu Chen", "William Yang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, On-",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "KGPT: knowledge-grounded pretraining for data-to-text generation",
      "author" : [ "Wenhu Chen", "Yu Su", "Xifeng Yan", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, On-",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Tabfact: A large-scale dataset for table-based fact verification",
      "author" : [ "Wenhu Chen", "Hongmin Wang", "Jianshu Chen", "Yunkai Zhang", "Hong Wang", "Shiyang Li", "Xiyou Zhou", "William Yang Wang." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Chen et al\\.,? 2020c",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring logically dependent multi-task learning with causal inference",
      "author" : [ "Wenqing Chen", "Jidong Tian", "Liqiang Xiao", "Hao He", "Yaohui Jin." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
      "citeRegEx" : "Chen et al\\.,? 2020d",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Logic2text: Highfidelity natural language generation from logical forms",
      "author" : [ "Zhiyu Chen", "Wenhu Chen", "Hanwen Zha", "Xiyou Zhou", "Yunkai Zhang", "Sairam Sundaresan", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Chen et al\\.,? 2020e",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end content and plan selection for data-to-text generation",
      "author" : [ "Sebastian Gehrmann", "Falcon Z. Dai", "Henry Elder", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 11th International Conference on Natural Language Generation, Tilburg University,",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Creativity: Generating diverse questions using variational autoencoders",
      "author" : [ "Unnat Jain", "Ziyu Zhang", "Alexander G. Schwing." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,",
      "citeRegEx" : "Jain et al\\.,? 2017",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2017
    }, {
      "title" : "Text and causal inference: A review of using text to remove confounding",
      "author" : [ "Katherine A. Keith", "David Jensen", "Brendan O’Connor" ],
      "venue" : null,
      "citeRegEx" : "Keith et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Keith et al\\.",
      "year" : 2020
    }, {
      "title" : "Key fact as pivot: A",
      "author" : [ "Jie Zhou", "Xu Sun" ],
      "venue" : null,
      "citeRegEx" : "Zhou and Sun.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhou and Sun.",
      "year" : 2019
    }, {
      "title" : "Identifying causal effects with proxy variables of an unmeasured confounder",
      "author" : [ "Wang Miao", "Zhi Geng", "Eric J Tchetgen Tchetgen." ],
      "venue" : "Biometrika, 105(4):987–993.",
      "citeRegEx" : "Miao et al\\.,? 2018",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Vol-",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Totto: A controlled table-totext generation dataset",
      "author" : [ "Ankur P. Parikh", "Xuezhi Wang", "Sebastian Gehrmann", "Manaal Faruqui", "Bhuwan Dhingra", "Diyi Yang", "Dipanjan Das." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Parikh et al\\.,? 2020",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2020
    }, {
      "title" : "On measurement bias in causal inference",
      "author" : [ "Judea Pearl." ],
      "venue" : "uncertainty in artificial intelligence, pages 425–432.",
      "citeRegEx" : "Pearl.,? 2010",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2010
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Structural information preserving for graph-to-text generation",
      "author" : [ "Linfeng Song", "Ante Wang", "Jinsong Su", "Yue Zhang", "Kun Xu", "Yubin Ge", "Dong Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "VAE with a vampprior",
      "author" : [ "Jakub M. Tomczak", "Max Welling." ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 911 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, volume 84 of Proceedings of Machine",
      "citeRegEx" : "Tomczak and Welling.,? 2018",
      "shortCiteRegEx" : "Tomczak and Welling.",
      "year" : 2018
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space",
      "author" : [ "Liwei Wang", "Alexander G. Schwing", "Svetlana Lazebnik." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards faithful neural table-to-text generation with content-matching constraints",
      "author" : [ "Zhenyi Wang", "Xiaoyang Wang", "Bang An", "Dong Yu", "Changyou Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Line graph enhanced amr-to-text generation with mix-order graph attention networks",
      "author" : [ "Yanbin Zhao", "Lu Chen", "Zhi Chen", "Ruisheng Cao", "Su Zhu", "Kai Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : ", 2020b), graph-to-text (Song et al., 2020), and table-to-text (Wang et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : ", 2020), and table-to-text (Wang et al., 2020; Parikh et al., 2020) generation.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : ", 2020), and table-to-text (Wang et al., 2020; Parikh et al., 2020) generation.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "and logically faithful text from tables (Chen et al., 2020a).",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "the use of pretrained language models (LMs) like GPT-2 (Radford et al., 2018), which was shown to perform much better than non-pretrained models (Chen et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "However, it is still arguable that whether pretrained LMs can correctly capture the logics, as pretrained LMs like BERT would use spurious statistical cues for inference (Niven and Kao, 2019).",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "We estimate zc in the latent space based on variational inference, and cooperate the causal intervention based on Pearl’s do-calculus (Pearl, 2010) to learn the objective p(y|do(x)) instead of p(y|x).",
      "startOffset" : 134,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : "Transformerbased (Vaswani et al., 2017) models were also proposed which improved the ability to capture long-",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "a dataset collected from open domain (Chen et al., 2020a), which would score low on those models ignoring logical consistency.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "Machine learning models often suffer from the spurious statistical correlations brought by unmeasured or latent confounders (Keith et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "confounding bias, one approach is applying the causal intervention based on Pearl’s do-calculus (Pearl, 2010).",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "However, it remains an open problem to choose proper confounders, and the language of text itself could be a confounder (Keith et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "It is worth noting that high-quality observations of the mediators can also reduce the confounding bias, as the models will reduce the possibility of counting on the confounders (Chen et al., 2020d).",
      "startOffset" : 178,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "In the vanilla CVAE formulation, such as the ones adopted in (Kingma et al., 2014; Jain et al., 2017), the prior distribution p(z|x) is approximated to p(z), which is independent on x and fixed to a zero-mean unit-variance Gaussian distribution N (0, I).",
      "startOffset" : 61,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "However, this formulation is shown to induce a strong model bias (Tomczak and Welling, 2018) and empirically perform worse than non-variational models (Wang et al.",
      "startOffset" : 65,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "However, this formulation is shown to induce a strong model bias (Tomczak and Welling, 2018) and empirically perform worse than non-variational models (Wang et al., 2017) in multi-modal situation.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "Proxy variables are the proxies of hidden or unmeasured variables (Miao et al., 2018).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "entities, the logical types, or the logical forms (Chen et al., 2020e).",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Infusing+Trans\" (Chen et al., 2020a) or a pretrained model \"GPT-TabGen\" (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : ", 2020a) or a pretrained model \"GPT-TabGen\" (Chen et al., 2020a).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "More details about the table linearization can be found in (Chen et al., 2020a).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "We use a mix of BLEU-3 (Papineni et al., 2002) and NLI-Acc (Chen et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "We conduct experiments on two datasets: LogicNLG (Chen et al., 2020a) and Logic2Text (Chen et al.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "(Chen et al., 2020c), which contains rich logical inferences in the annotated statements.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "In terms of the surface-level consistency, we evaluate models on the sentence-level BLEU scores (Papineni et al., 2002) based on 1-3 grams matching.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "In terms of logical fidelity, we follow the recent work and apply three metrics including SP-Acc and NLIAcc based on semantic parsing and pretrained NLI model, respectively (Chen et al., 2020a).",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 2,
      "context" : "Following recent work (Chen et al., 2020a), we also perform human evaluation on the fluency and fluency % logical fidelity %",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "It is worth noting that the NLI selector can be represented by the scorer PNLI(ỹ,x), which does not require the ground-truth sentence y to be available (Chen et al., 2020a).",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "cal forms (Chen et al., 2020e) can be utilized as the mediator.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "of work utilized the existing knowledge graphs or crawled data from Wikipedia (Chen et al., 2020b) to help models better encode/represent non-linguistic inputs, such as the numbers, the time, or the scores in the tables.",
      "startOffset" : 78,
      "endOffset" : 98
    } ],
    "year" : 2021,
    "abstractText" : "Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables. The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text. The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table x and the sentence y. Specifically, in the training stage, a model can get a low empirical loss without understanding x and use spurious statistical cues instead. In this paper, we propose a de-confounded variational encoder-decoder (DCVED) based on causal intervention, learning the objective p(y|do(x)). Firstly, we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl’s do-calculus to alleviate the spurious correlations. Secondly, to make the latent confounder meaningful, we propose a backprediction process to predict the not-used entities but linguistically similar to the exactly selected ones. Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best candidate sentence for the given table. An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity.",
    "creator" : "LaTeX with hyperref package"
  }
}