{
  "name" : "2021.acl-long.538.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-Lingual Abstractive Summarization with Limited Parallel Resources",
    "authors" : [ "Yu Bai", "Yang Gao", "Heyan Huang" ],
    "emails" : [ "yubai@bit.edu.cn", "gyang@bit.edu.cn", "hhy63@bit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6910–6924\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6910"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-lingual summarization (CLS) helps people efficiently grasp salient information from articles in a foreign language. Neural approaches to CLS require large scale datasets containing millions of cross-lingual document-summary pairs (Zhu et al.,\n∗Corresponding author.\n2019; Cao et al., 2020; Zhu et al., 2020). However, two challenges arise with these approaches: 1) most languages are low-resource, thereby lacking document-summary paired data; 2) large parallel datasets across different languages for neuralbased CLS are rare and expensive, especially under the current trend of neural networks. Therefore, a low-resource setting is more realistic, and challenging, one for cross-lingual summarization. To our best knowledge, cross-lingual summarization under low-resource settings has not been well investigated and explored. Therefore, in this paper, we will develop a new model for cross-lingual abstractive summarization under limited supervision.\nFor low-resource settings, multi-task learning has been shown to be an effective method since it can borrow useful knowledge from other relevant tasks to use in the target task (Yan et al., 2015; Wang et al., 2020; Motiian et al., 2017). Crosslingual summarization can be viewed as the combination of two tasks, i.e., monolingual summarization (MS) and cross-lingual translation (Zhu et al., 2019). A wealth of relationships exist across the target summaries of MS and CLS tasks, such as translation alignments and summarization patterns. Illustrated in Figure 1, “叙利亚” is mapped to “Syria”, and similar maping is done with the other\naligned phrases. Obviously, leveraging these relationships is crucial for the task of transferring summarization knowledge from high-resource languages to low-resource languages. Unfortunately, existing multi-task frameworks simply utilize independent decoders to conduct MS and CLS task separately (Zhu et al., 2019; Cao et al., 2020), which leads to failure in capturing these relationships.\nTo solve this problem, we establish reliant connections between MS and CLS tasks, making the monolingual task a prerequisite for the crosslingual task. Specifically, one decoder is shared by both MS and CLS tasks; this is done by setting the generation target as a sequential concatenation of a monolingual summary and the corresponding cross-lingual summary. Sequentially generating monolingual and cross-lingual summaries, the decoder also conducts the translation task between them, which enhances the interactions between different languages. These interactions implicitly involve translation alignments, similarity in semantic units, and summary patterns across different lingual summaries. To demonstrate these decoder interactions, we further visualize them by probing Transformer attention heads in the model. Based on this process, the new structure with these advanced interactions enhances low-resource scenarios which require the model to be capable of transferring summary knowledge from high-resource languages to low-resource language. We name our model Multi-task Cross-Lingual Abstractive Summarization (MCLAS) under limited resources.\nIn terms of a training strategy under limited resources, we first pre-train MCLAS on large-scale monolingual document-summary parallel datasets to well-equip the decoder with general summary capability. Given a small amount of parallel crosslingual summary samples, the model is then finetuned and is able to transfer the learned summary capability to the low-resource language, leveraging the interactions uncovered by the shared decoder.\nExperiments on Zh2EnSum (Zhu et al., 2019) and a newly developed En2DeSum dataset demonstrate that MCLAS offers significant improvements when compared with state-of-the-art cross-lingual summarization models in both low-resource scenarios and full-dataset scenario. At the same time, we also achieved competitive performances in the En2ZhSum dataset (Zhu et al., 2019). Human evaluation results show that MCLAS produces more fluent, concise and informative summaries\nthan baselines models under limited parallel resources. In addition, we analyzed the length of generated summaries and the success of monolingual generation to verify advantages offered by identifying interactions between languages. We further investigate the explainability of the proposed multi-task structure by probing the attention heads in the unified decoder, proving that MCLAS learns the alignments and interactions between two languages, and this facilitates translation and summarization in the decoder stage. Our analysis provides a clear explanation of why MCLAS is capable of supporting CLS under limited resources. Our implementation and data are available at https://github.com/WoodenWhite/MCLAS."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Cross-Lingual Summarization",
      "text" : "Recently, cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information.\nTraditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016). These pipeline systems first translate the document and then summarize it or vice versa. Shen et al. (2018) propose the use of pseudo summaries to train the cross-lingual abstractive summarization model. In contrast, Duan et al. (2019a) and Ouyang et al. (2019) generate pseudo sources to construct the cross-lingual summarization dataset.\nThe first large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019). Additionly, Zhu et al. (2019) propose a multi-task framework to improve their cross-lingual summarization system. Following Zhu et al. (2019), more methods have been proposed to improve the CLS task. Zhu et al. (2020) use a pointer-generator network to exploit the translation patterns in cross-lingual summarization. Cao et al. (2020) utilize two encoders and two decoders to jointly learn to align and summarize. In contrast to previous methods, MCLAS generates the concatenation of monolingual and crosslingual summaries, thereby modeling relationships between them."
    }, {
      "heading" : "2.2 Low-Resource Natural Language Generation",
      "text" : "Natural language generation (NLG) for lowresource languages or domains has attracted lots of attention. Gu et al. (2018) leverage meta-learning\nto improve low-resource neural machine translation. Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a). However, these models require large-scale pretraining. Our work does not require any large pretrained generation models or translation models, enabling a vital decreases in training cost."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Neural Cross-lingual Summarization",
      "text" : "Given a source document DA = {xA1 , xA2 , . . . , xAm} in language A, a monolingual summarization system converts the source into a summary SA = {yA1 , yA2 , . . . , yAn }, wherem and n are the lengths of DA and SA, respectively. A cross-lingual summarization system produces a summary SB = {yB1 , yB2 , . . . , yBn′} consisting of tokens yB in target language B, where n′ is the length of SB . Note that the mentioned xA, yA, and yB are all tokens.\nZhu et al. (2019) propose using the Transformer (Vaswani et al., 2017) to conduct crosslingual summarization tasks. The Transformer is composed of stacked encoder and decoder layers. The encoder layer is comprised of a self-attention layer and a feed-forward layer. The decoder layer shares the same architecture as the encoder except for an extra encoder-decoder attention layer, which performs multi-head attention over the output of stacked encoder layers. The whole Transformer model θ is trained to maximize the conditional probability of the target sequence SB as follows:\nLNCLS = N∑ t=1 logP (yBt |yB<t, DA) (1)"
    }, {
      "heading" : "3.2 Improving NCLS with Multi-Task Frameworks",
      "text" : "Considering the relationship between CLS and MS, in which they share the same goal to summarize important information in a document, Zhu et al. (2019) proposed employing a one-to-many multitask framework to enhance the basic Transformer model. In this framework, one encoder is employed to encode the source document DA. Two separate decoders simultaneously generate a monolingual summary SA and a cross-lingual summary SB , leading to a loss as follows:\nLNCLS+MS = ∑n t=1 logP (y A t |yA<t, DA)\n+ ∑n′\nt=1 logP (y B t |yB<t, DA)\n(2)\nThis multi-task framework shares encoder representation to enhance cross-lingual summarization. However, independent decoders in this model are incapable of establishing alignments and connections between cross-lingual summaries."
    }, {
      "heading" : "4 MCLAS with Limited Parallel Resources",
      "text" : "To strengthen the connections mentioned, we propose making the monolingual task a prerequisite for the cross-lingual task through modeling interactions. According to previous work (Wan et al., 2010; Yao et al., 2015; Zhang et al., 2016), interactions between cross-lingual summaries (important phrase alignments, sentence lengths, and summary patterns, etc) are crucial for the final summary’s quality. We leverage these interactions to further transfer the rich-resource language knowledge. Detailed descriptions of this step are presented in following sections."
    }, {
      "heading" : "4.1 Multi-Task Learning in MCLAS",
      "text" : "To model interactions between languages, we need to share the decoder’s parameters. Inspired by Dong et al. (2019), we propose sharing the whole decoder to carry out both the translation and the summarization tasks. Specifically, we substitute the generation target SA with the sequential concatenation of SA and SB:\nSAB = {[BOS], yA1 , yA2 , . . . , yAn , [LSEP], yB1 , y B 2 , . . . , y B n′ , [EOS]}\n(3)\nwhere [BOS] and [EOS] are the beginning and end token of the output summaries, respectively. And\n[LSEP] is the special token used as the separator of SA and SB .\nWith the new generation target, the decoder learns to first generate SA, and then generate SB conditioned on SA and DA. The whole generation process is illustrated in Figure 2.\nFormally, we maximize the joint probability for monolingual and cross-lingual summarization:\nLMCLAS = ∑n t=1 logP (y A t |yA<t, DA)\n+ ∑n′\nt=1 logP (y B t |yB<t, SA, DA)\n(4)\nThe loss function can be divided into two terms. When generating SA, the decoder conducts the MS task based on DA, corresponding to the first term in Equation (4). When generating SB , the decoder already knows the information of corresponding monolingual summaries. In this way, it performs the translation task (for SA) and the CLS task (for DA), achieved by optimizing the second term in Equation (4). With the modification of the target, our model can easily capture interactions between cross-lingual summaries. The trained model shows effectiveness in aligning the summaries. Not only the output tokens, but also the attention distributions are aligned. The model we designed leverages this phenomenon to enable monolingual knowledge to be transferred under low-resource scenarios. Detailed investigation is presented in Section 6.\nWe adopt Transformers as our base model. In addition, we use multilingual BERT (Devlin et al., 2019) to initialize the encoder, improving its ability to produce multilingual representations. Additionally, having tried many different position embedding and language segmentation embedding methods, we find that [LSEP] is enough for the model to distinguish whether it is generating SB . Hence keeping the original position embedding (Vaswani et al., 2017) and employing no segmentation embedding are best for performance and efficiency."
    }, {
      "heading" : "4.2 Learning Schemes for MCLAS under Limited Resources",
      "text" : "Since our proposed framework enforces interactions between cross multilingual summaries, it has further benefits to the low-resource scenario, as only a few training summary samples are available in a cross-language. Yet, simply training from scratch can not make the best of our proposed model in low-resource scenarios. Hence we use a pre-training and fine-tuning paradigm to transfer the rich-resource language knowledge.\nFirst, we train the model in a monolingual summarization dataset. In this step, the model learns how to produce a monolingual summary for a given document. Then, we jointly learn MS and CLS with few training samples, optimizing Equation (4). We adopt similar initialization to existing CLS methods, which is introduced in Section 5.3."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "we conduct experiments on the En2ZhSum, Zh2EnSum CLS datasets1 (Zhu et al., 2019) and a newly constructed En2DeSum dataset. En2ZhSum is an English-to-Chinese dataset containing 364,687 training samples, 3,000 validation, and 3,000 testing samples. The dataset is converted from the union set of CNN/DM (Hermann et al., 2015) and MSMO (Zhu et al., 2018) using a round-trip translation strategy. Converted from the LCSTS dataset, Zh2EnSum contains 1,693,713 Chinese-to-English training samples, 3,000 validation, and 3,000 testing samples. To better verify the CLS ability of MCLAS, we construct a new English-to-German dataset (En2DeSum), using the same methods proposed by Zhu et al. (2019). We use WMT’19 English-German winner2 as our translation model to process the English Gigaword dataset.3 We set the threshold T1 = 0.6 and T2 = 0.2. The final En2DeSum contains 429,393 training samples, 4,305 validation samples, and 4,099 testing samples.\nAll the training samples contain a source document, a monolingual summary, and a cross-lingual summary. For the full-dataset scenario, we train the model with the whole dataset. For low-resource scenarios, we randomly select 3 different amounts\n1www.nlpr.ia.ac.cn/cip/dataset.htm 2https://github.com/pytorch/fairseq/\ntree/master/examples/translation 3LDC2011T07\n(minimum, medium, and maximum) of training samples for all datasets to evaluate our model’s performance under low-resource scenarios. Detailed numbers are presented in Table 1."
    }, {
      "heading" : "5.2 Training and Inference",
      "text" : "We use multilingual BERT (mBERT) (Devlin et al., 2019) to initialize our Transformer encoder. The decoder is a Transformer decoder with 6 layers. Each attention module has 8 different attention heads. The hidden size of the decoder’s self-attention is 768 and that of the feed-forward network is 2048. The final model contains 296,046,231 parameters. Because the encoder is pretrained when the decoder is randomly initialized, we use two separate optimizers for the encoder and the decoder (Liu and Lapata, 2019). The encoder’s learning rate ηe is set as 0.005, while the decoder’s learning rate ηd is 0.2. Warmup-steps for the encoder are 10,000 and 5,000 for the decoder. We train the model on two TITAN RTX GPUs for one day with gradient accumulation every 5 steps. Dropout with a probability 0.1 is applied before all the linear layers. We find that the target vocabulary type doesn’t have much influence on the final result. Therefore, we directly use mBERT’s subwords vocabulary as our target vocabulary. Nevertheless, in case tokens would be produced in the wrong language, we constructe a target token vocabulary for each target language. In the inference period, we only generate tokens from the corresponding vocabulary. During the decoding stage, we use beam search (size 5) and trigram block to avoid repetition. Length penalty is set between 0.6 and 1. All the hyperparameters are manually tuned using PPL and accuracy metric on the validation set."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compare MCLAS in low-resource scenarios with the following baselines:\nNCLS CLS model proposed by Zhu et al. (2019). In low-resource scenarios, we initialize our model with the pretrained MS model and then use a few samples to optimize Equation (1).\nNCLS+MS Multi-task framework proposed by Zhu et al. (2019). We find that NCLS+MS fails to converge when it is partly initialized by the pretrained MS model (the CLS decoder is randomly initialized). Hence, we fully initialize the multitask model using the pretrained MS model. Specifically, the two separate decoders are both initialized\nby the pretrained monolingual decoder. Then the model is optimized with Equation (2).\nTLTran Transformer-based Late Translation is a pipeline method. First, a monolingual summarization model summarizes the source document. A translation model is then applied to translate the summary. The summarization model is trained with monolingual document-summary pairs in three datasets. Specifically, we continue using WMT’19 English-German winner as the translation model for En2DeSum.\nSome recent proposed models improve the performance of CLS task. Methods NCLS+MT, TETran (Zhu et al., 2019), and the system proposed by Ouyang et al. (2019) require external long document machine translation (MT) corpora. The method proposed by Cao et al. (2020) requires not only parallel summaries but also document pairs translated by MT systems. Another method proposed by Zhu et al. (2020) requires bilingual lexicons extracted from large parallel MT datasets (2.08M sentence pairs from eight LDC corpora). We choose not to use these models as baselines since comparing MCLAS with them is unfair."
    }, {
      "heading" : "5.4 Automatic Evaluation Results",
      "text" : "The overall results under low-resource scenarios and full-dataset scenario are shown in Table 2. We reimplement a variety of models and evaluate them using F1 scores of the standard ROUGE metric (Lin, 2004) (ROUGE-1, ROUGE-2, and ROUGEL) and BERTScore4 (Zhang et al., 2019b). The following analysis is from our observations.\nIn the Zh2EnSum and En2DeSum datasets, MCLAS achieves significant improvements over baselines in all the low-resource scenarios. It is worth noting that combining NCLS+MS in our experiments does not bring much improvement to the NCLS model. We consider that this is because mBERT has already provided multilingual encoding for our models.\nHowever, we find that in the En2ZhSum dataset, MCLAS did not perform as well as that in the other two datasets. We speculate that is due to the imbalance of English reference and Chinese reference. The average length of SA and SB in En2ZhSum is 55.21 and 95.96, respectively (Zhu et al., 2019). This condition largely breaks the alignment between languages, leading to MCLAS\n4https://github.com/Tiiiger/bert_score\nthe performing slightly weaker. Despite this, results in En2DeSum and Zh2EnSum demonstrate that our proposed MCLAS model is effective for CLS under limited resources.\nFinally, our proposed model also has superior performance compared to baseline models given the full training dataset, achieving the best ROUGE score in En2DeSum and Zh2EnSum datasets."
    }, {
      "heading" : "5.5 Human Evaluation",
      "text" : "In addition to automatic evaluation, we conduct a human evaluation to verify our model’s performance. We randomly chose 60 examples (20 for each low-resource scenario) from the Zh2EnSum test dataset. Seven graduate students with high levels of fluency in English and Chinese are asked to assess the generated summaries and gold summaries from independent perspectives: informativeness, fluency, and conciseness. We follow the Best-Worst Scaling method (Kiritchenko and Mohammad, 2017). Participants are asked to indicate\nthe best and worst items from each perspective. The result scores are calculated based on the percentage of times each system is selected as best minus the times it is selected as worst. Hence, final scores range from -1 (worst) to 1 (best). Results are shown in Table 3.\nAs the data size increases, all the models achieve better results. Our proposed MCLAS outperformed NCLS and NCLS+MS in all the metrics. We notice that MCLAS is especially strong in conciseness. This phenomenon will be analyzed in Section 5.7\nWe show Fleiss’ Kappa scores of our conducted human evaluation in Table 4, which demonstrates a good inter-agreement among the participants."
    }, {
      "heading" : "5.6 Analysis on Initialization Methods",
      "text" : "We use a monolingual summarization model to initialize our model. However, whether this initialization method works is still in question. Therefore we compare our models with non-initialized models, shown in Figure 3. Among the three datasets, the initialization methods bring a huge improvement to all of the models."
    }, {
      "heading" : "5.7 Analysis on Summary Length",
      "text" : "One of the goals of automatic summarization is to produce brief text. Yet many neural auto-regressive models tend to produce a longer summary to improve the recall metric. Results in Table 5 show that interactions enable MCLAS to generate shorter summaries than other models, which more closely resembles human summaries. We can safely conclude that MCLAS can keep the summary in a fairly appropriate length, leading to concise generated summaries. We speculate that this is due to its ability to capture interactions between languages,\nconditioning cross-lingual summaries on relatively precise monolingual summaries."
    }, {
      "heading" : "5.8 Analysis on Monolingual Summarization",
      "text" : "Modeling interactions between languages brings many advantages. Specifically, we find that MCLAS can preserve more monolingual summarization knowledge than the NCLS+MS model during low-resource fine-tuning, or even promote its performance. We generate monolingual summaries with models trained in the maximum lowresource scenario. In Table 6, we can clearly see that MCLAS retains more monolingual summarization knowledge in the Zh2EnSum dataset. In the En2DeSum dataset, monolingual summarization performance is even significantly improved. We speculate that this is due to MCLAS’s ability to provide the interactions between languages.\nWe focus specifically on digging into results in En2DeSum, evaluating its detailed ROUGE and average summary length, presented in Table 7. We find that ROUGE improvement mainly resulted from precision while recall barely decrease the performances. This and the Avg. length metric shows that MCLAS produces more precise summaries while retaining most of the important information, leading to the metric increase in ROUGE."
    }, {
      "heading" : "5.9 Case Study",
      "text" : "In Figure 4, on the Zh2EnSum dataset, there is a list comparing the reference summary and outputs of models trained in the maximum low-resource scenario. Clearly, the NCLS model loses the information “two cars” and generates the wrong information “No.2 factory”. The NCLS+MS model is not accurate when describing the number of injured people, dropping important information “more than”. Additionally, the NCLS+MS model also has fluency and repetition issues: “in zhengzhou” appears twice in its generated summary. In contrast, MCLAS captures all of this information mentioned in both its Chinese and English output, and the English summary is well aligned with the Chinese summary. Finally, all of the models ignore the information “foxconn printed on the body of the car”. See Appendix A for more examples."
    }, {
      "heading" : "6 Probing into Attention Heads",
      "text" : "We have observed a successful alignment between SA and SB produced by our model in Section 5.9. In this section, we dig into this and analyze how the model learns the relationships. For a CLS task from document DA to SB , our hypotheses are: (1) the unified decoder is implicitly undertaking translation from SA to SB; (2) the unified decoder also conducts both monolingual and cross-lingual summarization. To verify these hypotheses, we visualize attention distributions of the Transformer decoders trained on En2ZhSum. Neural models\ncan be explicitly explained using probing into the attention heads (Michel et al., 2019; Voita et al., 2019). We follow the previous work and visualize the function of all attention heads in the decoder to verify the relationships of the concatenated cross-lingual summaries (i.e., translation) and cross-lingual document-summary pairs (i.e., summarization)."
    }, {
      "heading" : "6.1 Analysis on Translation",
      "text" : "We assume that the decoder translates only if the source summary SA and the target summary SB align well. This means that MCLAS is transferring knowledge from SA to SB . We visualize and probe all 48 self-attention heads in the unified decoder. We find 23 (47.9%) translation heads, defined as the heads attending from yBj to the corresponding words in language A. These heads undertake a translation function. 19 (39.6%) heads are local heads, attending to a few words before them and modeling context information. 12 (25%) heads are self heads, which only attend to themselves to retain the primary information. Some of the heads can be categorized into two types. Note that all of the heads behave similarly across different samples. We find that most of the heads are translation heads, indicating that our unified decoder is translating SA into SB . We sample some representative heads in Figure 5 to show their functionalities."
    }, {
      "heading" : "6.2 Analysis on Summarization",
      "text" : "To analyze whether the decoder for SB is simply translating from SA or that it also summarizes the source document, we visualize the distribution of 48 encoder-decoder attention heads. We find 28 (58.3%) summarization heads that attend to the document’s important parts when generating both the monolingual summary and the cross-lingual summary. We also find 20 (41.7%) translation heads, which focus on the source document when generating SA, while focusing on nothing when generating SB . We speculate that summarization heads are responsible for the summarization function and that translation heads cut down the relation between SB and source documentDA, leaving space for translation. Again, all the heads behave similarly across different samples. We select two representative samples in Figure 6.\nThe existence of both summarization and translation heads in encoder-decoder attention components supports our views: the unified decoder simultaneously conducts translation and summarization. Therefore, our model enhances the interactions between different languages, being able to facilitate cross-lingual summarization under lowresource scenarios. See Appendix B for detailed visualization results."
    }, {
      "heading" : "7 Discussions",
      "text" : "An ideal low-resource experiment should be conducted with real low-resource languages. Although possible, it takes much effort to acquire such datasets. Hence, it is the second-best choice that we simulate our low-resource scenarios by artificially limiting the amount of the available data. Some may question it about the feasibility of our method in real low-resource languages since machine translation systems, which is used to generate document-summary pairs, would be of lower quality for truly low-resource languages. For this concern, we consider it still possible to acquire thousands of high-quality human translated parallel summaries, as Duan et al. (2019b) adopt on their test set, to apply our method."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we propose a novel multi-task learning framework MCLAS to achieve cross-lingual abstractive summarization with limited parallel resources. Our model shares a unified decoder that\nsequentially generates both monolingual and crosslingual summaries. Experiments on two crosslingual summarization datasets demonstrate that our framework outperforms all the baseline models in low-resource and full-dataset scenarios."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the Joint Funds of the National Natural Science Foundation of China (Grant No. U19B2020), the Funds of the Integrated Application Software Project. We appreciate the helpful discussions with Sanxing Chen, Jia-Ao Zhan, Xuyang Lu, Xiao Liu, and Yuxiang Zhou. We also thank all the anonymous reviewers for their insightful suggestions."
    }, {
      "heading" : "A Samples",
      "text" : "We list some samples from outputs of various models. Samples from En2DeSum dataset are shown in Figure 7. Samples from Zh2EnSum dataset are shown in Figure 8. We randomly selected one sample from each low-resource scenario."
    }, {
      "heading" : "B Attention Distributions",
      "text" : "In Section “Probing into Attention Heads”, we selected some representative attention heads. We list all of our trained attention heads among 6 Transformer decoder layers in Figure 9 and Figure 10 for reference."
    } ],
    "references" : [ {
      "title" : "Jointly learning to align and summarize for neural crosslingual summarization",
      "author" : [ "Yue Cao", "Hui Liu", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6220–6231.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual natural language generation via pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Wenhui Wang", "XianLing Mao", "Heyan Huang." ],
      "venue" : "AAAI, pages 7570–7577.",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot crosslingual abstractive sentence summarization through teaching generation and attention",
      "author" : [ "Xiangyu Duan", "Mingming Yin", "Min Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Duan et al\\.,? 2019a",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot crosslingual abstractive sentence summarization through teaching generation and attention",
      "author" : [ "Xiangyu Duan", "Mingming Yin", "Min Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Duan et al\\.,? 2019b",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor OK Li", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in neural information processing systems, pages 1693–1701.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Bestworst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2017",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2017
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Are sixteen heads really better than one",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot adversarial domain adaptation",
      "author" : [ "Saeid Motiian", "Quinn Jones", "Seyed Mehdi Iranmanesh", "Gianfranco Doretto." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Motiian et al\\.,? 2017",
      "shortCiteRegEx" : "Motiian et al\\.",
      "year" : 2017
    }, {
      "title" : "A robust abstractive system for cross-lingual summarization",
      "author" : [ "Jessica Ouyang", "Boya Song", "Kathy McKeown." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Ouyang et al\\.,? 2019",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot cross-lingual neural headline generation",
      "author" : [ "Shi-qi Shen", "Yun Chen", "Cheng Yang", "Zhi-yuan Liu", "Mao-song Sun" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Shen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5926–5936.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Using bilingual information for cross-language document summarization",
      "author" : [ "Xiaojun Wan." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1546–1555.",
      "citeRegEx" : "Wan.,? 2011",
      "shortCiteRegEx" : "Wan.",
      "year" : 2011
    }, {
      "title" : "Cross-language document summarization based on machine translation quality prediction",
      "author" : [ "Xiaojun Wan", "Huiying Li", "Jianguo Xiao." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917–926.",
      "citeRegEx" : "Wan et al\\.,? 2010",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2010
    }, {
      "title" : "Generalizing from a few examples: A survey on few-shot learning",
      "author" : [ "Yaqing Wang", "Quanming Yao", "James T Kwok", "Lionel M Ni." ],
      "venue" : "ACM Computing Surveys (CSUR), 53(3):1–34.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask transfer methods to improve one-shot learning for multimedia event detection",
      "author" : [ "Wang Yan", "Jordan Yap", "Greg Mori." ],
      "venue" : "BMVC, pages 37–",
      "citeRegEx" : "Yan et al\\.,? 2015",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2015
    }, {
      "title" : "Phrase-based compressive cross-language summarization",
      "author" : [ "Jin-ge Yao", "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 118–127.",
      "citeRegEx" : "Yao et al\\.,? 2015",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2015
    }, {
      "title" : "Abstractive cross-language summarization via translation model enhanced predicate argument structure fusing",
      "author" : [ "Jiajun Zhang", "Yu Zhou", "Chengqing Zong." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(10):1842–1853.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1912.08777.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Msmo: Multimodal summarization with multimodal output",
      "author" : [ "Junnan Zhu", "Haoran Li", "Tianshang Liu", "Yu Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2018 conference on empirical methods in natural language processing, pages",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "Ncls: Neural cross-lingual summarization",
      "author" : [ "Junnan Zhu", "Qian Wang", "Yining Wang", "Yu Zhou", "Jiajun Zhang", "Shaonan Wang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    }, {
      "title" : "Attend, translate and summarize: An efficient method for neural cross-lingual summarization",
      "author" : [ "Junnan Zhu", "Yu Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "has been shown to be an effective method since it can borrow useful knowledge from other relevant tasks to use in the target task (Yan et al., 2015; Wang et al., 2020; Motiian et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 189
    }, {
      "referenceID" : 21,
      "context" : "has been shown to be an effective method since it can borrow useful knowledge from other relevant tasks to use in the target task (Yan et al., 2015; Wang et al., 2020; Motiian et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "has been shown to be an effective method since it can borrow useful knowledge from other relevant tasks to use in the target task (Yan et al., 2015; Wang et al., 2020; Motiian et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 189
    }, {
      "referenceID" : 28,
      "context" : ", monolingual summarization (MS) and cross-lingual translation (Zhu et al., 2019).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "Unfortunately, existing multi-task frameworks simply utilize independent decoders to conduct MS and CLS task separately (Zhu et al., 2019; Cao et al., 2020), which leads to failure in capturing these relationships.",
      "startOffset" : 120,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Unfortunately, existing multi-task frameworks simply utilize independent decoders to conduct MS and CLS task separately (Zhu et al., 2019; Cao et al., 2020), which leads to failure in capturing these relationships.",
      "startOffset" : 120,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "Experiments on Zh2EnSum (Zhu et al., 2019) and a newly developed En2DeSum dataset demonstrate that MCLAS offers significant improvements when compared with state-of-the-art cross-lingual summarization models in both low-resource scenarios and full-dataset scenario.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "At the same time, we also achieved competitive performances in the En2ZhSum dataset (Zhu et al., 2019).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 95,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 95,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 95,
      "endOffset" : 175
    }, {
      "referenceID" : 25,
      "context" : "Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a).",
      "startOffset" : 95,
      "endOffset" : 175
    }, {
      "referenceID" : 17,
      "context" : "(2019) propose using the Transformer (Vaswani et al., 2017) to conduct crosslingual summarization tasks.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "According to previous work (Wan et al., 2010; Yao et al., 2015; Zhang et al., 2016), interactions between cross-lingual summaries (important",
      "startOffset" : 27,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "According to previous work (Wan et al., 2010; Yao et al., 2015; Zhang et al., 2016), interactions between cross-lingual summaries (important",
      "startOffset" : 27,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "According to previous work (Wan et al., 2010; Yao et al., 2015; Zhang et al., 2016), interactions between cross-lingual summaries (important",
      "startOffset" : 27,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "In addition, we use multilingual BERT (Devlin et al., 2019) to initialize the encoder, improving its ability to produce multilingual representations.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "Hence keeping the original position embedding (Vaswani et al., 2017) and employing no segmentation embedding are best for performance and efficiency.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "we conduct experiments on the En2ZhSum, Zh2EnSum CLS datasets1 (Zhu et al., 2019) and a newly constructed En2DeSum dataset.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "verted from the union set of CNN/DM (Hermann et al., 2015) and MSMO (Zhu et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : ", 2015) and MSMO (Zhu et al., 2018) using a round-trip translation strategy.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "We use multilingual BERT (mBERT) (Devlin et al., 2019) to initialize our Transformer encoder.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "Methods NCLS+MT, TETran (Zhu et al., 2019), and the system proposed by Ouyang et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "using F1 scores of the standard ROUGE metric (Lin, 2004) (ROUGE-1, ROUGE-2, and ROUGEL) and BERTScore4 (Zhang et al.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "using F1 scores of the standard ROUGE metric (Lin, 2004) (ROUGE-1, ROUGE-2, and ROUGEL) and BERTScore4 (Zhang et al., 2019b).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "We follow the Best-Worst Scaling method (Kiritchenko and Mohammad, 2017).",
      "startOffset" : 40,
      "endOffset" : 72
    } ],
    "year" : 2021,
    "abstractText" : "Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequenceto-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to lowresource languages. To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting. Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the cross-lingual summarization (CLS) task. In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources.",
    "creator" : "LaTeX with hyperref"
  }
}