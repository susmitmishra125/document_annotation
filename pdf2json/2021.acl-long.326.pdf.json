{
  "name" : "2021.acl-long.326.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Psycholinguistic Tripartite Graph Network for Personality Detection",
    "authors" : [ "Tao Yang", "Feifan Yang", "Haolan Ouyang", "Xiaojun Quan" ],
    "emails" : [ "yangt225@mail2.sysu.edu.cn", "yangff6@mail2.sysu.edu.cn", "ouyhlan@mail2.sysu.edu.cn", "quanxj3@mail.sysu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4229–4239\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4229"
    }, {
      "heading" : "1 Introduction",
      "text" : "Personality detection from online posts aims to identify one’s personality traits from the online texts he creates. This emerging task has attracted great interest from researchers in computational psycholinguistics and natural language processing due to the extensive application scenarios such as\n∗Corresponding author.\npersonalized recommendation systems (Yang and Huang, 2019; Jeong et al., 2020), job screening (Hiemstra et al., 2019) and psychological studies (Goreis and Voracek, 2019).\nPsychological research shows that the words people use in daily life reflect their cognition, emotion, and personality (Gottschalk, 1997; Golbeck, 2016). As a major psycholinguistic instrument, Linguistic Inquiry and Word Count (LIWC) (Tausczik and Pennebaker, 2010) divides words into psychologically relevant categories (e.g., Function, Affect, and Social as shown in Figure 1) and is commonly used to extract psycholinguistic features in conventional methods (Golbeck et al., 2011; Sumner et al., 2012). Nevertheless, most recent works (Hernandez and Knight, 2017; Jiang et al., 2020; Keh et al., 2019; Lynn et al., 2020; Gjurković et al., 2020) tend to adopt deep neural networks (DNNs) to represent the posts and build predictive models in a data-driven manner. They first encode each post separately and then aggregate the post representations into a user representation. Although numerous improvements have been made over the traditional methods, they are likely to suffer from limitations as follows. First, the input of this task\nis usually a set of topic-agnostic posts, some of which may contain few personality cues. Hence, directly aggregating these posts based on their contextual representations may inevitably introduce noise. Second, personality detection is a typical data-hungry task since it is non-trivial to obtain personality tags, while DNNs implicitly extract personality cues from the texts and call for tremendous training data. Naturally, it is desirable to explicitly introduce psycholinguistic knowledge into the models to capture critical personality cues.\nMotivated by the above discussions, we propose a psycholinguistic knowledge-based tripartite graph network, namely TrigNet, which consists of a tripartite graph network to model the psycholinguistic knowledge and a graph initializer using a pre-trained language model such as BERT (Devlin et al., 2019) to generate the initial representations for all the nodes. As illustrated in Figure 1, a specific tripartite graph is constructed for each user, where three heterogeneous types of nodes, namely post, word, and category, are used to represent the posts of a user, the words contained both in his posts and the LIWC dictionary, and the psychologically relevant categories of the words, respectively. The edges are determined by the subordination between word and post nodes as well as between word and category nodes. Besides, considering that there are no direct edges between homogeneous nodes (e.g., between post nodes) in the tripartite graph, a novel flow GAT is proposed to only transmit messages between neighboring parties to reduce the computational cost and to allow for more effective interaction between nodes. Finally, we regard the averaged post node representation as the final user representation for personality classification. Benefiting from the tripartite graph structure, the interaction between posts is based on psychologically relevant words and categories rather than topic-agnostic context.\nWe conduct extensive experiments on the Kaggle and Pandora datasets to evaluate our TrigNet model. Experimental results show that it achieves consistent improvements over several strong baselines. Comparing to the state-of-the-art model, SN+Att (Lynn et al., 2020), TrigNet brings a remarkable boost of 3.47 in averaged Macro-F1 (%) on Kaggle and a boost of 2.10 on Pandora. Besides, thorough ablation studies and analyses are conducted and demonstrate that the tripartite graph and the flow GAT play an irreplaceable role in the boosts of\nperformance and decreases of computational cost. Our contributions are summarized as follows:\n• This is the first effort to use a tripartite graph to explicitly introduce psycholinguistic knowledge for personality detection, providing a new perspective of using domain knowledge.\n• We propose a novel tripartite graph network, TrigNet, with a flow GAT to reduce the computational cost in graph learning.\n• We demonstrate the outperformance of our TrigNet over baselines as well as the effectiveness of the tripartite graph and the flow GAT by extensive studies and analyses."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Personality Detection",
      "text" : "As an emerging research problem, text-based personality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).\nTraditional studies on this problem generally resort to feature-engineering methods, which first extracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical features by the bag-of-words model (Zhang et al., 2010). These features are then fed into a classifier such as SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al., 2018) to predict the personality traits. Despite interpretable features that can be expected, feature engineering has such limitations as it relies heavily on manually designed features.\nWith the advances of deep neural networks (DNNs), great success has been achieved in personality detection. Tandera et al. (2017) apply LSTM (Hochreiter and Schmidhuber, 1997) on each post to predict the personality traits. Xue et al. (2018) develop a hierarchical DNN, which depends on an AttRCNN and a variant of Inception (Szegedy et al., 2017) to learn deep semantic features from the posts. Lynn et al. (2020) first encode each post by a GRU (Cho et al., 2014) with attention and then pass the post representations to another GRU to produce the whole contextual representations. Recently, pre-trained language models have been applied to this task. Jiang et al. (2020) simply concatenate all the utterances from a single user into a document and encode it with BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Gjurković\net al. (2020) first encode each post by BERT and then use CNN (LeCun et al., 1998) to aggregate the post representations. Most of them focus on how to obtain more effective contextual representations, with only several exceptions that try to introduce psycholinguistic features into DNNs, such as Majumder et al. (2017) and Xue et al. (2018). However, these approaches simply concatenate psycholinguistic features with contextual representations, ignoring the gap between the two spaces."
    }, {
      "heading" : "2.2 Graph Neural Networks",
      "text" : "Graph neural networks (GNNs) can effectively deal with tasks with rich relational structures and learn a feature representation for each node in the graph according to the structural information. Recently, GNNs have attracted wide attention in NLP (Cao et al., 2019; Yao et al., 2019; Wang et al., 2020b,a). Among these research, graph construction lies at the heart as it directly impacts the final performance. Cao et al. (2019) build a graph for question answering, where the nodes are entities, and the edges are determined by whether two nodes are in the same document. Yao et al. (2019) construct a heterogeneous graph for text classification, where the nodes are documents and words, and the edges depend on word co-occurrences and document-word relations. Wang et al. (2020b) define a dependency-based graph by utilizing dependency parsing, in which the nodes are words, and the edges rely on the relations in the dependency parsing tree. Wang et al. (2020a) present a heterogeneous graph for extractive document summarization, where the nodes are words and sentences, and the edges depend on sentence-word relations. Inspired by the above successes, we construct a tripartite graph, which exploits psycholinguistic knowledge instead of simple document-word or sentence-word relations and is expected to contribute towards psychologically relevant node representations."
    }, {
      "heading" : "3 Our Approach",
      "text" : "Personality detection can be formulated as a multidocument multi-label classification task (Lynn et al., 2020; Gjurković et al., 2020). Formally, each user has a set P= {p1, p2, . . . , pr} of posts. Let pi= [wi,1, wi,2, . . . , wi,s] be the i-th post with s words, where pi can be viewed as a document. The goal of this task is to predict T personality traits Y= { yt }T t=1 for this user based on P , where yt ∈\n{0, 1} is a binary variable. Figure 2 presents the overall architecture of the proposed TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The former module aims to explicitly infuse psycholinguistic knowledge to uncover personality cues contained in the posts and the latter to encode each post and provide initial embeddings for the tripartite graph nodes. In the following subsections, we detail how the two modules work in four steps: graph construction, graph initialization, graph learning, and merge & classification."
    }, {
      "heading" : "3.1 Graph Construction",
      "text" : "As a major psycholinguistic analysis instrument, LIWC (Tausczik and Pennebaker, 2010) divides words into psychologically relevant categories and is adopted in this paper to construct a heterogeneous tripartite graph for each user.\nAs shown in the right part of Figure 2, the constructed tripartite graph G= (V, E) contains three heterogeneous types of nodes, namely post, word, and category, where V denotes the set of nodes and E represents the edges between nodes. Specifically, we define V=Vp ∪ Vw ∪ Vc, where Vp=P= {p1, p2, · · · , pr} denotes r posts, Vw= {w1, w2, · · · , wm} denotes m unique psycholinguistic words that appear both in the posts P and the LIWC dictionary, and Vc= {c1, c2, · · · , cn} represents n psychologically relevant categories selected from LIWC. The undirected edge eij between nodes i and j indicates word i either belongs to a post j or a category j.\nThe interaction between posts in the tripartite graph is implemented by two flows: (1) “p↔w↔p”, which means posts interact via their shared psycholinguistic words (e.g., “p1↔w1↔p2” as shown by the red lines in Figure 2); (2) “p↔w↔c↔w↔p”, which suggests that posts interact by words that share the same category (e.g., “p1↔w2↔c2↔w3↔p2” as shown by the green lines in Figure 2). Hence, the interaction between posts is based on psychologically relevant words or categories rather than topic-agnostic context."
    }, {
      "heading" : "3.2 Graph Initialization",
      "text" : "As shown in the left part of Figure 2, we employ BERT (Devlin et al., 2019) to obtain the initial embeddings of all the nodes. BERT is built upon the multi-layer Transformer encoder (Vaswani et al., 2017), which consists of a word embedding layer\nand 12 Transformer layers.1 Post Node Embedding The representations at the 12-th layer of BERT are usually used to represent an input sequence. This may not be appropriate for our task as personality is only weakly related to the higher order semantic features of posts, making it risky to rely solely on the final layer representations. In our experiments (Section 5.4), we find that the representations at the 11-th and 10-th layers are also useful for this task. Therefore, we utilize the representations at the last three layers to initialize the post node embeddings. Formally, the representations xjpi of the i-th post at the j-th layer can be obtained by:\nxjpi=BERT j ([CLS, wi,1, · · · , wi,m, SEP]) (1)\nwhere “CLS” and “SEP” are special tokens to denote the start and end of an input sentence, respectively, and BERTj (·) denotes the representation of the special token “CLS” at the j-th layer. In this way, we obtain the representations[ x10pi , x 11 pi , x 12 pi\n]T ∈ R3×d of the last three layers, where d is the dimension of each representation. We then apply layer attention (Peters et al., 2018) to collapse the three representations into a single vector xpi :\nxpi = 12∑ j=10 αjx j pi (2)\nwhere αj are softmax-normalized layer-specific weights to be learned. Consequently, we can obtain\n1“BERT-BASE-UNCASED” is used in this study.\na set of post representations for the given r posts of a user Xp = [xp1 , xp2 , · · · , xpr ]\nT ∈ Rr×d Word Node Embedding BERT applies WordPiece (Wu et al., 2016) to split words, which also cuts out-of-vocabulary words into small pieces. Thus, we obtain the initial node embedding of each word in Vw by considering two cases: (1) If the word is not out of vocabulary, we directly look up the BERT embedding layer to obtain its embedding; (2) If the word is out of vocabulary, we use the averaged embedding of its pieces as its initial node embedding. The initial word node embeddings are represented as Xw=[xw1 , xw2 , · · · , xwm ]\nT ∈ Rm×d. Category Node Embedding The LIWC2 dictionary divides words into 9 main categories and 64 subcategories.3 Empirically, subcategories such as Pronouns, Articles, and Prepositions are not task-related. Besides, our initial experiments show that excessive introduction of subcategories in the tripartite graph makes the graph sparse and makes the learning difficult, resulting in performance deterioration. For these reasons, we select all 9 main categories and the 6 personalconcern subcategories for our study. Particularly, the 9 main categories Function, Affect, Social, Cognitive Processes, Perceptual Processes, Biological Processes, Drives, Relativity, and Informal Language, and 6 personal-concern subcategories Work, Leisure, Home, Money, Religion, and Death are used as our category nodes. Then, we replace the “UNUSED” tokens in BERT’s vocab-\n2http://liwc.wpengine.com/ 3Details of the categories are listed in Appendix.\nulary by the 15 category names and look up the BERT embedding layer to generate their embeddings Xc=[xc1 , xc2 , · · · , xcn ] T ∈ Rn×d."
    }, {
      "heading" : "3.3 Graph Learning",
      "text" : "Graph attention network (GAT) (Veličković et al., 2018) can be applied over a graph to calculate the attention weight of each edge and update the node representations. However, unlike the traditional graph in which any two nodes may have edges, the connections in our tripartite graph only occur between neighboring parties (i.e., Vw ↔ Vp and Vw ↔ Vc), as shown in Figure 3. Therefore, applying the original GAT over our tripartite graph will lead to unnecessary computational costs. Inspired by Wang et al. (2020a), we propose a flow GAT for the tripartite graph. Particularly, considering that the interaction between posts in our tripartite graph can be accounted for by two flows “p↔w↔p” and “p↔w↔c↔w↔p”, we design a message passing mechanism that only transmits message by the two flows in the tripartite graph.\nFormally, given a constructed tripartite graph G = (V, E), where V = Vp∪Vw∪Vc, and the initial node embeddings X=Xp∪Xw∪Xc, we compute H (l+1)\np , H (l+1) w , and H (l+1)\nc as the hidden states of Vp, Vw and Vc at the (l+1)-th layer. The flow GAT layer is defined as follows:\nH (l+1) p ,H (l+1) w ,H (l+1) c = FGAT ( H (l) p ,H (l) w ,H (l) c ) (3)\nwhere H (1) p = Xp, H (1) w = Xw, and H (1)\nc = Xc. The function FGAT (·) is implemented by the two flows:\nĤ (l) w←p=MP ( H (l) w ,H (l) p ) H (l) p←w,p = MP ( H (l) p , Ĥ (l) w←p\n) (4) H (l) c←w,p = MP ( H (l) c , Ĥ (l) w←p\n) H (l) w←c,w,p = MP ( Ĥ (l) w←p,H (l) c←w,p\n) H (l) p←w,c,w,p = MP ( H (l) p ,H (l) w←c,w,p ) (5)\nH (l+1) p = mean ( H (l) p←w,p,H (l) p←w,c,w,p ) H (l+1) w = mean ( Ĥ (l) w←p,H (l) w←c,w,p\n) H (l+1)\nc = H (l) c←w,p\n(6)\nwhere← means the message is transmitted from the right nodes to the left nodes, mean (·) is the mean pooling function, and MP (·) represents the\nmessage passing function. Eq. (4) and Eq. (5) illustrate that message is transmitted by the flows “p↔w↔p” and p↔w↔c↔w↔p, respectively.\nWe take MP ( H (l)\nw ,H (l) p\n) in Eq. (4) as an ex-\nample to introduce the massage passing function, where H (l) w = [ h (l) w1 , h (l) w2 , · · · , h (l) wm ] are used as\nthe attention query and H (l) p = [ h (l) p1 , h (l) p2 , · · · , h (l) pr ] as the key and value. MP ( H (l)\nw ,H (l) p\n) can be de-\ncomposed into three steps. First, it calculates the attention weight βkij between node i in Vw and its neighbor node j in Vp at the k-th head:\nzkij = σ ( Wkz [ Wkwh (l) wi ||W k ph (l) pj ]) (7)\nβkij = exp\n( zkij ) ∑\nq∈Ni exp ( zkiq ) (8) where σ is the LeakyReLU activation function, Wkz , W k w and W k p are learnable weights, Ni means that the neighbor nodes of node i in Vp, and || is the concatenation operation. Second, the updated hidden state h̃(l)wi is obtained by a weighted combination of its neighbor nodes in Vp:\nh̃(l)wi = K\n|| k=1 tanh ∑ j∈Ni βkijW k vh (l) pj  (9) where K is the number of heads and Wkv is a learnable weight matrix. Third, noting that the above steps do not take the information of node i itself into account and to avoid gradient vanishing, we introduce a residual connection to produce the final updated node representation:\nĥ(l)wi = h (l) wi + h̃ (l) wi (10)"
    }, {
      "heading" : "3.4 Merge & Classification",
      "text" : "After L layers of iteration, we obtain the final node representations H(L)=H (L)\np ∪H (L) w ∪H (L) c . Then,\nwe merge all post node representations H (L)\np via mean pooling to produce the user representation:\nu = mean ([ h(L)p1 , h (L) p2 , · · · , h (L) pr ]) (11)\nFinally, we employ T softmax-normalized linear transformations to predict T personality traits. For the t-th personality trait, we compute:\np ( yt ) = softmax ( uWtu + b t u ) (12)\nwhere Wtu is a trainable weight matrix and b t u is a bias term. The objective function of our TrigNet model is defined as:\nJ (θ) = 1\nV V∑ v=1 T∑ t=1 [ −ytv log p ( ytv|θ )] (13)\nwhere V is the number of training samples, T is the number of personality traits, ytv is the true label for the t-th trait, and p(ytv|θ) is the predicted probability for this trait under parameters θ."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we introduce the datasets, baselines, and settings of our experiments."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020). The Kaggle dataset4 is collected from PersonalityCafe,5 where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pandora6 is another dataset collected from Reddit,7 where personality labels are extracted from short descriptions of users with MBTI results to introduce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nThe traits of MBTI include Introversion vs. Extroversion (I/E), Sensing vs. iNtuition (S/N), Think vs. Feeling (T/F), and Perception vs. Judging (P/J).\n4kaggle.com/datasnaek/mbti-type 5http://personalitycafe.com/forum 6https://psy.takelab.fer.hr/datasets/ 7https://www.reddit.com/\nFollowing previous works (Majumder et al., 2017; Jiang et al., 2020), we delete words that match any personality label to avoid information leaks. The Macro-F1 metric is adopted to evaluate the performance in each personality trait since both datasets are highly imbalanced, and average MacroF1 is used to measure the overall performance. We shuffle the datasets and split them in a 60-20-20 proportion for training, validation, and testing, respectively. According to our statistics, there are respectively 20.45 and 28.01 LIWC words on average in each post in the two datasets, and very few posts (0.021/0.002 posts per user) are presented as disconnected nodes in the graph. We show the statistics of the two datasets in Table 1."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al., 2018): Support vector machine (SVM) or XGBoost is utilized as the classifier with features extracted by TF-IDF and LIWC from all posts. BiLSTM (Tandera et al., 2017): Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) is firstly employed to encode each post, and then the averaged post representation is used for user representation. Glove (Pennington et al., 2014) is employed for the word embeddings. BERT (Keh et al., 2019): The fine-tuned BERT is firstly used to encode each post, and then mean pooling is performed over the post representations to generate the user representation. AttRCNN: This model adopts a hierarchical structure, in which a variant of Inception (Szegedy et al., 2017) is utilized to encode each post and a CNNbased aggregator is employed to obtain the user representation. Besides, it considers psycholinguistic knowledge by concatenating the LIWC features with the user representation.\nSN+Attn (Lynn et al., 2020): As the latest model, SN+Attn employs a hierarchical attention network, in which a GRU (Cho et al., 2014) with word-level attention is used to encode each post and another GRU with post-level attention is used to generate the user representation.\nTo make a fair comparison between the baselines and our model, we replace the post encoders in AttRCNN and SN+Attn with the pre-trained BERT."
    }, {
      "heading" : "4.3 Training Details",
      "text" : "We implement our TrigNet in Pytorch8 and train it on four NVIDIA RTX 2080Ti GPUs. Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of other components set to 1e-3. We set the maximum number of posts, r, to 50 and the maximum length of each post, s, to 70, considering the limit of available computational resources. After tuning on the validation dataset, we set the dropout rate to 0.2 and the mini-batch size to 32. The maximum number of nodes, r+m+ n, is set to 500 for Kaggle and 970 for Pandora, which cover 98.95% and 97.07% of the samples, respectively. Moreover, the two hyperparameters, the numbers of flow GAT layers L and heads K, are searched in {1, 2, 3} and {1, 2, 4, 6, 8, 12, 16, 24}, respectively, and the best choices are L = 1 and K = 12. The reasons for L = 1 are likely twofold. First, our flow GAT can already realize the interactions between nodes when L = 1, whereas the vanilla GAT needs to stack 4 layers. Second, after trying L = 2 and L = 3, we find that they lead to slight performance drops compared to that of L = 1."
    }, {
      "heading" : "5 Results and Analyses",
      "text" : "In this section, we report the overall results and provide thorough analyses and discussions.\n8https://pytorch.org/"
    }, {
      "heading" : "5.1 Overall Results",
      "text" : "The overall results are presented in Table 2, from which our observations are described as follows. First, the proposed TrigNet consistently surpasses the other competitors in F1 scores, demonstrating the superiority of our model on text-based personality detection with state-of-the-art performance. Specifically, compared with the existing state of the art, SN+Attn, TrigNet achieves 3.47 and 2.10 boosts in average F1 on the Kaggle and Pandora datasets, respectively. Second, compared with BERT, a basic module utilized in TrigNet, TrigNet yields 4.62 and 2.46 improvements in average F1 on the two datasets, verifying that the tripartite graph network can effectively capture the psychological relations between posts. Third, compared with AttRCNN, another method of leveraging psycholinguistic knowledge, TrigNet outperforms it with 3.61 and 2.38 increments in average F1 on the two datasets, demonstrating that our solution that injects psycholinguistic knowledge via the tripartite graph is more effective. Besides, the shallow models SVM and XGBoost achieve comparable performance to the non-pre-trained model BiLSTM, further showing that the words people used are important for personality detection."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "We conduct an ablation study of our TrigNet model on the Kaggle dataset by removing each component to investigate their contributions. Table 3 shows the results which are categorized into two groups.\nIn the first group, we investigate the contributions of the network components. We can see that removing the flow “p↔w↔c↔w↔p” defined in Eq. (5) results in higher performance declines than removing the flow “p↔w↔p” defined in Eq. (4), implying that the category nodes are helpful to capture personality cues from the texts. Besides, removing the layer attention mechanism also leads\nto considerable performance degradation. In the second group, we investigate the contribution of each category node. The results, sorted by scores of decrease from small to large, demonstrate that the introduction of every category node is beneficial to TrigNet. Among these category nodes, the Affect is shown to be the most crucial one to our model, as the average Macro-F1 score drops most significantly after it is removed. This implies that the Affect category reflects one’s personality obviously. Similar conclusions are reported by Depue and Collins (1999) and Zhang et al. (2019). In addition, the Function node is the least impactful category node. The reason could be that functional words reflect pure linguistic knowledge and are weakly connected to personality."
    }, {
      "heading" : "5.3 Analysis of the Computational Cost",
      "text" : "In this work we propose a flow GAT to reduce the computational cost of vanilla GAT. To show its\neffect, we compare it with vanilla GAT (as illustrated in the left part of Figure 3). The results are reported in Table 4, from which we can observe that flow GAT successfully reduces the computational cost in FLOPS and Memory by 38% and 32%, respectively, without extra parameters introduced. Besides, flow GAT is superior to vanilla GAT when the number of layers is 1. The cause is that the former can already capture adequate interactions between nodes with one layer, while the latter has to stack four layers to achieve this.\nWe also compare our TrigNet with the vanilla BERT in terms of the computational cost. The result show that the flow GAT takes about 1.14% more FLOPS than the vanilla BERT(297.3G)."
    }, {
      "heading" : "5.4 Layer Attention Analysis",
      "text" : "This study adopts layer attention (Peters et al., 2018) as shown in Eq. (2) to produce initial embeddings for post nodes. To show which layers are more useful, we conduct a simple experiment on the two datasets by using all the 12 layer representations of BERT and visualize the attention weight of each layer. As plotted in Figure 4, we find that the attention weights from layers 10 to 12 are significantly greater than that of the rest layers on both datasets, which explains why the last three layers are chosen for layer attention in our model."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we proposed a novel psycholinguistic knowledge-based tripartite graph network, TrigNet, for personality detection. TrigNet aims to introduce\nstructural psycholinguistic knowledge from LIWC via constructing a tripartite graph, in which interactions between posts are captured through psychologically relevant words and categories rather than simple document-word or sentence-word relations. Besides, a novel flow GAT that only transmits messages between neighboring parties was developed to reduce the computational cost. Extensive experiments and analyses on two datasets demonstrate the effectiveness and efficiency of TrigNet. This work is the first effort to leverage a tripartite graph to explicitly incorporate psycholinguistic knowledge for personality detection, providing a new perspective for exploiting domain knowledge."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The paper was fully supported by the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (No.2017ZT07X355).\nEthical Statement\nThis study aims to develop a technical method to incorporate psycholinguistic knowledge into neural models, rather than creating a privacy-invading tool. We worked within the purview of acceptable privacy practices and strictly followed the data usage policy. The datasets used in this study are all from public sources with all user information anonymized. The assessment results of the proposed model are sensitive and should be shared selectively and subject to the approval of the institutional review board (IRB). Any research or application based on this study is only allowed for research purposes, and any attempt to use the proposed model to infer sensitive user characteristics from publicly accessible data is strictly prohibited. To get the code, researchers need to sign an ethical statement and explain the purpose clearly."
    }, {
      "heading" : "A Categories of LIWC",
      "text" : "As shown in Figure 5, a total of 73 categories and subcategories are defined in the LIWC-2015 dictionary. There are 9 main categories: Function, Affect, Social, Cognitive Processes, Perceptual Processes, Biological Processes, Drives, Relativity, and Informal Language, in which 20 standard linguistic subcategories are included in the Function category and 44 psychological-relevant subcategories are defined in the rest 8 categories."
    } ],
    "references" : [ {
      "title" : "Bag: Bi-directional attention entity graph convolutional network for multi-hop reasoning question answering",
      "author" : [ "Yu Cao", "Meng Fang", "Dacheng Tao." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Survey analysis of machine learning methods for natural language processing for mbti personality type prediction",
      "author" : [ "Brandon Cui", "Calvin Qi." ],
      "venue" : "Available online: http://cs229.stanford.edu/proj2017/final-",
      "citeRegEx" : "Cui and Qi.,? 2017",
      "shortCiteRegEx" : "Cui and Qi.",
      "year" : 2017
    }, {
      "title" : "Neurobiology of the structure of personality: Dopamine, facilitation of incentive motivation, and extraversion",
      "author" : [ "Richard A Depue", "Paul F Collins." ],
      "venue" : "Behavioral and Brain Sciences, 22(3):491–517.",
      "citeRegEx" : "Depue and Collins.,? 1999",
      "shortCiteRegEx" : "Depue and Collins.",
      "year" : 1999
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Pandora talks: Personality and demographics on reddit",
      "author" : [ "Matej Gjurković", "Mladen Karan", "Iva Vukojević", "Mihaela Bošnjak", "Jan Šnajder." ],
      "venue" : "arXiv preprint arXiv:2004.04460.",
      "citeRegEx" : "Gjurković et al\\.,? 2020",
      "shortCiteRegEx" : "Gjurković et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting personality with social media",
      "author" : [ "Jennifer Golbeck", "Cristina Robles", "Karen Turner." ],
      "venue" : "CHI’11 Extended Abstracts on Human Factors in Computing Systems, pages 253–262.",
      "citeRegEx" : "Golbeck et al\\.,? 2011",
      "shortCiteRegEx" : "Golbeck et al\\.",
      "year" : 2011
    }, {
      "title" : "Predicting personality from social media text",
      "author" : [ "Jennifer Ann Golbeck." ],
      "venue" : "AIS Transactions on Replication Research, 2(1):2.",
      "citeRegEx" : "Golbeck.,? 2016",
      "shortCiteRegEx" : "Golbeck.",
      "year" : 2016
    }, {
      "title" : "A systematic review and meta-analysis of psychological research on conspiracy beliefs: Field characteristics, measurement instruments, and associations with personality traits",
      "author" : [ "Andreas Goreis", "Martin Voracek." ],
      "venue" : "Frontiers in Psychology, 10:205.",
      "citeRegEx" : "Goreis and Voracek.,? 2019",
      "shortCiteRegEx" : "Goreis and Voracek.",
      "year" : 2019
    }, {
      "title" : "The unobtrusive measurement of psychological states and traits",
      "author" : [ "Louis A Gottschalk." ],
      "venue" : "Text Analysis for the Social Sciences: Methods for Drawing Statistical Inferences from Texts and Transcripts, pages 117–129.",
      "citeRegEx" : "Gottschalk.,? 1997",
      "shortCiteRegEx" : "Gottschalk.",
      "year" : 1997
    }, {
      "title" : "Predicting myersbridge type indicator with text classification",
      "author" : [ "R Hernandez", "IS Knight." ],
      "venue" : "Proceedings of the 31st Conference on Neural Information Processing Systems, Long Beach, CA, USA, pages 4–9.",
      "citeRegEx" : "Hernandez and Knight.,? 2017",
      "shortCiteRegEx" : "Hernandez and Knight.",
      "year" : 2017
    }, {
      "title" : "Applicant perceptions of initial job candidate screening with asynchronous job interviews: Does personality matter",
      "author" : [ "Annemarie MF Hiemstra", "Janneke K Oostrom", "Eva Derous", "Alec W Serlie", "Marise Ph Born" ],
      "venue" : "Journal of Personnel Psychology,",
      "citeRegEx" : "Hiemstra et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hiemstra et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adaptive recommendation system for tourism by personality type using deep learning",
      "author" : [ "Chi-Seo Jeong", "Jong-Yong Lee", "Kye-Dong Jung." ],
      "venue" : "International Journal of Internet, Broadcasting and Communication, 12(1):55–60.",
      "citeRegEx" : "Jeong et al\\.,? 2020",
      "shortCiteRegEx" : "Jeong et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic text-based personality recognition on monologues and multiparty dialogues using attentive networks and contextual embeddings (student abstract)",
      "author" : [ "Hang Jiang", "Xianzhe Zhang", "Jinho D Choi." ],
      "venue" : "Proceedings of the AAAI Conference",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Myersbriggs personality classification and personalityspecific language generation using pre-trained language models. arXiv preprint arXiv:1907.06333",
      "author" : [ "Sedrick Scott Keh", "I Cheng" ],
      "venue" : null,
      "citeRegEx" : "Keh and Cheng,? \\Q2019\\E",
      "shortCiteRegEx" : "Keh and Cheng",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner." ],
      "venue" : "Proceedings of the IEEE, 86(11):2278–2324.",
      "citeRegEx" : "LeCun et al\\.,? 1998",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical modeling for user personality prediction: The role of messagelevel attention",
      "author" : [ "Veronica Lynn", "Niranjan Balasubramanian", "H Andrew Schwartz." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Lynn et al\\.,? 2020",
      "shortCiteRegEx" : "Lynn et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning-based document modeling for personality detection from text",
      "author" : [ "Navonil Majumder", "Soujanya Poria", "Alexander Gelbukh", "Erik Cambria." ],
      "venue" : "IEEE Intelligent Systems, 32(2):74–79.",
      "citeRegEx" : "Majumder et al\\.,? 2017",
      "shortCiteRegEx" : "Majumder et al\\.",
      "year" : 2017
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Predicting dark triad personality traits from twitter usage and a linguistic analysis of tweets",
      "author" : [ "Chris Sumner", "Alison Byers", "Rachel Boochever", "Gregory J Park." ],
      "venue" : "2012 11th International Conference",
      "citeRegEx" : "Sumner et al\\.,? 2012",
      "shortCiteRegEx" : "Sumner et al\\.",
      "year" : 2012
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alexander Alemi." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Szegedy et al\\.,? 2017",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2017
    }, {
      "title" : "Personality predictions based on user behavior on the facebook social media platform",
      "author" : [ "Michael M Tadesse", "Hongfei Lin", "Bo Xu", "Liang Yang." ],
      "venue" : "IEEE Access, 6:61959–61969.",
      "citeRegEx" : "Tadesse et al\\.,? 2018",
      "shortCiteRegEx" : "Tadesse et al\\.",
      "year" : 2018
    }, {
      "title" : "Personality prediction system from facebook users",
      "author" : [ "Tommy Tandera", "Derwin Suhartono", "Rini Wongso", "Yen Lina Prasetio" ],
      "venue" : "Procedia computer science,",
      "citeRegEx" : "Tandera et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tandera et al\\.",
      "year" : 2017
    }, {
      "title" : "The psychological meaning of words: Liwc and computerized text analysis methods",
      "author" : [ "Yla R Tausczik", "James W Pennebaker." ],
      "venue" : "Journal of Language and Social Psychology, 29(1):24–54.",
      "citeRegEx" : "Tausczik and Pennebaker.,? 2010",
      "shortCiteRegEx" : "Tausczik and Pennebaker.",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Heterogeneous graph neural networks for extractive document summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Yining Zheng", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Relational graph attention network for aspect-based sentiment analysis",
      "author" : [ "Kai Wang", "Weizhou Shen", "Yunyi Yang", "Xiaojun Quan", "Rui Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229—-",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning-based personality recognition from text posts of online social networks",
      "author" : [ "Di Xue", "Lifa Wu", "Zheng Hong", "Shize Guo", "Liang Gao", "Zhiyong Wu", "Xiaofeng Zhong", "Jianshan Sun." ],
      "venue" : "Applied Intelligence, 48(11):4232–4246.",
      "citeRegEx" : "Xue et al\\.,? 2018",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2018
    }, {
      "title" : "Mining personality traits from social messages for game recommender systems",
      "author" : [ "Hsin-Chang Yang", "Zi-Rui Huang." ],
      "venue" : "Knowledge-Based Systems, 165:157–168.",
      "citeRegEx" : "Yang and Huang.,? 2019",
      "shortCiteRegEx" : "Yang and Huang.",
      "year" : 2019
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7370–7377.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Persemon: A deep network for joint analysis of apparent personality, emotion and their relationship",
      "author" : [ "Le Zhang", "Songyou Peng", "Stefan Winkler." ],
      "venue" : "IEEE Transactions on Affective Computing.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding bag-of-words model: a statistical framework",
      "author" : [ "Yin Zhang", "Rong Jin", "Zhi-Hua Zhou." ],
      "venue" : "International Journal of Machine Learning and Cybernetics, 1(1-4):43–52.",
      "citeRegEx" : "Zhang et al\\.,? 2010",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "personalized recommendation systems (Yang and Huang, 2019; Jeong et al., 2020), job screening (Hiemstra et al.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "personalized recommendation systems (Yang and Huang, 2019; Jeong et al., 2020), job screening (Hiemstra et al.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : ", 2020), job screening (Hiemstra et al., 2019) and psychological studies (Goreis and Voracek, 2019).",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and psychological studies (Goreis and Voracek, 2019).",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Psychological research shows that the words people use in daily life reflect their cognition, emotion, and personality (Gottschalk, 1997; Golbeck, 2016).",
      "startOffset" : 119,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Psychological research shows that the words people use in daily life reflect their cognition, emotion, and personality (Gottschalk, 1997; Golbeck, 2016).",
      "startOffset" : 119,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "As a major psycholinguistic instrument, Linguistic Inquiry and Word Count (LIWC) (Tausczik and Pennebaker, 2010) divides words into psychologically relevant categories (e.",
      "startOffset" : 81,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : ", Function, Affect, and Social as shown in Figure 1) and is commonly used to extract psycholinguistic features in conventional methods (Golbeck et al., 2011; Sumner et al., 2012).",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : ", Function, Affect, and Social as shown in Figure 1) and is commonly used to extract psycholinguistic features in conventional methods (Golbeck et al., 2011; Sumner et al., 2012).",
      "startOffset" : 135,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "Nevertheless, most recent works (Hernandez and Knight, 2017; Jiang et al., 2020; Keh et al., 2019; Lynn et al., 2020; Gjurković et al., 2020) tend to adopt deep neural networks (DNNs) to represent the posts and build predictive models in a data-driven manner.",
      "startOffset" : 32,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "Nevertheless, most recent works (Hernandez and Knight, 2017; Jiang et al., 2020; Keh et al., 2019; Lynn et al., 2020; Gjurković et al., 2020) tend to adopt deep neural networks (DNNs) to represent the posts and build predictive models in a data-driven manner.",
      "startOffset" : 32,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "Nevertheless, most recent works (Hernandez and Knight, 2017; Jiang et al., 2020; Keh et al., 2019; Lynn et al., 2020; Gjurković et al., 2020) tend to adopt deep neural networks (DNNs) to represent the posts and build predictive models in a data-driven manner.",
      "startOffset" : 32,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "Nevertheless, most recent works (Hernandez and Knight, 2017; Jiang et al., 2020; Keh et al., 2019; Lynn et al., 2020; Gjurković et al., 2020) tend to adopt deep neural networks (DNNs) to represent the posts and build predictive models in a data-driven manner.",
      "startOffset" : 32,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "Motivated by the above discussions, we propose a psycholinguistic knowledge-based tripartite graph network, namely TrigNet, which consists of a tripartite graph network to model the psycholinguistic knowledge and a graph initializer using a pre-trained language model such as BERT (Devlin et al., 2019) to generate the initial representations for all the nodes.",
      "startOffset" : 281,
      "endOffset" : 302
    }, {
      "referenceID" : 19,
      "context" : "Comparing to the state-of-the-art model, SN+Att (Lynn et al., 2020), TrigNet brings a remarkable boost of 3.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 204
    }, {
      "referenceID" : 32,
      "context" : "sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 204
    }, {
      "referenceID" : 25,
      "context" : "sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 204
    }, {
      "referenceID" : 27,
      "context" : "Traditional studies on this problem generally resort to feature-engineering methods, which first extracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical features by the bag-of-words model (Zhang et al.",
      "startOffset" : 148,
      "endOffset" : 179
    }, {
      "referenceID" : 36,
      "context" : "Traditional studies on this problem generally resort to feature-engineering methods, which first extracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical features by the bag-of-words model (Zhang et al., 2010).",
      "startOffset" : 230,
      "endOffset" : 250
    }, {
      "referenceID" : 2,
      "context" : "These features are then fed into a classifier such as SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "These features are then fed into a classifier such as SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al., 2018) to predict the personality traits.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "(2017) apply LSTM (Hochreiter and Schmidhuber, 1997) on each post to predict the personality traits.",
      "startOffset" : 18,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "(2018) develop a hierarchical DNN, which depends on an AttRCNN and a variant of Inception (Szegedy et al., 2017) to learn deep semantic features from the posts.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "(2020) first encode each post by a GRU (Cho et al., 2014) with attention and then pass the post representations to another GRU to produce the whole contextual representations.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "(2020) simply concatenate all the utterances from a single user into a document and encode it with BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "(2020) first encode each post by BERT and then use CNN (LeCun et al., 1998) to aggregate the post representations.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Personality detection can be formulated as a multidocument multi-label classification task (Lynn et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Personality detection can be formulated as a multidocument multi-label classification task (Lynn et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "As a major psycholinguistic analysis instrument, LIWC (Tausczik and Pennebaker, 2010) divides words into psychologically relevant categories and is adopted in this paper to construct a heterogeneous tripartite graph for each user.",
      "startOffset" : 54,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "As shown in the left part of Figure 2, we employ BERT (Devlin et al., 2019) to obtain the initial embeddings of all the nodes.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : "BERT is built upon the multi-layer Transformer encoder (Vaswani et al., 2017), which consists of a word embedding layer",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "We then apply layer attention (Peters et al., 2018) to collapse the three representations into a single vector xpi :",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "Graph attention network (GAT) (Veličković et al., 2018) can be applied over a graph to calculate the attention weight of each edge and update the node representations.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 215
    }, {
      "referenceID" : 10,
      "context" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 215
    }, {
      "referenceID" : 20,
      "context" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 215
    }, {
      "referenceID" : 5,
      "context" : "We choose two public MBTI datasets for evaluations, which have been widely used in recent studies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 215
    }, {
      "referenceID" : 20,
      "context" : "Following previous works (Majumder et al., 2017; Jiang et al., 2020), we delete words that match any personality label to avoid information leaks.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Following previous works (Majumder et al., 2017; Jiang et al., 2020), we delete words that match any personality label to avoid information leaks.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) and XGBoost (Tadesse et al., 2018): Support vector machine (SVM) or XGBoost is utilized as the classifier with features extracted by TF-IDF and LIWC from all posts.",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 26,
      "context" : "BiLSTM (Tandera et al., 2017): Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) is firstly employed to encode each post, and then the averaged post representation is used for user representation.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : ", 2017): Bi-directional LSTM (Hochreiter and Schmidhuber, 1997) is firstly employed to encode each post, and then the averaged post representation is used for user representation.",
      "startOffset" : 29,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "Glove (Pennington et al., 2014) is employed for the word embeddings.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : "AttRCNN: This model adopts a hierarchical structure, in which a variant of Inception (Szegedy et al., 2017) is utilized to encode each post and a CNNbased aggregator is employed to obtain the user representation.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "SN+Attn (Lynn et al., 2020): As the latest model, SN+Attn employs a hierarchical attention network, in which a GRU (Cho et al.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : ", 2020): As the latest model, SN+Attn employs a hierarchical attention network, in which a GRU (Cho et al., 2014) with word-level attention is used to encode each post and another GRU with post-level attention is used to generate the user representation.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "This study adopts layer attention (Peters et al., 2018) as shown in Eq.",
      "startOffset" : 34,
      "endOffset" : 55
    } ],
    "year" : 2021,
    "abstractText" : "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one’s language usage and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge from LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The graph initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.",
    "creator" : "LaTeX with hyperref"
  }
}